{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a><h2></h2>\n<div style=\"background-color:rgba(0, 153, 0, 0.5);border-radius: 15px 50px;display:fill\">\n    <h1><center>1. Introduction</center></h1>\n</div>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.1 | Goal</b></p>\n</div>\n\nWelcome to the May edition of the 2022 Tabular Playground Series! This month's challenge is a binary classification problem that includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions. It wouldn't have been possible without the open-source contributions of other participants, in particular: <br>\n\n1) @wti200 work on feature interactions <br>\n2) @cabaxiom for idenfitying unique_characters feature <br>\n3) GM @ambrosm for three public notebooks which are main inspiration for this notebook <br>\n4) GM @sudalairajkumar for listing all the Feature Interactions <br>\n\n\n\n\n# The overview of the buidling of metaclassfier:\n\n<div class=\"alert alert-block alert-info\"> \n    The architecture of a meta learning model involves two or more base models, often referred to as level-0 models, and a meta-model that combines the predictions of the base models, referred to as a level-1 model.\n      Level-0 Models (Base-Models): Models fit on the training data and whose predictions are compiled.<br>\n      Level-1 Model (Meta-Model): Model that learns how to best combine the predictions of the base models. <br> \n      It worth noting that when the predictions from Level-0 classifiers are used as the inputs for the Level-1 classifier,extreme care care must be taken as it can lead to overfitting. The below figure represents framework of the meta learning. The Leve-0 models need to do prediction on all each single fold, and learn from every four folds from the full data.<br> \n    \n</div>\n\n\n<br>\n<img src= \"https://yanpuli.github.io/images/stacking.jpg\" alt =\"TPA MAY\" style='width: 800px;'>\n\n<a id=\"1\"></a><h2></h2>\n## <font color=\"#blue\">The structure of notebook.</font>\n### <a href='#1'>1. Introduction </a><br>\n### <a href='#2'>2. Load data and preprocessing</a><br>\n### <a href='#3'>3. Feature Engineering</a><br>\n### <a href='#4'>4. Neural network based prediction</a><br>\n### <a href='#5'>5. LGBM based prediction</a><br>\n### <a href='#6'>6. XGB based prediction</a><br>\n### <a href='#7'>7. HistGradient based prediction</a><br>\n### <a href='#8'>8. Catboost based prediction</a><br>\n### <a href='#9'>9. Adaboost based prediction</a><br>\n### <a href='#10'>10. Random Forest based prediction</a><br>\n### <a href='#11'>11. Meta classifiers- preprocessing</a><br>\n### <a href='#12'>12. Meta classifiers</a><br>\n### <a href='#13'>13. References</a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a><h2></h2>\n<div style=\"background-color:rgba(0, 153, 0, 0.5);border-radius: 15px 50px;display:fill\">\n    <h1><center>2. Loading the data </center></h1>\n</div>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.1 | Train data</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport matplotlib.style as style \nstyle.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.offline as py \nfrom plotly.offline import init_notebook_mode, iplot\npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\n\n\n\ncell_hover = {  # for row hover use <tr> instead of <td>\n    'selector': 'td:hover',\n    'props': [('background-color', '#ffffb3')]\n}\nindex_names = {\n    'selector': '.index_name',\n    'props': 'font-style: italic; color: darkgrey; font-weight:normal;'\n}\nheaders = {\n    'selector': 'th:not(.index_name)',\n    'props': 'background-color: #000000; color: white;'\n}\nfrom IPython.display import HTML\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\nimport math\nimport random\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add\nfrom tensorflow.keras.utils import plot_model\n\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplt.rcParams['axes.facecolor'] = primary_bgcolor","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-25T02:35:49.680858Z","iopub.execute_input":"2022-05-25T02:35:49.681745Z","iopub.status.idle":"2022-05-25T02:36:01.637514Z","shell.execute_reply.started":"2022-05-25T02:35:49.681638Z","shell.execute_reply":"2022-05-25T02:36:01.636647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\ns=train.head()\ns.style.set_table_styles([cell_hover, index_names])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T02:36:01.639233Z","iopub.execute_input":"2022-05-25T02:36:01.639493Z","iopub.status.idle":"2022-05-25T02:36:20.125121Z","shell.execute_reply.started":"2022-05-25T02:36:01.639465Z","shell.execute_reply":"2022-05-25T02:36:20.122395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.2 | Feature Engineering</b></p>\n</div>\n\nThis below model is modified version of public notebooks of @ambrosm. Consider upvoting the original work. <br>\nTwo additional features are introduced (Feature interaction between f_00, f_01 with f_26 ) <br>","metadata":{}},{"cell_type":"code","source":"features = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nfor df in [train, test]:\n    # Extract the 10 letters of f_27 into individual features\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    # Feature interactions: create three ternary features\n    # Every ternary feature can have the values -1, 0 and +1\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    df['i_00_26'] = (df.f_00 + df.f_26 > 5.1).astype(int) - (df.f_00 + df.f_26 < -5.4).astype(int)  #addtional feature\n    df['i_01_26'] = (df.f_01 + df.f_26 > 5.1).astype(int) - (df.f_01 + df.f_26 < -5.4).astype(int)  #addtional feature\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    \nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nint_features = [f for f in features if test[f].dtype == int and f.startswith('f')]\nch_features = [f for f in features if f.startswith('ch')]\n# test[features].head(2)\ncorr = train.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(90, 50), facecolor=primary_bgcolor)\n# ax.text(-1.1, 0.16, 'Correlation between the Continuous Features', fontsize=10, fontweight='bold', fontfamily='serif')\nax.text(-1.1, 0.3, 'There is no features that pass more than 0.32 correlation within each other', fontsize=13, fontweight='light', fontfamily='serif')\n\n\n# plot heatmap\nres=sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',annot_kws={\"size\": 25},\n            cbar_kws={\"shrink\": .2}, vmin=0, vmax=1)\nres.set_xticklabels(res.get_xmajorticklabels(), fontsize = 28)\nres.set_yticklabels(res.get_ymajorticklabels(), fontsize = 28)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T02:36:20.129388Z","iopub.execute_input":"2022-05-25T02:36:20.130263Z","iopub.status.idle":"2022-05-25T02:36:54.388891Z","shell.execute_reply.started":"2022-05-25T02:36:20.130123Z","shell.execute_reply":"2022-05-25T02:36:54.387594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-info\"> 📌 \n     Great news, as we can from the above figure that two new features that were introdcued in this notebook i_00_26 and i_01_26 are showing good correlation with Target. Lets use them</div>","metadata":{}},{"cell_type":"code","source":"# It has been noticed that in addtion to various features extracted in f_27, by label encoding the F_27 the results are improved slightly.\n# there are several unique lables are noticed in f_27 column of test data set. Hence both train and test data is merged for label encoding.\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntemp_df =(pd.concat([train.f_27, test.f_27],ignore_index=True)).to_frame() # dataframe co\ntemp_df['f_27']  = le.fit_transform(temp_df['f_27'])\ntest['f_27']  = le.transform(test['f_27'])\ntrain['f_27'] = le.transform(train['f_27'])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T02:36:54.392534Z","iopub.execute_input":"2022-05-25T02:36:54.392886Z","iopub.status.idle":"2022-05-25T02:37:08.028855Z","shell.execute_reply.started":"2022-05-25T02:36:54.392846Z","shell.execute_reply":"2022-05-25T02:37:08.028119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a><h2></h2>\n<div style=\"background-color:rgba(0, 153, 0, 0.5);border-radius: 15px 50px;display:fill\">\n    <h1><center>3. Modeling </center></h1>\n</div>\n\n<br>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.1 | Neural network based prediction</b></p>\n</div>\nThis below model is modified version of public notebooks of @ambrosm. Consider upvoting the original work. The main differnce is number of Epchos and features <br>","metadata":{}},{"cell_type":"code","source":"def my_model():\n    \"\"\"Simple sequential neural network with four hidden layers.\n    \n    Returns a (not yet compiled) instance of tensorflow.keras.models.Model.\n    \"\"\"\n    activation = 'swish'\n    inputs = Input(shape=(len(features)))\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(inputs)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(1, #kernel_regularizer=tf.keras.regularizers.l2(1e-6),\n              activation='sigmoid',\n             )(x)\n    model = Model(inputs, x)\n    return model\n\nplot_model(my_model(), show_layer_names=False, show_shapes=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-25T02:37:08.030141Z","iopub.execute_input":"2022-05-25T02:37:08.030568Z","iopub.status.idle":"2022-05-25T02:37:09.454902Z","shell.execute_reply.started":"2022-05-25T02:37:08.030538Z","shell.execute_reply":"2022-05-25T02:37:09.453897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 500\nEPOCHS_COSINEDECAY = 150\nCYCLES = 1\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nDIAGRAMS = True\nUSE_PLATEAU = False\nBATCH_SIZE = 2048\nONLY_FIRST_FOLD = False\n\n# see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)\nfeatures = [f for f in test.columns if f != 'id']\ndef fit_model(X_tr, y_tr, X_va=None, y_va=None, run=0):\n    \"\"\"Scale the data, fit a model, plot the training history and optionally validate the model\n    \n    Returns a trained instance of tensorflow.keras.models.Model.\n    \n    As a side effect, updates y_va_pred, history_list and score_list.\n    \"\"\"\n    global y_va_pred\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    X_tr = scaler.fit_transform(X_tr)\n    \n    if X_va is not None:\n        X_va = scaler.transform(X_va)\n        validation_data = (X_va, y_va)\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    lr_start=0.01\n    if USE_PLATEAU and X_va is not None: # use early stopping\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=12, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else: # use cosine learning rate decay rather than early stopping\n        epochs = EPOCHS_COSINEDECAY\n        lr_end = 0.0002\n        def cosine_decay(epoch):\n            # w decays from 1 to 0 in every cycle\n            # epoch == 0                  -> w = 1 (first epoch of cycle)\n            # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n            epochs_per_cycle = epochs // CYCLES\n            epoch_in_cycle = epoch % epochs_per_cycle\n            if epochs_per_cycle > 1:\n                w = (1 + math.cos(epoch_in_cycle / (epochs_per_cycle-1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = my_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start),\n                  metrics='AUC',\n                  loss=tf.keras.losses.BinaryCrossentropy())\n\n    # Train the model\n    history = model.fit(X_tr, y_tr, \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    \n    if X_va is None:\n        print(f\"Training loss: {history_list[-1]['loss'][-1]:.4f}\")\n    else:\n        lastloss = f\"Training loss: {history_list[-1]['loss'][-1]:.4f} | Val loss: {history_list[-1]['val_loss'][-1]:.4f}\"\n        \n        # Inference for validation\n        y_va_pred = model.predict(X_va, batch_size=len(X_va), verbose=0)\n        #oof_list[run][val_idx] = y_va_pred\n        \n        # Evaluation: Execution time, loss and AUC\n        score = roc_auc_score(y_va, y_va_pred)\n        print(f\"Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}\"\n              f\" | {lastloss} | AUC: {score:.5f}\")\n        score_list.append(score)\n    return model, scaler\n\n\nprint(f\"{len(features)} features\")\nhistory_list = []\nscore_list = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    fit_model(X_tr, y_tr, X_va, y_va)\n    if ONLY_FIRST_FOLD: break # we only need the first fold\n\nprint(f\"OOF AUC:                       {np.mean(score_list):.5f}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-25T02:37:09.457049Z","iopub.execute_input":"2022-05-25T02:37:09.457319Z","iopub.status.idle":"2022-05-25T03:22:20.80682Z","shell.execute_reply.started":"2022-05-25T02:37:09.457286Z","shell.execute_reply":"2022-05-25T03:22:20.804926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(f\"{len(features)} features\")\n\nX_tr = train[features]\ny_tr = train.target\n\npredtest_list_cnn = []\npredtrain_list_cnn=[]\nfor seed in range(10):\n    # see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n    print(f\"{seed:2}\")\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    model, scaler = fit_model(X_tr, y_tr, run=seed)\n    predtest_list_cnn.append(model.predict(scaler.transform(test[features]),batch_size=BATCH_SIZE, verbose=VERBOSE))\n    predtrain_list_cnn.append(model.predict(scaler.transform(train[features]),batch_size=BATCH_SIZE, verbose=VERBOSE))  ","metadata":{"execution":{"iopub.status.busy":"2022-05-25T03:22:20.808595Z","iopub.execute_input":"2022-05-25T03:22:20.808833Z","iopub.status.idle":"2022-05-25T05:00:27.747336Z","shell.execute_reply.started":"2022-05-25T03:22:20.808806Z","shell.execute_reply":"2022-05-25T05:00:27.746411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.2 |  LGBM based prediction</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfeatures = [f for f in test.columns if f != 'id']\nntrain = train[features].shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits= NFOLDS, random_state=SEED, shuffle=True)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-25T05:00:27.74887Z","iopub.execute_input":"2022-05-25T05:00:27.749101Z","iopub.status.idle":"2022-05-25T05:00:27.920051Z","shell.execute_reply.started":"2022-05-25T05:00:27.749072Z","shell.execute_reply":"2022-05-25T05:00:27.919135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-25T05:00:27.921461Z","iopub.execute_input":"2022-05-25T05:00:27.921705Z","iopub.status.idle":"2022-05-25T05:00:27.935196Z","shell.execute_reply.started":"2022-05-25T05:00:27.921674Z","shell.execute_reply":"2022-05-25T05:00:27.934286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMClassifier\n\nlgb_params = {\n    'n_estimators': 300,\n    'min_child_samples': 8,\n    'max_bins': 51,\n    \n}\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(5):\n    print(f\"{seed:2}\")\n    lgb = SklearnHelper(clf=LGBMClassifier, seed=seed, params=lgb_params)\n    lgb_oof_train, lgb_oof_test = get_oof(lgb,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(lgb_oof_train))\n    print(score)\n    pred_train_list_clf.append(lgb_oof_train)\n    pred_test_list_clf.append(lgb_oof_test)\nlgb_oof_train=np.array(pred_train_list_clf).mean(axis=0)\nlgb_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(lgb_oof_train))\nprint(\"final_LGBM_AUC\",score)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T05:00:27.93806Z","iopub.execute_input":"2022-05-25T05:00:27.938302Z","iopub.status.idle":"2022-05-25T05:14:16.903177Z","shell.execute_reply.started":"2022-05-25T05:00:27.938271Z","shell.execute_reply":"2022-05-25T05:14:16.902199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.3 |  XGB based prediction</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_params = {\n    'n_estimators': 100,\n    'eval_metric': 'auc',\n    'silent': 0,\n    \n}\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(2): # you can increase maximum value of range upto 10. I have choosen it to be 1 so that the runtime is shorter.\n    print(f\"{seed:2}\")\n    xgb = SklearnHelper(clf=XGBClassifier, seed=SEED, params=xgb_params)\n    xgb_oof_train, xgb_oof_test = get_oof(xgb,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(xgb_oof_train))\n    print(score)\n    pred_train_list_clf.append(xgb_oof_train)\n    pred_test_list_clf.append(xgb_oof_test)\nxgb_oof_train=np.array(pred_train_list_clf).mean(axis=0)\nxgb_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(xgb_oof_train))\nprint(\"final_XGB_AUC\",score)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-25T05:14:16.904498Z","iopub.execute_input":"2022-05-25T05:14:16.904815Z","iopub.status.idle":"2022-05-25T05:51:26.304471Z","shell.execute_reply.started":"2022-05-25T05:14:16.90478Z","shell.execute_reply":"2022-05-25T05:51:26.303419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.4 |  HistGradientBoostingClassifier</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfeatures = [f for f in test.columns if f != 'id']\nHist_params = {\n    'max_iter': 200,    \n}\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(2): # you can increase maximum value of range upto 10. I have choosen it to be 1 so that the runtime is shorter.\n    print(f\"{seed:2}\")\n    Hist = SklearnHelper(clf=HistGradientBoostingClassifier, seed=SEED, params=Hist_params)\n    Hist_oof_train, Hist_oof_test = get_oof(Hist,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(Hist_oof_train))\n    print(score)\n    pred_train_list_clf.append(Hist_oof_train)\n    pred_test_list_clf.append(Hist_oof_test)\nHist_oof_train=np.array(pred_train_list_clf).mean(axis=0)\nHist_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(Hist_oof_train))\nprint(\"final_HIST_AUC\",score)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T05:51:26.305919Z","iopub.execute_input":"2022-05-25T05:51:26.306132Z","iopub.status.idle":"2022-05-25T05:58:40.543397Z","shell.execute_reply.started":"2022-05-25T05:51:26.306106Z","shell.execute_reply":"2022-05-25T05:58:40.542309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.5 |  CatBoostClassifier</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom catboost import CatBoostClassifier\niterations = 4500\ncat_params = {\n    'iterations': iterations,\n    'learning_rate': 0.02,\n    'early_stopping_rounds': 150,\n    'max_depth': 5,\n    'eval_metric': 'Accuracy',\n    'loss_function': 'Logloss',\n    'verbose': int(iterations/10)    \n}\n\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(2): # you can increase maximum value of range upto 10. I have choosen it to be 1 so that the runtime is shorter.\n    print(f\"{seed:2}\")\n    cat = SklearnHelper(clf=CatBoostClassifier, seed=SEED, params=cat_params)\n    cat_oof_train, cat_oof_test = get_oof(cat,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(cat_oof_train))\n    print(score)\n    pred_train_list_clf.append(cat_oof_train)\n    pred_test_list_clf.append(cat_oof_test)\ncat_oof_train=np.array(pred_train_list_clf).mean(axis=0)\ncat_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(cat_oof_train))\nprint(\"final_AUC\",score) ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-25T05:58:40.545362Z","iopub.execute_input":"2022-05-25T05:58:40.545729Z","iopub.status.idle":"2022-05-25T07:25:33.142829Z","shell.execute_reply.started":"2022-05-25T05:58:40.545683Z","shell.execute_reply":"2022-05-25T07:25:33.141752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.6 |  AdaBoostClassifier</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import AdaBoostClassifier\niterations = 100\nada_params = {\n    'n_estimators': iterations,  \n}\n\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(2):   # you can increase maximum value of range upto 10. I have choosen it to be 1 so that the runtime is shorter.\n    print(f\"{seed:2}\")\n    ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n    ada_oof_train, ada_oof_test = get_oof(ada,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(ada_oof_train))\n    print(score)\n    pred_train_list_clf.append(ada_oof_train)\n    pred_test_list_clf.append(ada_oof_test) \nada_oof_train=np.array(pred_train_list_clf).mean(axis=0)\nada_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(ada_oof_train))\nprint(\"final_AUC\",score)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T07:25:33.144287Z","iopub.execute_input":"2022-05-25T07:25:33.144633Z","iopub.status.idle":"2022-05-25T08:46:10.06Z","shell.execute_reply.started":"2022-05-25T07:25:33.144599Z","shell.execute_reply":"2022-05-25T08:46:10.059041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.7 |  RandomForestClassifier</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\nrf_params = {\n    'n_estimators': 100,\n}\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['target'].values\nx_train = train[features].values # Creates an array of the train data\nx_test = test[features].values # Creats an array of the test data\npred_test_list_clf= []\npred_train_list_clf = []\nfor seed in range(2): # you can increase maximum value of range upto 10. I have choosen it to be 1 so that the runtime is shorter.\n    print(f\"{seed:2}\")\n    rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n    rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) \n    score = roc_auc_score(y_train, np.array(rf_oof_train))\n    print(score)\n    pred_train_list_clf.append(rf_oof_train)\n    pred_test_list_clf.append(rf_oof_test)\nrf_oof_train=np.array(pred_train_list_clf).mean(axis=0)\nrf_oof_test=np.array(pred_test_list_clf).mean(axis=0)\nscore = roc_auc_score(y_train, np.array(rf_oof_train))\nprint(\"final_AUC\",score)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T08:46:10.061927Z","iopub.execute_input":"2022-05-25T08:46:10.062195Z","iopub.status.idle":"2022-05-25T10:32:13.950906Z","shell.execute_reply.started":"2022-05-25T08:46:10.062156Z","shell.execute_reply":"2022-05-25T10:32:13.94856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a><h2></h2>\n<div style=\"background-color:rgba(0, 153, 0, 0.5);border-radius: 15px 50px;display:fill\">\n    <h1><center>4. Meta Learning </center></h1>\n</div>\nTypically, a meta-model is trained on the predictions made by base models on out-of-sample data. That is, data not used to train the base models is fed to the base models, predictions are made, and these predictions, along with the expected outputs, provide the input and output pairs of the training dataset used to fit the meta-model. However, in this notebook I have used prediction of the whole data. <br>\n<div class=\"alert alert-block alert-info\">\n\nImportant Notes: \n1) Prediction from both CNN, XGB and other predictions are merged along with few other dominating features and then are used as training data for Meta learning classifier. <br>\n2) To avoid overfitting, a meta-model should be fit with light-weight algorithums. In here, I used Linear regression. For experiment purpose I have tried several other algorithums also. \n.</div>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.1 |  Preprocessing for Meta learning</b></p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"#merging all the Level-0 prediction of the traing data\nMetainput_train2 =pd.DataFrame()\nMetainput_train2['id']=train['id']\nMetainput_train2['target']=train['target']\nMetainput_train2['LGBM_train_pred'] = lgb_oof_train.ravel()\nMetainput_train2['XGB_train_pred'] =  xgb_oof_train.ravel()\nMetainput_train2['CNN_train_pred'] =  np.array(predtrain_list_cnn).mean(axis=0)\nMetainput_train2['HIST_train_pred'] =  Hist_oof_train.ravel()\nMetainput_train2['catboost'] =  cat_oof_train.ravel()\nMetainput_train2['adaboost'] =  ada_oof_train.ravel()\nMetainput_train2['rf'] =  rf_oof_train.ravel()\nMetainput_train2.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:32:13.95593Z","iopub.execute_input":"2022-05-25T10:32:13.956303Z","iopub.status.idle":"2022-05-25T10:32:14.444614Z","shell.execute_reply.started":"2022-05-25T10:32:13.956267Z","shell.execute_reply":"2022-05-25T10:32:14.443991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging all the Level-0 prediction of the test data\nMetainput_test2 =pd.DataFrame()\nMetainput_test2['id']=test['id']\nMetainput_test2['LGBM_train_pred'] = lgb_oof_test.ravel()\nMetainput_test2['XGB_train_pred'] =  xgb_oof_test.ravel()\nMetainput_test2['CNN_train_pred'] =  np.array(predtest_list_cnn).mean(axis=0)\nMetainput_test2['HIST_train_pred'] =  Hist_oof_test.ravel()\nMetainput_test2['catboost'] =  cat_oof_test.ravel()\nMetainput_test2['adaboost'] =  ada_oof_test.ravel()\nMetainput_test2['rf'] =  rf_oof_test.ravel()\nMetainput_test2.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:32:14.445946Z","iopub.execute_input":"2022-05-25T10:32:14.446644Z","iopub.status.idle":"2022-05-25T10:32:14.728872Z","shell.execute_reply.started":"2022-05-25T10:32:14.4466Z","shell.execute_reply":"2022-05-25T10:32:14.727949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_features_notarget = [x for x in Metainput_train2.columns if x != 'target']\nfrom sklearn.model_selection import StratifiedKFold\nval = np.zeros(Metainput_train2.shape[0])\npred = np.zeros(Metainput_train2.shape[0])\nx = Metainput_train2[imp_features_notarget].values\ny = Metainput_train2['target'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:32:14.730605Z","iopub.execute_input":"2022-05-25T10:32:14.730937Z","iopub.status.idle":"2022-05-25T10:32:14.946768Z","shell.execute_reply.started":"2022-05-25T10:32:14.730903Z","shell.execute_reply":"2022-05-25T10:32:14.945911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.1 |  Meta-learning classifier- LinearRegression</b></p>\n</div>\n\nit is extremly important that we do not over fit the Meta-learning classifier. For this reason very light weight classfier should be used. lets start with Linear regression first and see how it predicts.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel_lr = LinearRegression()\npred_list_lr = []\nfor seed in range(10):\n    print(f\"{seed:2}\")\n    folds = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n    for fold_index, (train_index,val_index) in enumerate(folds.split(x,y)):\n        print('Batch {} started...'.format(fold_index))\n        bst = model_lr.fit(x[train_index],y[train_index])\n        val[val_index] = model_lr.predict(x[val_index])\n        print('auc of this val set is {}'.format(roc_auc_score(y[val_index],val[val_index])))\n    pred_list_lr.append(model_lr.predict(Metainput_test2))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:32:14.948185Z","iopub.execute_input":"2022-05-25T10:32:14.948443Z","iopub.status.idle":"2022-05-25T10:32:25.036023Z","shell.execute_reply.started":"2022-05-25T10:32:14.948402Z","shell.execute_reply":"2022-05-25T10:32:25.033059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Based on the above its can be seen that Linear repression based Meta-classifier is unable to increase the accuracy  when compared with level-0 CNN model.\n### I have tried various models as Meta-classifier(Catboost, XGB, RF and Ridge...) with an object not to overfit the model. The following has proven to be best one.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"13\"></a><h2></h2>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#DAA520;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.2 |  Meta learning classifier- RandomForestRegressor</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import auc\nclf = RandomForestClassifier(\n    n_estimators=5,\n)\ndef compute_roc_auc(index):\n    y_predict = clf.predict_proba(x.iloc[index])[:,1]\n    fpr, tpr, thresholds = roc_curve(y.iloc[index], y_predict)\n    auc_score = auc(fpr, tpr)\n    \n    return fpr, tpr, auc_score\ncv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\nresults = pd.DataFrame(columns=['training_score', 'test_score'])\nfprs, tprs, scores = [], [], []\n\n\nx = Metainput_train2[imp_features_notarget]\ny = Metainput_train2['target']\nx_test = Metainput_test2[imp_features_notarget].values\npred_metarf_list = []\nfor (train_meta, test_meta), i in zip(cv.split(x, y), range(10)):\n    clf.fit(x.iloc[train_meta], y.iloc[train_meta])\n    _, _, auc_score_train = compute_roc_auc(train_meta)\n    print(auc_score_train)\n    fpr, tpr, auc_score = compute_roc_auc(test_meta)\n    scores.append((auc_score_train, auc_score))\n    fprs.append(fpr)\n    tprs.append(tpr)\n    clf.fit(x.iloc[train_meta], y.iloc[train_meta])\n    pred_metarf_list.append( (clf.predict_proba(Metainput_test2[imp_features_notarget].values)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:32:25.038049Z","iopub.execute_input":"2022-05-25T10:32:25.038616Z","iopub.status.idle":"2022-05-25T10:33:39.805829Z","shell.execute_reply.started":"2022-05-25T10:32:25.038557Z","shell.execute_reply":"2022-05-25T10:33:39.804805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test[['id']].copy()\nsubmission['target'] = np.array(pred_metarf_list).mean(axis=0)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:33:39.808897Z","iopub.execute_input":"2022-05-25T10:33:39.809249Z","iopub.status.idle":"2022-05-25T10:33:41.765278Z","shell.execute_reply.started":"2022-05-25T10:33:39.809203Z","shell.execute_reply":"2022-05-25T10:33:41.764289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a><h2></h2>\n<div style=\"background-color:rgba(0, 153, 0, 0.5);border-radius: 15px 50px;display:fill\">\n    <h1><center>4. References </center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/javigallego/tps-apr22-eda-fe-lstm-tutorial#1-|-Introduction <br>\nhttps://www.kaggle.com/code/andreshg/tps-apr-data-visualization-and-highlights <br>\nhttps://machinelearningmastery.com/histogram-based-gradient-boosting-ensembles/ <br>\nhttps://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard <br>\nhttps://yanpuli.github.io/posts/2018/01/blog-post-10/ <br>\nhttps://www.kaggle.com/code/remekkinas/ensemble-learning-meta-classifier-for-stacking#10.-Final-prediction-(ensemble-learning-with-meta-classifier)","metadata":{}}]}