{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# custom installs\n!pip uninstall matplotlib -y\n!pip install matplotlib==3.2.2\n!pip install mplcyberpunk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T07:37:44.241034Z","iopub.execute_input":"2022-06-16T07:37:44.241609Z","iopub.status.idle":"2022-06-16T07:38:08.93823Z","shell.execute_reply.started":"2022-06-16T07:37:44.241511Z","shell.execute_reply":"2022-06-16T07:38:08.937552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\nprint(matplotlib.__version__)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-16T07:38:08.940127Z","iopub.execute_input":"2022-06-16T07:38:08.940347Z","iopub.status.idle":"2022-06-16T07:38:08.945969Z","shell.execute_reply.started":"2022-06-16T07:38:08.94032Z","shell.execute_reply":"2022-06-16T07:38:08.944646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import gridspec\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom lightgbm import LGBMClassifier\nimport shap\nimport datatable\nimport mplcyberpunk\nplt.style.use(\"cyberpunk\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:08.947015Z","iopub.execute_input":"2022-06-16T07:38:08.947255Z","iopub.status.idle":"2022-06-16T07:38:10.578264Z","shell.execute_reply.started":"2022-06-16T07:38:08.947224Z","shell.execute_reply":"2022-06-16T07:38:10.577669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom imports\nimport datatable\nimport mplcyberpunk\nplt.style.use(\"cyberpunk\")\n\n# helpers\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",round(start_mem_usg,2),\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n#             print(\"******************************\")\n#             print(\"Column: \",col)\n#             print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n#             print(\"dtype after: \",props[col].dtype)\n#             print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",round(mem_usg,2),\" MB\")\n    print(\"This is \",round(100*mem_usg/start_mem_usg,2),\"% of the initial size\")\n    return props, NAlist","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-16T07:38:10.57959Z","iopub.execute_input":"2022-06-16T07:38:10.5799Z","iopub.status.idle":"2022-06-16T07:38:10.604976Z","shell.execute_reply.started":"2022-06-16T07:38:10.57986Z","shell.execute_reply":"2022-06-16T07:38:10.604113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df = datatable.fread(\"/kaggle/input/tabular-playground-series-may-2022/train.csv\").to_pandas().set_index('id') # faster but it sometimes converts the target variable from int type to boolean type.\n# train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2022/train.csv\").set_index('id')\ntrain_df,_ = reduce_mem_usage(train_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:10.60772Z","iopub.execute_input":"2022-06-16T07:38:10.60797Z","iopub.status.idle":"2022-06-16T07:38:12.157866Z","shell.execute_reply.started":"2022-06-16T07:38:10.607936Z","shell.execute_reply":"2022-06-16T07:38:12.157271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_df = datatable.fread(\"/kaggle/input/tabular-playground-series-may-2022/test.csv\").to_pandas().set_index('id')\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2022/test.csv\").set_index('id')\ntest_df,_ = reduce_mem_usage(test_df)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:12.158938Z","iopub.execute_input":"2022-06-16T07:38:12.159734Z","iopub.status.idle":"2022-06-16T07:38:18.048199Z","shell.execute_reply.started":"2022-06-16T07:38:12.159699Z","shell.execute_reply":"2022-06-16T07:38:18.047036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:18.04933Z","iopub.execute_input":"2022-06-16T07:38:18.049581Z","iopub.status.idle":"2022-06-16T07:38:18.125727Z","shell.execute_reply.started":"2022-06-16T07:38:18.049547Z","shell.execute_reply":"2022-06-16T07:38:18.124817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.9 Million samples in training data, 0.7 million samples in test data.  \nThere are 16 float features, 14 int features and 1 object feature","metadata":{}},{"cell_type":"code","source":"train_df.target.value_counts(normalize=True).round(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:18.127375Z","iopub.execute_input":"2022-06-16T07:38:18.127685Z","iopub.status.idle":"2022-06-16T07:38:18.141688Z","shell.execute_reply.started":"2022-06-16T07:38:18.127634Z","shell.execute_reply":"2022-06-16T07:38:18.14117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"51% of the samples are negative class, 49% of the samples are positive class","metadata":{}},{"cell_type":"code","source":"# Sampling 20 percent of the training data for faster experimentation (0.18 million)\ntrain_df_sampled = train_df.sample(int(len(train_df) * 0.2))\ntrain_df_sampled.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:18.142598Z","iopub.execute_input":"2022-06-16T07:38:18.143271Z","iopub.status.idle":"2022-06-16T07:38:18.30119Z","shell.execute_reply.started":"2022-06-16T07:38:18.143242Z","shell.execute_reply":"2022-06-16T07:38:18.300729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# String Feature\n\n9,00,000 training samples contain 7,41,354 unique values with the most occurring string sequence appearing 12 times","metadata":{}},{"cell_type":"code","source":"train_df[\"f_27\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:18.302246Z","iopub.execute_input":"2022-06-16T07:38:18.302593Z","iopub.status.idle":"2022-06-16T07:38:19.648054Z","shell.execute_reply.started":"2022-06-16T07:38:18.302565Z","shell.execute_reply":"2022-06-16T07:38:19.647486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above 10 values has constant length of 10. Checking if the length of the string sequence always has length of 10.","metadata":{}},{"cell_type":"code","source":"(train_df.f_27.str.len().min(), train_df.f_27.str.len().max(), test_df.f_27.str.len().min(), test_df.f_27.str.len().min())","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:19.649108Z","iopub.execute_input":"2022-06-16T07:38:19.64971Z","iopub.status.idle":"2022-06-16T07:38:21.531778Z","shell.execute_reply.started":"2022-06-16T07:38:19.649675Z","shell.execute_reply":"2022-06-16T07:38:21.531031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9,00,000 training samples contain 7,41,354 unique string sequence in f_27 (most of the values are different). \nLets check if the string sequences in test data is same as in training data. There are 11,81,880 - 7,41,354 = 440526 string sequences which do not occur in the test data. So, this feature should not be treated as a categorical feature as the model will learn to rely on strings which do not occur in the test data.","metadata":{}},{"cell_type":"code","source":"train_df.f_27.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:21.532797Z","iopub.execute_input":"2022-06-16T07:38:21.532985Z","iopub.status.idle":"2022-06-16T07:38:22.551103Z","shell.execute_reply.started":"2022-06-16T07:38:21.53296Z","shell.execute_reply":"2022-06-16T07:38:22.550525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.concat([train_df,test_df]).f_27.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:22.552183Z","iopub.execute_input":"2022-06-16T07:38:22.553035Z","iopub.status.idle":"2022-06-16T07:38:24.020704Z","shell.execute_reply.started":"2022-06-16T07:38:22.552998Z","shell.execute_reply":"2022-06-16T07:38:24.019532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Letter at position 0, 2 and 5 just takes 2 values in the training data. Other take more than 2 values. Every position gives some information about the target variable.","metadata":{}},{"cell_type":"code","source":"for pos in range(10):\n    g_df = train_df.groupby(train_df.f_27.str.get(pos))\n    print(f\"Position -> {pos}\")\n    display(pd.DataFrame({\"size\":g_df.target.size(),\"mean_target\":g_df.target.mean().round(2)}))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:24.024412Z","iopub.execute_input":"2022-06-16T07:38:24.024643Z","iopub.status.idle":"2022-06-16T07:38:32.184463Z","shell.execute_reply.started":"2022-06-16T07:38:24.024617Z","shell.execute_reply":"2022-06-16T07:38:32.183633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets find which of the 26 alphabets appears in this string sequence, along with their counts","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nc = Counter([letter for seq_list in train_df.f_27.to_list() for letter in seq_list])\nfor key in sorted(c.keys()):\n    print(key, c[key])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:32.185723Z","iopub.execute_input":"2022-06-16T07:38:32.185938Z","iopub.status.idle":"2022-06-16T07:38:34.030863Z","shell.execute_reply.started":"2022-06-16T07:38:32.185908Z","shell.execute_reply":"2022-06-16T07:38:34.02944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_27_len_unique_chars = train_df.f_27.apply(lambda x : len(set(x))).rename(\"f_27_len_unique_chars\")\ng_df = train_df.groupby(f_27_len_unique_chars)\ndisplay(pd.DataFrame({\"size\":g_df.target.size(),\"mean_target\":g_df.target.mean().round(2)}))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:34.032116Z","iopub.execute_input":"2022-06-16T07:38:34.032381Z","iopub.status.idle":"2022-06-16T07:38:35.247046Z","shell.execute_reply.started":"2022-06-16T07:38:34.032348Z","shell.execute_reply":"2022-06-16T07:38:35.246267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\n1. f_27 should be split into 10 characters since the character in each position gives some information about the target variable\n2. the number of unique characters in f_27 can be considered as a seperate feature","metadata":{}},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:35.248134Z","iopub.execute_input":"2022-06-16T07:38:35.248327Z","iopub.status.idle":"2022-06-16T07:38:35.253491Z","shell.execute_reply.started":"2022-06-16T07:38:35.248301Z","shell.execute_reply":"2022-06-16T07:38:35.252525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:35.25459Z","iopub.execute_input":"2022-06-16T07:38:35.254828Z","iopub.status.idle":"2022-06-16T07:38:35.271505Z","shell.execute_reply.started":"2022-06-16T07:38:35.254798Z","shell.execute_reply":"2022-06-16T07:38:35.270719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [train_df, test_df]:\n    for i in range(10):\n        df[f\"ch{i}\"] = df.f_27.str.get(i).apply(ord) - ord('A')\n    df[\"f_27_len_unique_chars\"] = df.f_27.apply(lambda x : len(set(x)))\n    df.drop(columns=[\"f_27\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:35.272533Z","iopub.execute_input":"2022-06-16T07:38:35.273057Z","iopub.status.idle":"2022-06-16T07:38:57.631242Z","shell.execute_reply.started":"2022-06-16T07:38:35.273029Z","shell.execute_reply":"2022-06-16T07:38:57.63039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:57.632527Z","iopub.execute_input":"2022-06-16T07:38:57.632752Z","iopub.status.idle":"2022-06-16T07:38:57.639025Z","shell.execute_reply.started":"2022-06-16T07:38:57.63272Z","shell.execute_reply":"2022-06-16T07:38:57.637961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:57.64045Z","iopub.execute_input":"2022-06-16T07:38:57.640689Z","iopub.status.idle":"2022-06-16T07:38:57.655228Z","shell.execute_reply.started":"2022-06-16T07:38:57.640659Z","shell.execute_reply":"2022-06-16T07:38:57.654506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Float Features","metadata":{}},{"cell_type":"markdown","source":"There are 16 float features","metadata":{}},{"cell_type":"code","source":"float_features_train = [col for col in train_df.columns if train_df[col].dtype == \"float32\"]\nprint(float_features_train)\nprint(f\"len = {len(float_features_train)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:57.656392Z","iopub.execute_input":"2022-06-16T07:38:57.656702Z","iopub.status.idle":"2022-06-16T07:38:57.669374Z","shell.execute_reply.started":"2022-06-16T07:38:57.656674Z","shell.execute_reply":"2022-06-16T07:38:57.668664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the histogram of all the 16 float features show that all of them are normally distributed with mean/centre at 0.  \nFeatures f_00 to f_06 has std of 1.0  \nFeatures f_19 to f_26 has std b/w 2.3 and 2.5  \nFeatures f_28 has std of 238  ","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(4,4, figsize=(16,16))\n\nfor feature, ax in zip(float_features_train,axs.ravel()):\n    ax.hist(train_df[feature], bins=100)\n    ax.set_title(f\"Train {feature}, mean={train_df[feature].mean():.1f}, std_dev={train_df[feature].std():.1f}\")\n\nplt.suptitle(\"Histogram of the 16 float features\", fontsize=20, y=0.93)\nmplcyberpunk.add_glow_effects()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:38:57.670871Z","iopub.execute_input":"2022-06-16T07:38:57.671293Z","iopub.status.idle":"2022-06-16T07:39:02.008515Z","shell.execute_reply.started":"2022-06-16T07:38:57.67126Z","shell.execute_reply":"2022-06-16T07:39:02.007709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test data also follows similar distribution","metadata":{}},{"cell_type":"code","source":"float_features_test = [col for col in test_df.columns if test_df[col].dtype == \"float32\"]\nprint(float_features_test)\nprint(f\"len = {len(float_features_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:02.009642Z","iopub.execute_input":"2022-06-16T07:39:02.010262Z","iopub.status.idle":"2022-06-16T07:39:02.019584Z","shell.execute_reply.started":"2022-06-16T07:39:02.010223Z","shell.execute_reply":"2022-06-16T07:39:02.017935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(4,4, figsize=(16,16))\n\nfor feature, ax in zip(float_features_test,axs.ravel()):\n    ax.hist(test_df[feature], bins=100)\n    ax.set_title(f\"Test {feature}, mean={test_df[feature].mean():.1f}, std_dev={test_df[feature].std():.1f}\")\n\nplt.suptitle(\"Histogram of the 16 float features\", fontsize=20, y=0.93)\nmplcyberpunk.add_glow_effects()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:02.020579Z","iopub.execute_input":"2022-06-16T07:39:02.021274Z","iopub.status.idle":"2022-06-16T07:39:06.389051Z","shell.execute_reply.started":"2022-06-16T07:39:02.02124Z","shell.execute_reply":"2022-06-16T07:39:06.388531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the correlation method used is pearson correlation coefficient.  \nfeatures f_00 to f_06 are correlated with f_28, but not with each other.  \nfeatures f_19 to f_26 are slighly correlated with each other.  \nfeature f_21 is slightly correlated with the target variable. No feature is strongly correlated with the target.  ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(train_df[float_features_train + [\"target\"]].corr(), annot=True, fmt=\".2f\", center=0);","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:06.390048Z","iopub.execute_input":"2022-06-16T07:39:06.391163Z","iopub.status.idle":"2022-06-16T07:39:08.317214Z","shell.execute_reply.started":"2022-06-16T07:39:06.391101Z","shell.execute_reply":"2022-06-16T07:39:08.316722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation matrix shows only linear dependencies.  \nTo see non-linear dependencies, we can plot a rolling mean of the target probability for every feature","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:44:39.516488Z","iopub.execute_input":"2022-06-06T10:44:39.516894Z","iopub.status.idle":"2022-06-06T10:44:39.547921Z","shell.execute_reply.started":"2022-06-06T10:44:39.516784Z","shell.execute_reply":"2022-06-06T10:44:39.546825Z"}}},{"cell_type":"markdown","source":"Trying out the rolling mean on a small subset of data","metadata":{}},{"cell_type":"code","source":"#float_features_train[0] #f_00\nfeature = \"f_00\"\ntemp = pd.DataFrame({feature:train_df[feature].values,\n                            'state':train_df.target.values})\ntemp = temp.head(10)\ntemp.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:08.318351Z","iopub.execute_input":"2022-06-16T07:39:08.318708Z","iopub.status.idle":"2022-06-16T07:39:08.32943Z","shell.execute_reply.started":"2022-06-16T07:39:08.318678Z","shell.execute_reply":"2022-06-16T07:39:08.328491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = temp.sort_values(feature)\ntemp.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:08.330792Z","iopub.execute_input":"2022-06-16T07:39:08.331068Z","iopub.status.idle":"2022-06-16T07:39:08.341249Z","shell.execute_reply.started":"2022-06-16T07:39:08.331028Z","shell.execute_reply":"2022-06-16T07:39:08.340439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.state.rolling(5, min_periods=1, center=True).mean() #center : Rolling sum with the result assigned to the center of the window index.","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:08.342448Z","iopub.execute_input":"2022-06-16T07:39:08.342674Z","iopub.status.idle":"2022-06-16T07:39:08.351336Z","shell.execute_reply.started":"2022-06-16T07:39:08.342644Z","shell.execute_reply":"2022-06-16T07:39:08.350857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nrows(features, ncols=4):\n    return (len(features)+ncols-1) // ncols\n\nassert get_nrows([1]*16, ncols=4)==4\nassert get_nrows([1]*17, ncols=4)==5\nassert get_nrows([1]*19, ncols=4)==5\nassert get_nrows([1]*20, ncols=4)==5\nassert get_nrows([1]*21, ncols=4)==6","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:08.352583Z","iopub.execute_input":"2022-06-16T07:39:08.352902Z","iopub.status.idle":"2022-06-16T07:39:08.362701Z","shell.execute_reply.started":"2022-06-16T07:39:08.352862Z","shell.execute_reply":"2022-06-16T07:39:08.362035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A horizontal line means that the target does not depend on the feature (e.g., f_03, f_04, f_06),   \n\na line with low minimum and high maximum shows a high mutual information between feature and target (e.g., f_21, f_22, f_26, f_28).","metadata":{}},{"cell_type":"markdown","source":"When the window size for rolling mean is small, we can see noise in the plot. As we increase it, it becomes less noisy but we also start losing more information.","metadata":{}},{"cell_type":"code","source":"%%time\n# Plot dependence b/w every feature and the target\ndef plot_mutual_info_diagram(df, features, window=15000, ncols=4, by_quantile=True, mutual_info=True, title=\"How the target probability depends on single features\"):\n    def H(p):\n        \"\"\"Entropy of a binary random variable in nat(natural unit of mutual information - based on natural logarithm)\"\"\"\n        return -np.log(p) * p - np.log(1-p) * (1-p)\n    \n    nrows = get_nrows(features)\n    fig, axs = plt.subplots(nrows, ncols, figsize=(16,4*nrows), sharey=True) \n    for feature, ax in zip(features, axs.ravel()):\n        temp = pd.DataFrame({feature:df[feature].values,\n                            'state':df.target.values})\n        temp = temp.sort_values(feature)\n        temp.reset_index(inplace=True)\n        rolling_mean = temp.state.rolling(window, center=True, min_periods=1).mean()\n        if by_quantile:\n            values = temp.index.values\n            ax.scatter(values, rolling_mean, s=2)\n            ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n            ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n        else:\n            values = temp[feature].values\n            ax.scatter(values, rolling_mean, s=2)\n            ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n            ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n        if mutual_info and by_quantile:\n            ax.set_xlabel(f'{feature} mi={H(temp.state.mean()) - H(rolling_mean[~rolling_mean.isna()].values).mean():.5f}')\n        else:\n            ax.set_xlabel(f'{feature}')\n    plt.suptitle(title, y=0.93, fontsize=20)\n    plt.show()\n\nplot_mutual_info_diagram(train_df, float_features_train,window=100,\n                         title='How the target probability depends on the float features')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:08.364671Z","iopub.execute_input":"2022-06-16T07:39:08.364972Z","iopub.status.idle":"2022-06-16T07:39:33.10188Z","shell.execute_reply.started":"2022-06-16T07:39:08.364948Z","shell.execute_reply":"2022-06-16T07:39:33.101256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_mutual_info_diagram(train_df, float_features_train,window=1000,\n                         title='How the target probability depends on the float features')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:33.103068Z","iopub.execute_input":"2022-06-16T07:39:33.1033Z","iopub.status.idle":"2022-06-16T07:39:50.078071Z","shell.execute_reply.started":"2022-06-16T07:39:33.103272Z","shell.execute_reply":"2022-06-16T07:39:50.076857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_mutual_info_diagram(train_df, float_features_train,window=10000,\n                         title='How the target probability depends on the float features')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:39:50.080207Z","iopub.execute_input":"2022-06-16T07:39:50.080521Z","iopub.status.idle":"2022-06-16T07:40:07.810969Z","shell.execute_reply.started":"2022-06-16T07:39:50.080453Z","shell.execute_reply":"2022-06-16T07:40:07.809876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we plot the rolling mean we sort the feature values, so that the rolling mean at a feature values x approximates the target probability in an environment x-e, x+e of this feature value.    \n\nThe plot basically show the relationship between the rank of the feature values and the target probability averaged over x-7500 and x+7000 target probabilities.  \n\nInsight:  \nThere are many non linear relationships (and some nonmonotonic) between individial features and the target. Linear Classifiers would be able to do a good job.","metadata":{}},{"cell_type":"markdown","source":"Side Note:  \nNotice how in the heatmap plotter earlier, the pearson correlation between f_00 and target shows 0.06 meaning little linearly correlated, but in the above plot - it seems like there is a relation. The correlation (0.06) is positive meaning the line goes upward. This is clear when the window size for the rolling mean is 10000 but not when window size is small(say 100) in which case the upward slope of the line disappears in the noise. When the window size is large, it hides the noise.","metadata":{}},{"cell_type":"markdown","source":"# Integer features","metadata":{}},{"cell_type":"code","source":"int_features_train = [feature for feature in train_df.columns if train_df[feature].dtype == 'uint8' and feature not in [\"id\",\"target\"]]\nprint(int_features_train)\nprint(f\"len -> {len(int_features_train)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:40:07.812575Z","iopub.execute_input":"2022-06-16T07:40:07.813297Z","iopub.status.idle":"2022-06-16T07:40:07.820223Z","shell.execute_reply.started":"2022-06-16T07:40:07.813251Z","shell.execute_reply":"2022-06-16T07:40:07.819075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"f_29 is a binary feature.  \nf_30 is a ternary feature.  \nAll the other features have a right skewed distribution.","metadata":{}},{"cell_type":"code","source":"nrows = get_nrows(int_features_train)\nfig, axs = plt.subplots(nrows, 4, figsize=(16,nrows*4))\nfor feature, ax in zip(int_features_train, axs.ravel()):\n    vc = train_df[feature].value_counts()\n    ax.bar(vc.index, vc)\n    ax.set_xlabel(f\"Train {feature}\")\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nplt.suptitle(\"Histogram of the integer features\", fontsize=20, y=0.93)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:40:07.821661Z","iopub.execute_input":"2022-06-16T07:40:07.821919Z","iopub.status.idle":"2022-06-16T07:40:10.381886Z","shell.execute_reply.started":"2022-06-16T07:40:07.821886Z","shell.execute_reply":"2022-06-16T07:40:10.380784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_mutual_info_diagram(train_df, int_features_train, window=10000,\n                         title='How the target probability depends on the integer features')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:40:10.382962Z","iopub.execute_input":"2022-06-16T07:40:10.383163Z","iopub.status.idle":"2022-06-16T07:40:24.019372Z","shell.execute_reply.started":"2022-06-16T07:40:10.383127Z","shell.execute_reply":"2022-06-16T07:40:24.018221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysing Interactions with SHAP","metadata":{}},{"cell_type":"markdown","source":"To use the SHAP package we first need to train a model since SHAP is a model agnostic approach designed to explain any given black-box model. If you are applying SHAP to a real-world problem you should follow best practices. Specifically, you should ensure your model performs well on both a training and validation set. The better your model the more reliable your results will be. As a quick check on this model, I have calculated the AUC of the validation set which 99%. The model should be fine to demonstrate the SHAP package.","metadata":{}},{"cell_type":"code","source":"%%time\nX = train_df[[feature for feature in train_df.columns if feature != \"target\"]]\ny = train_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\nlgbm_model = LGBMClassifier(n_estimators=5000, min_child_samples=80, random_state=42)\n\nlgbm_model.fit(X_train.values, y_train)\ny_pred = lgbm_model.predict_proba(X_test.values)[:,1]\n\nscore = roc_auc_score(y_test, y_pred)\nprint(f\"Validation AUC:{score:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:40:24.020409Z","iopub.execute_input":"2022-06-16T07:40:24.020651Z","iopub.status.idle":"2022-06-16T07:46:19.877316Z","shell.execute_reply.started":"2022-06-16T07:40:24.020622Z","shell.execute_reply":"2022-06-16T07:46:19.876493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_test.shape # 3,60,000\n\n# Using a random sample of the dataframe for better time computation\nX_test_sampled = X_test.sample(20000, random_state=1307)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:46:19.878567Z","iopub.execute_input":"2022-06-16T07:46:19.878838Z","iopub.status.idle":"2022-06-16T07:46:19.905072Z","shell.execute_reply.started":"2022-06-16T07:46:19.8788Z","shell.execute_reply":"2022-06-16T07:46:19.904355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# explain the model's predictions using SHAP values\nexplainer = shap.TreeExplainer(lgbm_model)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:46:19.906319Z","iopub.execute_input":"2022-06-16T07:46:19.907217Z","iopub.status.idle":"2022-06-16T07:46:24.539381Z","shell.execute_reply.started":"2022-06-16T07:46:19.90717Z","shell.execute_reply":"2022-06-16T07:46:24.538595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nshap_values = explainer.shap_values(X_test_sampled)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:46:24.540509Z","iopub.execute_input":"2022-06-16T07:46:24.54071Z","iopub.status.idle":"2022-06-16T07:53:44.306904Z","shell.execute_reply.started":"2022-06-16T07:46:24.540685Z","shell.execute_reply":"2022-06-16T07:53:44.306169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Get SHAP interaction values. Beware it is time consuming to calculate the interaction values.\n# shap_interaction_20 = explainer.shap_interaction_values(X_test_sampled)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:53:44.31255Z","iopub.execute_input":"2022-06-16T07:53:44.312746Z","iopub.status.idle":"2022-06-16T07:53:44.317695Z","shell.execute_reply.started":"2022-06-16T07:53:44.31272Z","shell.execute_reply":"2022-06-16T07:53:44.317214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Get SHAP interaction values. Beware it is time consuming to calculate the interaction values.\n# X_test_sampled = X_test.sample(40, random_state=42)\n# shap_interaction_40 = explainer.shap_interaction_values(X_test_sampled)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:53:44.318443Z","iopub.execute_input":"2022-06-16T07:53:44.318731Z","iopub.status.idle":"2022-06-16T07:53:44.330836Z","shell.execute_reply.started":"2022-06-16T07:53:44.318708Z","shell.execute_reply":"2022-06-16T07:53:44.330339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_arr = np.loadtxt('../input/shap-interaction/shap_interaction_20k.txt')\nfeatures_list_shap_dataset = [\"f_00\",\"f_01\",\"f_02\",\"f_03\",\"f_04\",\"f_05\",\"f_06\",\"f_07\",\"f_08\",\"f_09\",\"f_10\",\"f_11\",\"f_12\",\"f_13\",\"f_14\",\"f_15\",\"f_16\",\"f_17\",\"f_18\",\"f_19\",\"f_20\",\"f_21\",\"f_22\",\"f_23\",\"f_24\",\"f_25\",\"f_26\",\"f_28\",\"f_29\",\"f_30\",\"ch0\",\"f_27_len_unique_chars\",'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8', 'ch9']\nload_original_arr = loaded_arr.reshape(\n    #loaded_arr.shape[0], loaded_arr.shape[1] // shap_interaction.shape[2], shap_interaction.shape[2])\n    loaded_arr.shape[0], loaded_arr.shape[1] // 41, 41)\n\nshap_interaction_dataset = load_original_arr","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:53:44.33186Z","iopub.execute_input":"2022-06-16T07:53:44.332176Z","iopub.status.idle":"2022-06-16T07:54:15.185663Z","shell.execute_reply.started":"2022-06-16T07:53:44.332149Z","shell.execute_reply":"2022-06-16T07:54:15.185067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step1: Absolute mean plot\n\nIndividual contribution matrices allows us to explain individual model predictions at local level. But what if we want to explain how the model makes predictions at a global level? To do this we can aggregate the values in the contribution matrices by taking the absolute mean. Presenting the results in a heatmap can be effective to highlight important main effects and interaction effects.","metadata":{}},{"cell_type":"code","source":"# Get absolute mean of matrices\nmean_shap = np.abs(shap_interaction_dataset).mean(axis=0)\ndf = pd.DataFrame(mean_shap, index=features_list_shap_dataset, columns=features_list_shap_dataset)\n\n# times off diagonal by 2\ndf.where(df.values == np.diagonal(df),df.values*2, inplace=True)\n\n# display \nfig = plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\nsns.heatmap(df.round(decimals=3), cmap='coolwarm', annot=True, fmt='.6g', cbar=False, ax=ax, )\nax.tick_params(axis='x', colors='w', labelsize=15, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=15)\n\nplt.suptitle(\"SHAP interaction values\", color=\"white\", fontsize=60, y=0.97)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:15.186792Z","iopub.execute_input":"2022-06-16T07:54:15.187095Z","iopub.status.idle":"2022-06-16T07:54:20.082701Z","shell.execute_reply.started":"2022-06-16T07:54:15.18707Z","shell.execute_reply":"2022-06-16T07:54:20.082163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The diagonal elements are the main effects of features at global level(absolute mean of each of the 20k predictions) and off diagonal elements are the interaction effect of features at global level","metadata":{}},{"cell_type":"markdown","source":"Insights:  \n\nFor instance we can see that the main effect is large for features f_21 (0.539) , f_26 (0.612) and unique_characters(0.712). This tells us that these features tend to have large positive or negative main effects. In other words, these features tend to have a significant impact on the model’s predictions.  \n\nSimilarly, we can see that interaction effects for (f_00, f_26) --> (0.513) and (f_02, f_21) --> (0.482) are significant. These are just some examples.","metadata":{}},{"cell_type":"markdown","source":"Step2: Feature interaction analysis\n\nSo, now that we have calcuated the average (main/interaction)effect of features at global level it is interseting to deep dive into features that show large interaction effects at global level. We can use utilize the dependence plot to better understand the nature of the interactions. For the sake of demonstration I will be focusing on f_02 & f_21 and f_24 & f_30 interaction effects at the local level.","metadata":{}},{"cell_type":"code","source":"# %%time\n# # dependency up until this point\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# !pip uninstall matplotlib -y\n# !pip install matplotlib==3.2.2\n# !pip install mplcyberpunk\n# import matplotlib\n# print(matplotlib.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:20.083792Z","iopub.execute_input":"2022-06-16T07:54:20.084112Z","iopub.status.idle":"2022-06-16T07:54:20.086501Z","shell.execute_reply.started":"2022-06-16T07:54:20.084084Z","shell.execute_reply":"2022-06-16T07:54:20.086046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# from matplotlib.ticker import MaxNLocator\n# from matplotlib import gridspec\n# import seaborn as sns\n# from IPython.display import display\n# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n# from sklearn.metrics import roc_auc_score, roc_curve\n# from lightgbm import LGBMClassifier\n# import shap\n# import datatable\n# import mplcyberpunk\n# plt.style.use(\"cyberpunk\")\n# # train_df = datatable.fread(\"/kaggle/input/tabular-playground-series-may-2022/train.csv\").to_pandas().set_index('id')\n# train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2022/train.csv\").set_index('id')\n# # test_df = datatable.fread(\"/kaggle/input/tabular-playground-series-may-2022/test.csv\").to_pandas().set_index('id')\n# test_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2022/test.csv\").set_index('id')\n# for df in [train_df, test_df]:\n#     for i in range(10):\n#         df[f\"ch{i}\"] = df.f_27.str.get(i).apply(ord) - ord('A')\n#     df[\"f_27_len_unique_chars\"] = df.f_27.apply(lambda x : len(set(x)))\n#     df.drop(columns=[\"f_27\"], inplace=True)\n# X = train_df[[feature for feature in train_df.columns if feature != \"target\"]]\n# y = train_df[\"target\"]\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# lgbm_model = LGBMClassifier(n_estimators=5000, min_child_samples=80, random_state=42)\n\n# lgbm_model.fit(X_train.values, y_train)\n# y_pred = lgbm_model.predict_proba(X_test.values)[:,1]\n\n# score = roc_auc_score(y_test, y_pred)\n# print(f\"Validation AUC:{score:.3f}\")\n# X_test_sampled = X_test.sample(20000, random_state=1307)\n# explainer = shap.TreeExplainer(lgbm_model)\n# shap_values = explainer.shap_values(X_test_sampled)\n# loaded_arr = np.loadtxt('../input/shap-interaction/shap_interaction_20k.txt')\n# features_list_shap_dataset = [\"f_00\",\"f_01\",\"f_02\",\"f_03\",\"f_04\",\"f_05\",\"f_06\",\"f_07\",\"f_08\",\"f_09\",\"f_10\",\"f_11\",\"f_12\",\"f_13\",\"f_14\",\"f_15\",\"f_16\",\"f_17\",\"f_18\",\"f_19\",\"f_20\",\"f_21\",\"f_22\",\"f_23\",\"f_24\",\"f_25\",\"f_26\",\"f_28\",\"f_29\",\"f_30\",\"ch0\",\"f_27_len_unique_chars\",'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8', 'ch9']\n# load_original_arr = loaded_arr.reshape(\n#     #loaded_arr.shape[0], loaded_arr.shape[1] // shap_interaction.shape[2], shap_interaction.shape[2])\n#     loaded_arr.shape[0], loaded_arr.shape[1] // 41, 41)\n\n# shap_interaction_dataset = load_original_arr","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:20.087448Z","iopub.execute_input":"2022-06-16T07:54:20.087783Z","iopub.status.idle":"2022-06-16T07:54:20.101434Z","shell.execute_reply.started":"2022-06-16T07:54:20.087756Z","shell.execute_reply":"2022-06-16T07:54:20.100928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get absolute mean of matrices\nmean_shap = np.abs(shap_interaction_dataset).mean(axis=0)\ndf = pd.DataFrame(mean_shap, index=features_list_shap_dataset, columns=features_list_shap_dataset)\n\n# times off diagonal by 2\ndf.where(df.values == np.diagonal(df),df.values*2, inplace=True)\n\n# display \nfig = plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\nsns.heatmap(df.round(decimals=3), cmap='coolwarm', annot=True, fmt='.6g', cbar=False, ax=ax, )\nax.tick_params(axis='x', colors='w', labelsize=15, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=15)\n\nplt.suptitle(\"SHAP interaction values\", color=\"white\", fontsize=60, y=0.97)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:20.10272Z","iopub.execute_input":"2022-06-16T07:54:20.103048Z","iopub.status.idle":"2022-06-16T07:54:24.901746Z","shell.execute_reply.started":"2022-06-16T07:54:20.103021Z","shell.execute_reply":"2022-06-16T07:54:24.901152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following feature interactions(f1,f2) has significantly higher shap interaction values when compared to its main interaction(f1,f1):  \n\nInvolving f_30 (ternary feature):  \nf_24, f_30  \nf_25, f_30  \n\nBoth feature belonging to real numbers:  \nf_02, f_21  \nf_05, f_22  \nf_00, f_26  \nf_01, f_26  \n\nFollowing main interactions has high shap values:  \nf_21  \nf_26    \nf_27_len_unique_chars  \n\nFor example:  \nBased on SHAP feature interactions heatmap f_02 & f_21 interactions effect which is 0.482 is larger than the main effect of f_02 which is 0.27. Lets deep dive into the local level effects.","metadata":{}},{"cell_type":"code","source":"# A dependence plot is a scatter plot that shows the effect a single feature has on the predictions made by the model.\n# Each dot is a single prediction (row) from the dataset.\n# The x-axis is the value of the feature (from the X matrix).\n# The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that sample's prediction.\n# The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically)\n\n# The first argument is the index of the feature we want to plot\n# The second argument is the matrix of SHAP values (it is the same shape as the data matrix)\n# The third argument is the data matrix (a pandas dataframe or numpy array)\n# The interaction_index argument can be used to explicitly set which feature gets used for coloring\n\n# Sample\n# shap.dependence_plot(0, shap_values, X, interaction_index=\"Education-Num\")\n\n# Reference:  \n# https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html\n\n# Plot feature interaction\ndef plot_feature_interaction_with_f2_being_ternary_feature(f1, f2, f2_condition=2):\n    \n    fig = plt.figure(figsize=(30,15))\n    spec = gridspec.GridSpec(nrows=2, ncols=3, figure=fig)\n    \n    # SHAP main effect {f1}\n    ax = fig.add_subplot(spec[0,0])\n    shap.dependence_plot(f1, shap_values[1], X_test_sampled, display_features=X_test_sampled, interaction_index=f2, ax=ax, show=False)\n    ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n    ax.set_title(f'SHAP main effect ({f1})', fontsize=10)\n    \n#     # SHAP main effect {f2}\n#     ax = fig.add_subplot(spec[0,1])\n#     shap.dependence_plot(f2, shap_values[1], X_test_sampled, display_features=X_test_sampled, interaction_index=f1, ax=ax, show=False)\n#     ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n#     ax.set_title(f'SHAP main effect ({f2})', fontsize=10)\n    \n    # SHAP interaction effect\n    ax = fig.add_subplot(spec[0,1])\n    shap.dependence_plot((f1,f2), shap_interaction_dataset, X_test_sampled, display_features=X_test_sampled, ax=ax, show=False)\n    ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n    ax.set_title(f'SHAP interaction effect', fontsize=10)\n    \n    # How the target probability depends on the {f1}\n    ax = fig.add_subplot(spec[1,0])\n    temp = pd.DataFrame({f1: train_df[f1].values,\n                        \"target\": train_df.target.values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    rolling_mean = temp.target.rolling(15_000, center=True).mean()    \n    sns.scatterplot(x=temp[f1], y=rolling_mean, data=temp, s=2, ax=ax)\n    ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n    ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n    ax.set_title(f\"How the target probability depends on {f1}\")\n    \n    # How the target probability depends on the {f1} & {f2==2}\n    ax = fig.add_subplot(spec[1,1])\n    temp = pd.DataFrame({f1: train_df.loc[train_df[f2]==f2_condition, f1].values,\n                        \"target\": train_df.loc[train_df[f2]==f2_condition, \"target\"].values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    rolling_mean = temp.target.rolling(15_000, center=True).mean()    \n    sns.scatterplot(x=temp[f1], y=rolling_mean, data=temp, s=2, ax=ax)\n    ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n    ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n    ax.set_title(f\"How the target probability depends on {f1} & {f2}=={f2_condition}\")\n    \n    # How the target probability depends on the {f1} & {f2!=2}\n    ax = fig.add_subplot(spec[1,2])\n    temp = pd.DataFrame({f1: train_df.loc[train_df[f2]!=f2_condition, f1].values,\n                        \"target\": train_df.loc[train_df[f2]!=f2_condition, \"target\"].values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    rolling_mean = temp.target.rolling(15_000, center=True).mean()    \n    sns.scatterplot(x=temp[f1], y=rolling_mean, data=temp, s=2, ax=ax)\n    ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n    ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n    ax.set_title(f\"How the target probability depends on {f1} & {f2}!={f2_condition}\")\n    \n    plt.suptitle(f\"Feature Interaction Analysis\\n {f1} and {f2}\", fontsize=25, y=1.15)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:24.903087Z","iopub.execute_input":"2022-06-16T07:54:24.903565Z","iopub.status.idle":"2022-06-16T07:54:24.922029Z","shell.execute_reply.started":"2022-06-16T07:54:24.903527Z","shell.execute_reply":"2022-06-16T07:54:24.921279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1='f_24'\nf2='f_30'\nplot_feature_interaction_with_f2_being_ternary_feature(f1, f2, f2_condition=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:24.923131Z","iopub.execute_input":"2022-06-16T07:54:24.923322Z","iopub.status.idle":"2022-06-16T07:54:28.277232Z","shell.execute_reply.started":"2022-06-16T07:54:24.923299Z","shell.execute_reply":"2022-06-16T07:54:28.276668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first plot one can conclude that for f_30 == 2 main SHAP effect of f_24 decreases for the predictions as f_24 gets larger.  \nFrom the second plot one can conlcude that for f_30 == 2 the interaction effect between f_30 & f_24 decreases as f_24 get larger. The opposite is true for f_30 != 2.  \nThe third plot indicates that as f_24 gets larger the probability for state==1 increases.  \nThe fourth and fifth plot are the same as third plot but they are conditioned on f_30==2 and f_30!=2. The results are clearly different.  ","metadata":{}},{"cell_type":"code","source":"f1='f_25'\nf2='f_30'\nplot_feature_interaction_with_f2_being_ternary_feature(f1, f2, f2_condition=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:28.278368Z","iopub.execute_input":"2022-06-16T07:54:28.278707Z","iopub.status.idle":"2022-06-16T07:54:31.932051Z","shell.execute_reply.started":"2022-06-16T07:54:28.278677Z","shell.execute_reply":"2022-06-16T07:54:31.931337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first plot, although noisy, we can see that for f_30==1, main SHAP effect of f_25 increases rapidly for the predictions as f_25 gets larger, for f_30==0, main SHAP effect of f_25 reduces slightly for the predictions as f_25 gets larger, for f_30==2, main SHAP effect of f_25 reduces rapidly for the predictions as f_25 gets larger.  \nFrom the second plot, we can conclude that for f_30==1, the interaction effect between f_25 and f_30 increases as f_25 gets larger.  \nThe third plot indicates that as f_25 gets larger, the probability for state==1 reduces.  \nThe fourth and fifth plot are the same as third plot but they are conditioned on f_30==1 and f_30!=1. The results are clearly different.  \n\n","metadata":{}},{"cell_type":"code","source":"# A dependence plot is a scatter plot that shows the effect a single feature has on the predictions made by the model.\n# Each dot is a single prediction (row) from the dataset.\n# The x-axis is the value of the feature (from the X matrix).\n# The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that sample's prediction.\n# The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically)\n\n# The first argument is the index of the feature we want to plot\n# The second argument is the matrix of SHAP values (it is the same shape as the data matrix)\n# The third argument is the data matrix (a pandas dataframe or numpy array)\n# The interaction_index argument can be used to explicitly set which feature gets used for coloring\n\n# Sample\n# shap.dependence_plot(0, shap_values, X, interaction_index=\"Education-Num\")\n\ndef plot_feature_interaction(f1, f2):\n    \n    fig = plt.figure(figsize=(20,10), tight_layout=True)\n    spec = gridspec.GridSpec(2, 3, figure=fig)\n    \n    # SHAP main effect {f1}\n    ax = fig.add_subplot(spec[0,0])\n    shap.dependence_plot(f1, shap_values[1], X_test_sampled, display_features=X_test_sampled, interaction_index=None, ax=ax, show=False)\n    ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n    ax.set_title(f'SHAP main effect ({f1})', fontsize=10)\n    \n    # SHAP main effect {f2}\n    ax = fig.add_subplot(spec[0,1])\n    shap.dependence_plot(f2, shap_values[1], X_test_sampled, display_features=X_test_sampled, interaction_index=None, ax=ax, show=False)\n    ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n    ax.set_title(f'SHAP main effect ({f2})', fontsize=10)\n    \n    # SHAP interaction effect {f2}\n    ax = fig.add_subplot(spec[0,2])\n    shap.dependence_plot((f1,f2), shap_interaction_dataset, X_test_sampled, display_features=X_test_sampled, interaction_index=None, ax=ax, show=False)\n    ax.yaxis.label.set_color('white'); ax.xaxis.label.set_color('white'); ax.tick_params(axis='x', colors='white'); ax.tick_params(axis='y', colors='white')\n    ax.set_title(f'SHAP interaction effect ({f1} and {f2})', fontsize=10)\n    \n    # How the target probability depends on the {f1}\n    ax = fig.add_subplot(spec[1,0])\n    temp = pd.DataFrame({f1: train_df[f1].values,\n                        \"target\": train_df.target.values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    rolling_mean = temp.target.rolling(15_000, center=True).mean()    \n    sns.scatterplot(x=temp[f1], y=rolling_mean, data=temp, s=2, ax=ax)\n    ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n    ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n    ax.set_title(f\"How the target probability depends on {f1}\")\n    \n    # How the target probability depends on the {f2}\n    ax = fig.add_subplot(spec[1,1])\n    temp = pd.DataFrame({f2: train_df[f2].values,\n                        \"target\": train_df.target.values})\n    temp = temp.sort_values(f2)\n    temp.reset_index(inplace=True)\n    \n    rolling_mean = temp.target.rolling(15_000, center=True).mean()    \n    sns.scatterplot(x=temp[f2], y=rolling_mean, data=temp, s=2, ax=ax)\n    ax.axhline(y=max(rolling_mean), color='g', linestyle='-')\n    ax.axhline(y=min(rolling_mean), color='r', linestyle='-')\n    ax.set_title(f\"How the target probability depends on {f2}\")\n    \n    # Scatter plot\n    ax = fig.add_subplot(spec[1,2])\n    sns.scatterplot(x=f1, y=f2, data=train_df, hue=\"target\", ax=ax, s=2)\n    \n    if f1==\"f_02\" and f2==\"f_21\":\n        ax.text(-1.5, -5, \"1\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        ax.text(0, 1, \"2\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        ax.text(1, 6.7, \"3\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n    \n    ax.set_title(f'scatter plot', fontsize=10)\n    \n    plt.suptitle(f\"Feature Interaction Analysis \\n {f1} and {f2}\", fontsize=25, y=1.15)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:31.933602Z","iopub.execute_input":"2022-06-16T07:54:31.933962Z","iopub.status.idle":"2022-06-16T07:54:31.955077Z","shell.execute_reply.started":"2022-06-16T07:54:31.933932Z","shell.execute_reply":"2022-06-16T07:54:31.95427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1=\"f_02\"\nf2=\"f_21\"\nplot_feature_interaction(f1, f2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:31.956456Z","iopub.execute_input":"2022-06-16T07:54:31.956734Z","iopub.status.idle":"2022-06-16T07:54:48.028382Z","shell.execute_reply.started":"2022-06-16T07:54:31.956699Z","shell.execute_reply":"2022-06-16T07:54:48.027736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, plot 2, 5 and 6 are interesting.  \nIn plot 2, we can see that the main shap effect for f_21 increases monotonically for f_21 features. The main shape effect reduces drastically as the f_21 values decreases post -3.8 ish and the same increases drastically as the f_21 values increases post 3.8 ish.  \nIn plot 5, we can see the same thing in a different aspect (f_21 values vs probability for state==1)  \nIn plot 6, we did a simple scatter plot of f_02 and f_21 with target as hue.  We can clearly how both these features interact which have an impact on the target probability.  \n\nWe can see that the projection to f_02 and f_21 is partitoned into 3 regions, each one having different target probabilities.\n\nIn the region labelled as 1 : the probability for target==1 is low  \nIn the region labelled as 2 : the probability for target==1 is medium  \nIn the region labelled as 3 : the probability for target==1 is high  \n\nWe can now either hope that our classifier finds these borders by itself, or we can help the classifier by creating a ternary categorical feature that indicates which region a sample belong to.  \n\nBottom left region (low probability for target==1) -> -1  \nMiddle region (medium probability for target==1) -> 0  \nTop right region (high probability for target==1) -> 1  \n\nWe can get the coordinates of the 2 borders which divides the region into 3 regions by plotting the distribution of f_02 + f_21 with hue as target variable.  \n","metadata":{}},{"cell_type":"code","source":"def scatter_plot_f1_f2(f1, f2):\n    fig, axs = plt.subplots(1, 2, figsize=(24,12))\n    sns.scatterplot(x=f1, y=f2, data=train_df, ax=axs[0], s=2)\n    axs[0].set_title(f'Correlation', fontsize=10)\n    \n    sns.scatterplot(x=f1, y=f2, data=train_df, hue=\"target\", ax=axs[1], s=2)\n    axs[1].set_title(f'Interaction', fontsize=10)\n    if f1==\"f_02\" and f2==\"f_21\":\n        axs[1].text(-1.2, -5, \"1\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        axs[1].text(0, 1, \"2\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        axs[1].text(1, 5, \"3\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n    plt.suptitle(\"Correlation vs Interaction\", fontsize=20, y=0.97)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:48.029451Z","iopub.execute_input":"2022-06-16T07:54:48.029656Z","iopub.status.idle":"2022-06-16T07:54:48.037377Z","shell.execute_reply.started":"2022-06-16T07:54:48.02963Z","shell.execute_reply":"2022-06-16T07:54:48.036629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_f1_f2(\"f_02\", \"f_21\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:48.038346Z","iopub.execute_input":"2022-06-16T07:54:48.038539Z","iopub.status.idle":"2022-06-16T07:55:01.767691Z","shell.execute_reply.started":"2022-06-16T07:54:48.038513Z","shell.execute_reply":"2022-06-16T07:55:01.766575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of the plot implies that there is no correlation between the two features. In the second plot we have the same scatterplot but now we have colored the observations with the target value. Here we can clearly observe that there are three different regions where f_21 and f_02 interact to give different target distribution per region","metadata":{}},{"cell_type":"markdown","source":"We can see that the projection to f_02 and f_21 is partitoned into 3 regions, each one having different target probabilities.\n\nIn the region labelled as 1 : the probability for target==1 is low  \nIn the region labelled as 2 : the probability for target==1 is medium  \nIn the region labelled as 3 : the probability for target==1 is high  \n\nWe can now either hope that our classifier finds these borders by itself, or we can help the classifier by creating a ternary categorical feature that indicates which region a sample belong to.  \n\nBottom left region (low probability for target==1) -> -1  \nMiddle region (medium probability for target==1) -> 0  \nTop right region (high probability for target==1) -> 1  \n\nWe can get the coordinates of the 2 borders which divides the region into 3 regions by plotting the distribution of f_02 + f_21 with hue as target variable.  ","metadata":{}},{"cell_type":"code","source":"temp = train_df[[\"f_02\",\"f_21\",\"target\"]].copy()\ntemp[f\"f_02 + f_21\"] = temp[\"f_02\"] + temp[\"f_21\"]\n\nfig = plt.figure(figsize=(30,10))\nax = fig.add_subplot()\nsns.histplot(data=temp, x=\"f_02 + f_21\", hue=\"target\", bins=300, ax=ax)\nax.axvline(x=-5.3, color='y', linestyle='-', label=\"-5.3\")\nax.text(x=-6.1,y=5000,s=\"-5.3\",fontsize=15,fontweight='bold',color='y')\nax.axvline(x=5.2, color='y', linestyle='-')\nax.text(x=5.4,y=5000,s=\"5.2\",fontsize=15,fontweight='bold',color='y')\n\nax.text(x=-10,y=3500,s=\"<equation> = -1\",fontsize=15,fontweight='bold',color='w')\nax.text(x=-1.5,y=3500,s=\"<equation> = 0\",fontsize=15,fontweight='bold',color='w')\nax.text(x=7,y=3500,s=\"<equation> = 1\",fontsize=15,fontweight='bold',color='w')\nplt.suptitle(\"f_02 + f_21 distribution\",fontsize=20,fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:01.7693Z","iopub.execute_input":"2022-06-16T07:55:01.769561Z","iopub.status.idle":"2022-06-16T07:55:03.270256Z","shell.execute_reply.started":"2022-06-16T07:55:01.769528Z","shell.execute_reply":"2022-06-16T07:55:03.269528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"i_f02_f21\"] = (train_df.f_02 + train_df.f_21 > 5.2).astype(int) - \\\n                        (train_df.f_02 + train_df.f_21 < -5.3).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:03.271664Z","iopub.execute_input":"2022-06-16T07:55:03.271893Z","iopub.status.idle":"2022-06-16T07:55:03.281209Z","shell.execute_reply.started":"2022-06-16T07:55:03.271862Z","shell.execute_reply":"2022-06-16T07:55:03.280507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1=\"f_05\"\nf2=\"f_22\"\nplot_feature_interaction(f1, f2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:03.282225Z","iopub.execute_input":"2022-06-16T07:55:03.282417Z","iopub.status.idle":"2022-06-16T07:55:19.489556Z","shell.execute_reply.started":"2022-06-16T07:55:03.282392Z","shell.execute_reply":"2022-06-16T07:55:19.488753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, plot 2, 5 and 6 are interesting.  \nIn plot 2, we can see that the main shap effect for f_22 increases monotonically for f_22 features. The main shape effect reduces drastically as the f_22 values decreases post -3.8 ish and the same increases drastically as the f_22 values increases post 3.8 ish.  \nIn plot 5, we can see the same thing in a different aspect (f_22 values vs probability for state==1)  \nIn plot 6, we did a simple scatter plot of f_05 and f_22 with target as hue.  We can clearly how both these features interact which have an impact on the target probability.  \n\nWe can see that the projection to f_05 and f_22 is partitoned into 3 regions, each one having different target probabilities.\n\nIn the region labelled as 1 : the probability for target==1 is low  \nIn the region labelled as 2 : the probability for target==1 is medium  \nIn the region labelled as 3 : the probability for target==1 is high  \n\nWe can now either hope that our classifier finds these borders by itself, or we can help the classifier by creating a ternary categorical feature that indicates which region a sample belong to.  \n\nBottom left region (low probability for target==1) -> -1  \nMiddle region (medium probability for target==1) -> 0  \nTop right region (high probability for target==1) -> 1  \n\nWe can get the coordinates of the 2 borders which divides the region into 3 regions by plotting the distribution of f_05 + f_22 with hue as target variable.  \n","metadata":{}},{"cell_type":"code","source":"scatter_plot_f1_f2(\"f_05\", \"f_22\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:19.490873Z","iopub.execute_input":"2022-06-16T07:55:19.491072Z","iopub.status.idle":"2022-06-16T07:55:33.190388Z","shell.execute_reply.started":"2022-06-16T07:55:19.491047Z","shell.execute_reply":"2022-06-16T07:55:33.189548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = train_df[[\"f_05\",\"f_22\",\"target\"]].copy()\ntemp[f\"f_05 + f_22\"] = temp[\"f_05\"] + temp[\"f_22\"]\n\nfig = plt.figure(figsize=(30,10))\nax = fig.add_subplot()\nsns.histplot(data=temp, x=\"f_05 + f_22\", hue=\"target\", bins=300, ax=ax)\nax.axvline(x=-5.4, color='y', linestyle='-', label=\"-5.3\")\nax.text(x=-6.1,y=5000,s=\"-5.4\",fontsize=15,fontweight='bold',color='y')\nax.axvline(x=5.1, color='y', linestyle='-')\nax.text(x=5.4,y=5000,s=\"5.1\",fontsize=15,fontweight='bold',color='y')\n\nax.text(x=-10,y=3500,s=\"<equation> = -1\",fontsize=15,fontweight='bold',color='w')\nax.text(x=-1.5,y=3500,s=\"<equation> = 0\",fontsize=15,fontweight='bold',color='w')\nax.text(x=7,y=3500,s=\"<equation> = 1\",fontsize=15,fontweight='bold',color='w')\nplt.suptitle(\"f_05 + f_22 distribution\",fontsize=20,fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:33.191866Z","iopub.execute_input":"2022-06-16T07:55:33.192073Z","iopub.status.idle":"2022-06-16T07:55:34.711061Z","shell.execute_reply.started":"2022-06-16T07:55:33.192046Z","shell.execute_reply":"2022-06-16T07:55:34.710378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"i_f05_f22\"] = (train_df.f_05 + train_df.f_22 > 5.1).astype(int) - \\\n                        (train_df.f_05 + train_df.f_22 < -5.4).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:34.712117Z","iopub.execute_input":"2022-06-16T07:55:34.712305Z","iopub.status.idle":"2022-06-16T07:55:34.721392Z","shell.execute_reply.started":"2022-06-16T07:55:34.712282Z","shell.execute_reply":"2022-06-16T07:55:34.720504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1='f_00'\nf2='f_26'\nplot_feature_interaction(f1, f2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:34.722464Z","iopub.execute_input":"2022-06-16T07:55:34.722676Z","iopub.status.idle":"2022-06-16T07:55:50.638683Z","shell.execute_reply.started":"2022-06-16T07:55:34.722651Z","shell.execute_reply":"2022-06-16T07:55:50.63775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interaction Plot shows some feature space region partitioning but its not very distinct(borders of the regions) because of the noise.","metadata":{}},{"cell_type":"code","source":"f1='f_01'\nf2='f_26'\nplot_feature_interaction(f1, f2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:55:50.63997Z","iopub.execute_input":"2022-06-16T07:55:50.640169Z","iopub.status.idle":"2022-06-16T07:56:07.119918Z","shell.execute_reply.started":"2022-06-16T07:55:50.640144Z","shell.execute_reply":"2022-06-16T07:56:07.118915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interaction Plot shows some feature space region partitioning but its not very distinct(borders of the regions) because of the noise. Since f_26 appeared alongside both f_00 and f_01, lets try plotting it against f_00 + f_01 (combined using sum operation)","metadata":{}},{"cell_type":"code","source":"def scatter_plot_f1comb_f2(f1_comb_tuple, f2):\n\n    temp = train_df[[f1_comb_tuple[0],f1_comb_tuple[1],f2,\"target\"]].copy()\n    temp[f\"{f1_comb_tuple[0]} + {f1_comb_tuple[1]}\"] = temp[f1_comb_tuple[0]] + temp[f1_comb_tuple[1]]\n    \n    fig, axs = plt.subplots(1, 2, figsize=(24,12))\n    sns.scatterplot(x=f\"{f1_comb_tuple[0]} + {f1_comb_tuple[1]}\", y=f2, data=temp, ax=axs[0], s=2)\n    axs[0].set_title(f'Correlation', fontsize=10)\n    \n    sns.scatterplot(x=f\"{f1_comb_tuple[0]} + {f1_comb_tuple[1]}\", y=f2, data=temp, hue=\"target\", ax=axs[1], s=2)\n    axs[1].set_title(f'Interaction', fontsize=10)\n    if f1==\"f_02\" and f2==\"f_21\":\n        axs[1].text(-1.2, -5, \"1\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        axs[1].text(0, 1, \"2\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n        axs[1].text(1, 5, \"3\", fontsize=20, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n    plt.suptitle(\"Correlation vs Interaction\", fontsize=20, y=0.97)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:56:07.12099Z","iopub.execute_input":"2022-06-16T07:56:07.121218Z","iopub.status.idle":"2022-06-16T07:56:07.135113Z","shell.execute_reply.started":"2022-06-16T07:56:07.121175Z","shell.execute_reply":"2022-06-16T07:56:07.13416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_f1comb_f2((\"f_00\",\"f_01\"), \"f_26\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:56:07.137702Z","iopub.execute_input":"2022-06-16T07:56:07.137879Z","iopub.status.idle":"2022-06-16T07:56:20.650062Z","shell.execute_reply.started":"2022-06-16T07:56:07.137857Z","shell.execute_reply":"2022-06-16T07:56:20.649311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = train_df[[\"f_00\",\"f_01\",\"f_26\",\"target\"]].copy()\ntemp[f\"f_00 + f_01 + f_26\"] = temp[\"f_00\"] + temp[\"f_01\"] + temp[\"f_26\"]\n\nfig = plt.figure(figsize=(30,10))\nax = fig.add_subplot()\nsns.histplot(data=temp, x=\"f_00 + f_01 + f_26\", hue=\"target\", bins=300, ax=ax)\nax.axvline(x=-5.0, color='y', linestyle='-', label=\"-5.3\")\nax.text(x=-6.1,y=5000,s=\"-5.0\",fontsize=15,fontweight='bold',color='y')\nax.axvline(x=5.0, color='y', linestyle='-')\nax.text(x=5.4,y=5000,s=\"5.0\",fontsize=15,fontweight='bold',color='y')\n\nax.text(x=-10,y=3500,s=\"<equation> = -1\",fontsize=15,fontweight='bold',color='w')\nax.text(x=-1.5,y=3500,s=\"<equation> = 0\",fontsize=15,fontweight='bold',color='w')\nax.text(x=7,y=3500,s=\"<equation> = 1\",fontsize=15,fontweight='bold',color='w')\nplt.suptitle(\"f_00 + f_01 + f_26 distribution\",fontsize=20,fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:56:20.65166Z","iopub.execute_input":"2022-06-16T07:56:20.65189Z","iopub.status.idle":"2022-06-16T07:56:22.204153Z","shell.execute_reply.started":"2022-06-16T07:56:20.651858Z","shell.execute_reply":"2022-06-16T07:56:22.203512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"i_f00_f01_f26\"] = (train_df.f_00 + train_df.f_01 + train_df.f_26 > 5.0).astype(int) - \\\n                        (train_df.f_00 + train_df.f_01 + train_df.f_26 < -5.0).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:56:22.205182Z","iopub.execute_input":"2022-06-16T07:56:22.205866Z","iopub.status.idle":"2022-06-16T07:56:22.216661Z","shell.execute_reply.started":"2022-06-16T07:56:22.205834Z","shell.execute_reply":"2022-06-16T07:56:22.215744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus using the SHAP interaction analysis, we were able to engineer 3 additional features :  \n1. i_f02_f21\n2. i_f05_f22\n3. i_f00_f01_f26","metadata":{}},{"cell_type":"markdown","source":"Credits:\n\n[TPSMAY22 EDA which makes sense](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense/notebook)  \n\n[Analysing Interactions with SHAP](https://www.kaggle.com/code/wti200/analysing-interactions-with-shap/notebook)","metadata":{}}]}