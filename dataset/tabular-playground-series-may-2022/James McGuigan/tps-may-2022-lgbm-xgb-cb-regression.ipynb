{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS May 2022 - LGBM + XGB + CB - Regression\n\nThe top three models that are usally suggested by PyCaret are \n[LightGBM](https://lightgbm.readthedocs.io/en/latest/) \n[XGBoost](https://xgboost.readthedocs.io/en/latest/) and \n[CatBoost](https://catboost.ai/en/docs/)\nso lets just do a little data-preprocessing and put them in an ensemble\n\nWe are actually going to do this two ways, comparing both regression and classification in two seperate notebooks:\n- [LGBM + XGB + CB - Regression](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-regression)\n- [LGBM + XGB + CB - Classification](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-classification)\n","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import numpy  as np \nimport pandas as pd \nimport re\nimport sklearn\nimport scipy\nimport lightgbm\nimport catboost\nimport xgboost\n\npd.options.display.max_columns = 999\npd.options.display.max_rows    = 6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T20:18:31.25114Z","iopub.execute_input":"2022-05-31T20:18:31.251672Z","iopub.status.idle":"2022-05-31T20:18:34.829438Z","shell.execute_reply.started":"2022-05-31T20:18:31.251631Z","shell.execute_reply":"2022-05-31T20:18:34.82862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Use float32 + int32 to save memory vs 64bit types\n# Question: should the ints be cast to category???\ncol_dtypes = {\n  \"f_00\": \"float32\",\n  \"f_01\": \"float32\",\n  \"f_02\": \"float32\",\n  \"f_03\": \"float32\",\n  \"f_04\": \"float32\",\n  \"f_05\": \"float32\",\n  \"f_06\": \"float32\",\n  \"f_07\": \"int32\",\n  \"f_08\": \"int32\",\n  \"f_09\": \"int32\",\n  \"f_10\": \"int32\",\n  \"f_11\": \"int32\",\n  \"f_12\": \"int32\",\n  \"f_13\": \"int32\",\n  \"f_14\": \"int32\",\n  \"f_15\": \"int32\",\n  \"f_16\": \"int32\",\n  \"f_17\": \"int32\",\n  \"f_18\": \"int32\",\n  \"f_19\": \"float32\",\n  \"f_20\": \"float32\",\n  \"f_21\": \"float32\",\n  \"f_22\": \"float32\",\n  \"f_23\": \"float32\",\n  \"f_24\": \"float32\",\n  \"f_25\": \"float32\",\n  \"f_26\": \"float32\",\n  \"f_27\": \"category\",\n  \"f_28\": \"float32\",\n  \"f_29\": \"category\", # was: \"int32\",\n  \"f_30\": \"category\", # was: \"int32\",\n  \"target\": \"int32\",\n}\ndef preprocess_df(df):\n    # Expand out the f_27 column\n    df[['f_27_0','f_27_1','f_27_2','f_27_3','f_27_4','f_27_5','f_27_6','f_27_7','f_27_8','f_27_9','f_27_10','f_27_00']] \\\n        = df['f_27'].str.split('',expand=True).astype(\"category\")\n    del df['f_27']     # very high cardinality | BUGFIX: LightGBMError: bin size 672 cannot run on GPU\n    del df['f_27_0']   # str.split('') adds empty columns on either side\n    del df['f_27_00']  # str.split('') adds empty columns on either side\n       \n    # Engineering the top three feature interactions \n    # Source: https://www.kaggle.com/competitions/tabular-playground-series-may-2022/discussion/323892\n    df['i_02_21'] = ((df.f_21 + df.f_02 >  5.2).astype(int)\n                  -  (df.f_21 + df.f_02 < -5.3).astype(int))\n    df['i_05_22'] = ((df.f_22 + df.f_05 >  5.1).astype(int)\n                   - (df.f_22 + df.f_05 < -5.4).astype(int))\n    df['i_00_01_26'] = (((df.f_00 + df.f_01 + df.f_26) >  5.0).astype(int)\n                      - ((df.f_00 + df.f_01 + df.f_26) < -5.0).astype(int))\n        \n    return df\n\ndef fix_missing_columns(train_df, test_df):\n    # Find all columns present in one dataframe, but not in the other\n    missing_cols = (set(train_df.columns) - set(test_df.columns))  \\\n                 | (set(test_df.columns)  - set(train_df.columns)) \n    missing_cols -= set([\"target\"])\n    for col in missing_cols:\n        train_df[col] = train_df.get(col,0)  # add zeros column if missing\n        test_df[col]  = test_df.get(col,0)   # add zeros column if missing\n        \n    assert set(train_df.columns) - set(test_df.columns) == set([\"target\"])\n    return train_df, test_df\n\ntrain_df = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id', dtype=col_dtypes)\ntest_df  = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',  index_col='id', dtype=col_dtypes)\ntrain_df = preprocess_df(train_df)\ntest_df  = preprocess_df(test_df)\nfix_missing_columns(train_df, test_df)\n\ncolumns = test_df.columns\nX       = train_df[columns]\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\nX_test  = test_df[columns]\n\ndisplay('train_df')\ndisplay( train_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( train_df )\ndisplay('test_df')\n# display( test_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( test_df )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:18:34.831371Z","iopub.execute_input":"2022-05-31T20:18:34.831877Z","iopub.status.idle":"2022-05-31T20:19:03.041388Z","shell.execute_reply.started":"2022-05-31T20:18:34.831816Z","shell.execute_reply":"2022-05-31T20:19:03.040478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"%%time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nbest_rmse     = 9999999999\nbest_params   = {}\nbest_lightgbm = None\n\ndef train_lightgbm(parameters, default_params):    \n    # global best_rmse, best_params, best_model\n    # DOCS: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html\n    model = lightgbm.train(\n        {\n            **default_params,\n            **parameters,\n        },\n        train_set  = lightgbm.Dataset(X_train, label=Y_train),\n        valid_sets = lightgbm.Dataset(X_valid, label=Y_valid),\n        num_boost_round       = 5000,\n        early_stopping_rounds = 100,\n        verbose_eval          = False,\n    )\n    rmse = sklearn.metrics.mean_squared_error(Y_valid, model.predict(X_valid), squared=False)\n    \n    print(f'rmse: {rmse:.5f} | parameters: {parameters}')\n    return rmse, model\n    \n    \n# NOTE: Reusing Hyperparamters from TPS Jan 2021\nfor seed in [42]:\n    # for boosting in ['gbdt', 'goss', 'dart']:                     # \n    # for max_depth in [1,2,4,6,8,10,12,16,32,64,-1]:               # \n    # for tree_learner in ['serial', 'feature', 'data', 'voting']:  # was: no effect\n    # for extra_trees in [True, False]:                             # was: no effect\n    # for learning_rate in [0.001, 0.01, 0.1, 0.5, 0.9]:            # \n    # for max_bin in [64,128,256], # ,512,1024,2048]:               # gpu max_bin = 255\n    # for num_leaves in [32, 64, 128, 256, 512, 1024, 2048, 4096]:  # \n\n    # DOCS: https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst\n    # DOCS: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n    default_params = {\n        'device':         'gpu',   \n        'boosting_type':  'gbdt',        # default\n        'objective':      'regression',  # 'classification'\n        'metric':         'auc',         # ROC not RMSE is competition metric\n        'learning_rate':   0.1,                     \n        'max_depth':       16,\n        'max_bin':         256-1,        # gpu max_bin = 255\n        'num_leaves':      64-1,\n        'seed':            42,\n        'verbose':         -1,\n    }\n    parameters = {\n        # 'boosting_type':   boosting,\n        # 'max_depth':       max_depth, \n        # 'tree_learner':    tree_learner,\n        # 'extra_trees':     extra_trees,\n        # 'learning_rate':   learning_rate,\n        # 'max_bin':         max_bin-1,\n        # 'num_leaves':      num_leaves-1,\n    }\n    rmse, model = train_lightgbm(parameters, default_params)\n\n    if rmse < best_rmse:\n        best_rmse     = rmse\n        best_params   = parameters\n        best_lightgbm = model\n\nprint()\nprint(f'BEST rmse: {rmse:.5f} | parameters: {best_params} | model: {best_lightgbm}')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:19:03.04285Z","iopub.execute_input":"2022-05-31T20:19:03.043531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprediction_X_train = best_lightgbm.predict(X_train)\nprediction_X_valid = best_lightgbm.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"%%time\n\n# DOCS: https://xgboost.readthedocs.io/en/stable/parameter.html\n# best_xgboost = xgboost.XGBClassifier(\nbest_xgboost = xgboost.XGBRegressor(\n    n_jobs=-1,\n    verbosity=0,\n    random_state=42,\n    enable_categorical=True,\n    tree_method='gpu_hist',\n    objective='reg:squarederror',\n    eval_metric='auc',  # ROC not RMSE is competition metric\n\n    # NOTE: Hyperparameters stolen from: https://www.kaggle.com/code/jamesmcguigan/tps-pycaret2-automl-regression\n    base_score=0.5, booster='gbtree', colsample_bylevel=1,\n    colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,\n    importance_type='gain', interaction_constraints='',\n    learning_rate=0.274, max_delta_step=0, max_depth=3,\n    min_child_weight=3, monotone_constraints='()',\n    n_estimators=200, num_parallel_tree=1,\n    reg_alpha=0.7,\n    reg_lambda=0.15, scale_pos_weight=48.30000000000001, subsample=0.7,\n)\nbest_xgboost.fit(\n    X_train, Y_train, \n    eval_set=[(X_valid, Y_valid)],\n    verbose=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"cat_features = list(X_train.select_dtypes(\"category\").columns)\nstr(cat_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\n# best_catboost = catboost.CatBoostClassifier(\nbest_catboost = catboost.CatBoostRegressor(\n    task_type     = \"GPU\",\n    eval_metric   = 'AUC',  # ROC not RMSE is competition metric\n    cat_features  = cat_features,\n    verbose       = 1000,    \n\n    # NOTE: Hyperparameters stolen from https://www.kaggle.com/code/utkarshshukla2912/cat-boost-feature-engineering/notebook?scriptVersionId=95518961    \n    n_estimators  = 3161,\n    learning_rate = 0.06483741752243545,\n    depth         = 11,\n    l2_leaf_reg   = 5,\n)\n# DOCS: https://catboost.ai/en/docs/concepts/python-usages-examples\nbest_catboost.fit(\n               catboost.Pool(X_train, Y_train, baseline=prediction_X_train, cat_features=cat_features), \n    eval_set = catboost.Pool(X_valid, Y_valid, baseline=prediction_X_valid, cat_features=cat_features),\n    use_best_model=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Binary prediction requires 3-way voting\npredictions = np.array([\n    best_lightgbm.predict(X_test),\n    best_xgboost.predict(X_test),\n    best_catboost.predict(X_test)\n])\ndisplay( predictions )\nprint( '' )\ndisplay( scipy.stats.describe(predictions.T) )\nprint( '' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For regression     we get floats back and use .clip() \n# For classification we get ints   back and use .round()\nprediction_ensemble = ( \n    predictions\n    .clip(0, 1)    # prevent regression out-of-bounds predictions\n    .mean(axis=0)  # 3-way voting\n    .clip(0, 1)    # prevent regression out-of-bounds predictions\n)\ndisplay( prediction_ensemble )\nprint( '' )\ndisplay( scipy.stats.describe(prediction_ensemble) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv', index_col='id')\nsubmission_df['target'] = prediction_ensemble\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Tabular Data:\n\n[Titanic](https://www.kaggle.com/competitions/titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n\n[Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n- 0.69932 - [XGBoost](https://www.kaggle.com/code/jamesmcguigan/spaceship-titanic-xgboost)\n\n[Tabular Playground - Jan 2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021)\n- 0.72746 / 0.72935 - [scikit-learn Ensemble](https://www.kaggle.com/jamesmcguigan/tabular-playground-scikit-learn-ensemble)\n- 0.71552 / 0.71659 - [Fast.ai Tabular Solver](https://www.kaggle.com/jamesmcguigan/fast-ai-tabular-solver)\n- 0.70317 / 0.70426 - [XGBoost](https://www.kaggle.com/jamesmcguigan/tabular-playground-xgboost)\n- 0.70011 / 0.70181 - [LightGBM](https://www.kaggle.com/jamesmcguigan/tabular-playground-lightgbm)\n\n[Tabular Playground - Feb 2021](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.84452 - [PyCaret2 AutoML Regression](https://www.kaggle.com/jamesmcguigan/tps-pycaret2-automl-regression)\n\n[Tabular Playground - May 2022](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.97134 - [LightGBM](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lightgbm-regression)\n- [LGBM + XGB + CB - Regression](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-regression)\n- [LGBM + XGB + CB - Classification](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-classification)\n\nIf you found this notebook useful or learnt something new, then please upvote!","metadata":{}}]}