{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS May 2022 - Scikit-Learn Ensemble\n\nThe competition description says this is a linear regression problem, so lets see what Scikit Learn can do\n- https://scikit-learn.org/stable/modules/linear_model.html#linear-model","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport sklearn\nfrom operator import itemgetter\nfrom sklearn.linear_model import *\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom markupsafe import escape\nimport pprint\n\npp = pprint.PrettyPrinter(width=41, compact=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T18:10:03.223205Z","iopub.execute_input":"2022-05-31T18:10:03.223771Z","iopub.status.idle":"2022-05-31T18:10:04.467052Z","shell.execute_reply.started":"2022-05-31T18:10:03.22365Z","shell.execute_reply":"2022-05-31T18:10:04.466065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncol_dtypes = {\n  \"f_00\": \"float32\",\n  \"f_01\": \"float32\",\n  \"f_02\": \"float32\",\n  \"f_03\": \"float32\",\n  \"f_04\": \"float32\",\n  \"f_05\": \"float32\",\n  \"f_06\": \"float32\",\n  \"f_07\": \"int32\",\n  \"f_08\": \"int32\",\n  \"f_09\": \"int32\",\n  \"f_10\": \"int32\",\n  \"f_11\": \"int32\",\n  \"f_12\": \"int32\",\n  \"f_13\": \"int32\",\n  \"f_14\": \"int32\",\n  \"f_15\": \"int32\",\n  \"f_16\": \"int32\",\n  \"f_17\": \"int32\",\n  \"f_18\": \"int32\",\n  \"f_19\": \"float32\",\n  \"f_20\": \"float32\",\n  \"f_21\": \"float32\",\n  \"f_22\": \"float32\",\n  \"f_23\": \"float32\",\n  \"f_24\": \"float32\",\n  \"f_25\": \"float32\",\n  \"f_26\": \"float32\",\n  \"f_27\": \"category\",\n  \"f_28\": \"float32\",\n  \"f_29\": \"int32\",\n  \"f_30\": \"int32\",\n  \"target\": \"int32\",\n}\ndef preprocess_df(df):\n    # Split f_27 into letters\n    col_names = ['f_27_0','f_27_1','f_27_2','f_27_3','f_27_4','f_27_5','f_27_6','f_27_7','f_27_8','f_27_9','f_27_10','f_27_00']\n    df[col_names] = df['f_27'].str.split('',expand=True).astype(\"category\")\n    del df['f_27']\n    del df['f_27_0']   # split gives emoty columns on end\n    del df['f_27_00']  # split gives emoty columns on end\n    \n    # One Hot Encode\n    for col_name in col_names:\n        if col_name not in df.columns: continue \n        try:\n            dummies = pd.get_dummies(df[col_name], prefix=col_name, drop_first=False)\n            df = pd.concat([df, dummies], axis=1)\n        except: continue\n        del df[col_name]                        \n    return df\n\ndef fix_missing_columns(train_df, test_df):\n    missing_cols = (set(train_df.columns) - set(test_df.columns))  \\\n                 | (set(test_df.columns)  - set(train_df.columns)) \\\n                 - set([\"target\"])\n    for col in missing_cols:\n        train_df[col] = train_df.get(col,0)  # add zeros column if missing\n        test_df[col]  = test_df.get(col,0)   # add zeros column if missing\n    return train_df, test_df\n    \n\ntrain_df = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id', dtype=col_dtypes)\ntest_df  = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',  index_col='id', dtype=col_dtypes)\ntrain_df = preprocess_df(train_df)\ntest_df  = preprocess_df(test_df)\ntrain_df, test_df = fix_missing_columns(train_df, test_df)\n\ncolumns = test_df.columns\nX       = train_df[columns]\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\nX_test  = test_df[columns]\n\ndisplay('train_df')\ndisplay( train_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( train_df )\ndisplay('test_df')\n# display( test_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( test_df )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:10:04.468664Z","iopub.execute_input":"2022-05-31T18:10:04.469053Z","iopub.status.idle":"2022-05-31T18:10:41.081185Z","shell.execute_reply.started":"2022-05-31T18:10:04.46902Z","shell.execute_reply":"2022-05-31T18:10:41.080107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nAdding PolynomialFeatures actually reduces the score, so this is indeed a pure linear regression problem\n- `degree=2` = `0.76347`\n- `degree=1` = `0.72935`","metadata":{}},{"cell_type":"code","source":"# DOCS: https://scikit-learn.org/stable/modules/preprocessing.html\n# DOCS: https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions\ndef preprocess_X(X, degree=1):\n    # NOTE: PolynomialFeatures needs to be done before scaling - https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9\n    X = PolynomialFeatures(degree=degree).fit_transform(X)\n    X = StandardScaler().fit_transform(X)\n    return X\n\ncolumns = test_df.columns\nX_test  = preprocess_X( test_df[columns],  degree=2 )\nX       = preprocess_X( train_df[columns], degree=2 )\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.05, random_state=42)\n\nprint('X_train.shape', X_train.shape)\nprint('Y_train.shape', Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:10:41.0825Z","iopub.execute_input":"2022-05-31T18:10:41.082999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Models\n\nLets try out each of the Linear Models from scikit-learn in a loop\n- https://scikit-learn.org/0.20/modules/classes.html#module-sklearn.linear_model","metadata":{}},{"cell_type":"code","source":"# Lazy Load models to avoid out of memory errors\nmodels = [\n    (sklearn.linear_model.ARDRegression, {}),          \n    # ([n_iter, tol, …])\tBayesian ARD regression.\n    \n    (sklearn.linear_model.BayesianRidge, {}),          \n    # ([n_iter, tol, …])\tBayesian ridge regression\n    \n    (sklearn.linear_model.ElasticNet, {}),             \n    # ([alpha, l1_ratio, …])\tLinear regression with combined L1 and L2 priors as regularizer.\n    \n    # (sklearn.linear_model.ElasticNetCV, {\"max_iter\":10_000}),           \n    # ([l1_ratio, eps, …])\tElastic Net model with iterative fitting along a regularization path.\n    \n    (sklearn.linear_model.HuberRegressor, {\"max_iter\":1000}),         \n    # ([epsilon, …])\tLinear regression model that is robust to outliers.\n    \n    (sklearn.linear_model.Lars, {}),                   \n    # ([fit_intercept, verbose, …])\tLeast Angle Regression model a.k.a.\n    \n    (sklearn.linear_model.LarsCV, {}),                 \n    # ([fit_intercept, …])\tCross-validated Least Angle Regression model.\n    \n    (sklearn.linear_model.Lasso, {}),                  \n    # ([alpha, fit_intercept, …])\tLinear Model trained with L1 prior as regularizer (aka the Lasso)\n    \n    (sklearn.linear_model.LassoCV, {\"max_iter\":10_000}), \n    # ([eps, n_alphas, …])\tLasso linear model with iterative fitting along a regularization path.\n    \n    (sklearn.linear_model.LassoLars, {}),              \n    # ([alpha, …])\tLasso model fit with Least Angle Regression a.k.a.\n    \n    (sklearn.linear_model.LassoLarsCV, {}),            \n    # ([fit_intercept, …])\tCross-validated Lasso, using the LARS algorithm.\n    \n    (sklearn.linear_model.LassoLarsIC, {}),            \n    # ([criterion, …])\tLasso model fit with Lars using BIC or AIC for model selection\n    \n    (sklearn.linear_model.LinearRegression, {}),       \n    # ([…])\tOrdinary least squares Linear Regression.  \n    \n    # (sklearn.linear_model.LogisticRegression, {}),     \n    # ([penalty, …])\tLogistic Regression (aka logit, MaxEnt) classifier.\n    \n    # (sklearn.linear_model.LogisticRegressionCV, {}),   \n    # ([Cs, …])\tLogistic Regression CV (aka logit, MaxEnt) classifier.\n    \n    # (sklearn.linear_model.MultiTaskLasso, {}),         \n    # ([alpha, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n    \n    # (sklearn.linear_model.MultiTaskElasticNet, {}),    \n    # ([alpha, …])\tMulti-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n    \n    # (sklearn.linear_model.MultiTaskLassoCV, {}),       \n    # ([eps, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n    \n    # (sklearn.linear_model.MultiTaskElasticNetCV, {}),  \n    # ([…])\tMulti-task L1/L2 ElasticNet with built-in cross-validation.\n    \n    (sklearn.linear_model.OrthogonalMatchingPursuit, {}),    \n    # ([…])\tOrthogonal Matching Pursuit model(OMP)\n    \n    (sklearn.linear_model.OrthogonalMatchingPursuitCV, {}),  \n    # ([…])\tCross-validated Orthogonal Matching Pursuit model (OMP).\n    \n    # (sklearn.linear_model.PassiveAggressiveClassifier, {}),  \n    # ([…])\tPassive Aggressive Classifier\n    \n    (sklearn.linear_model.PassiveAggressiveRegressor, {}),   \n    # ([C, …])\tPassive Aggressive Regressor # Poor Score\n    \n    # (sklearn.linear_model.Perceptron, {}),             \n    # ([penalty, alpha, …])\tRead more in the User Guide.\n    \n    (sklearn.linear_model.RANSACRegressor, {}),        \n    # ([…])\tRANSAC (RANdom SAmple Consensus) algorithm.  # Poor Score\n    \n    (sklearn.linear_model.Ridge, {}),                  \n    # ([alpha, fit_intercept, …])\tLinear least squares with l2 regularization.\n    \n    # (sklearn.linear_model.RidgeClassifier, {}),        \n    # ([alpha, …])\tClassifier using Ridge regression.\n    \n    # (sklearn.linear_model.RidgeClassifierCV, {}),      \n    # ([alphas, …])\tRidge classifier with built-in cross-validation.\n    \n    (sklearn.linear_model.RidgeCV, {}),                \n    # ([alphas, …])\tRidge regression with built-in cross-validation.\n    \n    # (sklearn.linear_model.SGDClassifier, {}),          \n    # ([loss, penalty, …])\tLinear classifiers(SVM, logistic regression, a.o.) with SGD training.\n    \n    (sklearn.linear_model.SGDRegressor, {}),           \n    # ([loss, penalty, …])\tLinear model fitted by minimizing a regularized empirical loss with SGD\n    \n    # (sklearn.linear_model.TheilSenRegressor, {}),      \n    # ([…])\tTheil-Sen Estimator: robust multivariate regression model.  # Cause OOM Exception\n    \n    # sklearn.linear_model.enet_path(X_train, Y_train),           \n    # (X, y[, l1_ratio, …])\tCompute elastic net path with coordinate descent\n    \n    # sklearn.linear_model.lars_path(X_train, Y_train),           \n    # (X, y[, Xy, Gram, …])\tCompute Least Angle Regression or Lasso path using LARS algorithm [1]\n    \n    # sklearn.linear_model.lasso_path(X_train, Y_train),          \n    # (X, y[, eps, …])\tCompute Lasso path with coordinate descent\n    \n    # sklearn.linear_model.logistic_regression_path(X_train, Y_train),  \n    # (X, y)\tCompute a Logistic Regression model for a list of regularization parameters.\n    \n    # sklearn.linear_model.orthogonal_mp(X_train, Y_train),       \n    # (X, y[, …])\tOrthogonal Matching Pursuit(OMP)\n    \n    # sklearn.linear_model.orthogonal_m_gram(X_train, Y_train),  \n    # (Gram, Xy[, …])\tGram Orthogonal Matching Pursuit(OMP)\n    \n    # sklearn.linear_model.ridge_regression(X_train, Y_train, alpha=0.1),    \n    # (X, y, alpha[, …])\tSolve the ridge equation by the method of normal equations.\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport time\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning) \n\ndef fit_predict(model_class, kwargs, verbose=True):\n    time_start = time.perf_counter()    \n\n    name  = model_class.__name__\n    model = model_class(**kwargs)  \n    model.fit(X_train, Y_train)\n    rmse       = sklearn.metrics.mean_squared_error(Y_valid, model.predict(X_valid), squared=False)\n    prediction = model.predict(X_test)\n    \n    time_taken = time_start - time.perf_counter()\n    if verbose: print(f'{time_taken:3.1f}s | rmse = {rmse:3.3f} | {name}')        \n    return name, rmse, prediction \n\n\nscores      = {}\npredictions = {}\nfor model_class, kwargs in models:\n    try:\n        name, rmse, prediction = fit_predict(model_class, kwargs)\n        scores[name]      = rmse\n        predictions[name] = prediction\n    except:\n        print('ERROR', model_class.__name__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = dict(sorted(scores.items(), key=itemgetter(1), reverse=False))\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"Y_test = np.mean([\n    predictions[name]\n    for name, score in scores.items()\n    if score <= 0.8  # Remove poor performers\n], axis=0)\nprint('Y_test.shape', Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv', index_col='id')\nsubmission_df['target'] = Y_test\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Tabular Data:\n\n[Titanic](https://www.kaggle.com/competitions/titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n\n[Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n- 0.69932 - [XGBoost](https://www.kaggle.com/code/jamesmcguigan/spaceship-titanic-xgboost)\n\n[Tabular Playground - Jan 2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021)\n- 0.72746 / 0.72935 - [scikit-learn Ensemble](https://www.kaggle.com/jamesmcguigan/tabular-playground-scikit-learn-ensemble)\n- 0.71552 / 0.71659 - [Fast.ai Tabular Solver](https://www.kaggle.com/jamesmcguigan/fast-ai-tabular-solver)\n- 0.70317 / 0.70426 - [XGBoost](https://www.kaggle.com/jamesmcguigan/tabular-playground-xgboost)\n- 0.70011 / 0.70181 - [LightGBM](https://www.kaggle.com/jamesmcguigan/tabular-playground-lightgbm)\n\n[Tabular Playground - Feb 2021](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.84452 - [PyCaret2 AutoML Regression](https://www.kaggle.com/jamesmcguigan/tps-pycaret2-automl-regression)\n\n[Tabular Playground - May 2022](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.97134 - [LightGBM + XGBoost + Catboost](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lightgbm-xgboost-catboost)\n\n\nIf you found this notebook useful or learnt something new, then please upvote!","metadata":{}}]}