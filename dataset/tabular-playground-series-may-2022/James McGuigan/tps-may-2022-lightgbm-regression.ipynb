{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground May 2022 - LightGBM\n\n[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a popular alterative to \n[XGBoost](https://xgboost.readthedocs.io/en/latest/) and \n[CatBoost](https://catboost.ai/en/docs/)\nand it performs surprising well with a `0.97134` score in regression mode\n","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import numpy  as np \nimport pandas as pd \nimport re\nimport sklearn\nimport scipy\nimport lightgbm\nimport catboost\nimport xgboost\n\npd.options.display.max_columns = 999\npd.options.display.max_rows    = 6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T18:56:30.644558Z","iopub.execute_input":"2022-05-31T18:56:30.647379Z","iopub.status.idle":"2022-05-31T18:56:30.657123Z","shell.execute_reply.started":"2022-05-31T18:56:30.647331Z","shell.execute_reply":"2022-05-31T18:56:30.65596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncategory_dtype = 'category'  # 'category'  # catboost doesn't like category\ncol_dtypes = {\n  \"f_00\": \"float32\",\n  \"f_01\": \"float32\",\n  \"f_02\": \"float32\",\n  \"f_03\": \"float32\",\n  \"f_04\": \"float32\",\n  \"f_05\": \"float32\",\n  \"f_06\": \"float32\",\n  \"f_07\": \"int32\",\n  \"f_08\": \"int32\",\n  \"f_09\": \"int32\",\n  \"f_10\": \"int32\",\n  \"f_11\": \"int32\",\n  \"f_12\": \"int32\",\n  \"f_13\": \"int32\",\n  \"f_14\": \"int32\",\n  \"f_15\": \"int32\",\n  \"f_16\": \"int32\",\n  \"f_17\": \"int32\",\n  \"f_18\": \"int32\",\n  \"f_19\": \"float32\",\n  \"f_20\": \"float32\",\n  \"f_21\": \"float32\",\n  \"f_22\": \"float32\",\n  \"f_23\": \"float32\",\n  \"f_24\": \"float32\",\n  \"f_25\": \"float32\",\n  \"f_26\": \"float32\",\n  \"f_27\": category_dtype,\n  \"f_28\": \"float32\",\n  \"f_29\": \"int32\",\n  \"f_30\": \"int32\",\n  \"target\": \"int32\",\n}\ndef preprocess_df(df):\n    df[['f_27_0','f_27_1','f_27_2','f_27_3','f_27_4','f_27_5','f_27_6','f_27_7','f_27_8','f_27_9','f_27_10','f_27_00']] \\\n        = df['f_27'].str.split('',expand=True).astype(category_dtype)\n    del df['f_27']     # very high cardinality | BUGFIX: LightGBMError: bin size 672 cannot run on GPU\n    del df['f_27_0']   # str.split('') adds empty columns on either side\n    del df['f_27_00']  # str.split('') adds empty columns on either side\n    return df\n\ndef fix_missing_columns(train_df, test_df):\n    # Find all columns present in one dataframe, but not in the other\n    missing_cols = (set(train_df.columns) - set(test_df.columns))  \\\n                 | (set(test_df.columns)  - set(train_df.columns)) \n    missing_cols -= set([\"target\"])\n    for col in missing_cols:\n        train_df[col] = train_df.get(col,0)  # add zeros column if missing\n        test_df[col]  = test_df.get(col,0)   # add zeros column if missing\n        \n    assert set(train_df.columns) - set(test_df.columns) == set([\"target\"])\n    return train_df, test_df\n\ntrain_df = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id', dtype=col_dtypes)\ntest_df  = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',  index_col='id', dtype=col_dtypes)\ntrain_df = preprocess_df(train_df)\ntest_df  = preprocess_df(test_df)\nfix_missing_columns(train_df, test_df)\n\ncolumns = test_df.columns\nX       = train_df[columns]\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\nX_test  = test_df[columns]\n\ndisplay('train_df')\ndisplay( train_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( train_df )\ndisplay('test_df')\n# display( test_df.info(verbose=True, memory_usage=\"deep\") )\ndisplay( test_df )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:44:18.525365Z","iopub.execute_input":"2022-05-31T18:44:18.525797Z","iopub.status.idle":"2022-05-31T18:44:42.807291Z","shell.execute_reply.started":"2022-05-31T18:44:18.525753Z","shell.execute_reply":"2022-05-31T18:44:42.805963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"%%time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nbest_rmse     = 9999999999\nbest_params   = {}\nbest_lightgbm = None\n\ndef train_lightgbm(parameters, default_params):    \n    # global best_rmse, best_params, best_model\n    # DOCS: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html\n    model = lightgbm.train(\n        {\n            **default_params,\n            **parameters,\n        },\n        train_set  = lightgbm.Dataset(X_train, label=Y_train),\n        valid_sets = lightgbm.Dataset(X_valid, label=Y_valid),\n        num_boost_round       = 5000,\n        early_stopping_rounds = 100,\n        verbose_eval          = False,\n    )\n    rmse = sklearn.metrics.mean_squared_error(Y_valid, model.predict(X_valid), squared=False)\n    \n    print(f'rmse: {rmse:.5f} | parameters: {parameters}')\n    return rmse, model\n    \n    \n# NOTE: Reusing Hyperparamters from TPS Jan 2021\nfor seed in [42]:\n    # for boosting in ['gbdt', 'goss', 'dart']:                     # \n    # for max_depth in [1,2,4,6,8,10,12,16,32,64,-1]:               # \n    # for tree_learner in ['serial', 'feature', 'data', 'voting']:  # was: no effect\n    # for extra_trees in [True, False]:                             # was: no effect\n    # for learning_rate in [0.001, 0.01, 0.1, 0.5, 0.9]:            # \n    # for max_bin in [64,128,256], # ,512,1024,2048]:               # gpu max_bin = 255\n    # for num_leaves in [32, 64, 128, 256, 512, 1024, 2048, 4096]:  # \n\n    # DOCS: https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst\n    # DOCS: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n    default_params = {\n        'device':         'gpu',   \n        'boosting_type':  'gbdt',  # default\n        'objective':      'regression',\n        'metric':         'rmse',\n        'learning_rate':   0.1,                     \n        'max_depth':       16,\n        'max_bin':         256-1,  # gpu max_bin = 255\n        'num_leaves':      64-1,\n        'seed':            42,\n        'verbose':         -1,\n    }\n    parameters = {\n        # 'boosting_type':   boosting,\n        # 'max_depth':       max_depth, \n        # 'tree_learner':    tree_learner,\n        # 'extra_trees':     extra_trees,\n        # 'learning_rate':   learning_rate,\n        # 'max_bin':         max_bin-1,\n        # 'num_leaves':      num_leaves-1,\n    }\n    rmse, model = train_lightgbm(parameters, default_params)\n\n    if rmse < best_rmse:\n        best_rmse     = rmse\n        best_params   = parameters\n        best_lightgbm = model\n\nprint()\nprint(f'BEST rmse: {rmse:.5f} | parameters: {best_params} | model: {best_lightgbm}')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:44:42.80983Z","iopub.execute_input":"2022-05-31T18:44:42.810291Z","iopub.status.idle":"2022-05-31T18:47:51.850556Z","shell.execute_reply.started":"2022-05-31T18:44:42.810232Z","shell.execute_reply":"2022-05-31T18:47:51.849301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprediction_X_train = best_lightgbm.predict(X_train)\nprediction_X_valid = best_lightgbm.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:47:51.852244Z","iopub.execute_input":"2022-05-31T18:47:51.853167Z","iopub.status.idle":"2022-05-31T18:51:12.556091Z","shell.execute_reply.started":"2022-05-31T18:47:51.85312Z","shell.execute_reply":"2022-05-31T18:51:12.553355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:51:12.559918Z","iopub.execute_input":"2022-05-31T18:51:12.560698Z","iopub.status.idle":"2022-05-31T18:51:12.625708Z","shell.execute_reply.started":"2022-05-31T18:51:12.56065Z","shell.execute_reply":"2022-05-31T18:51:12.62431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \npredictions = best_lightgbm.predict(X_test)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:56:30.666925Z","iopub.execute_input":"2022-05-31T18:56:30.669382Z","iopub.status.idle":"2022-05-31T18:59:03.505459Z","shell.execute_reply.started":"2022-05-31T18:56:30.669337Z","shell.execute_reply":"2022-05-31T18:59:03.504338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scipy.stats.describe(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:59:03.508087Z","iopub.execute_input":"2022-05-31T18:59:03.509047Z","iopub.status.idle":"2022-05-31T18:59:03.533252Z","shell.execute_reply.started":"2022-05-31T18:59:03.509004Z","shell.execute_reply":"2022-05-31T18:59:03.53232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For some reason, the lightgbm predictions don't come out as integers, so lets round them","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv', index_col='id')\nsubmission_df['target'] = predictions\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-05-31T18:53:47.391503Z","iopub.execute_input":"2022-05-31T18:53:47.392157Z","iopub.status.idle":"2022-05-31T18:53:50.889465Z","shell.execute_reply.started":"2022-05-31T18:53:47.392061Z","shell.execute_reply":"2022-05-31T18:53:50.888259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Tabular Data:\n\n[Titanic](https://www.kaggle.com/competitions/titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n\n[Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)\n- [Profilereport EDA](https://www.kaggle.com/code/jamesmcguigan/titanic-profilereport-eda)\n- 0.69932 - [XGBoost](https://www.kaggle.com/code/jamesmcguigan/spaceship-titanic-xgboost)\n\n[Tabular Playground - Jan 2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021)\n- 0.72746 / 0.72935 - [scikit-learn Ensemble](https://www.kaggle.com/jamesmcguigan/tabular-playground-scikit-learn-ensemble)\n- 0.71552 / 0.71659 - [Fast.ai Tabular Solver](https://www.kaggle.com/jamesmcguigan/fast-ai-tabular-solver)\n- 0.70317 / 0.70426 - [XGBoost](https://www.kaggle.com/jamesmcguigan/tabular-playground-xgboost)\n- 0.70011 / 0.70181 - [LightGBM](https://www.kaggle.com/jamesmcguigan/tabular-playground-lightgbm)\n\n[Tabular Playground - Feb 2021](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.84452 - [PyCaret2 AutoML Regression](https://www.kaggle.com/jamesmcguigan/tps-pycaret2-automl-regression)\n\n[Tabular Playground - May 2022](https://www.kaggle.com/c/tabular-playground-series-feb-2021)\n- 0.97134 - [LightGBM](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lightgbm-regression)\n- [LGBM + XGB + CB - Regression](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-regression)\n- [LGBM + XGB + CB - Classification](https://www.kaggle.com/jamesmcguigan/tps-may-2022-lgbm-xgb-cb-classification)\n\nIf you found this notebook useful or learnt something new, then please upvote!","metadata":{}}]}