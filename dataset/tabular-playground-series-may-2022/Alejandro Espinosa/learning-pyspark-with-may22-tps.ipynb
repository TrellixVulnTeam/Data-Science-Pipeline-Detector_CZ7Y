{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I am learning `pyspark` and for this test, I am using the [May 2022 Tabular Playground Series](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/overview).  \n\nDisclaimer: I am not claiming that this is the best solution for that Series, again I am learning this package.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## Downloading and loading packages","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-01T08:49:29.763435Z","iopub.execute_input":"2022-06-01T08:49:29.763888Z","iopub.status.idle":"2022-06-01T08:50:20.91779Z","shell.execute_reply.started":"2022-06-01T08:49:29.763793Z","shell.execute_reply":"2022-06-01T08:50:20.917024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark import keyword_only\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.classification import LogisticRegression, GBTClassifier\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorIndexer, StandardScaler, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml import Pipeline, Transformer\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-01T08:50:20.919881Z","iopub.execute_input":"2022-06-01T08:50:20.92059Z","iopub.status.idle":"2022-06-01T08:50:21.247804Z","shell.execute_reply.started":"2022-06-01T08:50:20.920542Z","shell.execute_reply":"2022-06-01T08:50:21.246789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder.appName('learning').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:21.249189Z","iopub.execute_input":"2022-06-01T08:50:21.249531Z","iopub.status.idle":"2022-06-01T08:50:27.600415Z","shell.execute_reply.started":"2022-06-01T08:50:21.249494Z","shell.execute_reply":"2022-06-01T08:50:27.599227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I am not sure if there are benefits on manually creating the input file structure before loading the `csv` file.   ","metadata":{}},{"cell_type":"code","source":"listStruct = []\nlistStruct.append( StructField('id', IntegerType(), nullable=True) )\nfor f in [ 'f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06']:\n    listStruct.append( StructField(f, FloatType(), nullable=True) )\nfor i in [ 'f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18' ]:\n    listStruct.append( StructField(i, IntegerType(), nullable=True) )\nfor f in [ 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26' ]:\n    listStruct.append( StructField(f, DoubleType(), nullable=True) )\nlistStruct.append( StructField('f_27', StringType(), nullable=True) )\nlistStruct.append( StructField('f_28', DoubleType(), nullable=True) )\nfor i in [ 'f_29', 'f_30' ]:\n    listStruct.append( StructField(i, IntegerType(), nullable=True) )\n\nschema_test = StructType(listStruct)\n\nlistStruct.append( StructField('target', IntegerType(), nullable=True) )\nschema = StructType(listStruct)\n\ndf_train = spark.read.csv('/kaggle/input/tabular-playground-series-may-2022/train.csv', header=True, schema=schema)\ndf_test = spark.read.csv('/kaggle/input/tabular-playground-series-may-2022/test.csv', header=True, schema=schema_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:27.603054Z","iopub.execute_input":"2022-06-01T08:50:27.60363Z","iopub.status.idle":"2022-06-01T08:50:30.413094Z","shell.execute_reply.started":"2022-06-01T08:50:27.603588Z","shell.execute_reply":"2022-06-01T08:50:30.412121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:30.4143Z","iopub.execute_input":"2022-06-01T08:50:30.41459Z","iopub.status.idle":"2022-06-01T08:50:33.437531Z","shell.execute_reply.started":"2022-06-01T08:50:30.414553Z","shell.execute_reply":"2022-06-01T08:50:33.43649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:33.438874Z","iopub.execute_input":"2022-06-01T08:50:33.439239Z","iopub.status.idle":"2022-06-01T08:50:33.464658Z","shell.execute_reply.started":"2022-06-01T08:50:33.439191Z","shell.execute_reply":"2022-06-01T08:50:33.463246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From previous studies, it is known that `f_27` is a string that it is not clear what it means. In the next cells, I tried to see if there are some features I can extract from that variable.","metadata":{}},{"cell_type":"code","source":"df_27 = df_train.groupby('f_27').count().sort('count', ascending=False)\ndf_27.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:33.466946Z","iopub.execute_input":"2022-06-01T08:50:33.46793Z","iopub.status.idle":"2022-06-01T08:50:40.15077Z","shell.execute_reply.started":"2022-06-01T08:50:33.467862Z","shell.execute_reply":"2022-06-01T08:50:40.14962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a \"index\" for each type of string in `f_27`.","metadata":{}},{"cell_type":"code","source":"indexer = StringIndexer( inputCol='f_27', outputCol='f_27_ind' )\nindexed = indexer.fit(df_train).transform(df_train)\nindexed.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:40.152109Z","iopub.execute_input":"2022-06-01T08:50:40.152497Z","iopub.status.idle":"2022-06-01T08:50:56.110322Z","shell.execute_reply.started":"2022-06-01T08:50:40.152448Z","shell.execute_reply":"2022-06-01T08:50:56.109331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create one column per character in the string.","metadata":{}},{"cell_type":"code","source":"split_col = F.split( df_train['f_27'], '')\ndf_tmp = ( df_train.select([ 'f_'+str(i).zfill(2) for i in range(0, 31) ]+['target'] )\n     .withColumn( 'f_27_0', split_col.getItem(0) )\n     .withColumn( 'f_27_1', split_col.getItem(1) )\n     .withColumn( 'f_27_2', split_col.getItem(2) )\n     .withColumn( 'f_27_3', split_col.getItem(3) )\n     .withColumn( 'f_27_4', split_col.getItem(4) )\n     .withColumn( 'f_27_5', split_col.getItem(5) )\n     .withColumn( 'f_27_6', split_col.getItem(6) )\n     .withColumn( 'f_27_7', split_col.getItem(7) )\n     .withColumn( 'f_27_8', split_col.getItem(8) )\n     .withColumn( 'f_27_9', split_col.getItem(9) )\n#      .show() \n)\ndf_tmp.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:56.111641Z","iopub.execute_input":"2022-06-01T08:50:56.111989Z","iopub.status.idle":"2022-06-01T08:50:57.258325Z","shell.execute_reply.started":"2022-06-01T08:50:56.111931Z","shell.execute_reply":"2022-06-01T08:50:57.257343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the number of distintic characters per new columns:","metadata":{}},{"cell_type":"code","source":"for i in range(0, 10):\n    df_tmp.groupby(f'f_27_{i}').count().show()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:57.296659Z","iopub.execute_input":"2022-06-01T08:50:57.300278Z","iopub.status.idle":"2022-06-01T08:51:25.918Z","shell.execute_reply.started":"2022-06-01T08:50:57.30021Z","shell.execute_reply":"2022-06-01T08:51:25.916986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OHE = OneHotEncoder( inputCols=['f_27_0'], outputCols=['f_27_0_OHE'] )\n# OHE = VectorIndexer( inputCol='f_27_0', outputCol='f_27_0_OHE' )\n# OHE.fit(df_tmp)\ntmp_String = StringIndexer( inputCol='f_27_0', outputCol='f_27_0_SI' )\ntmp_String.fit(df_tmp).transform(df_tmp)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:25.919345Z","iopub.execute_input":"2022-06-01T08:51:25.919657Z","iopub.status.idle":"2022-06-01T08:51:28.591554Z","shell.execute_reply.started":"2022-06-01T08:51:25.919616Z","shell.execute_reply":"2022-06-01T08:51:28.58974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make some basic plots:","metadata":{}},{"cell_type":"code","source":"listOfInts = [ f.name for f in df_tmp.schema.fields if isinstance(f.dataType, IntegerType) ]\nlistOfFloats = [ f.name for f in df_tmp.schema.fields if isinstance(f.dataType, FloatType) ]\n\nfor f in listOfInts: df_tmp.select(f).toPandas().hist()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:28.593574Z","iopub.execute_input":"2022-06-01T08:51:28.594265Z","iopub.status.idle":"2022-06-01T08:53:15.611534Z","shell.execute_reply.started":"2022-06-01T08:51:28.594214Z","shell.execute_reply":"2022-06-01T08:53:15.610757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipelines and Classification","metadata":{}},{"cell_type":"markdown","source":"In this part, I want it to check pipelines in `pyspark` for feature extraction and the classification process.","metadata":{}},{"cell_type":"code","source":"train_data, test_data = df_train.randomSplit([.8,.2], seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:53:15.613096Z","iopub.execute_input":"2022-06-01T08:53:15.613862Z","iopub.status.idle":"2022-06-01T08:53:15.709986Z","shell.execute_reply.started":"2022-06-01T08:53:15.613813Z","shell.execute_reply":"2022-06-01T08:53:15.709231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a transformer for the `f_27` column. This is only needed because it is part of the pipeline","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n\nclass splitColumns(Transformer):\n    inputCol = Param(Params._dummy(), \"inputCol\", \"input column name.\", typeConverter=TypeConverters.toString)\n    @keyword_only\n    def __init__(self, inputCol: str = 'input'):\n        super(splitColumns, self).__init__()\n        self._setDefault(inputCol=None)\n        kwargs = self._input_kwargs\n        self.set_params(**kwargs)\n        \n    @keyword_only\n    def set_params(self, inputCol: str = \"input\"):\n        kwargs = self._input_kwargs\n        self._set(**kwargs)\n        \n    def get_input_col(self):\n        return self.getOrDefault(self.inputCol)\n  \n    def _transform(self, df: DataFrame) -> DataFrame:\n        inCol = self.get_input_col()\n        split_col = F.split( df[inCol], '')\n        return ( df.withColumn( f'{inCol}_0', split_col.getItem(0) )\n                 .withColumn( f'{inCol}_1', split_col.getItem(1) )\n                 .withColumn( f'{inCol}_2', split_col.getItem(2) )\n                 .withColumn( f'{inCol}_3', split_col.getItem(3) )\n                 .withColumn( f'{inCol}_4', split_col.getItem(4) )\n                 .withColumn( f'{inCol}_5', split_col.getItem(5) )\n                 .withColumn( f'{inCol}_6', split_col.getItem(6) )\n                 .withColumn( f'{inCol}_7', split_col.getItem(7) )\n                 .withColumn( f'{inCol}_8', split_col.getItem(8) )\n                 .withColumn( f'{inCol}_9', split_col.getItem(9) )\n             )\n\n# sc = splitColumns(inputCol='f_27')\n# sc.transform(df_train).show()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:50:57.263895Z","iopub.execute_input":"2022-06-01T08:50:57.264265Z","iopub.status.idle":"2022-06-01T08:50:57.289542Z","shell.execute_reply.started":"2022-06-01T08:50:57.264222Z","shell.execute_reply":"2022-06-01T08:50:57.288494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a list of features and stages for the pipeline:","metadata":{}},{"cell_type":"code","source":"%%time\nlistOfInts = [ f.name for f in df_tmp.schema.fields if isinstance(f.dataType, IntegerType) ]\nlistOfFloats = [ f.name for f in df_tmp.schema.fields if isinstance(f.dataType, FloatType) ]\nlistOfNumbers = listOfInts + listOfFloats\n\nlistOfStages = []\nlistOfStages += [ VectorAssembler(inputCols=[i], outputCol=f'{i}_vec') for i in listOfNumbers ]\nlistOfStages += [ StandardScaler(inputCol=f'{i}_vec', outputCol=f'{i}_scaled') for i in listOfNumbers ]\n\n# listOfStages += [ splitColumns(inputCol='f_27') ]\n# listOfStages += [ StringIndexer( inputCol='f_27', outputCol='f_27_ind' ) ]\n# listOfStages += [ StringIndexer( inputCol=f'f_27_{i}', outputCol=f'f_27_{i}_ind' ) for i in range(0,10) ]\n# listOfStages += [ VectorAssembler(inputCols=[f'f_27_{i}_ind'], outputCol=f'f_27_{i}_vec') for i in range(0,10) ]\n# listOfStages += [ StandardScaler(inputCol=f'f_27_{i}_vec', outputCol=f'f_27_{i}_scaled') for i in range(0,10) ]\n\nlistFinalFeatures = [ f'{i}_scaled' for i in listOfNumbers ] #+ [ f'f_27_{i}_scaled' for i in range(0,10) ]\nlistOfStages += [ VectorAssembler( inputCols=listFinalFeatures , outputCol='features' ) ]\n# listOfStages += [ VectorAssembler( inputCols=['target'] , outputCol='indexedLabel' ) ]\n\nlistOfStages += [ LogisticRegression( featuresCol='features', labelCol='target' ) ]\n# listOfStages += [ GBTClassifier( featuresCol='features', labelCol='target', maxIter=100 ) ]\n\npipeline = Pipeline( stages=listOfStages )\nmodel = pipeline.fit(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:53:15.710901Z","iopub.execute_input":"2022-06-01T08:53:15.711098Z","iopub.status.idle":"2022-06-01T09:00:32.742844Z","shell.execute_reply.started":"2022-06-01T08:53:15.711072Z","shell.execute_reply":"2022-06-01T09:00:32.741942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_data = model.transform( test_data )\nval_data = model.transform( train_data )\n# val_data.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:00:32.745672Z","iopub.execute_input":"2022-06-01T09:00:32.745991Z","iopub.status.idle":"2022-06-01T09:00:34.252989Z","shell.execute_reply.started":"2022-06-01T09:00:32.745949Z","shell.execute_reply":"2022-06-01T09:00:34.251979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol='target')\n# mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol='target')\nprint( mcEvaluator.evaluate(val_data) )\n# print(\"Area under ROC = %s\" % metrics.areaUnderROC)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:00:34.261384Z","iopub.execute_input":"2022-06-01T09:00:34.261791Z","iopub.status.idle":"2022-06-01T09:01:19.718947Z","shell.execute_reply.started":"2022-06-01T09:00:34.261748Z","shell.execute_reply":"2022-06-01T09:01:19.718061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.withColumn('target', F.lit(0))\nprediction = model.transform( df_test )","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:01:19.720175Z","iopub.execute_input":"2022-06-01T09:01:19.720478Z","iopub.status.idle":"2022-06-01T09:01:21.141956Z","shell.execute_reply.started":"2022-06-01T09:01:19.720435Z","shell.execute_reply":"2022-06-01T09:01:21.140888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred = prediction.select('id', 'prediction').toPandas()\ndf_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:01:21.143016Z","iopub.execute_input":"2022-06-01T09:01:21.143469Z","iopub.status.idle":"2022-06-01T09:02:00.575306Z","shell.execute_reply.started":"2022-06-01T09:01:21.143431Z","shell.execute_reply":"2022-06-01T09:02:00.574333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred.rename(columns={'prediction':'target'}, inplace=True )\ndf_pred.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:02:00.576703Z","iopub.execute_input":"2022-06-01T09:02:00.57708Z","iopub.status.idle":"2022-06-01T09:02:02.042202Z","shell.execute_reply.started":"2022-06-01T09:02:00.577049Z","shell.execute_reply":"2022-06-01T09:02:02.041077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}