{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - May 2022\n\nI fit an XGBoost (Regressor) model. Feature engineering includes separation of [f_27 string feature](https://www.kaggle.com/code/nnjjpp/eda-may-2022-exploring-the-string-feature-f-27), and feature interactions taken from https://www.kaggle.com/competitions/tabular-playground-series-may-2022/discussion/323892. XGBoost hyperparameters tuned using RandomizedSearchCV. To speed things up I use the GPU accelerator with the `tree_method = 'gpu_hist'` option in the XGBRegressor constructor.","metadata":{}},{"cell_type":"code","source":"import itertools as it\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Notebook parameters\nN_ESTIMATORS_TUNING = 120\nN_ESTIMATORS_FITTING = 1500\nN_PARAMETER_SAMPLES = 20\nPARAMETER_SPLIT = 3\nXGB_TREE_METHOD ='gpu_hist'#'auto'#\nCV_RANDOM_STATE = 123\n\n# Read in data\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv', index_col='id')\nsummary_stats = train.describe().T\nsummary_stats\ntrain.head()\n\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv', index_col='id')\ntest_summary_stats = test.describe().T\ntest.head()\n#test_summary_stats\n\nN = train.shape[0] + test.shape[0]\nNtrain = train.shape[0]\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T00:29:22.331998Z","iopub.execute_input":"2022-05-31T00:29:22.332347Z","iopub.status.idle":"2022-05-31T00:29:43.499301Z","shell.execute_reply.started":"2022-05-31T00:29:22.332262Z","shell.execute_reply":"2022-05-31T00:29:43.498292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train['target']\ntrain = train.drop('target', axis=1)\n\nboth = pd.concat([train, test])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T00:29:48.256039Z","iopub.execute_input":"2022-05-31T00:29:48.256353Z","iopub.status.idle":"2022-05-31T00:29:48.678082Z","shell.execute_reply.started":"2022-05-31T00:29:48.256319Z","shell.execute_reply":"2022-05-31T00:29:48.676596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n## split f_27 variable","metadata":{}},{"cell_type":"code","source":"both['f_27'].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T00:29:51.538141Z","iopub.execute_input":"2022-05-31T00:29:51.538497Z","iopub.status.idle":"2022-05-31T00:29:51.551018Z","shell.execute_reply.started":"2022-05-31T00:29:51.538461Z","shell.execute_reply":"2022-05-31T00:29:51.549284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f27 = both['f_27']\nboth = both.drop('f_27', axis=1)\nprint(f27)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T00:29:53.471693Z","iopub.execute_input":"2022-05-31T00:29:53.47208Z","iopub.status.idle":"2022-05-31T00:29:53.963395Z","shell.execute_reply.started":"2022-05-31T00:29:53.472041Z","shell.execute_reply":"2022-05-31T00:29:53.962329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f27.head().apply(lambda x: pd.Series(list(x)))\n#both[[f'f_27_{i} for i in range(10)']] = f27.apply(lambda x: pd.Series(list(x)))\nboth[[f'f_27_{i}' for i in range(10)]] = pd.DataFrame([list(x) for x in f27])\nboth.head()\nordA = ord('A')\nfor i in range(10):\n    lab_i = f'f_27_{i}'\n    both[lab_i] = both[lab_i].map(lambda x: ord(x) - ordA)\n\nboth.head()\n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T00:29:56.643947Z","iopub.execute_input":"2022-05-31T00:29:56.644241Z","iopub.status.idle":"2022-05-31T00:30:10.985604Z","shell.execute_reply.started":"2022-05-31T00:29:56.644207Z","shell.execute_reply":"2022-05-31T00:30:10.984237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unique string values","metadata":{}},{"cell_type":"code","source":"both['unique_vals_f_27'] = f27.map(lambda x: len(np.unique(list(x))))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T00:30:22.091131Z","iopub.execute_input":"2022-05-31T00:30:22.091471Z","iopub.status.idle":"2022-05-31T00:30:47.325002Z","shell.execute_reply.started":"2022-05-31T00:30:22.091423Z","shell.execute_reply":"2022-05-31T00:30:47.324015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interactions\nSee https://www.kaggle.com/competitions/tabular-playground-series-may-2022/discussion/323892","metadata":{}},{"cell_type":"code","source":"both['i_02_21'] = (both.f_21 + both.f_02 > 5.2).astype(int) - \\\n                  (both.f_21 + both.f_02 < -5.3).astype(int)\nboth['i_05_22'] = (both.f_22 + both.f_05 > 5.1).astype(int) - \\\n                  (both.f_22 + both.f_05 < -5.4).astype(int)\ni_00_01_26 = both.f_00 + both.f_01 + both.f_26\nboth['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - \\\n                   (i_00_01_26 < -5.0).astype(int)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"both.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T23:59:07.624769Z","iopub.execute_input":"2022-05-30T23:59:07.625062Z","iopub.status.idle":"2022-05-30T23:59:07.653252Z","shell.execute_reply.started":"2022-05-30T23:59:07.625013Z","shell.execute_reply":"2022-05-30T23:59:07.652373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"both.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain = both.iloc[:Ntrain,:]\nXtest = both.iloc[Ntrain:,:]\nytrain = target","metadata":{"execution":{"iopub.status.busy":"2022-05-30T23:59:20.431761Z","iopub.execute_input":"2022-05-30T23:59:20.432045Z","iopub.status.idle":"2022-05-30T23:59:20.437015Z","shell.execute_reply.started":"2022-05-30T23:59:20.432007Z","shell.execute_reply":"2022-05-30T23:59:20.436175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model fitting - XGBoost\n\n## Explore effect of number of estimators ","metadata":{}},{"cell_type":"code","source":"%%time \n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import cross_validate, ShuffleSplit#train_test_split\n\nfrom sklearn.metrics import roc_auc_score, make_scorer\n\nX = [5,10,20,50,100]\ncv_auc = []\nfor N_ESTIMATORS in X:\n    print(N_ESTIMATORS)\n    xgb = XGBRegressor(n_estimators = N_ESTIMATORS,\n                       objective = 'binary:logistic',\n                       eval_metric = 'auc',\n                      tree_method = XGB_TREE_METHOD)\n\n\n    splitter = ShuffleSplit(n_splits=1, random_state=1, test_size=0.15) # equivalent to train_test_split\n\n    cv = cross_validate(xgb, Xtrain, ytrain, n_jobs=-1, scoring = make_scorer(roc_auc_score),\n                       cv = splitter)\n    cv_auc.append(cv['test_score'])\nimport matplotlib.pyplot as plt\nplt.plot(X, cv_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning\nI look at gamma, eta, subsample and min_child_weight. Search one at a time using RandomizedSearchCV.","metadata":{}},{"cell_type":"code","source":"\n%%time \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nparams = {'gamma':[0,50],\n          'eta':[0,1],\n          'subsample':[0,1],\n          'min_child_weight':[0,50]}\ndefaults = {'gamma':0,\n          'eta':0.3,\n          'subsample':1,\n          'min_child_weight':1}\nfinal = {}\nplt.figure(figsize=(12,3))\nfor i, pname in enumerate(params.keys()):\n    print('\\n',pname)\n    model = XGBRegressor(n_estimators = N_ESTIMATORS_TUNING,\n                           objective = 'binary:logistic',\n                           eval_metric = 'auc',\n                        tree_method = XGB_TREE_METHOD)\n    rcv = RandomizedSearchCV(estimator = model, \n                            param_distributions = {pname: uniform(*params[pname])},\n                            n_iter=N_PARAMETER_SAMPLES,\n                            verbose=0,#99,\n                            n_jobs=-1,\n                            scoring = make_scorer(roc_auc_score),\n                            cv = PARAMETER_SPLIT,\n                            random_state = CV_RANDOM_STATE)\n    rcv.fit(Xtrain, ytrain)\n\n    # Plot the results:\n    plt.subplot(1,len(params),i+1)\n    plt.plot([x[pname] for x in rcv.cv_results_['params']], rcv.cv_results_['mean_test_score'],'o')\n    plt.title(pname)\n    g = plt.gca()\n    x = np.arange(20)/19 * (params[pname][1] - params[pname][0]) + params[pname][0]\n    xm = np.column_stack([pow(x, i) for i in range(2,-1,-1)])\n\n    # Fit a quadratic to the cv results and find maximum value:\n    p = np.polyfit([x[pname] for x in rcv.cv_results_['params']], rcv.cv_results_['mean_test_score'], 2)\n    plt.plot(x, np.dot(xm, p))\n    g.set_ylim([0.7,1.0])\n    def f(z, pp):\n        return pp[0] * z**2 + p[1] * z + p[2]\n    try: \n        \n        # y = p[0]x**2 + p[1]x + p[2]\n        # dy/dx = 2p[0]x + p[1]\n        # max/min occurs at dy/dx = 0:\n        # mx_x = -p[1]/2/p[0]\n        mx_x0 = -p[1]/2/p[0]\n        # d2y/dx2 = 2p[0]\n\n        # If maximum of f occurs outside search range,\n        # just set it equal to one of the edges:\n        mx_x = max(params[pname][0], mx_x0)\n        mx_x = min(params[pname][1], mx_x)\n        #\n        # Check edges of parameter search range:\n        if f(params[pname][0], p) > f(mx_x, p):\n            mx_x = params[pname[0]]\n        if f(params[pname][1], p) > f(mx_x, p):\n            mx_x = params[pname[1]]\n    except:\n        print(\"  Default used (polyfit didn't work)\")\n        mx_x = defaults[pname]\n    print(f'  Max value: {mx_x:.3f}')\n    final[pname] = mx_x\n\nprint('Final parameters:')\nprint(final)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T01:22:29.953718Z","iopub.execute_input":"2022-05-06T01:22:29.954661Z","iopub.status.idle":"2022-05-06T01:26:34.152731Z","shell.execute_reply.started":"2022-05-06T01:22:29.954613Z","shell.execute_reply":"2022-05-06T01:26:34.151722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit model with optimal parameters on all data with more estimators","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor(n_estimators = N_ESTIMATORS_FITTING,\n                   objective = 'binary:logistic',\n                   eval_metric = 'auc',\n                   gamma = final['gamma'],\n                   eta = final['eta'],\n                   subsample = final['subsample'],\n                   min_child_weight = final['min_child_weight'],\n                   tree_method = XGB_TREE_METHOD)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T23:21:42.474501Z","iopub.execute_input":"2022-05-05T23:21:42.475449Z","iopub.status.idle":"2022-05-05T23:21:42.707179Z","shell.execute_reply.started":"2022-05-05T23:21:42.475397Z","shell.execute_reply":"2022-05-05T23:21:42.706317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nxgb.fit(Xtrain, ytrain)\nypred = xgb.predict(Xtest)\nprint(ypred)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T22:41:32.996664Z","iopub.execute_input":"2022-05-05T22:41:32.996906Z","iopub.status.idle":"2022-05-05T22:41:40.690328Z","shell.execute_reply.started":"2022-05-05T22:41:32.996876Z","shell.execute_reply":"2022-05-05T22:41:40.689725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nytrain_predict = xgb.predict(Xtrain)\nplt.plot(ytrain, ytrain_predict,'o')\n\n\nprint(f'ROC score on training data = {roc_auc_score(ytrain, ytrain_predict):.3f}')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T22:41:40.691678Z","iopub.execute_input":"2022-05-05T22:41:40.692156Z","iopub.status.idle":"2022-05-05T22:41:44.249926Z","shell.execute_reply.started":"2022-05-05T22:41:40.692121Z","shell.execute_reply":"2022-05-05T22:41:44.248925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(ytrain_predict - ytrain)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T22:41:44.251031Z","iopub.status.idle":"2022-05-05T22:41:44.251623Z","shell.execute_reply.started":"2022-05-05T22:41:44.251383Z","shell.execute_reply":"2022-05-05T22:41:44.251416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainerr = ytrain_predict - ytrain\n\nplt.figure(figsize=(25,15))\nfor i in range(40):\n    plt.subplot(5,8,i+1)\n    plt.plot(trainerr,Xtrain.iloc[:,i],'o')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T22:41:44.252646Z","iopub.status.idle":"2022-05-05T22:41:44.252985Z","shell.execute_reply.started":"2022-05-05T22:41:44.252808Z","shell.execute_reply":"2022-05-05T22:41:44.252832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function courtesy of Tyrion Lannister-lzy:\n# https://www.kaggle.com/code/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980\n\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' feature importance plot')\n    plt.xlabel('IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(xgb.feature_importances_,\n                        Xtrain.columns,\n                        f'XGBoost Regressor, {N_ESTIMATORS_FITTING} estimators', max_features = 25)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test.index,\n                           'target': ypred})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T22:41:44.254178Z","iopub.status.idle":"2022-05-05T22:41:44.254535Z","shell.execute_reply.started":"2022-05-05T22:41:44.254371Z","shell.execute_reply":"2022-05-05T22:41:44.254389Z"},"trusted":true},"execution_count":null,"outputs":[]}]}