{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aim of this notebook is to explore this dataset in much simple and clean way.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T12:54:37.089705Z","iopub.execute_input":"2022-05-03T12:54:37.090386Z","iopub.status.idle":"2022-05-03T12:54:37.097775Z","shell.execute_reply.started":"2022-05-03T12:54:37.090347Z","shell.execute_reply":"2022-05-03T12:54:37.096853Z"}}},{"cell_type":"markdown","source":"# **Import**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom statsmodels.graphics.gofplots import qqplot\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nsns.set(style = \"darkgrid\")\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\n\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:23.906592Z","iopub.execute_input":"2022-05-27T14:21:23.906862Z","iopub.status.idle":"2022-05-27T14:21:23.917305Z","shell.execute_reply.started":"2022-05-27T14:21:23.906832Z","shell.execute_reply":"2022-05-27T14:21:23.915973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Read Dataset**","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")\ntrain = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\")\nprint(\"Sample\")\ndisplay(sample.head(2))\nprint()\nprint(\"Train\")\ndisplay(train.head(2))\nprint()\nprint(\"Test\")\ndisplay(test.head(2))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:23.943241Z","iopub.execute_input":"2022-05-27T14:21:23.943603Z","iopub.status.idle":"2022-05-27T14:21:31.101811Z","shell.execute_reply.started":"2022-05-27T14:21:23.943557Z","shell.execute_reply":"2022-05-27T14:21:31.101036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Overall view of dataset**\n* train 900000 rows, test 700000 rows\n* no nan values in our datasets\n* 31 features\n* f_00 - f_06 and f_19 - f_26 and f_28 := float columns\n* f_07 - f_18 and f_29 - f_30 := int columns\n* f_27 := object column\n* target columns := binary (0/1) and target is almost balanced #0: 462161 and #1: 437839","metadata":{}},{"cell_type":"code","source":"print(\"Sample, train, test\")\nprint(sample.shape, train.shape, test.shape)\nprint()\nprint(\"No of null values\")\nprint(sample.isnull().sum().sum(), train.isnull().sum().sum(), test.isnull().sum().sum())\nprint()\nfeatures = test.drop(\"id\", axis=1).columns.tolist()\nprint(features)\nprint()\nprint(train.info())","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-27T14:21:31.10356Z","iopub.execute_input":"2022-05-27T14:21:31.103891Z","iopub.status.idle":"2022-05-27T14:21:31.510562Z","shell.execute_reply.started":"2022-05-27T14:21:31.103855Z","shell.execute_reply":"2022-05-27T14:21:31.509511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.target.value_counts())\nsns.countplot(x=train['target'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:21:31.512177Z","iopub.execute_input":"2022-05-27T14:21:31.512548Z","iopub.status.idle":"2022-05-27T14:21:31.718988Z","shell.execute_reply.started":"2022-05-27T14:21:31.512505Z","shell.execute_reply":"2022-05-27T14:21:31.717972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Here we look at no of unique value in each columns**\n\n\n* 700000+900000 = 1600000 [train+test]\n* all float columns has different values in each row <br>\n* all int columns has total at most 17 different unique values ( so these are some sort of categorical variables))<br>\n* f_27 which is an object column(as has string entry) has total 1181880 unique values 160000-1181880=418120 repetitions\n* In int columns there are many unique values whose frequency is less than 1%(see below). We can combine them to create new feature.","metadata":{}},{"cell_type":"code","source":"full_data = pd.concat([train[features],test[features]], axis=0)\nprint(full_data.shape)\nprint()\nlist(zip(full_data.columns, full_data.dtypes, full_data.nunique()))","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-27T14:21:31.721523Z","iopub.execute_input":"2022-05-27T14:21:31.722509Z","iopub.status.idle":"2022-05-27T14:21:33.772258Z","shell.execute_reply.started":"2022-05-27T14:21:31.722463Z","shell.execute_reply":"2022-05-27T14:21:33.771516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = [i for i in features if full_data[i].nunique() <= 17]\nnum_features = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26', 'f_28']\nprint(\"features with no of unique values less than equal to 17\")\nprint(cat_features)\nprint()\nprint(\"% of unique values\")\nfor feat in cat_features:\n    print(feat,\":\")\n    a = full_data[feat].value_counts()*100/full_data.shape[0]\n    print(a)\n    print(\"=\"*40)\n    print()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:33.773601Z","iopub.execute_input":"2022-05-27T14:21:33.77403Z","iopub.status.idle":"2022-05-27T14:21:36.057788Z","shell.execute_reply.started":"2022-05-27T14:21:33.773991Z","shell.execute_reply":"2022-05-27T14:21:36.056933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"f_27 :\")\nprint(full_data.f_27.value_counts())","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-27T14:21:36.059751Z","iopub.execute_input":"2022-05-27T14:21:36.060314Z","iopub.status.idle":"2022-05-27T14:21:37.251748Z","shell.execute_reply.started":"2022-05-27T14:21:36.060274Z","shell.execute_reply":"2022-05-27T14:21:37.251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **f_27**\n**The f_27 column contains string of length 10 characters, Let's try to explore these encoding.**\n\nWe first created a new dataframe from f_27 by splitting these strings into 10 columns of each characters.<br>\nWe notice following things of this encoding:\n* f0, f2, f5 : contains only two characters A,B  (can be used to create new features)\n* f1, f3, f4, f6, f8, f9: all contains characters from A to O \n* f7: contains charactes from A to T\n* f1, f3, f4, f6, f8, f9 : all has same distribution of characters \n* except f7 which has almost same frequency of each character","metadata":{}},{"cell_type":"code","source":"data_f_27 = pd.DataFrame([list(i) for i in sorted(full_data.f_27.value_counts().index.values)])\ndata_f_27.columns = [\"f0\",\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\"]\ndata_f_27.head(3)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:37.253124Z","iopub.execute_input":"2022-05-27T14:21:37.253519Z","iopub.status.idle":"2022-05-27T14:21:41.935319Z","shell.execute_reply.started":"2022-05-27T14:21:37.253483Z","shell.execute_reply":"2022-05-27T14:21:41.934629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print(data_f_27.groupby([f\"f{i}\"]).count().iloc[:,0])\n    print(\"=\"*40)\n    print()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:41.936528Z","iopub.execute_input":"2022-05-27T14:21:41.936851Z","iopub.status.idle":"2022-05-27T14:21:53.247113Z","shell.execute_reply.started":"2022-05-27T14:21:41.936817Z","shell.execute_reply":"2022-05-27T14:21:53.246318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* f0 and f5 have very similar distribution while f2 has just opposite distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nfor i in [0,2,5]:\n    d= data_f_27[f\"f{i}\"].value_counts()\n    plt.plot(d,label=f\"f{i}\")\nplt.legend()\nplt.show()\nplt.figure(figsize=(12,6))\nfor i in [1,3,4,6,7,8,9]:\n    d= data_f_27[f\"f{i}\"].value_counts()\n    plt.plot(d, label=f\"f{i}\")\nplt.legend()\nplt.show()","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:53.248484Z","iopub.execute_input":"2022-05-27T14:21:53.2489Z","iopub.status.idle":"2022-05-27T14:21:54.867692Z","shell.execute_reply.started":"2022-05-27T14:21:53.248863Z","shell.execute_reply":"2022-05-27T14:21:54.867011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<code>px.treemap()</code> is used to visualize proportions for multiple columns at at time.","metadata":{}},{"cell_type":"code","source":"fig  = px.treemap(data_f_27.sample(20), path= data_f_27.columns.tolist() ) \nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:54.870811Z","iopub.execute_input":"2022-05-27T14:21:54.871042Z","iopub.status.idle":"2022-05-27T14:21:55.985122Z","shell.execute_reply.started":"2022-05-27T14:21:54.871014Z","shell.execute_reply":"2022-05-27T14:21:55.984439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Parallel Sets represents contribution of columns on each other. <br>\n* It is used to represent inter-connection among columns.<br>\n* Note: It works only for Object and int data type columns.<br>\n* We can set color value based on a column which can be int/float type.\n> We have created two plots:-\n1. In first plot we have taken full_data i.e. train+test \n1. In second plot we have taken only train data with target as color","metadata":{}},{"cell_type":"code","source":"px.parallel_categories(data_f_27.sample(200)) # train+test","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:21:55.98649Z","iopub.execute_input":"2022-05-27T14:21:55.98675Z","iopub.status.idle":"2022-05-27T14:21:56.08746Z","shell.execute_reply.started":"2022-05-27T14:21:55.986715Z","shell.execute_reply":"2022-05-27T14:21:56.086696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_f_27= pd.DataFrame([list(i) for i in train.f_27.value_counts().index.values])\ntrain_f_27.columns = [\"f0\",\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\"]\ntrain_f_27[\"target\"] = train.target\ntrain_f_27.head(3)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:56.088886Z","iopub.execute_input":"2022-05-27T14:21:56.089128Z","iopub.status.idle":"2022-05-27T14:21:57.865437Z","shell.execute_reply.started":"2022-05-27T14:21:56.089097Z","shell.execute_reply":"2022-05-27T14:21:57.864698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.parallel_categories(train_f_27.head(800),color=\"target\") # train","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:21:57.86659Z","iopub.execute_input":"2022-05-27T14:21:57.867173Z","iopub.status.idle":"2022-05-27T14:21:57.981708Z","shell.execute_reply.started":"2022-05-27T14:21:57.86713Z","shell.execute_reply":"2022-05-27T14:21:57.981029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# features = cat_features + num_features + f_27\n* cat_features := 14\n* num_features := 16\n* f_27","metadata":{}},{"cell_type":"code","source":"display(full_data[cat_features].head(2))\n\ndisplay(full_data[num_features].head(2))\n\ndisplay(full_data[[\"f_27\"]].head(2))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:21:57.98302Z","iopub.execute_input":"2022-05-27T14:21:57.983425Z","iopub.status.idle":"2022-05-27T14:21:58.361486Z","shell.execute_reply.started":"2022-05-27T14:21:57.983393Z","shell.execute_reply":"2022-05-27T14:21:58.360643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SHAP\nMost of the ideas below are inspired from @ambrosm and @wti200 work.\nLink of the notebooks: \n\nhttps://www.kaggle.com/code/wti200/analysing-interactions-with-shap\n\nhttps://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense/notebook","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/ambrosm/tpsmay22-eda-which-makes-sense\nfor df in [train, test]:\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:21:58.362928Z","iopub.execute_input":"2022-05-27T14:21:58.363187Z","iopub.status.idle":"2022-05-27T14:22:16.992247Z","shell.execute_reply.started":"2022-05-27T14:21:58.363152Z","shell.execute_reply":"2022-05-27T14:22:16.991499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:16.993514Z","iopub.execute_input":"2022-05-27T14:22:16.993745Z","iopub.status.idle":"2022-05-27T14:22:17.021956Z","shell.execute_reply.started":"2022-05-27T14:22:16.993714Z","shell.execute_reply":"2022-05-27T14:22:17.021295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train.columns if col != \"target\" and col !=\"f_27\" and col != \"id\"]\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:17.023078Z","iopub.execute_input":"2022-05-27T14:22:17.02381Z","iopub.status.idle":"2022-05-27T14:22:17.029005Z","shell.execute_reply.started":"2022-05-27T14:22:17.023775Z","shell.execute_reply":"2022-05-27T14:22:17.028315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:17.030342Z","iopub.execute_input":"2022-05-27T14:22:17.031167Z","iopub.status.idle":"2022-05-27T14:22:17.038507Z","shell.execute_reply.started":"2022-05-27T14:22:17.031115Z","shell.execute_reply":"2022-05-27T14:22:17.037656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we have to create shap_interaction matrix. Since this takes much time so we will take it other notebook. Thanks to @wti200 for making it public :-))\n\n# xtr = train[features]\n# xte = test[features]\n# X_train, X_val, y_train, y_val = train_test_split(xtr,train.target, test_size=0.4, random_state = 42)\n# print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n\n# from lightgbm import LGBMClassifier\n# from sklearn.metrics import roc_auc_score\n# # Train model\n# lgbm_model =LGBMClassifier(n_estimators=5000, min_child_samples=80, random_state=1307)\n# lgbm_model.fit(X_train.values, y_train)\n# y_val_pred = lgbm_model.predict_proba(X_val.values)[:,1]\n# score = roc_auc_score(y_val, y_val_pred)\n# print(f\"Validation AUC:{(score):.3f}\")\n\n# # Using a random sample of the dataframe for better time computation\n# X_sampled = X_val.sample(20000, random_state=1307)\n# X_sampled.shape\n\n# # explain the model's predictions using SHAP values\n# explainer = shap.TreeExplainer(lgbm_model)\n# shap_values = explainer.shap_values(X_sampled)\n# print(shap_values[0].shape, shap_values[1].shape, len(shap_values))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:17.039934Z","iopub.execute_input":"2022-05-27T14:22:17.040558Z","iopub.status.idle":"2022-05-27T14:22:17.048568Z","shell.execute_reply.started":"2022-05-27T14:22:17.04052Z","shell.execute_reply":"2022-05-27T14:22:17.04781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get SHAP interaction values. Beware it is time consuming to calculate the interaction values.\n# shap_interaction = explainer.shap_interaction_values(X_sampled)\n# print(np.shape(shap_interaction))\n\nloaded_arr = np.loadtxt('../input/tpsmay-shap/shap_interaction_20k.txt')\nload_original_arr = loaded_arr.reshape(\n    #loaded_arr.shape[0], loaded_arr.shape[1] // shap_interaction.shape[2], shap_interaction.shape[2])\n    loaded_arr.shape[0], loaded_arr.shape[1] // 41, 41)\n\nshap_interaction = load_original_arr\nprint(np.shape(shap_interaction))\n\n# SHAP values are used to explain individual predictions made by a model.\nprint(shap_interaction[0].shape)\n\nmean_shap = np.abs(shap_interaction).mean(0)\nprint(mean_shap.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:17.051284Z","iopub.execute_input":"2022-05-27T14:22:17.051477Z","iopub.status.idle":"2022-05-27T14:22:53.76917Z","shell.execute_reply.started":"2022-05-27T14:22:17.051453Z","shell.execute_reply":"2022-05-27T14:22:53.763355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(shap_interaction[0])\nplt.show()\nsns.heatmap(mean_shap)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:53.770516Z","iopub.execute_input":"2022-05-27T14:22:53.770809Z","iopub.status.idle":"2022-05-27T14:22:54.70146Z","shell.execute_reply.started":"2022-05-27T14:22:53.770773Z","shell.execute_reply":"2022-05-27T14:22:54.699401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(mean_shap, index=features, columns=features)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:54.702754Z","iopub.execute_input":"2022-05-27T14:22:54.703676Z","iopub.status.idle":"2022-05-27T14:22:54.742535Z","shell.execute_reply.started":"2022-05-27T14:22:54.703636Z","shell.execute_reply":"2022-05-27T14:22:54.741376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10 ))\nsns.heatmap(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:54.744085Z","iopub.execute_input":"2022-05-27T14:22:54.744431Z","iopub.status.idle":"2022-05-27T14:22:55.871297Z","shell.execute_reply.started":"2022-05-27T14:22:54.744399Z","shell.execute_reply":"2022-05-27T14:22:55.870034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.heatmap(df.where(df.values == np.diagonal(df)))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:55.872656Z","iopub.execute_input":"2022-05-27T14:22:55.873396Z","iopub.status.idle":"2022-05-27T14:22:56.757134Z","shell.execute_reply.started":"2022-05-27T14:22:55.87336Z","shell.execute_reply":"2022-05-27T14:22:56.756442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndf.where( cond, 3) := fill by 3 where cond is FALSE\ndf.mask(cond, 3) := fill by 3 where cond is TRUE\n\"\"\"\ndf1= df.where(df.values == np.diagonal(df), 2*df.values) # increase all non diagonal elements by factor of 2\nfig= plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\nsns.heatmap(df1.round(decimals=3), cmap=\"coolwarm\", annot=True, fmt=\".6g\", cbar=False, ax=ax)\nax.tick_params(axis='x', colors='w', labelsize=15, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=15)\n\nplt.suptitle(\"SHAP interaction values\", color=\"white\", fontsize=60, y=0.97)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:22:56.758236Z","iopub.execute_input":"2022-05-27T14:22:56.759912Z","iopub.status.idle":"2022-05-27T14:23:02.240737Z","shell.execute_reply.started":"2022-05-27T14:22:56.759872Z","shell.execute_reply":"2022-05-27T14:23:02.239958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# threshold\n\ndf1= df.where(df.values == np.diagonal(df), 2*df.values)\n\ndf1 = df1.where(df1.values >= .4)\nfig= plt.figure(figsize=(30, 30), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\nsns.heatmap(df1.round(decimals=2), cmap=\"coolwarm\", annot=True, fmt=\".6g\", cbar=False, ax=ax, linewidths=.2, mask= np.triu(df1)) # triu: tril\nax.tick_params(axis='x', colors='w', labelsize=15, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=15)\n\nplt.suptitle(\"SHAP interaction values\", color=\"white\", fontsize=60, y=0.97)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:27:18.050662Z","iopub.execute_input":"2022-05-27T14:27:18.051146Z","iopub.status.idle":"2022-05-27T14:27:18.98313Z","shell.execute_reply.started":"2022-05-27T14:27:18.051107Z","shell.execute_reply":"2022-05-27T14:27:18.981751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> f_00 and f_26\n\n> f_01 and f_26\n\n> f_02 and f_21 \n\n> f_05 and f_22 \n\n> f_24 adn f_30 \n\n> f_25 and f_30 \n\n> f_26 and f_30\n\nWe will further verify these interaction.","metadata":{}},{"cell_type":"code","source":"interaction_cols= [\"f_00\", \"f_01\", \"f_02\", \"f_05\", \"f_21\",\"f_22\", \"f_24\", \"f_25\" ,\"f_26\", \"f_30\"]\ninteraction_cols","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:27:18.984806Z","iopub.execute_input":"2022-05-27T14:27:18.985274Z","iopub.status.idle":"2022-05-27T14:27:18.99139Z","shell.execute_reply.started":"2022-05-27T14:27:18.985237Z","shell.execute_reply":"2022-05-27T14:27:18.990748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(interaction_cols)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:27:18.992651Z","iopub.execute_input":"2022-05-27T14:27:18.993105Z","iopub.status.idle":"2022-05-27T14:27:19.003003Z","shell.execute_reply.started":"2022-05-27T14:27:18.993071Z","shell.execute_reply":"2022-05-27T14:27:19.002093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We manually check if there is any relation between features\nfig,axes  = plt.subplots(9,5,figsize=(30,30))\nflatten_axes = axes.flatten()\ncounter = 0\nfor i in range(len(interaction_cols)):\n    for j in range(len(interaction_cols)):\n        if i>j:\n            c1 = interaction_cols[i]\n            c2 = interaction_cols[j]\n            cmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n            flatten_axes[counter].scatter(train[c1], train[c2], s=1,c=train.target, cmap=cmap,)\n            flatten_axes[counter].set_xlabel(c1)\n            flatten_axes[counter].set_ylabel(c2)\n            #flatten_axes[counter].set_aspect('equal')\n            counter += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:27:19.004998Z","iopub.execute_input":"2022-05-27T14:27:19.005487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finds all possible selection possible\ncol_list = [\"f_00\",\"f_01\",\"f_02\",\"f_05\"]\nnew_list = []\nfor i in range(16):\n    val = int(bin(i)[2:])\n    temp_list = []\n    counter = 1\n    while val != 0:\n        if val %10 == 1:\n            # take it\n            temp_list.append(col_list[-counter])\n        val = val//10 \n        counter += 1\n    new_list.append(temp_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_list, len(new_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ls in new_list:\n    if len(ls) == 0:\n        continue \n    print(ls)\n    val= train[ls].sum(axis=1).values\n    cmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n    plt.scatter(val ,train['f_21'], s=1,c=train.target, cmap=cmap)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\nplt.scatter(train['f_00'] + train['f_01'] ,train['f_26'], s=1,c=train.target, cmap=cmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Althoug f_02 and f_21 shows high correlation but other featues like f_01, f_05, f_00 also show significant correlation with f_21 :\n### We will create features and test models to see which works for us.\n\n> f_00 + f_01  f_26 \n\n> f_05  f_22 \n\n> f_02 f_21 \n\n> ---------\n\n> f_00 f_21 \n\n> f_01 f_21 \n\n> f_05 f_21","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [train, test]:\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature correlation\n* correlation between numerical features <b>[ there is some correlation between (f_28, f_2) (f_28, f_3) (f_28, f_5) and (f_25, f_23) ]</b>\n* correlation between categorical features <b>[ there is no correlation among categorical features ]</b>","metadata":{}},{"cell_type":"code","source":"feat = num_features \nfig, ax = plt.subplots(1,2,figsize=(32,11))         # Sample figsize in inches\nax[0].title.set_text(\"train\")\nax[1].title.set_text(\"test\")\nsns.heatmap(train[feat + ['target']].corr().abs(), cmap=\"viridis\", linewidths=.5, ax=ax[0], annot=True, fmt=\".2f\")\nsns.heatmap(test[feat].corr().abs(), cmap=\"viridis\",linewidths=.5, ax=ax[1], annot=True, fmt=\".2f\")\nplt.show()\n\n## threshold of .2\nfig, ax = plt.subplots(1,2,figsize=(32,11))         # Sample figsize in inches\nax[0].title.set_text(\"train\")\nax[1].title.set_text(\"test\")\nsns.heatmap(train[feat+ ['target']].corr().abs()>.2, cmap=\"coolwarm\", linewidths=.5, ax=ax[0],annot=True, fmt=\".2f\")\nsns.heatmap(test[feat].corr().abs()>.2, cmap=\"coolwarm\",linewidths=.5, ax=ax[1],annot=True, fmt=\".2f\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = cat_features \nfig, ax = plt.subplots(1,2,figsize=(32,11))         \nax[0].title.set_text(\"train\")\nax[1].title.set_text(\"test\")\nsns.heatmap(train[feat+ ['target']].corr().abs(), cmap=\"viridis\", linewidths=.5, ax=ax[0], annot=True, fmt=\".2f\")\nsns.heatmap(test[feat].corr().abs(), cmap=\"viridis\",linewidths=.5, ax=ax[1], annot=True, fmt=\".2f\")\nplt.show()\n## threshold of .2\nfig, ax = plt.subplots(1,2,figsize=(32,11))         \nax[0].title.set_text(\"train\")\nax[1].title.set_text(\"test\")\nsns.heatmap(train[feat+ ['target']].corr().abs()>.2, cmap=\"coolwarm\", linewidths=.5, ax=ax[0],annot=True, fmt=\".2f\")\nsns.heatmap(test[feat].corr().abs()>.2, cmap=\"coolwarm\",linewidths=.5, ax=ax[1],annot=True, fmt=\".2f\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Drawback: It only shows linear dependency between target and other features.\n\n> No feature is Strongly correlated with target.\n\nIf we plot a rolling mean of the target probability for every feature, we'll see nonlinear dependences as well. A horizontal line means that the target does not depend on the feature (e.g., f_03, f_04, f_06), a line with low minimum and high maximum shows a high mutual information between feature and target (e.g., f_19, f_21, f_28). Credit: @ambrosm\n\nProceidure: \nFor each feature, we sort target by feature value. \nThen we take rolling mean of target and assign it to corresponding data point. \nThen we scatter plot feature vs rolling mean ","metadata":{}},{"cell_type":"code","source":"from cycler import cycler\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dependence between every feature and the target\ndef plot_mutual_info_diagram(df, features, ncols=4, by_quantile=True, mutual_info=True,\n                             title='How the target probability depends on single features'):\n    def H(p):\n        \"\"\"Entropy of a binary random variable in nat\"\"\"\n        # Entropy means randomness\n        return -np.log(p) * p - np.log(1-p) * (1-p)\n                 \n    nrows = (len(features) + ncols - 1) // ncols\n    fig, axs = plt.subplots(nrows, ncols, figsize=(16, nrows*4), sharey=True)\n    for f, ax in zip(features, axs.ravel()):\n        temp = pd.DataFrame({f: df[f].values,\n                             'state': df.target.values})\n        temp = temp.sort_values(f)\n        temp.reset_index(inplace=True)\n        rolling_mean = temp.state.rolling(15000, center=True, min_periods=1).mean()\n        if by_quantile:\n            ax.scatter(temp.index, rolling_mean, s=2)\n        else:\n            ax.scatter(temp[f], rolling_mean, s=2)\n        if mutual_info and by_quantile:\n            # entropy of target_mean - mean( entropy of rolling means)\n            ax.set_xlabel(f'{f} mi={H(temp.state.mean()) - H(rolling_mean[~rolling_mean.isna()].values).mean():.5f}')\n        else:\n            ax.set_xlabel(f'{f}')\n    plt.suptitle(title, y=0.90, fontsize=20)\n    plt.show()\n\nplot_mutual_info_diagram(train, num_features,\n                         title='How the target probability depends on the float features')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There are many non linear relationship","metadata":{}},{"cell_type":"code","source":"plot_mutual_info_diagram(train, cat_features,\n                         title='How the target probability depends on the float features')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.358019Z","iopub.status.idle":"2022-05-27T14:23:02.358866Z","shell.execute_reply.started":"2022-05-27T14:23:02.35861Z","shell.execute_reply":"2022-05-27T14:23:02.358633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorial features(features which has no of unique values less than equal to 17)\n* both train and test set have same distribution \n* both train and test set don't follow normal distribution\n\nQ-Q plot also known as (Quantile-Quantile plot) is used to check whether our data follows normal distribution or not.\nIf our plot lies on the red line(y=x) then it is normally distributed. It it don't lie on the y=x line then our feature is not normally distributed.","metadata":{}},{"cell_type":"code","source":"print(\"histplot\",\" \"*3,\"Kde plot\",\" \"*3, \"Boxplot\",\" \"*3,\"QQplot train\",\" \"*3,\"QQplot test\")\nfig, axes = plt.subplots(14,5, figsize=(25,60))\naxes = axes.flatten()\nfor i in range(0,len(axes),5):\n    col = cat_features[i//5]\n    ax = axes[i]\n    train[col].hist(ax= ax,bins=20, color=\"r\",alpha=.5, label=\"train\")\n    test[col].hist(ax= ax,bins=20, color=\"b\", alpha=.5, label=\"test\")\n    \n    sns.kdeplot(train[col], color=\"red\", label=\"train\", ax=axes[i+1])\n    sns.kdeplot(test[col],  color=\"green\", label=\"test\", ax=axes[i+1])\n    axes[i+1].legend()\n    \n    sns.boxplot(data=train[col], color=\"red\",ax=axes[i+2])\n    sns.boxplot(data= test[col],  color=\"green\", ax=axes[i+2])\n    axes[i+2].legend() \n    \n    t1= (train[col].values - train[col].values.mean())/ train[col].values.std()\n    t2= (test[col].values - test[col].values.mean())/ test[col].values.std()\n    qqplot(t1,line=\"s\",ax=axes[i+3])\n    qqplot(t2,line=\"s\",ax=axes[i+4])\n    ax.get_yaxis().set_visible(False)\n    ax.set_title(f'f{cat_features[i//5]}', loc = 'right', fontsize = 12)\n    ax.legend()\n    fig.suptitle(\"distribution of train-test cat_features\")\n    fig.tight_layout()  \nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.359945Z","iopub.status.idle":"2022-05-27T14:23:02.360803Z","shell.execute_reply.started":"2022-05-27T14:23:02.360547Z","shell.execute_reply":"2022-05-27T14:23:02.360573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## numerical features\n* both train and test set have same distribution \n* both train and test set follow normal distribution <b>[with slight deviation from normal behaviour for f_25 and f_26]</b>","metadata":{}},{"cell_type":"code","source":"print(\"histplot\",\" \"*3,\"Kde plot\",\" \"*3, \"Boxplot\",\" \"*3,\"QQplot train\",\" \"*3,\"QQplot test\")\nfig, axes = plt.subplots(16,5, figsize=(25,70))\naxes = axes.flatten()\nfor i in range(0,len(axes),5):\n    col = num_features[i//5]\n    ax = axes[i]\n    train[col].hist(ax= ax,bins=20, color=\"r\",alpha=.5, label=\"train\")\n    test[col].hist(ax= ax,bins=20, color=\"b\", alpha=.5, label=\"test\")\n\n    sns.kdeplot(train[col], color=\"red\", label=\"train\", ax=axes[i+1])\n    sns.kdeplot(test[col],  color=\"green\", label=\"test\", ax=axes[i+1])\n    axes[i+1].legend()\n    \n    sns.boxplot(data=train[col], color=\"red\",ax=axes[i+2])\n    sns.boxplot(data= test[col],  color=\"green\", ax=axes[i+2])\n    axes[i+2].legend()    \n    \n    t1= (train[col].values - train[col].values.mean())/ train[col].values.std()\n    t2= (test[col].values - test[col].values.mean())/ test[col].values.std()\n    qqplot(t1,line=\"s\",ax=axes[i+3])\n    qqplot(t2,line=\"s\",ax=axes[i+4])\n    ax.get_yaxis().set_visible(False)\n    ax.set_title(f'{num_features[i//5]}', loc = 'right', fontsize = 12)\n    ax.legend()\n    fig.suptitle(\"distribution of train-test num_features\")\n    fig.tight_layout()   \nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.36195Z","iopub.status.idle":"2022-05-27T14:23:02.362795Z","shell.execute_reply.started":"2022-05-27T14:23:02.36254Z","shell.execute_reply":"2022-05-27T14:23:02.362565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now for numerical columns we will see if there is any outlier, if present then we will remove it.","metadata":{}},{"cell_type":"code","source":"def check_outlier(data,col_name):\n    \"\"\"\n    input:= data, column name\n    output:= Lower wishker and Upper wishker \n    \"\"\"\n    Q3 = data[col_name].quantile(0.75)\n    Q1 = data[col_name].quantile(0.25)\n    IQR = Q3-Q1 \n    print(\"75%:\", Q3)\n    print(\"25%\",Q1)\n    print(\"IQR:\",IQR)\n    \n    LW = Q1 - 1.5*IQR \n    UW = Q3 + 1.5*IQR \n    print(\"Lower and Upper Wishker: \",LW, UW)\n    print(\"Min and Max value: \", np.min(data[col_name]),np.max(data[col_name]))\n    print(\"Full data:\", data.shape)\n    print(\"No of outliers: \",data[(data[col_name]<LW) | (data[col_name]>UW)].shape)\n    \n    sns.boxplot(x=data[col_name])\n    sns.stripplot(x=data[col_name], color=\"0.5\")\n    plt.show()\n    return LW, UW\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.363921Z","iopub.status.idle":"2022-05-27T14:23:02.364743Z","shell.execute_reply.started":"2022-05-27T14:23:02.364505Z","shell.execute_reply":"2022-05-27T14:23:02.364529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in num_features:\n    print(\"Column: \",c)\n    LW, UW= check_outlier(train,c)\n    print(\"After removing outliers\")\n    train=train[(train[c]>= LW) & (train[c]<= UW)]\n    sns.boxplot(x=train[c])\n    sns.stripplot(x=train[c], color=\"0.5\")\n    plt.show()\n    print(\"=\"*40)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.365869Z","iopub.status.idle":"2022-05-27T14:23:02.366686Z","shell.execute_reply.started":"2022-05-27T14:23:02.366447Z","shell.execute_reply":"2022-05-27T14:23:02.366471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot after removing outliers from train set.\n* We can see that, now our train set is not following normal distribution in tail region, because we have removed outliers(from tails). But now our dataset is much more stable.","metadata":{}},{"cell_type":"code","source":"print(\"histplot\",\" \"*3,\"Kde plot\",\" \"*3, \"Boxplot\",\" \"*3,\"QQplot train\",\" \"*3,\"QQplot test\")\nfig, axes = plt.subplots(16,5, figsize=(25,70))\naxes = axes.flatten()\nfor i in range(0,len(axes),5):\n    col = num_features[i//5]\n    ax = axes[i]\n    train[col].hist(ax= ax,bins=20, color=\"r\",alpha=.5, label=\"train\")\n    test[col].hist(ax= ax,bins=20, color=\"b\", alpha=.5, label=\"test\")\n\n    sns.kdeplot(train[col], color=\"red\", label=\"train\", ax=axes[i+1])\n    sns.kdeplot(test[col],  color=\"green\", label=\"test\", ax=axes[i+1])\n    axes[i+1].legend()\n    \n    sns.boxplot(data=train[col], color=\"red\",ax=axes[i+2])\n    sns.boxplot(data= test[col],  color=\"green\", ax=axes[i+2])\n    axes[i+2].legend()    \n    \n    t1= (train[col].values - train[col].values.mean())/ train[col].values.std()\n    t2= (test[col].values - test[col].values.mean())/ test[col].values.std()\n    qqplot(t1,line=\"s\",ax=axes[i+3])\n    qqplot(t2,line=\"s\",ax=axes[i+4])\n    ax.get_yaxis().set_visible(False)\n    ax.set_title(f'{num_features[i//5]}', loc = 'right', fontsize = 12)\n    ax.legend()\n    fig.suptitle(\"distribution of train-test num_features\")\n    fig.tight_layout()   \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T14:23:02.367846Z","iopub.status.idle":"2022-05-27T14:23:02.368669Z","shell.execute_reply.started":"2022-05-27T14:23:02.368432Z","shell.execute_reply":"2022-05-27T14:23:02.368456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's check now the correlations","metadata":{}},{"cell_type":"code","source":"for ls in new_list:\n    if len(ls) == 0:\n        continue \n    print(ls)\n    val= train[ls].sum(axis=1).values\n    cmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n    plt.scatter(val ,train['f_21'], s=1,c=train.target, cmap=cmap)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:23:02.369752Z","iopub.status.idle":"2022-05-27T14:23:02.370582Z","shell.execute_reply.started":"2022-05-27T14:23:02.37035Z","shell.execute_reply":"2022-05-27T14:23:02.370374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We manually check if there is any relation between features\nfig,axes  = plt.subplots(9,5,figsize=(30,30))\nflatten_axes = axes.flatten()\ncounter = 0\nfor i in range(len(interaction_cols)):\n    for j in range(len(interaction_cols)):\n        if i>j:\n            c1 = interaction_cols[i]\n            c2 = interaction_cols[j]\n            cmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n            flatten_axes[counter].scatter(train[c1], train[c2], s=1,c=train.target, cmap=cmap,)\n            flatten_axes[counter].set_xlabel(c1)\n            flatten_axes[counter].set_ylabel(c2)\n            #flatten_axes[counter].set_aspect('equal')\n            counter += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:23:02.371742Z","iopub.status.idle":"2022-05-27T14:23:02.372294Z","shell.execute_reply.started":"2022-05-27T14:23:02.372059Z","shell.execute_reply":"2022-05-27T14:23:02.372085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you like my work please do upvote!\n**<span style=\"color:#444160;\"> Thanks!🙂</span>**<br>\n.<br>\n.<br>\n.\n\n<img src=\"https://media.giphy.com/media/SfYTJuxdAbsVW/giphy.gif\" width=70%>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}