{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - May 2022: Feature Boundary Investigation\n\nBased on the amazing work seen across these notebooks:\n- https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense by @ambrosm\n- https://www.kaggle.com/code/wti200/analysing-interactions-with-shap by @wti200\n- https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model by @cabaxiom\n\nit seemed that the standard approach to the feature interaction issue was to create ternary features\nthat explicitly defined boundaries:\n\n`X[\"f_21_f_02\"] = (X.f_02 + X.f_21 > 5.2).astype('int') - (X.f_02 + X.f_21 < -5.3).astype('int')`\n\nI was curious if defining a simpler feature would allow the classifier to fit an optimal boundary:\n\n`X[\"f_21_f_02\"] = X.f_02 + X.f_21`\n\nthus performing as well (or better) than the more elaborate interaction features.\n\nSo, here we fit classifiers to both sets of features, compare performance, and analyze the outcome.\n\n\\[NOTE: I used a GPU and 200 trees for training\\]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from pathlib import Path\nfrom warnings import simplefilter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nsimplefilter(\"ignore\")\n\nRANDOM_STATE=42","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:14:21.233206Z","iopub.execute_input":"2022-06-02T00:14:21.233872Z","iopub.status.idle":"2022-06-02T00:14:21.963294Z","shell.execute_reply.started":"2022-06-02T00:14:21.233718Z","shell.execute_reply":"2022-06-02T00:14:21.96251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load training data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntrain = train.set_index('id').sort_index()\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\ntest = test.set_index('id').sort_index()\ndisplay(train.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:14:23.571301Z","iopub.execute_input":"2022-06-02T00:14:23.571647Z","iopub.status.idle":"2022-06-02T00:14:36.572993Z","shell.execute_reply.started":"2022-06-02T00:14:23.57162Z","shell.execute_reply":"2022-06-02T00:14:36.571809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make features\n\ncreate the interaction features as simple sums (allowing classifier to fit a boundary), or as ternary features with explicit boundaries.","metadata":{}},{"cell_type":"code","source":"def make_features(X_in, boundaries=True):\n    \"\"\"\n    generate features for incoming dataframe\n    \n    boundaries: specifies whether interaction features incorporate explicit boundaries\n    \n    returns: dataframe with features\n    \"\"\"\n    \n    # start with float and int features\n    X = X_in.select_dtypes(['float64','int64'])\n \n    # manufacture features from f_27:\n    # - feature for each character position, with ordinal-encoding (10 features)\n    # - feature with total number of distinct characters\n    for i in range(10):\n        X[f\"f_27_{i}\"] = X_in[\"f_27\"].str[i].apply(ord) - ord(\"A\")\n        X[\"f_27_count\"] =  X_in[\"f_27\"].apply(lambda s: len(set(s)))\n        \n    # interaction features:\n    # if boundaries==True, create 3 ternary features based on explicit boundaries\n    if boundaries: \n        X[\"f_21_f_02\"] = (X.f_02 + X.f_21 > 5.2).astype('int') - (X.f_02 + X.f_21 < -5.3).astype('int')\n        X[\"f_26_f_00_f_01\"] = (X.f_01 + X.f_00 + X.f_26 > 5.0).astype('int') - (X.f_01 + X.f_00 + X.f_26 < -5.0).astype('int')\n        X[\"f_22_f_05\"] =( X.f_22 + X.f_05 > 5.1).astype('int') - (X.f_22 + X.f_05 < -5.4).astype('int')\n    else:\n        X[\"f_21_f_02\"] = X.f_02 + X.f_21 \n        X[\"f_26_f_00_f_01\"] = X.f_01 + X.f_00 + X.f_26\n        X[\"f_22_f_05\"] = X.f_22 + X.f_05\n\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:14:38.504019Z","iopub.execute_input":"2022-06-02T00:14:38.504553Z","iopub.status.idle":"2022-06-02T00:14:38.51666Z","shell.execute_reply.started":"2022-06-02T00:14:38.504518Z","shell.execute_reply":"2022-06-02T00:14:38.5158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmark Alternative Feature Sets\n\nWe aren't trying to optimize for score:\n- build a simple, quick-to-train classifier that's reasonably performant\n- run once with explicit boundary features, again with boundaryless features\n- use cross-validation just to ensure we don't see variability across folds\n- compare performance of the 2 feature sets","metadata":{}},{"cell_type":"code","source":"# change tree method if you don't want to use GPU\n# change n_estimators if you want to experiment with different AUCs\n\ndef make_xgb(random_state=RANDOM_STATE):\n    return XGBClassifier(n_estimators=200,\n                         objective='binary:logistic',\n                         eval_metric='auc',\n                         random_state=random_state,\n                         tree_method='gpu_hist'\n                        )","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:14:49.82594Z","iopub.execute_input":"2022-06-02T00:14:49.826878Z","iopub.status.idle":"2022-06-02T00:14:49.832761Z","shell.execute_reply.started":"2022-06-02T00:14:49.826824Z","shell.execute_reply":"2022-06-02T00:14:49.831763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ny = train.target\n\nskf = StratifiedKFold(n_splits=3)\n\nxgb1 = make_xgb()\nX = make_features(train.drop(columns=['target']), boundaries=True)   \nscores1 = cross_val_score(xgb1, X, y, cv=skf, scoring=\"roc_auc\", verbose=2)\n\nxgb2 = make_xgb()\nX = make_features(train.drop(columns=['target']), boundaries=False)   \nscores2 = cross_val_score(xgb2, X, y, cv=skf, scoring=\"roc_auc\", verbose=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:14:52.798756Z","iopub.execute_input":"2022-06-02T00:14:52.799111Z","iopub.status.idle":"2022-06-02T00:15:58.106085Z","shell.execute_reply.started":"2022-06-02T00:14:52.799084Z","shell.execute_reply":"2022-06-02T00:15:58.105268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean1, std1 = np.mean(scores1), np.std(scores1)\nmean2, std2 = np.mean(scores2), np.std(scores2)\nprint(f\"boundaries: AUC-{mean1} (std-{std1})\")\nprint(f\"no boundaries: AUC-{mean2} (std-{std2})\")\nprint(f\"performance increase: {mean1-mean2}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:16:05.007177Z","iopub.execute_input":"2022-06-02T00:16:05.00754Z","iopub.status.idle":"2022-06-02T00:16:05.014377Z","shell.execute_reply.started":"2022-06-02T00:16:05.007509Z","shell.execute_reply":"2022-06-02T00:16:05.013071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Results\nWhile this doesn't seem like a huge difference, it would mean the difference between 1st place and 300th place in the TPS May 22 competition!","metadata":{}},{"cell_type":"markdown","source":"# Analysis\n\nFirst lets examine scatterplots of the interacting features\n\nWe see well-defined boundaries for all 3 interactions.","metadata":{}},{"cell_type":"code","source":"X[\"f_00 + f_01 + f_26\"] = X[\"f_00\"] + X[\"f_01\"] + X[\"f_26\"]\nX[\"f_02 + f_21\"] = X[\"f_02\"] + X[\"f_21\"]\nX[\"f_05 + f_22\"] = X[\"f_05\"] + X[\"f_22\"]\nX[\"random\"] = np.random.randn(len(X))\n\nf,axs = plt.subplots(1,3, figsize=(20,10), sharey=True)\nsns.scatterplot(data = X, y=\"f_00 + f_01 + f_26\", x=\"random\", hue=y, s=2, ax=axs[0])\nsns.scatterplot(data = X, y=\"f_02 + f_21\", x=\"random\", hue=y, s=2, ax=axs[1])\nsns.scatterplot(data = X, y=\"f_05 + f_22\", x=\"random\", hue=y, s=2, ax=axs[2])\nf.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:16:22.825244Z","iopub.execute_input":"2022-06-02T00:16:22.825623Z","iopub.status.idle":"2022-06-02T00:17:09.947649Z","shell.execute_reply.started":"2022-06-02T00:16:22.825591Z","shell.execute_reply":"2022-06-02T00:17:09.946919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Explicit boundaries\n\nFit the model with the explicit boundary feature set so we can examine the features in the trees. \n\nWe inspect the tree dump and as an example we see that `f26+f00+f01` is the criterion for 441 nodes.","metadata":{}},{"cell_type":"code","source":"xgb1 = make_xgb()\nX = make_features(train.drop(columns=['target']), boundaries=True)   \nxgb1.fit(X,y)\n\ntrees1 = xgb1.get_booster().trees_to_dataframe()\nnodes = trees1[trees1.Feature=='f_26_f_00_f_01'].shape[0]\nprint(f\"total nodes with f26+f00+f01: {nodes}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:17:59.440126Z","iopub.execute_input":"2022-06-02T00:17:59.440671Z","iopub.status.idle":"2022-06-02T00:18:21.882005Z","shell.execute_reply.started":"2022-06-02T00:17:59.440628Z","shell.execute_reply":"2022-06-02T00:18:21.880462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### No Boundaries\n\nFit the model with the no boundary feature set.\n\nWe inspect the tree dump and now see that `f26+f00+f01` is the criterion for 704 nodes.","metadata":{}},{"cell_type":"code","source":"xgb2 = make_xgb()\nX = make_features(train.drop(columns=['target']), boundaries=False)   \nxgb2.fit(X,y)\n\ntrees2 = xgb2.get_booster().trees_to_dataframe()\ntrees2[trees2.Feature=='f_26_f_00_f_01'].shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:18:48.61963Z","iopub.execute_input":"2022-06-02T00:18:48.620281Z","iopub.status.idle":"2022-06-02T00:19:10.857382Z","shell.execute_reply.started":"2022-06-02T00:18:48.62023Z","shell.execute_reply":"2022-06-02T00:19:10.85663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Furthermore, we see that the early trees have split values close to the explicit boundary values.\n\nHowever, later trees (that are targeting subsets of the training instances) have split values diverging\nfrom the explicit boundaries.\n\n(Here we are looking for -5.0 and 5.0)","metadata":{}},{"cell_type":"code","source":"display(trees2[(trees2.Tree==0) & (trees2.Feature=='f_26_f_00_f_01')])\ndisplay(trees2[(trees2.Tree==50) & (trees2.Feature=='f_26_f_00_f_01')])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:19:18.962535Z","iopub.execute_input":"2022-06-02T00:19:18.962895Z","iopub.status.idle":"2022-06-02T00:19:18.998962Z","shell.execute_reply.started":"2022-06-02T00:19:18.962863Z","shell.execute_reply":"2022-06-02T00:19:18.998069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking f22+f05, we see a similar outcome with earlier vs later trees.\n\n(Here we are looking for 5.1 and -5.4)","metadata":{}},{"cell_type":"code","source":"display(trees2[(trees2.Tree==0) & (trees2.Feature=='f_22_f_05')])\ndisplay(trees2[(trees2.Tree==50) & (trees2.Feature=='f_22_f_05')])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T00:19:27.15414Z","iopub.execute_input":"2022-06-02T00:19:27.154498Z","iopub.status.idle":"2022-06-02T00:19:27.189716Z","shell.execute_reply.started":"2022-06-02T00:19:27.15447Z","shell.execute_reply":"2022-06-02T00:19:27.189013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAt first blush, it would seem simpler to create the interaction features as simple sums without explicit boundary definitions, allowing\nthe classifier to fit boundaries.\n\nBut we see a slight decrease in performance, as it gives the classifier too much flexibility, essentially giving the classifier\npermission to overfit with the feature anywhere in the domain/range.\n","metadata":{}}]}