{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Getting started with TensorFlow Decision Forests\n","metadata":{}},{"cell_type":"markdown","source":"## References : \n\nGetting started with TensorFlow Decision Forests by PAUL MOONEY\nhttps://www.kaggle.com/code/paultimothymooney/getting-started-with-tensorflow-decision-forests\n\nTPSMAY22 EDA which makes sense by AMBROSM\nhttps://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense/notebook\n\n[TPS-MAY-22] In-Depth EDA + Feature Engineering by CABAXIOM\nhttps://www.kaggle.com/code/cabaxiom/tps-may-22-in-depth-eda-feature-engineering/notebook\n\nAnalysing Interactions with SHAP by WTI 200\nhttps://www.kaggle.com/code/wti200/analysing-interactions-with-shap/notebook\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"**How to use this notebook**:\n - Click on the \"copy & edit\" button in the top right corner. Run the code cells from top to bottom and save a new version.\n - Read through and understand both the markdown cells as well as the code cells and their outputs.\n - Make a submission to the [Tabular Playground Series](https://www.kaggle.com/competitions/tabular-playground-series-may-2022) competition. Experiment and try to increase your score (model selection, hyperparameter choices, feature engineering, feature selection, etc)\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The goal of this notebook is to help Kagglers to get started with the [TensorFlow Decision Forests (TF-DF)](https://www.tensorflow.org/decision_forests) Python API.  We will use data from the [Tabular Playground Series](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data) Kaggle competition to train ML models using TF-DF.\n\nWe'll be working with the [Tabular Playground Series May 2022](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data) Kaggle Dataset.  It is a tabular dataset with 900,000 rows and 33 columns (318MB .CSV training dataset + 247MB .CSV test set) that is suitable for training algorithms to solve binary classification problems (in this case to determine if a machine is in a state of \"0\" or \"1\" based off of input sensor data).  \n\nWe'll be using [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) (TF-DF) to train our model.  TensorFlow Decision Forests is a TensorFlow wrapper for the [Yggdrasil Decision Forests C++ libraries](https://github.com/google/yggdrasil-decision-forests).  TF-DF makes it very easy to train, serve and interpret various Decision Forest models such as [RandomForests](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel) and [GrandientBoostedTrees](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/GradientBoostedTreesModel).  These types of decision forest models require minimal pre-processing of the data and are great when working with tabular datasets and/or small datasets (especially if you just want a quick baseline result to compare against).\n\nBy studying this tutorial you will learn how to quickly train a GradientBoostedTrees model to perform a binary classification task using tabular data.","metadata":{}},{"cell_type":"markdown","source":"Step 1: Import Python packages","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_decision_forests","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T05:55:07.043635Z","iopub.execute_input":"2022-05-21T05:55:07.043961Z","iopub.status.idle":"2022-05-21T05:56:36.678298Z","shell.execute_reply.started":"2022-05-21T05:55:07.043931Z","shell.execute_reply":"2022-05-21T05:56:36.676633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Python packages\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nprint(\"TensorFlow Decision Forests v\" + tfdf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T05:36:48.527099Z","iopub.execute_input":"2022-05-21T05:36:48.528361Z","iopub.status.idle":"2022-05-21T05:36:49.733636Z","shell.execute_reply.started":"2022-05-21T05:36:48.528213Z","shell.execute_reply":"2022-05-21T05:36:49.73256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define helper functions:  \n# One for plotting training evaluation curves, and another for expanding feature number 27.\n# This bit of code is not particularly important with regards to learning how to use TensorFlow Decision Forests (TF-DF)\n# If you are just trying to learn how to use TF-DF then my recommendation would be to skip this code cell and instead focus on understanding all the rest\n\ndef plot_tfdf_model_training_curves(model):\n    # This function was adapted from the following tutorial:\n    # https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\n    logs = model.make_inspector().training_logs()\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    # Plot accuracy vs number of trees\n    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Accuracy (out-of-bag)\")\n    plt.subplot(1, 2, 2)\n    # Plot loss vs number of trees\n    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Logloss (out-of-bag)\")\n    plt.show()\n    \n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-20T16:51:09.315057Z","iopub.execute_input":"2022-05-20T16:51:09.315402Z","iopub.status.idle":"2022-05-20T16:51:09.333564Z","shell.execute_reply.started":"2022-05-20T16:51:09.315357Z","shell.execute_reply":"2022-05-20T16:51:09.332737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_feature_27(data):\n    # This function was adapted from the following notebooks:\n    # https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model and\n    # https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart\n    for df in [data]:\n        # Extract the 10 letters of f_27 into individual features\n        for i in range(10):\n            df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n        # Feature interactions: create three ternary features\n        # Every ternary feature can have the values -1, 0 and +1\n        df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n        df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n        i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n        df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-05-21T05:36:56.412336Z","iopub.execute_input":"2022-05-21T05:36:56.412633Z","iopub.status.idle":"2022-05-21T05:36:56.422955Z","shell.execute_reply.started":"2022-05-21T05:36:56.412601Z","shell.execute_reply":"2022-05-21T05:36:56.421736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engg(df):\n    df['i_25_30'] = np.where( ((df['f_30'] == 0) & (df['f_25']  < -0.20)), 1,-1)\n\n    df['i_25_30'] = np.where( ((df['f_30'] == 1) & (df['f_25'] > 0.05)), 1,-1)\n                        \n    df['i_25_30'] = np.where(((df['f_30'] == 2) & (df['f_25'] > 0.10)),0,-1)\n\n\n    df['i_24_30'] = np.where( ((df['f_30'] == 0) & (df['f_24'] > 0)), 1,0)\n\n    df['i_24_30'] = np.where( ((df['f_30'] == 1) & (df['f_24']  < -0.20)), 0,-1)\n                        \n    df['i_24_30'] = np.where(((df['f_30'] == 2) & (df['f_24'] < -0.10)),1,-1)\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 2: Identify the location of the data","metadata":{}},{"cell_type":"code","source":"# print list of all data and files attached to this notebook\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-21T05:36:59.571463Z","iopub.execute_input":"2022-05-21T05:36:59.572046Z","iopub.status.idle":"2022-05-21T05:36:59.580671Z","shell.execute_reply.started":"2022-05-21T05:36:59.571995Z","shell.execute_reply":"2022-05-21T05:36:59.579983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 3: Load the data","metadata":{}},{"cell_type":"code","source":"# load to pandas dataframe (for data exploration)\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\n\n# load to tensorflow dataset (for model training)\ntrain_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"target\")\ntest_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T05:37:02.687129Z","iopub.execute_input":"2022-05-21T05:37:02.687599Z","iopub.status.idle":"2022-05-21T05:37:18.867865Z","shell.execute_reply.started":"2022-05-21T05:37:02.687562Z","shell.execute_reply":"2022-05-21T05:37:18.866808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 4: Explore the data","metadata":{}},{"cell_type":"code","source":"# print column names\nprint(train_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T16:51:19.730324Z","iopub.execute_input":"2022-05-20T16:51:19.730544Z","iopub.status.idle":"2022-05-20T16:51:19.736259Z","shell.execute_reply.started":"2022-05-20T16:51:19.730517Z","shell.execute_reply":"2022-05-20T16:51:19.735294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview first few rows of data\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T16:51:19.737784Z","iopub.execute_input":"2022-05-20T16:51:19.738143Z","iopub.status.idle":"2022-05-20T16:51:19.777009Z","shell.execute_reply.started":"2022-05-20T16:51:19.7381Z","shell.execute_reply":"2022-05-20T16:51:19.776235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print basic summary statistics\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T16:51:19.780362Z","iopub.execute_input":"2022-05-20T16:51:19.78059Z","iopub.status.idle":"2022-05-20T16:51:21.014486Z","shell.execute_reply.started":"2022-05-20T16:51:19.780561Z","shell.execute_reply":"2022-05-20T16:51:21.013488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing values\nsns.heatmap(train_df.isnull(), cbar=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T16:51:21.016133Z","iopub.execute_input":"2022-05-20T16:51:21.016562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 5: Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"With reference to Correlations and Interactions notebook:\nhttps://www.kaggle.com/code/wti200/analysing-interactions-with-shap\n\nObserved that there is some relation between f_24 and f_25 and f_30.\nWill use that info to create new features with values -1,0,1\n","metadata":{}},{"cell_type":"code","source":"\nsns.barplot(x = 'f_30',\n            y = 'f_24',\n            data = train_df,\n           hue = 'target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x = 'f_30',\n            y = 'f_25',\n            data = train_df,\n           hue = 'target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we just expand out feature number 27. There are 10 unique character positions in feature number 27, and the following bit of code expands feature 27 to instead be 10+ features instead of only one feature. Adding in this step boosts our score by >>5%.","metadata":{}},{"cell_type":"code","source":"print('Feature number 27 is a string') \nprint('with 10 different character positions (1-10)') \nprint('where each character position will contain')\nprint('one of 26 possible characters (A-Z):\\n\\n')\ntrain_df[['f_27']].head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-21T05:37:34.125244Z","iopub.execute_input":"2022-05-21T05:37:34.125544Z","iopub.status.idle":"2022-05-21T05:37:34.162582Z","shell.execute_reply.started":"2022-05-21T05:37:34.125513Z","shell.execute_reply":"2022-05-21T05:37:34.161979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_df = expand_feature_27(train_df)\ntest_df = expand_feature_27(test_df)\n\ntrain_df = feature_engg(train_df)\ntest_df = feature_engg(test_df)\n\ntrain_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"target\")\ntest_tfds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-21T05:37:38.462726Z","iopub.execute_input":"2022-05-21T05:37:38.463231Z","iopub.status.idle":"2022-05-21T05:37:58.809321Z","shell.execute_reply.started":"2022-05-21T05:37:38.463183Z","shell.execute_reply":"2022-05-21T05:37:58.808586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n\\nNew features split out from f_27:\\n\\n')\ntrain_df[['f_27','ch0', 'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7',\n       'ch8', 'ch9', 'unique_characters', 'i_02_21', 'i_05_22',\n       'i_00_01_26']].head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get to the top of the leaderboard you will likely want to do a lot more [feature engineering and feature selection](https://www.kaggle.com/learn/feature-engineering), as these steps were intentionally kept to a minimum in this tutorial (for the sake of brevity).","metadata":{}},{"cell_type":"markdown","source":"# RandomForest\n\nNext we will take our training data and we will use it to train a Random Forest model (to predict whether a given piece of machinery is in a state of \"0\" or \"1\").","metadata":{}},{"cell_type":"markdown","source":"Step 6: Train a [Random Forest](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) Model\n\n\n\n> \"A Random Forest is a collection of deep CART decision trees trained independently and without pruning. Each tree is trained on a random subset of the original training dataset (sampled with replacement).\n> \n> The algorithm is unique in that it is robust to overfitting, even in extreme cases e.g. when there is more features than training examples.\n> \n> It is probably the most well-known of the Decision Forest training algorithms\"\n\n\n\n\n ~ Quoted from [TFDF RandomForest documentation ](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel)","metadata":{}},{"cell_type":"markdown","source":"One neat thing about TF-DF is that in addition to having a default set of hyper-parameters, you are also provided with a list of additional hyper-parameter choices to consider.  This makes it a lot easier to optimize model performance because you do not have to do this expensive hyper-parameter optimization step all by yourself.","metadata":{}},{"cell_type":"code","source":"#print(tfdf.keras.RandomForestModel.predefined_hyperparameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\n#rf_model = tfdf.keras.RandomForestModel(hyperparameter_template=\"better_default\")\n#rf_model.compile(metrics=[tf.keras.metrics.AUC(curve=\"ROC\")]) \n#rf_model.fit(x=train_tfds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the model\n# Currently this step works in the Kaggle Notebook Editor but unfortunately displays an empty/blank visualization in the Notebook Viewer\n#tfdf.model_plotter.plot_model_in_colab(rf_model, tree_idx=0, max_depth=3)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 7: Evaluate your Random Forest Model","metadata":{}},{"cell_type":"code","source":"#plot_tfdf_model_training_curves(rf_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspector = rf_model.make_inspector()\n#inspector.evaluation()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_model.evaluate(train_tfds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Model type:\", inspector.model_type())\n#print(\"Objective:\", inspector.objective())\n#print(\"Evaluation:\", inspector.evaluation())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"90% accuracy is not a bad baseline result given how quickly we put this together (and with so few lines of code).","metadata":{}},{"cell_type":"markdown","source":"Step 8: Investigate variable importances for the RandomForest model","metadata":{}},{"cell_type":"markdown","source":"\nVariable importances (VI) describe the impact of each feature to the model.\n - > VIs generally indicates how much a variable contributes to the model predictions or quality. Different VIs have different semantics and are generally not comparable.\n - > The VIs returned by variable_importances() depends on the learning algorithm and its hyper-parameters. For example, the hyperparameter compute_oob_variable_importances=True of the Random Forest learner enables the computation of permutation out-of-bag variable importances.\n - > Variable importances can be obtained with tfdf.inspector.make_inspector(path).variable_importances().\n\nThe available variable importances are:\n - > Model agnostic\n  - > MEAN_{INCREASE,DECREASE}_IN_{metric}: Estimated metric change from removing a feature using permutation importance . Depending on the learning algorithm and hyper-parameters, the VIs can be computed with validation, cross-validation or out-of-bag. For example, the MEAN_DECREASE_IN_ACCURACY of a feature is the drop in accuracy (the larger, the most important the feature) caused by shuffling the values of a features. For example, MEAN_DECREASE_IN_AUC_3_VS_OTHERS is the expected drop in AUC when comparing the label class \"3\" to the others.\n - > Decision Forests specific\n  - > SUM_SCORE: Sum of the split scores using a specific feature. The larger, the most important.\n  - > NUM_AS_ROOT: Number of root nodes using a specific feature. The larger, the most important.\n  - > NUM_NODES: Number of nodes using a specific feature. The larger, the most important.\n  - > MEAN_MIN_DEPTH: Average minimum depth of the first occurence of a feature across all the tree paths. The smaller, the most important.\n  \n\n~ Quoted from [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/inspector/AbstractInspector#variable_importances) documentation and [yggdrasil-decision-forests](https://github.com/google/yggdrasil-decision-forests/blob/main/documentation/user_manual.md#variable-importances) documentation.\n","metadata":{}},{"cell_type":"code","source":"# Adapted from https://www.tensorflow.org/decision_forests/tutorials/advanced_colab\n# See list of inspector methods from:\n# [field for field in dir(inspector) if not field.startswith(\"_\")]\n#print(f\"Available variable importances:\")\n#for importance in inspector.variable_importances().keys():\n#  print(\"\\t\", importance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable importances describe how each feature impacts the model. Here we can see that our 4 most important features were \"unique_characters\",\"i_00_01_26\",\"i_02_21\", and \"i_05_22\". We created these features during our feature engineering step and it looks like it made a big difference!","metadata":{}},{"cell_type":"code","source":"#inspector.variable_importances()[\"SUM_SCORE\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GradientBoostedTrees\n\nNext we will take our training data and we will use it to train a Gradient Boosted model (to predict whether a given piece of machinery is in a state of \"0\" or \"1\").","metadata":{}},{"cell_type":"markdown","source":"Step 9: Train a [GradientBoostedTrees](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) model.  GradientBoostedTrees often perform better than the RandomForests we were using previously.\n\n\n\n> \"A GBT (Gradient Boosted Tree) is a set of shallow decision trees trained sequentially. Each tree is trained to predict and then \"correct\" for the errors of the previously trained trees (more precisely each tree predict the gradient of the loss relative to the model output)\"\n\n\n\n ~ Quoted from [TFDF GradientBoostedTrees documentation ](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/GradientBoostedTreesModel)\n","metadata":{}},{"cell_type":"code","source":"# As mentioned previously, TF-DF gives you lots of different \"default\" hyper-parameter settings to choose from.\nprint(tfdf.keras.GradientBoostedTreesModel.predefined_hyperparameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ngb_model = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\",num_trees=300)\ngb_model.compile(metrics=[tf.keras.metrics.AUC(curve=\"ROC\")])\ngb_model.fit(x=train_tfds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the model\n# Currently this step works in the Kaggle Notebook Editor but unfortunately displays an empty/blank visualization in the Notebook Viewer\ntfdf.model_plotter.plot_model_in_colab(gb_model, tree_idx=0, max_depth=3)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 10: Evaluate your GradientBoostedTrees  Model","metadata":{}},{"cell_type":"code","source":"plot_tfdf_model_training_curves(gb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector = gb_model.make_inspector()\ninspector.evaluation()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_model.evaluate(train_tfds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model type:\", inspector.model_type())\nprint(\"Objective:\", inspector.objective())\nprint(\"Evaluation:\", inspector.evaluation())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_model.evaluate(train_tfds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"95% accuracy is not a bad baseline result given how quickly we put this together (and with so few lines of code).","metadata":{}},{"cell_type":"markdown","source":"Step 10: Investigate variable importances for the GradientBoostedTrees model","metadata":{}},{"cell_type":"markdown","source":"As mentioned previously, variable importances describe how each feature impacts the model. Variable importances can tell you how much a given variable contributes to the model's predictions. \n","metadata":{}},{"cell_type":"code","source":"# Adapted from https://www.tensorflow.org/decision_forests/tutorials/advanced_colab\n# See list of inspector methods from:\n# [field for field in dir(inspector) if not field.startswith(\"_\")]\nprint(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable importances describe how each feature impacts the model. Once again we can see that our most important features were the features that we created during our feature engineering step. ","metadata":{}},{"cell_type":"code","source":"inspector.variable_importances()[\"SUM_SCORE\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfdf.model_plotter.plot_model_in_colab(gb_model, tree_idx=0, max_depth=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 11: Submit your results","metadata":{}},{"cell_type":"code","source":"sample_submission_df = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')\nsample_submission_df['target'] = gb_model.predict(test_tfds)\nsample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\nsample_submission_df.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TF-DF makes it very easy to find lots of useful information about your model.  For example, the following code cell provides a tremendous amount of information with just a single line of code.  You can preview the output of this code cell by clicking on the \"show output\" button below.","metadata":{}},{"cell_type":"code","source":"gb_model.summary()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"[TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) (TF-DF) made it  quick and easy to train our RandomForest and GradientBoostedTrees models.  These types of decision forest models require minimal pre-processing of the data and are great when working with tabular datasets and/or small datasets (especially if you just want a quick baseline result to compare against).  Some of my favorite parts about  working with TF-DF were: (1) I was able to train a GradientBoostedTrees model with only a few lines of code; (2) there were lots of different default hyper-parameter options that I could choose from; (3) it was easy to visualize the structure/architecture of my models; and (4) it was easy to explore what features were most important to my model (to interpret and explain its decisions).\n\n\nWe worked with the [Tabular Playground Series May 2022](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data) Kaggle Dataset.  It was a tabular dataset with 900,000 rows and 33 columns that contained data from industrial sensors, designed t be used to determine whether that piece of industrial equipment was in a state of  \"0\" or \"1\".\n\nWe were able to solve this task with an accuracy of ~95% which is not a bad baseline result given how quickly we were able to put this together (and with so few lines of code).\n\nTo learn more about TF-DF visit https://www.tensorflow.org/decision_forests.\n\nNext steps?\n - Click on the \"copy & edit\" button in the top right corner of this notebook\n - Experiment and try to increase the score.  My recommendation would be to focus on the [feature engineering and feature selection](https://www.kaggle.com/learn/feature-engineering) steps, as these steps were omitted from this tutorial (for the sake of brevity)\n - Make a submission to https://www.kaggle.com/competitions/tabular-playground-series-may-2022","metadata":{}},{"cell_type":"markdown","source":"Works Cited:\n - [Build, train and evaluate models with TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests/tutorials/beginner_colab) from @[tensorflow](https://www.tensorflow.org/decision_forests/tutorials/)\n  - Code snippets for model training visualization \n  - See comments in plot_tfdf_model_training_curves() for more detail\n - [[TPS-MAY-22] EDA & LGBM Model](https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model) from @[cabaxiom](https://www.kaggle.com/cabaxiom)\n  - Feature engineering code snippets\n  - See comments in expand_feature_27() for more detail\n - [TPSMAY22 Gradient-Boosting Quickstart](https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart) from @[ambrosm](https://www.kaggle.com/ambrosm)\n  - Feature engineering code snippets\n  - See comments in expand_feature_27() for more detail\n\n\nOther Useful References:\n - https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\n - https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab\n - https://www.tensorflow.org/decision_forests/tutorials/advanced_colab","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}