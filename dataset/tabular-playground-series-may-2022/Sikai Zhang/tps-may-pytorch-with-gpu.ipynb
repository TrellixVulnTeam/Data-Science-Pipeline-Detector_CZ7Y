{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-24T06:45:18.77426Z","iopub.execute_input":"2022-05-24T06:45:18.774645Z","iopub.status.idle":"2022-05-24T06:45:18.805774Z","shell.execute_reply.started":"2022-05-24T06:45:18.774557Z","shell.execute_reply":"2022-05-24T06:45:18.80474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt\n\n!pip install -q torchviz\nfrom torchviz import make_dot\n\nimport random\nimport scipy","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:45:18.807956Z","iopub.execute_input":"2022-05-24T06:45:18.808683Z","iopub.status.idle":"2022-05-24T06:45:38.938783Z","shell.execute_reply.started":"2022-05-24T06:45:18.808638Z","shell.execute_reply":"2022-05-24T06:45:38.937546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',index_col='id')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:45:38.941048Z","iopub.execute_input":"2022-05-24T06:45:38.941588Z","iopub.status.idle":"2022-05-24T06:45:52.262407Z","shell.execute_reply.started":"2022-05-24T06:45:38.941538Z","shell.execute_reply":"2022-05-24T06:45:52.261265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\nA simple realization of DNN with PyTorch and GPU acceleration. \n* V3: add some plots to visualize the model and the learning curve.\n* V4: use all training to finalize models and ensemble them. Add regularization.\n* V5: tune regularization.\n* V6: use k-fold training to finalize models.\n* V7: remove rankdata","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\nSee AMBROSM's [notebook](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense).","metadata":{}},{"cell_type":"code","source":"for df in [train, test]:\n    # Extract the 10 letters of f_27 into individual features\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    # Feature interactions: create three ternary features\n    # Every ternary feature can have the values -1, 0 and +1\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n\ny = train.target.values\ntrain = train.drop(['f_27', 'target'], axis = 1)\ntest = test.drop('f_27', axis = 1)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:45:52.264832Z","iopub.execute_input":"2022-05-24T06:45:52.265073Z","iopub.status.idle":"2022-05-24T06:46:13.590186Z","shell.execute_reply.started":"2022-05-24T06:45:52.26503Z","shell.execute_reply":"2022-05-24T06:46:13.589219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling test and train\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:13.591893Z","iopub.execute_input":"2022-05-24T06:46:13.592441Z","iopub.status.idle":"2022-05-24T06:46:14.799378Z","shell.execute_reply.started":"2022-05-24T06:46:13.592379Z","shell.execute_reply":"2022-05-24T06:46:14.798251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define DNN\nThe DNN structure is inspired by AMBROSM's [notebook](https://www.kaggle.com/code/ambrosm/tpsmay22-advanced-keras).","metadata":{}},{"cell_type":"code","source":"# Definition of a DNN Model class\nclass DNN(nn.Module):\n    def __init__(self, input_size, output_size=1):\n        super(DNN, self).__init__()\n        \n        \n        # Fully Connected Layer\n        self.layers = nn.Sequential(nn.Linear(input_size, 64),\n                                nn.SiLU(),\n                                nn.Linear(64, 64),\n                                nn.SiLU(),\n                                nn.Linear(64, 64),\n                                nn.SiLU(),\n                                nn.Linear(64, 16),\n                                nn.SiLU(),\n                                nn.Linear(16, output_size),\n                                nn.Sigmoid()\n                               )\n        \n    def forward(self, x):\n        x = x.reshape(x.shape[0], -1)\n        x = self.layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:14.801147Z","iopub.execute_input":"2022-05-24T06:46:14.801444Z","iopub.status.idle":"2022-05-24T06:46:14.811142Z","shell.execute_reply.started":"2022-05-24T06:46:14.801402Z","shell.execute_reply":"2022-05-24T06:46:14.810112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dnn_plot = DNN(train.shape[1])\nx_plot = torch.randn(1, train.shape[1])\ny_plot = model_dnn_plot(x_plot)\nmake_dot(y_plot.mean(), params=dict(model_dnn_plot.named_parameters()))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:14.812735Z","iopub.execute_input":"2022-05-24T06:46:14.813692Z","iopub.status.idle":"2022-05-24T06:46:15.903501Z","shell.execute_reply.started":"2022-05-24T06:46:14.813645Z","shell.execute_reply":"2022-05-24T06:46:15.902386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Functions","metadata":{}},{"cell_type":"code","source":"# Validation function\ndef validation(model, loader, criterion, device=\"cpu\"):\n    model.eval()\n    loss = 0\n    preds_all = torch.LongTensor()\n    labels_all = torch.LongTensor()\n    \n    with torch.no_grad():\n        for batch_x, labels in loader:\n            labels_all = torch.cat((labels_all, labels), dim=0)\n            batch_x, labels = batch_x.to(device), labels.to(device)\n            labels = labels.unsqueeze(1).float()\n            \n            output = model.forward(batch_x)\n            loss += criterion(output,labels).item()\n            preds_all = torch.cat((preds_all, output.to(\"cpu\")), dim=0)\n    total_loss = loss/len(loader)\n    auc_score = roc_auc_score(labels_all, preds_all)\n    return total_loss, auc_score","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:15.905715Z","iopub.execute_input":"2022-05-24T06:46:15.906399Z","iopub.status.idle":"2022-05-24T06:46:15.918274Z","shell.execute_reply.started":"2022-05-24T06:46:15.906351Z","shell.execute_reply":"2022-05-24T06:46:15.917276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function\ndef train_model(model, trainloader, validloader, criterion, optimizer, \n                scheduler, epochs=20, device=\"cpu\", print_every=1):\n    model.to(device)\n    best_auc = 0\n    best_epoch = 0\n    \n    # to track the training loss as the model trains\n    train_losses = []\n    # to track the validation loss as the model trains\n    valid_losses = []\n    # to track the learning rates in each eporch\n    learning_rates = []\n    \n    for e in range(epochs):\n        model.train()\n        \n        for batch_x, labels in trainloader:\n            batch_x, labels = batch_x.to(device), labels.to(device)\n            labels = labels.unsqueeze(1).float()\n            \n            # Training \n            optimizer.zero_grad()\n            output = model.forward(batch_x)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        # at the end of each epoch calculate loss and auc score:\n        model.eval()\n        train_loss, train_auc = validation(model, trainloader, criterion, device)\n        valid_loss, valid_auc = validation(model, validloader, criterion, device)\n        \n        #### record loss and learning rate\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        learning_rates.append(scheduler._last_lr)\n        \n        if valid_auc > best_auc:\n            best_auc = valid_auc\n            best_epoch = e\n            torch.save(model.state_dict(), \"best-state.pt\")\n        if e % print_every == 0:\n            to_print = \"Epoch: \"+str(e+1)+\" of \"+str(epochs)\n            to_print += \".. Train Loss: {:.4f}\".format(train_loss)\n            to_print += \".. Valid Loss: {:.4f}\".format(valid_loss)\n            to_print += \".. Valid AUC: {:.3f}\".format(valid_auc)\n            print(to_print)\n    # After Training:\n    model.load_state_dict(torch.load(\"best-state.pt\"))\n    to_print = \"\\nTraining completed. Best state dict is loaded.\\n\"\n    to_print += \"Best Valid AUC is: {:.4f} after {} epochs\".format(best_auc,best_epoch+1)\n    print(to_print)\n    return train_losses, valid_losses, learning_rates","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:15.923446Z","iopub.execute_input":"2022-05-24T06:46:15.924331Z","iopub.status.idle":"2022-05-24T06:46:15.940418Z","shell.execute_reply.started":"2022-05-24T06:46:15.924268Z","shell.execute_reply":"2022-05-24T06:46:15.938676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction function\ndef prediction(model, loader, device=\"cpu\"):\n    model.to(device)\n    model.eval()\n    preds_all = torch.LongTensor()\n    \n    with torch.no_grad():\n        for batch_x in loader:\n            batch_x = batch_x.to(device)\n            \n            output = model.forward(batch_x).to(\"cpu\")\n            preds_all = torch.cat((preds_all, output), dim=0)\n    return preds_all","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:15.942226Z","iopub.execute_input":"2022-05-24T06:46:15.94289Z","iopub.status.idle":"2022-05-24T06:46:15.956554Z","shell.execute_reply.started":"2022-05-24T06:46:15.942845Z","shell.execute_reply":"2022-05-24T06:46:15.955452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Ensembling","metadata":{}},{"cell_type":"code","source":"# Checking if GPU is available\nif torch.cuda.is_available():\n    my_device = \"cuda\"\n    print(\"GPU is enabled\")\nelse:\n    my_device = \"cpu\"\n    print(\"No GPU :(\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:15.958401Z","iopub.execute_input":"2022-05-24T06:46:15.959075Z","iopub.status.idle":"2022-05-24T06:46:16.029652Z","shell.execute_reply.started":"2022-05-24T06:46:15.959031Z","shell.execute_reply":"2022-05-24T06:46:16.028495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = train.shape[1]\ntest_tensor = torch.tensor(test).float()\ntest_DL = DataLoader(test_tensor, batch_size=2048)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:16.031191Z","iopub.execute_input":"2022-05-24T06:46:16.031741Z","iopub.status.idle":"2022-05-24T06:46:16.298518Z","shell.execute_reply.started":"2022-05-24T06:46:16.031698Z","shell.execute_reply":"2022-05-24T06:46:16.297438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# max_learning_rate = 0.01\n# epochs = 30\n# pred_list = []\n\n\n# # Prepare Data\n# ## Converting train and validation labels into tensors\n# X_train_tensor = torch.tensor(train).float()\n# y_train_tensor = torch.tensor(y)\n# ## Creating train and validation tensors\n# train_DS = TensorDataset(X_train_tensor, y_train_tensor)\n# ## Defining the dataloaders\n# train_DL = DataLoader(train_DS, batch_size=2048, shuffle=True)\n\n\n# for seed in range(10):\n#     print(f\"** Seed: {seed+1} ** ........training ...... \\n\")\n#     np.random.seed(seed)\n#     random.seed(seed)\n#     torch.manual_seed(seed)\n    \n#     # Initialize Model\n#     model_dnn = DNN(input_size)\n#     # criterion, optimizer, scheduler\n#     criterion = nn.BCELoss()\n#     optimizer = optim.Adam(model_dnn.parameters(), lr=max_learning_rate, weight_decay=1e-4)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n#                                               max_lr = max_learning_rate,\n#                                               epochs = epochs,\n#                                               steps_per_epoch = len(train_DL),\n#                                               pct_start = 0.01,\n#                                               anneal_strategy = \"cos\")\n    \n#     # Training\n#     train_losses, valid_losses, learning_rates = train_model(model = model_dnn,\n#                                                              trainloader = train_DL,\n#                                                              validloader = train_DL,\n#                                                              criterion = criterion,\n#                                                              optimizer = optimizer,\n#                                                              scheduler = scheduler,\n#                                                              epochs = epochs,\n#                                                              device = my_device,\n#                                                              print_every = round(epochs/2)-1)\n    \n#     model_dnn.load_state_dict(torch.load(\"best-state.pt\"))\n#     pred_test = prediction(model_dnn, test_DL, device=my_device)\n#     pred_test_rank = scipy.stats.rankdata(pred_test.tolist())\n#     pred_list.append(pred_test_rank)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:46:16.300305Z","iopub.execute_input":"2022-05-24T06:46:16.300669Z","iopub.status.idle":"2022-05-24T06:46:16.307862Z","shell.execute_reply.started":"2022-05-24T06:46:16.300632Z","shell.execute_reply":"2022-05-24T06:46:16.306658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n# K-Fold Cross Validation\nmax_learning_rate = 0.01\nepochs = 30\npred_list = []\n\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    print(f\"** fold: {fold+1} ** ........training ...... \\n\")\n    \n    # Initialize Model\n    model_dnn = DNN(input_size)\n    \n    # Prepare Data\n    X_train, X_valid = train[idx_tr], train[idx_va]\n    y_train, y_valid = y[idx_tr], y[idx_va]\n    \n    ## Converting train and validation labels into tensors\n    X_train_tensor = torch.tensor(X_train).float()\n    X_valid_tensor = torch.tensor(X_valid).float()\n    y_train_tensor = torch.tensor(y_train)\n    y_valid_tensor = torch.tensor(y_valid)\n\n    ## Creating train and validation tensors\n    train_DS = TensorDataset(X_train_tensor, y_train_tensor)\n    valid_DS = TensorDataset(X_valid_tensor, y_valid_tensor)\n\n    ## Defining the dataloaders\n    train_DL = DataLoader(train_DS, batch_size=2048, shuffle=True)\n    valid_DL = DataLoader(valid_DS, batch_size=2048)\n    \n    # criterion, optimizer, scheduler\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model_dnn.parameters(), lr=max_learning_rate, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n                                              max_lr = max_learning_rate,\n                                              epochs = epochs,\n                                              steps_per_epoch = len(train_DL),\n                                              pct_start = 0.01,\n                                              anneal_strategy = \"cos\")\n    \n    # Training\n    train_losses, valid_losses, learning_rates = train_model(model = model_dnn,\n                                                 trainloader = train_DL,\n                                                 validloader = valid_DL,\n                                                 criterion = criterion,\n                                                 optimizer = optimizer,\n                                                 scheduler = scheduler,\n                                                 epochs = epochs,\n                                                 device = my_device,\n                                                 print_every = round(epochs/2)-1)\n#     break # test\n    model_dnn.load_state_dict(torch.load(\"best-state.pt\"))\n    pred_test = prediction(model_dnn, test_DL, device=my_device)\n    pred_list.append(pred_test.tolist())\n#     pred_test_rank = scipy.stats.rankdata(pred_test.tolist())\n#     pred_list.append(pred_test_rank)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T07:33:22.879368Z","iopub.execute_input":"2022-05-24T07:33:22.879669Z","iopub.status.idle":"2022-05-24T07:42:41.516699Z","shell.execute_reply.started":"2022-05-24T07:33:22.879639Z","shell.execute_reply":"2022-05-24T07:42:41.515698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots()\nax1.plot(range(epochs), train_losses, label='Train Loss')\nax1.plot(range(epochs), valid_losses, label='Valid Loss')\nax1.set_title('Learning Curve')\nax1.set_xlabel(\"Number of Epochs\")\nax1.set_ylabel(\"Loss\")\nax2 = ax1.twinx()\nax2.set_ylabel(\"Learning Rate\")\nax2.plot(range(epochs), learning_rates, label='Learning Rate', color='g')\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T07:42:41.518642Z","iopub.execute_input":"2022-05-24T07:42:41.520793Z","iopub.status.idle":"2022-05-24T07:42:41.840354Z","shell.execute_reply.started":"2022-05-24T07:42:41.520746Z","shell.execute_reply":"2022-05-24T07:42:41.8394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv\")\nsubmission['target'] = np.array(pred_list).mean(axis=0)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-24T06:55:50.965951Z","iopub.execute_input":"2022-05-24T06:55:50.966523Z","iopub.status.idle":"2022-05-24T06:55:53.28513Z","shell.execute_reply.started":"2022-05-24T06:55:50.96646Z","shell.execute_reply":"2022-05-24T06:55:53.284307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}