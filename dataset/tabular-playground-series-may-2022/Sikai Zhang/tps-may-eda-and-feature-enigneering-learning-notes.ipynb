{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T06:58:25.455566Z","iopub.execute_input":"2022-05-19T06:58:25.455906Z","iopub.status.idle":"2022-05-19T06:58:25.488617Z","shell.execute_reply.started":"2022-05-19T06:58:25.45582Z","shell.execute_reply":"2022-05-19T06:58:25.48795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, normalized_mutual_info_score\nfrom sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier\nfrom sklearn.calibration import CalibrationDisplay\nfrom lightgbm import LGBMClassifier\n\nimport shap\nfrom matplotlib import gridspec\n\nimport plotly.express as px\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport copy","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:58:25.490128Z","iopub.execute_input":"2022-05-19T06:58:25.490578Z","iopub.status.idle":"2022-05-19T06:58:45.300955Z","shell.execute_reply.started":"2022-05-19T06:58:25.490544Z","shell.execute_reply":"2022-05-19T06:58:45.29985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',index_col='id')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:58:45.302465Z","iopub.execute_input":"2022-05-19T06:58:45.302725Z","iopub.status.idle":"2022-05-19T06:59:02.832582Z","shell.execute_reply.started":"2022-05-19T06:58:45.302691Z","shell.execute_reply":"2022-05-19T06:59:02.831608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\nThis notebook is used as the learning notes of the EDA and the feature engineering. Welcome to read and comment on it :)\n\n# Table of Contents\n1. [What is Feature Interaction?](#1.-What-is-Feature-Interaction?)\n2. [How to measure the feature relevance?](#2.-How-to-measure-the-feature-relevance?)\n    1. [Pearson's Correlation Matrix](#2.1-Pearson's-Correlation-Matrix)\n    2. [Mutual Information Matrix](#2.2-Mutual-Information-Matrix)\n3. [Feature types](#3.-Feature-types)\n4. [Feature 27](#4.-Feature-27)\n    1. [Categorical feature?](#4.1-Categorical-feature?)\n    2. [Position of str](#4.2-Position-of-str)\n    3. [Unique value](#4.3-Unique-value)\n    4. [Interaction plot](#4.4-Interaction-plot)\n5. [Modelling](#5.-Modelling)\n6. [Analysis with SHAP](#6.-Analysis-with-SHAP)\n    1. [SHAP interaction values](#6.1-SHAP-interaction-values)\n    2. [Dependence plot](#6.2-Dependence-plot)","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgement\n* [AMBROSM](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense/notebook)\n* [CABAXIOM](https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model)\n* [WTI 200](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/discussion/323766)","metadata":{}},{"cell_type":"markdown","source":"# 1. What is Feature Interaction?\n* Feature **Relevance**: the association between a feature and the target\n* Feature **Redundancy**: the association between a feature and another feature\n* Feature **Interaction**: the association between two features and the target, when the features appear together, e.g., XOR gate. Note: the two features may have small relevance with the target individually, but large interaction with the target when they are put together.","metadata":{}},{"cell_type":"markdown","source":"# 2. How to measure the feature relevance?\n* Linear feature relevance: [Pearson's Correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n* Nonlinear feature relevance: [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)\n\n## 2.1 Pearson's Correlation Matrix","metadata":{}},{"cell_type":"code","source":"r_matrix = train.drop('f_27', axis = 1).corr()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:20:42.327081Z","iopub.execute_input":"2022-05-19T07:20:42.328541Z","iopub.status.idle":"2022-05-19T07:20:45.125928Z","shell.execute_reply.started":"2022-05-19T07:20:42.328473Z","shell.execute_reply":"2022-05-19T07:20:45.124842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.imshow(r_matrix, text_auto=\".2f\", width=1000, height=1000)\nfig.update_layout(title=\"Pearson's Correlation Heatmap\", title_x=0.5)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:20:45.127757Z","iopub.execute_input":"2022-05-19T07:20:45.128008Z","iopub.status.idle":"2022-05-19T07:20:46.670603Z","shell.execute_reply.started":"2022-05-19T07:20:45.127967Z","shell.execute_reply":"2022-05-19T07:20:46.66766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Mutual Information Matrix\nBefore computing mutual information matrix, the continues features should be discretized. \nThe features whose number of unique values are greater than 20 are regarded as continues features.","metadata":{}},{"cell_type":"code","source":"N_unique = train.nunique().to_frame(name = 'N_unique')\ndisplay(N_unique)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:20:51.834931Z","iopub.execute_input":"2022-05-19T07:20:51.835457Z","iopub.status.idle":"2022-05-19T07:20:53.300295Z","shell.execute_reply.started":"2022-05-19T07:20:51.83542Z","shell.execute_reply":"2022-05-19T07:20:53.299472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns\ncontinues_features = train[features[N_unique.N_unique > 20]].drop('f_27', axis = 1)\ncontinues_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:22:29.854489Z","iopub.execute_input":"2022-05-19T07:22:29.85486Z","iopub.status.idle":"2022-05-19T07:22:29.991184Z","shell.execute_reply.started":"2022-05-19T07:22:29.85482Z","shell.execute_reply":"2022-05-19T07:22:29.989991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use K-Bins Discretizer to discretize the features into 5-value discrete features.","metadata":{}},{"cell_type":"code","source":"est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\nest.fit(continues_features)\ndiscretized_feature = est.transform(continues_features)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:22:40.676403Z","iopub.execute_input":"2022-05-19T07:22:40.676722Z","iopub.status.idle":"2022-05-19T07:22:41.684374Z","shell.execute_reply.started":"2022-05-19T07:22:40.67669Z","shell.execute_reply":"2022-05-19T07:22:41.682726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_discrete = copy.deepcopy(train.drop('f_27', axis = 1))\ntrain_discrete.update(pd.DataFrame(discretized_feature, columns = continues_features.columns))\ntrain_discrete.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:22:45.308207Z","iopub.execute_input":"2022-05-19T07:22:45.308496Z","iopub.status.idle":"2022-05-19T07:22:45.786046Z","shell.execute_reply.started":"2022-05-19T07:22:45.308466Z","shell.execute_reply":"2022-05-19T07:22:45.784956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use `normalized_mutual_info_score` to compute normlized mutual information.\nIt should be noted that the minimum of MI is 0, but the maximum of MI is not a fixed value.\nNormalized MI will transfer the maximum MI to 1.","metadata":{}},{"cell_type":"code","source":"n = train_discrete.shape[1]\nmi_matrix = np.ones((n, n))\nfor i in range(n):\n    for j in range(i+1, n):\n        mi_matrix[i, j] = normalized_mutual_info_score(train_discrete.iloc[:, i], train_discrete.iloc[:, j])\n        mi_matrix[j, i] = mi_matrix[i, j]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:22:56.584476Z","iopub.execute_input":"2022-05-19T07:22:56.584784Z","iopub.status.idle":"2022-05-19T07:26:27.802359Z","shell.execute_reply.started":"2022-05-19T07:22:56.584753Z","shell.execute_reply":"2022-05-19T07:26:27.801132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_df_matrix = pd.DataFrame(mi_matrix, index = train_discrete.columns, columns = train_discrete.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:27:05.782161Z","iopub.execute_input":"2022-05-19T07:27:05.782453Z","iopub.status.idle":"2022-05-19T07:27:05.788393Z","shell.execute_reply.started":"2022-05-19T07:27:05.782424Z","shell.execute_reply":"2022-05-19T07:27:05.787039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.triu(np.ones_like(mi_df_matrix))\nsns.set(rc={'figure.figsize':(30, 30)})\ndataplot = sns.heatmap(mi_df_matrix, cmap=\"YlGnBu\", annot=True, mask=mask, fmt='.2f')\ndataplot.axes.set_title(\"Mutual Information Matrix\", fontsize=50)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:14.467301Z","iopub.execute_input":"2022-05-19T07:31:14.467614Z","iopub.status.idle":"2022-05-19T07:31:16.879421Z","shell.execute_reply.started":"2022-05-19T07:31:14.467583Z","shell.execute_reply":"2022-05-19T07:31:16.878369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature types\n* **Numerical** feature: values can be compared and can be used in algebraic operations\n    * **Continues** feature: variable interval between neighbour values\n    * **Discrete** feature: constant interval between neighbour values\n* **Categorical** feature: values cannot be compared, and data encoding methods are required.\n\nData Encoding Methods:\n* [Ordinal Encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n* [One Hot (Dummy) Encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n    * m Dummy Encode\n    * [m-1 Dummy Encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=Specifies%20a%20methodology%20to%20use%20to%20drop%20one%20of%20the%20categories%20per%20feature.): If One Hot Encode is adopted for a feature, the feature matrix will not be full rank after centring. If the full rank feature is required, m-1 Dummy Encode is preferred.\n\n<table>\n<tr><th>One Hot Encoding Feature (rank = 3) </th><th>Centred Feature (rank = 2)</th></tr>\n<tr><td>\n\n| Value | Encode 1 | Encode 2 | Encode 3 |\n| --- | --- | --- | --- |\n| A | 1 | 0 | 0 |\n| B | 0 | 1 | 0 |\n| C | 0 | 0 | 1 |\n| B | 0 | 1 | 0 |\n| C | 0 | 0 | 1 |\n\n</td><td>\n\n| Value | Encode 1 | Encode 2 | Encode 3 |\n| --- | --- | --- | --- |\n| A | 0.8 | -0.4 | -0.4 |\n| B | -0.2 | 0.6 | -0.4 |\n| C | -0.2 | -0.4 | 0.6 |\n| B | -0.2 | 0.6 | -0.4 |\n| C | -0.2 | -0.4 | 0.6 |\n\n</td></tr> </table>\n\n\n<table>\n<tr><th>m-1 Dummy Encoding Feature (rank = 2) </th><th>Centred Feature (rank = 2)</th></tr>\n<tr><td>\n\n| Value | Encode 1 | Encode 2 |\n| --- | --- | --- |\n| A | 0 | 0 |\n| B | 1 | 0 |\n| C | 0 | 1 |\n| B | 1 | 0 |\n| C | 0 | 1 |\n\n</td><td>\n\n| Value | Encode 1 | Encode 2 |\n| --- | --- | --- |\n| A | -0.4 | -0.4 |\n| B | 0.6 | -0.4 |\n| C | -0.4 | 0.6 |\n| B | 0.6 | -0.4 |\n| C | -0.4 | 0.6 |\n\n</td></tr> </table>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# 4. Feature 27\nExamples:","metadata":{}},{"cell_type":"code","source":"train.f_27.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:28.412104Z","iopub.execute_input":"2022-05-19T07:31:28.4124Z","iopub.status.idle":"2022-05-19T07:31:28.422759Z","shell.execute_reply.started":"2022-05-19T07:31:28.412369Z","shell.execute_reply":"2022-05-19T07:31:28.421577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Categorical feature?\n> It is important to understand whether the f_27 strings in test are the same as in training. Unfortunately, test contains 1181880 - 741354 = 440526 strings which do not occur in training.\n\n> Insight: We must not use this string as a categorical feature in a classifier. Otherwise, the model learns to rely on strings which never occur in the test data.","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame({'Dataset': ['Train', 'Test'], \n                     'No. of Unique': [train.f_27.nunique(), test.f_27.nunique()],\n                     'Percentage of length': [train.f_27.nunique()/len(train)*100, test.f_27.nunique()/len(test)*100]}).set_index('Dataset')\nprint(temp)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:30.997178Z","iopub.execute_input":"2022-05-19T07:31:30.997526Z","iopub.status.idle":"2022-05-19T07:31:32.139298Z","shell.execute_reply.started":"2022-05-19T07:31:30.997489Z","shell.execute_reply":"2022-05-19T07:31:32.138176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Position of str","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame(columns=['Position', 'No. of Unique'])\nfor i in range(10):\n    temp = temp.append({'Position': i, 'No. of Unique': train.f_27.str.get(i).nunique()}, ignore_index=True)\nprint(temp.set_index('Position'))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:34.847495Z","iopub.execute_input":"2022-05-19T07:31:34.847827Z","iopub.status.idle":"2022-05-19T07:31:42.336327Z","shell.execute_reply.started":"2022-05-19T07:31:34.84779Z","shell.execute_reply":"2022-05-19T07:31:42.335323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Unique value\nThe table clearly shows that the target probability depends on the unique character count:","metadata":{}},{"cell_type":"code","source":"unique_characters = train.f_27.apply(lambda s: len(set(s))).rename('unique_characters')\ntg = train.groupby(unique_characters)\ntemp = pd.DataFrame({'size': tg.size(), 'probability': tg.target.mean().round(2)})\nprint(temp)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:45.310143Z","iopub.execute_input":"2022-05-19T07:31:45.310429Z","iopub.status.idle":"2022-05-19T07:31:46.342255Z","shell.execute_reply.started":"2022-05-19T07:31:45.310399Z","shell.execute_reply":"2022-05-19T07:31:46.34143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Interaction plot\nThe interaction between the features and the target can be viewed in the scatter plots.\nFor example, in the first plot, it is shown that the single feature f_02 cannot discriminate between the yellow and blue dots, which means that f_02 has low relevance to the target.\nHowever, f_02 and f_21 together can discriminate between the yellow and blue dots, which implies that f_02 and f_21 have interaction with the target.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['axes.facecolor'] = 'k'\nplt.figure(figsize=(11, 5))\ncmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n# target == 0 → yellow; target == 1 → blue\n\nax = plt.subplot(1, 3, 1)\nax.scatter(train['f_02'], train['f_21'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_02')\nax.set_ylabel('f_21')\nax.set_aspect('equal')\nax0 = ax\n\nax = plt.subplot(1, 3, 2, sharex=ax0, sharey=ax0)\nax.scatter(train['f_05'], train['f_22'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_05')\nax.set_ylabel('f_22')\nax.set_aspect('equal')\n\nax = plt.subplot(1, 3, 3, sharex=ax0, sharey=ax0)\nax.scatter(train['f_00'] + train['f_01'], train['f_26'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_00 + f_01')\nax.set_ylabel('f_26')\nax.set_aspect('equal')\n\nplt.tight_layout(w_pad=1.0)\nplt.savefig('three-projections.png')\nplt.show()\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:31:57.337384Z","iopub.execute_input":"2022-05-19T07:31:57.337859Z","iopub.status.idle":"2022-05-19T07:33:31.896204Z","shell.execute_reply.started":"2022-05-19T07:31:57.337825Z","shell.execute_reply":"2022-05-19T07:33:31.894819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Modelling\nThe model is built for SHAP analysis.\n\nSee [AMBROSM](https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart/notebook)","metadata":{}},{"cell_type":"code","source":"for df in [train, test]:\n    # Extract the 10 letters of f_27 into individual features\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    # Feature interactions: create three ternary features\n    # Every ternary feature can have the values -1, 0 and +1\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    \nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\ntest[features].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:43:07.784118Z","iopub.execute_input":"2022-05-19T07:43:07.784462Z","iopub.status.idle":"2022-05-19T07:43:28.646456Z","shell.execute_reply.started":"2022-05-19T07:43:07.784425Z","shell.execute_reply":"2022-05-19T07:43:28.645338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\ndef my_booster(random_state=1):\n    return LGBMClassifier(n_estimators=10000, min_child_samples=80,\n                          max_bins=511, random_state=random_state)\n      \nprint(f\"{len(features)} features\")\nscore_list = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    model = my_booster()\n\n    if True or type(model) != XGBClassifier and type(model) != LGBMClassifier: # no early stopping except hgb\n        model.fit(X_tr.values, y_tr)\n    else: # early stopping\n        model.fit(X_tr.values, y_tr, eval_set = [(X_va.values, y_va)], \n                  early_stopping_rounds=30, verbose=100)\n    y_va_pred = model.predict_proba(X_va.values)[:,1]\n    score = roc_auc_score(y_va, y_va_pred)\n    try:\n        print(f\"Fold {fold}: n_iter ={model.n_iter_:5d}    AUC = {score:.5f}\")\n    except AttributeError:\n        print(f\"Fold {fold}:                  AUC = {score:.5f}\")\n    score_list.append(score)\n    break # we only need the first fold\n    \nprint(f\"OOF AUC:                       {np.mean(score_list):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:44:19.909277Z","iopub.execute_input":"2022-05-19T07:44:19.909587Z","iopub.status.idle":"2022-05-19T07:57:30.355443Z","shell.execute_reply.started":"2022-05-19T07:44:19.909554Z","shell.execute_reply":"2022-05-19T07:57:30.354362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Analysis with SHAP\n[Shapley value](https://en.wikipedia.org/wiki/Shapley_value#:~:text=Shapley%20values%20provide%20a%20natural%20way%20to%20compute%20which%20features%20contribute%20to%20a%20prediction) is a popular indicator for feature importance analysis. The feature importance is different from the feature relevance.\n* Feature relevance: the association (i.e. mutual info and correlation) of a feature is estimated isolatedly.\n* Feature importance: the association between a feature and the target consider other features' affect.\n\nNote: as the feature relevance does not ","metadata":{}},{"cell_type":"code","source":"# The computation of SHAP interaction is quite time consuming, here we only take 200 samples as an example.\nX_sampled = train[features].sample(200, random_state=1307)  \nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_sampled)\nshap_interaction = explainer.shap_interaction_values(X_sampled)\nprint(np.shape(shap_interaction))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T08:01:34.085912Z","iopub.execute_input":"2022-05-19T08:01:34.086273Z","iopub.status.idle":"2022-05-19T08:12:16.007675Z","shell.execute_reply.started":"2022-05-19T08:01:34.086236Z","shell.execute_reply":"2022-05-19T08:12:16.007018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.1 SHAP interaction values","metadata":{}},{"cell_type":"code","source":"# Get absolute mean of matrices\nmean_shap = np.abs(shap_interaction).mean(0)\ndf = pd.DataFrame(mean_shap, index=features, columns=features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.imshow(df, text_auto=\".2f\", width=1000, height=1000)\nfig.update_layout(title=\"SHAP interaction values\", title_x=0.5)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T08:19:37.106847Z","iopub.execute_input":"2022-05-19T08:19:37.107639Z","iopub.status.idle":"2022-05-19T08:19:37.168078Z","shell.execute_reply.started":"2022-05-19T08:19:37.107593Z","shell.execute_reply":"2022-05-19T08:19:37.167095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Dependence plot\n\n> Plots the value of the feature on the x-axis and the SHAP value of the same feature on the y-axis. This shows how the model depends on the given feature, and is like a richer extenstion of the classical parital dependence plots. Vertical dispersion of the data points represents interaction effects.\n\nSee [ref1](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.dependence_plot.html) and [ref2](https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html)\n\n* The color corresponds to the feature f_28. \n* The x-axis corresponds to f_02. \n* The y-axis corresponds to **shape value** of f_02 (Note: not shap interaction value but shap value).\n\nThe interaction of the two features can be viewed by the vertical dispersion of the plot.\nFor example, when f_02 at around -1, the red dots are generally split from the blue dots, which suggests an interaction effect between f_02 and f_28.","metadata":{}},{"cell_type":"code","source":"shap.dependence_plot('f_02', shap_values[0], X_sampled[features], interaction_index='f_28')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T08:52:20.03332Z","iopub.execute_input":"2022-05-19T08:52:20.034039Z","iopub.status.idle":"2022-05-19T08:52:20.273013Z","shell.execute_reply.started":"2022-05-19T08:52:20.033931Z","shell.execute_reply":"2022-05-19T08:52:20.27201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}