{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, I explore using optuna to perform hyperparameter searching. The underlying classification model is XGBoost. \n\nPreprocessing is done using a scikit-learn Pipeline, with custom written transformer classes to handle feature f_27. Feature f_27 (which consists of a string of 10 characters) is processed by splitting the string into 10 separate features (each containing a single character), and then one-hot encoding. All other features are left unchanged in preprocessing, I made this choice as the tree models used by XGBoost do not require normalization of numerical features, and can use categorical features directly.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random\nfrom xgboost import XGBRegressor\nimport time\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T22:57:51.192206Z","iopub.execute_input":"2022-05-19T22:57:51.192784Z","iopub.status.idle":"2022-05-19T22:57:53.215429Z","shell.execute_reply.started":"2022-05-19T22:57:51.192741Z","shell.execute_reply":"2022-05-19T22:57:53.214636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class split_f27_transformer():\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        \"create new table with column f_27 split into 10 columns\"\n        d={}\n        for k in range(10):\n            colname='f_27p%d'%k\n            d[colname]=[x[k] for x in X['f_27']]\n        df=pd.DataFrame(d)\n        Xc=pd.concat((X,df),axis=1)\n        Xc.drop('f_27',axis=1,inplace=True)\n        return Xc            \n    \nclass ohe_f27_transformer():\n    def __init__(self):\n        self.ohe=OneHotEncoder(handle_unknown='ignore',sparse=False)\n        self.categorical_cols=list('f_27p%d'%k for k in range(10))\n    def fit(self,X,y=None):\n        self.ohe.fit(X[self.categorical_cols])\n        return self\n    def transform(self,X):\n        df_cc=pd.DataFrame(self.ohe.transform(X[self.categorical_cols]),columns=self.ohe.get_feature_names_out())\n        Xt=pd.concat((X.drop(self.categorical_cols,axis=1),df_cc),axis=1)\n        return Xt","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:17.293594Z","iopub.execute_input":"2022-05-12T23:57:17.293865Z","iopub.status.idle":"2022-05-12T23:57:17.305109Z","shell.execute_reply.started":"2022-05-12T23:57:17.29383Z","shell.execute_reply":"2022-05-12T23:57:17.304368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading / preprocessing \n\nHere, I load the training and test data, create and fit the preprocessing pipeline, and transform the training and test datasets. My pipeline was designed to keep the data as a pandas DataFrame all the way through, so the preprocessed X_train_c and X_test_c are pandas DataFrames.","metadata":{}},{"cell_type":"code","source":"X=pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ny=X['target']\nX_train = X.drop(['id','target'],axis=1)\nX_test=pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\n\npipe=make_pipeline(split_f27_transformer(),ohe_f27_transformer())\npipe.fit(X_train)\nX_train_c=pipe.transform(X_train)\nX_test_c=pipe.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T22:57:47.361276Z","iopub.execute_input":"2022-05-19T22:57:47.361975Z","iopub.status.idle":"2022-05-19T22:57:47.450253Z","shell.execute_reply.started":"2022-05-19T22:57:47.361878Z","shell.execute_reply":"2022-05-19T22:57:47.449059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View some of preprocessed training data set, as sanity check\nX_train_c.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:34.830606Z","iopub.execute_input":"2022-05-12T23:57:34.831014Z","iopub.status.idle":"2022-05-12T23:57:34.868493Z","shell.execute_reply.started":"2022-05-12T23:57:34.830979Z","shell.execute_reply":"2022-05-12T23:57:34.867781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View some of preprocessed test data set, as sanity check\nX_test_c.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:34.869656Z","iopub.execute_input":"2022-05-12T23:57:34.870408Z","iopub.status.idle":"2022-05-12T23:57:34.914678Z","shell.execute_reply.started":"2022-05-12T23:57:34.870363Z","shell.execute_reply":"2022-05-12T23:57:34.913871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# training/validation split\n\nI split the training data set into training/validation data. I also added code to allow selecting a smaller set of the training data, to allow faster training during debugging the code. For the final version, I set Ns to 900000 to select the whole training data set.\n\nI could potentially have done n-fold cross validation, to assess the performance of set of hyperparameters, however as the training size is pretty large I did not do this, and am using a single train/validation split.","metadata":{}},{"cell_type":"code","source":"# Select training/validation set from Ns size subset of training data\nNs=900000\nidx=list(range(X_train_c.shape[0]))\nrandom.shuffle(idx)\nX_t,X_v,y_t,y_v=train_test_split(X_train_c.iloc[idx[0:Ns],:],y[idx[0:Ns]],train_size=.8)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:34.918779Z","iopub.execute_input":"2022-05-12T23:57:34.919046Z","iopub.status.idle":"2022-05-12T23:57:39.585975Z","shell.execute_reply.started":"2022-05-12T23:57:34.91901Z","shell.execute_reply":"2022-05-12T23:57:39.585157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I define here a function to evaluate how good a particular set of Hyperparameters are. This is done by training a XGBoost model on the training data, then computing the AUC metric with that trained model on the validation data.\n\nI made the choice to have this function take a dictionary of parameters and pass it to the constructor for the XGBoost regression model. The code for this function does not use optuna, this lets me separate cleanly the code that uses optuna from the code that does not. (i.e. if I later wanted to evaluate a particular single hyperparameter set, I could do so without needing to use optuna)","metadata":{}},{"cell_type":"code","source":"\ndef is_gpu_available():\n    import subprocess\n    try :\n        t=subprocess.run(['nvidia-smi'],capture_output=True,text=True).stdout\n    except:\n        t=''\n    return t.find('CUDA')>0\n\ndef create_xgbr_model(params):\n    local_params=params\n    if is_gpu_available():\n        local_params['tree_method']='gpu_hist'\n    local_params['objective']='binary:logistic'\n    model=XGBRegressor(**local_params)\n    return model\n\n# Define function to evaluate how good a particular set of hyperparameters are,\n# by constructing and training a model using those hyperparameters, and then\n# evaluating the auc score for those predictions (on the validation set)\ndef xgbr_model_auc_score(params):\n    model=create_xgbr_model(params)\n    model.fit(X_t,y_t)\n    auc_v=roc_auc_score(y_v,model.predict(X_v))\n    return auc_v","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:39.587501Z","iopub.execute_input":"2022-05-12T23:57:39.587815Z","iopub.status.idle":"2022-05-12T23:57:39.597993Z","shell.execute_reply.started":"2022-05-12T23:57:39.587779Z","shell.execute_reply":"2022-05-12T23:57:39.597148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize and run optuna study \n\nThe objective function contains the code that defines the random distributions that are used to generate the parameter values. Here I am optimizing the hyperpameters \n\n* n_estimators\n* learning_rate\n* gamma\n* subsample\n* reg_lambda\n* reg_alpha\n\nThis is not all of the possible XGBoost regression parameters, but from what I have read it covers the most important ones.\n\nI then run the study with a specified number of trials, and save the study object.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params={'n_estimators':trial.suggest_int(\"n_estimators\",50,600),\n           'learning_rate':trial.suggest_float(\"learning_rate\",.001,.5,log=True),\n            'gamma':trial.suggest_float(\"gamma\",0,1),\n            'max_depth':trial.suggest_int('max_depth',5,9),\n            'subsample':trial.suggest_float('subsample',.5,1),\n            'reg_lambda':trial.suggest_float('reg_lambda',0.01,10,log=True),\n            'reg_alpha':trial.suggest_float('reg_alpha',0,10)\n           }\n    return xgbr_model_auc_score(params)\n\nstudy=optuna.create_study(direction='maximize')\nn_trials=300\ntstart=time.time()\nr=study.optimize(objective,n_trials=n_trials)\nelapsed_time=time.time()-tstart\n\n# save study, in case I wish to look at it later\nwith open('hyperparameter_study.pkl','wb') as fid:\n    pickle.dump(study,fid)\n    \nprint('Training using %d data points, validating using %d data points'%(X_t.shape[0],X_v.shape[0]))\nprint('Completed %d trials in %f seconds, average %f seconds per trial'%(n_trials,elapsed_time,elapsed_time/n_trials))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:57:39.600869Z","iopub.execute_input":"2022-05-12T23:57:39.601173Z","iopub.status.idle":"2022-05-12T23:58:40.093014Z","shell.execute_reply.started":"2022-05-12T23:57:39.601136Z","shell.execute_reply":"2022-05-12T23:58:40.092096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Report / visualize study results \n\nI report the best parameters, and visualize the parameter importances and dependence of the quality on a few pairs of hyperparameters","metadata":{}},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:58:40.094944Z","iopub.execute_input":"2022-05-12T23:58:40.095199Z","iopub.status.idle":"2022-05-12T23:58:40.104656Z","shell.execute_reply.started":"2022-05-12T23:58:40.095159Z","shell.execute_reply":"2022-05-12T23:58:40.103978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:58:40.293764Z","iopub.execute_input":"2022-05-12T23:58:40.294231Z","iopub.status.idle":"2022-05-12T23:58:40.659636Z","shell.execute_reply.started":"2022-05-12T23:58:40.294192Z","shell.execute_reply":"2022-05-12T23:58:40.658988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_contour(study,params=['learning_rate','gamma'])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:58:40.106077Z","iopub.execute_input":"2022-05-12T23:58:40.106362Z","iopub.status.idle":"2022-05-12T23:58:40.292336Z","shell.execute_reply.started":"2022-05-12T23:58:40.106297Z","shell.execute_reply":"2022-05-12T23:58:40.291695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_contour(study,params=['learning_rate','max_depth'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_contour(study,params=['learning_rate','n_estimators'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrain and generate predictions \n\nLastly, I use the best identified parameters to create the submission. I retrain on the entire data set (X_train_c rather than X_t), calculate predictions, and save them.","metadata":{}},{"cell_type":"code","source":"##### train on all training data using best parameters, \n# predict on test data, and write submission\nif True:\n    model=create_xgbr_model(study.best_params)\n    model.fit(X_train_c,y)\n    preds=model.predict(X_test_c.drop('id',axis=1))\n    out=pd.DataFrame({'id':X_test_c['id'],'target':preds})\n    out.set_index('id')\n    out.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T23:58:40.678521Z","iopub.execute_input":"2022-05-12T23:58:40.680571Z","iopub.status.idle":"2022-05-12T23:58:57.434842Z","shell.execute_reply.started":"2022-05-12T23:58:40.680532Z","shell.execute_reply":"2022-05-12T23:58:57.434024Z"},"trusted":true},"execution_count":null,"outputs":[]}]}