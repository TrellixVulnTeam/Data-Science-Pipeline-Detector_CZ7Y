{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# basic library\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Data Read","metadata":{}},{"cell_type":"code","source":"# read the train and test data\ndf_train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col = 0)\ndf_test = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv', index_col = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the shape for training set\ndf_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the shape for testing set\ndf_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the amount for duplicated sample in training set\nprint(f\"Duplicated sample: {df_train.duplicated().sum()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the missing value in the training set\nfor i in df_train.columns:\n    print(f\"Missing value for {i}: {df_train[i].isna().sum()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Statistical analysis","metadata":{}},{"cell_type":"markdown","source":"##### Distribution","metadata":{}},{"cell_type":"code","source":"# train set\nint_list = []\nfloat_list = []\nobject_list = []\nfor i in df_train.columns:\n    if df_train[i].dtypes == 'int64':\n        int_list.append(i)\n    elif df_train[i].dtypes == 'float64':\n        float_list.append(i)\n    else:\n        object_list.append(i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"int type features: {int_list}\")\nprint(f\"float type features: {float_list}\")\nprint(f\"object type features: {object_list}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test set\nint_list_test = []\nfloat_list_test = []\nobject_list_test = []\nfor i in df_test.columns:\n    if df_test[i].dtypes == 'int64':\n        int_list_test.append(i)\n    elif df_test[i].dtypes == 'float64':\n        float_list_test.append(i)\n    else:\n        object_list_test.append(i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"int type features: {int_list_test}\")\nprint(f\"float type features: {float_list_test}\")\nprint(f\"object type features: {object_list_test}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"features for both train set and test set should be the same instead of the target column","metadata":{}},{"cell_type":"markdown","source":"##### int type","metadata":{}},{"cell_type":"code","source":"# checking the range for the int_type feature in train set\nfor i in int_list:\n    temp_list = list(df_train[i].unique())\n    temp_list.sort()\n    print(f\"{i}, min: {temp_list[0]}, max: {temp_list[-1]}, number of value: {len(temp_list)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking up the value of target, it was treated as a binary classification.","metadata":{}},{"cell_type":"code","source":"# checking the range for the int_type feature in test set\nfor i in int_list_test:\n    temp_list_test = list(df_test[i].unique())\n    temp_list_test.sort()\n    print(f\"{i}, min: {temp_list_test[0]}, max: {temp_list_test[-1]}, number of value: {len(temp_list_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the two data set\nfor i in int_list_test: # using test set to get rid of the target column\n    temp_list = list(df_train[i].unique())\n    temp_list_test = list(df_test[i].unique())\n    print(f\"Different value of {i}: {len(temp_list) - len(temp_list_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By checking these, there is some different for column: f_08, f_09, f_10, f_11, f_13, f_14, f_15, f_16.\n\nPossible way to deal with: Elimination of those data to ensure the unity of the data between test set and train set","metadata":{}},{"cell_type":"markdown","source":"##### Visualisation of the data distribution\n\nBar chart was used for the int type data column","metadata":{}},{"cell_type":"code","source":"# train set\n# check with the distribution for the int type features\n# melt the data and build a counts column for visualisation\nf = pd.melt(df_train, value_vars = int_list)\nf['counts'] = 1\nf = f.groupby(['variable','value']).sum()\nncols = 3\nnrows = round(len(int_list) / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\nax = axes.ravel()\nfor i in range(len(int_list)):\n    ax[i].bar(data = f.loc[int_list[i]], x = f.loc[int_list[i]].index, height = 'counts')\n    ax[i].set_title(int_list[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For f_7 to f_18, the skewness and distribution is similar. For f_29, it it a binary feature as well but the distribution is different and it is imbalanced. For f_30, the range of the value will be from 0 to 2 and the distribution is quite even.\n\nFor target, the distribution is quite balanced.","metadata":{}},{"cell_type":"code","source":"# test set\n# check with the distribution for the int type features\n# melt the data and build a counts column for visualisation\nf = pd.melt(df_test, value_vars = int_list_test)\nf['counts'] = 1\nf = f.groupby(['variable','value']).sum()\nncols = 3\nnrows = round(len(int_list_test) / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\nax = axes.ravel()\nfor i in range(len(int_list_test)):\n    ax[i].bar(data = f.loc[int_list_test[i]], x = f.loc[int_list_test[i]].index, height = 'counts')\n    ax[i].set_title(int_list_test[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By checking the distribution for both train set and test set, the distribution is quite similar and it can be checked once the FE was done since there is some value missing in both set.","metadata":{}},{"cell_type":"markdown","source":"##### float type\n\nUsing describe function to check about the floating column","metadata":{}},{"cell_type":"code","source":"df_train[float_list].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[float_list_test].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the max and min for both set by it's ratio\ndf_comparing = pd.DataFrame()\ndf_comparing['max_ratio'] = df_train[float_list].describe().T['max'] / df_test[float_list_test].describe().T['max']\ndf_comparing['min_ratio'] = df_train[float_list].describe().T['min'] / df_test[float_list_test].describe().T['min']\ndf_comparing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By comparing two data set, f_03, f_04, f_21, f_24, f_25 got a different ratio more than 10% no matter in maximum or minimum.","metadata":{}},{"cell_type":"markdown","source":"##### visualisation of float data","metadata":{}},{"cell_type":"code","source":"import math\n# train set\nncols = 3\nnrows = math.ceil(len(float_list) / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\nax = axes.ravel()\nfor i in range(len(float_list)):\n    # using histogram to visualize with auto bin width\n    # plot for both train and testing set\n    ax[i].hist(df_train[float_list[i]])\n    ax[i].hist(df_test[float_list_test[i]])\n    ax[i].set_title(float_list[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's switch it to kernal density estimation plot\nimport math\n# train set\nncols = 3\nnrows = math.ceil(len(float_list) / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\nax = axes.ravel()\nfor i in range(len(float_list)):\n    # plot the distribution for both train and test set\n    ax[i] = sns.kdeplot(data = df_train, x = float_list[i], label = 'training', color = 'b', shade = False, ax = ax[i])\n    ax[i] = sns.kdeplot(data = df_test,x = float_list_test[i], label = 'testing', color= 'r', shade = False, ax = ax[i])\n    # show the legend for the labels\n    ax[i].legend()\n    ax[i].set_title(float_list[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By checking the kernal density estimation plot, there is no different in both train and test set.\n\nThere is some different by the visualisation using the histogram. \n\nIt can be caused by the range of the value such as f_24 & f_25.\n\nLet's check about the target distribution in these floating parameters.","metadata":{}},{"cell_type":"code","source":"# let's switch it to kernal density estimation plot\nimport math\n# train set\nncols = 3\nnrows = math.ceil(len(float_list) / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, round(nrows*16/ncols)))\nax = axes.ravel()\nfor i in range(len(float_list)):\n    # show the distribution according to the target\n    ax[i] = sns.kdeplot(data = df_train, x = float_list[i], hue = 'target', shade = True, ax = ax[i])\n    ax[i].set_title(float_list[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a bit different for some parameters as it got a bit imbalanced. It have to be tackled by balancing the data or it can be ignore as the distribution for both training and testing set is quite similar. Hope the propability distribution for the machine model was balanced and there is no need for FE in these section.","metadata":{}},{"cell_type":"markdown","source":"##### Correlation","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,8))\nsns.heatmap(df_train.corr(), annot = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the heatmap, there is some correlation between f_28 and the other feature.\n\nThe coefficient of some features and target will be extracted and check with to milticolinearity.","metadata":{}},{"cell_type":"markdown","source":"##### f_07","metadata":{}},{"cell_type":"code","source":"# check the absolute linear coefficient with other for f_07\nabs(df_train.corr()['f_07']).sort_values(ascending = False)[1:6]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### f_28","metadata":{}},{"cell_type":"code","source":"# check the absolute linear coefficient with other for f_28\nabs(df_train.corr()['f_28']).sort_values(ascending = False)[1:6]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### f_30","metadata":{}},{"cell_type":"code","source":"# check the absolute linear coefficient with other for f_30\nabs(df_train.corr()['f_30']).sort_values(ascending = False)[1:6]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### target","metadata":{}},{"cell_type":"code","source":"# check the absolute linear coefficient with other for target\nabs(df_train.corr()['target']).sort_values(ascending = False)[1:6]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Multicolinearity was found in some feature. Although linear regression may not be applied, it still needed to be fixed to help with the model performance.","metadata":{}},{"cell_type":"markdown","source":"#### Feature engineering","metadata":{}},{"cell_type":"markdown","source":"The first parameter will be f_27 as it is an object type column and it has to be converted into number before the work on ML part.","metadata":{}},{"cell_type":"markdown","source":"##### f_27 (object)","metadata":{}},{"cell_type":"code","source":"# determine the unique value in training set for f_27\nlen(df_train['f_27'].unique())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# determine the unique value in testing set for f_27\nlen(df_test['f_27'].unique())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The unique value for both train and test set is different.\n\nLet's take a look for the data and find out any pattern can be used as a feature.","metadata":{}},{"cell_type":"code","source":"df_train['f_27']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is different 10 alphbets in the sequence. The first FE will be the assignment of number for each alphbets and get a sum of the number.\n\nLet's assign A = 0, B = 1, C = 2, ..., Y = 24, Z = 25 accordingly.\n\nE.g. ABABABABAB -> 5\n\nLet's build a dictionary to calculate the sum and create a new column called f_27_FE","metadata":{}},{"cell_type":"code","source":"f_27_dict = {\n            'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4,\n            'F' : 5, 'G' : 6, 'H' : 7, 'I' : 8, 'J' : 9,\n            'K' : 10, 'L' : 11, 'M' : 12, 'N' : 13, 'O' : 14,\n            'P' : 15, 'Q' : 16, 'R' : 17, 'S' : 18, 'T' : 19,\n            'U' : 20, 'V' : 21, 'W': 22, 'X' : 23, 'Y' : 24, 'Z':25 \n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an empty series as a container\nf_27_FE = pd.Series(dtype = 'int')\nfor sequence in df_train['f_27']:\n    temp = 0\n    for char in sequence:\n        # sum up all the number according to the dict\n        temp += f_27_dict[char]\n    # update the series with latest value calculated for each rows\n    temp = pd.Series(temp)\n    # concat the container with the calculated value\n    f_27_FE = pd.concat([f_27_FE, temp])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a copied for training set and \nnew_train = df_train.copy()\n# reset all the index and get ready to combine with original dataframe\nf_27_FE = f_27_FE.reset_index(drop = True)\n# join the dataframe with created feature\nnew_train['f_27_FE'] = f_27_FE\n# remove the original column after FE\nnew_train.drop('f_27', axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# review after the FE\nnew_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It takes a long time for FE and it needed to be iterated for the whole data set.\n\nLet's check about the distribution and counting after the FE.","metadata":{}},{"cell_type":"code","source":"# check the range for the latest FE column\ntemp_list = list(new_train['f_27_FE'].unique())\ntemp_list.sort()\nprint(f\"f_27_FE, min: {temp_list[0]}, max: {temp_list[-1]}, number of value: {len(temp_list)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check with the distribution for f_27_FE\n# melt the data and build a counts column for visualisation\nf = pd.melt(new_train, value_vars = 'f_27_FE')\nf['counts'] = 1\nf = f.groupby(['value']).sum()\nplt.bar(data = f, x = f.index, height = 'counts');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's work for the testing set and perform the same FE.","metadata":{}},{"cell_type":"code","source":"# create an empty series as a container for test set\nf_27_FE = pd.DataFrame(dtype = 'int')\nfor sequence in df_test['f_27']:\n    temp = 0\n    for char in sequence:\n        # sum up all the number according to the dict\n        temp += f_27_dict[char]\n    # update the series with latest value calculated for each rows    \n    temp = pd.Series(temp)\n    # concat the container with the calculated value\n    f_27_FE = pd.concat([f_27_FE, temp])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a copied for training set and \nnew_test = df_test.copy()\n# reset all the index and get ready to combine with original dataframe\nf_27_FE = f_27_FE.set_index(df_test.index, drop = True)\n# join the dataframe with created feature\nnew_test['f_27_FE'] = f_27_FE[0]\nnew_test['f_27_FE'] = new_test['f_27_FE'].astype('int64')\n# remove the original column after FE\nnew_test.drop('f_27', axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the range for the latest FE column\ntemp_list = list(new_test['f_27_FE'].unique())\ntemp_list.sort()\nprint(f\"f_27_FE, min: {temp_list[0]}, max: {temp_list[-1]}, number of value: {len(temp_list)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the FE, the number of value for f_27_FE in testing set is the same as training set.\n\nLet's take a look for the distribution.","metadata":{}},{"cell_type":"code","source":"# check with the distribution for f_27_FE\n# melt the data and build a counts column for visualisation\nf = pd.melt(new_test, value_vars = 'f_27_FE')\nf['counts'] = 1\nf = f.groupby(['value']).sum()\nplt.bar(data = f, x = f.index, height = 'counts');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution for both training set and testing set is similar as a bell shape.\n\nThe only different is that that range was shipped one in training set for the minimum value.\n\nPossible FE: get rid of one in training set to make same data distribution.\n\nBy turning it into sum of number, it can be used for the first ML training and prediction and check with the result for further modification.","metadata":{}},{"cell_type":"code","source":"# convert the dataframe after FE into csv to save more time\nnew_train.to_csv('new_train.csv')\nnew_test.to_csv('new_test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Simple Binary Classification","metadata":{}},{"cell_type":"markdown","source":"##### Data Slicing","metadata":{}},{"cell_type":"code","source":"# seperate the training features and the predictor variable\nX = new_train.drop('target', axis = 1)\ny = new_train['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Using the logistic regression to perform a preliminary prediction.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# seperate the train and test set for model training\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n# build up a model with more iteration\nmodel = LogisticRegression(max_iter = 10000)\n# train the model and predict the splitted test set\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# using confusion matrix to determine the precision & recall\nconfusion_matrix(y_test, pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By checking the confusion matrix, the model didn't perform well at all","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n# calculate the Precision and Recall\nprint(f'Precision: {precision_score(y_test, pred)}') # TP / (TP + FP)\nprint(f'Recall : {recall_score(y_test, pred)}') # TP / (TP + FN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the precision and recall is quite similar, let's work on the f1_score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n# calculate the f1_score\nf1_score(y_test, pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's perform the predicton for the final test set and check the accuracy for the metrics and model performance in real instances.","metadata":{}},{"cell_type":"code","source":"pred_test = model.predict(new_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_output = pd.DataFrame()\ndf_output['id'] = new_test.index\ndf_output['target'] = pred_test\ndf_output.to_csv('df_output_logistic_v1.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After submisstion, the score was 0.62809 and it is quite close to the precision.\n\nFor next step, PCA & other model can be considered to improve the performance.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}