{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background:#2b6684   ;font-family:'Times';font-size:35px;color:  #F0CB8E\" >&ensp;TPS - MAY2022</div>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T16:12:01.844228Z","iopub.execute_input":"2022-05-08T16:12:01.844634Z","iopub.status.idle":"2022-05-08T16:12:01.877769Z","shell.execute_reply.started":"2022-05-08T16:12:01.844541Z","shell.execute_reply":"2022-05-08T16:12:01.8769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport heapq\nimport scipy\nfrom imblearn.under_sampling import (\n            RandomUnderSampler,\n            OneSidedSelection,\n            InstanceHardnessThreshold,\n        )\nimport lightgbm as lgbm\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif,f_classif\nstyle.use('fivethirtyeight')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T16:12:01.879074Z","iopub.execute_input":"2022-05-08T16:12:01.87973Z","iopub.status.idle":"2022-05-08T16:12:04.756638Z","shell.execute_reply.started":"2022-05-08T16:12:01.879689Z","shell.execute_reply":"2022-05-08T16:12:04.755729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:04.758188Z","iopub.execute_input":"2022-05-08T16:12:04.758629Z","iopub.status.idle":"2022-05-08T16:12:20.531488Z","shell.execute_reply.started":"2022-05-08T16:12:04.758578Z","shell.execute_reply":"2022-05-08T16:12:20.530492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_27']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:20.533671Z","iopub.execute_input":"2022-05-08T16:12:20.533948Z","iopub.status.idle":"2022-05-08T16:12:20.548784Z","shell.execute_reply.started":"2022-05-08T16:12:20.533917Z","shell.execute_reply":"2022-05-08T16:12:20.547961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nmapper = dict(zip(string.ascii_uppercase,np.arange(0,len(string.ascii_lowercase))))\nfor i in range(10):\n    train[str(i) +'_27_num']=train.f_27.str[i].map(mapper)\n    test[str(i) +'_27_num']=test.f_27.str[i].map(mapper)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:20.550143Z","iopub.execute_input":"2022-05-08T16:12:20.551508Z","iopub.status.idle":"2022-05-08T16:12:33.251528Z","shell.execute_reply.started":"2022-05-08T16:12:20.551457Z","shell.execute_reply":"2022-05-08T16:12:33.250688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\">\n<ul style=\"font-family:cursive;font-size:18px; color:#A20404\">Observations: \n<li>We observe that not just the combination but the permutations can perform differently as well ! .</li>\n<li>8_27_num+_7_27_num performs better than 7_27_num+_8_27_num.</li>\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"train['7_27_num+_8_27_num']=(train['7_27_num'].astype(str) + train['8_27_num'].astype(str)).astype(int)\ntest['7_27_num+_8_27_num'] = (test['7_27_num'].astype(str) + test['8_27_num'].astype(str)).astype(int)\n\ntrain['8_27_num+_7_27_num']=(train['8_27_num'].astype(str) + train['7_27_num'].astype(str)).astype(int)\ntest['8_27_num+_7_27_num'] = (test['8_27_num'].astype(str) + test['7_27_num'].astype(str)).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:33.252802Z","iopub.execute_input":"2022-05-08T16:12:33.253058Z","iopub.status.idle":"2022-05-08T16:12:40.884464Z","shell.execute_reply.started":"2022-05-08T16:12:33.253027Z","shell.execute_reply":"2022-05-08T16:12:40.883514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [str(x)+'_27_num' for x in range(0,10)] +['7_27_num+_8_27_num','8_27_num+_7_27_num']\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(train[categorical_columns], train['target'])\nplt.figure(figsize=(10,7))\nsns.barplot(x='feat',y='imp',data=pd.DataFrame({'feat':categorical_columns,'imp':fs.scores_}).sort_values(['imp'],ascending=False))\nplt.xticks(rotation=70)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:40.885699Z","iopub.execute_input":"2022-05-08T16:12:40.885928Z","iopub.status.idle":"2022-05-08T16:12:41.784976Z","shell.execute_reply.started":"2022-05-08T16:12:40.885901Z","shell.execute_reply":"2022-05-08T16:12:41.783935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeature_list = [f for f in train.columns if train[f].dtype in ['float64','int64'] and f not in ['target','id','7_27_num+_8_27_num','8_27_num+_7_27_num']]\nX_train, X_test, y_train, y_test = train_test_split( train[feature_list], train['target'], test_size=0.2, random_state=42, stratify=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:49:37.827538Z","iopub.execute_input":"2022-05-08T17:49:37.828044Z","iopub.status.idle":"2022-05-08T17:49:38.818822Z","shell.execute_reply.started":"2022-05-08T17:49:37.827979Z","shell.execute_reply":"2022-05-08T17:49:38.817988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:49:07.603358Z","iopub.execute_input":"2022-05-08T17:49:07.603701Z","iopub.status.idle":"2022-05-08T17:49:07.619724Z","shell.execute_reply.started":"2022-05-08T17:49:07.603668Z","shell.execute_reply":"2022-05-08T17:49:07.61908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684   ;font-family:'Times';font-size:35px;color:  #F0CB8E\" >&ensp;f_27 Feature combination search using Genetic algo</div>\n<div class=\"alert alert-warning\" role=\"alert\">\n<p style=\"font-family:cursive;font-size:20px;color:  #A20404\"> We can use two modes of fitness calculation</p>\n\n<li style=\"font-family:cursive;font-size:18px; color:#A20404\">Chi2 value as fitness.</li>\n<li style=\"font-family:cursive;font-size:18px; color:#A20404\">Feature importance using a model fit on Undersampled Data</li>","metadata":{"execution":{"iopub.status.busy":"2022-05-06T12:12:32.399Z","iopub.execute_input":"2022-05-06T12:12:32.399772Z","iopub.status.idle":"2022-05-06T12:12:32.407319Z","shell.execute_reply.started":"2022-05-06T12:12:32.3997Z","shell.execute_reply":"2022-05-06T12:12:32.405471Z"}}},{"cell_type":"code","source":"from typing import Union,TypeVar\n\nclass GeneticAlgo:\n    \"\"\"@jaswinder.singh\n        kaggle : https://www.kaggle.com/baratheonr6\n        github : https://github.com/jaswinder9051998\n        \n        Links for Learning if you are intereseted -->\n        Short introduction to genetic algo : https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3\n        Heap data structure : https://www.geeksforgeeks.org/heap-data-structure/\"\"\"\n    def __init__(\n        self,\n        populationsize:int,\n        genelength:int,\n        elitism:int,\n        X_train:pd.DataFrame,\n        y_train:Union[pd.DataFrame,pd.Series],\n        data:pd.DataFrame,\n        mutation_rate:float,\n        n_generations:int,\n        non_num_columns:list,\n    ):\n        \"\"\"__init__ \n\n        Parameters\n        ----------\n        populationsize : int\n            population size \n        genelength : int\n            number of combinations you want to work with \n        elitism : int\n            number of fittest individuals to consider\n        X_train : pd.DataFrame\n            training data , required for feature importance method\n        y_train : Union[pd.DataFrame,pd.Series]\n            data , required for feature importance method\n        data : pd.DataFrame\n            data , required for chi2 method\n        mutation_rate : float\n            rate of mutation required for genetic algo\n        n_generations : int\n            number of iterations\n        non_num_columns : list\n            columns excluding f_27\n        \"\"\"\n        self.populationsize = populationsize\n        self.population = []\n        self.genelength = genelength # if i want feature combinations up to 3 eg.[4,5,6],then genelength =3\n        self.elitism = elitism\n        self.data = data\n        self.mutation_rate = mutation_rate\n        self.n_generations = n_generations\n        self.non_num_columns = non_num_columns\n        \n        # heap will get us best k features\n        self.li = [(-np.inf, [0])] * 10\n        heapq.heapify(self.li)\n\n        # Undersampling for feature importance method\n        from imblearn.under_sampling import RandomUnderSampler\n        nm = RandomUnderSampler(sampling_strategy={0: 150000, 1: 150000})\n        self.X_res, self.y_res = nm.fit_resample(X_train, y_train)\n\n    def _check_individuals(self):\n        \"\"\" -1 represent empty choice.\n            This functions checks and replaces all individuals which are completely empty \"\"\"\n        all_empty_individuals = np.where(self.population.sum(axis=1) == -self.genelength)[0]\n        for inx in all_empty_individuals:\n            pop = np.random.choice(\n                np.arange(0, 10),\n                size=np.random.choice(np.arange(1, self.genelength + 1)),\n                replace=False,\n            )\n            pop = np.concatenate([pop, np.array([-1] * (self.genelength - len(pop)))])\n            self.population[inx] = pop\n\n    def initialize_population(self):\n        \"\"\" initialize the first population\"\"\"\n        for _ in range(self.populationsize):\n            # we choose without replacement to avoid repetition of same feature\n            pop = np.random.choice(\n                np.arange(0, 10, dtype=np.int64),\n                size=np.random.choice(np.arange(1, self.genelength + 1, dtype=np.int64)),\n                replace=False,\n            )\n            pop = np.concatenate([pop, np.array([-1] * (self.genelength - len(pop)))])\n            self.population.append(pop)\n        self.population = np.array(self.population)\n\n    def fitness_via_model_split(self):\n        \"\"\" Calculate fitness value via feature importance \"\"\"\n        self._all = pd.Series(dtype=\"float64\")\n        for individual in self.population:\n            self._all = pd.concat(\n                [\n                    self._all,\n                    self.X_res[[str(int(x)) + \"_27_num\" for x in individual if x != -1]]\n                    .astype(str)\n                    .sum(axis=1)\n                    .astype(int)\n                    .rename( '||'+'+'.join([str(int(x))+'_27_num' for x in individual if x!=-1])),\n                ],\n                axis=1,\n            )\n\n        self._all = self._all.drop(columns=[0])\n        all_tomap = list(self._all.columns)\n\n        # the geneticalgo population can have duplicates,but fitting duplicate columns wont give us true importace, hence remove duplicates\n        self._all = self._all.loc[:,~self._all.columns.duplicated()]\n        self._all = pd.concat([self._all, self.X_res[self.non_num_columns]], axis=1)\n\n        params = {\n            \"n_estimators\": 1000,\n            \"reg_lambda\": 0.0015,\n            \"learning_rate\": 0.09,\n            \"max_depth\": 11,\n            \"min_child_weight\": 135,\n        }\n        model = lgbm.LGBMClassifier(**params)\n        model.fit(self._all, self.y_res)\n        #dd=classification_report(self.y_res, model.predict(self._all),output_dict=True)\n        \n        # iteration to only include fitness of our generated features\n        rank_variable = (1/(1 + len(model.feature_importances_) - scipy.stats.rankdata(model.feature_importances_)))\n        _dict = dict(zip(self._all.columns, rank_variable ) )\n        self.population_fitness = [ _dict[key] for key in all_tomap ]\n\n        # if new importance for a features is found , update the importance in heap\n        for individual, _fit in zip(self.population, self.population_fitness):\n            if list(individual) in [_iter[1] for _iter in self.li]:\n                ix  = [_iter[1] for _iter in self.li].index(list(individual))\n                if _fit>self.li[ix][0]:\n                    self.li[ix]=(_fit,list(individual))\n                    heapq.heapify(self.li)\n            else:   \n                # push new individual to heap\n                heapq.heappushpop(self.li, (_fit, list(individual)))\n\n        self.ranks = scipy.stats.rankdata(self.population_fitness, method=\"average\")\n        self.fitness_ranks = 2 * self.ranks\n\n        if np.max(self.population_fitness) > self.best_fitess:\n            self.best_fitess = np.max(self.population_fitness)\n            self.best_individual = self.population[np.argmax(self.population_fitness)]\n\n    def fitness(self):\n        self._all = pd.Series(dtype=\"float64\")\n        for individual in self.population:\n            self._all = pd.concat(\n                [\n                    self._all,\n                    self.data[[str(int(x)) + \"_27_num\" for x in individual if x != -1]]\n                    .astype(str)\n                    .sum(axis=1)\n                    .astype(int)\n                    .rename( '||'+'+'.join([str(int(x))+'_27_num' for x in individual if x!=-1])),\n                ],\n                axis=1,\n            )\n\n        self._all = self._all.drop(columns=[0])\n        all_tomap = list(self._all.columns)\n\n        self._all = self._all.loc[:,~self._all.columns.duplicated()]\n        \n        fs = SelectKBest(score_func=chi2, k=\"all\")\n        fs.fit(self._all, self.data[\"target\"])\n        \n        _dict = dict(zip(self._all.columns, fs.scores_ ) )\n        self.population_fitness = [ _dict[key] for key in all_tomap ]\n\n        for individual, _fit in zip(self.population, self.population_fitness):\n            if list(individual) in [_iter[1] for _iter in self.li]:\n                ix  = [_iter[1] for _iter in self.li].index(list(individual))\n                if _fit>self.li[ix][0]:\n                    self.li[ix]=(_fit,list(individual))\n                    heapq.heapify(self.li)\n            else:   \n                # push new individual to heap\n                heapq.heappushpop(self.li, (_fit, list(individual)))\n\n        self.ranks = scipy.stats.rankdata(self.population_fitness, method=\"average\")\n        self.fitness_ranks = 2 * self.ranks\n\n        if np.max(self.population_fitness) > self.best_fitess:\n            self.best_fitess = np.max(fs.scores_)\n            self.best_individual = self.population[np.argmax(fs.scores_)]\n\n    def _select_individuals(self):\n        #self.fitness()\n        self.fitness_via_model_split()\n\n        # sorted based on rank\n        sorted_individuals_fitness = sorted(\n            zip(self.population, self.fitness_ranks), key=lambda x: x[1], reverse=True\n        )\n        elite_individuals = np.array(\n            [individual for individual, fitness in sorted_individuals_fitness[: self.elitism]] \n        )\n        non_elite_individuals = np.array(\n            [individual[0] for individual in sorted_individuals_fitness[self.elitism :]]\n        )\n\n        non_elite_individuals_fitness = [\n            individual[1] for individual in sorted_individuals_fitness[self.elitism :]\n        ]\n        selection_probability = non_elite_individuals_fitness / np.sum(\n            non_elite_individuals_fitness\n        )\n\n        selected_indices = np.random.choice(\n            range(len(non_elite_individuals)), self.populationsize // 2, p=selection_probability\n        )\n        selected_individuals = non_elite_individuals[selected_indices, :]\n        self.fit_individuals = np.vstack((elite_individuals, selected_individuals))\n\n    def _mutate(self, array):\n        mutated_array = np.copy(array)\n        for idx, gene in enumerate(array):\n            if np.random.random() < self.mutation_rate:\n                array[idx] = np.random.choice(np.arange(-1, 10))\n\n        return mutated_array\n\n    def fix_repeatition(self, ind):\n        s = set()\n        ind_copy = ind\n        for i, ix in enumerate(ind):\n            if ix in s:\n                ind_copy[i] = -1\n            s.add(ix)\n        return ind_copy\n\n    def _produce_next_generation(self):\n        new_population = np.empty(shape=(self.populationsize, self.genelength), dtype=np.float64)\n\n        for i in range(0, self.populationsize, 2):\n            parents = self.fit_individuals[\n                np.random.choice(self.fit_individuals.shape[0], 2, replace=False), :\n            ]\n\n            crossover_index = np.random.randint(0, len(self.population[0]))\n            new_population[i] = np.hstack(\n                (parents[0][:crossover_index], parents[1][crossover_index:])\n            )\n\n            new_population[i + 1] = np.hstack(\n                (parents[1][:crossover_index], parents[0][crossover_index:])\n            )\n\n            new_population[i] = self.fix_repeatition(self._mutate(new_population[i]))\n            new_population[i + 1] = self.fix_repeatition(self._mutate(new_population[i + 1]))\n        self.population = new_population\n\n    def fit(self):\n        self.initialize_population()\n\n        self.best_fitess = -np.inf\n        self.best_individual = [-1] * self.genelength\n        for i in range(self.n_generations):\n            self._check_individuals()\n\n            self._select_individuals()\n\n            self._produce_next_generation()\n\n            print(\n                \"Iteration-->\",\n                i,\n                \" Best feature-->\",\n                self.best_individual,\n                \" Best fitness-->\",\n                self.best_fitess,\n            )\n\ncolz = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_07',\n       'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16',\n       'f_17', 'f_18', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25',\n       'f_26', 'f_28', 'f_29', 'f_30','0_27_num',\n       '1_27_num', '2_27_num', '3_27_num', '4_27_num', '5_27_num', '6_27_num',\n       '7_27_num', '8_27_num', '9_27_num']\n\nobj = GeneticAlgo(\n    populationsize=50,\n    genelength=3,\n    elitism=2,\n    X_train=X_train,\n    y_train=y_train,\n    data=train,\n    mutation_rate=0.1,\n    n_generations=10,\n    non_num_columns=colz,\n)\nobj.fit()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:49:41.642308Z","iopub.execute_input":"2022-05-08T17:49:41.642848Z","iopub.status.idle":"2022-05-08T18:03:47.757494Z","shell.execute_reply.started":"2022-05-08T17:49:41.642804Z","shell.execute_reply":"2022-05-08T18:03:47.756465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted(obj.li, key=lambda x:x[0],reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:13:53.140193Z","iopub.execute_input":"2022-05-08T18:13:53.140737Z","iopub.status.idle":"2022-05-08T18:13:53.151604Z","shell.execute_reply.started":"2022-05-08T18:13:53.140691Z","shell.execute_reply":"2022-05-08T18:13:53.150761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ll=[]\nfor i,ix in enumerate(obj.li):\n    if '+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]) not in ll:\n        ll.append('+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]))\n        X_train['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=X_train[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)\n        X_test['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=X_test[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)\n        test['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=test[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:13:57.747294Z","iopub.execute_input":"2022-05-08T18:13:57.747623Z","iopub.status.idle":"2022-05-08T18:14:48.084293Z","shell.execute_reply.started":"2022-05-08T18:13:57.747589Z","shell.execute_reply":"2022-05-08T18:14:48.08344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = [f for f in X_train.columns if X_train[f].dtype in ['float64','int64'] and f not in ['target','id']]\nimport lightgbm as lgbm\nparams = {'n_estimators': 10000,\n          'lambda_l2': 0.0015, \n          'alpha': 9.82, \n          'learning_rate': 0.02, \n          'max_depth': 11, \n          'min_child_weight': 135}\nmodel = lgbm.LGBMClassifier(**params)\nmodel.fit(X_train[colz+ll], y_train)\n\nprint(classification_report(y_test, model.predict(X_test[colz+ll])))\nprint(classification_report(y_train, model.predict(X_train[colz+ll])))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:14:48.085899Z","iopub.execute_input":"2022-05-08T18:14:48.086664Z","iopub.status.idle":"2022-05-08T18:35:55.28087Z","shell.execute_reply.started":"2022-05-08T18:14:48.086626Z","shell.execute_reply":"2022-05-08T18:35:55.279857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\nlgbm.plot_importance(model,ignore_zero=False,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:35:55.282225Z","iopub.execute_input":"2022-05-08T18:35:55.282532Z","iopub.status.idle":"2022-05-08T18:35:56.419387Z","shell.execute_reply.started":"2022-05-08T18:35:55.282498Z","shell.execute_reply":"2022-05-08T18:35:56.418306Z"},"trusted":true},"execution_count":null,"outputs":[]}]}