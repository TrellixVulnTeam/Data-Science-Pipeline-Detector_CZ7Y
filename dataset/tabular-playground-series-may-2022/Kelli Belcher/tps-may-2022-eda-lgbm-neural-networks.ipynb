{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#38A6A5;overflow:hidden;font-weight:500\">TPS May 2022</div>\n\n# <b><span style='color:#444444'>1 |</span><span style='color:#38A6A5'> Competition Overview</span></b>\n\nThe [May edition](https://www.kaggle.com/competitions/tabular-playground-series-may-2022) of the 2022 Tabular Playground Series is a binary classification problem. The task for this challenge is to predict whether a machine is in `State 0` or `State 1` using a variety of different feature interactions based on simulated manufacturing control data. There are several types of feature interactions in the data that may be important in determining the machine state. Below is a brief description of the variables in the dataset.\n- Variables `f_00` - `f_06`, `f_19` - `f_26` and `f_28` consist of continuous, numeric values.\n- Variables `f_07` - `f_18` and `f_29` - `f_30` are discrete, whole numbers consisting of between 2-16 unique values in each.\n- Variable `f_27` is a character string consisting of 10 letters.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold \nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom lightgbm import LGBMClassifier\nimport warnings, gc, string, random\nwarnings.filterwarnings(\"ignore\")\nimport plotly.figure_factory as ff\n\ninit_notebook_mode(connected=True)\ncolor=px.colors.qualitative.Plotly\ntemp=dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12), \n                           height=500, width=1000))\n\ntrain=pd.read_csv('../input/tabular-playground-series-may-2022/train.csv', index_col='id')\ntest=pd.read_csv('../input/tabular-playground-series-may-2022/test.csv', index_col='id')\nsub=pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv')\n\nprint(\"Train Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(train.shape[0], train.shape[1],train.isna().sum().sum(), train.duplicated().sum()))\nprint(\"Test Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(test.shape[0], test.shape[1], test.isna().sum().sum(), test.duplicated().sum()))\ndf=train.describe()\ndisplay(df.style.format('{:,.3f}')\n        .background_gradient(subset=(df.index[1:],df.columns[:]), cmap='GnBu'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:11:28.361891Z","iopub.execute_input":"2022-05-28T03:11:28.362717Z","iopub.status.idle":"2022-05-28T03:11:57.145725Z","shell.execute_reply.started":"2022-05-28T03:11:28.36254Z","shell.execute_reply":"2022-05-28T03:11:57.144753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#444444'>2 |</span><span style='color:#38A6A5'> Exploratory Data Analysis</span></b>","metadata":{}},{"cell_type":"code","source":"target=train.target.value_counts(normalize=True)[::-1]\ntext=['State {}'.format(i) for i in target.index]\ncolor,pal=['#38A6A5','#E1B580'],['#88CAC9','#EDD3B3']\nif text[0]=='State 0':\n    color,pal=color,pal\nelse:\n    color,pal=color[::-1],pal[::-1]\nfig=go.Figure()\nfig.add_trace(go.Pie(labels=target.index, values=target*100, hole=.5, \n                     text=text, sort=False, showlegend=False,\n                     marker=dict(colors=pal,line=dict(color=color,width=2)),\n                     hovertemplate = \"State %{label}: %{value:.2f}%<extra></extra>\"))\nfig.update_layout(template=temp, title='Target Distribution', \n                  uniformtext_minsize=15, uniformtext_mode='hide',width=700)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:11:57.147678Z","iopub.execute_input":"2022-05-28T03:11:57.147938Z","iopub.status.idle":"2022-05-28T03:11:57.226869Z","shell.execute_reply.started":"2022-05-28T03:11:57.147907Z","shell.execute_reply":"2022-05-28T03:11:57.226049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target variable is evenly distributed with around 50% in each state. \n\n#### <b><div style='padding:20px;color:white;margin:0;display:fill;border-radius:5px;background-color:#5B5B5B;overflow:hidden;font-weight:600'>2.1 | EDA of Numerical Variables</div></b>","metadata":{}},{"cell_type":"code","source":"float_cols=train.select_dtypes('float')\ndf=pd.concat([float_cols,train['target']], axis=1)\ntitles=['Feature {}'.format(i.split('_')[-1]) for i in df.columns[:-1]]\nfig, ax = plt.subplots(4,4, figsize=(14,24))\nrow=0\ncol=[0,1,2,3]*4\nfor i, column in enumerate(df.columns[:-1]):\n    if (i!=0) & (i%4==0):\n        row+=1\n    color='#38A6A5'\n    rgb=matplotlib.colors.to_rgba(color,0.2)\n    ax[row,col[i]].boxplot(df[df.target==0][column], positions=[0], \n                           widths=0.7, patch_artist=True,\n                           boxprops=dict(color=color, facecolor=rgb, linewidth=1.5),\n                           capprops=dict(color=color,linewidth=1.5),\n                           whiskerprops=dict(color=color,linewidth=1.5),\n                           flierprops=dict(markerfacecolor=rgb, markeredgecolor=color),\n                           medianprops=dict(color=color,linewidth=1.5))\n    color='#E1B580'\n    rgb=matplotlib.colors.to_rgba(color,0.2)\n    ax[row,col[i]].boxplot(df[df.target==1][column], positions=[1],\n                           widths=0.7, patch_artist=True,\n                           boxprops=dict(color=color, facecolor=rgb, linewidth=1.5),\n                           capprops=dict(color=color, linewidth=1.5),\n                           whiskerprops=dict(color=color, linewidth=1.5),\n                           flierprops=dict(markerfacecolor=rgb,markeredgecolor=color),\n                           medianprops=dict(color=color,linewidth=1.5))\n    ax[row,col[i]].grid(visible=True, which='major', axis='y', color='#F2F2F2')\n    ax[row,col[i]].tick_params(left=False,bottom=False)\n    ax[row,col[i]].set_title('\\n\\n{}'.format(titles[i]))\nsns.despine(bottom=True, trim=True)\nplt.suptitle('Distributions of Numerical Variables',fontsize=16)\nplt.tight_layout(rect=[0, 0.2, 1, 0.99])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:11:57.228386Z","iopub.execute_input":"2022-05-28T03:11:57.228637Z","iopub.status.idle":"2022-05-28T03:12:02.364562Z","shell.execute_reply.started":"2022-05-28T03:11:57.228608Z","shell.execute_reply":"2022-05-28T03:12:02.363875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"float_cols=pd.concat([float_cols,train['target']],axis=1)\nfig=make_subplots(rows=4,cols=4,\n                  subplot_titles=titles,\n                  shared_yaxes=True)\ncol=[1,2,3,4]*4\nrow=0\npal=sns.color_palette(\"GnBu\",30).as_hex()[12:]\nfor i,column in enumerate(float_cols.columns[:-1]):\n    if i%4==0:\n        row+=1\n    float_cols['bins'] = pd.cut(float_cols[column],250)\n    float_cols['mean'] = float_cols.bins.apply(lambda x: x.mid)\n    df = float_cols.groupby('mean')[column,'target'].transform('mean')\n    df = df.drop_duplicates(subset=[column]).sort_values(by=column)\n    fig.add_trace(go.Scatter(x=df[column], y=df.target, name=column,\n                             marker_color=pal[i],showlegend=False),\n                  row=row, col=col[i])\n    fig.update_xaxes(zeroline=False, row=row, col=col[i])\n    if i%4==0:\n        fig.update_yaxes(title='Target Probabilitiy',row=row,col=col[i])\nfig.update_layout(template=temp, title='Feature Relationships with Target', \n                  hovermode=\"x unified\",height=1000,width=900)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:02.366385Z","iopub.execute_input":"2022-05-28T03:12:02.36679Z","iopub.status.idle":"2022-05-28T03:12:05.286297Z","shell.execute_reply.started":"2022-05-28T03:12:02.36675Z","shell.execute_reply":"2022-05-28T03:12:05.285515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In each variable, both States 0 and 1 have approximately symmetric distributions. When we look at the probability of the target sorted by feature values, we see there are non-linear relationships between the features and the target, especially in features with higher ranges of values.\n\n#### <b><div style='padding:20px;color:white;margin:0;display:fill;border-radius:5px;background-color:#5B5B5B;overflow:hidden;font-weight:600'>2.2 | EDA of Discrete Variables</div></b>","metadata":{}},{"cell_type":"code","source":"int_df=train.select_dtypes('int')\nsub_titles=['Feature {}'.format(i.split('_')[-1]) for i in int_df.columns[:-1]]\n\npal=['#38A6A5','#E1B580']\nrgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.6)) for i in pal]\n\nfig = make_subplots(rows=5, cols=3, subplot_titles=sub_titles)\nrow=0\nc=[1,2,3]*5\nfor i,col in enumerate(int_df.columns[:-1]):\n    if i%3==0:\n        row+=1\n    df=int_df.groupby(col)['target'].value_counts().rename('count').reset_index()\n    fig.add_trace(go.Bar(x=df[df.target==0][col], y=df[df.target==0]['count'],width=.3,\n                         marker_color=rgb[0], marker_line=dict(color=pal[0],width=2.5),\n                         hovertemplate='Value: %{x}<br>Count: %{y}',\n                         name='State 0', showlegend=(True if i==0 else False)),\n                  row=row, col=c[i])\n    fig.add_trace(go.Bar(x=df[df.target==1][col], y=df[df.target==1]['count'],width=.3,\n                         marker_color=rgb[1], marker_line=dict(color=pal[1],width=2.5), \n                         hovertemplate='Value: %{x}<br>Count: %{y}',\n                         name='State 1', showlegend=(True if i==0 else False)),\n                  row=row, col=c[i])\n    if i%3==0:\n        fig.update_yaxes(title='Count',row=row,col=c[i])\nfig.update_layout(template=temp,title=\"Distributions of Discrete Variables\",\n                  legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.03,xanchor=\"right\",x=.95),\n                  barmode='group',height=1500,width=900)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:05.287896Z","iopub.execute_input":"2022-05-28T03:12:05.288366Z","iopub.status.idle":"2022-05-28T03:12:07.613563Z","shell.execute_reply.started":"2022-05-28T03:12:05.288334Z","shell.execute_reply":"2022-05-28T03:12:07.612696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is low cardinality in the discrete variables, with fewer than 16 unique values in each. Both states have similar proportions within each value and all are positively skewed, except Features 29 and 30, which contain just a few distinct values.\n#### <b><div style='padding:20px;color:white;margin:0;display:fill;border-radius:5px;background-color:#5B5B5B;overflow:hidden;font-weight:600'>2.3 | Correlations</div></b>","metadata":{}},{"cell_type":"code","source":"corr=train.corr().round(2)  \ncorr=corr.iloc[:-1,-1].sort_values(ascending=False)\ntitles=['Feature '+str(i.split('_')[1]) for i in corr.index]\ncorr.index=titles\npal=sns.color_palette(\"RdYlBu\",32).as_hex()\npal=[j for i,j in enumerate(pal) if i not in (14,15)]\nrgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.8)) for i in pal] \nfig=go.Figure()\nfig.add_trace(go.Bar(x=corr.index, y=corr, marker_color=rgb,\n                     marker_line=dict(color=pal,width=2),\n                     hovertemplate='%{x} correlation with Target = %{y}',\n                     showlegend=False, name=''))\nfig.update_layout(template=temp, title='Feature Correlations with Target', \n                  yaxis_title='Correlation', xaxis_tickangle=45, width=800)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:07.615157Z","iopub.execute_input":"2022-05-28T03:12:07.615497Z","iopub.status.idle":"2022-05-28T03:12:10.258761Z","shell.execute_reply.started":"2022-05-28T03:12:07.615455Z","shell.execute_reply":"2022-05-28T03:12:10.257941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr=train.iloc[:,:-1].corr().round(2)  \nmask=np.triu(np.ones_like(corr, dtype=bool))\nc_mask = np.where(~mask, corr, 100)\nc=[]\nfor i in c_mask.tolist()[1:]:\n    c.append([x for x in i if x != 100])\n    \ncor=c[::-1]\nx=corr.index.tolist()[:-1]\ny=corr.columns.tolist()[1:][::-1]\nfig=ff.create_annotated_heatmap(z=cor, x=x, y=y,\n                                hovertemplate='Correlation between %{x} and %{y}= %{z}',\n                                colorscale='emrld', reversescale=True, name='')\nfig.update_layout(template=temp, title='Correlations between Features',\n                  yaxis=dict(showgrid=False,autorange=\"reversed\"),\n                  xaxis=dict(showgrid=False), height=1000,width=1000)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:10.260565Z","iopub.execute_input":"2022-05-28T03:12:10.261043Z","iopub.status.idle":"2022-05-28T03:12:13.191894Z","shell.execute_reply.started":"2022-05-28T03:12:10.26099Z","shell.execute_reply":"2022-05-28T03:12:13.190849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlations overall are relatively low between the features, with the strongest positive relationship between Features 3 and 28 of 0.33.\n\n# <b><span style='color:#444444'>3 |</span><span style='color:#38A6A5'> Feature Engineering</span></b>\n\nFeature 27 consists of a string of 10 characters. For this variable, I will create several new features, one that counts the number of unique characters in the string and one for each position in the string that represents the ordinally-encoded letter at that position for a total of 11 new features. The graphs below show the most common character strings and letters in the dataset.","metadata":{}},{"cell_type":"code","source":"enc = OrdinalEncoder()\ndef feature_eng(df):\n    df=df.copy()\n    df['char_unique']=df['f_27'].apply(lambda x: len(set(x)))\n    for i in range(df.f_27.str.len().max()):\n        df['f_27_char{}'.format(i+1)]=enc.fit_transform(df['f_27'].str.get(i).values.reshape(-1,1))\n    return df.drop(['f_27'],axis=1)\n\ntrain_df=feature_eng(df=train)\ntest_df=feature_eng(df=test)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:12:13.193727Z","iopub.execute_input":"2022-05-28T03:12:13.194166Z","iopub.status.idle":"2022-05-28T03:12:35.935512Z","shell.execute_reply.started":"2022-05-28T03:12:13.194119Z","shell.execute_reply":"2022-05-28T03:12:35.934662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char=train['f_27'].value_counts().nlargest(20)\npal=sns.color_palette(\"Spectral\",22).as_hex() \npal=[j for i,j in enumerate(pal) if i not in (10,11)]\nrgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.75)) for i in pal] \nfig = go.Figure()\nfig.add_trace(go.Bar(x=char.index, y=char, marker_color=rgb, \n                     marker_line=dict(color=pal,width=2), name='',\n                     hovertemplate='String: %{x}, Frequency: %{y}',\n                     showlegend=False))\nfig.update_layout(template=temp,title=\"Most Common Character Strings\",\n                  yaxis_title=\"Frequency\", width=800)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:35.936928Z","iopub.execute_input":"2022-05-28T03:12:35.9372Z","iopub.status.idle":"2022-05-28T03:12:37.133365Z","shell.execute_reply.started":"2022-05-28T03:12:35.937169Z","shell.execute_reply":"2022-05-28T03:12:37.132557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=train[['f_27']]\nfor letter in string.ascii_uppercase:\n    df['{}'.format(letter)]=df['f_27'].str.count(letter)\ndf_sum=df.iloc[:,1:].sum(axis=0).rename('sum').reset_index()\npal=sns.color_palette(\"Spectral_r\",28).as_hex()\npal=[j for i,j in enumerate(pal) if i !=14]\nrgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.8)) for i in pal] \nfig = go.Figure()\nfig.add_trace(go.Bar(x=df_sum['index'], y=df_sum['sum'], marker_color=rgb, \n                     marker_line=dict(color=pal,width=2), name='',\n                     hovertemplate='Letter: %{x}, Frequency: %{y}',\n                     showlegend=False))\nfig.update_layout(template=temp,title=\"Most Common Letters\",\n                  yaxis_title=\"Frequency\", width=800)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:12:37.136246Z","iopub.execute_input":"2022-05-28T03:12:37.136698Z","iopub.status.idle":"2022-05-28T03:12:58.086479Z","shell.execute_reply.started":"2022-05-28T03:12:37.136658Z","shell.execute_reply":"2022-05-28T03:12:58.08552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#444444'>4 |</span><span style='color:#38A6A5'> Gradient Boosting</span></b>\nDue to some of the non-linear relationships in the data, the first model I will fit is a Gradient Boosting model as a baseline.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\ny=train_df['target']\nX=train_df.drop(['target'], axis=1)\nX=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)\nX_test=pd.DataFrame(scaler.transform(test_df))\n\ny_valid, gbm_val_preds, gbm_test_preds=[],[],[]\ncal_true, cal_pred=[],[]\nfeat_importance=pd.DataFrame(index=X.columns)\nk_fold = KFold(n_splits=5, shuffle=True, random_state=21)\nfor fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):\n    \n    print(\"\\nFold {}\".format(fold+1))\n    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n    X_val, y_val = X.iloc[val_idx,:], y[val_idx]\n    print(\"Train shape: {}, {}, Valid shape: {}, {}\".format(\n        X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n    \n    params = {'boosting_type': 'gbdt',\n              'n_estimators': 250,\n              'num_leaves': 50,\n              'learning_rate': 0.1,\n              'colsample_bytree': 0.9,\n              'subsample': 0.8,\n              'reg_alpha': 0.1,\n              'objective': 'binary',\n              'metric': 'auc',\n              'random_state': 21}\n    \n    gbm = LGBMClassifier(**params).fit(X_train, y_train, \n                                       eval_set=[(X_train, y_train), (X_val, y_val)],\n                                       verbose=100,\n                                       eval_metric=['binary_logloss','auc'])\n    \n    gbm_prob = gbm.predict_proba(X_val)[:,1]\n    y_valid.append(y_val)\n    gbm_val_preds.append(gbm_prob)\n    gbm_test_preds.append(gbm.predict_proba(X_test)[:,1])\n    feat_importance[\"Importance_Fold\"+str(fold)]=gbm.feature_importances_\n    \n    calibrated_gbm = CalibratedClassifierCV(base_estimator=gbm, cv=\"prefit\")\n    cal_fit = calibrated_gbm.fit(X_train, y_train)\n    cal_probs = calibrated_gbm.predict_proba(X_val)[:, 1]\n    prob_true, prob_pred = calibration_curve(y_val, cal_probs, n_bins=10)\n    cal_true.append(prob_true)\n    cal_pred.append(prob_pred)\n    auc_score=roc_auc_score(y_val, gbm_prob)\n    print(\"Validation AUC = {:.4f}\".format(auc_score))\n      \n    del X_train, y_train, X_val, y_val\n    gc.collect()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:12:58.087819Z","iopub.execute_input":"2022-05-28T03:12:58.088077Z","iopub.status.idle":"2022-05-28T03:18:54.004716Z","shell.execute_reply.started":"2022-05-28T03:12:58.088047Z","shell.execute_reply":"2022-05-28T03:18:54.003699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors=px.colors.qualitative.Prism\ndef plot_roc_calibration(y_val, y_prob, mpv_cal, fop_cal):\n    fig=go.Figure()\n    fig.add_trace(go.Scatter(x=np.linspace(0,1,11), y=np.linspace(0,1,11), \n                             name='Random Chance',mode='lines',\n                             line=dict(color=\"Black\", width=1, dash=\"dot\")))\n    for i in range(len(y_val)):\n        y=y_val[i]\n        prob=y_prob[i]\n        fpr, tpr, thresh = roc_curve(y, prob)\n        roc_auc = auc(fpr,tpr)\n        fig.add_trace(go.Scatter(x=fpr, y=tpr, line=dict(color=colors[::-1][i+6], width=3), \n                                 hovertemplate = 'True positive rate = %{y:.3f}, False positive rate = %{x:.3f}',\n                                 name='Fold {} AUC = {:.4f}'.format(i+1,roc_auc)))\n    fig.update_layout(template=temp, title=\"Cross-Validation ROC Curves\", \n                      hovermode=\"x unified\", width=600,height=500,\n                      xaxis_title='False Positive Rate (1 - Specificity)',\n                      yaxis_title='True Positive Rate (Sensitivity)',\n                      legend=dict(orientation='v', y=.07, x=1, xanchor=\"right\",\n                                  bordercolor=\"black\", borderwidth=.5))\n    fig.show()\n    fig=go.Figure()\n    fig.add_trace(go.Scatter(x=np.linspace(0,1,11), y=np.linspace(0,1,11), \n                             name='Perfectly Calibrated',mode='lines',\n                             line=dict(color=\"Black\", width=1, dash=\"dot\"),legendgroup=2))\n    for i in range(len(mpv_cal)):\n        mpv=mpv_cal[i]\n        fop=fop_cal[i]\n        fig.add_trace(go.Scatter(x=mpv, y=fop, line=dict(color=colors[::-1][i+6], width=3), \n                                 hovertemplate = 'Proportion of Positives = %{y:.3f}, Mean Predicted Probability = %{x:.3f}',\n                                 name='Fold {}'.format(i+1),legendgroup=2))\n    fig.update_layout(template=temp, title=\"Probability Calibration Curves\", \n                      hovermode=\"x unified\", width=600,height=500,\n                      xaxis_title='Mean Predicted Probability',\n                      yaxis_title='Proportion of Positives',\n                      legend=dict(orientation='v', y=.07, x=1, xanchor=\"right\",\n                                  bordercolor=\"black\", borderwidth=.5))\n    fig.show()\n    \ndef plot_target_predictions(df):\n    plot_df=pd.DataFrame.from_dict({'1':(len(df[df.target>0.5])/len(df.target))*100, \n                                    '0':(len(df[df.target<=0.5])/len(df.target))*100}, \n                                   orient='index', columns=['pct'])\n    text=['State {}'.format(i) for i in plot_df.index]\n    color,pal=['#38A6A5','#E1B580'],['#88CAC9','#EDD3B3']\n    if text[0]=='State 0':\n        color,pal=color,pal\n    else:\n        color,pal=color[::-1],pal[::-1]\n    fig=go.Figure()\n    fig.add_trace(go.Pie(labels=plot_df.index, values=plot_df.pct, hole=.5, \n                         text=text, sort=False, showlegend=False,\n                         marker=dict(colors=pal,line=dict(color=color,width=2)),\n                         hovertemplate = \"State %{label}: %{value:.2f}%<extra></extra>\"))\n    fig.update_layout(template=temp, title='Predicted Target Distribution', width=700,\n                      uniformtext_minsize=15, uniformtext_mode='hide')\n    fig.show()\n    \nplot_roc_calibration(y_valid, gbm_val_preds, cal_true, cal_pred)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:18:54.00666Z","iopub.execute_input":"2022-05-28T03:18:54.007029Z","iopub.status.idle":"2022-05-28T03:18:55.69235Z","shell.execute_reply.started":"2022-05-28T03:18:54.006972Z","shell.execute_reply":"2022-05-28T03:18:55.691295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The LGBM Model did quite well with an Area Under the Curve [(AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) of over 0.97 in each cross-validation set. The mean predicted probabilities are also fairly close to the true proportion of positives in the Probability Calibration Curves graph. The graph below shows the most important features in the model averaged across each fold.\n# <b><span style='color:#444444'>5 |</span><span style='color:#38A6A5'> Feature Importance</span></b>","metadata":{}},{"cell_type":"code","source":"feat_importance['avg']=feat_importance.mean(axis=1)\nfeat_importance=feat_importance.sort_values(by='avg',ascending=True)\n\npal=sns.color_palette(\"YlGnBu\", 55).as_hex()\nfig=go.Figure()\nfor i in range(len(feat_importance.index)):\n    fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n                       line_color=pal[::-1][i],opacity=0.8,line_width=4))\nfig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers', \n                         marker_color=pal[::-1], marker_size=8,\n                         hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))\nfig.update_layout(template=temp,title='Feature Importance', \n                  xaxis=dict(title='Average Importance',zeroline=False),\n                  yaxis_showgrid=False, height=900, width=800)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T03:33:49.412891Z","iopub.execute_input":"2022-05-28T03:33:49.413637Z","iopub.status.idle":"2022-05-28T03:33:49.820806Z","shell.execute_reply.started":"2022-05-28T03:33:49.413593Z","shell.execute_reply":"2022-05-28T03:33:49.820091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_gbm=sub.copy()\nsub_gbm['target']=np.mean(gbm_test_preds, axis=0)\nsub_gbm.to_csv(\"sub_gbm.csv\", index=False)\nplot_target_predictions(sub_gbm)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:18:56.184583Z","iopub.execute_input":"2022-05-28T03:18:56.185474Z","iopub.status.idle":"2022-05-28T03:18:58.970638Z","shell.execute_reply.started":"2022-05-28T03:18:56.185422Z","shell.execute_reply":"2022-05-28T03:18:58.969726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#444444'>6 |</span><span style='color:#38A6A5'> Neural Network</span></b>\nThe next model I will fit is a Multi-Layer Neural Network. This model will consist of an input node with the shape of our training set, a 41-dimensional vector which corresponds to the number of features in the data, and 5 hidden layers with 512, 384, 256, 128, and 64 neurons, with each layer using the Swish activation function and L2 regularization to prevent overfitting. The Sigmoid activation function will be used in the last layer of the model with one output for binary classification. Below is a graph of the model's architecture.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import metrics, regularizers\nfrom tensorflow.keras.utils import plot_model\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-28T03:18:58.972156Z","iopub.execute_input":"2022-05-28T03:18:58.974065Z","iopub.status.idle":"2022-05-28T03:19:11.415969Z","shell.execute_reply.started":"2022-05-28T03:18:58.974012Z","shell.execute_reply":"2022-05-28T03:19:11.414994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nn_model():\n    \n    x_input = Input(shape=(X.shape[1]))\n    x = Dense(512, kernel_regularizer=regularizers.l2(1e-5),\n              activation='swish')(x_input)\n    x = Dense(384, kernel_regularizer=regularizers.l2(1e-5),\n              activation='swish')(x)\n    x = Dense(256, kernel_regularizer=regularizers.l2(1e-5),\n              activation='swish')(x)\n    x = Dense(128, kernel_regularizer=regularizers.l2(1e-5),\n              activation='swish')(x)\n    x = Dense(64, kernel_regularizer=regularizers.l2(1e-5),\n              activation='swish')(x)\n    output = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=x_input, outputs=output)\n    \n    return model\n\nmodel = nn_model()\nplot_model(model, show_layer_names=False, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:19:11.417405Z","iopub.execute_input":"2022-05-28T03:19:11.417693Z","iopub.status.idle":"2022-05-28T03:19:12.856087Z","shell.execute_reply.started":"2022-05-28T03:19:11.41766Z","shell.execute_reply":"2022-05-28T03:19:12.854868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train_df['target']\nX=train_df.drop(['target'], axis=1)\nX=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)\nX_test=pd.DataFrame(scaler.transform(test_df))\n\ny_valid, nn_val_preds, nn_test_preds=[],[],[]\ncal_true, cal_pred=[],[]\nk_fold = KFold(n_splits=5, shuffle=True, random_state=21)\n\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)\n\nfor fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):\n    \n    print(\"\\n*****Fold {}*****\".format(fold+1))\n    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n    X_val, y_val = X.iloc[val_idx,:], y[val_idx]\n    print(\"Train shape: {}, {}, Valid shape: {}, {}\".format(\n        X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n    \n    with tpu_strategy.scope():\n\n        model = nn_model()\n        \n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                      loss=tf.keras.losses.BinaryCrossentropy(),\n                      metrics=[metrics.AUC(name = 'auc')])\n        \n        lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5,  patience=3, verbose=True)\n        es = EarlyStopping(monitor='val_auc', mode='max', patience=5, \n                           restore_best_weights=True, verbose=True)\n        \n        model.fit(X_train, y_train,\n                  validation_data=(X_val, y_val), \n                  epochs=50, batch_size=4096, \n                  callbacks=[es,lr], verbose=True, shuffle=True)\n        \n        nn_preds = model.predict(X_val).squeeze()\n        y_valid.append(y_val)\n        nn_val_preds.append(nn_preds)\n        nn_test_preds.append(model.predict(X_test).squeeze())\n        \n        prob_true, prob_pred = calibration_curve(y_val, nn_preds, n_bins=10)\n        cal_true.append(prob_true)\n        cal_pred.append(prob_pred)\n      \n    del X_train, y_train, X_val, y_val\n    gc.collect()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:19:12.857896Z","iopub.execute_input":"2022-05-28T03:19:12.858164Z","iopub.status.idle":"2022-05-28T03:28:15.128992Z","shell.execute_reply.started":"2022-05-28T03:19:12.858128Z","shell.execute_reply":"2022-05-28T03:28:15.127754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_calibration(y_valid, nn_val_preds, cal_true, cal_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:28:15.132027Z","iopub.execute_input":"2022-05-28T03:28:15.132351Z","iopub.status.idle":"2022-05-28T03:28:17.057257Z","shell.execute_reply.started":"2022-05-28T03:28:15.13231Z","shell.execute_reply":"2022-05-28T03:28:17.056634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Neural Network increased the Area Under the Curve by about 1.8% from the baseline, bringing the AUC to ~0.997 in each validation set. ","metadata":{}},{"cell_type":"code","source":"sub_nn=sub.copy()\nsub_nn['target']=np.mean(nn_test_preds, axis=0)\nsub_nn.to_csv(\"sub_nn.csv\", index=False)\nplot_target_predictions(sub_nn)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:28:17.058337Z","iopub.execute_input":"2022-05-28T03:28:17.059118Z","iopub.status.idle":"2022-05-28T03:28:19.325989Z","shell.execute_reply.started":"2022-05-28T03:28:17.059085Z","shell.execute_reply":"2022-05-28T03:28:19.32514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style='color:#38A6A5;text-align:center;font-size:90%'> Thank you for reading!<br>Please let me know if you have any questions and I look forward to any suggestions 🙂</p>","metadata":{}}]}