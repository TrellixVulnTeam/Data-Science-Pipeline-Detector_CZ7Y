{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Loading Data and Fetures","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from refence 1 notebook \nfor df in [train, test]:\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\ntest[features].head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.f_00[0:100].plot();\ntrain.f_00[0:100].ewm(com=0.8,min_periods=1).mean().fillna(0).plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Statistical Features\n* mean : --,average at axis =1\n* std : --,standart deviation at axis =1\n* sum : --,summation at axis =1\n* var : --,variance at axis =1\n* euc : reduce_euclidean_norm,`Computes the Euclidean norm of elements across dimensions of a tensor.`[*](http://www.tensorflow.org/api_docs/python/tf/math/reduce_euclidean_norm)\n* ptp : peak to peak,`Range of values (maximum - minimum) along an axis.`[**](https://numpy.org/doc/stable/reference/generated/numpy.ptp.html)\n* logsumexp : log(sum(exp(value)))`Computes log(sum(exp(elements across dimensions of a tensor))).`[***](https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp)\n\n\n","metadata":{}},{"cell_type":"code","source":"stat = [\"mean\",\"std\",\"sum\",\"var\",\"euc\",\"ptp\",\"logsumexp\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train.dtypes[train.dtypes==float].index.to_list()\ndef new_feats(df):\n    \n    df[\"mean\"] = df[cols].mean(axis=1).values\n    df[\"std\"] = df[cols].std(axis=1).values\n    df[\"sum\"] = df[cols].sum(axis=1).values\n    df[\"var\"] = df[cols].var(axis=1).values\n    df[\"euc\"] =tf.math.reduce_euclidean_norm(df[cols],axis=1).numpy()\n    df[\"ptp\"] = df[cols].values.ptp(axis=1)\n    df[\"logsumexp\"] = tf.math.reduce_logsumexp(df[cols],axis=1).numpy()\n    for col in cols:\n        df[f\"roll_{col}\"] = df[col].ewm(com=0.8,min_periods=1).mean().fillna(0)\n    \n    \nnew_feats(train)\nnew_feats(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rolls = [f for f in test.columns if f[0:4] == 'roll']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale_cols = [f for f in test.columns if train[f].dtype == float]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Standart scaling, standart scale only applied for float values, integers not scaled\nsc=StandardScaler()\nsc.fit(train[scale_cols].values)\nscaled_st_train = sc.transform(train[scale_cols].values)\nscaled_st_test = sc.transform(test[scale_cols].values)\ntrain[scale_cols] = scaled_st_train\ntest[scale_cols] = scaled_st_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[features+stat+rolls].values.reshape(8,8,train.shape[0],1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape np.array for convolution\ntraincon = train[features+stat+rolls].values.reshape(train.shape[0],8,8)\ntestcon = test[features+stat+rolls].values.reshape(test.shape[0],8,8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(traincon[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=train.target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting samples","metadata":{}},{"cell_type":"code","source":"total_sample = 20\nrandInt = np.array([random.choice(labels[labels==1].index) for x in range(int(total_sample/2))]+\n                        [random.choice(labels[labels==0].index) for x in range(int(total_sample/2)+1)])\nfig,axs=plt.subplots(nrows=5,ncols=4,figsize=(20,20))\nplt.subplots_adjust(wspace=-0.2, hspace=0.6)\nfor i, ax in enumerate(axs.flat):\n    pcm = ax.imshow(traincon[randInt[i]].squeeze() ,cmap=plt.cm.magma_r)\n    if labels[i]==0:\n        ax.set_title(f\"loc:{randInt[i]} state :{labels[i]}\",fontdict={\"color\":\"blue\"})\n    else :\n        ax.set_title(f\"loc:{randInt[i]} state :{labels[i]}\",fontdict={\"color\":\"green\"})\n    ax.set_xlabel(\"features\",fontdict={\"color\":\"green\"})\n    ax.set_ylabel(\"features\",fontdict={\"color\":\"green\"})\n    fig.colorbar(pcm, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cols = test.columns.to_list()\ntrain_cols.remove(\"id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(features+stat+rolls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\ndef plot_loss_auc(history,y_true,prediction):\n    \"\"\"\n    history: history = model.fit() \n    y_true: true validation set or test set labels\n    prediction: prediction on val set or test set\n    \"\"\"\n    fp, tp, _ = roc_curve(y_true, prediction)\n    _,ax = plt.subplots( ncols=4,nrows=1,figsize=(20,3))\n    ax[1].set_xlabel(\"epochs\")\n    ax[1].set_ylabel(\"loss\")\n    ax[1].set_title(\"final val_loss %1.4f\"%(history.history[\"val_loss\"][-1:][0]))\n    ax[2].set_xlabel(\"epochs\")\n    ax[2].set_ylabel(\"auc\")\n    ax[2].set_title(\"final val_auc %1.4f\"%(history.history[\"val_auc\"][-1:][0]))\n    ax[0].set_xlabel(\"learning rate\")\n    ax[0].set_ylabel(\"loss\")\n    ax[0].set_title(\"semilogx lr vs loss\")\n    ax[3].plot(fp, tp,label=\"ROC\", linewidth=2)\n    ax[3].vlines(x=0,ymin=0.0,ymax=1.0,linewidth=0.5,color=\"r\",linestyles=\"--\")\n    ax[3].hlines(y=1,xmin=0.0,xmax=1.0,linewidth=0.5,color=\"r\",linestyles=\"--\")\n    ax[3].set_xlabel('False positives')\n    ax[3].set_ylabel('True positives')\n    ax[3].set_title(\"ROC\")\n    ax[0].semilogx(history.history[\"lr\"], history.history[\"loss\"])\n    ax[0].set_ylim(ymax=0.11)\n    pd.DataFrame([history.history[\"auc\"],history.history[\"val_auc\"]],index=[\"auc\",\"val_auc\"]).T.plot(ax=ax[2])\n    pd.DataFrame([history.history[\"loss\"],history.history[\"val_loss\"]],index=[\"loss\",\"val_loss\"]).T.plot(ax=ax[1])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\ndef plot_cm(y_true, prediction, p=0.5):\n    \"\"\"\n    y_true: true validation set or test set labels\n    prediction: prediction on val set or test set\n    \n    \"\"\"\n    cm = confusion_matrix(y_true, prediction > p)\n    plt.figure(figsize=(3,3))\n    sns.heatmap(cm, annot=True, fmt=\"d\",cbar=False)\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    \n    print('\\nState 0 Detected (True Negatives): ', cm[0][0])\n    print('State 1 Incorrectly Detected (False Positives): ', cm[0][1])\n    print('State 1 Missed (False Negatives): ', cm[1][0])\n    print('State 1 Detected (True Positives): ', cm[1][1])\n    print('Total States : ', np.sum(cm[1]))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  this callback for verbose 0 console reporting, it doesn't dislay epoch steps.\nclass reps(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        \n        print(f\"epoch : {epoch}, loss : {logs.get('loss'):.4f}, val loss : {logs.get('val_loss'):.4f}, auc : {logs.get('auc'):.4f}, val auc : {logs.get('val_auc'):.4f}\")\n            \n\nreport_callback = reps()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ploting lrdecay ","metadata":{}},{"cell_type":"code","source":"epochs = 50\nexp=-0.05\nlr=0.001 # lr at begin\nstart = 10 #epoch\nmid = 17 #epoch\ndef lr_decay(epoch,lr):\n            if epoch< start:\n                return lr\n            elif epoch<mid:\n                return lr*(1 + np.cos(epoch / (epochs-1.5) * np.pi)) / 1.8 # from reference 1 notebook\n            else:\n                return lr * tf.math.exp(exp)\n\n\ndef plot_lr_decay(epochs,lr):\n    x = np.arange(0,epochs)\n    lrs = [] \n    lr2=lr\n    for epoch in x:\n        lr =  lr_decay(epoch,lr)\n        lrs.append(lr)\n    y = np.array(lrs)\n    plt.figure(figsize=(8,4))\n    plt.plot(x,y)\n    plt.vlines(x=start-1,linestyles=\"--\",colors=\"g\",ymin=y[-1],ymax=lr2,linewidth=0.95)\n    plt.vlines(x=mid-1,linestyles=\"--\",colors=\"orange\",ymin=y[-1],ymax=lr2,linewidth=0.95)\n    plt.vlines(x=35,linestyles=\"--\",colors=\"r\",ymin=y[-1],ymax=lr2,linewidth=0.95)\n    plt.hlines(y=0,linestyles=\"--\",colors=\"r\",xmin=start-1,xmax=50,linewidth=0.95)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"learning rate\")\n    plt.title(\"learning rate decay\")\nplot_lr_decay(epochs,lr)\nlrDecay = tf.keras.callbacks.LearningRateScheduler(lr_decay) # lr decay callback\ncallbacks = [lrDecay,\n            report_callback]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D,Conv2D,Flatten,Dropout,Conv1DTranspose,Conv2DTranspose,Dense,Reshape ,GlobalAveragePooling1D\nfrom tensorflow.keras import Input\nfrom tensorflow import keras\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"# 2d + 1d convolutions this model used for training\nlr = 0.001\ninput_shape = (8,8)\ndef cnn2():\n    \n    model = keras.Sequential(\n    [\n        Input(shape=input_shape),\n        Reshape(target_shape=(8, 8,1)),\n        Conv2D(\n            filters=144, kernel_size=(7,7), padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Dropout(rate=0.1),\n        Reshape(target_shape=(4*12, 4*12)),\n        Conv1D(\n            filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Conv1DTranspose(\n            filters=36, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n#         \n        Conv1DTranspose(\n            filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Dropout(rate=0.1),\n        Conv1DTranspose(\n            filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Flatten(),\n        Dropout(rate=0.4),\n        Dense(1,activation=\"sigmoid\")\n        ]\n    )   \n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    model.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics = [\"AUC\"])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1d convolutions \nlr = 0.001\ninput_shape = (8,8)\ndef cnn():\n    \n    model = keras.Sequential(\n    [\n        Input(shape=input_shape),\n#         Reshape(target_shape=(8, 8,1)),\n        Conv1D(\n            filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Dropout(rate=0.2),\n#         Reshape(target_shape=(4*12, 4*12)),\n        Conv1D(\n            filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Conv1DTranspose(\n            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n#         \n        Conv1DTranspose(\n            filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n        Dropout(rate=0.2),\n        Conv1DTranspose(\n            filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n        ),\n#         Conv1DTranspose(filters=3, kernel_size=7, padding=\"same\"),\n        Flatten(),\n#         Dropout(rate=0.4),\n        Dense(1,activation=\"sigmoid\")\n        ]\n    )   \n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    model.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics = [\"AUC\"])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = cnn2()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats\nfrom sklearn.model_selection import KFold,train_test_split","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"epochs = 45\nbatch_size = 32\npred_list = []\ncv = 2 # min value 2 for fold split\nverbose = 0 \nkf = KFold(n_splits=cv)\nfold_split = False # {True: KFold split | False: train_test_split} \nif fold_split:\n    for fold, (split_train, split_test) in enumerate(kf.split(train)):\n        print(\"\\n\",\"*=\"*10+\"*\",f\"fold {fold+1}\",\"*=\"*10+\"*\",\"\\n\")\n        X_train = traincon[split_train]\n        X_test = traincon[split_test]\n        y_train = labels[split_train]\n        y_test = labels[split_test]\n\n        with strategy.scope():\n            model = cnn2()\n            history = model.fit(X_train, y_train,batch_size=batch_size,epochs=epochs,callbacks=callbacks,\n                                validation_data=(X_test,y_test),verbose=verbose,shuffle=True,steps_per_epoch = X_train.shape[0]//batch_size)\n        pred_list.append(scipy.stats.rankdata(model.predict(testcon)))\n        pred_nn = ((model.predict(X_test).reshape(1,-1)[0])>0.5).astype(int)\n        plot_loss_auc(history,y_test,pred_nn)\n        plot_cm(y_test,pred_nn, p=0.5)\n\nelse: # train_test_split\n    random_states=np.linspace(789,9876,cv).astype(int)\n    print(\"random_states:\",random_states)\n    for fold,random_state in zip(range(cv),random_states):\n        print(\"\\n\",\"*=\"*10+\"*\",f\"fold {fold+1}\",\"*=\"*10+\"*\",\"\\n\")\n        X_train, X_test, y_train, y_test = train_test_split(traincon,labels,test_size=0.06,random_state=random_state)\n        with strategy.scope():\n            model = cnn2()\n            history = model.fit(X_train, y_train,batch_size=batch_size,epochs=45,callbacks=callbacks,\n                                validation_data=(X_test,y_test),verbose=verbose,shuffle=True,steps_per_epoch = X_train.shape[0]//batch_size)\n        pred_list.append(scipy.stats.rankdata(model.predict(testcon)))\n        pred_nn = ((model.predict(X_test).reshape(1,-1)[0])>0.5).astype(int)\n        plot_loss_auc(history,y_test,pred_nn)\n        plot_cm(y_test,pred_nn, p=0.5)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"\nmean_cnn = np.array(pred_list).mean(axis=0).reshape(1,-1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['target'] = mean_cnn\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n<p></p>\n\n[ref 1. tpsmay22-keras-quickstart from AMBROSM](https://www.kaggle.com/code/ambrosm/tpsmay22-keras-quickstart)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}