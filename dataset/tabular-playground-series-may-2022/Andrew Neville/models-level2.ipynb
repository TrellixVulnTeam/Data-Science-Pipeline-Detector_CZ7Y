{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import RobustScaler\n#from sklearn.model_selection import train_test_split\n#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n#from sklearn.linear_model import LinearRegression\nimport optuna\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check on the panda version and its dependencies\n# i run this from time to time to ensure all is up to date\npd.__version__\n#pd.show_versions()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect number of folds\nfold_no = df_train['kfolds'].max() +1\nfold_no","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taken from the hypertuning notebook\n\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'hist',\n    'use_label_encoder': False,\n    'n_estimators': 10000,\n    'learning_rate': 0.049543722885399176,\n    'reg_lambda': 1.878873269789419,\n    'reg_alpha': 0.1292588205628619,\n    'subsample': 0.4597818683023742,\n    'colsample_bytree': 0.7949921440875072,\n    'max_depth': 5}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(fold_no):\n    xtrain =  df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n#     xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n#     model = XGBRegressor(random_state=0, n_jobs=6) # i have 8 cores but want to keep 2 open\n    \n    model = XGBRegressor(random_state=0, n_jobs=-1, **params)\n       \n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    #rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    roc = roc_auc_score(yvalid, preds_valid)\n    print(fold, roc)\n    scores.append(roc)\n\nprint(np.mean(scores), np.std(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_1_20220524.csv\", index=False)\n\ndf_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\ndf_sampleSubmission.columns = [\"id\", \"pred_1\"]\ndf_sampleSubmission.to_csv(\"level1_test_pred_1_20220524.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column_names = [\"id\", \"target\"]\n\n# # df_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\n# df_sampleSubmission.to_csv(\"submission20220524d.csv\", header=column_names, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sampleSubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taken from the hypertuning notebook\n\nparams_cb = {\n    'loss_function': 'CrossEntropy',\n    'eval_metric': 'AUC',\n    'bootstrap_type': 'Bernoulli',\n    'n_estimators': 10000,\n    'learning_rate': 0.04639909669169314,\n    'l2_leaf_reg': 4.764270064283827,\n    'min_data_in_leaf': 47,\n    'depth': 6,\n    'leaf_estimation_iterations': 3,\n    'subsample': 0.8221602391299252}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(fold_no):\n    xtrain =  df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()    \n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n#     xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n#     model = XGBRegressor(random_state=0, n_jobs=6) # i have 8 cores but want to keep 2 open\n    \n    model = CatBoostClassifier(random_state=0,  **params_cb)\n       \n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict_proba(xvalid)[:, -1]\n    test_preds = model.predict_proba(xtest)[:, -1]\n    final_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))    \n    #rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    roc = roc_auc_score(yvalid, preds_valid)\n    print(fold, roc)\n    scores.append(roc)\n   \n    \nprint(np.mean(scores), np.std(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_2_20220524.csv\", index=False)\n\ndf_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\ndf_sampleSubmission.columns = [\"id\", \"pred_2\"]\ndf_sampleSubmission.to_csv(\"level1_test_pred_2_20220524.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column_names = [\"id\", \"target\"]\n\n# # df_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\n# df_sampleSubmission.to_csv(\"submission20220524e.csv\", header=column_names, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sampleSubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taken from the hypertuning notebook\n\nparams_lgb = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'n_estimators': 20000,\n    'learning_rate': 0.040751948678898225,\n    'reg_lambda': 0.0011184733873157485,\n    'reg_alpha': 0.18066237242292785,\n    'subsample': 0.24508506693514687,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.5051094430082244,\n    'min_child_weight': 3,\n    'min_child_samples': 126}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(fold_no):\n    xtrain =  df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()    \n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n#     xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n#     model = XGBRegressor(random_state=0, n_jobs=6) # i have 8 cores but want to keep 2 open\n    \n    model = lgb.LGBMClassifier(random_state=0, n_jobs=-1, **params_lgb)\n       \n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict_proba(xvalid)[:, -1]\n    test_preds = model.predict_proba(xtest)[:, -1]\n    final_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))    \n    #rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    roc = roc_auc_score(yvalid, preds_valid)\n    print(fold, roc)\n    scores.append(roc)\n\nprint(np.mean(scores), np.std(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_3_20220524.csv\", index=False)\n\ndf_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\ndf_sampleSubmission.columns = [\"id\", \"pred_3\"]\ndf_sampleSubmission.to_csv(\"level1_test_pred_3_20220524.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column_names = [\"id\", \"target\"]\n\n# # df_sampleSubmission.target = np.mean(np.column_stack(final_predictions), axis=1)\n# df_sampleSubmission.to_csv(\"submission20220524f.csv\", header=column_names, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sampleSubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}