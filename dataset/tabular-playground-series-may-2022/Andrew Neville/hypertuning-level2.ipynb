{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import RobustScaler\n#from sklearn.model_selection import train_test_split\n#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n#from sklearn.linear_model import LinearRegression\nimport optuna\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check on the panda version and its dependencies\n# i run this from time to time to ensure all is up to date\npd.__version__\n#pd.show_versions()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect number of folds\nfold_no = df_train['kfolds'].max() +1\nfold_no","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(trial):\n    fold = 0 # lets run over 1 fold only, not all the folds\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n\n    xtrain = df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n\n\n# do i still need to set use_label_encoder='false'?\n\n    model = XGBRegressor(\n        random_state=0,\n        tree_method=\"hist\",\n        eval_metric='auc',\n        objective='binary:logistic',\n        use_label_encoder='false',\n        n_jobs=-1,\n        n_estimators=10000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    roc = roc_auc_score(yvalid, preds_valid)\n    return roc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run, n_trials=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 14/5/2022\n\n# {'learning_rate': 0.04827433382927939,\n#  'reg_lambda': 36.38854397255708,\n#  'reg_alpha': 0.00020787799265006187,\n#  'subsample': 0.7939584892843453,\n#  'colsample_bytree': 0.7350355235544905,\n#  'max_depth': 2}\n\n# 0.972784862545458","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 21/5/2022\n\n# {'learning_rate': 0.03747017734195057,\n#  'reg_lambda': 0.00039998088402975774,\n#  'reg_alpha': 0.015311444035654846,\n#  'subsample': 0.9313237060297096,\n#  'colsample_bytree': 0.8135196342799582,\n#  'max_depth': 2}\n\n# 0.9900480410556488","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 23/5/2022\n\n# {'learning_rate': 0.03414055159418502,\n#  'reg_lambda': 5.841507236625909e-07,\n#  'reg_alpha': 1.2933505668277112e-05,\n#  'subsample': 0.3180298158205705,\n#  'colsample_bytree': 0.7009939864826797,\n#  'max_depth': 7}\n\n# 0.9900832566065393","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 24/5/2022\n\n# {'learning_rate': 0.027156139688948214,\n#  'reg_lambda': 6.284693161900589e-05,\n#  'reg_alpha': 6.763746178982408e-07,\n#  'subsample': 0.7857662348327735,\n#  'colsample_bytree': 0.9785863619292364,\n#  'max_depth': 3}\n\n# 0.9907230990193749","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_cb(trial):\n    fold = 0 # lets run over 1 fold only, not all the folds\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    l2_leaf_reg = trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10)\n    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 250)\n    depth = trial.suggest_int(\"depth\", 1, 7)\n    leaf_estimation_iterations=trial.suggest_int(\"leaf_estimation_iterations\", 1 , 10) \n    subsample=trial.suggest_float(\"subsample\", 1e-2, 1, log=True)\n\n    xtrain = df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n\n\n# do i still need to set use_label_encoder='false'?\n\n    model = CatBoostClassifier(\n        random_state=0,\n        loss_function=\"CrossEntropy\",\n        eval_metric='AUC',\n        bootstrap_type='Bernoulli',\n        n_estimators=10000,\n        learning_rate=learning_rate,\n        l2_leaf_reg=l2_leaf_reg,\n        min_data_in_leaf=min_data_in_leaf,\n        depth=depth,\n        leaf_estimation_iterations=leaf_estimation_iterations,\n        subsample=subsample,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict_proba(xvalid)[:, -1]\n    roc = roc_auc_score(yvalid, preds_valid)\n    return roc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run_cb, n_trials=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 14/5/2022\n\n# {'learning_rate': 0.01785878100577341,\n#  'l2_leaf_reg': 4.04235486206517,\n#  'min_data_in_leaf': 5,\n#  'depth': 5,\n#  'leaf_estimation_iterations': 1,\n#  'subsample': 0.5162065044245909}\n\n# 0.9727399956695909","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 21/5/2022\n\n# {'learning_rate': 0.030090670734864194,\n#  'l2_leaf_reg': 9.007007254109197,\n#  'min_data_in_leaf': 156,\n#  'depth': 4,\n#  'leaf_estimation_iterations': 10,\n#  'subsample': 0.22381455661476138}\n\n# 0.9899503993719659","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 23/5/2022\n\n# {'learning_rate': 0.038248260169775346,\n#  'l2_leaf_reg': 3.3729308831930993,\n#  'min_data_in_leaf': 114,\n#  'depth': 3,\n#  'leaf_estimation_iterations': 9,\n#  'subsample': 0.6576236055970964}\n\n# 0.9900696906307224","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 24/5/2022\n\n# {'learning_rate': 0.1357006347561937,\n#  'l2_leaf_reg': 7.668752327125118,\n#  'min_data_in_leaf': 142,\n#  'depth': 4,\n#  'leaf_estimation_iterations': 4,\n#  'subsample': 0.7886238858489611}\n\n# 0.9906441382320859","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfolds-5/train_folds_5.csv is an output from https://www.kaggle.com/code/andrewnuk/kfolds-5\n\ndf_train = pd.read_csv('/kaggle/input/kfolds-5/train_folds_5.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\ndf_sampleSubmission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output files from https://www.kaggle.com/code/andrewnuk/models-level1-stacking/notebook\n\ndf1 = pd.read_csv(\"../input/models-level1-stacking/train_pred_1_FE_20220524.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/models-level1-stacking/train_pred_2_FE_20220524.csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/models-level1-stacking/train_pred_3_FE_20220524.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\n\ndf_test1 = pd.read_csv(\"../input/models-level1-stacking/test_pred_1_FE_20220524.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/models-level1-stacking/test_pred_2_FE_20220524.csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/models-level1-stacking/test_pred_3_FE_20220524.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\n\ndf_train = df_train.merge(df1, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df2, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_lgb(trial):\n    fold = 0 # lets run over 1 fold only, not all the folds\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)    \n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)    \n    subsample_freq = trial.suggest_int(\"subsample_freq\", 1, 10)    \n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 350)\n    min_child_samples = trial.suggest_int(\"min_child_samples\", 1, 250)\n\n\n    xtrain = df_train[df_train['kfolds'] != fold].reset_index(drop=True)\n    xvalid = df_train[df_train['kfolds'] == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n#     xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n\n\n# do i still need to set use_label_encoder='false'?\n\n    model = lgb.LGBMClassifier(\n        random_state=0,\n        objective='binary',\n        metric='auc',\n        n_estimators=20000,\n        learning_rate =learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        subsample_freq=subsample_freq,\n        colsample_bytree=colsample_bytree,\n        min_child_weight=min_child_weight,\n        min_child_samples=min_child_samples,\n)\n    \n    model.fit(xtrain, ytrain, early_stopping_rounds=1000, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict_proba(xvalid)[:, -1]\n    roc = roc_auc_score(yvalid, preds_valid)\n    return roc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run_lgb, n_trials=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 14/5/2022\n\n# {'learning_rate': 0.09487432843504863,\n#  'reg_lambda': 1.8360888906724572e-08,\n#  'reg_alpha': 0.0003764803574815724,\n#  'subsample': 0.13870435688141589,\n#  'subsample_freq': 8,\n#  'colsample_bytree': 0.997890142817157,\n#  'min_child_weight': 155,\n#  'min_child_samples': 51}\n\n# 0.9724389946784788","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 21/5/2022\n\n# {'learning_rate': 0.17818666911001538,\n#  'reg_lambda': 0.01936480267082536,\n#  'reg_alpha': 0.00040662863909439586,\n#  'subsample': 0.8859563624407328,\n#  'subsample_freq': 10,\n#  'colsample_bytree': 0.695155159055407,\n#  'min_child_weight': 149,\n#  'min_child_samples': 48\n \n# 0.9898674099724503","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 23/5/2022\n\n# {'learning_rate': 0.0885347675461033,\n#  'reg_lambda': 1.2701699792245506e-05,\n#  'reg_alpha': 6.357172866480366e-07,\n#  'subsample': 0.4391508507045103,\n#  'subsample_freq': 4,\n#  'colsample_bytree': 0.952787779118162,\n#  'min_child_weight': 251,\n#  'min_child_samples': 196}\n\n# 0.9899642526293053","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 24/5/2022\n\n# {'learning_rate': 0.11344474715935352,\n#  'reg_lambda': 0.0009274217057765754,\n#  'reg_alpha': 0.013111761237607475,\n#  'subsample': 0.6006055511455205,\n#  'subsample_freq': 1,\n#  'colsample_bytree': 0.5587762899397076,\n#  'min_child_weight': 41,\n#  'min_child_samples': 167}\n\n# 0.9904058041442192","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}