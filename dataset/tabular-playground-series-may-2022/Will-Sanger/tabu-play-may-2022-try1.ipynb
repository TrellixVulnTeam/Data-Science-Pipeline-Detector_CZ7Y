{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Data competition -- May 2022\n\nMy strategy will be to quickly get to a prediction using DL, then use the model to help with EDA.\n\nA couple main sources:\n- [`fastai` tabular data tutorial](https://docs.fast.ai/tutorial.tabular.html)\n- [\"Iterate Like A Grandmaster\" kaggle kernel\"](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster)","metadata":{}},{"cell_type":"markdown","source":"# 0. Get environment set up\n\nRun first:\n- Import useful stuff using fastai (pd, np, plt, etc.)\n- Check if this notebook runs on kaggle or somewhere else\n\nIf you're on kaggle, do this first:\n- If you're doing any actual training, add a GPU via three-dot menu at right -> \"Accelerators\"\n- If you don't have the data saved locally, and you're running this on Kaggle, then use [\"+ Add Data\"](https://www.kaggle.com/docs/notebooks#datasets) in the right-hand sidebar\n\n\nIf you're not on kaggle:\n- pip install [kaggle Python API](https://www.kaggle.com/code/donkeys/kaggle-python-api/notebook)\n- set up kaggle token","metadata":{}},{"cell_type":"code","source":"from fastai.imports import *\nimport os\niskaggle = len(os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')) > 0\n\nif not iskaggle:\n    print(\"We're running this notebook somewhere besides kaggle\")\n    !pip install kaggle\n    ### CHECK THIS -- it may be missing pieces!\n    # if runninng on non-kaggle system, replace following\n    # with token from Kaggle user-profile section\n    # with JSON format like\n    # creds = '{\"username\":\"xxx\",\"key\":\"xxx\"}'\n    creds = ''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T20:47:56.255645Z","iopub.execute_input":"2022-05-31T20:47:56.255929Z","iopub.status.idle":"2022-05-31T20:47:56.712506Z","shell.execute_reply.started":"2022-05-31T20:47:56.255896Z","shell.execute_reply":"2022-05-31T20:47:56.711632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install and/or import some stuff that's specific to tabular data\n\n!pip install -Uqq waterfallcharts treeinterpreter dtreeviz\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\nfrom sklearn.inspection import plot_partial_dependence\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nimport multiprocessing as mp","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:32:42.738999Z","iopub.execute_input":"2022-05-31T21:32:42.739896Z","iopub.status.idle":"2022-05-31T21:32:53.291909Z","shell.execute_reply.started":"2022-05-31T21:32:42.739849Z","shell.execute_reply":"2022-05-31T21:32:53.290861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"has_gpu = torch.cuda.is_available(); print(\"Has GPU:\", has_gpu)\nprint(\"Torch version:\", torch.__version__)\nimport fastai\nprint(\"Fast-ai version:\", fastai.__version__)\nimport sys\nprint(\"Python version:\", sys.version)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:48:15.322706Z","iopub.execute_input":"2022-05-31T20:48:15.323003Z","iopub.status.idle":"2022-05-31T20:48:15.330892Z","shell.execute_reply.started":"2022-05-31T20:48:15.322967Z","shell.execute_reply":"2022-05-31T20:48:15.330151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load data","metadata":{}},{"cell_type":"markdown","source":"### get path to the data\n\nIf not on kaggle, use [\"kaggle competitions download\"](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data) to download data","metadata":{}},{"cell_type":"code","source":"path = (Path('../input/tabular-playground-series-may-2022') if iskaggle\n    else Path.home()/'data'/'tabular-playground-series-may-2022')\n\nif not iskaggle and not path.exists():\n    from zipfile import ZipFile\n    api.competition_download_cli(str(path))\n    ZipFile(f'{path}.zip').extractall(path)\n    \npath.ls()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:48:15.332696Z","iopub.execute_input":"2022-05-31T20:48:15.33313Z","iopub.status.idle":"2022-05-31T20:48:15.348313Z","shell.execute_reply.started":"2022-05-31T20:48:15.33309Z","shell.execute_reply":"2022-05-31T20:48:15.347481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data!","metadata":{}},{"cell_type":"code","source":"# low_memory=False means pandas can read the full dataset at once\n# --this may help Pandas avoid changing dtypes between rows\ndf = pd.read_csv(path/'train.csv', low_memory=False)\ndf.T","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:48:15.350124Z","iopub.execute_input":"2022-05-31T20:48:15.350607Z","iopub.status.idle":"2022-05-31T20:48:54.613022Z","shell.execute_reply.started":"2022-05-31T20:48:15.350567Z","shell.execute_reply":"2022-05-31T20:48:54.612334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_shrink(df)\ndf.corr();df.info();","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:48:54.61447Z","iopub.execute_input":"2022-05-31T20:48:54.614869Z","iopub.status.idle":"2022-05-31T20:49:01.037159Z","shell.execute_reply.started":"2022-05-31T20:48:54.614831Z","shell.execute_reply":"2022-05-31T20:49:01.036148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature engineering for the text column\n\nMy tree-based learners haven't been using Column 27 enough.\n\nPer [this kernel by c4rl05/v](https://www.kaggle.com/code/cv13j0/tps-may22-eda-gbdt), which has been using the text columns a lot (high feature importance), I am going to try two changes:\n- convert characters to a numeric format using `ord` built-in function\n- add a feature: count of number of unique characters in column 27","metadata":{}},{"cell_type":"code","source":"# what the heck is column 27?\nprint(\"Number of unique values:\", df['f_27'].nunique())\nprint(\"Some examples:\")\ndf['f_27'].sample(n=5)\n## This might just be a 'red herring' column\n## I am going to ignore it for now.","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:01.038931Z","iopub.execute_input":"2022-05-31T20:49:01.039228Z","iopub.status.idle":"2022-05-31T20:49:01.095647Z","shell.execute_reply.started":"2022-05-31T20:49:01.039188Z","shell.execute_reply":"2022-05-31T20:49:01.094724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def substr_cols(df, colname):\n    '''split a string column into one column per position in string\n    also turn characters into numeric values in case these are handled\n    better by lightgbm and xgboost\n    '''\n    maxchars = max(df[colname].str.len())\n    for i in range(maxchars):\n        subcolname = colname+\"_\"+str(i).zfill(len(str(maxchars)))\n        # get(i) would just get the ith character in the string\n        # ord function converts this character into a numeric value\n        df[subcolname] = df[colname].str.get(i).apply(ord) - ord('A')\n    return df\n\ndef prep_text_col(df, colname):\n    '''\n    splits text column into sub-string columns (one character per column),\n    converts this column into numeric format,\n    counts unique character occurences in this string column,\n    and then drops the original text column\n    '''\n    df = substr_cols(df,colname)\n    df[colname+\"_nuniq_chars\"] = df[colname].apply(lambda s: len(set(s)))\n    return df.drop(labels=[colname], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:01.097123Z","iopub.execute_input":"2022-05-31T20:49:01.097977Z","iopub.status.idle":"2022-05-31T20:49:01.107158Z","shell.execute_reply.started":"2022-05-31T20:49:01.097935Z","shell.execute_reply":"2022-05-31T20:49:01.106216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = prep_text_col(df, \"f_27\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:01.111307Z","iopub.execute_input":"2022-05-31T20:49:01.111598Z","iopub.status.idle":"2022-05-31T20:49:13.470779Z","shell.execute_reply.started":"2022-05-31T20:49:01.111569Z","shell.execute_reply":"2022-05-31T20:49:13.469929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count-occurences and count-chars features\n\n\nI'll try doing this as well.","metadata":{}},{"cell_type":"code","source":"def count_unique_chars(df, colname):\n    '''\n    Count number of unique characters in a text column\n    '''\n    maxchars = max(df[colname].str.len())\n\n    for i in range(maxchars):\n        df[f'ch_{i}'] = df[colname].str.get(i).apply(ord) - ord('A')\n    \n    new_colname = colname + \"_chars_uniq\"\n    df[new_colname] = df[colname].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:13.472313Z","iopub.execute_input":"2022-05-31T20:49:13.472598Z","iopub.status.idle":"2022-05-31T20:49:13.481454Z","shell.execute_reply.started":"2022-05-31T20:49:13.47256Z","shell.execute_reply":"2022-05-31T20:49:13.480607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Deep-learning-based model using fastai\n\nThis follows the [`fastai` tabular learner tutorial](https://docs.fast.ai/tutorial.tabular)","metadata":{}},{"cell_type":"code","source":"cont_names, cat_names = cont_cat_split(df)\nL(cont_names).remove(\"id\"); L(cat_names).remove(\"target\")\nsplits = RandomSplitter(valid_pct=0.2)(range_of(df))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:13.482967Z","iopub.execute_input":"2022-05-31T20:49:13.483734Z","iopub.status.idle":"2022-05-31T20:49:13.796801Z","shell.execute_reply.started":"2022-05-31T20:49:13.483688Z","shell.execute_reply":"2022-05-31T20:49:13.795913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Ratio of 1s to 0s:\",len(df[df['target']==1]) / len(df))\nfor c in cat_names:\n    print(f\"{c} uniques:\", df[c].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:13.798524Z","iopub.execute_input":"2022-05-31T20:49:13.79879Z","iopub.status.idle":"2022-05-31T20:49:13.997361Z","shell.execute_reply.started":"2022-05-31T20:49:13.798756Z","shell.execute_reply":"2022-05-31T20:49:13.996559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to = TabularPandas(df, procs = [Categorify, FillMissing, Normalize],\n    y_names = \"target\", y_block = CategoryBlock,\n    cat_names = cat_names, cont_names = cont_names,\n    splits = splits)\n\ndls = to.dataloaders(bs=2**10)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:13.998688Z","iopub.execute_input":"2022-05-31T20:49:13.998976Z","iopub.status.idle":"2022-05-31T20:49:17.110713Z","shell.execute_reply.started":"2022-05-31T20:49:13.998936Z","shell.execute_reply":"2022-05-31T20:49:17.109898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note I'm using a pretty big batch size (2**13).\n\nI did some tests using the training code below. Small batch sizes trained slowly and I had tons of GPU memory left.\n\nLarger batch sizes trained faster -- up to a point.\n\nHere are my tests:\n\n| Batch size                 | 65536 | 16384 | 4096 | 1024 |\n|----------------------------|-------|-------|------|------|\n| iterations to 81% accuracy | 25    | 10    | 6    | 4    |\n| seconds to 81% accuracy    | 50    | 20    | 24   | 44   |","metadata":{}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:17.112153Z","iopub.execute_input":"2022-05-31T20:49:17.11241Z","iopub.status.idle":"2022-05-31T20:49:18.320877Z","shell.execute_reply.started":"2022-05-31T20:49:17.112376Z","shell.execute_reply":"2022-05-31T20:49:18.320096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = tabular_learner(dls, metrics=[accuracy,RocAucBinary()], loss_func=CrossEntropyLossFlat(),\n                        layers=[2048,1024,512,250,100,64,64,16], cbs=[ShowGraphCallback()],\n#                         opt_func=Adam(params=params, lr=self.lr)\n                        opt_func=Adam\n                       )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:18.325534Z","iopub.execute_input":"2022-05-31T20:49:18.327894Z","iopub.status.idle":"2022-05-31T20:49:18.415623Z","shell.execute_reply.started":"2022-05-31T20:49:18.327848Z","shell.execute_reply":"2022-05-31T20:49:18.414381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.opt_func; learn.loss_func; learn.opt_func","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:18.420381Z","iopub.execute_input":"2022-05-31T20:49:18.42108Z","iopub.status.idle":"2022-05-31T20:49:18.438542Z","shell.execute_reply.started":"2022-05-31T20:49:18.421032Z","shell.execute_reply":"2022-05-31T20:49:18.43384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:18.440401Z","iopub.execute_input":"2022-05-31T20:49:18.440749Z","iopub.status.idle":"2022-05-31T20:49:26.566384Z","shell.execute_reply.started":"2022-05-31T20:49:18.440707Z","shell.execute_reply":"2022-05-31T20:49:26.565622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(15, lr_max=0.1, wd=1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:49:26.567623Z","iopub.execute_input":"2022-05-31T20:49:26.568539Z","iopub.status.idle":"2022-05-31T20:54:08.467189Z","shell.execute_reply.started":"2022-05-31T20:49:26.568496Z","shell.execute_reply":"2022-05-31T20:54:08.466357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_y_valid = learn.get_preds(dl=dls.valid)[0]\nprint(\"dl alone:\")\nprint(roc_auc_score(dls.valid.y,dl_y_valid.argmax(dim=1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:54:08.468848Z","iopub.execute_input":"2022-05-31T20:54:08.469137Z","iopub.status.idle":"2022-05-31T20:54:10.089845Z","shell.execute_reply.started":"2022-05-31T20:54:08.469099Z","shell.execute_reply":"2022-05-31T20:54:10.088932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:54:10.091364Z","iopub.execute_input":"2022-05-31T20:54:10.091867Z","iopub.status.idle":"2022-05-31T20:54:10.375944Z","shell.execute_reply.started":"2022-05-31T20:54:10.091824Z","shell.execute_reply":"2022-05-31T20:54:10.375168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit(10, wd=0.1, lr=10**-1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:54:10.377307Z","iopub.execute_input":"2022-05-31T20:54:10.378116Z","iopub.status.idle":"2022-05-31T20:56:54.957242Z","shell.execute_reply.started":"2022-05-31T20:54:10.37807Z","shell.execute_reply":"2022-05-31T20:56:54.956485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ten more epochs","metadata":{}},{"cell_type":"code","source":"learn.fit(10, wd=0.1, lr=10**-2)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:56:54.958841Z","iopub.execute_input":"2022-05-31T20:56:54.95935Z","iopub.status.idle":"2022-05-31T20:59:38.802917Z","shell.execute_reply.started":"2022-05-31T20:56:54.959311Z","shell.execute_reply":"2022-05-31T20:59:38.802074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit(10, wd=0.1, lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:59:38.804714Z","iopub.execute_input":"2022-05-31T20:59:38.805009Z","iopub.status.idle":"2022-05-31T21:02:23.486567Z","shell.execute_reply.started":"2022-05-31T20:59:38.804969Z","shell.execute_reply":"2022-05-31T21:02:23.485638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(15, lr_max=0.1, wd=1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:02:23.491617Z","iopub.execute_input":"2022-05-31T21:02:23.491859Z","iopub.status.idle":"2022-05-31T21:07:06.659081Z","shell.execute_reply.started":"2022-05-31T21:02:23.491832Z","shell.execute_reply":"2022-05-31T21:07:06.658277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"dl alone:\")\nprint(roc_auc_score(dls.valid.y,dl_y_valid.argmax(dim=1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:18.916297Z","iopub.execute_input":"2022-05-31T21:12:18.917194Z","iopub.status.idle":"2022-05-31T21:12:19.005225Z","shell.execute_reply.started":"2022-05-31T21:12:18.917141Z","shell.execute_reply":"2022-05-31T21:12:19.004171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:20.263308Z","iopub.execute_input":"2022-05-31T21:12:20.263808Z","iopub.status.idle":"2022-05-31T21:12:23.996976Z","shell.execute_reply.started":"2022-05-31T21:12:20.263768Z","shell.execute_reply":"2022-05-31T21:12:23.996101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results();","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:30.081175Z","iopub.execute_input":"2022-05-31T21:12:30.081486Z","iopub.status.idle":"2022-05-31T21:12:30.379732Z","shell.execute_reply.started":"2022-05-31T21:12:30.081452Z","shell.execute_reply":"2022-05-31T21:12:30.378898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Random forests model, then some EDA","metadata":{}},{"cell_type":"code","source":"xs, y = to.train.xs, to.train.y","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:32.441519Z","iopub.execute_input":"2022-05-31T21:12:32.442107Z","iopub.status.idle":"2022-05-31T21:12:32.529234Z","shell.execute_reply.started":"2022-05-31T21:12:32.442066Z","shell.execute_reply":"2022-05-31T21:12:32.528362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = tree.DecisionTreeClassifier(max_leaf_nodes=4)\nm.fit(xs, y);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:33.203116Z","iopub.execute_input":"2022-05-31T21:12:33.203403Z","iopub.status.idle":"2022-05-31T21:12:45.049164Z","shell.execute_reply.started":"2022-05-31T21:12:33.203372Z","shell.execute_reply":"2022-05-31T21:12:45.048335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_tree(m)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:45.050895Z","iopub.execute_input":"2022-05-31T21:12:45.051187Z","iopub.status.idle":"2022-05-31T21:12:45.55815Z","shell.execute_reply.started":"2022-05-31T21:12:45.051148Z","shell.execute_reply":"2022-05-31T21:12:45.557315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"here's the first column the tree split on:\\n\", xs.columns[23])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:45.559571Z","iopub.execute_input":"2022-05-31T21:12:45.560068Z","iopub.status.idle":"2022-05-31T21:12:45.565617Z","shell.execute_reply.started":"2022-05-31T21:12:45.560023Z","shell.execute_reply":"2022-05-31T21:12:45.564491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -Uqq dtreeviz\n\nimport dtreeviz","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:45.567855Z","iopub.execute_input":"2022-05-31T21:12:45.568153Z","iopub.status.idle":"2022-05-31T21:12:55.781395Z","shell.execute_reply.started":"2022-05-31T21:12:45.568113Z","shell.execute_reply":"2022-05-31T21:12:55.780239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txs, ty = to.valid.xs, to.valid.y","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:55.783764Z","iopub.execute_input":"2022-05-31T21:12:55.784114Z","iopub.status.idle":"2022-05-31T21:12:55.811952Z","shell.execute_reply.started":"2022-05-31T21:12:55.784069Z","shell.execute_reply":"2022-05-31T21:12:55.81106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random forest can train on both train and validation data\n# still use the to. version of data since it's been pre-processed\nrf_xs = pd.concat([to.train.xs, to.valid.xs]).sort_index()\nrf_y = pd.concat([to.train.y, to.valid.y]).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:13:45.171075Z","iopub.execute_input":"2022-05-31T21:13:45.171373Z","iopub.status.idle":"2022-05-31T21:13:45.758665Z","shell.execute_reply.started":"2022-05-31T21:13:45.17134Z","shell.execute_reply":"2022-05-31T21:13:45.757744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This doesn't work, could try to read docs at\n# https://github.com/parrt/dtreeviz\nsamp_idx = np.random.permutation(len(y))[:500]\n# dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, \"value\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:07:35.559911Z","iopub.execute_input":"2022-05-31T21:07:35.560248Z","iopub.status.idle":"2022-05-31T21:07:35.585636Z","shell.execute_reply.started":"2022-05-31T21:07:35.560207Z","shell.execute_reply":"2022-05-31T21:07:35.584504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying this time with RAPIDS (\"cuML\") Random Forest\n\nNote annoying limitations:\n- Doesn't have a way to compute OOB score\n- Doesn't record feature importance\n\nHowever, it is sooo much faster than training with scikit-learn random forest,\nso I decided to use it to train a big forest.","metadata":{}},{"cell_type":"code","source":"from cuml import RandomForestClassifier as cuRF","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:14:03.811774Z","iopub.execute_input":"2022-05-31T21:14:03.812066Z","iopub.status.idle":"2022-05-31T21:14:07.041296Z","shell.execute_reply.started":"2022-05-31T21:14:03.812036Z","shell.execute_reply":"2022-05-31T21:14:07.040359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def rf(xs, y, n_estimators=100, max_samples=100000,\n#        max_features=0.8, min_samples_leaf=1, **kwargs):\n#     return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n#         max_samples=max_samples, max_features=max_features,\n#         criterion=\"entropy\", n_jobs: mp.cpu_count()\n#         min_samples_leaf=min_samples_leaf, oob_score=True,).fit(xs, y)\n\ndef rf(xs, y, n_estimators=1000, max_samples=0.5, n_bins=256,\n       max_features=0.8, min_samples_leaf=1, **kwargs):\n    return cuRF(n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        split_criterion=1,\n        min_samples_leaf=min_samples_leaf).fit(xs, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:32:35.321411Z","iopub.execute_input":"2022-05-31T21:32:35.322024Z","iopub.status.idle":"2022-05-31T21:32:35.33059Z","shell.execute_reply.started":"2022-05-31T21:32:35.321986Z","shell.execute_reply":"2022-05-31T21:32:35.32949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = rf(rf_xs, rf_y);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:32:56.205092Z","iopub.execute_input":"2022-05-31T21:32:56.20551Z","iopub.status.idle":"2022-05-31T21:34:52.122142Z","shell.execute_reply.started":"2022-05-31T21:32:56.205453Z","shell.execute_reply":"2022-05-31T21:34:52.121245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## OOB error degraded when I changed max_features from \"0.5\" to \"sqrt\"\n# print(\"OOB score (higher is better):\",m.oob_score_)\nprint(\"AUC, higher is better:\", roc_auc_score(rf_y,m.predict(rf_xs)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:34:52.123966Z","iopub.execute_input":"2022-05-31T21:34:52.124261Z","iopub.status.idle":"2022-05-31T21:34:58.94006Z","shell.execute_reply.started":"2022-05-31T21:34:52.124218Z","shell.execute_reply":"2022-05-31T21:34:58.938074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance","metadata":{}},{"cell_type":"code","source":"# def rf_feat_importance(m, df):\n#     return pd.DataFrame({'cols':df.columns, 'imp': m.feature_importances_}\n#                        ).sort_values('imp', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:11.707162Z","iopub.status.idle":"2022-05-31T21:12:11.707888Z","shell.execute_reply.started":"2022-05-31T21:12:11.707608Z","shell.execute_reply":"2022-05-31T21:12:11.707637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fi = rf_feat_importance(m, rf_xs)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:11.709115Z","iopub.status.idle":"2022-05-31T21:12:11.709811Z","shell.execute_reply.started":"2022-05-31T21:12:11.709558Z","shell.execute_reply":"2022-05-31T21:12:11.709584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot_fi(fi):\n#     return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\n# plot_fi(fi);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:11.711075Z","iopub.status.idle":"2022-05-31T21:12:11.712026Z","shell.execute_reply.started":"2022-05-31T21:12:11.71178Z","shell.execute_reply":"2022-05-31T21:12:11.711805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO: Replace this with https://github.com/parrt/random-forest-importances\n# from sklearn.inspection import plot_partial_dependence\n# from sklearn.inspection import PartialDependenceDisplay\n# fig,ax = plt.subplots(figsize=(12,4))\n# PartialDependenceDisplay.from_estimator(m, xs,\n#                                         [\"f_26\", \"f_21\", \"f_27_08\"],\n#                                         grid_resolution=20,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:11.713437Z","iopub.status.idle":"2022-05-31T21:12:11.71433Z","shell.execute_reply.started":"2022-05-31T21:12:11.714088Z","shell.execute_reply":"2022-05-31T21:12:11.714113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Very quick gradient boosting","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:34:58.941571Z","iopub.execute_input":"2022-05-31T21:34:58.942261Z","iopub.status.idle":"2022-05-31T21:34:59.055412Z","shell.execute_reply.started":"2022-05-31T21:34:58.942195Z","shell.execute_reply":"2022-05-31T21:34:59.054616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_m = xgb.XGBClassifier(n_estimators = 5000, learning_rate=0.05, num_leaves=50,\n                          colsample_bytree=0.9, predictor=\"gpu_predictor\",\n                          min_child_weight=0.96,\n                          subsample=0.8, objective=\"binary:logistic\", eval_metric=\"auc\",\n                         enable_categorical=True, tree_method=\"gpu_hist\")\nxgb_m_f = xgb_m.fit(to.train.xs, to.train.y)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:34:59.058046Z","iopub.execute_input":"2022-05-31T21:34:59.058357Z","iopub.status.idle":"2022-05-31T21:35:44.613309Z","shell.execute_reply.started":"2022-05-31T21:34:59.058317Z","shell.execute_reply":"2022-05-31T21:35:44.61243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we could test for XGBoost: change max_depth","metadata":{}},{"cell_type":"markdown","source":"# 6. Very quick lightgbm test\n\nDid some quick parameter testing based on https://neptune.ai/blog/lightgbm-parameters-guide\n\nwhich is partly based on this amazing post: https://sites.google.com/view/lauraepp/parameters\n\n...and then I just copied parameters used in this nice Kaggle kernel:\nhttps://www.kaggle.com/code/jitensharma597/tsp2022-lgbm-light-gradient-boosting-machine#Modeling:-LGBM\n(but without messing with the 5-fold CV)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:35:44.616739Z","iopub.execute_input":"2022-05-31T21:35:44.617018Z","iopub.status.idle":"2022-05-31T21:35:45.422199Z","shell.execute_reply.started":"2022-05-31T21:35:44.61698Z","shell.execute_reply":"2022-05-31T21:35:45.421238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_m = lgb.LGBMClassifier(n_estimators = 5000, learning_rate=0.05, num_leaves=50,\n                          colsample_bytree=0.9, min_child_samples=96,\n                          max_bins=255,\n                          subsample=0.8, objective=\"binary\", metric=\"auc\", device=\"gpu\")\nlg_m.fit(to.train.xs, to.train.y)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:19:41.602235Z","iopub.execute_input":"2022-05-31T22:19:41.602521Z","iopub.status.idle":"2022-05-31T22:23:45.474102Z","shell.execute_reply.started":"2022-05-31T22:19:41.602484Z","shell.execute_reply":"2022-05-31T22:23:45.473296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Test ensembling models","metadata":{}},{"cell_type":"code","source":"dl_y_valid = learn.get_preds(dl=dls.valid)[0]\nrf_y_valid = Tensor(m.predict_proba(txs))\nxg_y_valid = Tensor(xgb_m_f.predict_proba(txs))\nlg_y_valid = Tensor(lg_m.predict_proba(txs))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:23:45.475974Z","iopub.execute_input":"2022-05-31T22:23:45.476335Z","iopub.status.idle":"2022-05-31T22:25:29.955499Z","shell.execute_reply.started":"2022-05-31T22:23:45.476294Z","shell.execute_reply":"2022-05-31T22:25:29.954491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"dl alone:\")\nprint(roc_auc_score(ty,dl_y_valid.argmax(dim=1)))\nprint(\"rf alone:\")\nprint(roc_auc_score(ty,rf_y_valid.argmax(dim=1)))\nprint(\"xgb alone:\")\nprint(roc_auc_score(ty,xg_y_valid.argmax(dim=1)))\nprint(\"lightgbm alone:\")\nprint(roc_auc_score(ty,lg_y_valid.argmax(dim=1)))\nprint(\"ensemble by average of dl and rf:\")\nprint(roc_auc_score(ty,((dl_y_valid+rf_y_valid)/2).argmax(dim=1)))\n\nprint(\"two-to-one ensemble by average of dl and rf:\")\nprint(roc_auc_score(ty,((2*dl_y_valid+rf_y_valid)/3).argmax(dim=1)))\n\nprint(\"ensemble by average of dl and xg:\")\nprint(roc_auc_score(ty,((dl_y_valid+xg_y_valid)/2).argmax(dim=1)))\n\nprint(\"ensemble by average of dl and lg:\")\nprint(roc_auc_score(ty,((dl_y_valid+lg_y_valid)/2).argmax(dim=1)))\n\nprint(\"ensemble by average of dl, rf, and lg:\")\nprint(roc_auc_score(ty,((3*dl_y_valid+rf_y_valid+lg_y_valid)/5).argmax(dim=1)))\n\nprint(\"ensemble by average of dl, rf, and xg:\")\nprint(roc_auc_score(ty,((dl_y_valid+rf_y_valid+xg_y_valid)/3).argmax(dim=1)))\n\nprint(\"ensemble by average of dl, rf, xgb, and lightgbm:\")\nprint(roc_auc_score(ty,((2*dl_y_valid+2*rf_y_valid+lg_y_valid+xg_y_valid)/6).argmax(dim=1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:51:02.011672Z","iopub.execute_input":"2022-05-31T22:51:02.012023Z","iopub.status.idle":"2022-05-31T22:51:03.140781Z","shell.execute_reply.started":"2022-05-31T22:51:02.011987Z","shell.execute_reply":"2022-05-31T22:51:03.139945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now let's try inference and submit some results\n\n[This tutorial](https://fastai1.fast.ai/tutorial.inference.html#Tabular) only works for single items.\n\nWe need to do something like `get_preds` instead.\n\nBut first, need to load the test data.","metadata":{}},{"cell_type":"code","source":"learn.save('mini_train')\nlearn.export()\ndf_test = pd.read_csv(path/\"test.csv\")\ndf_test = prep_text_col(df_test, \"f_27\")\ntest_dl = learn.dls.test_dl(df_test)\n# kaggle competitions submit -c tabular-playground-series-may-2022 -f submission.csv -m \"Message\"","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:33:14.883408Z","iopub.execute_input":"2022-05-31T22:33:14.883834Z","iopub.status.idle":"2022-05-31T22:33:31.94735Z","shell.execute_reply.started":"2022-05-31T22:33:14.883797Z","shell.execute_reply":"2022-05-31T22:33:31.946469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get probabilities on the test dataset\ndlpreds = learn.get_preds(dl=test_dl)[0]\nrfpreds = m.predict_proba(test_dl.xs)\nxgpreds = xgb_m_f.predict_proba(test_dl.xs)\nlgpreds = lg_m.predict_proba(test_dl.xs)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:33:31.949152Z","iopub.execute_input":"2022-05-31T22:33:31.949469Z","iopub.status.idle":"2022-05-31T22:40:09.765022Z","shell.execute_reply.started":"2022-05-31T22:33:31.949411Z","shell.execute_reply":"2022-05-31T22:40:09.764098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble the results\nens_three_preds = ((3*dlpreds+lgpreds+rfpreds)/5).argmax(dim=1)\nens_two_preds = ((3*dlpreds+2*rfpreds)/5).argmax(dim=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:51:38.015494Z","iopub.execute_input":"2022-05-31T22:51:38.016062Z","iopub.status.idle":"2022-05-31T22:51:38.324478Z","shell.execute_reply.started":"2022-05-31T22:51:38.01602Z","shell.execute_reply":"2022-05-31T22:51:38.323722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens_two_preds_probs = ((3*dlpreds+2*rfpreds)/5)[:,1]","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:57:16.553364Z","iopub.execute_input":"2022-05-31T22:57:16.553757Z","iopub.status.idle":"2022-05-31T22:57:16.574384Z","shell.execute_reply.started":"2022-05-31T22:57:16.553717Z","shell.execute_reply":"2022-05-31T22:57:16.573502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds = pd.DataFrame({\"id\":df_test[\"id\"].values, \"target\": ens_two_preds_probs[:,1]})\ndf_preds.to_csv(\"2022-05-31_1758_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:58:14.819276Z","iopub.execute_input":"2022-05-31T22:58:14.819995Z","iopub.status.idle":"2022-05-31T22:58:16.567014Z","shell.execute_reply.started":"2022-05-31T22:58:14.819956Z","shell.execute_reply":"2022-05-31T22:58:16.566269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ideas to try next:\n\n- Do data exploration and modeling in some kind of loop\n- Is this the right loss function?\n\n### Data exploration\n- [Correlations cross-plot](https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1)\n- Find top predictive features (how to do this?)\n- Plot some stuff!!\n- Find top losses\n- [MC dropout](https://docs.fast.ai/callback.preds.html#MCDropoutCallback) and find high-uncertainty rows\n- [Pair Plots](https://seaborn.pydata.org/examples/scatterplot_matrix.html) of top features?\n\n### Modeling\n1. Random forests\n2. XG Boosts\n3. Random forests using an embedding from DL model\n4. Combine some models (average)\n\n### DL refinements\n1. Learning rate finder\n2. Schedule learning rate (cosine saw or something)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}