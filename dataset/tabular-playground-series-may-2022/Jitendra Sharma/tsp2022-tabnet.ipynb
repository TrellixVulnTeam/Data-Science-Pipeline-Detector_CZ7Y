{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\npip install pytorch-tabnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-15T15:00:35.038998Z","iopub.execute_input":"2022-05-15T15:00:35.03951Z","iopub.status.idle":"2022-05-15T15:00:50.025267Z","shell.execute_reply.started":"2022-05-15T15:00:35.039413Z","shell.execute_reply":"2022-05-15T15:00:50.023624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import roc_curve, roc_auc_score, mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport string\nimport random\nimport time\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:00:50.027179Z","iopub.execute_input":"2022-05-15T15:00:50.027446Z","iopub.status.idle":"2022-05-15T15:00:53.000491Z","shell.execute_reply.started":"2022-05-15T15:00:50.027412Z","shell.execute_reply":"2022-05-15T15:00:52.999619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    input = \"../input/tabular-playground-series-may-2022\"\n    \n    n_splits = 10\n    seed     = 42\n    n_bins   = 50\n    \n    target   = 'target'\n    tab_pred = 'tab_pred'\n    pred     = 'pred'\n    \n    int1_features = ['f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12',\n                     'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18']\n    int2_features = ['f_29', 'f_30']\n    int_features  = int1_features + int2_features\n    \n    float1_features = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06']\n    float2_features = ['f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26']\n    float3_features = ['f_28']\n    float_features  = float1_features + float2_features + float3_features","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:01:49.188235Z","iopub.execute_input":"2022-05-15T15:01:49.188522Z","iopub.status.idle":"2022-05-15T15:01:49.195631Z","shell.execute_reply.started":"2022-05-15T15:01:49.188491Z","shell.execute_reply":"2022-05-15T15:01:49.194495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms = True\n    \nseed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:01:51.611514Z","iopub.execute_input":"2022-05-15T15:01:51.611872Z","iopub.status.idle":"2022-05-15T15:01:51.622126Z","shell.execute_reply.started":"2022-05-15T15:01:51.611833Z","shell.execute_reply":"2022-05-15T15:01:51.621243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Reference](https://www.kaggle.com/code/aboriginal3153/tps-mar-22-neural-network-by-pytorch)","metadata":{}},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\n# test_df  = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\n# print(train_df.shape)\n# print(test_df.shape)\n# train_df.head()\n\ntrain = pd.read_csv(\"/\".join([CFG.input, \"train.csv\"]))\ntest  = pd.read_csv(\"/\".join([CFG.input, \"test.csv\"]))\nsubmission = pd.read_csv(\"/\".join([CFG.input, \"sample_submission.csv\"]))\n\nprint(train.shape)\nprint(test.shape)\nprint(submission.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:02:39.018954Z","iopub.execute_input":"2022-05-15T15:02:39.020054Z","iopub.status.idle":"2022-05-15T15:02:54.45722Z","shell.execute_reply.started":"2022-05-15T15:02:39.019987Z","shell.execute_reply":"2022-05-15T15:02:54.456023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n## Feature Engineering","metadata":{}},{"cell_type":"code","source":"all_df = pd.concat([train, test]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:03:24.67163Z","iopub.execute_input":"2022-05-15T15:03:24.672477Z","iopub.status.idle":"2022-05-15T15:03:25.20003Z","shell.execute_reply.started":"2022-05-15T15:03:24.672424Z","shell.execute_reply":"2022-05-15T15:03:25.198569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_df.shape)\nall_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:04:23.731223Z","iopub.execute_input":"2022-05-15T15:04:23.732112Z","iopub.status.idle":"2022-05-15T15:04:23.792385Z","shell.execute_reply.started":"2022-05-15T15:04:23.732051Z","shell.execute_reply":"2022-05-15T15:04:23.79142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class feature_engineering:\n    def __init__(self, df):\n        self.df = df\n        self.f_27_len = len(self.df['f_27'][0])\n        self.alphabet_upper = list(string.ascii_uppercase)\n        \n    def get_features(self):\n        for i in range(10):\n            self.df[f'ch{i}'] = self.df.f_27.str.get(i).apply(ord) - ord('A')\n            self.df[\"unique_characters\"] = self.df.f_27.apply(lambda s: len(set(s)))\n            self.df['i_02_21'] = (self.df.f_21 + self.df.f_02 > 5.2).astype(int) - (self.df.f_21 + self.df.f_02 < -5.3).astype(int)\n            self.df['i_05_22'] = (self.df.f_22 + self.df.f_05 > 5.1).astype(int) - (self.df.f_22 + self.df.f_05 < -5.4).astype(int)\n            i_00_01_26 = self.df.f_00 + self.df.f_01 + self.df.f_26\n            self.df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n        \n        return self.df\n        \n#     def get_features(self):\n#         for i in range(self.f_27_len):\n#             self.df[f'f_27_{i}'] = self.df['f_27'].apply(lambda x: x[i])\n\n#         for letter in tqdm(self.alphabet_upper):\n#             self.df[f'f_27_{letter}_count'] = self.df['f_27'].str.count(letter)\n\n#         self.df['f_27_nunique'] = self.df['f_27'].apply(lambda x: len(set(x)))\n\n#         return self.df\n    \n    def scaling(self, features):\n        sc = StandardScaler()\n        self.df[features] = sc.fit_transform(self.df[features])\n\n        return self.df\n\n    def label_encoding(self, features):\n        new_features = []\n        \n        for feature in features:\n            if self.df[feature].dtype == 'O':\n                le = LabelEncoder()\n                self.df[f'{feature}_enc'] = le.fit_transform(self.df[feature])\n                new_features.append(f'{feature}_enc')\n            else:\n                new_features.append(feature)\n\n        return self.df, new_features\n    \n    def onehot_encoding(self, features):\n        new_features = []\n        self.df = pd.get_dummies(self.df, columns=features)\n        \n        feats = [col for col in self.df.columns if CFG.target not in col]\n        for feat in feats:\n            if self.df[feat].dtype == 'uint8':\n                new_features.append(feat)\n\n        return self.df, new_features","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:16:54.125262Z","iopub.execute_input":"2022-05-15T15:16:54.125576Z","iopub.status.idle":"2022-05-15T15:16:54.147913Z","shell.execute_reply.started":"2022-05-15T15:16:54.125544Z","shell.execute_reply":"2022-05-15T15:16:54.146733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfe     = feature_engineering(all_df)\nall_df = fe.get_features()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:16:55.875726Z","iopub.execute_input":"2022-05-15T15:16:55.876698Z","iopub.status.idle":"2022-05-15T15:17:36.552377Z","shell.execute_reply.started":"2022-05-15T15:16:55.876657Z","shell.execute_reply":"2022-05-15T15:17:36.551379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_df.shape)\nall_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:17:36.5543Z","iopub.execute_input":"2022-05-15T15:17:36.554835Z","iopub.status.idle":"2022-05-15T15:17:36.594034Z","shell.execute_reply.started":"2022-05-15T15:17:36.554786Z","shell.execute_reply":"2022-05-15T15:17:36.592669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [col for col in all_df.columns if CFG.target not in col]\nnum_features = []\ncat_features = []\n\nfor feature in features:\n    if all_df[feature].dtype == float:\n        num_features.append(feature)\n    else:\n        cat_features.append(feature)\n\ncat_features.remove('id')\ncat_features.remove('f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:17:36.595593Z","iopub.execute_input":"2022-05-15T15:17:36.596226Z","iopub.status.idle":"2022-05-15T15:17:36.606864Z","shell.execute_reply.started":"2022-05-15T15:17:36.596181Z","shell.execute_reply":"2022-05-15T15:17:36.605802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling and encoding","metadata":{}},{"cell_type":"code","source":"all_df, cat_features = fe.label_encoding(cat_features)\n\nall_df       = fe.scaling(num_features)\nall_features = cat_features + num_features","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:18:05.93807Z","iopub.execute_input":"2022-05-15T15:18:05.939224Z","iopub.status.idle":"2022-05-15T15:18:06.3756Z","shell.execute_reply.started":"2022-05-15T15:18:05.939172Z","shell.execute_reply":"2022-05-15T15:18:06.373803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_len = train.shape[0]\ntrain     = all_df[:train_len]\ntest      = all_df[train_len:].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:18:18.245893Z","iopub.execute_input":"2022-05-15T15:18:18.246216Z","iopub.status.idle":"2022-05-15T15:18:18.312213Z","shell.execute_reply.started":"2022-05-15T15:18:18.246182Z","shell.execute_reply":"2022-05-15T15:18:18.310222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train[all_features])\ndisplay(test[all_features])","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:18:27.237214Z","iopub.execute_input":"2022-05-15T15:18:27.23764Z","iopub.status.idle":"2022-05-15T15:18:27.446077Z","shell.execute_reply.started":"2022-05-15T15:18:27.237581Z","shell.execute_reply":"2022-05-15T15:18:27.444973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabNet","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=train[CFG.target])):\n    X_train = train[all_features].to_numpy()[trn_idx]\n    y_train = train[CFG.target].to_numpy()[trn_idx]\n    X_valid = train[all_features].to_numpy()[val_idx]\n    y_valid = train[CFG.target].to_numpy()[val_idx]\n    X_test = test[all_features].to_numpy()\n    \n    print(f\"===== FOLD {fold} =====\")\n    \n    tabnet_params = dict(\n        n_d=64,\n        n_steps=5,\n        gamma=1.3,\n        n_independent=3,\n        n_shared=3,\n        seed=CFG.seed,\n        momentum=2e-2,\n        lambda_sparse=1e-6,\n\n        optimizer_fn=torch.optim.Adam,\n        optimizer_params=dict(\n            lr=1e-2,\n            weight_decay=1e-7\n        ),\n        \n        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n        scheduler_params=dict(\n            mode='max',\n            factor=0.9,\n            patience=3,\n            min_lr=1e-6,\n        ),\n        verbose=10,\n        device_name='auto',\n        mask_type='sparsemax',\n    )\n    \n    # Defining TabNet model\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(\n        X_train=X_train,\n        y_train=y_train,\n        from_unsupervised=None,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_name=[\"train\", \"valid\"],\n        eval_metric=[\"auc\"],\n        batch_size=2048,\n        virtual_batch_size=2048,\n        max_epochs=200,\n        drop_last=True,\n        pin_memory=True,\n        patience=20,\n        num_workers=4,\n    )\n\n    train.loc[val_idx, CFG.tab_pred] = model.predict_proba(X_valid)[:, -1]\n    print(f\"auc score: {roc_auc_score(y_true=y_valid, y_score=train.loc[val_idx, CFG.tab_pred]):.6f}\\n\")\n    \n    test[f'{CFG.tab_pred}_{fold}'] = model.predict_proba(X_test)[:, -1]\n\nprint(f\"auc score : {roc_auc_score(y_true=train[CFG.target], y_score=train[CFG.tab_pred]):.6f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:20:12.281919Z","iopub.execute_input":"2022-05-15T15:20:12.282535Z","iopub.status.idle":"2022-05-15T15:20:30.219877Z","shell.execute_reply.started":"2022-05-15T15:20:12.282487Z","shell.execute_reply":"2022-05-15T15:20:30.217951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [col for col in test.columns if CFG.tab_pred in col]\n\nsubmission[CFG.target] = test[cols].mean(axis=1)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:19:41.687723Z","iopub.execute_input":"2022-05-14T18:19:41.688163Z","iopub.status.idle":"2022-05-14T18:19:43.524095Z","shell.execute_reply.started":"2022-05-14T18:19:41.688119Z","shell.execute_reply":"2022-05-14T18:19:43.523281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to obtain the activation function\n# def get_activation(activation_name):\n#     if activation_name == 'Relu':\n#         activation = F.relu\n#     elif activation_name == 'ELU':\n#         activation = F.elu\n#     else:\n#         activation = F.leaky_relu\n#     return activation\n\n# # Function to get optimize method\n# def get_optimizer(model, optimizer_name, lr, weight_decay):\n#     if optimizer_name == 'MomentumSGD': \n#         optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n#     elif optimizer_name == 'Adam':\n#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n#     elif optimizer_name == 'Adagrad':\n#         optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=weight_decay)      \n#     else:\n#         optimizer = torch.optim.RMSprop(model.parameters())\n#     return optimizer\n\n# # Function to train Neural Network\n# def train(model, train_dataloader, optimizer):\n#     # check whether GPU is available\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     model.to(device)\n#     # Define the error function\n#     criterion = nn.MSELoss()\n#     #　Model in learning mode\n#     model.train()\n#     #　If the network is somewhat fixed, make it faster\n#     torch.backends.cudnn.benchark = True\n#     # epoch loss\n#     epoch_loss = 0\n#     iteration = 0\n#     for batch, (data, target) in enumerate(train_dataloader):\n#         data, target = data.to(device), target.to(device)\n#         optimizer.zero_grad()\n#         output = model(data)\n#         output = output.view(1, -1)[0]\n#         # print(output.shape, target.shape)\n#         target = target.to(torch.float32)\n#         loss   = criterion(output, target)\n#         epoch_loss += loss.item()\n#         loss.backward()\n#         optimizer.step()\n#         iteration += 1\n#     epoch_loss /= iteration\n#     return epoch_loss\n\n# # Function for prediction\n# def predict(model, dataloader):\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     model.eval()\n#     model.to(device)\n#     y_pred = np.array([])\n#     with torch.no_grad():\n#         for data in dataloader:\n#             data = data[0]\n#             #output = model(data)\n#             output = model(data.to(device))\n#             output = output.view(1, -1)\n#             output = output.to('cpu').detach().numpy().copy()\n#             #output = output.to(device)\n#             y_pred = np.append(y_pred, output[0])\n#         y_pred = np.array(y_pred)\n#     return y_pred\n\n# # Function for plot loss function of each epoch\n# def loss_plot(logs_train, logs_valid):\n#     plt.plot(logs_train[0][1:], logs_train[1][1:], '-b', label='train')\n#     plt.plot(logs_valid[0][1:], logs_valid[1][1:], '-r', label='test')\n#     plt.xlabel('epoch')\n#     plt.ylabel('loss')\n#     plt.legend()\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params = {'num_layer': 2, \n#           'num_nodes_0': 24, \n#           'num_nodes_1': 12, \n#           'dropout_rate': 0.5, \n#           'activation': 'leaky_relu', \n#           'optimizer': 'Adam', \n#           'weight_decay': 1e-10, \n#           'Adam_lr': 0.001}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n# ### prameter\n# k_split = 10\n# num_epochs = 100\n# batch_size = 64\n# ###\n\n# # k-fold cross-validation\n# kfold = StratifiedKFold(n_splits=k_split,random_state=1, shuffle=True).split(X_train_std, y_train)\n# #### get parameter\n# num_layer       = params['num_layer']\n# num_nodes       = [int(params[s]) for s in params.keys() if 'num_nodes' in s]\n# dropout_rate    = params['dropout_rate']\n# activation_name = params['activation']\n# optimizer_name  = params['optimizer']\n# lr              = params[optimizer_name+'_lr']\n# weight_decay    = params['weight_decay']\n# ######\n\n# scores = []   # list to save score \n# models = []   # list to save model\n# for k, (train_id, test_id) in enumerate(kfold):\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     # Instantiate Model\n#     model = Net(input_size=X_train_std.shape[1],\n#                 num_layer=num_layer, \n#                 num_nodes=num_nodes, \n#                 dropout_rate=dropout_rate, \n#                 activation_name=activation_name)\n#     # model to GPU\n#     model.to(device)\n#     optimizer = get_optimizer(model, optimizer_name, lr, weight_decay)\n#     # data to dataloader\n#     dataset          = torch.utils.data.TensorDataset(torch.Tensor(X_train_std[train_id]), \n#                                              torch.tensor(y_train[train_id]))\n#     train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n#     valid_dataset    = torch.utils.data.TensorDataset(torch.Tensor(X_train_std[test_id]))\n#     valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n#     # training each epoch\n#     logs_train = [[0], [np.inf]]\n#     logs_valid = [[0], [np.inf]]\n#     for epoch in tqdm(range(num_epochs)):\n#         epoch_loss = train(model, train_dataloader, optimizer)\n#         valid_pred = predict(model, valid_dataloader)\n#         valid_loss = mean_squared_error(y_train[test_id], valid_pred)\n#         if epoch_loss < min(logs_valid[1]):\n#             torch.save(model.state_dict(), './models'+str(k))\n#         logs_train[0].append(epoch+1)\n#         logs_train[1].append(epoch_loss)\n#         logs_valid[0].append(epoch+1)\n#         logs_valid[1].append(valid_loss)   \n#     # valid\n#     model.load_state_dict(torch.load('./models'+str(k)))\n#     pred_y_k = predict(model, valid_dataloader)\n#     # score\n#     score = roc_auc_score(y_train[test_id], pred_y_k)\n#     print('Fold: %2d, AUC: %.3f' % (k+1, score))\n#     scores.append(score)\n#     models.append(model)\n#     loss_plot(logs_train, logs_valid)\n# print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation with testdata\n* Define a function to predict and summarize the results in each of the models created by the k-fold cross-validation.","metadata":{}},{"cell_type":"code","source":"# def predict_kfold(models, X_test):\n#     # Create array for storing test data\n#     y_pred = np.zeros((len(X_test), len(models)))\n#     # Crate dataloader\n#     test_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test))\n#     test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n#     for fold_, model_ in enumerate(models):\n#         model_.load_state_dict(torch.load('./models'+str(fold_)))\n#         # predict\n#         pred_ = predict(model_, test_dataloader)\n#         # store\n#         y_pred[:, fold_] = pred_ \n#     y_pred = y_pred.mean(axis=1)\n#     return y_pred\n# y_pred = predict_kfold(models, X_test_std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # calclate auc and roc curves and evaluate performance on test data\n# roc = roc_curve(y_test, y_pred)\n# print(\"roc\", roc_auc_score(y_test, y_pred))\n# fpr, tpr, thresholds = roc\n# plt.plot(fpr, tpr, marker='o')\n# plt.xlabel('FPR: False positive rate')\n# plt.ylabel('TPR: True positive rate')\n# plt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Try the same calculations on training data\n# y_pred_train = predict_kfold(models, X_train_std)\n# roc = roc_curve(y_train, y_pred_train)\n# print(\"roc\", roc_auc_score(y_train, y_pred_train))\n# fpr, tpr, thresholds = roc\n# plt.plot(fpr, tpr, marker='o')\n# plt.xlabel('FPR: False positive rate')\n# plt.ylabel('TPR: True positive rate')\n# plt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit data\n* Apply model to test_df and create submit data","metadata":{}},{"cell_type":"code","source":"# X_submit = test_df.values\n# X_submit_std = stdsc.transform(X_submit)\n# y_submit = predict_kfold(models, X_submit_std)\n# print(y_submit)\n# print(y_submit.shape)\n# plt.hist(y_submit, bins=30, density=True)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv')\n# print(submission_df.shape)\n# submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df['target'] = pd.DataFrame(y_submit)\n# submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n## A method of hyperparameter tuning using a technique called `optuna`\n\n* First, split the training data into data used for training and data used for tuning. And then **standardize.**","metadata":{}},{"cell_type":"code","source":"# valid_size = 0.1\n# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size,\n#                                                       stratify=y_train)\n# print(X_train.shape, X_valid.shape)\n# print(y_train.shape, y_valid.shape)\n\n# stdsc = StandardScaler()\n# X_train_std   = stdsc.fit_transform(X_train)\n# X_valid_std   = stdsc.transform(X_valid)\n# train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train_std), torch.tensor(y_train))\n# valid_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_valid_std), torch.tensor(y_valid))\n# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tuning is performed below using optuna. Here, we define trial and explore each parameter.**","metadata":{}},{"cell_type":"code","source":"# # define the class\n# class Net(nn.Module):\n#     def __init__(self, trial, input_size, num_layer, num_nodes, dropout_rate):\n#         super(Net, self).__init__()\n#         self.activation = get_activation(trial)\n#         self.linears = nn.ModuleList([nn.Linear(input_size, num_nodes[0])])\n#         self.batchnorms = nn.ModuleList([nn.BatchNorm1d(num_nodes[0])])\n#         for i in range(1, num_layer):\n#             self.linears.append(nn.Linear(num_nodes[i-1], num_nodes[i]))\n#             self.batchnorms.append(nn.BatchNorm1d(num_nodes[i]))\n#         self.fcl = nn.Linear(num_nodes[-1], 1)\n#         self.dropout = nn.Dropout(dropout_rate)\n\n#     def forward(self, x):\n#         for i, d in enumerate(zip(self.linears, self.batchnorms)):\n#             l, b = d[0], d[1]\n#             x = b(self.activation(l(x)))\n#             x = self.dropout(x)\n#         x = torch.sigmoid(self.fcl(x))\n#         return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(model, device, train_dataloader, optimizer):\n#     model.train()\n#     criterion = nn.MSELoss()\n#     for batch, (data, target) in enumerate(train_dataloader):\n#         data, target = data.to(device), target.to(device)\n#         optimizer.zero_grad()\n#         output = model(data)\n#         output = output.view(1, -1)[0]\n#         # print(output.shape, target.shape)\n#         target = target.to(torch.float32)\n#         loss = criterion(output, target)\n#         loss.backward()\n#         optimizer.step()\n# def test(model, device, valid_dataloader):\n#     model.eval()\n#     criterion = nn.MSELoss()\n#     loss = 0\n#     iteration = 0\n#     with torch.no_grad():\n#         for data, target in valid_dataloader:\n#             data, target = data.to(device), target.to(device)\n#             output = model(data)\n#             output = output.view(1, -1)[0]\n#             loss += criterion(output, target)\n#             iteration += 1\n#     loss /= iteration\n#     return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_optimizer(trial, model):\n#     optimizer_names = ['MomentumSGD', 'Adam', 'Adagrad']\n#     optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n#     if optimizer_name == optimizer_names[0]: \n#         momentum_sgd_lr = trial.suggest_loguniform('Momentum_SGD_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.SGD(model.parameters(), lr=momentum_sgd_lr, momentum=0.9, weight_decay=weight_decay)\n#     elif optimizer_name == optimizer_names[1]:\n#         adam_lr = trial.suggest_loguniform('Adam_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.Adam(model.parameters(), lr=adam_lr, weight_decay=weight_decay)\n#     elif optimizer_name == optimizer_names[2]:\n#         adagrad_lr = trial.suggest_loguniform('Adagrad_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.Adagrad(model.parameters(), lr=adagrad_lr, weight_decay=weight_decay)      \n#     return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_activation(trial):\n#     activation_names = ['ReLU', 'ELU', 'leaky_relu']\n#     activation_name = trial.suggest_categorical('activation', activation_names)\n#     if activation_name == activation_names[0]:\n#         activation = F.relu\n#     elif activation_name == activation_names[1]:\n#         activation = F.elu\n#     else:\n#         activation = F.leaky_relu\n#     return activation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 30\n# def objective(trial):\n#     device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n#     # hidden layer\n#     num_layer = trial.suggest_int('num_layer', 2, 7)\n#     # the number of nodes\n#     num_nodes = [int(trial.suggest_discrete_uniform('num_nodes_'+str(i), 16, 128, 16)) for i in range(num_layer)]\n#     # dropout ratio\n#     dropout_rate = trial.suggest_float('dropout_rate', 0.0, 1.0)\n\n#     model = Net(trial, X_train.shape[1],num_layer, num_nodes, dropout_rate).to(device)\n#     optimizer = get_optimizer(trial, model)\n#     error_rate = 0\n#     for epoch in range(epochs):\n#         train(model, device, train_dataloader, optimizer)\n#     error_rate = test(model, device, valid_dataloader)\n#     return error_rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# TRIAL_SIZE = 100\n# study = optuna.create_study()\n# study.optimize(objective, n_trials=TRIAL_SIZE)\n# best_params = study.best_params\n# print(best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # params = best_params\n# params={'num_layer': 4, 'num_nodes_0': 128.0, 'num_nodes_1': 112.0, 'num_nodes_2': 96.0,\n#         'num_nodes_3': 96.0, 'dropout_rate': 0.08387843261849516, 'activation': 'ReLU',\n#         'optimizer': 'Adam', 'weight_decay': 2.227219890291524e-09, 'Adam_lr': 0.0019802197708342255}","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}