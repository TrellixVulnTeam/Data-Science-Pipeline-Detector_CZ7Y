{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **[Reference 1: LGBM](https://www.kaggle.com/code/akioonodera/tps-may2022-lgbm-binary)**\n\n# **[Reference 2: LGBM](https://www.kaggle.com/code/kellibelcher/tps-may-2022-eda-lgbm-neural-networks/notebook#3-|-Feature-Engineering)**","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport time\nimport gc\nimport numpy as np\nimport pandas as pd\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n#import optuna.integration.lightgbm as lgb\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error, roc_curve, roc_auc_score, mean_squared_error, r2_score, auc\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.inspection import PartialDependenceDisplay\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport seaborn as sns\nsns.set()\n\nimport missingno as msno\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\n\ninit_notebook_mode(connected=True)\ncolor=px.colors.qualitative.Plotly\ntemp=dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12), \n                           height=500, width=1000))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#################################\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from pytorch_tabnet.pretraining import TabNetPretrainer\n# from pytorch_tabnet.tab_model import TabNetClassifier\n\n%matplotlib inline\n# pd.options.display.max_rows = 100\n# pd.options.display.max_columns = 100","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:58:48.117292Z","iopub.execute_input":"2022-05-17T20:58:48.11766Z","iopub.status.idle":"2022-05-17T20:58:52.574897Z","shell.execute_reply.started":"2022-05-17T20:58:48.117551Z","shell.execute_reply":"2022-05-17T20:58:52.574054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 600)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:13.326223Z","iopub.execute_input":"2022-05-17T20:59:13.326498Z","iopub.status.idle":"2022-05-17T20:59:13.331152Z","shell.execute_reply.started":"2022-05-17T20:59:13.326468Z","shell.execute_reply":"2022-05-17T20:59:13.330469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    input = \"../input/tabular-playground-series-may-2022\"\n    \n    n_splits = 10\n    seed     = 42\n    n_bins   = 50\n    \n    target   = 'target'\n    tab_pred = 'tab_pred'\n    pred     = 'pred'\n    \n    int1_features = ['f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12',\n                     'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18']\n    int2_features = ['f_29', 'f_30']\n    int_features  = int1_features + int2_features\n    \n    float1_features = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06']\n    float2_features = ['f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26']\n    float3_features = ['f_28']\n    float_features  = float1_features + float2_features + float3_features","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:14.409276Z","iopub.execute_input":"2022-05-17T20:59:14.410049Z","iopub.status.idle":"2022-05-17T20:59:14.416614Z","shell.execute_reply.started":"2022-05-17T20:59:14.410012Z","shell.execute_reply":"2022-05-17T20:59:14.415863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.use_deterministic_algorithms = True\n    \nseed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:17.163641Z","iopub.execute_input":"2022-05-17T20:59:17.164245Z","iopub.status.idle":"2022-05-17T20:59:17.17032Z","shell.execute_reply.started":"2022-05-17T20:59:17.164189Z","shell.execute_reply":"2022-05-17T20:59:17.169635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\n# test_df  = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\n# print(train_df.shape)\n# print(test_df.shape)\n# train_df.head()\n\ntrain      = pd.read_csv(\"/\".join([CFG.input, \"train.csv\"]))\ntest       = pd.read_csv(\"/\".join([CFG.input, \"test.csv\"]))\nsubmission = pd.read_csv(\"/\".join([CFG.input, \"sample_submission.csv\"]))\n\nprint(train.shape)\nprint(test.shape)\nprint(submission.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:17.420967Z","iopub.execute_input":"2022-05-17T20:59:17.421503Z","iopub.status.idle":"2022-05-17T20:59:33.114668Z","shell.execute_reply.started":"2022-05-17T20:59:17.421469Z","shell.execute_reply":"2022-05-17T20:59:33.113637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:33.116299Z","iopub.execute_input":"2022-05-17T20:59:33.11652Z","iopub.status.idle":"2022-05-17T20:59:34.410465Z","shell.execute_reply.started":"2022-05-17T20:59:33.116493Z","shell.execute_reply":"2022-05-17T20:59:34.409643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:34.411692Z","iopub.execute_input":"2022-05-17T20:59:34.412057Z","iopub.status.idle":"2022-05-17T20:59:35.377684Z","shell.execute_reply.started":"2022-05-17T20:59:34.412025Z","shell.execute_reply":"2022-05-17T20:59:35.37703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:35.379209Z","iopub.execute_input":"2022-05-17T20:59:35.380148Z","iopub.status.idle":"2022-05-17T20:59:35.432991Z","shell.execute_reply.started":"2022-05-17T20:59:35.380099Z","shell.execute_reply":"2022-05-17T20:59:35.432285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\", \"lightseagreen\",\n          \"cornflowerblue\", \"mediumpurple\", \"palevioletred\", \"lightskyblue\", \"sandybrown\",\n          \"yellowgreen\", \"indianred\", \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:35.434104Z","iopub.execute_input":"2022-05-17T20:59:35.434851Z","iopub.status.idle":"2022-05-17T20:59:35.440095Z","shell.execute_reply.started":"2022-05-17T20:59:35.434807Z","shell.execute_reply":"2022-05-17T20:59:35.439185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA of Training data\n\nfigure = plt.figure(figsize=(16, 8))\nfor feat in range(31):\n    feat_name = f'f_{feat:02d}'\n    if(feat_name != 'f_27'):\n        plt.subplot(8, 4, feat+1)\n        plt.hist(train[feat_name], bins=100)\n        plt.title(f'{feat_name}')\n\nfigure.tight_layout(h_pad=1.0, w_pad=0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:35.441353Z","iopub.execute_input":"2022-05-17T20:59:35.441661Z","iopub.status.idle":"2022-05-17T20:59:47.218751Z","shell.execute_reply.started":"2022-05-17T20:59:35.441626Z","shell.execute_reply":"2022-05-17T20:59:47.217685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA of Test data\n\nfigure = plt.figure(figsize=(16, 8))\nfor feat in range(31):\n    feat_name = f'f_{feat:02d}'\n    if(feat_name != 'f_27'):\n        plt.subplot(8, 4, feat+1)\n        plt.hist(test[feat_name], bins=100)\n        plt.title(f'{feat_name}')\n\nfigure.tight_layout(h_pad=1.0, w_pad=0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:47.220535Z","iopub.execute_input":"2022-05-17T20:59:47.221672Z","iopub.status.idle":"2022-05-17T20:59:58.343696Z","shell.execute_reply.started":"2022-05-17T20:59:47.221624Z","shell.execute_reply":"2022-05-17T20:59:58.342994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target     = train.target.value_counts(normalize=True)[::-1]\ntext       = ['State {}'.format(i) for i in target.index]\ncolor,pal  = ['#38A6A5','#E1B580'],['#88CAC9','#EDD3B3']\nif text[0] == 'State 0':\n    color,pal=color,pal\nelse:\n    color,pal = color[::-1],pal[::-1]\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=target.index, values=target*100, hole=.5, \n                     text=text, sort=False, showlegend=False,\n                     marker=dict(colors=pal,line=dict(color=color,width=2)),\n                     hovertemplate = \"State %{label}: %{value:.2f}%<extra></extra>\"))\nfig.update_layout(template=temp, title='Target Distribution', \n                  uniformtext_minsize=15, uniformtext_mode='hide',width=700)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:58.344997Z","iopub.execute_input":"2022-05-17T20:59:58.345363Z","iopub.status.idle":"2022-05-17T20:59:58.436597Z","shell.execute_reply.started":"2022-05-17T20:59:58.345319Z","shell.execute_reply":"2022-05-17T20:59:58.435746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# all_df = pd.concat([train, test]).reset_index(drop=True)\n# print(all_df.shape)\n# all_df.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:59.154107Z","iopub.execute_input":"2022-05-17T20:59:59.155174Z","iopub.status.idle":"2022-05-17T20:59:59.158789Z","shell.execute_reply.started":"2022-05-17T20:59:59.155123Z","shell.execute_reply":"2022-05-17T20:59:59.15813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class feature_engineering:\n    def __init__(self, df):\n        self.df = df\n        self.f_27_len = len(self.df['f_27'][0])\n        self.alphabet_upper = list(string.ascii_uppercase)\n\n    def get_features(self):\n        '''\n        https://www.kaggle.com/code/kellibelcher/tps-may-2022-eda-lgbm-neural-networks?scriptVersionId=95621151&cellId=14\n        '''\n        enc     = OrdinalEncoder()\n        self.df = self.df.copy()\n        self.df['char_unique'] = self.df['f_27'].apply(lambda x: len(set(x)))\n        for i in range(self.df.f_27.str.len().max()):\n            self.df['f_27_char{}'.format(i+1)]=enc.fit_transform(self.df['f_27'].str.get(i).values.reshape(-1,1))\n        return self.df.drop(['f_27'],axis=1)\n        \n#     def get_features(self):\n#         for i in range(10):\n#             self.df[f'ch{i}'] = self.df.f_27.str.get(i).apply(ord) - ord('A')\n#             self.df[\"unique_characters\"] = self.df.f_27.apply(lambda s: len(set(s)))\n#             self.df['i_02_21'] = (self.df.f_21 + self.df.f_02 > 5.2).astype(int) - \\\n#                                  (self.df.f_21 + self.df.f_02 <-5.3).astype(int)\n#             self.df['i_05_22'] = (self.df.f_22 + self.df.f_05 > 5.1).astype(int) - \\\n#                                  (self.df.f_22 + self.df.f_05 <-5.4).astype(int)\n#             i_00_01_26 = self.df.f_00 + self.df.f_01 + self.df.f_26\n#             self.df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n        \n#         return self.df\n        \n#     def get_features(self):\n#         for i in range(self.f_27_len):\n#             self.df[f'f_27_{i}'] = self.df['f_27'].apply(lambda x: x[i])\n\n#         for letter in tqdm(self.alphabet_upper):\n#             self.df[f'f_27_{letter}_count'] = self.df['f_27'].str.count(letter)\n\n#         self.df['f_27_nunique'] = self.df['f_27'].apply(lambda x: len(set(x)))\n\n#         return self.df\n    \n    def scaling(self, features):\n        sc = StandardScaler()\n        self.df[features] = sc.fit_transform(self.df[features])\n\n        return self.df\n\n    def label_encoding(self, features):\n        new_features = []\n        \n        for feature in features:\n            if self.df[feature].dtype == 'O':\n                le = LabelEncoder()\n                self.df[f'{feature}_enc'] = le.fit_transform(self.df[feature])\n                new_features.append(f'{feature}_enc')\n            else:\n                new_features.append(feature)\n\n        return self.df, new_features\n    \n    def onehot_encoding(self, features):\n        new_features = []\n        self.df = pd.get_dummies(self.df, columns=features)\n        \n        feats = [col for col in self.df.columns if CFG.target not in col]\n        for feat in feats:\n            if self.df[feat].dtype == 'uint8':\n                new_features.append(feat)\n\n        return self.df, new_features","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:59:59.521494Z","iopub.execute_input":"2022-05-17T20:59:59.521803Z","iopub.status.idle":"2022-05-17T20:59:59.539178Z","shell.execute_reply.started":"2022-05-17T20:59:59.521766Z","shell.execute_reply":"2022-05-17T20:59:59.53804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# fe     = feature_engineering(all_df)\n# all_df = fe.get_features()\n\ntrain_df = feature_engineering(train).get_features()\ntest_df  = feature_engineering(test).get_features()\n\nprint(train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T21:00:54.342129Z","iopub.execute_input":"2022-05-17T21:00:54.342424Z","iopub.status.idle":"2022-05-17T21:01:14.122083Z","shell.execute_reply.started":"2022-05-17T21:00:54.342394Z","shell.execute_reply":"2022-05-17T21:01:14.121207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling and encoding","metadata":{}},{"cell_type":"code","source":"# print(all_df.shape)\n# all_df.head()\n\n# all_df, cat_features = fe.label_encoding(cat_features)\n# all_df               = fe.scaling(num_features)\n# all_features         = cat_features + num_features\n\n\nscaler = StandardScaler()\ny      = train_df['target']\nX      = train_df.drop(['target'], axis=1)\nX      = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)\nX_test = pd.DataFrame(scaler.transform(test_df))\n\nprint(X.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T21:02:11.230758Z","iopub.execute_input":"2022-05-17T21:02:11.231574Z","iopub.status.idle":"2022-05-17T21:02:12.279671Z","shell.execute_reply.started":"2022-05-17T21:02:11.23153Z","shell.execute_reply":"2022-05-17T21:02:12.278993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# char = train['f_27'].value_counts().nlargest(20)\n# pal  = sns.color_palette(\"Spectral\",22).as_hex() \n# pal  = [j for i,j in enumerate(pal) if i not in (10,11)]\n# rgb  = ['rgba' + str(matplotlib.colors.to_rgba(i,0.75)) for i in pal] \n# fig  = go.Figure()\n# fig.add_trace(go.Bar(x=char.index, y=char, marker_color=rgb, \n#                      marker_line=dict(color=pal,width=2), name='',\n#                      hovertemplate='String: %{x}, Frequency: %{y}',\n#                      showlegend=False))\n# fig.update_layout(template=temp,title=\"Most Common Character Strings\",\n#                   yaxis_title=\"Frequency\", width=800)\n# fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = train[['f_27']]\n# for letter in string.ascii_uppercase:\n#     df['{}'.format(letter)]=df['f_27'].str.count(letter)\n# df_sum = df.iloc[:,1:].sum(axis=0).rename('sum').reset_index()\n# pal = sns.color_palette(\"Spectral_r\",28).as_hex()\n# pal = [j for i,j in enumerate(pal) if i !=14]\n# rgb = ['rgba'+str(matplotlib.colors.to_rgba(i,0.8)) for i in pal] \n# fig = go.Figure()\n# fig.add_trace(go.Bar(x=df_sum['index'], y=df_sum['sum'], marker_color=rgb, \n#                      marker_line=dict(color=pal,width=2), name='',\n#                      hovertemplate='Letter: %{x}, Frequency: %{y}',\n#                      showlegend=False))\n# fig.update_layout(template=temp,title=\"Most Common Letters\",\n#                   yaxis_title=\"Frequency\", width=800)\n# fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_len = train.shape[0]\n# train     = all_df[:train_len]\n# test      = all_df[train_len:].reset_index(drop=True)\n\n# display(train[all_features])\n# display(test[all_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling: LGBM","metadata":{}},{"cell_type":"code","source":"y_valid, gbm_val_preds, gbm_test_preds=[],[],[]\ncal_true, cal_pred = [],[]\nfeat_importance    = pd.DataFrame(index=X.columns)\nk_fold             = KFold(n_splits=5, shuffle=True, random_state=21)\n\nfor fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):\n    \n    print(\"\\nFold {}\".format(fold+1))\n    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n    X_val, y_val     = X.iloc[val_idx,:], y[val_idx]\n    print(f\"Train shape: {X_train.shape}, {y_train.shape}, Valid shape: {X_val.shape}, {y_val.shape}\")\n    \n    params = {'boosting_type': 'gbdt',\n              'n_estimators': 500,\n              'num_leaves': 50,\n              'learning_rate': 0.05,\n              'colsample_bytree': 0.9,\n              'subsample': 0.8,\n              'reg_alpha': 0.1,\n              'objective': 'binary',\n              'metric': 'auc',\n              'random_state': 21}\n    \n    gbm = LGBMClassifier(**params).fit(X_train, y_train, \n                                       eval_set    = [(X_train, y_train), (X_val, y_val)],\n                                       verbose     = 100,\n                                       eval_metric = ['binary_logloss','auc'])\n    \n    gbm_prob = gbm.predict_proba(X_val)[:,1]\n    y_valid.append(y_val)\n    gbm_val_preds.append(gbm_prob)\n    gbm_test_preds.append(gbm.predict_proba(X_test)[:,1])\n    feat_importance[\"Importance_Fold\"+str(fold)]=gbm.feature_importances_\n    \n    calibrated_gbm = CalibratedClassifierCV(base_estimator=gbm, cv=\"prefit\")\n    cal_fit        = calibrated_gbm.fit(X_train, y_train)\n    cal_probs      = calibrated_gbm.predict_proba(X_val)[:, 1]\n    prob_true, prob_pred = calibration_curve(y_val, cal_probs, n_bins=10)\n    cal_true.append(prob_true)\n    cal_pred.append(prob_pred)\n    auc_score=roc_auc_score(y_val, gbm_prob)\n    print(\"Validation AUC = {:.4f}\".format(auc_score))\n      \n    del X_train, y_train, X_val, y_val\n    gc.collect()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:06:34.071981Z","iopub.execute_input":"2022-05-17T22:06:34.072418Z","iopub.status.idle":"2022-05-17T22:08:01.819882Z","shell.execute_reply.started":"2022-05-17T22:06:34.072379Z","shell.execute_reply":"2022-05-17T22:08:01.819031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = px.colors.qualitative.Prism\n\ndef plot_roc_calibration(y_val, y_prob, mpv_cal, fop_cal):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x = np.linspace(0,1,11), y=np.linspace(0,1,11), \n                             name = 'Random Chance',mode='lines',\n                             line = dict(color=\"Black\", width=1, dash=\"dot\")))\n    for i in range(len(y_val)):\n        y=y_val[i]\n        prob=y_prob[i]\n        fpr, tpr, thresh = roc_curve(y, prob)\n        roc_auc = auc(fpr,tpr)\n        fig.add_trace(go.Scatter(x=fpr, y=tpr, line=dict(color=colors[::-1][i+6], width=3), \n                                 hovertemplate = 'True positive rate = %{y:.3f}, False positive rate = %{x:.3f}',\n                                 name='Fold {} AUC = {:.4f}'.format(i+1,roc_auc)))\n    fig.update_layout(template=temp, title=\"Cross-Validation ROC Curves\", \n                      hovermode=\"x unified\", width=600,height=500,\n                      xaxis_title='False Positive Rate (1 - Specificity)',\n                      yaxis_title='True Positive Rate (Sensitivity)',\n                      legend=dict(orientation='v', y=.07, x=1, xanchor=\"right\",\n                                  bordercolor=\"black\", borderwidth=.5))\n    fig.show()\n    fig=go.Figure()\n    fig.add_trace(go.Scatter(x=np.linspace(0,1,11), y=np.linspace(0,1,11), \n                             name='Perfectly Calibrated',mode='lines',\n                             line=dict(color=\"Black\", width=1, dash=\"dot\"),legendgroup=2))\n    for i in range(len(mpv_cal)):\n        mpv=mpv_cal[i]\n        fop=fop_cal[i]\n        fig.add_trace(go.Scatter(x=mpv, y=fop, line=dict(color=colors[::-1][i+6], width=3), \n                            hovertemplate = 'Proportion of Positives = %{y:.3f}, Mean Predicted Probability = %{x:.3f}',\n                            name='Fold {}'.format(i+1),legendgroup=2))\n    fig.update_layout(template=temp, title=\"Probability Calibration Curves\", \n                      hovermode=\"x unified\", width=600,height=500,\n                      xaxis_title='Mean Predicted Probability',\n                      yaxis_title='Proportion of Positives',\n                      legend=dict(orientation='v', y=.07, x=1, xanchor=\"right\",\n                                  bordercolor=\"black\", borderwidth=.5))\n    fig.show()\n    \ndef plot_target_predictions(df):\n    plot_df=pd.DataFrame.from_dict({'1':(len(df[df.target>0.5])/len(df.target))*100, \n                                    '0':(len(df[df.target<=0.5])/len(df.target))*100}, \n                                   orient='index', columns=['pct'])\n    text=['State {}'.format(i) for i in plot_df.index]\n    color,pal=['#38A6A5','#E1B580'],['#88CAC9','#EDD3B3']\n    if text[0]=='State 0':\n        color,pal=color,pal\n    else:\n        color,pal=color[::-1],pal[::-1]\n    fig=go.Figure()\n    fig.add_trace(go.Pie(labels=plot_df.index, values=plot_df.pct, hole=.5, \n                         text=text, sort=False, showlegend=False,\n                         marker=dict(colors=pal,line=dict(color=color,width=2)),\n                         hovertemplate = \"State %{label}: %{value:.2f}%<extra></extra>\"))\n    fig.update_layout(template=temp, title='Predicted Target Distribution', width=700,\n                      uniformtext_minsize=15, uniformtext_mode='hide')\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:05:55.550925Z","iopub.status.idle":"2022-05-17T22:05:55.551512Z","shell.execute_reply.started":"2022-05-17T22:05:55.551328Z","shell.execute_reply":"2022-05-17T22:05:55.551348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_calibration(y_valid, gbm_val_preds, cal_true, cal_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:05:55.552423Z","iopub.status.idle":"2022-05-17T22:05:55.552916Z","shell.execute_reply.started":"2022-05-17T22:05:55.552746Z","shell.execute_reply":"2022-05-17T22:05:55.552765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"feat_importance['avg'] = feat_importance.mean(axis=1)\nfeat_importance        = feat_importance.sort_values(by='avg',ascending=True)\n\npal = sns.color_palette(\"YlGnBu\", 52).as_hex()\nfig = go.Figure()\nfor i in range(len(feat_importance.index)):\n    fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n                       line_color=pal[::-1][i],opacity=0.8,line_width=4))\n\nfig.add_trace(go.Scatter(x = feat_importance['avg'], y = feat_importance.index, mode='markers', \n                         marker_color  = pal[::-1], marker_size=8,\n                         hovertemplate = '%{y} Importance = %{x:.0f}<extra></extra>'))\n\nfig.update_layout(template = temp, title = 'Feature Importance', \n                  xaxis = dict(title = 'Average Importance',zeroline = False),\n                  yaxis_showgrid = False, height = 900, width = 800)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:05:55.553821Z","iopub.status.idle":"2022-05-17T22:05:55.554271Z","shell.execute_reply.started":"2022-05-17T22:05:55.554103Z","shell.execute_reply":"2022-05-17T22:05:55.554122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"%%time\n# X_test = test.drop(columns=['id'])\n\n# sample_submission['target'] = model.predict(X_test)\n# submission.to_csv(\"submission.csv\", index=False)\n# sample_submission\n\nsub_gbm = submission.copy()\nsub_gbm['target'] = np.mean(gbm_test_preds, axis=0)\nsub_gbm.to_csv(\"submission_LGBM.csv\", index=False)\nplot_target_predictions(sub_gbm)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T21:18:45.87468Z","iopub.execute_input":"2022-05-17T21:18:45.875035Z","iopub.status.idle":"2022-05-17T21:18:48.322625Z","shell.execute_reply.started":"2022-05-17T21:18:45.874999Z","shell.execute_reply":"2022-05-17T21:18:48.321657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_gbm","metadata":{"execution":{"iopub.status.busy":"2022-05-17T21:20:51.896082Z","iopub.execute_input":"2022-05-17T21:20:51.896389Z","iopub.status.idle":"2022-05-17T21:20:51.912832Z","shell.execute_reply.started":"2022-05-17T21:20:51.896359Z","shell.execute_reply":"2022-05-17T21:20:51.911725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabNet","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=train[CFG.target])):\n    X_train = train[all_features].to_numpy()[trn_idx]\n    y_train = train[CFG.target].to_numpy()[trn_idx]\n    X_valid = train[all_features].to_numpy()[val_idx]\n    y_valid = train[CFG.target].to_numpy()[val_idx]\n    X_test = test[all_features].to_numpy()\n    \n    print(f\"===== FOLD {fold} =====\")\n    \n    tabnet_params = dict(\n        n_d=64,\n        n_steps=5,\n        gamma=1.3,\n        n_independent=3,\n        n_shared=3,\n        seed=CFG.seed,\n        momentum=2e-2,\n        lambda_sparse=1e-6,\n\n        optimizer_fn=torch.optim.Adam,\n        optimizer_params=dict(\n            lr=1e-2,\n            weight_decay=1e-7\n        ),\n        \n        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n        scheduler_params=dict(\n            mode='max',\n            factor=0.9,\n            patience=3,\n            min_lr=1e-6,\n        ),\n        verbose=10,\n        device_name='auto',\n        mask_type='sparsemax',\n    )\n    \n    # Defining TabNet model\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(\n        X_train=X_train,\n        y_train=y_train,\n        from_unsupervised=None,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_name=[\"train\", \"valid\"],\n        eval_metric=[\"auc\"],\n        batch_size=2048,\n        virtual_batch_size=2048,\n        max_epochs=200,\n        drop_last=True,\n        pin_memory=True,\n        patience=20,\n        num_workers=4,\n    )\n\n    train.loc[val_idx, CFG.tab_pred] = model.predict_proba(X_valid)[:, -1]\n    print(f\"auc score: {roc_auc_score(y_true=y_valid, y_score=train.loc[val_idx, CFG.tab_pred]):.6f}\\n\")\n    \n    test[f'{CFG.tab_pred}_{fold}'] = model.predict_proba(X_test)[:, -1]\n\nprint(f\"auc score : {roc_auc_score(y_true=train[CFG.target], y_score=train[CFG.tab_pred]):.6f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [col for col in test.columns if CFG.tab_pred in col]\n\nsubmission[CFG.target] = test[cols].mean(axis=1)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to obtain the activation function\n# def get_activation(activation_name):\n#     if activation_name == 'Relu':\n#         activation = F.relu\n#     elif activation_name == 'ELU':\n#         activation = F.elu\n#     else:\n#         activation = F.leaky_relu\n#     return activation\n\n# # Function to get optimize method\n# def get_optimizer(model, optimizer_name, lr, weight_decay):\n#     if optimizer_name == 'MomentumSGD': \n#         optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n#     elif optimizer_name == 'Adam':\n#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n#     elif optimizer_name == 'Adagrad':\n#         optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=weight_decay)      \n#     else:\n#         optimizer = torch.optim.RMSprop(model.parameters())\n#     return optimizer\n\n# # Function to train Neural Network\n# def train(model, train_dataloader, optimizer):\n#     # check whether GPU is available\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     model.to(device)\n#     # Define the error function\n#     criterion = nn.MSELoss()\n#     #　Model in learning mode\n#     model.train()\n#     #　If the network is somewhat fixed, make it faster\n#     torch.backends.cudnn.benchark = True\n#     # epoch loss\n#     epoch_loss = 0\n#     iteration = 0\n#     for batch, (data, target) in enumerate(train_dataloader):\n#         data, target = data.to(device), target.to(device)\n#         optimizer.zero_grad()\n#         output = model(data)\n#         output = output.view(1, -1)[0]\n#         # print(output.shape, target.shape)\n#         target = target.to(torch.float32)\n#         loss   = criterion(output, target)\n#         epoch_loss += loss.item()\n#         loss.backward()\n#         optimizer.step()\n#         iteration += 1\n#     epoch_loss /= iteration\n#     return epoch_loss\n\n# # Function for prediction\n# def predict(model, dataloader):\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     model.eval()\n#     model.to(device)\n#     y_pred = np.array([])\n#     with torch.no_grad():\n#         for data in dataloader:\n#             data = data[0]\n#             #output = model(data)\n#             output = model(data.to(device))\n#             output = output.view(1, -1)\n#             output = output.to('cpu').detach().numpy().copy()\n#             #output = output.to(device)\n#             y_pred = np.append(y_pred, output[0])\n#         y_pred = np.array(y_pred)\n#     return y_pred\n\n# # Function for plot loss function of each epoch\n# def loss_plot(logs_train, logs_valid):\n#     plt.plot(logs_train[0][1:], logs_train[1][1:], '-b', label='train')\n#     plt.plot(logs_valid[0][1:], logs_valid[1][1:], '-r', label='test')\n#     plt.xlabel('epoch')\n#     plt.ylabel('loss')\n#     plt.legend()\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params = {'num_layer': 2, \n#           'num_nodes_0': 24, \n#           'num_nodes_1': 12, \n#           'dropout_rate': 0.5, \n#           'activation': 'leaky_relu', \n#           'optimizer': 'Adam', \n#           'weight_decay': 1e-10, \n#           'Adam_lr': 0.001}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n# ### prameter\n# k_split = 10\n# num_epochs = 100\n# batch_size = 64\n# ###\n\n# # k-fold cross-validation\n# kfold = StratifiedKFold(n_splits=k_split,random_state=1, shuffle=True).split(X_train_std, y_train)\n# #### get parameter\n# num_layer       = params['num_layer']\n# num_nodes       = [int(params[s]) for s in params.keys() if 'num_nodes' in s]\n# dropout_rate    = params['dropout_rate']\n# activation_name = params['activation']\n# optimizer_name  = params['optimizer']\n# lr              = params[optimizer_name+'_lr']\n# weight_decay    = params['weight_decay']\n# ######\n\n# scores = []   # list to save score \n# models = []   # list to save model\n# for k, (train_id, test_id) in enumerate(kfold):\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     # Instantiate Model\n#     model = Net(input_size=X_train_std.shape[1],\n#                 num_layer=num_layer, \n#                 num_nodes=num_nodes, \n#                 dropout_rate=dropout_rate, \n#                 activation_name=activation_name)\n#     # model to GPU\n#     model.to(device)\n#     optimizer = get_optimizer(model, optimizer_name, lr, weight_decay)\n#     # data to dataloader\n#     dataset          = torch.utils.data.TensorDataset(torch.Tensor(X_train_std[train_id]), \n#                                              torch.tensor(y_train[train_id]))\n#     train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n#     valid_dataset    = torch.utils.data.TensorDataset(torch.Tensor(X_train_std[test_id]))\n#     valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n#     # training each epoch\n#     logs_train = [[0], [np.inf]]\n#     logs_valid = [[0], [np.inf]]\n#     for epoch in tqdm(range(num_epochs)):\n#         epoch_loss = train(model, train_dataloader, optimizer)\n#         valid_pred = predict(model, valid_dataloader)\n#         valid_loss = mean_squared_error(y_train[test_id], valid_pred)\n#         if epoch_loss < min(logs_valid[1]):\n#             torch.save(model.state_dict(), './models'+str(k))\n#         logs_train[0].append(epoch+1)\n#         logs_train[1].append(epoch_loss)\n#         logs_valid[0].append(epoch+1)\n#         logs_valid[1].append(valid_loss)   \n#     # valid\n#     model.load_state_dict(torch.load('./models'+str(k)))\n#     pred_y_k = predict(model, valid_dataloader)\n#     # score\n#     score = roc_auc_score(y_train[test_id], pred_y_k)\n#     print('Fold: %2d, AUC: %.3f' % (k+1, score))\n#     scores.append(score)\n#     models.append(model)\n#     loss_plot(logs_train, logs_valid)\n# print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation with testdata\n* Define a function to predict and summarize the results in each of the models created by the k-fold cross-validation.","metadata":{}},{"cell_type":"code","source":"# def predict_kfold(models, X_test):\n#     # Create array for storing test data\n#     y_pred = np.zeros((len(X_test), len(models)))\n#     # Crate dataloader\n#     test_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test))\n#     test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n#     for fold_, model_ in enumerate(models):\n#         model_.load_state_dict(torch.load('./models'+str(fold_)))\n#         # predict\n#         pred_ = predict(model_, test_dataloader)\n#         # store\n#         y_pred[:, fold_] = pred_ \n#     y_pred = y_pred.mean(axis=1)\n#     return y_pred\n# y_pred = predict_kfold(models, X_test_std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # calclate auc and roc curves and evaluate performance on test data\n# roc = roc_curve(y_test, y_pred)\n# print(\"roc\", roc_auc_score(y_test, y_pred))\n# fpr, tpr, thresholds = roc\n# plt.plot(fpr, tpr, marker='o')\n# plt.xlabel('FPR: False positive rate')\n# plt.ylabel('TPR: True positive rate')\n# plt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Try the same calculations on training data\n# y_pred_train = predict_kfold(models, X_train_std)\n# roc = roc_curve(y_train, y_pred_train)\n# print(\"roc\", roc_auc_score(y_train, y_pred_train))\n# fpr, tpr, thresholds = roc\n# plt.plot(fpr, tpr, marker='o')\n# plt.xlabel('FPR: False positive rate')\n# plt.ylabel('TPR: True positive rate')\n# plt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit data\n* Apply model to test_df and create submit data","metadata":{}},{"cell_type":"code","source":"# X_submit = test_df.values\n# X_submit_std = stdsc.transform(X_submit)\n# y_submit = predict_kfold(models, X_submit_std)\n# print(y_submit)\n# print(y_submit.shape)\n# plt.hist(y_submit, bins=30, density=True)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv')\n# print(submission_df.shape)\n# submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df['target'] = pd.DataFrame(y_submit)\n# submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n## A method of hyperparameter tuning using a technique called `optuna`\n\n* First, split the training data into data used for training and data used for tuning. And then **standardize.**","metadata":{}},{"cell_type":"code","source":"# valid_size = 0.1\n# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size,\n#                                                       stratify=y_train)\n# print(X_train.shape, X_valid.shape)\n# print(y_train.shape, y_valid.shape)\n\n# stdsc = StandardScaler()\n# X_train_std   = stdsc.fit_transform(X_train)\n# X_valid_std   = stdsc.transform(X_valid)\n# train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train_std), torch.tensor(y_train))\n# valid_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_valid_std), torch.tensor(y_valid))\n# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tuning is performed below using optuna. Here, we define trial and explore each parameter.**","metadata":{}},{"cell_type":"code","source":"# # define the class\n# class Net(nn.Module):\n#     def __init__(self, trial, input_size, num_layer, num_nodes, dropout_rate):\n#         super(Net, self).__init__()\n#         self.activation = get_activation(trial)\n#         self.linears = nn.ModuleList([nn.Linear(input_size, num_nodes[0])])\n#         self.batchnorms = nn.ModuleList([nn.BatchNorm1d(num_nodes[0])])\n#         for i in range(1, num_layer):\n#             self.linears.append(nn.Linear(num_nodes[i-1], num_nodes[i]))\n#             self.batchnorms.append(nn.BatchNorm1d(num_nodes[i]))\n#         self.fcl = nn.Linear(num_nodes[-1], 1)\n#         self.dropout = nn.Dropout(dropout_rate)\n\n#     def forward(self, x):\n#         for i, d in enumerate(zip(self.linears, self.batchnorms)):\n#             l, b = d[0], d[1]\n#             x = b(self.activation(l(x)))\n#             x = self.dropout(x)\n#         x = torch.sigmoid(self.fcl(x))\n#         return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(model, device, train_dataloader, optimizer):\n#     model.train()\n#     criterion = nn.MSELoss()\n#     for batch, (data, target) in enumerate(train_dataloader):\n#         data, target = data.to(device), target.to(device)\n#         optimizer.zero_grad()\n#         output = model(data)\n#         output = output.view(1, -1)[0]\n#         # print(output.shape, target.shape)\n#         target = target.to(torch.float32)\n#         loss = criterion(output, target)\n#         loss.backward()\n#         optimizer.step()\n# def test(model, device, valid_dataloader):\n#     model.eval()\n#     criterion = nn.MSELoss()\n#     loss = 0\n#     iteration = 0\n#     with torch.no_grad():\n#         for data, target in valid_dataloader:\n#             data, target = data.to(device), target.to(device)\n#             output = model(data)\n#             output = output.view(1, -1)[0]\n#             loss += criterion(output, target)\n#             iteration += 1\n#     loss /= iteration\n#     return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_optimizer(trial, model):\n#     optimizer_names = ['MomentumSGD', 'Adam', 'Adagrad']\n#     optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n#     if optimizer_name == optimizer_names[0]: \n#         momentum_sgd_lr = trial.suggest_loguniform('Momentum_SGD_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.SGD(model.parameters(), lr=momentum_sgd_lr, momentum=0.9, weight_decay=weight_decay)\n#     elif optimizer_name == optimizer_names[1]:\n#         adam_lr = trial.suggest_loguniform('Adam_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.Adam(model.parameters(), lr=adam_lr, weight_decay=weight_decay)\n#     elif optimizer_name == optimizer_names[2]:\n#         adagrad_lr = trial.suggest_loguniform('Adagrad_lr', 1e-5, 1e-1)\n#         optimizer = torch.optim.Adagrad(model.parameters(), lr=adagrad_lr, weight_decay=weight_decay)      \n#     return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_activation(trial):\n#     activation_names = ['ReLU', 'ELU', 'leaky_relu']\n#     activation_name = trial.suggest_categorical('activation', activation_names)\n#     if activation_name == activation_names[0]:\n#         activation = F.relu\n#     elif activation_name == activation_names[1]:\n#         activation = F.elu\n#     else:\n#         activation = F.leaky_relu\n#     return activation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 30\n# def objective(trial):\n#     device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n#     # hidden layer\n#     num_layer = trial.suggest_int('num_layer', 2, 7)\n#     # the number of nodes\n#     num_nodes = [int(trial.suggest_discrete_uniform('num_nodes_'+str(i), 16, 128, 16)) for i in range(num_layer)]\n#     # dropout ratio\n#     dropout_rate = trial.suggest_float('dropout_rate', 0.0, 1.0)\n\n#     model = Net(trial, X_train.shape[1],num_layer, num_nodes, dropout_rate).to(device)\n#     optimizer = get_optimizer(trial, model)\n#     error_rate = 0\n#     for epoch in range(epochs):\n#         train(model, device, train_dataloader, optimizer)\n#     error_rate = test(model, device, valid_dataloader)\n#     return error_rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# TRIAL_SIZE = 100\n# study = optuna.create_study()\n# study.optimize(objective, n_trials=TRIAL_SIZE)\n# best_params = study.best_params\n# print(best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # params = best_params\n# params={'num_layer': 4, 'num_nodes_0': 128.0, 'num_nodes_1': 112.0, 'num_nodes_2': 96.0,\n#         'num_nodes_3': 96.0, 'dropout_rate': 0.08387843261849516, 'activation': 'ReLU',\n#         'optimizer': 'Adam', 'weight_decay': 2.227219890291524e-09, 'Adam_lr': 0.0019802197708342255}","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}