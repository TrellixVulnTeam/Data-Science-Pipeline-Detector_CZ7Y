{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:#EAB4DE;margin:0;font-size:200%;text-align:center;border-radius:5px;overflow:hidden;font-weight:500\">TPS May 2022</div>\n\n# <b><span style='color:#EAB4DE'>1 |</span><span style='color:#EAB4DE'> Competition Overview</span></b>\n\nThe May edition of the 2022 Tabular Playground series is a binary classification problem that includes a number of different feature interactions. \nThe dataset contains several variables representing simulated manufacturing control datawhich can be useful to predict whether the machine is in State 0 or State 1.","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'>2 |</span><span style='color:#EAB4DE'>Exploratory Data Analysis</span></b>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\nsubm = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')\n\nprint(\"The Training dataset is made of {} rows and {} columns.\".format(len(train_df), len(train_df.columns)))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:28:46.409853Z","iopub.execute_input":"2022-05-31T06:28:46.410163Z","iopub.status.idle":"2022-05-31T06:29:03.36281Z","shell.execute_reply.started":"2022-05-31T06:28:46.41014Z","shell.execute_reply":"2022-05-31T06:29:03.360006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we can see some rows from the trainig dataset, in order to see how the data include in it look like:","metadata":{}},{"cell_type":"code","source":"pd.options.display.max_columns = train_df.shape[1]\ntrain_df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:03.367199Z","iopub.execute_input":"2022-05-31T06:29:03.368465Z","iopub.status.idle":"2022-05-31T06:29:03.45186Z","shell.execute_reply.started":"2022-05-31T06:29:03.368339Z","shell.execute_reply":"2022-05-31T06:29:03.450493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that not all variables have the same type of data in it.\nIn fact, if we look at each column, the types we have are the following:","metadata":{}},{"cell_type":"code","source":"columns = train_df.dtypes\n\nfor elem in range(len(columns.index)):\n    print(\"- {}: type {} \\n\".format(columns.index[elem], columns.values[elem]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:03.454244Z","iopub.execute_input":"2022-05-31T06:29:03.454992Z","iopub.status.idle":"2022-05-31T06:29:03.473107Z","shell.execute_reply.started":"2022-05-31T06:29:03.454944Z","shell.execute_reply":"2022-05-31T06:29:03.47055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that the dataset includes both continuos and categorical data, therefore we can interpret integer column as categories and floating columns as numeric variables.\n\nBefore we dive into the analysis of the features, let's have a look at how the target variable is distributed in our training set, in order to make sure that we don't have skewed information, which may lead to a wrong choice when looking at the model.","metadata":{}},{"cell_type":"code","source":"counting = train_df['target'].value_counts()\nlbl = []\nfor elem in counting.index:\n    lbl.append('Target {}'.format(counting.index.values[elem]))\n\nplt.figure(figsize=(15,8))\nfont = {'family' : 'serif',\n        'weight' : 'bold',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\n\ncolors = sns.color_palette(\"husl\", 2)\nplt.pie(counting, labels = lbl, colors = colors, autopct='%.0f%%', explode=(0, 0.1),\n        shadow=True, startangle=90\n       )\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:03.477011Z","iopub.execute_input":"2022-05-31T06:29:03.477813Z","iopub.status.idle":"2022-05-31T06:29:03.699662Z","shell.execute_reply.started":"2022-05-31T06:29:03.477709Z","shell.execute_reply":"2022-05-31T06:29:03.698852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the dataset is not skewed in representing the target variable.","metadata":{}},{"cell_type":"code","source":"print(\"The number of missing values in the training set is equal to: {}.\".format(train_df.isnull().sum().sum()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:03.700747Z","iopub.execute_input":"2022-05-31T06:29:03.70135Z","iopub.status.idle":"2022-05-31T06:29:03.882337Z","shell.execute_reply.started":"2022-05-31T06:29:03.701315Z","shell.execute_reply":"2022-05-31T06:29:03.881522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train_df.drop(columns = ['id', 'target'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:03.883516Z","iopub.execute_input":"2022-05-31T06:29:03.884195Z","iopub.status.idle":"2022-05-31T06:29:04.09055Z","shell.execute_reply.started":"2022-05-31T06:29:03.884154Z","shell.execute_reply":"2022-05-31T06:29:04.089323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'>2.1 |</span><span style='color:#EAB4DE'>EDA Continuous Variables</span></b>","metadata":{}},{"cell_type":"markdown","source":"All variables included in the dataframe are numeric (either float or integer), except for f_27.\nSince the integer columns represent factorial variables, we'll be looking only at the float ones.\nIn order to better understand what's inside these columns, we can look at a brief statistical summary:","metadata":{}},{"cell_type":"code","source":"x_float = train_df.select_dtypes('float64')\nx_float.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:04.091949Z","iopub.execute_input":"2022-05-31T06:29:04.092224Z","iopub.status.idle":"2022-05-31T06:29:05.450586Z","shell.execute_reply.started":"2022-05-31T06:29:04.092181Z","shell.execute_reply":"2022-05-31T06:29:05.447307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have many different columns, looking at a table and getting some useful insights may be quiet difficult.\nTherefore looking at a plot may be more useful:","metadata":{}},{"cell_type":"code","source":"sns.color_palette(\"husl\", 8)\nplt.figure(figsize=(15,8))\nax = sns.boxplot(data=x_float, orient=\"h\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:05.45364Z","iopub.execute_input":"2022-05-31T06:29:05.45447Z","iopub.status.idle":"2022-05-31T06:29:07.00582Z","shell.execute_reply.started":"2022-05-31T06:29:05.454331Z","shell.execute_reply":"2022-05-31T06:29:07.005017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since f_28 is on a different scale compared to the other features, instead of plotting the boxplots all in one graph, it is better to split them by column and maybe divide them by the value of the target variable:","metadata":{}},{"cell_type":"code","source":"float_and_tgt=pd.concat([x_float,train_df['target']], axis=1)\ntitles=['Feature {}'.format(i.split('_')[-1]) for i in x_float]\nfig, ax = plt.subplots(4,4, figsize=(14,24))\nrow=0\ncol=[0,1,2,3]*4\nfor i, column in enumerate(float_and_tgt.columns[:-1]):\n    if (i!=0) & (i%4==0):\n        row+=1\n    color='#2CB4CF'\n    rgb=matplotlib.colors.to_rgba(color,0.2)\n    ax[row,col[i]].boxplot(float_and_tgt[float_and_tgt['target']==0][column], positions=[0],\n                           widths=0.7, patch_artist=True,\n                           boxprops=dict(color=color, facecolor=rgb, linewidth=1.5))\n    color='#EAB4DE'\n    rgb=matplotlib.colors.to_rgba(color,0.2)\n    ax[row,col[i]].boxplot(float_and_tgt[float_and_tgt['target']==1][column], positions=[1],\n                           widths=0.7, patch_artist=True,\n                           boxprops=dict(color=color, facecolor=rgb, linewidth=1.5))\n    ax[row,col[i]].grid(visible=True, which='major', axis='y', color='#F2F2F2')\n    ax[row,col[i]].tick_params(left=False,bottom=False)\n    ax[row,col[i]].set_title('\\n\\n{}'.format(titles[i]))\nsns.despine(bottom=True, trim=True)\nplt.suptitle('Distributions of Numerical Variables',fontsize=16)\nplt.tight_layout(rect=[0, 0.2, 1, 0.99])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:07.006998Z","iopub.execute_input":"2022-05-31T06:29:07.007444Z","iopub.status.idle":"2022-05-31T06:29:11.400862Z","shell.execute_reply.started":"2022-05-31T06:29:07.007412Z","shell.execute_reply":"2022-05-31T06:29:11.400048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that all features as pretty much symmetrical and not very skewed. To confirm this hypothesis, let's have a look at the distributions by using some histograms:","metadata":{}},{"cell_type":"code","source":"float_and_tgt=pd.concat([x_float,train_df['target']], axis=1)\ntitles=['Feature {}'.format(i.split('_')[-1]) for i in x_float]\nfig, ax = plt.subplots(4,4, figsize=(14,24))\nrow=0\ncol=[0,1,2,3]*4\nfor i, column in enumerate(float_and_tgt.columns[:-1]):\n    if (i!=0) & (i%4==0):\n        row+=1\n    color='#2CB4CF'\n    rgb=matplotlib.colors.to_rgba(color,0.3)\n    ax[row,col[i]].hist(float_and_tgt[float_and_tgt['target']==0][column],\n                        color=rgb, density=True, bins=40)\n    color='#EAB4DE'\n    rgb=matplotlib.colors.to_rgba(color,0.3)\n    ax[row,col[i]].hist(float_and_tgt[float_and_tgt['target']==1][column],\n                       color=rgb, density=True, bins=40)\n    #ax[row,col[i]].grid(visible=True, which='major', axis='y', color='#F2F2F2')\n    ax[row,col[i]].tick_params(left=False,bottom=False)\n    ax[row,col[i]].set_title('\\n\\n{}'.format(titles[i]))\nsns.despine(bottom=True, trim=True)\nplt.suptitle('Distributions of Numerical Variables',fontsize=16)\nplt.tight_layout(rect=[0, 0.2, 1, 0.99])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:11.403272Z","iopub.execute_input":"2022-05-31T06:29:11.404347Z","iopub.status.idle":"2022-05-31T06:29:17.71305Z","shell.execute_reply.started":"2022-05-31T06:29:11.404289Z","shell.execute_reply":"2022-05-31T06:29:17.712094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can confirm that the variables are symmetrical and are Normally distributed.\nWe can also notice that the distribution is the same for both values of the target variable for all features, except for some small spikes.","metadata":{}},{"cell_type":"markdown","source":"Now that we had a look at the distribution of the features, it can be useful to see if there is any relevant correlation between some of them:","metadata":{}},{"cell_type":"code","source":"corr = x_float.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(14, 24))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(145, 300, s=60, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:17.714192Z","iopub.execute_input":"2022-05-31T06:29:17.715315Z","iopub.status.idle":"2022-05-31T06:29:18.606479Z","shell.execute_reply.started":"2022-05-31T06:29:17.715259Z","shell.execute_reply":"2022-05-31T06:29:18.604908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like there is no particular correlation between these variables, except for Feature 28, that seems a little bit correlated (with a score between 0.2 and 0.3) with the first 6 features.","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'>2.2 |</span><span style='color:#EAB4DE'>EDA Categorical Variables</span></b>","metadata":{}},{"cell_type":"markdown","source":"Now that we analyzed how the continuous variables are distributed in our dataset, we will begin to have a look at the categorical ones.","metadata":{}},{"cell_type":"code","source":"x_int = train_df.select_dtypes('int64')\nx_int = x_int.drop('id', axis=1)\n#x_int_tgt = pd.concat([x_int,train_df['target']], axis=1)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:18.608311Z","iopub.execute_input":"2022-05-31T06:29:18.608777Z","iopub.status.idle":"2022-05-31T06:29:18.669909Z","shell.execute_reply.started":"2022-05-31T06:29:18.608735Z","shell.execute_reply":"2022-05-31T06:29:18.668566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_titles=['Feature {}'.format(i.split('_')[-1]) for i in x_int.columns[:-1]]\n\nfig, ax = plt.subplots(4,4, figsize=(14,24))\n\nfor i, f in enumerate(x_int.columns[:-1]):\n    plt.subplot(4, 4, i+1)\n    ax = plt.gca()\n    color='#2CB4CF'\n    rgb=matplotlib.colors.to_rgba(color,0.3)\n    \n    vc_0 = x_int[x_int['target']==0][f].value_counts()\n    ax.bar(vc_0.index, vc_0, color=rgb)\n    \n    color='#EAB4DE'\n    rgb=matplotlib.colors.to_rgba(color,0.3)\n    vc_1 = x_int[x_int['target']==1][f].value_counts()\n    ax.bar(vc_1.index, vc_1, color=rgb)\n    #ax.hist(train[f], density=False, bins=(train[f].max()-train[f].min()+1))\n    #ax.set_xlabel(f'Feature {f}')\n    ax.set_title('\\n\\n{}'.format(sub_titles[i]))\n    #ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\nsns.despine(bottom=True, trim=True)\nplt.suptitle('Distributions of Categorical Variables',fontsize=16)\nplt.tight_layout(rect=[0, 0.2, 1, 0.99])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:29:18.67118Z","iopub.execute_input":"2022-05-31T06:29:18.672063Z","iopub.status.idle":"2022-05-31T06:29:22.208429Z","shell.execute_reply.started":"2022-05-31T06:29:18.671929Z","shell.execute_reply":"2022-05-31T06:29:22.207398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at the different bar charts, we notice that the majority of the categorical features has between 10 and 15 different levels, with a higher concentration of data in the first 5 levels.\nFeature 29 has only two levels, so we may think of it as a boolean variable (more skewed on 0 than 1), while Feature 30 has 3 levels, quiet uniformely distributed.\n\nThe counts for each categorical feature are similar both for status 0 and for status 1.","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:56:05.878593Z","iopub.execute_input":"2022-05-29T12:56:05.879029Z"}}},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'>3 |</span><span style='color:#EAB4DE'>Logistic Regression</span></b>","metadata":{}},{"cell_type":"markdown","source":"The first model we could try could be a Logistic Regression, since the target variable is a binary one.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection  import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:29:22.209565Z","iopub.execute_input":"2022-05-31T06:29:22.210036Z","iopub.status.idle":"2022-05-31T06:29:22.519272Z","shell.execute_reply.started":"2022-05-31T06:29:22.210005Z","shell.execute_reply":"2022-05-31T06:29:22.518392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_features = pd.concat([x_float,x_int], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:29:22.520415Z","iopub.execute_input":"2022-05-31T06:29:22.521177Z","iopub.status.idle":"2022-05-31T06:29:22.571857Z","shell.execute_reply.started":"2022-05-31T06:29:22.52115Z","shell.execute_reply":"2022-05-31T06:29:22.570023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, cv_x, y_train, y_cv  = train_test_split(x_features,train_df['target'],\n                                                 stratify=train_df['target'])\n\ntrain_x.drop(['target'],axis=1,inplace=True)\ncv_x.drop([\"target\"],axis=1,inplace=True)\n\nlr = LogisticRegression(max_iter=500)\nlr.fit(train_x.values, y_train.values)\npred = lr.predict(train_x.values)\nprint(\"The train accuracy of the Logistic Regression is \",accuracy_score(y_train.values,pred))\npred  = lr.predict(cv_x.values)\nprint(\"The cv accuracy of the Logistic Regression is \",accuracy_score(y_cv.values, pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:29:22.573997Z","iopub.execute_input":"2022-05-31T06:29:22.574476Z","iopub.status.idle":"2022-05-31T06:29:44.666301Z","shell.execute_reply.started":"2022-05-31T06:29:22.574445Z","shell.execute_reply":"2022-05-31T06:29:44.665611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score we got from the logisti model isn't the best, but we noticed that the training set contains various outliers in many columns: let's try to remove them and see if we get better results.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nx_float_no = x_float[(np.abs(stats.zscore(x_float)) < 3).all(axis=1)]\nindex_list_no = x_float_no.index.to_list()\nx_int_no = x_int.iloc[index_list_no]\ntarget_no = x_int_no['target']\nx_int_no.drop('target', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:29:44.669704Z","iopub.execute_input":"2022-05-31T06:29:44.671366Z","iopub.status.idle":"2022-05-31T06:29:45.3327Z","shell.execute_reply.started":"2022-05-31T06:29:44.671333Z","shell.execute_reply":"2022-05-31T06:29:45.332036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_features_no = pd.concat([x_float_no,x_int_no], axis=1)\n\ntrain_x, cv_x, y_train, y_cv  = train_test_split(x_features_no,target_no,\n                                                 stratify=target_no)\n\n#train_x.drop(['target'],axis=1,inplace=True)\n#cv_x.drop([\"target\"],axis=1,inplace=True)\n\nlr = LogisticRegression(max_iter=500)\nlr.fit(train_x.values, y_train.values)\npred = lr.predict(train_x.values)\nprint(\"The train accuracy of the Logistic Regression without outliers is \",accuracy_score(y_train.values,pred))\npred  = lr.predict(cv_x.values)\nprint(\"The cv accuracy of the Logistic Regression without outliers is \",accuracy_score(y_cv.values, pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:29:45.334Z","iopub.execute_input":"2022-05-31T06:29:45.334485Z","iopub.status.idle":"2022-05-31T06:30:07.440989Z","shell.execute_reply.started":"2022-05-31T06:29:45.33446Z","shell.execute_reply":"2022-05-31T06:30:07.440343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like removing the outliers didn't make the model better, but worse.","metadata":{}},{"cell_type":"code","source":"test_x = test_df.select_dtypes([\"int\",\"float\"])\ntest_id = test_x['id'].values\ntest_x.drop(\"id\",axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:30:07.442173Z","iopub.execute_input":"2022-05-31T06:30:07.442647Z","iopub.status.idle":"2022-05-31T06:30:07.586008Z","shell.execute_reply.started":"2022-05-31T06:30:07.44261Z","shell.execute_reply":"2022-05-31T06:30:07.585054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = lr.predict(test_x.values)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:30:07.587411Z","iopub.execute_input":"2022-05-31T06:30:07.587784Z","iopub.status.idle":"2022-05-31T06:30:07.662957Z","shell.execute_reply.started":"2022-05-31T06:30:07.587743Z","shell.execute_reply":"2022-05-31T06:30:07.658214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at how the submissions should be like:","metadata":{}},{"cell_type":"code","source":"subm.head()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-31T06:30:07.664065Z","iopub.execute_input":"2022-05-31T06:30:07.664462Z","iopub.status.idle":"2022-05-31T06:30:07.683367Z","shell.execute_reply.started":"2022-05-31T06:30:07.664428Z","shell.execute_reply":"2022-05-31T06:30:07.682529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"submission_df = pd.DataFrame({\n    \"id\" : test_id,\n    \"target\": pred\n})\nsubmission_df.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:14:50.853021Z","iopub.execute_input":"2022-05-30T06:14:50.853667Z","iopub.status.idle":"2022-05-30T06:14:52.081994Z","shell.execute_reply.started":"2022-05-30T06:14:50.853621Z","shell.execute_reply":"2022-05-30T06:14:52.081225Z"},"_kg_hide-input":true}},{"cell_type":"markdown","source":"As expected, the score the logistic regression got is quiet low (0.48).\nWe could try to improve it by using cross validation or try to implement a more complex model.","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'>4 |</span><span style='color:#EAB4DE'>Random Forest</span></b>","metadata":{}},{"cell_type":"markdown","source":"First, we'll try with a simple RandomForestClassifier:","metadata":{}},{"cell_type":"code","source":"#x_features_no.drop('target', axis=1, inplace=True)\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:30:07.687113Z","iopub.execute_input":"2022-05-31T06:30:07.68792Z","iopub.status.idle":"2022-05-31T06:30:07.917408Z","shell.execute_reply.started":"2022-05-31T06:30:07.687881Z","shell.execute_reply":"2022-05-31T06:30:07.916347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_features_notgt = x_features.drop('target', axis=1)\ntarget = x_features['target']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:30:07.918924Z","iopub.execute_input":"2022-05-31T06:30:07.919315Z","iopub.status.idle":"2022-05-31T06:30:07.972705Z","shell.execute_reply.started":"2022-05-31T06:30:07.919281Z","shell.execute_reply":"2022-05-31T06:30:07.972146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"x_train, x_test, y_train, y_test = train_test_split(x_features_notgt, target)\n\n\nmodel = RandomForestClassifier(n_jobs=-1)\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:56:16.659005Z","iopub.execute_input":"2022-05-30T08:56:16.659438Z","iopub.status.idle":"2022-05-30T08:59:32.727216Z","shell.execute_reply.started":"2022-05-30T08:56:16.659404Z","shell.execute_reply":"2022-05-30T08:59:32.726228Z"},"_kg_hide-input":true}},{"cell_type":"markdown","source":"pred = model.predict(test_df.drop([\"id\",\"f_27\"],axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:59:46.782333Z","iopub.execute_input":"2022-05-30T08:59:46.782725Z","iopub.status.idle":"2022-05-30T08:59:55.130334Z","shell.execute_reply.started":"2022-05-30T08:59:46.782694Z","shell.execute_reply":"2022-05-30T08:59:55.129228Z"},"_kg_hide-input":true}},{"cell_type":"markdown","source":"The Random Forest got a better scoring (0.51), compared to Logistic Regression.\nMaybe working on it could bring to better results.","metadata":{}},{"cell_type":"code","source":"train_features, test_features, train_labels, test_labels = train_test_split(x_features_no,target_no, test_size = 0.25, random_state = 42)\n\nrf = RandomForestRegressor(n_estimators = 100,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            n_jobs = -1,\n                            random_state = 42)\nrf.fit(train_features, train_labels)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:30:07.974809Z","iopub.execute_input":"2022-05-31T06:30:07.97586Z","iopub.status.idle":"2022-05-31T06:42:52.895039Z","shell.execute_reply.started":"2022-05-31T06:30:07.975827Z","shell.execute_reply":"2022-05-31T06:42:52.893846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we identified our model, let's look at the importance of the variables we included, in order to check if it's better to reduce the number of features:","metadata":{}},{"cell_type":"code","source":"feature_list = list(train_features.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:42:52.897469Z","iopub.execute_input":"2022-05-31T06:42:52.898781Z","iopub.status.idle":"2022-05-31T06:42:52.903108Z","shell.execute_reply.started":"2022-05-31T06:42:52.898708Z","shell.execute_reply":"2022-05-31T06:42:52.902311Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {} Importance: {}'.format(*pair)) for pair in feature_importances]","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:42:52.904454Z","iopub.execute_input":"2022-05-31T06:42:52.904751Z","iopub.status.idle":"2022-05-31T06:42:53.03937Z","shell.execute_reply.started":"2022-05-31T06:42:52.904728Z","shell.execute_reply":"2022-05-31T06:42:53.038321Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the first 8 features (ordered by importance) cover more than 80% of the importance, so maybe we should try to create a model only with those.","metadata":{}},{"cell_type":"code","source":"feature_importances[1][0]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:42:53.040778Z","iopub.execute_input":"2022-05-31T06:42:53.041256Z","iopub.status.idle":"2022-05-31T06:42:53.048487Z","shell.execute_reply.started":"2022-05-31T06:42:53.041221Z","shell.execute_reply":"2022-05-31T06:42:53.04767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_8_features = []\nfor row in range(8):\n    top_8_features.append(feature_importances[row][0])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:42:53.053658Z","iopub.execute_input":"2022-05-31T06:42:53.054134Z","iopub.status.idle":"2022-05-31T06:42:53.07357Z","shell.execute_reply.started":"2022-05-31T06:42:53.054094Z","shell.execute_reply":"2022-05-31T06:42:53.072403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_top_8 = x_features_no[top_8_features]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:42:53.075315Z","iopub.execute_input":"2022-05-31T06:42:53.079565Z","iopub.status.idle":"2022-05-31T06:42:53.110779Z","shell.execute_reply.started":"2022-05-31T06:42:53.079509Z","shell.execute_reply":"2022-05-31T06:42:53.109764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features, test_features, train_labels, test_labels = train_test_split(x_top_8,target_no, test_size = 0.25, random_state = 42)\n\nrf_top8 = RandomForestRegressor(n_estimators = 100,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            n_jobs = -1,\n                            random_state = 42)\nrf_top8.fit(train_features, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:42:53.112252Z","iopub.execute_input":"2022-05-31T06:42:53.112613Z","iopub.status.idle":"2022-05-31T06:47:47.740185Z","shell.execute_reply.started":"2022-05-31T06:42:53.112579Z","shell.execute_reply":"2022-05-31T06:47:47.739333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions = rf_top8.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:47:47.741388Z","iopub.execute_input":"2022-05-31T06:47:47.741679Z","iopub.status.idle":"2022-05-31T06:47:48.762255Z","shell.execute_reply.started":"2022-05-31T06:47:47.741651Z","shell.execute_reply":"2022-05-31T06:47:48.760582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:47:48.764035Z","iopub.execute_input":"2022-05-31T06:47:48.765Z","iopub.status.idle":"2022-05-31T06:47:48.771783Z","shell.execute_reply.started":"2022-05-31T06:47:48.764948Z","shell.execute_reply":"2022-05-31T06:47:48.77111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = test_df[top_8_features]\ntest_id = test_df['id'].values\n#test_x.drop(\"id\",axis=1,inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:47:48.77281Z","iopub.execute_input":"2022-05-31T06:47:48.773247Z","iopub.status.idle":"2022-05-31T06:47:48.796023Z","shell.execute_reply.started":"2022-05-31T06:47:48.773189Z","shell.execute_reply":"2022-05-31T06:47:48.795268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = rf_top8.predict(test_x.values)\npred","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:47:48.796864Z","iopub.execute_input":"2022-05-31T06:47:48.797478Z","iopub.status.idle":"2022-05-31T06:47:51.945974Z","shell.execute_reply.started":"2022-05-31T06:47:48.797452Z","shell.execute_reply":"2022-05-31T06:47:51.944861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    \"id\" : test_id,\n    \"target\": pred\n})\nsubmission_df.to_csv(\"submission.csv\",index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T06:47:51.947431Z","iopub.execute_input":"2022-05-31T06:47:51.947971Z","iopub.status.idle":"2022-05-31T06:47:54.040075Z","shell.execute_reply.started":"2022-05-31T06:47:51.947938Z","shell.execute_reply":"2022-05-31T06:47:54.038808Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like limiting the features included in the model improved consistently the score of the model (0.84).","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#EAB4DE'></span><span style='color:#EAB4DE'>Disclaimer</span></b>","metadata":{}},{"cell_type":"markdown","source":"This is my first Kaggle competition.\nIn order to do this EDA Analysis I got some inspiration from Notebooks that have been published by other Kagglers and tried to do the best I could with the packages I knew.\nFeel free to add suggestions both on what other analyses could be done and how the results I presented could be done better!","metadata":{}}]}