{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Advanced Keras for TPSMAY22\n\nThis notebook shows how to train a Keras model. It wouldn't have been possible without the open-source contributions of other participants, in particular:\n- [Analysing Interactions with SHAP](https://www.kaggle.com/code/wti200/analysing-interactions-with-shap) where @wti200 shows how to analyze feature interactions\n- [EDA & LGBM Model](https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model) where @cabaxiom introduces the unique_characters feature\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\nimport math\nimport random\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate\nfrom tensorflow.keras.utils import plot_model\n\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:34:54.810749Z","iopub.execute_input":"2022-05-08T21:34:54.811017Z","iopub.status.idle":"2022-05-08T21:34:54.820201Z","shell.execute_reply.started":"2022-05-08T21:34:54.81099Z","shell.execute_reply":"2022-05-08T21:34:54.819514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last n_epochs epochs of) the training history\n    \n    Plots loss and optionally val_loss and lr.\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:33:03.174991Z","iopub.execute_input":"2022-05-08T21:33:03.17536Z","iopub.status.idle":"2022-05-08T21:33:03.191607Z","shell.execute_reply.started":"2022-05-08T21:33:03.175329Z","shell.execute_reply":"2022-05-08T21:33:03.190849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:33:03.192688Z","iopub.execute_input":"2022-05-08T21:33:03.193228Z","iopub.status.idle":"2022-05-08T21:33:13.679592Z","shell.execute_reply.started":"2022-05-08T21:33:03.193182Z","shell.execute_reply":"2022-05-08T21:33:13.678768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Top three feature interactions\n\nIf we look at 2d-scatterplots of the features, colored by target, we find three feature combinations where the projection is subdivided into three distinct regions with sharp borders in between. We can either hope for the neural network to find these borders by itself, or we can help it find them.\n\nAnd how can we help the neural network? We create three ternary categorical features which indicate in which region a sample is. The following diagrams show the three projections which lead to three regions each:","metadata":{}},{"cell_type":"code","source":"plt.rcParams['axes.facecolor'] = 'k'\nplt.figure(figsize=(11, 5))\ncmap = ListedColormap([\"#ffd700\", \"#0057b8\"])\n\nax = plt.subplot(1, 3, 1)\nax.scatter(train['f_02'], train['f_21'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_02')\nax.set_ylabel('f_21')\nax.set_aspect('equal')\nax0 = ax\n\nax = plt.subplot(1, 3, 2, sharex=ax0, sharey=ax0)\nax.scatter(train['f_05'], train['f_22'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_05')\nax.set_ylabel('f_22')\nax.set_aspect('equal')\n\nax = plt.subplot(1, 3, 3, sharex=ax0, sharey=ax0)\nax.scatter(train['f_00'] + train['f_01'], train['f_26'], s=1,\n           c=train.target, cmap=cmap)\nax.set_xlabel('f_00 + f_01')\nax.set_ylabel('f_26')\nax.set_aspect('equal')\n\nplt.tight_layout(w_pad=1.0)\nplt.show()\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:35:07.36666Z","iopub.execute_input":"2022-05-08T21:35:07.367171Z","iopub.status.idle":"2022-05-08T21:35:38.134605Z","shell.execute_reply.started":"2022-05-08T21:35:07.367118Z","shell.execute_reply":"2022-05-08T21:35:38.133735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n\nWe read the data and apply minimal feature engineering:\n- We split the `f_27` string into ten separate features as described in the [EDA](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense).\n- We count the unique characters in the string.\n- We introduce three categorical features for the feature interactions described above.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nfor df in [train, test]:\n    # Extract the 10 letters of f_27 into individual features\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    # Feature interactions: create three ternary features\n    # Every ternary feature can have the values -1, 0 and +1\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    \nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nint_features = [f for f in features if test[f].dtype == int and f.startswith('f')]\nch_features = [f for f in features if f.startswith('ch')]\ntest[features].head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:35:38.136634Z","iopub.execute_input":"2022-05-08T21:35:38.137611Z","iopub.status.idle":"2022-05-08T21:36:08.765932Z","shell.execute_reply.started":"2022-05-08T21:35:38.137557Z","shell.execute_reply":"2022-05-08T21:36:08.764942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The model\n\nThe model is sequential and has four hidden layers. To counter overfitting, I added a kernel_regularizer to all hidden layers.\n","metadata":{}},{"cell_type":"code","source":"def my_model():\n    \"\"\"Simple sequential neural network with four hidden layers.\n    \n    Returns a (not yet compiled) instance of tensorflow.keras.models.Model.\n    \"\"\"\n    activation = 'swish'\n    inputs = Input(shape=(len(features)))\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(inputs)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(1, #kernel_regularizer=tf.keras.regularizers.l2(1e-6),\n              activation='sigmoid',\n             )(x)\n    model = Model(inputs, x)\n    return model\n\nplot_model(my_model(), show_layer_names=False, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:36:08.767548Z","iopub.execute_input":"2022-05-08T21:36:08.767804Z","iopub.status.idle":"2022-05-08T21:36:09.281614Z","shell.execute_reply.started":"2022-05-08T21:36:08.767762Z","shell.execute_reply":"2022-05-08T21:36:09.280544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nFor cross-validation, we use a simple KFold with five splits. It has turned out that the scores of the five splits are very similar so that I usually run only the first split. This one split is good enough to evaluate the model.\n\nI like to first train the model with early stopping to see what are good initial and final learning rates and the number of epochs, and then I switch to cosine learning rate decay. You can switch back to early stopping anytime by setting the parameter `USE_PLATEAU`.","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\nEPOCHS = 200\nEPOCHS_COSINEDECAY = 150\nCYCLES = 1\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nDIAGRAMS = True\nUSE_PLATEAU = False\nBATCH_SIZE = 2048\nONLY_FIRST_FOLD = False\n\n# see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)\n\ndef fit_model(X_tr, y_tr, X_va=None, y_va=None, run=0):\n    \"\"\"Scale the data, fit a model, plot the training history and optionally validate the model\n    \n    Returns a trained instance of tensorflow.keras.models.Model.\n    \n    As a side effect, updates y_va_pred, history_list and score_list.\n    \"\"\"\n    global y_va_pred\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    X_tr = scaler.fit_transform(X_tr)\n    \n    if X_va is not None:\n        X_va = scaler.transform(X_va)\n        validation_data = (X_va, y_va)\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    lr_start=0.01\n    if USE_PLATEAU and X_va is not None: # use early stopping\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=12, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else: # use cosine learning rate decay rather than early stopping\n        epochs = EPOCHS_COSINEDECAY\n        lr_end = 0.0002\n        def cosine_decay(epoch):\n            # w decays from 1 to 0 in every cycle\n            # epoch == 0                  -> w = 1 (first epoch of cycle)\n            # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n            epochs_per_cycle = epochs // CYCLES\n            epoch_in_cycle = epoch % epochs_per_cycle\n            if epochs_per_cycle > 1:\n                w = (1 + math.cos(epoch_in_cycle / (epochs_per_cycle-1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = my_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start),\n                  metrics='AUC',\n                  loss=tf.keras.losses.BinaryCrossentropy())\n\n    # Train the model\n    history = model.fit(X_tr, y_tr, \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    \n    if X_va is None:\n        print(f\"Training loss: {history_list[-1]['loss'][-1]:.4f}\")\n    else:\n        lastloss = f\"Training loss: {history_list[-1]['loss'][-1]:.4f} | Val loss: {history_list[-1]['val_loss'][-1]:.4f}\"\n        \n        # Inference for validation\n        y_va_pred = model.predict(X_va, batch_size=len(X_va), verbose=0)\n        #oof_list[run][val_idx] = y_va_pred\n        \n        # Evaluation: Execution time, loss and AUC\n        score = roc_auc_score(y_va, y_va_pred)\n        print(f\"Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}\"\n              f\" | {lastloss} | AUC: {score:.5f}\")\n        score_list.append(score)\n        \n        if DIAGRAMS and fold == 0 and run == 0:\n            # Plot training history\n            plot_history(history_list[-1], \n                         title=f\"Learning curve (validation AUC = {score:.5f})\",\n                         plot_lr=True)\n\n            # Plot y_true vs. y_pred\n            plt.figure(figsize=(10, 4))\n            plt.hist(y_va_pred[y_va == 0], bins=np.linspace(0, 1, 21),\n                     alpha=0.5, density=True)\n            plt.hist(y_va_pred[y_va == 1], bins=np.linspace(0, 1, 21),\n                     alpha=0.5, density=True)\n            plt.xlabel('y_pred')\n            plt.ylabel('density')\n            plt.title('OOF Predictions')\n            plt.show()\n\n    return model, scaler\n\n\nprint(f\"{len(features)} features\")\nhistory_list = []\nscore_list = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    fit_model(X_tr, y_tr, X_va, y_va)\n    if ONLY_FIRST_FOLD: break # we only need the first fold\n\nprint(f\"OOF AUC:                       {np.mean(score_list):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:36:09.284115Z","iopub.execute_input":"2022-05-08T21:36:09.2845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Three diagrams for model evaluation\n\nWe plot the ROC curve just because it looks nice. The area under the red curve is the score of our model.\n","metadata":{}},{"cell_type":"code","source":"# Plot the roc curve for the last fold\ndef plot_roc_curve(y_va, y_va_pred):\n    plt.figure(figsize=(8, 8))\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n    plt.plot(fpr, tpr, color='r', lw=2)\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n    plt.gca().set_aspect('equal')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic\")\n    plt.show()\n\nplot_roc_curve(y_va, y_va_pred)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:33:14.040702Z","iopub.status.idle":"2022-05-08T21:33:14.040999Z","shell.execute_reply.started":"2022-05-08T21:33:14.04084Z","shell.execute_reply":"2022-05-08T21:33:14.040856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, we plot a histogram of the out-of-fold predictions. Many predictions are near 0.0 or near 1.0; this means that in many cases the classifier's predictions have high confidence:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.hist(y_va_pred, bins=25, density=True)\nplt.title('Histogram of the oof predictions')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:33:14.042238Z","iopub.status.idle":"2022-05-08T21:33:14.042533Z","shell.execute_reply.started":"2022-05-08T21:33:14.042379Z","shell.execute_reply":"2022-05-08T21:33:14.042395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we plot the calibration curve. The curve here is almost a straight line, which means that the predicted probabilities are almost exact: ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nCalibrationDisplay.from_predictions(y_va, y_va_pred, n_bins=50, strategy='quantile', ax=plt.gca())\nplt.title('Probability calibration')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-08T21:33:14.043741Z","iopub.status.idle":"2022-05-08T21:33:14.044034Z","shell.execute_reply.started":"2022-05-08T21:33:14.043879Z","shell.execute_reply":"2022-05-08T21:33:14.043895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nFor the submission, we re-train the model on the complete training data with several different seeds and then submit the mean of the predicted ranks.","metadata":{}},{"cell_type":"code","source":"%%time\n# Create submission\nprint(f\"{len(features)} features\")\n\nX_tr = train[features]\ny_tr = train.target\n\npred_list = []\nfor seed in range(10):\n    # see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    model, scaler = fit_model(X_tr, y_tr, run=seed)\n    pred_list.append(scipy.stats.rankdata(model.predict(scaler.transform(test[features]),\n                                                        batch_size=len(test), verbose=0)))\n    print(f\"{seed:2}\", pred_list[-1])\nprint()\nsubmission = test[['id']].copy()\nsubmission['target'] = np.array(pred_list).mean(axis=0)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:33:14.045299Z","iopub.status.idle":"2022-05-08T21:33:14.045625Z","shell.execute_reply.started":"2022-05-08T21:33:14.045464Z","shell.execute_reply":"2022-05-08T21:33:14.045481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What next?\n\nNow it's your turn! Try to improve this model by\n- Changing the network architecture \n- Engineering more features\n- Tuning hyperparameters, optimizers, learning rate schedules and so on...\n\nOr, if you prefer gradient boosting, you can have a look at the [Gradient Boosting Quickstart](https://www.kaggle.com/ambrosm/tpsmay22-gradient-boosting-quickstart).\n","metadata":{}}]}