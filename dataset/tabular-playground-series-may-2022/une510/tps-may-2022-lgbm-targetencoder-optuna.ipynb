{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b>UPDATES:</b>\n\n5/27/22\n\n- Reconstructed the Optuna function and added separate cross-validation technique.\n\n5/30/22\n\n- Used bag of words and added a new feature counting the amount of unique letters in each entry. \n- Added visualizations to said features.\n- Changed Cross Validation code to a ROC AUC plot.\n\n<b>TO-DO-LIST:</b>\n\n- Experiment more w/ feature engineering (will try One-Hot encoding first, then try counting the frequencies of letters. Possibly make a Document Term Matrix)\n\n\nThis is my submission for the TPS May 2022 competition. For my notebook, I will be using the LGBM classification model along with Optuna to tune the hyperparameters.\n\nTo start, we will import the needed packages.","metadata":{}},{"cell_type":"code","source":"from string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, RocCurveDisplay\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport optuna\nfrom lightgbm import LGBMClassifier, plot_importance\nfrom category_encoders import TargetEncoder\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T19:22:25.418068Z","iopub.execute_input":"2022-05-31T19:22:25.418275Z","iopub.status.idle":"2022-05-31T19:22:25.435888Z","shell.execute_reply.started":"2022-05-31T19:22:25.418252Z","shell.execute_reply":"2022-05-31T19:22:25.43478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_path = '../input/tabular-playground-series-may-2022/' #the main folder where all the csv files are located\n\n#configure options for pandas\npd.set_option('display.float_format', '{:4f}'.format)\npd.set_option('display.max_columns', None)\n\n#load datsets\ntrain = pd.read_csv(main_path + 'train.csv', index_col='id')\ntest = pd.read_csv(main_path + 'test.csv', index_col='id')\nall_data = pd.concat([train,test])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:25.437685Z","iopub.execute_input":"2022-05-31T19:22:25.438077Z","iopub.status.idle":"2022-05-31T19:22:39.284583Z","shell.execute_reply.started":"2022-05-31T19:22:25.43804Z","shell.execute_reply":"2022-05-31T19:22:39.283452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Profiling","metadata":{}},{"cell_type":"code","source":"#Disply dimensions of each dataset\nprint('Shape of training dataset: {} rows and {} columns'.format(train.shape[0], train.shape[1]))\nprint('Shape of testing dataset: {} rows and {} columns'.format(test.shape[0], test.shape[1]))\nprint('Shape of all data: {} rows and {} columns'.format(all_data.shape[0], all_data.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:39.285717Z","iopub.execute_input":"2022-05-31T19:22:39.285883Z","iopub.status.idle":"2022-05-31T19:22:39.292894Z","shell.execute_reply.started":"2022-05-31T19:22:39.285861Z","shell.execute_reply":"2022-05-31T19:22:39.291708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the combining the data, it turns out that there are over 1.5 million entries of numbers. The features do not mean anything; However, features 7 to 18 could indicate the following:\n- The values are encoded from ordinal data\n- The values indicate either a quantity or a rank","metadata":{}},{"cell_type":"code","source":"all_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:39.295011Z","iopub.execute_input":"2022-05-31T19:22:39.295353Z","iopub.status.idle":"2022-05-31T19:22:39.452709Z","shell.execute_reply.started":"2022-05-31T19:22:39.295256Z","shell.execute_reply":"2022-05-31T19:22:39.451842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this list, we can see that there are no missing values. The target feature is indicating that there are missing values becuse the testing dataset doesn't have a target feature.","metadata":{}},{"cell_type":"code","source":"all_data.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:39.453947Z","iopub.execute_input":"2022-05-31T19:22:39.454144Z","iopub.status.idle":"2022-05-31T19:22:41.194104Z","shell.execute_reply.started":"2022-05-31T19:22:39.454106Z","shell.execute_reply":"2022-05-31T19:22:41.193116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:41.195327Z","iopub.execute_input":"2022-05-31T19:22:41.195545Z","iopub.status.idle":"2022-05-31T19:22:41.208793Z","shell.execute_reply.started":"2022-05-31T19:22:41.195522Z","shell.execute_reply":"2022-05-31T19:22:41.20802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display the percentages of each label\nprint('Percentage of target entries with a value of 0: {:.4f}'.format(len(train.query('target==0')) / len(train)))\nprint('Percentage of target entries with a value of 1: {:.4f}'.format(len(train.query('target==1')) / len(train)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:41.209769Z","iopub.execute_input":"2022-05-31T19:22:41.210248Z","iopub.status.idle":"2022-05-31T19:22:41.444191Z","shell.execute_reply.started":"2022-05-31T19:22:41.21018Z","shell.execute_reply":"2022-05-31T19:22:41.44256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the last two cells, we can see that the target vaiables are slightly imbalanced.\n\n## Feature Engineering\n\nNotice that in the dataset, we have a categorical feature called f_27, which shows a sequence of letters. This could potentially be used for feature engineering so our model will work better. First, let's see of there are any frequencies in this feature.","metadata":{"execution":{"iopub.status.busy":"2022-05-05T07:35:33.322854Z","iopub.execute_input":"2022-05-05T07:35:33.323777Z","iopub.status.idle":"2022-05-05T07:35:33.331106Z","shell.execute_reply.started":"2022-05-05T07:35:33.323705Z","shell.execute_reply":"2022-05-05T07:35:33.329055Z"}}},{"cell_type":"code","source":"def create_countplot(x, title, ax=None, data=all_data):  \n    values = data[x].value_counts(ascending=False)\n    g = sns.countplot(x=x, data=data, order=values.index, ax=ax)\n    g.set_title(title)\n    g.set_xlabel('Letter')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:41.446049Z","iopub.execute_input":"2022-05-31T19:22:41.446262Z","iopub.status.idle":"2022-05-31T19:22:41.451022Z","shell.execute_reply.started":"2022-05-31T19:22:41.446238Z","shell.execute_reply":"2022-05-31T19:22:41.450267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.f_27.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:41.45173Z","iopub.execute_input":"2022-05-31T19:22:41.451914Z","iopub.status.idle":"2022-05-31T19:22:43.739602Z","shell.execute_reply.started":"2022-05-31T19:22:41.451885Z","shell.execute_reply":"2022-05-31T19:22:43.738435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of the dataset with over 1.5 million entries, there are at most 15 frquencies! Let's see how many unique entries there are:","metadata":{}},{"cell_type":"code","source":"all_data.f_27.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:43.74281Z","iopub.execute_input":"2022-05-31T19:22:43.74302Z","iopub.status.idle":"2022-05-31T19:22:44.532399Z","shell.execute_reply.started":"2022-05-31T19:22:43.742997Z","shell.execute_reply":"2022-05-31T19:22:44.53127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are over 1.1 million unique entries. Theoretically, it could be possible that these are routines for the machine. Now that this feature is analyzed, we can start our engineering process. I will begin with counting the amount of unique letters in each entry, then splitting each string into its own feature.","metadata":{}},{"cell_type":"code","source":"#Amount of different letters\nfunc = lambda x: len(set(x))\ntrain['n_unique'] = train.f_27.apply(func)\ntest['n_unique'] = test.f_27.apply(func)\nall_data['n_unique'] = all_data.f_27.apply(func)\n\n#Split strings into separate features\nnew_features = [f'f_27{i}' for i in 'abcdefghij']\ntrain[new_features] = train.f_27.str.split('', expand=True).loc[:,1:10]\ntest[new_features] = test.f_27.str.split('', expand=True).loc[:,1:10]\nall_data[new_features] = all_data.f_27.str.split('', expand=True).loc[:,1:10]\nall_data[new_features].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:22:44.533605Z","iopub.execute_input":"2022-05-31T19:22:44.533832Z","iopub.status.idle":"2022-05-31T19:22:48.931561Z","shell.execute_reply.started":"2022-05-31T19:22:44.5338Z","shell.execute_reply":"2022-05-31T19:22:48.930475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display frequency details\ndescriptive_stats = all_data[new_features].describe().T\ndescriptive_stats = descriptive_stats.sort_values('unique', ascending=False)\ndescriptive_stats","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:04.953241Z","iopub.execute_input":"2022-05-31T19:23:04.953579Z","iopub.status.idle":"2022-05-31T19:23:06.796734Z","shell.execute_reply.started":"2022-05-31T19:23:04.953542Z","shell.execute_reply":"2022-05-31T19:23:06.796019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of these new features have only 2, 10, 15, and 20 unique values. Each feature uses the sequence of the alpha ending in B, O, and T.\n\nWe can now plot the count values for each position.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 2, figsize=(20,15))\nfor i, (ax, letter) in enumerate(zip(ax.flatten(), ascii_letters[:10])):\n    create_countplot(f'f_27{letter}', f'Position {i+1}', ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:06.797686Z","iopub.execute_input":"2022-05-31T19:23:06.797869Z","iopub.status.idle":"2022-05-31T19:23:19.506247Z","shell.execute_reply.started":"2022-05-31T19:23:06.797838Z","shell.execute_reply":"2022-05-31T19:23:19.505672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data['n_unique'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:19.507352Z","iopub.execute_input":"2022-05-31T19:23:19.507726Z","iopub.status.idle":"2022-05-31T19:23:19.555879Z","shell.execute_reply.started":"2022-05-31T19:23:19.507685Z","shell.execute_reply":"2022-05-31T19:23:19.555047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(x='n_unique', kde=True, bins=9, data=all_data);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:19.55733Z","iopub.execute_input":"2022-05-31T19:23:19.55763Z","iopub.status.idle":"2022-05-31T19:23:27.174864Z","shell.execute_reply.started":"2022-05-31T19:23:19.557589Z","shell.execute_reply":"2022-05-31T19:23:27.173821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation","metadata":{}},{"cell_type":"code","source":"#Correlation table\ncorr = all_data.corr()\nplt.figure(figsize=(18,18))\n\n#Hide the upper triangular part of the coorelation matrix\n#Code from: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n#This will create a diagonal correlation matric\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, linewidth=0.1, fmt='.2f', annot=True, annot_kws={'size': 8});","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:27.176229Z","iopub.execute_input":"2022-05-31T19:23:27.177127Z","iopub.status.idle":"2022-05-31T19:23:32.886616Z","shell.execute_reply.started":"2022-05-31T19:23:27.177075Z","shell.execute_reply":"2022-05-31T19:23:32.885596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation heatmap shows that barely any of the features are correlated. The highest values are around 30-33%.\n\n## EDA\n\nThere isn't mush exploration we can do with the data, except for checking for normality and outliers. Thus, I will plot histograms and boxplots to do exactly that.\n\nSince there are alot of features, I will split them in half so less memory can be used, and the plots are more readable.","metadata":{}},{"cell_type":"markdown","source":"### Histogram","metadata":{}},{"cell_type":"code","source":"#Split numerical columns in half for easier plotting\nnum_cols = list(all_data.select_dtypes(exclude=['object']))\nmidpoint = len(num_cols) // 2\ncols_first_half = num_cols[:midpoint]\ncols_second_half = num_cols[midpoint:]\ncols_second_half.pop(-1)\nkde_params = dict(data=all_data, shade=True, palette=['red','green'], hue='target')\nsubplot_params = dict(nrows=5, ncols=3, figsize=(20,20))\n\n#Plot the histogram for each feture on each axis\nfig, ax = plt.subplots(**subplot_params)\n\n#First half\nfor ax, col in zip(ax.flatten(), cols_first_half):\n    sns.kdeplot(x=col, ax=ax, **kde_params)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:23:32.887784Z","iopub.execute_input":"2022-05-31T19:23:32.887982Z","iopub.status.idle":"2022-05-31T19:25:07.057673Z","shell.execute_reply.started":"2022-05-31T19:23:32.887956Z","shell.execute_reply":"2022-05-31T19:25:07.057224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(**subplot_params)\n\n#Second half\nfor ax, col in zip(ax.flatten(), cols_second_half):\n    sns.kdeplot(x=col, ax=ax, **kde_params)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:25:07.058493Z","iopub.execute_input":"2022-05-31T19:25:07.058793Z","iopub.status.idle":"2022-05-31T19:26:07.72024Z","shell.execute_reply.started":"2022-05-31T19:25:07.05877Z","shell.execute_reply":"2022-05-31T19:26:07.716453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histograms, we can see that:\n- All of the continuous data is normally distributed\n- Histograms with multiple peaks show that the data is multimodal, and proves my point earlier that the data could either be discrete or already encoded\n- Feature 29 could've been formerly a boolean feature that was encoded\n- Feature 30 is a multiclass label feature\n\n### Boxplots","metadata":{}},{"cell_type":"code","source":"#Do the same with boxplots\nfig, ax = plt.subplots(5, 3, figsize=(15,10))\nfor ax, col in zip(ax.flatten(), cols_first_half):\n    sns.boxplot(x=col, data=all_data, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:26:07.722024Z","iopub.execute_input":"2022-05-31T19:26:07.722298Z","iopub.status.idle":"2022-05-31T19:26:11.550797Z","shell.execute_reply.started":"2022-05-31T19:26:07.722264Z","shell.execute_reply":"2022-05-31T19:26:11.549479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(4, 4, figsize=(15,10))\nfor ax, col in zip(ax.flatten(), cols_second_half):\n    sns.boxplot(x=col, data=all_data, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:26:11.55223Z","iopub.execute_input":"2022-05-31T19:26:11.552526Z","iopub.status.idle":"2022-05-31T19:26:14.765823Z","shell.execute_reply.started":"2022-05-31T19:26:11.552487Z","shell.execute_reply":"2022-05-31T19:26:14.764697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:26:14.767285Z","iopub.execute_input":"2022-05-31T19:26:14.767595Z","iopub.status.idle":"2022-05-31T19:26:14.771944Z","shell.execute_reply.started":"2022-05-31T19:26:14.767555Z","shell.execute_reply":"2022-05-31T19:26:14.771037Z"}}},{"cell_type":"code","source":"#Split data into training and testing\nX = train.drop(['f_27','target'], axis=1)\ny = train.target\nX_test = test.drop('f_27', axis=1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0, stratify=y)\n\nprint('Shape of training set: {} rows and {} columns'.format(X_train.shape[0], X_train.shape[1]))\nprint('Shape of validation set: {} rows and {} columns'.format(X_valid.shape[0], X_valid.shape[1]))\n\n#This will be used to encode catrgorical features\nencoder = TargetEncoder(smoothing=5)\n#transformer = make_column_transformer((TfidfVectorizer(analyzer='char'), 'f_27'), remainder='passthrough')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:26:14.772989Z","iopub.execute_input":"2022-05-31T19:26:14.774592Z","iopub.status.idle":"2022-05-31T19:26:16.861517Z","shell.execute_reply.started":"2022-05-31T19:26:14.774485Z","shell.execute_reply":"2022-05-31T19:26:16.86055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tune hyperparameters\ndef objective(trial):\n    param_grid = dict(n_estimators=trial.suggest_int('n_estimators', 20, 1000, 10), \n                      learning_rate=trial.suggest_float('learning_rate', 0, 1), \n                      max_depth=trial.suggest_int('max_depth', 3, 12), \n                      min_split_gain=trial.suggest_float('min_split_gain', 0, 5), \n                      min_child_weight=trial.suggest_float('min_child_weight', 1, 10), \n                      colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1),\n                      subsample=trial.suggest_float(\"subsample\", 0.2, 1),\n                     )\n \n    clf = make_pipeline(encoder, LGBMClassifier(**param_grid))\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict_proba(X_valid)[:,1]\n    return roc_auc_score(y_valid, y_pred).round(5)\n\nstudy = optuna.create_study(direction='maximize', study_name='Hyperparameter Tuning')\n\n#Test different hyperparameters 30 times\nstudy.optimize(objective, n_trials=30, show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:26:16.862733Z","iopub.execute_input":"2022-05-31T19:26:16.862969Z","iopub.status.idle":"2022-05-31T19:59:43.017833Z","shell.execute_reply.started":"2022-05-31T19:26:16.862932Z","shell.execute_reply":"2022-05-31T19:59:43.016738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gest the best parameters\nbest_params = study.best_params\nprint('Best parameter for:')\nfor k, v in best_params.items():\n    print('{}: {}'.format(k,v))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:59:43.01997Z","iopub.execute_input":"2022-05-31T19:59:43.020264Z","iopub.status.idle":"2022-05-31T19:59:43.028067Z","shell.execute_reply.started":"2022-05-31T19:59:43.020217Z","shell.execute_reply":"2022-05-31T19:59:43.026967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the K-Fold ROC AUC plot\n#Code from:\n#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n#https://www.kaggle.com/code/kanncaa1/roc-curve-with-k-fold-cv/notebook\n\ncv = StratifiedKFold(10)\nmodel = make_pipeline(encoder, LGBMClassifier(**best_params))\n\ntprs, aucs = [], []\navg_fpr = np.linspace(0,1,100)\nfig, ax = plt.subplots(figsize=(8,8))\nlim = [0,1]\n\nfor i, (train_index, valid_index) in enumerate(cv.split(X,y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict_proba(X_valid)[:,1]\n    \n    fpr, tpr, thresh = roc_curve(y_valid, y_pred)\n    tprs.append(np.interp(avg_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    \n    #roc_auc_plot = RocCurveDisplay.from_estimator(model, X_valid, y_valid, name=f'ROC Fold {i+1}', ax=ax)\n    plt.plot(fpr, tpr, label=f'ROC Fold {i+1} (AUC = {roc_auc:.5f})') #Display this on a legend\n    #plt.legend(loc='best')\n\nax.plot(lim, lim, linestyle='--', color='r', label='Chance') #Draw the chance boundary\n\navg_tpr = np.mean(tprs, axis=0)\nstd_tpr = np.std(tprs, axis=0)\navg_auc = auc(avg_fpr, avg_tpr)\nstd_auc = np.std(aucs)\ntprs_lower = np.maximum(avg_tpr - std_tpr, 0)\ntprs_upper = np.minimum(avg_tpr + std_tpr, 1)\n\nax.plot(avg_fpr, avg_tpr, color='b', label=f'Mean ROC (AUC = {avg_auc:.5f} $\\pm$ {std_auc:.5f})')\n\n#Display standard deviation voundaries\nax.fill_between(avg_fpr, tprs_lower, tprs_upper, color='grey', alpha=.8, label=f'Std. Dev. = {std_auc:.5f}');\nax.set(title='ROC AUC Plot', xlabel='False Positive Rate', ylabel='True Positive Rate')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-31T19:59:43.029717Z","iopub.execute_input":"2022-05-31T19:59:43.029993Z","iopub.status.idle":"2022-05-31T20:12:44.513399Z","shell.execute_reply.started":"2022-05-31T19:59:43.029954Z","shell.execute_reply":"2022-05-31T20:12:44.512468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create importance plot using our model\nplot_importance(model['lgbmclassifier'], dpi=90, figsize=(7,7));","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:18:19.70796Z","iopub.execute_input":"2022-05-31T20:18:19.708249Z","iopub.status.idle":"2022-05-31T20:18:20.335474Z","shell.execute_reply.started":"2022-05-31T20:18:19.708219Z","shell.execute_reply":"2022-05-31T20:18:20.334562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train)\ny_pred = model.predict_proba(X_test)[:,1]\n\n#Make submission file\nout = pd.DataFrame({'id': test.index, 'target': y_pred})\nout.to_csv('results.csv', index=False)\nout","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:18:39.741534Z","iopub.execute_input":"2022-05-31T20:18:39.741847Z","iopub.status.idle":"2022-05-31T20:20:03.065597Z","shell.execute_reply.started":"2022-05-31T20:18:39.741812Z","shell.execute_reply":"2022-05-31T20:20:03.064852Z"},"trusted":true},"execution_count":null,"outputs":[]}]}