{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kagglesdsdata/datasets/1900394/3155713/Lake4.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20220208%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220208T073857Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=1c61d4e9610dc576f0d193b7e095f8581c88b0224a1de29ffc5d4b9aa1a6f7e52203e39802cf2b9f71c6f8be82adb204f7b95b182759c11f1dcb406246e4c769ffc71428a2397faf36266010d8b6a120687f9127abcfd92dc20a7fd83c3d19c036ecca0811625cf7f0d3412c3a2c2cc1f70018c28f17e93fd23cc4bee1a1fba0766822f5c07b84ef65c604e16b9ec4c2163e134fb0061ae59890afd37279225992f62fc9d424f396c8bd94ada455fdfe867578eb7c4efcdb7a7b2d0ce5af8ac3e8f5d2cac2a9fc0f93c0638914db2e60910ffaca0a81bed3f1e8e2213e5ffaa60aa16cd14adb48ef613dbf90506d20e4df7a2a3c028dbe66fe894ed15fe1cd8f)","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Definition\" data-toc-modified-id=\"Problem-Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Definition</a></span></li><li><span><a href=\"#Data-Collection\" data-toc-modified-id=\"Data-Collection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Collection</a></span></li><li><span><a href=\"#Data-Analysis-and-Preprocessing\" data-toc-modified-id=\"Data-Analysis-and-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Analysis and Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Timestamps\" data-toc-modified-id=\"Timestamps-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Timestamps</a></span></li><li><span><a href=\"#Descriptive-Statistics\" data-toc-modified-id=\"Descriptive-Statistics-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Descriptive Statistics</a></span></li><li><span><a href=\"#Time-Series-Plots---Daily\" data-toc-modified-id=\"Time-Series-Plots---Daily-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Time Series Plots - Daily</a></span></li><li><span><a href=\"#Drop-Nulls-before-2006\" data-toc-modified-id=\"Drop-Nulls-before-2006-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Drop Nulls before 2006</a></span></li><li><span><a href=\"#Implausible-Zeroes\" data-toc-modified-id=\"Implausible-Zeroes-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Implausible Zeroes</a></span></li><li><span><a href=\"#Nulls-Diagram\" data-toc-modified-id=\"Nulls-Diagram-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Nulls Diagram</a></span></li><li><span><a href=\"#Time-Series-Plots---Weekly-since-2006\" data-toc-modified-id=\"Time-Series-Plots---Weekly-since-2006-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Time Series Plots - Weekly since 2006</a></span></li><li><span><a href=\"#Nulls-Processing\" data-toc-modified-id=\"Nulls-Processing-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Nulls Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fields-with-Alone-standing-Nulls\" data-toc-modified-id=\"Fields-with-Alone-standing-Nulls-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Fields with Alone-standing Nulls</a></span></li><li><span><a href=\"#Rainfall-Piaggione\" data-toc-modified-id=\"Rainfall-Piaggione-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Rainfall Piaggione</a></span></li><li><span><a href=\"#Temperature_Monte_Serra\" data-toc-modified-id=\"Temperature_Monte_Serra-3.8.3\"><span class=\"toc-item-num\">3.8.3&nbsp;&nbsp;</span>Temperature_Monte_Serra</a></span></li><li><span><a href=\"#Temperature_Ponte_a_Moriano\" data-toc-modified-id=\"Temperature_Ponte_a_Moriano-3.8.4\"><span class=\"toc-item-num\">3.8.4&nbsp;&nbsp;</span>Temperature_Ponte_a_Moriano</a></span></li><li><span><a href=\"#Hydrometry_Monte_S_Quirico\" data-toc-modified-id=\"Hydrometry_Monte_S_Quirico-3.8.5\"><span class=\"toc-item-num\">3.8.5&nbsp;&nbsp;</span>Hydrometry_Monte_S_Quirico</a></span></li><li><span><a href=\"#Select-Data-only-since-2011\" data-toc-modified-id=\"Select-Data-only-since-2011-3.8.6\"><span class=\"toc-item-num\">3.8.6&nbsp;&nbsp;</span>Select Data only since 2011</a></span></li><li><span><a href=\"#Hydrometry_Piaggione\" data-toc-modified-id=\"Hydrometry_Piaggione-3.8.7\"><span class=\"toc-item-num\">3.8.7&nbsp;&nbsp;</span>Hydrometry_Piaggione</a></span></li><li><span><a href=\"#Depth_to_Groundwater_SAL\" data-toc-modified-id=\"Depth_to_Groundwater_SAL-3.8.8\"><span class=\"toc-item-num\">3.8.8&nbsp;&nbsp;</span>Depth_to_Groundwater_SAL</a></span></li></ul></li></ul></li><li><span><a href=\"#Split-into-train-and-test-sets\" data-toc-modified-id=\"Split-into-train-and-test-sets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Split into train and test sets</a></span></li><li><span><a href=\"#Scaling\" data-toc-modified-id=\"Scaling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scaling</a></span></li><li><span><a href=\"#Adding-Features\" data-toc-modified-id=\"Adding-Features-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Adding Features</a></span></li><li><span><a href=\"#Target-Value-Smoothing\" data-toc-modified-id=\"Target-Value-Smoothing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Target Value Smoothing</a></span></li><li><span><a href=\"#Splits-for-CV\" data-toc-modified-id=\"Splits-for-CV-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Splits for CV</a></span></li><li><span><a href=\"#Own-Value-Model\" data-toc-modified-id=\"Own-Value-Model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Own Value Model</a></span></li><li><span><a href=\"#Predictions-on-Actual-Data\" data-toc-modified-id=\"Predictions-on-Actual-Data-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Predictions on Actual Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Model Selection</a></span></li><li><span><a href=\"#Model-Parameters-Selection\" data-toc-modified-id=\"Model-Parameters-Selection-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Model Parameters Selection</a></span></li><li><span><a href=\"#Features-Selection\" data-toc-modified-id=\"Features-Selection-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Features Selection</a></span></li><li><span><a href=\"#Own-Value-Lags-Selection\" data-toc-modified-id=\"Own-Value-Lags-Selection-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Own Value Lags Selection</a></span></li><li><span><a href=\"#Features-Lags-Selection\" data-toc-modified-id=\"Features-Lags-Selection-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Features Lags Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rain-Lags-Selection\" data-toc-modified-id=\"Rain-Lags-Selection-10.5.1\"><span class=\"toc-item-num\">10.5.1&nbsp;&nbsp;</span>Rain Lags Selection</a></span></li><li><span><a href=\"#Temperature-Lags-Selection\" data-toc-modified-id=\"Temperature-Lags-Selection-10.5.2\"><span class=\"toc-item-num\">10.5.2&nbsp;&nbsp;</span>Temperature Lags Selection</a></span></li><li><span><a href=\"#Volume-Lags-Selection\" data-toc-modified-id=\"Volume-Lags-Selection-10.5.3\"><span class=\"toc-item-num\">10.5.3&nbsp;&nbsp;</span>Volume Lags Selection</a></span></li></ul></li><li><span><a href=\"#Features-Selection-2\" data-toc-modified-id=\"Features-Selection-2-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Features Selection 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pipeline-without-Hydrometry\" data-toc-modified-id=\"Pipeline-without-Hydrometry-10.6.1\"><span class=\"toc-item-num\">10.6.1&nbsp;&nbsp;</span>Pipeline without Hydrometry</a></span></li></ul></li><li><span><a href=\"#Hydrometry-adding\" data-toc-modified-id=\"Hydrometry-adding-10.7\"><span class=\"toc-item-num\">10.7&nbsp;&nbsp;</span>Hydrometry adding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pipeline-with-Hydrometry\" data-toc-modified-id=\"Pipeline-with-Hydrometry-10.7.1\"><span class=\"toc-item-num\">10.7.1&nbsp;&nbsp;</span>Pipeline with Hydrometry</a></span></li></ul></li></ul></li><li><span><a href=\"#Filling-Nulls-of-Depth-to-Groundwater-SAL\" data-toc-modified-id=\"Filling-Nulls-of-Depth-to-Groundwater-SAL-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Filling Nulls of Depth to Groundwater SAL</a></span></li><li><span><a href=\"#Train-Set-Selection\" data-toc-modified-id=\"Train-Set-Selection-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Train Set Selection</a></span></li><li><span><a href=\"#Predictions-on-predicted-data\" data-toc-modified-id=\"Predictions-on-predicted-data-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Predictions on predicted data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Volume-Prediction\" data-toc-modified-id=\"Volume-Prediction-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Volume Prediction</a></span></li><li><span><a href=\"#Target-Value-Prediction\" data-toc-modified-id=\"Target-Value-Prediction-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Target Value Prediction</a></span></li><li><span><a href=\"#Hydrometry-Prediction\" data-toc-modified-id=\"Hydrometry-Prediction-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Hydrometry Prediction</a></span></li></ul></li><li><span><a href=\"#Model-Evaluation-on-Test-Set\" data-toc-modified-id=\"Model-Evaluation-on-Test-Set-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Model Evaluation on Test Set</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>","metadata":{"execution":{"iopub.status.busy":"2022-02-07T18:32:08.29538Z","iopub.execute_input":"2022-02-07T18:32:08.295759Z","iopub.status.idle":"2022-02-07T18:32:08.349624Z","shell.execute_reply.started":"2022-02-07T18:32:08.295676Z","shell.execute_reply":"2022-02-07T18:32:08.34873Z"}}},{"cell_type":"markdown","source":"# Problem Definition","metadata":{}},{"cell_type":"markdown","source":"**Our goal is to predict the future amount of water in Italy aquifer Auser.**\n\n**This is a multivariate time series regression task.**\n\nAs an inpute we have daily timeseries data, including:\n- temperature;\n- rainfall amount;\n- water volume, taken away for drinking water treatment plants;\n- groundwater level.\n\nAll values are given for multiple areas around Auser aquifer.\n\nGroundwater level are given in two different forms: hydrometry and depth to groundwater.\n- Hydrometry indicates the groundwater level, detected by the hydrometric station (experessed in meters);\n- Depth to groundwater indicates the groundwater level, expressed in ground level (meters from the ground floor), detected by the piezometer\n\nOur goal is to predict the future value of attiribute \"Depth to groundwater\".\n\nAs we will see below, the dataset contains several fields for \"Depth to groundwater\":\n- Depth_to_Groundwater_LT2\n- Depth_to_Groundwater_SAL\n- Depth_to_Groundwater_PAG\n- Depth_to_Groundwater_CoS\n- Depth_to_Groundwater_DIEC\n\n(groudwater level, detected in different places of Auser aquifer)\n\nAccording to requirements of this Kaggle competition, we have to predict three series: Depth_to_Groundwater_SAL, Depth_to_Groundwater_CoS, Depth_to_Groundwater_LT2.  \n**But in this notebook I will focus only on prediction of Depth_to_Groundwater_SAL.**\n\nIn this Kaggle competition there is no strict requirements for the number of periods in the future that need to be predicted. We have to choose prediction depth by ourselfs.\n\n**So I've chosen prediction depth = 3 month.**  \nBecause on the one side weather forecasters can approximately forecast the temperature and rainfall for this timespan, and on the other side 3 month is not so shirt interval: the water company probably may have time to take some actions, if necessary.","metadata":{}},{"cell_type":"markdown","source":"# Data Collection","metadata":{}},{"cell_type":"code","source":"!pip install -U scikit-learn==1.0.2","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:33.790591Z","iopub.execute_input":"2022-02-06T11:54:33.791209Z","iopub.status.idle":"2022-02-06T11:54:50.421395Z","shell.execute_reply.started":"2022-02-06T11:54:33.791164Z","shell.execute_reply":"2022-02-06T11:54:50.420659Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraries imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\n\nfrom pandas.plotting import scatter_matrix\n\nfrom matplotlib import cm\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.dates as mdates\n\nfrom datetime import timedelta\nfrom pandas.tseries.offsets import Day\n\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom IPython.core.debugger import set_trace\n\nimport warnings","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:54:50.423265Z","iopub.execute_input":"2022-02-06T11:54:50.4236Z","iopub.status.idle":"2022-02-06T11:54:51.791896Z","shell.execute_reply.started":"2022-02-06T11:54:50.423541Z","shell.execute_reply":"2022-02-06T11:54:51.791133Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data collection\nAuser = pd.read_csv('/kaggle/input/acea-water-prediction/Aquifer_Auser.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:51.792944Z","iopub.execute_input":"2022-02-06T11:54:51.793181Z","iopub.status.idle":"2022-02-06T11:54:51.878348Z","shell.execute_reply.started":"2022-02-06T11:54:51.793154Z","shell.execute_reply":"2022-02-06T11:54:51.877336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Auser.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:51.879705Z","iopub.execute_input":"2022-02-06T11:54:51.880117Z","iopub.status.idle":"2022-02-06T11:54:51.909112Z","shell.execute_reply.started":"2022-02-06T11:54:51.880051Z","shell.execute_reply":"2022-02-06T11:54:51.908153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Auser","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:51.911598Z","iopub.execute_input":"2022-02-06T11:54:51.912223Z","iopub.status.idle":"2022-02-06T11:54:51.955701Z","shell.execute_reply.started":"2022-02-06T11:54:51.912173Z","shell.execute_reply":"2022-02-06T11:54:51.954959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Timestamps","metadata":{}},{"cell_type":"markdown","source":"First of all let's convert date field to datetime pandas type, check dates order and continuity and move it to index.","metadata":{}},{"cell_type":"code","source":"# Convert date to pandas datetime type:\nAuser['Date'] = pd.to_datetime(Auser['Date'], format='%d/%m/%Y')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:51.956642Z","iopub.execute_input":"2022-02-06T11:54:51.956825Z","iopub.status.idle":"2022-02-06T11:54:51.982754Z","shell.execute_reply.started":"2022-02-06T11:54:51.956801Z","shell.execute_reply":"2022-02-06T11:54:51.981766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print min and max date:\nprint('Min date:', Auser['Date'].min().strftime('%Y-%m-%d'), \n      '\\nMax date:', Auser['Date'].max().strftime('%Y-%m-%d'))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:51.983987Z","iopub.execute_input":"2022-02-06T11:54:51.984557Z","iopub.status.idle":"2022-02-06T11:54:51.994469Z","shell.execute_reply.started":"2022-02-06T11:54:51.984525Z","shell.execute_reply":"2022-02-06T11:54:51.993716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking dates order and continuity\n# Generating dates with same start and end date as in field Date\ndates = pd.date_range(start='1998-03-05', end='2020-06-30')\n# Check if generated dates are equal to field Date\nAuser.Date.equals(pd.Series(dates))","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:54:51.996063Z","iopub.execute_input":"2022-02-06T11:54:51.996634Z","iopub.status.idle":"2022-02-06T11:54:52.008218Z","shell.execute_reply.started":"2022-02-06T11:54:51.996594Z","shell.execute_reply":"2022-02-06T11:54:52.007585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now we know, that field Date is chronological and continouos. We can use this field as index:","metadata":{}},{"cell_type":"code","source":"Auser.set_index('Date', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:52.009445Z","iopub.execute_input":"2022-02-06T11:54:52.009846Z","iopub.status.idle":"2022-02-06T11:54:52.016629Z","shell.execute_reply.started":"2022-02-06T11:54:52.009807Z","shell.execute_reply":"2022-02-06T11:54:52.015874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive Statistics","metadata":{}},{"cell_type":"markdown","source":"Let's display some decsriptive statistics.","metadata":{}},{"cell_type":"code","source":"# Descriptive statistics user function\ndef my_describe(df):\n    print(df.shape[0], 'rows,', df.shape[1], 'columns')\n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())    \n    distinct = df.apply(lambda x: x.value_counts().count())\n    nulls = df.apply(lambda x: x.isna().sum())   \n    min_ = pd.Series(\n        [df[c].min() if df[c].dtype == np.dtype('float64') else None for c in df]\n        , index=df.columns)    \n    mean = pd.Series(\n        [round(df[c].mean(), 1) if df[c].dtype == np.dtype('float64') else None for c in df]\n        , index=df.columns)\n    median = pd.Series(\n        [round(df[c].median(), 1) if df[c].dtype == np.dtype('float64') else None for c in df]\n        , index=df.columns)\n    max_ = pd.Series(\n        [df[c].max() if df[c].dtype == np.dtype('float64') else None for c in df]\n        , index=df.columns)    \n    std = pd.Series(\n        [df[c].std() if df[c].dtype == np.dtype('float64') else None for c in df]\n        , index=df.columns)    \n    null_ratio = df.apply(lambda x: '{:.0%}'.format(round(x.isna().sum() / df.shape[0], 2)))   \n    most_frequent_value = df.apply(lambda x: x.value_counts().index[0])\n    most_frequent_ratio = df.apply(lambda x: '{:.0%}'.format(round(x.value_counts().iloc[0]/df.shape[0], 2)))\n    sec_frequent_value = df.apply(lambda x: x.value_counts().index[1])\n    sec_frequent_ratio = df.apply(lambda x: '{:.0%}'.format(round(x.value_counts().iloc[1]/df.shape[0], 2)))\n    \n    result = pd.concat([counts, distinct, nulls, min_, mean, median, max_, std, null_ratio,\n                        most_frequent_value, most_frequent_ratio, sec_frequent_value, sec_frequent_ratio\n                       ], axis=1)\n    result.columns = ['count', 'distinct', 'nulls', 'min', 'mean', 'median', 'max', 'std', 'null_ratio',\n                      'most freq value', 'most freq ratio', 'sec freq value', 'sec freq ratio']\n    return result","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:54:52.017951Z","iopub.execute_input":"2022-02-06T11:54:52.018382Z","iopub.status.idle":"2022-02-06T11:54:52.031704Z","shell.execute_reply.started":"2022-02-06T11:54:52.018348Z","shell.execute_reply":"2022-02-06T11:54:52.030965Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descriptive statistics for our dataset\nmy_describe(Auser)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:52.032733Z","iopub.execute_input":"2022-02-06T11:54:52.032927Z","iopub.status.idle":"2022-02-06T11:54:52.147903Z","shell.execute_reply.started":"2022-02-06T11:54:52.032904Z","shell.execute_reply":"2022-02-06T11:54:52.147148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's for convenience give names to groups of columns: ","metadata":{}},{"cell_type":"code","source":"rain_cols = Auser.columns[:10].tolist()\ndepth_cols = Auser.columns[10:15].tolist()\ntemper_cols = Auser.columns[15:19].tolist()\nvolume_cols = Auser.columns[19:24].tolist() \nhydro_cols = Auser.columns[24:26].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:52.149053Z","iopub.execute_input":"2022-02-06T11:54:52.149262Z","iopub.status.idle":"2022-02-06T11:54:52.154199Z","shell.execute_reply.started":"2022-02-06T11:54:52.149237Z","shell.execute_reply":"2022-02-06T11:54:52.153306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Series Plots - Daily","metadata":{}},{"cell_type":"markdown","source":"Let's plot time series for all attributes.","metadata":{}},{"cell_type":"code","source":"def plot_timeseries_for_all_attributes(df, alignx):\n\n    n = df.shape[1]\n\n    fig, ax = plt.subplots(n, 1, figsize=(15, n*2))\n    i = 0\n    for col in df.columns:\n        ax[i].plot(df.index, df[col])\n        ax[i].set_ylabel(col, rotation=80)\n        # Alingment of all x axes:\n        if alignx:\n            ax[i].set_xlim([df.index.min() - 120 * Day(), df.index.max() + 120 * Day()])\n        i = i + 1","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:52.155342Z","iopub.execute_input":"2022-02-06T11:54:52.155542Z","iopub.status.idle":"2022-02-06T11:54:52.167532Z","shell.execute_reply.started":"2022-02-06T11:54:52.155517Z","shell.execute_reply":"2022-02-06T11:54:52.166462Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_timeseries_for_all_attributes(Auser, alignx=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:52.170159Z","iopub.execute_input":"2022-02-06T11:54:52.170538Z","iopub.status.idle":"2022-02-06T11:54:56.417796Z","shell.execute_reply.started":"2022-02-06T11:54:52.170499Z","shell.execute_reply":"2022-02-06T11:54:56.4172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From descriptive statistics above and from time series plots we can see:\n\n1. Our data have a lot of **null values**: for some attributes there are about 30% nulls, for some attributes even 50-60%.  \n **Major part of null values belongs to periods before 2006**;\n \n2. Some attributes have **implausible zero values** that should be replaced with nulls: atributes Depth_to_Groundwater_LT2, Depth_to_Groundwater_SAL, Depth_to_Groundwater_CoS and all temperatures.  \nVolume_CSA, Volume_CSAL contain zeroes too, but for volume zeroes may mean that no water were taken for drinking water treatment plants CSA, CSAL before 2014.  \nRainfall contains a lot of zeroes too, but for daily rainfall data it's normal: rain falls not every day;\n\n3. With rainfall plots it's hard to say, is there a season pattern. To figure it out **we'll have to plot smoothed or resampled graph** (for example month average or week average);\n\n4. The largest amount of precipitation falls in Fabbriche di Vallico, and the least precipitation falls in Croce Arcana;\n\n5. It seems that Depth_to_Groundwater_LT2 has upward trend, or maybe there was only one-time increasing from 2008 to 2014 and then value stabilized.  \n(By the way, the higher the line on the graph, the higher the water level)\n\n6. Depth_to_Groundwater_SAL has more sharp peaks than Depth_to_Groundwater_LT2. And Depth_to_Groundwater_PAG and Depth_to_Groundwater_DIEC have even more sharp peaks.  \nSmoothing or resampling will be needed;\n\n7. Hydrometry has more sharp peaks than depth to groundwater. Smoothing or resamplting will be needed;\n\n8. **Hydrometry_Piaggione has noticeably changed from 2008 to 2011**.\nIt may mean that just measurement system have changed, not the real values. We don't know for sure. For safety we just won't take into account all values for Hydrometry_Piaggione before 2011.\n\n9. Hydrometry has both positive and negative values, depth to groundwater has only negative value;  \n(By the way, for hydrometry it works the same way like for depth to groundwater: the higher the line on the graph, the higher the water level)\n\n10. Temperature has very clear seasonal pattern.  \nTemperature needs smooting too;\n\n11. It seems that Volume_POL has an upward trend;\n\n12. All volumes has strange line on plots (too wide). It may mean that volume has some repeated seasonal pattern (maybe weekly). Resampling may smooth the line;\n\n13. All volumes have negative values. Apparantly the lower the line on the plot, the more water the treatment facilities take away;\n\n14. **Volume_CSA and Volume_CSAL have non-zero values only since 2014**. And since 2014 two graphs looks like lines complement each other: for some periods the minimum of Volume_CSAL is accompanied by the maximum of Volume_CSA. Maybe it makes sense to add **new feature that sums Volume_CSA and Volume_CSAL**.","metadata":{}},{"cell_type":"markdown","source":"## Drop Nulls before 2006","metadata":{}},{"cell_type":"markdown","source":"I've decided to drop all data before 2006 because data before 2006 contains only temperature and partly hydrometry, and this data alone would't be of much use for us.","metadata":{}},{"cell_type":"code","source":"Auser = Auser.loc['2006':].copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:56.418896Z","iopub.execute_input":"2022-02-06T11:54:56.41927Z","iopub.status.idle":"2022-02-06T11:54:56.431847Z","shell.execute_reply.started":"2022-02-06T11:54:56.41924Z","shell.execute_reply":"2022-02-06T11:54:56.431279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implausible Zeroes","metadata":{}},{"cell_type":"markdown","source":"Before processing null values we have to replace implausible zeroes with nulls.\n\nTo do this we will replace all zeroes with nulls for the following attributes:\n- Temperature_Orentano\n- Temperature_Monte_Serra\n- Temperature_Ponte_a_Moriano\n- Temperature_Lucca_Orto_Botanico\n- Depth_to_Groundwater_LT2\n- Depth_to_Groundwater_SAL\n- Depth_to_Groundwater_CoS\n\nIt may be that temperature contains \"real\" zeroes, but such values would be rare, and replacing such values with nulls won't have a big impact.\n\nFor volume we won't replace zeroes with null, because for volume these zeroes could be normal values.","metadata":{}},{"cell_type":"code","source":"# Replacing zeroes with nulls\nAuser.loc[Auser.Temperature_Orentano == 0, 'Temperature_Orentano'] = np.nan \nAuser.loc[Auser.Temperature_Monte_Serra == 0, 'Temperature_Monte_Serra'] = np.nan \nAuser.loc[Auser.Temperature_Ponte_a_Moriano == 0, 'Temperature_Ponte_a_Moriano'] = np.nan\nAuser.loc[Auser.Temperature_Lucca_Orto_Botanico == 0, 'Temperature_Lucca_Orto_Botanico'] = np.nan\nAuser.loc[Auser.Depth_to_Groundwater_LT2 == 0, 'Depth_to_Groundwater_LT2'] = np.nan\nAuser.loc[Auser.Depth_to_Groundwater_SAL == 0, 'Depth_to_Groundwater_SAL'] = np.nan\nAuser.loc[Auser.Depth_to_Groundwater_CoS == 0, 'Depth_to_Groundwater_CoS'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:56.433029Z","iopub.execute_input":"2022-02-06T11:54:56.433414Z","iopub.status.idle":"2022-02-06T11:54:56.442986Z","shell.execute_reply.started":"2022-02-06T11:54:56.433384Z","shell.execute_reply":"2022-02-06T11:54:56.442396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also let's replace with nulls all values of Hydrometry_Piaggione before 2011 because these values do not inspire confidence.","metadata":{}},{"cell_type":"code","source":"Auser.loc[:'2010', 'Hydrometry_Piaggione'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:56.444272Z","iopub.execute_input":"2022-02-06T11:54:56.444685Z","iopub.status.idle":"2022-02-06T11:54:56.450965Z","shell.execute_reply.started":"2022-02-06T11:54:56.444656Z","shell.execute_reply":"2022-02-06T11:54:56.45033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Nulls Diagram","metadata":{}},{"cell_type":"markdown","source":"Let's plot a diagram of null periods:","metadata":{}},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser.T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,8))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Blues are nulls periods', fontsize=18);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:54:56.452474Z","iopub.execute_input":"2022-02-06T11:54:56.452908Z","iopub.status.idle":"2022-02-06T11:54:57.489069Z","shell.execute_reply.started":"2022-02-06T11:54:56.452842Z","shell.execute_reply":"2022-02-06T11:54:57.488157Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data truly contents a lot of null.\n\nWe will fill null values for rainfall, temperature, Hydrometry_Monte_S_Quirico, and for our target value - Depth_to_Groundwater_SAL.\n\nNow for our convinience let's write user function for plotting nulls periods that will unlike the previous graph:\n1. display only fields with null values;\n2. display even alone-standing nulls.","metadata":{}},{"cell_type":"code","source":"# User function for plotting nulls periods\ndef my_plot_null(df, figsize=(15,7), plot_title=''):\n    \n    # Displays diagram showing the periods in which attributes have empty values\n \n    df = pd.DataFrame(df, copy=True)\n    \n    # Replace columns wiht nulls indicators: if null then 1, else nan\n    for col in df.columns:\n        df[col] = np.where(df[col].isna(), 1, np.nan) \n\n    # List of columns for which there are null values\n    cols_with_nulls = [col for col in df.columns if df[col].sum() > 0]\n    \n    i = len(cols_with_nulls)\n    for col in cols_with_nulls:\n        df[col] = df[col] * i\n        i = i - 1    \n    \n    # Mark single null values \n    d_new_index = df.reset_index(drop=True)\n    d_prev = d_new_index.set_index(pd.Series(d_new_index.index) + 1)\n    d_next = d_new_index.set_index(pd.Series(d_new_index.index) - 1)\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    for col in cols_with_nulls:\n        # Find points which should be marked\n        d = pd.concat([d_new_index[col], d_prev[col], d_next[col]], \n                      axis=1)\n        d.columns = ['curr', 'prev', 'next']\n        # We remove the first and last element, because these elements were \n        # added after the concat operation (concat works as a full outer join)\n        d = d[1:-1]    \n        ax.plot(df.index, df[col], label=col, marker='o', ms=20, \n                markevery=d[d.curr.notna() & d.prev.isna() & d.next.isna()].index.to_list())\n        # Main diagram\n        ax.fill_between(df.index, df[col])\n    ax.legend()\n    plt.xticks(rotation=90)\n    ax.set_ylim(0, len(cols_with_nulls)*2) # *2 - just for formatting - to leave space above for legend\n    ax.set_yticks([])\n    ax.set_title(plot_title, fontdict={'fontsize':16})    \n    ;","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:54:57.490087Z","iopub.execute_input":"2022-02-06T11:54:57.49031Z","iopub.status.idle":"2022-02-06T11:54:57.505377Z","shell.execute_reply.started":"2022-02-06T11:54:57.490281Z","shell.execute_reply":"2022-02-06T11:54:57.50438Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_plot_null(Auser[rain_cols + temper_cols + hydro_cols + ['Depth_to_Groundwater_SAL']], \n             figsize=(15, 4), \n             plot_title='Nulls for Rains, Temperatures, Hydro and Depth_to_Groundwater_SAL')","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:57.507046Z","iopub.execute_input":"2022-02-06T11:54:57.507363Z","iopub.status.idle":"2022-02-06T11:54:58.143855Z","shell.execute_reply.started":"2022-02-06T11:54:57.507324Z","shell.execute_reply":"2022-02-06T11:54:58.143001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that:\n1. among rain only Rainfall_Monte_Serra and Rainfall_Piaggione have null values;\n2. all temperatures have null values. But Temperature_Orentano and Temperature_Lucca_Orto_Botanico have only alone-standing nulls.","metadata":{}},{"cell_type":"markdown","source":"## Time Series Plots - Weekly since 2006","metadata":{}},{"cell_type":"markdown","source":"First let's plot time series again - since 2006 and for weekly resempled data - it will help us to decide how we should fill null values","metadata":{}},{"cell_type":"code","source":"# Resampling to week frequency. Grouping week to final week day - Sunday.\nAuser_week = Auser.resample('W-SUN').mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:58.144876Z","iopub.execute_input":"2022-02-06T11:54:58.145068Z","iopub.status.idle":"2022-02-06T11:54:58.169498Z","shell.execute_reply.started":"2022-02-06T11:54:58.145043Z","shell.execute_reply":"2022-02-06T11:54:58.16878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now when we resampled our data to week basis and droped data before 2006, let's plot time series for all attributes again.","metadata":{}},{"cell_type":"code","source":"plot_timeseries_for_all_attributes(Auser_week, alignx=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:58.170633Z","iopub.execute_input":"2022-02-06T11:54:58.170838Z","iopub.status.idle":"2022-02-06T11:55:02.321893Z","shell.execute_reply.started":"2022-02-06T11:54:58.170813Z","shell.execute_reply":"2022-02-06T11:55:02.321123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resampling smoothed all lines. And, by the way, volume lines have lost its strange thickness.  \n\nNow we can see more clearly the following:\n1. All rainfall plots looks similar. Peaks are repeated on different rain plots: on some plots they are lower, on some - higher.  \nIt means that in different areas around Auser aquifer raif falls more or less evenly.  \n **Rainfall attributes are highly dependent on each other.**\n\n2. Peaks of Depth_to_Groundwater_SAL (our target value) repeat peaks on rainfall plots.  \nIt means that there is **dependency between water level and rain.**  \nWe are probably on the right way:)","metadata":{}},{"cell_type":"markdown","source":"## Nulls Processing","metadata":{}},{"cell_type":"markdown","source":"Let's fill null values for rainfall, temperature, Hydrometry_Monte_S_Quirico, and Depth_to_Groundwater_SAL.\n\nLet's start from attributes with few alone-standing nulls: Rainfall_Monte_Serra, Temperature_Orentano, Temperature_Lucca_Orto_Botanico.\nWe will fill them using average neighboring values (linear interpolation).","metadata":{}},{"cell_type":"markdown","source":"### Fields with Alone-standing Nulls","metadata":{}},{"cell_type":"code","source":"Auser['Rainfall_Monte_Serra'].interpolate(inplace=True)\nAuser['Temperature_Orentano'].interpolate(inplace=True)\nAuser['Temperature_Lucca_Orto_Botanico'].interpolate(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:02.32282Z","iopub.execute_input":"2022-02-06T11:55:02.323013Z","iopub.status.idle":"2022-02-06T11:55:02.331881Z","shell.execute_reply.started":"2022-02-06T11:55:02.322989Z","shell.execute_reply":"2022-02-06T11:55:02.331285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rainfall Piaggione","metadata":{}},{"cell_type":"markdown","source":"Rainfall_Piaggione has null values at whole 2009.\n\nAs we've already figured out, rains in all areas are highly dependent on each other.\n\nTo make sure that daily rains are highly dependent too, let's plot daily rain data for Rainfall_Piaggione and some another rain attributes for example Rainfall_Gallicano for some year, for example 2010.","metadata":{}},{"cell_type":"code","source":"col_pred = 'Rainfall_Piaggione'","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:02.332776Z","iopub.execute_input":"2022-02-06T11:55:02.333286Z","iopub.status.idle":"2022-02-06T11:55:02.344325Z","shell.execute_reply.started":"2022-02-06T11:55:02.333246Z","shell.execute_reply":"2022-02-06T11:55:02.343383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2010'\ndate_end = '2010'\nfig, ax = plt.subplots(figsize=(15, 3))    \nax.plot(Auser.loc[date_begin:date_end].index, Auser.loc[date_begin:date_end, col_pred], \n        label=col_pred, c='xkcd:true blue')\nax.plot(Auser.loc[date_begin:date_end].index, \n        Auser.loc[date_begin:date_end, 'Rainfall_Gallicano'], \n        label='Rainfall_Gallicano', c='xkcd:slate grey')    \nax.legend()\nax.set_title(col_pred + ' vs Rainfall_Gallicano - 2010', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:55:02.345771Z","iopub.execute_input":"2022-02-06T11:55:02.346224Z","iopub.status.idle":"2022-02-06T11:55:02.772755Z","shell.execute_reply.started":"2022-02-06T11:55:02.346182Z","shell.execute_reply":"2022-02-06T11:55:02.771898Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that daily rain data is highly dependent on each other too.\n\nSo let's fill nulls of Rainfall_Piaggione by predicting it using other rains.","metadata":{}},{"cell_type":"code","source":"features = ['Rainfall_Gallicano',\n            'Rainfall_Pontetetto',\n            'Rainfall_Monte_Serra',\n            'Rainfall_Orentano',\n            'Rainfall_Borgo_a_Mozzano',\n            'Rainfall_Calavorno',\n            'Rainfall_Croce_Arcana',\n            'Rainfall_Tereglio_Coreglia_Antelminelli',\n            'Rainfall_Fabbriche_di_Vallico']","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:55:02.773854Z","iopub.execute_input":"2022-02-06T11:55:02.774084Z","iopub.status.idle":"2022-02-06T11:55:02.77871Z","shell.execute_reply.started":"2022-02-06T11:55:02.774044Z","shell.execute_reply":"2022-02-06T11:55:02.777804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we are predicting just an intermediate feature, I won't spend time on selecting model, tuning parameters and choosing features. Let's just try random forest, taking all other rains as input, and see, what results it will give.  \nWe'll use as input only other rains and won't use seasonal attributes or own lagged values.\n\nFirst we will solve classification problem: predict whether it rained on a given day.  \nAnd then we will solve regression problem: predict the amount of precipitation only on the days when it rained.","metadata":{}},{"cell_type":"code","source":"class RainModel(BaseEstimator):\n    \n    # First predicts whether it rained or not (0 or 1) - classification\n    # Then predicts amount of precipitation only on the days when it rained - regression\n    \n    def __init__(self, classifier, regressor):        \n        self.classifier = classifier\n        self.regressor = regressor\n        \n    def fit(self, X, y):\n        # 1. Обучить RFC \n        # 2. Предсказать с помощью RFC 0 или 1 (на обуч)\n        # 3. Обучить RFR только на тех строках обуч. выборки, на которых были предсказаны 1\n        #    (или на которых реально дождь был)\n        \n        # Train classificator\n        y_bit = np.where(y == 0, 0, 1)\n        self.classifier.fit(X, y_bit)\n        \n        # Train regressor only on rows with rain > 0\n        X1 = X[y_bit == 1]\n        y1 = y[y_bit == 1]            \n        self.regressor.fit(X1, y1)\n        \n        return self\n    \n    def predict(self, X):\n        # 1. Предсказать 0 или 1\n        # 2. На тех строках, для которых был предсказан 1, предсказать количество осадков\n        \n        # Predict whether it rained or not (0 or 1) on a given day\n        pred_bit = self.classifier.predict(X)\n        \n        # Predict amount of precipitation only on the days for which 1 was predicted\n        X1 = X[pred_bit == 1]\n        pred = self.regressor.predict(X1)\n        \n        # Union pred_bit and pred\n        X_c = X.copy()\n        X_c['pred'] = np.array(pred_bit)\n        X_c.loc[pred_bit == 1, 'pred'] = np.array(pred)\n        \n        return X_c['pred']    ","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:55:02.780193Z","iopub.execute_input":"2022-02-06T11:55:02.780479Z","iopub.status.idle":"2022-02-06T11:55:02.790108Z","shell.execute_reply.started":"2022-02-06T11:55:02.780442Z","shell.execute_reply":"2022-02-06T11:55:02.789544Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm = RainModel(classifier=RandomForestClassifier(random_state=14), \n                      regressor=RandomForestRegressor(random_state=14))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:02.791097Z","iopub.execute_input":"2022-02-06T11:55:02.79141Z","iopub.status.idle":"2022-02-06T11:55:02.805536Z","shell.execute_reply.started":"2022-02-06T11:55:02.791384Z","shell.execute_reply":"2022-02-06T11:55:02.80458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First let's see how our model will predict values at non-empty periods of Rainfall_Piaggione - to evaluate model's efficiency.\n\nLet's write a function that will plot predicted values obtained by cross-validation.\n\nThis function will be usefull for us later too.","metadata":{}},{"cell_type":"code","source":"def plot_pred_all_splits(model, X, y, cols, cv, plot_title='', mark_pred_start=False, dropNA=True):\n    \n    # Displays a plot with predicted values obtained by cross-validation\n    # Returns an array of RMSE and returns predicted values. Outputs the average RMSE\n    # Deletes rows with nulls in advance\n    \n    NoNA = X.copy()\n    NoNA['y'] = np.array(y)\n    if dropNA:\n        NoNA.dropna(subset=cols + ['y'], inplace=True)\n\n    print('Number of rows without nulls:', len(NoNA))    \n    \n    if isinstance(cv, int):\n        cv = KFold(n_splits=cv) \n        \n    pred_all_splits = pd.Series([np.nan] * len(NoNA), index=NoNA.index)\n    points_to_mark = pd.Series([np.nan] * len(NoNA), index=NoNA.index)\n    \n    scores = []\n\n    for train_lix, test_lix in cv.split(NoNA):\n        \n        train_all_cols = NoNA.iloc[train_lix]\n        test_all_cols = NoNA.iloc[test_lix]\n        \n        X_train = train_all_cols[cols]\n        X_test = test_all_cols[cols]\n\n        y_train = train_all_cols['y']\n        y_test = test_all_cols['y']\n\n        model.fit(X_train, y_train)\n        \n        pred = model.predict(X_test)\n        \n        rmse = sqrt(mean_squared_error(y_test, pred))\n        \n        scores.append(rmse)\n        \n        pred_all_splits.iloc[test_lix] = np.array(pred).reshape(-1)\n        \n        if mark_pred_start:\n            points_to_mark.iloc[test_lix[0]] = True\n    \n    fig, ax = plt.subplots(figsize=(15, 4))    \n    ax.plot(X.index, y, label='real', c='xkcd:true blue')    \n    # Doing this prevents connected points on the plot for periods with nulls\n    X_c = X.copy()\n    X_c['predicted'] = pred_all_splits  \n    X_c['points_to_mark'] = points_to_mark\n    X_c['points_to_mark'].fillna(False, inplace=True)\n    ax.plot(X.index, X_c['predicted'], label='predicted', c='xkcd:cherry', \n            marker='o', ms=8, markevery=X_c['points_to_mark'])    \n    ax.legend()\n    ax.set_title(plot_title, fontdict={'fontsize':16})    \n    \n    print('RMSE', pd.Series(scores).mean())    \n    return scores, pred_all_splits","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:55:02.806689Z","iopub.execute_input":"2022-02-06T11:55:02.807011Z","iopub.status.idle":"2022-02-06T11:55:02.818213Z","shell.execute_reply.started":"2022-02-06T11:55:02.806983Z","shell.execute_reply":"2022-02-06T11:55:02.817377Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred_at_non_empty = plot_pred_all_splits(\n    rm, Auser, Auser[col_pred], cols=features, cv=10,\n    plot_title=col_pred + ' prediction with RF based on other rains')","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-06T11:55:02.819858Z","iopub.execute_input":"2022-02-06T11:55:02.820703Z","iopub.status.idle":"2022-02-06T11:55:15.492771Z","shell.execute_reply.started":"2022-02-06T11:55:02.820659Z","shell.execute_reply":"2022-02-06T11:55:15.491896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this scale it's hard to analyze this plot. Let's resample data to weeks and plot it again for some selected years only, for example 2010-2014.\n\nAs for RMSE = 4.23 -  it looks too large for attribute with standard deviation around 11 (see [Descriptive Statistics](#Descriptive-Statistics)). But RMSE is so large because of unstable behaviour of Rainfall_Piaggione: one day there was no rain and other day the monthly precipitation rate fell.  \nLet's resample data to weeks and calculate RMSE on weekly data.","metadata":{}},{"cell_type":"code","source":"pred_at_non_empty_week = pred_at_non_empty.resample('W-SUN').mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:15.493931Z","iopub.execute_input":"2022-02-06T11:55:15.494173Z","iopub.status.idle":"2022-02-06T11:55:15.510471Z","shell.execute_reply.started":"2022-02-06T11:55:15.494144Z","shell.execute_reply":"2022-02-06T11:55:15.50957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2010'\ndate_end = '2014'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser_week.loc[date_begin:date_end].index, Auser_week.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(pred_at_non_empty_week.loc[date_begin:date_end].index, \n        pred_at_non_empty_week.loc[date_begin:date_end], \n        label='predicted', c='xkcd:cherry')    \nax.legend()\nax.set_title(col_pred + ' prediction 2010-2014', fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T11:55:15.51184Z","iopub.execute_input":"2022-02-06T11:55:15.512364Z","iopub.status.idle":"2022-02-06T11:55:15.797385Z","shell.execute_reply.started":"2022-02-06T11:55:15.51232Z","shell.execute_reply":"2022-02-06T11:55:15.796674Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weekly prediction looks quite good. Mostly errors occur at peaks. Sometimes prediction overestimates real value, and sometimes underestimates. Maybe it's so because sometimes there was more precipitation in Piaggione area than in other areas, and sometimes less.","metadata":{}},{"cell_type":"code","source":"sqrt(mean_squared_error(Auser_week[col_pred].dropna(), pred_at_non_empty_week.dropna()))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:15.798528Z","iopub.execute_input":"2022-02-06T11:55:15.798865Z","iopub.status.idle":"2022-02-06T11:55:15.805455Z","shell.execute_reply.started":"2022-02-06T11:55:15.798837Z","shell.execute_reply":"2022-02-06T11:55:15.804672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weekly RMSE is significantly less than daily RMSE.\n\nLet's find most important rain feature. To do this let's train Random Forest Regressor.","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=14)\n\ntrain = Auser[Auser[col_pred].notna()]\nX_train = train[features]\ny_train = train[col_pred]\n\nmodel.fit(X_train, y_train)\n\nfeat_imp = pd.DataFrame([[f1, f2] for f1, f2 in zip(model.feature_importances_, X_train.columns)], columns=['importance', 'feature'])\nfeat_imp.sort_values(['importance', 'feature'], ascending=[False, True] )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-06T11:55:15.806726Z","iopub.execute_input":"2022-02-06T11:55:15.806926Z","iopub.status.idle":"2022-02-06T11:55:17.067913Z","shell.execute_reply.started":"2022-02-06T11:55:15.806902Z","shell.execute_reply":"2022-02-06T11:55:17.067106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Presumably the most important is Rainfall_Borgo_a_Mozzano.\n\nNow let's see how our model predicts empty period - 2009. Also let's add on the plot most important feature.","metadata":{}},{"cell_type":"code","source":"res = Auser[Auser[col_pred].isna()]\nX_res= res[features]\n\npred = rm.predict(X_res)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:17.06929Z","iopub.execute_input":"2022-02-06T11:55:17.069527Z","iopub.status.idle":"2022-02-06T11:55:17.11282Z","shell.execute_reply.started":"2022-02-06T11:55:17.069499Z","shell.execute_reply":"2022-02-06T11:55:17.111986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2008-06'\ndate_end = '2010-06'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser.loc[date_begin:date_end].index, Auser.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(Auser.loc[date_begin:date_end].index, \n        Auser.loc[date_begin:date_end, 'Rainfall_Borgo_a_Mozzano'], \n        label='Rainfall_Borgo_a_Mozzano', c='xkcd:slate grey')\nax.plot(X_res.index, pred, label='predicted', c='xkcd:cherry')\nax.legend()\nax.set_title(col_pred + ' prediction at empty period vs Rainfall_Borgo_a_Mozzano', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"scrolled":true,"execution":{"iopub.status.busy":"2022-02-06T11:55:17.114154Z","iopub.execute_input":"2022-02-06T11:55:17.114459Z","iopub.status.idle":"2022-02-06T11:55:17.428418Z","shell.execute_reply.started":"2022-02-06T11:55:17.114427Z","shell.execute_reply":"2022-02-06T11:55:17.427615Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that prediction at 2009 almost exactly repeat Rainfall_Borgo_a_Mozzano value.\n\nNow let's replace null values of Rainfall_Piaggione with prediction.","metadata":{}},{"cell_type":"code","source":"def replace_nulls_with_prediction(df, col_pred, cols_features, model):\n\n    col_pred_isna = df[col_pred].isna()    \n\n    if df[col_pred_isna][cols_features].isna().sum().sum() > 0:\n        print('There are null values of features in the rows that we are going to predict')\n        return -1    \n\n    train = df[~col_pred_isna]\n    # We select only those lines in which all fields are not empty\n    X = train[cols_features]\n    features_isna = X.isna().sum(axis=1)\n    X = X[features_isna == 0]\n    y = train.loc[features_isna == 0, col_pred] \n\n    # Data to predict\n    d_res = df[col_pred_isna]\n    X_res = d_res[cols_features]\n\n    model.fit(X, y)\n    res_pred = model.predict(X_res) # we get predictions in the form of a numpy array\n    # Add index to predictions\n    if res_pred.ndim == 1:\n        d_res_pred = pd.Series(res_pred, index=d_res.index, name=col_pred + '_pred')\n    else:\n        d_res_pred = pd.Series(res_pred.reshape(-1), index=d_res.index, name=col_pred + '_pred')\n\n    # In the original dataset, we replace nulls with predicted values\n    df[col_pred + '_pred'] = d_res_pred\n    df[col_pred] = np.where(df[col_pred].isna(), \n                            df[col_pred + '_pred'], \n                            df[col_pred])\n    df.drop(col_pred + '_pred', axis=1, inplace=True)","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-06T12:02:37.462947Z","iopub.execute_input":"2022-02-06T12:02:37.463706Z","iopub.status.idle":"2022-02-06T12:02:37.473584Z","shell.execute_reply.started":"2022-02-06T12:02:37.463657Z","shell.execute_reply":"2022-02-06T12:02:37.472486Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replace_nulls_with_prediction(Auser, col_pred, features, rm)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:02:39.903171Z","iopub.execute_input":"2022-02-06T12:02:39.903758Z","iopub.status.idle":"2022-02-06T12:02:41.269171Z","shell.execute_reply.started":"2022-02-06T12:02:39.903715Z","shell.execute_reply":"2022-02-06T12:02:41.268488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Temperature_Monte_Serra","metadata":{}},{"cell_type":"markdown","source":"Next let's fill null values in Temperature_Monte_Serra and Temperature_Ponte_a_Moriano.\n\nAs we can see at [Time Series Plots](#Time-Series-Plots---Weekly-since-2006) and [Nulls Diagram](#Nulls-Diagram):\n\n- In Temperature_Monte_Serra there are few null values in 2006-2007.\n- In Temperature_Ponte_a_Moriano there are null values since June 2017, and also there some null values in 2007, 2008, 2009.\n\nJust like with rains, we will predict the temperature based on other temperatures, because all temperatures behave very similarly.\n\nFirst we predict Temperature_Monte_Serra based on the Temperature_Orentano and Temperature_Lucca_Orto_Botanico.  \nAnd then we predict Temperature_Ponte_a_Moriano based on rest 3 temperatures.\n\nAs with rain we will use random forest and won't spend  time on selecting model or tuning parameters.","metadata":{}},{"cell_type":"code","source":"col_pred = 'Temperature_Monte_Serra'","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:19:54.009742Z","iopub.execute_input":"2022-02-04T14:19:54.010071Z","iopub.status.idle":"2022-02-04T14:19:54.014197Z","shell.execute_reply.started":"2022-02-04T14:19:54.010038Z","shell.execute_reply":"2022-02-04T14:19:54.013387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nfig, ax = plt.subplots(figsize=(15, 4))    \nfor i in range(len(temper_cols)):\n    ax.plot(Auser_week.loc[:'2012'].index, Auser_week.loc[:'2012', temper_cols[i]], label=temper_cols[i])\nax.legend()\nax.set_title('Temperatures in different areas are highly dependent on each other', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:19:54.015536Z","iopub.execute_input":"2022-02-04T14:19:54.01611Z","iopub.status.idle":"2022-02-04T14:19:54.393129Z","shell.execute_reply.started":"2022-02-04T14:19:54.016061Z","shell.execute_reply":"2022-02-04T14:19:54.392242Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=14)\ncol_pred = 'Temperature_Monte_Serra'\nfeatures = ['Temperature_Orentano', 'Temperature_Lucca_Orto_Botanico']","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:19:54.394464Z","iopub.execute_input":"2022-02-04T14:19:54.3948Z","iopub.status.idle":"2022-02-04T14:19:54.400194Z","shell.execute_reply.started":"2022-02-04T14:19:54.394753Z","shell.execute_reply":"2022-02-04T14:19:54.399286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred_at_non_empty = plot_pred_all_splits(\n    model, Auser, Auser[col_pred], cols=features, cv=10,\n    plot_title=col_pred + ' prediction with RF based on ' + str(features))","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:19:54.401422Z","iopub.execute_input":"2022-02-04T14:19:54.401731Z","iopub.status.idle":"2022-02-04T14:20:02.17628Z","shell.execute_reply.started":"2022-02-04T14:19:54.401685Z","shell.execute_reply":"2022-02-04T14:20:02.175485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_at_non_empty_week = pred_at_non_empty.resample('W-SUN').mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:02.177789Z","iopub.execute_input":"2022-02-04T14:20:02.178102Z","iopub.status.idle":"2022-02-04T14:20:02.20085Z","shell.execute_reply.started":"2022-02-04T14:20:02.178046Z","shell.execute_reply":"2022-02-04T14:20:02.199941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2010'\ndate_end = '2014'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser_week.loc[date_begin:date_end].index, Auser_week.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(pred_at_non_empty_week.loc[date_begin:date_end].index, \n        pred_at_non_empty_week.loc[date_begin:date_end], \n        label='predicted', c='xkcd:cherry')    \nax.legend()\nax.set_title(col_pred + ' prediction 2010-2014', fontdict={'fontsize':16});","metadata":{"code_folding":[0],"scrolled":true,"execution":{"iopub.status.busy":"2022-02-04T14:20:02.20232Z","iopub.execute_input":"2022-02-04T14:20:02.202697Z","iopub.status.idle":"2022-02-04T14:20:02.48328Z","shell.execute_reply.started":"2022-02-04T14:20:02.202633Z","shell.execute_reply":"2022-02-04T14:20:02.482775Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqrt(mean_squared_error(Auser_week[col_pred].dropna(), pred_at_non_empty_week.dropna()))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:02.484253Z","iopub.execute_input":"2022-02-04T14:20:02.484757Z","iopub.status.idle":"2022-02-04T14:20:02.491457Z","shell.execute_reply.started":"2022-02-04T14:20:02.484717Z","shell.execute_reply":"2022-02-04T14:20:02.490708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting prediction\ntrain = Auser[Auser[col_pred].notna()]\nX_train = train[features]\ny_train = train[col_pred]\n\nmodel.fit(X_train, y_train)\n\nres = Auser[Auser[col_pred].isna()]\nX_res= res[features]\n\npred = model.predict(X_res)\npred_d = pd.Series(pred, index=X_res.index)\nAuser_c = Auser.copy()\nAuser_c['predicted'] = pred_d\n\ndate_begin = '2006'\ndate_end = '2007-04'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser.loc[date_begin:date_end].index, Auser.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(Auser.loc[date_begin:date_end].index, \n        Auser.loc[date_begin:date_end, 'Temperature_Orentano'], \n        label='Temperature_Orentano', c='xkcd:slate grey')\nax.plot(Auser.loc[date_begin:date_end].index, Auser_c.loc[date_begin:date_end, 'predicted'], \n        label='predicted', c='xkcd:cherry', marker='o', markersize=3)\nax.legend()\nax.set_title(col_pred + ' prediction at empty period vs Temperature_Orentano - 2006-2007', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:02.492611Z","iopub.execute_input":"2022-02-04T14:20:02.493149Z","iopub.status.idle":"2022-02-04T14:20:03.596108Z","shell.execute_reply.started":"2022-02-04T14:20:02.493115Z","shell.execute_reply":"2022-02-04T14:20:03.59537Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replace_nulls_with_prediction(Auser, col_pred, features, model)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:03.597322Z","iopub.execute_input":"2022-02-04T14:20:03.597636Z","iopub.status.idle":"2022-02-04T14:20:04.38633Z","shell.execute_reply.started":"2022-02-04T14:20:03.597598Z","shell.execute_reply":"2022-02-04T14:20:04.385553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Temperature_Ponte_a_Moriano","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=14)\ncol_pred = 'Temperature_Ponte_a_Moriano'\nfeatures = ['Temperature_Orentano', 'Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra']","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:04.387954Z","iopub.execute_input":"2022-02-04T14:20:04.388242Z","iopub.status.idle":"2022-02-04T14:20:04.393316Z","shell.execute_reply.started":"2022-02-04T14:20:04.388203Z","shell.execute_reply":"2022-02-04T14:20:04.392293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred_at_non_empty = plot_pred_all_splits(\n    model, Auser, Auser[col_pred], cols=features, cv=10,\n    plot_title=col_pred + ' prediction with RF based on other temperatures' )","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:20:04.394818Z","iopub.execute_input":"2022-02-04T14:20:04.395119Z","iopub.status.idle":"2022-02-04T14:20:12.363227Z","shell.execute_reply.started":"2022-02-04T14:20:04.39508Z","shell.execute_reply":"2022-02-04T14:20:12.362447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_at_non_empty_week = pred_at_non_empty.resample('W-SUN').mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:12.364548Z","iopub.execute_input":"2022-02-04T14:20:12.364895Z","iopub.status.idle":"2022-02-04T14:20:12.384262Z","shell.execute_reply.started":"2022-02-04T14:20:12.364853Z","shell.execute_reply":"2022-02-04T14:20:12.38344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2010'\ndate_end = '2014'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser_week.loc[date_begin:date_end].index, Auser_week.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(pred_at_non_empty_week.loc[date_begin:date_end].index, \n        pred_at_non_empty_week.loc[date_begin:date_end], \n        label='predicted', c='xkcd:cherry')    \nax.legend()\nax.set_title(col_pred + ' prediction 2010-2014', fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:12.386074Z","iopub.execute_input":"2022-02-04T14:20:12.386537Z","iopub.status.idle":"2022-02-04T14:20:12.666457Z","shell.execute_reply.started":"2022-02-04T14:20:12.386493Z","shell.execute_reply":"2022-02-04T14:20:12.665747Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqrt(mean_squared_error(Auser_week[col_pred].dropna(), pred_at_non_empty_week.dropna()))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:12.667875Z","iopub.execute_input":"2022-02-04T14:20:12.668391Z","iopub.status.idle":"2022-02-04T14:20:12.676943Z","shell.execute_reply.started":"2022-02-04T14:20:12.668352Z","shell.execute_reply":"2022-02-04T14:20:12.676169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Achieved RMSE is even better than for Temperature_Monte_Serra.\n\nThis is because Temperature_Ponte_a_Moriano is very similar to Temperature_Orentano and Temperature_Lucca_Orto_Botanico on the basis of which it was predicted.","metadata":{}},{"cell_type":"code","source":"# Plotting prediction\ntrain = Auser[Auser[col_pred].notna()]\nX_train = train[features]\ny_train = train[col_pred]\n\nmodel.fit(X_train, y_train)\n\nres = Auser[Auser[col_pred].isna()]\nX_res= res[features]\n\npred = model.predict(X_res)\npred_d = pd.Series(pred, index=X_res.index)\nAuser_c = Auser.copy()\nAuser_c['predicted'] = pred_d\n\ndate_begin = '2006'\ndate_end = '2009'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser.loc[date_begin:date_end].index, Auser.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(Auser.loc[date_begin:date_end].index, \n        Auser.loc[date_begin:date_end, 'Temperature_Orentano'], \n        label='Temperature_Orentano', c='xkcd:slate grey')\nax.plot(Auser.loc[date_begin:date_end].index, Auser_c.loc[date_begin:date_end, 'predicted'], \n        label='predicted', c='xkcd:cherry')\nax.legend()\nax.set_title(col_pred + ' prediction at empty period vs Temperature_Orentano - 2006-2009', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:12.678383Z","iopub.execute_input":"2022-02-04T14:20:12.679888Z","iopub.status.idle":"2022-02-04T14:20:13.914998Z","shell.execute_reply.started":"2022-02-04T14:20:12.679844Z","shell.execute_reply":"2022-02-04T14:20:13.913989Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\ndate_begin = '2017'\ndate_end = '2020'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser.loc[date_begin:date_end].index, Auser.loc[date_begin:date_end, col_pred], \n        label='real', c='xkcd:true blue')\nax.plot(Auser.loc[date_begin:date_end].index, \n        Auser.loc[date_begin:date_end, 'Temperature_Orentano'], \n        label='Temperature_Orentano', c='xkcd:slate grey')\nax.plot(Auser.loc[date_begin:date_end].index, Auser_c.loc[date_begin:date_end, 'predicted'], label='predicted', \n        c='xkcd:cherry')\nax.legend()\nax.set_title(col_pred + ' prediction at empty period vs Temperature_Orentano - 2017-2020', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:13.916204Z","iopub.execute_input":"2022-02-04T14:20:13.91643Z","iopub.status.idle":"2022-02-04T14:20:14.281578Z","shell.execute_reply.started":"2022-02-04T14:20:13.916399Z","shell.execute_reply":"2022-02-04T14:20:14.280833Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replace_nulls_with_prediction(Auser, col_pred, features, model)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:14.282668Z","iopub.execute_input":"2022-02-04T14:20:14.282868Z","iopub.status.idle":"2022-02-04T14:20:15.143211Z","shell.execute_reply.started":"2022-02-04T14:20:14.282842Z","shell.execute_reply":"2022-02-04T14:20:15.142524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hydrometry_Monte_S_Quirico","metadata":{}},{"cell_type":"markdown","source":"Hydrometry_Monte_S_Quirico has nulls at October, November and December 2008. And also few alone-stranding nulls at other years.\n\nLets check if Hydrometry_Monte_S_Quirico is dependent on depth to groundwater.","metadata":{}},{"cell_type":"code","source":"cols = ['Hydrometry_Monte_S_Quirico'] + depth_cols\nplot_timeseries_for_all_attributes(Auser_week[cols], alignx=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:15.144476Z","iopub.execute_input":"2022-02-04T14:20:15.144854Z","iopub.status.idle":"2022-02-04T14:20:16.181571Z","shell.execute_reply.started":"2022-02-04T14:20:15.144813Z","shell.execute_reply":"2022-02-04T14:20:16.180859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Hydrometry_Monte_S_Quirico there is strong dependence with Depth to groudwater SAL, PAG and DIEC.","metadata":{}},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser[cols].T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,3))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Blues are nulls periods', fontsize=18);","metadata":{"code_folding":[0],"scrolled":true,"execution":{"iopub.status.busy":"2022-02-04T14:20:16.183133Z","iopub.execute_input":"2022-02-04T14:20:16.183426Z","iopub.status.idle":"2022-02-04T14:20:16.646492Z","shell.execute_reply.started":"2022-02-04T14:20:16.183385Z","shell.execute_reply":"2022-02-04T14:20:16.645761Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But unfortunatly Depth_to_Groundwater_PAG and Depth_to_Groundwater_DIEC has null values in the same periods with Hydrometry_Monte_S_Quirico. Because of it we can't use Depth_to_Groundwater_PAG and Depth_to_Groundwater_DIEC to predict null periods of Hydrometry_Monte_S_Quirico.\n\nSo let's use only Depth_to_Groundwater_SAL for Hydrometry_Monte_S_Quirico prediction.","metadata":{}},{"cell_type":"code","source":"col_pred = 'Hydrometry_Monte_S_Quirico'\nmodel = RandomForestRegressor(random_state=14)\nfeatures = ['Depth_to_Groundwater_SAL']","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:16.6475Z","iopub.execute_input":"2022-02-04T14:20:16.647701Z","iopub.status.idle":"2022-02-04T14:20:16.653069Z","shell.execute_reply.started":"2022-02-04T14:20:16.647674Z","shell.execute_reply":"2022-02-04T14:20:16.652385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScaleDF(BaseEstimator, TransformerMixin):\n    \n    # Apply StandardScaler and convert result to DataFrame, preserving columns names and index\n    \n    def __init__(self):\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        self.scaler.fit(X)        \n        return self\n    \n    def transform(self, X):        \n        X_sc = self.scaler.transform(X)\n        X_sc = pd.DataFrame(X_sc, columns=X.columns, index=X.index)        \n        return X_sc","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:20:16.654015Z","iopub.execute_input":"2022-02-04T14:20:16.654221Z","iopub.status.idle":"2022-02-04T14:20:16.663286Z","shell.execute_reply.started":"2022-02-04T14:20:16.654197Z","shell.execute_reply":"2022-02-04T14:20:16.662549Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting prediction\ntrain = Auser[Auser[col_pred].notna()].copy()\ntrain.dropna(subset=features, inplace=True)\nX_train = train[features]\ny_train = train[col_pred]\n\nmodel.fit(X_train, y_train)\n\nres = Auser[Auser[col_pred].isna()]\nX_res= res[features]\n\npred = model.predict(X_res)\npred_d = pd.Series(pred, index=X_res.index)\nAuser_c = Auser.copy()\nAuser_c['predicted'] = pred_d\n\nsdf = ScaleDF()\nAuser_cs = sdf.fit_transform(Auser_c)\n\ndate_begin = '2006'\ndate_end = '2009'\nfig, ax = plt.subplots(figsize=(15, 4))    \nax.plot(Auser_cs.loc[date_begin:date_end].index, Auser_cs.loc[date_begin:date_end, col_pred], \n        label='real - scaled', c='xkcd:true blue')\nax.plot(Auser_cs.loc[date_begin:date_end].index, \n        Auser_cs.loc[date_begin:date_end, 'Depth_to_Groundwater_SAL'], \n        label='Depth_to_Groundwater_SAL - scaled', c='xkcd:slate grey')\nax.plot(Auser_cs.loc[date_begin:date_end].index, Auser_cs.loc[date_begin:date_end, 'predicted'], \n        label='predicted - scaled', c='xkcd:cherry')\nax.legend()\nax.set_title(col_pred + ' prediction at empty period vs Depth_to_Groundwater_SAL - 2006-2009', \n             fontdict={'fontsize':16});","metadata":{"code_folding":[],"scrolled":true,"execution":{"iopub.status.busy":"2022-02-04T14:20:16.664307Z","iopub.execute_input":"2022-02-04T14:20:16.664509Z","iopub.status.idle":"2022-02-04T14:20:17.329505Z","shell.execute_reply.started":"2022-02-04T14:20:16.664485Z","shell.execute_reply":"2022-02-04T14:20:17.328771Z"},"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replace_nulls_with_prediction(Auser, col_pred, features, model)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:17.331047Z","iopub.execute_input":"2022-02-04T14:20:17.331511Z","iopub.status.idle":"2022-02-04T14:20:17.657558Z","shell.execute_reply.started":"2022-02-04T14:20:17.33147Z","shell.execute_reply":"2022-02-04T14:20:17.656851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select Data only since 2011","metadata":{}},{"cell_type":"markdown","source":"Now nulls are left only in fields depth to groundwater and Hydrometry_Piaggione:","metadata":{}},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser.T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,8))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Blues are nulls periods', fontsize=18);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:17.658716Z","iopub.execute_input":"2022-02-04T14:20:17.658964Z","iopub.status.idle":"2022-02-04T14:20:18.703302Z","shell.execute_reply.started":"2022-02-04T14:20:17.658937Z","shell.execute_reply":"2022-02-04T14:20:18.702427Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for depth to groundwater fields, we won't use them yet except for Depth_to_Groundwater_SAL (it's our target value).\n\nAnd Hydrometry_Piaggione has nulls only before 2011.\n\nLet's for now truncate data before 2011, and try to train models on data since 2011 only.  \nLater we will return for null filling for Hydrometry_Piaggione before 2011 and for traing models on train data since 2006.","metadata":{}},{"cell_type":"code","source":"Auser2011 = Auser.loc['2011':].copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:18.704619Z","iopub.execute_input":"2022-02-04T14:20:18.70499Z","iopub.status.idle":"2022-02-04T14:20:18.712487Z","shell.execute_reply.started":"2022-02-04T14:20:18.704946Z","shell.execute_reply":"2022-02-04T14:20:18.711472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser2011.T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,8))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Blues are nulls periods', fontsize=18);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:18.71403Z","iopub.execute_input":"2022-02-04T14:20:18.714309Z","iopub.status.idle":"2022-02-04T14:20:19.622121Z","shell.execute_reply.started":"2022-02-04T14:20:18.714271Z","shell.execute_reply":"2022-02-04T14:20:19.621255Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hydrometry_Piaggione","metadata":{}},{"cell_type":"markdown","source":"There are few nulls left in Hydrometry_Piaggione. Let's interpolate these values lineary.","metadata":{}},{"cell_type":"code","source":"Auser2011['Hydrometry_Piaggione'].interpolate(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.623452Z","iopub.execute_input":"2022-02-04T14:20:19.623859Z","iopub.status.idle":"2022-02-04T14:20:19.628713Z","shell.execute_reply.started":"2022-02-04T14:20:19.62383Z","shell.execute_reply":"2022-02-04T14:20:19.627916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Depth_to_Groundwater_SAL","metadata":{}},{"cell_type":"markdown","source":"There are some nulls in our target field. For now let's just interpolate it lineary. Later we will return and predict it based on other features.","metadata":{}},{"cell_type":"code","source":"Auser2011['Depth_to_Groundwater_SAL'].interpolate(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.630182Z","iopub.execute_input":"2022-02-04T14:20:19.630445Z","iopub.status.idle":"2022-02-04T14:20:19.640011Z","shell.execute_reply.started":"2022-02-04T14:20:19.630409Z","shell.execute_reply":"2022-02-04T14:20:19.639503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split into train and test sets","metadata":{}},{"cell_type":"markdown","source":"Now, as we filled in all nulls, let's put aside test set.\n\nAs test set we will use the last year of the data - since July 2019 till June 2020.","metadata":{}},{"cell_type":"code","source":"Ausert = Auser2011.iloc[:-365].copy() # train data","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.641262Z","iopub.execute_input":"2022-02-04T14:20:19.641682Z","iopub.status.idle":"2022-02-04T14:20:19.651795Z","shell.execute_reply.started":"2022-02-04T14:20:19.64164Z","shell.execute_reply":"2022-02-04T14:20:19.651257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"sdf = ScaleDF()\nAusert_sc = sdf.fit_transform(Ausert)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.653066Z","iopub.execute_input":"2022-02-04T14:20:19.653531Z","iopub.status.idle":"2022-02-04T14:20:19.66535Z","shell.execute_reply.started":"2022-02-04T14:20:19.65349Z","shell.execute_reply":"2022-02-04T14:20:19.664715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding Features","metadata":{}},{"cell_type":"markdown","source":"Now let's write transformers for adding extra features, such as:\n- week number;\n- sum of Volume_CSA and Volume_CSAL\n- sum of all volumes\n- lags for temperature, rain, volume and target value, such as:\n    - cumulative average for last n days\n    - shifted cumulative average, aka lagIxS, where I and S - integers, for example lag1x7  \n    lag1x7 mean that we take average for last 7 days, and then shift it for 1*7 days, see image below\n    - difference between neighboring shifted cumulative averages, aka difIxS  \n    $\\text{dif}IxS = lag(I-1)xS - lagIxS$  \n    For example, $\\text{dif}1x7 = \\text{dif0}x7 - \\text{dif}1x7 = \\text{avg}7 - \\text{dif}1x7$   \n    See image below.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='../input/acea-water-prediction-files/Lags.PNG', retina=True) ","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.666443Z","iopub.execute_input":"2022-02-04T14:20:19.666842Z","iopub.status.idle":"2022-02-04T14:20:19.681064Z","shell.execute_reply.started":"2022-02-04T14:20:19.666795Z","shell.execute_reply":"2022-02-04T14:20:19.680557Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddFeatures(BaseEstimator, TransformerMixin):\n    \n    # Transformer. Adds features, such as week number, volume average\n    \n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        res = X.copy()\n        res['week'] = res.index.isocalendar().week.astype('int32')\n        res['Volume_CSA_CSAL'] = (res['Volume_CSA'] + res['Volume_CSAL'])/2\n        res['Volume_all'] = (res['Volume_POL'] + res['Volume_CC1'] + res['Volume_CC2'] + res['Volume_CSA'] + res['Volume_CSAL'])/5\n        \n        weeks_in_year = 52.1429\n        res['week_sin'] = np.sin(2*np.pi*res.week/weeks_in_year)\n        res['week_cos'] = np.cos(2*np.pi*res.week/weeks_in_year)\n        \n        return res","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:19.682021Z","iopub.execute_input":"2022-02-04T14:20:19.682309Z","iopub.status.idle":"2022-02-04T14:20:19.689491Z","shell.execute_reply.started":"2022-02-04T14:20:19.682276Z","shell.execute_reply":"2022-02-04T14:20:19.688773Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmented volume columns\nvolume_cols_aug = volume_cols + ['Volume_CSA_CSAL', 'Volume_all']","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.690582Z","iopub.execute_input":"2022-02-04T14:20:19.690791Z","iopub.status.idle":"2022-02-04T14:20:19.699803Z","shell.execute_reply.started":"2022-02-04T14:20:19.690766Z","shell.execute_reply":"2022-02-04T14:20:19.699075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lag_flag(mode):\n    if mode == 'all':\n        lag_flag = True\n        avg_flag = True\n        dif_flag = True\n    if mode == 'lag and avg' or mode == 'avg and lag':\n        lag_flag = True\n        avg_flag = True\n        dif_flag = False\n    if mode == 'lag':\n        lag_flag = True\n        avg_flag = False\n        dif_flag = False\n    if mode == 'avg':\n        lag_flag = False\n        avg_flag = True\n        dif_flag = False\n    if mode == 'lag and dif':\n        lag_flag = True\n        avg_flag = False\n        dif_flag = True    \n    return lag_flag, avg_flag, dif_flag\n\ndef add_lags(df, cols, step, lag_nums, mode='all', min_periods=None, shift=0):\n    \n    # Adds lags:\n    # 1. cumulative average for last [step]*i days, where i in [lag_nums]\n    # 2. shifted cumulative average\n    # 3. difference between neighboring shifted cumulative averages\n    # Adds new columns to [df] inplace\n    # Adds for columns [cols]\n    \n    lag_flag, avg_flag, dif_flag = get_lag_flag(mode)\n    \n    day_num_for_rolling = np.array(lag_nums) * step\n    \n    for col in cols:\n        if avg_flag:        \n            for days in day_num_for_rolling:\n                avg_col_name = col + '_avg' + str(days)\n                df[avg_col_name] = df[col].rolling(days, min_periods=min_periods).mean().shift(shift)\n\n        if lag_flag:    \n            avg_step = df[col].rolling(step, min_periods=min_periods).mean()        \n            for lag in lag_nums:\n                lag_col_name = col + '_lag' + str(lag) + 'x' + str(step)\n                df[lag_col_name] = avg_step.shift(lag * step + shift)             \n                \n            if dif_flag:\n                for lag in lag_nums:\n                    dif_col_name = col + '_dif' + str(lag) + 'x' + str(step)\n                    dif = avg_step.shift((lag - 1) * step + shift) - avg_step.shift(lag * step + shift)   \n                    df[dif_col_name] = dif              \n                                     \ndef lags_col_name_list(cols, step, lag_nums, mode='all'):\n    \n    # Returns list of new columns names, added with function add_lags() \n\n    lag_flag, avg_flag, dif_flag = get_lag_flag(mode)    \n    res = []\n    day_num_for_rolling = np.array(lag_nums) * step\n    \n    for col in cols:\n        if avg_flag:        \n            for days in day_num_for_rolling:\n                res = res + [col + '_avg' + str(days)]\n\n        if lag_flag:                   \n            for lag in lag_nums:\n                res = res + [col + '_lag' + str(lag) + 'x' + str(step)]              \n                \n            if dif_flag:\n                for lag in lag_nums:\n                    res = res + [col + '_dif' + str(lag) + 'x' + str(step)]\n    return res        \n\ndef add_lags_on_first(df, cols_first_avg, step, lag_nums, mode='all'):\n\n    # Adds same lages as function add_lags(),\n    # but adds lags based on cumulative moving average for [step] days\n    # [cols_first_avg] should contain list of columns name from [df],\n    # containing moving average for [step] days\n    # Adds new columns to [df] inplace\n\n    lag_flag, avg_flag, dif_flag = get_lag_flag(mode)    \n    day_num_for_rolling = np.array(lag_nums) * step\n    \n    len_str_first_num = len(str(step))    \n    \n    for col in cols_first_avg:\n        if avg_flag:\n            for days in day_num_for_rolling:\n                s = df[col].copy()\n                for i in range(1, days // step):\n                    s = s + df[col].shift(i * step)    \n                new_col = col[:-len_str_first_num] + str(days)\n\n                if new_col != col:\n                    df[new_col] = s * step / days                \n            \n        if lag_flag:                    \n            for lag in lag_nums:\n                lag_col_name = col[:-len_str_first_num-4] + '_lag' + str(lag) + 'x' + str(step)\n                df[lag_col_name] = df[col].shift(lag * step)\n                \n            if dif_flag:\n                for lag in lag_nums:\n                    dif_col_name = col[:-len_str_first_num-4] + '_dif' + str(lag) + 'x' + str(step)\n                    df[dif_col_name] = df[col].shift((lag - 1) * step) - df[col].shift(lag * step)     ","metadata":{"code_folding":[0,23,54,76],"execution":{"iopub.status.busy":"2022-02-04T14:20:19.701038Z","iopub.execute_input":"2022-02-04T14:20:19.701254Z","iopub.status.idle":"2022-02-04T14:20:19.724684Z","shell.execute_reply.started":"2022-02-04T14:20:19.701228Z","shell.execute_reply":"2022-02-04T14:20:19.724021Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will design all estimators and transformers considering the following concepts:\n1. Data should be pandas dataframe (for X) or series (for y) and should have date index;\n2. Dates should be continious and there should be no nulls in the data other from nulls at the beginning;\n3. Test (validation) set should contain dates that follow dates of train set.  \nFor example, if train contain dates from 1 Jan 2011 till 31 Dec 2016, then test set should contain dates starting with 1 Jan 2017.","metadata":{}},{"cell_type":"markdown","source":"Transformer AddLags not only can add new columns, it also can predict values of features using model, given in input parameter *model*.\n\nIt will be useful for us when we will evaluate full prediction cycle: features (rain, temperature, volume) prediction + target value prediction.","metadata":{}},{"cell_type":"code","source":"class AddLags(BaseEstimator, TransformerMixin):\n    \n    # Transformer. Adds new columns:\n    # 1. cumulative average for last [step]*i days, where i in [lag_nums]\n    # 2. shifted cumulative average\n    # 3. difference between neighboring shifted cumulative averages\n    # For test set predicted values could be used.\n    # Then avg_step will be predicted, and then other columns will be calculated\n    # using avg_step\n    \n    def __init__(self, predict_time_features, cols, step, lag_nums, mode, model=None, features=None,\n                train_from_date=None):\n        # predict_time_features - if True, then new columns will be predicted fot test set\n        # if False, then actual data (given as input in test set) will be used        \n        # cols - columns for wich new columns will be added\n        # step - step (S) for calculating avgN (N=S*I), lagIxS, difIxS\n        # mode - should be 'all', 'lag and avg', 'lag', 'avg' \n        # lag_nums - list of lags numbers (I)\n        # model - model for predicting (in case if predict_time_features=True)\n        # features - list of features fot model\n        # train_from_date - date, from wich model will be trained\n        \n        self.predict_time_features = predict_time_features \n        self.cols = cols\n        self.step = step\n        self.lag_nums = lag_nums\n        self.mode = mode        \n        self.model = model\n        self.features = features\n        self.train_from_date = train_from_date\n        \n    def fit(self, X, y=None):\n        # Сохраняем обучающую выборку\n        self.saved_train = X.copy() \n        return self\n    \n    def transform(self, X):\n        \n        if X.index.min() == self.saved_train.index.min():\n            stage = 'train'\n        else:\n            stage = 'test'\n \n        df = X.copy()\n        res = []\n\n        # On train stage we just add new columns\n        if stage == 'train': \n            add_lags(df, self.cols, self.step, self.lag_nums, self.mode)\n            res = df\n                    \n        # On test stage we add new columns using saved train set (previous periods)\n        # If predict_time_features=True then we will predict date\n        # Else we will use unpute data\n        if stage == 'test':            \n            if self.predict_time_features:        \n                \n                train = self.saved_train.copy()\n                test = X.copy()\n                \n                # Adding columns to train set\n                add_lags(train, self.cols, self.step, lag_nums=[1], mode='avg')                \n                cols_avg_step = lags_col_name_list(self.cols, self.step, lag_nums=[1], mode='avg')\n                \n                # Predict avg_step and join predicted values to test set\n                for col_avg in cols_avg_step: \n                    \n                    if self.train_from_date is not None:\n                        train.loc[:self.train_from_date, col_avg] = np.nan\n                                        \n                    # Drop NA from train\n                    train_no_NA = train.copy()                    \n                    train_no_NA.dropna(subset=self.features + [col_avg], inplace=True)                     \n                    if len(train_no_NA) == 0:\n                        test[col_avg] = np.nan\n                    else:                        \n                        self.model.fit(train_no_NA[self.features], train_no_NA[col_avg])            \n                        test[col_avg] = self.model.predict(test[self.features])\n                                                      \n                # Union train set containing new columns avg_step\n                # with test set containing predicted columns avg_step\n                train_and_test_avg = pd.concat([train, test], axis=0)                \n                \n                # Calculate all other avg, lag, dif base on avg_step\n                add_lags_on_first(train_and_test_avg, cols_avg_step, self.step, self.lag_nums, self.mode)\n                \n                # We leave only those dates, which were in inpute test set\n                res = train_and_test_avg.iloc[-len(X):]\n                \n                # Make colums order unified\n                cols_avg = lags_col_name_list(self.cols, self.step, self.lag_nums, self.mode)\n                prime_cols = X.columns.tolist()\n                res = res[prime_cols + cols_avg]\n                \n            else: # actual data\n                # Union saved train set with unpute test set\n                train_and_test = pd.concat([self.saved_train, X], axis=0).sort_index()               \n                add_lags(train_and_test, self.cols, self.step, self.lag_nums, self.mode)                \n                res = train_and_test[-len(X):]\n                \n        return res","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:19.730493Z","iopub.execute_input":"2022-02-04T14:20:19.730988Z","iopub.status.idle":"2022-02-04T14:20:19.751788Z","shell.execute_reply.started":"2022-02-04T14:20:19.730953Z","shell.execute_reply":"2022-02-04T14:20:19.750985Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first let's try to use the following features:\n- Rainfall\n- Temperature\n- Volume","metadata":{}},{"cell_type":"markdown","source":"As a start let's try adding following lags for features:\n- for rainfall and temperature lets use cumulative moving averages with step = 5 days, lags_nums [1, 2, 3, 4, 5, 6, 9, 12]\n- for volume - cumulative moving averages with step = 7 days, lags_nums [1, 2, 3, 4, 5, 6, 7, 9]","metadata":{}},{"cell_type":"markdown","source":"Let's write a pipeline for adding features:","metadata":{}},{"cell_type":"code","source":"def get_pipe(predict_rain, predict_temp, predict_vol):\n    pipe = Pipeline([\n        ('AddFeatures', AddFeatures()),\n        ('AddRainLags', AddLags(predict_time_features=predict_rain, \n                                       cols=rain_cols, \n                                       step=5, lag_nums=[1,2,3,4,5,6,9,12], mode='avg'\n                                      )\n        ),\n        ('AddTempLags', AddLags(predict_time_features=predict_temp, \n                                       cols=temper_cols, \n                                       step=5, lag_nums=[1,2,3,4,5,6,9,12], mode='avg'\n                                      )\n        ),\n        ('AddVolLags', AddLags(predict_time_features=predict_vol, \n                                       cols=volume_cols_aug, \n                                       step=7, lag_nums=[1,2,3,4,5,6,7,9], mode='avg'                                 \n                                      )\n        )\n        ])\n    return pipe","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:20:19.752994Z","iopub.execute_input":"2022-02-04T14:20:19.753619Z","iopub.status.idle":"2022-02-04T14:20:19.764843Z","shell.execute_reply.started":"2022-02-04T14:20:19.753569Z","shell.execute_reply":"2022-02-04T14:20:19.764284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test our pipeline:","metadata":{}},{"cell_type":"code","source":"pipe_prepare_no_pred = get_pipe(predict_rain=False, \n                                predict_temp=False, \n                                predict_vol=False\n                               )\nAusert_with_new_cols = pipe_prepare_no_pred.fit_transform(Ausert)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.765971Z","iopub.execute_input":"2022-02-04T14:20:19.766752Z","iopub.status.idle":"2022-02-04T14:20:19.939594Z","shell.execute_reply.started":"2022-02-04T14:20:19.766686Z","shell.execute_reply":"2022-02-04T14:20:19.939003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.940536Z","iopub.execute_input":"2022-02-04T14:20:19.940838Z","iopub.status.idle":"2022-02-04T14:20:19.945384Z","shell.execute_reply.started":"2022-02-04T14:20:19.940811Z","shell.execute_reply":"2022-02-04T14:20:19.944708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert_with_new_cols.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.94657Z","iopub.execute_input":"2022-02-04T14:20:19.946811Z","iopub.status.idle":"2022-02-04T14:20:19.955068Z","shell.execute_reply.started":"2022-02-04T14:20:19.946782Z","shell.execute_reply":"2022-02-04T14:20:19.954628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert_with_new_cols.columns[27:]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:19.956039Z","iopub.execute_input":"2022-02-04T14:20:19.95675Z","iopub.status.idle":"2022-02-04T14:20:19.964543Z","shell.execute_reply.started":"2022-02-04T14:20:19.9567Z","shell.execute_reply":"2022-02-04T14:20:19.964049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that new columns have been added.\n\nFor prediction we will use only new added columns, because if we want to use original columns, then we can execute AddLags transformer with parameter step=1 - we will try it later, at feature selection stage.\n\nSo, we will pass to model columns from 29th column (from 'week_sin').","metadata":{}},{"cell_type":"markdown","source":"For now let's visualize some selected new features, to make sure, that pipeline works properly:","metadata":{}},{"cell_type":"code","source":"# Temperature_Orentano\nfig, ax = plt.subplots(figsize=(15,3))\nax.plot(Ausert_with_new_cols['Temperature_Orentano'], label='Temperature_Orentano')\nax.plot(Ausert_with_new_cols['Temperature_Orentano_avg60'], label='Temperature_Orentano_avg60')\nfig.legend()\nfig.suptitle('Temperature_Orentano - original and moving average for last 60 days', fontsize=16);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:19.965503Z","iopub.execute_input":"2022-02-04T14:20:19.965818Z","iopub.status.idle":"2022-02-04T14:20:20.276547Z","shell.execute_reply.started":"2022-02-04T14:20:19.965788Z","shell.execute_reply":"2022-02-04T14:20:20.275774Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rainfall_Gallicano\nfig, ax = plt.subplots(figsize=(15,3))\nax.plot(Ausert_with_new_cols['Rainfall_Gallicano'], label='Rainfall_Gallicano')\nax.plot(Ausert_with_new_cols['Rainfall_Gallicano_avg60'], label='Rainfall_Gallicano_avg60')\nfig.legend()\nfig.suptitle('Rainfall_Gallicano - original and moving average for last 60 days', fontsize=16);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:20.277669Z","iopub.execute_input":"2022-02-04T14:20:20.277964Z","iopub.status.idle":"2022-02-04T14:20:20.578099Z","shell.execute_reply.started":"2022-02-04T14:20:20.277932Z","shell.execute_reply":"2022-02-04T14:20:20.577344Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets use our pipeline on scaled data, and select only columns from 29th column. For this we will need a transformer for selecting columns from Nth column.","metadata":{}},{"cell_type":"code","source":"class SelectColsFromN(BaseEstimator, TransformerMixin):\n    def __init__(self, N):\n        self.N = N\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.iloc[:, self.N:]","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:20.579178Z","iopub.execute_input":"2022-02-04T14:20:20.579378Z","iopub.status.idle":"2022-02-04T14:20:20.584287Z","shell.execute_reply.started":"2022-02-04T14:20:20.579354Z","shell.execute_reply":"2022-02-04T14:20:20.583785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipe_prepare_no_pred.steps.append(['SelectFeatures', SelectColsFromN(29)])\n\nAusert_pr = pipe_prepare_no_pred.fit_transform(Ausert_sc)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:20.585245Z","iopub.execute_input":"2022-02-04T14:20:20.585958Z","iopub.status.idle":"2022-02-04T14:20:20.757326Z","shell.execute_reply.started":"2022-02-04T14:20:20.585928Z","shell.execute_reply":"2022-02-04T14:20:20.756545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Ausert_pr* is a DataFrame, containg all original columns and additionaly *week_sin*, *week_cos* and scaled lags of temperature, rain, and volume.","metadata":{}},{"cell_type":"markdown","source":"# Target Value Smoothing","metadata":{}},{"cell_type":"markdown","source":"By looking at targer value plot (see below) we can see, that line in some places is too thick. It means that smoothing is needed.","metadata":{}},{"cell_type":"code","source":"col_pred = 'Depth_to_Groundwater_SAL' # target value","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:20.758399Z","iopub.execute_input":"2022-02-04T14:20:20.758636Z","iopub.status.idle":"2022-02-04T14:20:20.762463Z","shell.execute_reply.started":"2022-02-04T14:20:20.758605Z","shell.execute_reply":"2022-02-04T14:20:20.761735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_smoothed = Ausert[col_pred].rolling(7, center=True, min_periods=1).mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:20.763607Z","iopub.execute_input":"2022-02-04T14:20:20.763911Z","iopub.status.idle":"2022-02-04T14:20:20.772409Z","shell.execute_reply.started":"2022-02-04T14:20:20.763879Z","shell.execute_reply":"2022-02-04T14:20:20.771768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing smoothed and unsmoothed plot of Depth_to_Groundwater_SAL\nfig, ax = plt.subplots(2,1, figsize=(15,6))\nax[0].plot(Ausert[col_pred], label='unsmoothed')\nax[1].plot(y_smoothed, label='smoothed')\nax[0].legend()\nax[1].legend()\nfig.suptitle(col_pred + ' - Comparing smoothed and unsmoothed', fontsize=16);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:20.773353Z","iopub.execute_input":"2022-02-04T14:20:20.773896Z","iopub.status.idle":"2022-02-04T14:20:21.269687Z","shell.execute_reply.started":"2022-02-04T14:20:20.773862Z","shell.execute_reply":"2022-02-04T14:20:21.269106Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splits for CV","metadata":{}},{"cell_type":"markdown","source":"As mentioned earlier, we are going to predict water level for 3 months in the future.\n\nIn real life, before choosing the depth of prediction, one should always ask the business customer about prediction purpose.  \nFor example the most important thing for the customer may be to know the water level exactly after 3 months, and no matter what will happen to the water level in between.\n\nThen the quality metrics should be adjusted in such a way that allow to achieve the best quality in those time intervals that are really important for customer.\n\nIn this task, I assumed that the customer is equally interested in the daily water levels throughout the next 3 months. Therefore, I predict daily water levels for the next 3 months.","metadata":{}},{"cell_type":"markdown","source":"For cross validation splitting method we've chosen sklearn class TimeSeriesSplit, which splits the data such way that the test set always directly follows the training set, see illustration below.  \nTest sets are yellow, train sets are green.","metadata":{}},{"cell_type":"code","source":"# Illustration of splits for CV - test_size = 90 days, 5 splits\ntscv = TimeSeriesSplit(n_splits=5, test_size=90)\n\ndf = Ausert.copy()\n\ni = 0\nfor train, test in tscv.split(df):\n    df['split' + str(i)] = np.nan\n    df.iloc[train, -1] = 0\n    df.iloc[test, -1] = 1\n    i = i + 1\n\ndft = df.iloc[:, -tscv.n_splits:].T\n\ndft.columns = dft.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,2))\nsns.heatmap(dft, cmap='Set3', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Splits for CV - test_size = 90 days, 5 splits', fontsize=18, y=1.07);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:21.270609Z","iopub.execute_input":"2022-02-04T14:20:21.270984Z","iopub.status.idle":"2022-02-04T14:20:21.567728Z","shell.execute_reply.started":"2022-02-04T14:20:21.27095Z","shell.execute_reply":"2022-02-04T14:20:21.566756Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we increase splits quantity to 25, then last 5 splits would remain the same, and new splits with test sets in earlier period would be added, see illustration below.","metadata":{}},{"cell_type":"code","source":"n_splits=25\ntest_size=90\n\ntscv=TimeSeriesSplit(n_splits=n_splits, test_size=test_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:20:21.569276Z","iopub.execute_input":"2022-02-04T14:20:21.569708Z","iopub.status.idle":"2022-02-04T14:20:21.574168Z","shell.execute_reply.started":"2022-02-04T14:20:21.569667Z","shell.execute_reply":"2022-02-04T14:20:21.573581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Illustration of splits for CV - test_size = 90 days, 25 splits\ndf = Ausert.copy()\n\ni = 0\nfor train, test in tscv.split(df):\n    df['split' + str(i)] = np.nan\n    df.iloc[train, -1] = 0\n    df.iloc[test, -1] = 1\n    i = i + 1\n\ndft = df.iloc[:, -tscv.n_splits:].T\n\ndft.columns = dft.columns.astype('str')\nfig, ax = plt.subplots(figsize=(15,6))\nsns.heatmap(dft, cmap='Set3', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Splits for CV - test_size = 90 days, 25 splits', fontsize=18, y=0.95);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:21.575759Z","iopub.execute_input":"2022-02-04T14:20:21.576278Z","iopub.status.idle":"2022-02-04T14:20:22.130606Z","shell.execute_reply.started":"2022-02-04T14:20:21.576237Z","shell.execute_reply":"2022-02-04T14:20:22.129754Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's choose n_splits=25.  \nFor this parameter value on the one hand there are enough test splits to smooth average RMSE, and on the other hand the minimum size of train set is sufficient for training (more than 2 years).","metadata":{}},{"cell_type":"markdown","source":"# Own Value Model","metadata":{}},{"cell_type":"markdown","source":"We are going to predict target value based not only on other features (like temperature, rain etc.) but also on it's own previous values.\n\nSo to predict data point t+2, we'll need t+1 prediction.  \nTo predict t+3, we'll need t+1, t+2 predictions.\n\nThus we will predict future values iteratively:\n1. Predict t+1\n2. Calculate required lags of target value for point t+1, considering t+1 predicted value;\n3. Predict t+2 based on  lags calculated at previous step;\n4. Calculate lags considering t+1 and t+2;  \nAnd so on...","metadata":{}},{"cell_type":"markdown","source":"Let's write an estimator that implements iterative prediction and calculating lags of target value.","metadata":{}},{"cell_type":"code","source":"class OwnValueModel(BaseEstimator):\n    \n    # Estimator. Adds new features - lags of y:\n    # 1. cumulative average for last [step]*i days, where i in [lag_nums]\n    # 2. shifted cumulative average\n    # 3. difference between neighboring shifted cumulative averages\n    # Train model taking into account new features too.\n    # Iteratively predicts point if future, every time calculating lags of y.\n    # Scales y, predicts scaled data, then unscales prediction to the original representation\n        \n    def __init__(self, model, step, lag_nums, mode='all', min_periods=1):\n        self.model = model\n        self.step = step\n        self.lag_nums = lag_nums\n        self.mode = mode\n        self.min_periods = min_periods\n                \n    def fit(self, X, y):        \n        \n        # Train and apply StandardScaler\n        self.scaler = StandardScaler()\n        y_scaled = pd.DataFrame(self.scaler.fit_transform(pd.DataFrame(y))).iloc[:, 0]\n        \n        # Save scaled y from train set  \n        self.last_own = y_scaled\n        \n        # Create single table with X and y, add in it scaled lags of y.\n        # Lags do not count current day (shifted by 1 day)\n        # Then delete rows, in which there is at least one NA\n        train = X.copy()\n        train['y_scaled'] = np.array(y_scaled)\n        add_lags(train, ['y_scaled'], self.step, self.lag_nums, self.mode, self.min_periods, shift=1)\n        lags_cols = lags_col_name_list(['y_scaled'], self.step, self.lag_nums, self.mode) \n        prime_cols = X.columns.tolist()\n        train.dropna(subset=prime_cols + lags_cols, inplace=True)       \n        \n        # Train the model   \n        self.model.fit(train[prime_cols + lags_cols], train['y_scaled'])\n        \n        # Save feature_importances_, if model have this attribute\n        if hasattr(self.model, 'feature_importances_'):\n            self.feature_importances_ = self.model.feature_importances_\n        # Save column names to use them while printing feature importances\n        self.cols = prime_cols + lags_cols\n        \n        return self\n    \n    def predict(self, X):\n        \n        pred = []\n        \n        # Array of scale y from train set\n        last_own = np.array(self.last_own)\n        \n        # Cycle by quantity of elements in test set\n        for i in range(len(X)):            \n            \n            # Element of test set wihtout lags columns\n            test_el = X.iloc[i]\n                        \n            last_own_df = pd.DataFrame(last_own, copy=True)\n            last_own_df.columns = ['y_scaled']\n            add_lags(last_own_df, ['y_scaled'], self.step, self.lag_nums, self.mode, \n                     self.min_periods) # there is no need to do shift=1\n            test_el = pd.concat([test_el, last_own_df.iloc[-1, 1:]])            \n            \n            # Predict scaled value\n            pred_el = self.model.predict(pd.DataFrame(test_el).T)[0]            \n            pred = pred + [pred_el]\n            \n            # Add predicted value\n            last_own = last_own.tolist()\n            last_own = last_own + [pred_el]\n            last_own = np.array(last_own)        \n\n        # Unscale prediction\n        pred_unscaled = self.scaler.inverse_transform(np.array(pred).reshape(-1, 1)).reshape(-1)    \n        \n        return pred_unscaled","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:22.132059Z","iopub.execute_input":"2022-02-04T14:20:22.132507Z","iopub.status.idle":"2022-02-04T14:20:22.149211Z","shell.execute_reply.started":"2022-02-04T14:20:22.132465Z","shell.execute_reply":"2022-02-04T14:20:22.148445Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on Actual Data","metadata":{}},{"cell_type":"markdown","source":"Now our plan is:\n\n1. Choose model for predicting target value based on actual temperature, rain, volume and on own previous values;\n2. Choose models for predicting temperature, rain, volume;\n3. Evaluate whole prediction pipeline, icluding both feature prediction and target value prediction.\n\nAs for point 1, here is detailed plan:\n\n1. Choose model type and model parameters;  \n2. Choose lags of target value;\n3. Choose lags of features;\n4. Drop unimportant features; \n5. After we have final model, we will fill in nulls in target value with prediction (those nulls, that we've filled with linear interplolation for now in paragraph [3.8.8](#Depth_to_Groundwater_SAL);\n6. Select train set.","metadata":{}},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"markdown","source":"Now let's test our OwnValueModel (OVM furter) and choose a model.  \nFor this we will create a pipeline. For start we will use following parameters OVM values:\n- lags for target value: cumulative moving averages with step = 10 days, lags_nums [1, 2, 3, 4, 5]","metadata":{}},{"cell_type":"code","source":"def test_OVM(model, model_title):\n    ovm = OwnValueModel(model, \n                        step=10, \n                        lag_nums=[1,2,3,4,5], \n                        mode='avg', \n                        min_periods=None\n                       )\n    scores, pred = plot_pred_all_splits(ovm, Ausert_pr.iloc[:,29:], \n                                        y=y_smoothed, \n                                        cols=Ausert_pr.iloc[:,29:].columns, cv=tscv, mark_pred_start=True,\n                                        plot_title=col_pred + ' - ' + model_title + ' prediction',\n                                        dropNA=False)","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:20:22.150148Z","iopub.execute_input":"2022-02-04T14:20:22.15033Z","iopub.status.idle":"2022-02-04T14:20:22.162763Z","shell.execute_reply.started":"2022-02-04T14:20:22.150307Z","shell.execute_reply":"2022-02-04T14:20:22.162104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_OVM(RandomForestRegressor(random_state=1), 'RF')","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:20:22.163781Z","iopub.execute_input":"2022-02-04T14:20:22.164295Z","iopub.status.idle":"2022-02-04T14:26:29.28849Z","shell.execute_reply.started":"2022-02-04T14:20:22.164258Z","shell.execute_reply":"2022-02-04T14:26:29.287628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_OVM(ExtraTreesRegressor(random_state=1), 'Extra Trees')","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:26:29.289911Z","iopub.execute_input":"2022-02-04T14:26:29.290785Z","iopub.status.idle":"2022-02-04T14:29:28.384649Z","shell.execute_reply.started":"2022-02-04T14:26:29.290737Z","shell.execute_reply":"2022-02-04T14:29:28.383479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_OVM(LinearRegression(), 'Linear Regression')","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:29:28.386946Z","iopub.execute_input":"2022-02-04T14:29:28.387305Z","iopub.status.idle":"2022-02-04T14:29:48.427954Z","shell.execute_reply.started":"2022-02-04T14:29:28.38726Z","shell.execute_reply":"2022-02-04T14:29:48.426918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_OVM(KNeighborsRegressor(), 'KNN')","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-02-04T14:29:48.429312Z","iopub.execute_input":"2022-02-04T14:29:48.429557Z","iopub.status.idle":"2022-02-04T14:30:22.807639Z","shell.execute_reply.started":"2022-02-04T14:29:48.429529Z","shell.execute_reply":"2022-02-04T14:30:22.807128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_OVM(SVR(), 'SVN')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-04T14:30:22.808686Z","iopub.execute_input":"2022-02-04T14:30:22.808922Z","iopub.status.idle":"2022-02-04T14:30:42.99259Z","shell.execute_reply.started":"2022-02-04T14:30:22.808893Z","shell.execute_reply":"2022-02-04T14:30:42.99186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the best results shows extra trees, random forest and SVN.\n\nAs for linear regression it shows awfull results, because at some splits prediction goes straight upward or straight downward. It seems that problem is in multicollinearity: we have a lot of similar features like rainfall in different areas or cumulative moving average for different days quantity of the same feature. Obviously these features are highly dependent on each other.","metadata":{}},{"cell_type":"markdown","source":"## Model Parameters Selection","metadata":{}},{"cell_type":"markdown","source":"It's time to check different model parameters for extra trees, random forest and SVN.  \nWe'll do it with grid search.\n\nFor brevity and for minimizing execution time of this notebook, I will give here only the results of grid search:\n- SVM:\n    - RMSE = 0.1766\n    - parameters: C=50, gamma=0.0005  \n    RMSE hightly depends on parameters C and gamma. For some other values of C and gamma RMSE is dramatically greater.\n- Extra Trees - the best results so far:  \n    - RMSE is around 0.173-0.177 - depending on random_state parameter value\n    - for some random_state values best parameters are default, and for some - min_samples_split=10  \n    For different parameter values RMSE is approximately the same.\n- Random Forest:\n    - RMSE = approximatly 0.181-0.189 - depending on random_state parameter value\n    - for different parameter values RMSE is approximately the same ","metadata":{}},{"cell_type":"markdown","source":"As example of grid search, a short one is given below.","metadata":{}},{"cell_type":"markdown","source":"First of all, some useful functions for printing grid search results:","metadata":{}},{"cell_type":"code","source":"def get_grid_results(grid):\n    return [(p1, p2) for p1, p2 in zip(grid.cv_results_['mean_test_score'], grid.cv_results_['params'])]\n\ndef get_value_from_dict(dictionary, key):\n    return dictionary[key]\n\ndef grid_results_to_csv(grid, file_name):\n    d = [(p1, p2) for p1, p2 in zip(grid.cv_results_['mean_test_score'], grid.cv_results_['params'])]\n    d = pd.DataFrame(d, columns=['Accuracy', 'Params']) \n    for k in grid.cv_results_['params'][0].keys():\n        d[k] = d['Params'].apply(get_value_from_dict, args=(k,))\n    d.drop('Params', axis=1, inplace=True) \n    d.to_csv(file_name)","metadata":{"code_folding":[0,3,6],"execution":{"iopub.status.busy":"2022-02-04T14:30:42.996372Z","iopub.execute_input":"2022-02-04T14:30:42.996579Z","iopub.status.idle":"2022-02-04T14:30:43.003546Z","shell.execute_reply.started":"2022-02-04T14:30:42.996553Z","shell.execute_reply":"2022-02-04T14:30:43.003003Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Secondly, useful function for searching best parameters in case if model takes as input another model (like OwnValueModel):","metadata":{}},{"cell_type":"code","source":"def construct_model_list_with_params(model_class, params):\n    \n    # Creates a list of models of the [model_class] class with all possible combinations \n    # of [params] parameters\n    # [params] is a dictionary of the form {'param1':[1,2,3], 'param2':[4,5]} \n    # (the same as for grid search)    \n    \n    # Loop that creates a DataFrame with all possible combinations of parameters\n    i = 0\n    for par in params.keys():\n        if i == 0:\n            df1 = pd.DataFrame(params[par])\n            df1.columns = [par]\n            df_res = df1\n        else:\n            df2 = pd.DataFrame(params[par])\n            df2.columns = [par]\n            df_res = df1.merge(df2, how='cross')\n            df1 = df_res        \n        i = i + 1    \n\n    # A cycle that creates a list of models with all possible combinations of parameters\n    models = []\n    for i in range(len(df_res)):\n        pars = df_res.iloc[i].to_dict()\n        models.append(model_class(**pars))\n    return models","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:30:43.004548Z","iopub.execute_input":"2022-02-04T14:30:43.004934Z","iopub.status.idle":"2022-02-04T14:30:43.01604Z","shell.execute_reply.started":"2022-02-04T14:30:43.004861Z","shell.execute_reply":"2022-02-04T14:30:43.015366Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing model and parameters values list:","metadata":{}},{"cell_type":"code","source":"ovm = OwnValueModel(SVR(), \n                    step=10, \n                    lag_nums=[1,2,3,4,5], \n                    mode='avg', \n                    min_periods=None\n                   )","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:30:43.017076Z","iopub.execute_input":"2022-02-04T14:30:43.017302Z","iopub.status.idle":"2022-02-04T14:30:43.026963Z","shell.execute_reply.started":"2022-02-04T14:30:43.017277Z","shell.execute_reply":"2022-02-04T14:30:43.026281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ET_param = {'n_estimators':[100, 400],\n          'min_samples_split':[2, 10, 100],\n          'random_state':[1]}\nET_list = construct_model_list_with_params(ExtraTreesRegressor, ET_param)\n\nSVR_param = {'C':[0.1,1,50],\n               'gamma':[0.0005, 0.01,0.1,1]\n              }\nSVR_list = construct_model_list_with_params(SVR, SVR_param)\n\nparam = {'model': ET_list + SVR_list}","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:30:43.029789Z","iopub.execute_input":"2022-02-04T14:30:43.030018Z","iopub.status.idle":"2022-02-04T14:30:43.058317Z","shell.execute_reply.started":"2022-02-04T14:30:43.029993Z","shell.execute_reply":"2022-02-04T14:30:43.057593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, grid search itself:","metadata":{}},{"cell_type":"code","source":"grid = GridSearchCV(ovm, \n                    param, \n                    cv=tscv, \n                    scoring='neg_root_mean_squared_error',\n                    n_jobs=-1)\ngrid.fit(Ausert_pr.iloc[:,29:], y=y_smoothed)\nget_grid_results(grid)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:30:43.060351Z","iopub.execute_input":"2022-02-04T14:30:43.060813Z","iopub.status.idle":"2022-02-04T14:43:42.29289Z","shell.execute_reply.started":"2022-02-04T14:30:43.060782Z","shell.execute_reply":"2022-02-04T14:43:42.291964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Selection","metadata":{}},{"cell_type":"markdown","source":"Now let's drop some unimportant features and try to compare different models again.\n\nWe've got a lot of similar features. Let's try to drop some. Let's write a function, that will try to drop features one by one and compare RMSE of model including this feature and not including.","metadata":{}},{"cell_type":"code","source":"def find_best_cols_comb(X, y, model, cols, start_comb, cv):\n \n    # Searches for the best combination of features for the [model] among the features [cols] \n    # As initial features combination uses [start_comb]\n    # At first tries to drop features. \n    # Tries to drop them one by one, each time recalculating RMSE.    \n    # Then one by one tries to add each feature that wasn't in the initial combination.\n    # For RMSE calculating cross-validation is used\n\n    sc = 'neg_root_mean_squared_error'\n    \n    # Transforms list of column names to set \n    cols = set(cols)\n    start_comb = set(start_comb)\n    res_comb = start_comb.copy()\n    \n    # Calculates RMSE on all features of initial combination\n    rmse_prev = cross_val_score(model, X[res_comb], y, cv=cv, scoring=sc).mean() * (-1)\n    \n    # Tries to drop each feature one by one\n    for col in start_comb:\n        res_comb.remove(col)\n        rmse_new = cross_val_score(model, X[res_comb], y, cv=cv, scoring=sc).mean() * (-1)\n        if rmse_new < rmse_prev:\n            print('Deleted', col, 'rmse before:', rmse_prev, 'rmse after:', rmse_new)\n            rmse_prev = rmse_new\n        else:\n            res_comb.add(col)\n    \n    # Finds all columns, that weren't in initial combination\n    cols_try_to_add = cols.difference(start_comb)    \n    \n    # One by one tries to add features, that weren't in initial combination\n    for col in cols_try_to_add:\n        res_comb.add(col)\n        rmse_new = cross_val_score(model, X[res_comb], y, cv=cv, scoring=sc).mean() * (-1)\n        if rmse_new < rmse_prev:\n            print('Added', col, 'rmse before:', rmse_prev, 'rmse after:', rmse_new)\n            rmse_prev = rmse_new\n        else:\n            res_comb.remove(col)\n\n    return list(res_comb)","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-02-04T14:43:42.294827Z","iopub.execute_input":"2022-02-04T14:43:42.295342Z","iopub.status.idle":"2022-02-04T14:43:42.306844Z","shell.execute_reply.started":"2022-02-04T14:43:42.295292Z","shell.execute_reply":"2022-02-04T14:43:42.305947Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will try deleting unimportant features using as estimator SVM model:","metadata":{}},{"cell_type":"code","source":"ovm = OwnValueModel(SVR(C=50, gamma=0.0005), \n                    step=10, \n                    lag_nums=[1,2,3,4,5], \n                    mode='avg', \n                    min_periods=None\n                   )","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:43:42.308054Z","iopub.execute_input":"2022-02-04T14:43:42.308279Z","iopub.status.idle":"2022-02-04T14:43:42.331568Z","shell.execute_reply.started":"2022-02-04T14:43:42.308251Z","shell.execute_reply":"2022-02-04T14:43:42.330887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will execute find_best_cols_comb twice.","metadata":{}},{"cell_type":"code","source":"new_cols = find_best_cols_comb(X=Ausert_pr.iloc[:,29:],\n                               y=y_smoothed,\n                               model=ovm,\n                               cols=Ausert_pr.iloc[:,29:].columns.tolist(),\n                               start_comb=Ausert_pr.iloc[:,29:].columns.tolist(),\n                               cv=tscv\n                              )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = find_best_cols_comb(X=Ausert_pr.iloc[:,29:],\n                                        y=y_smoothed,\n                                        model=ovm,\n                                        cols=Ausert_pr.iloc[:,29:].columns.tolist(),\n                                        start_comb=new_cols,\n                                        cv=tscv\n                                       )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE have improved to 0.1129 after deleting unimportant features.\n\nLet's plot the prediction.","metadata":{}},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(ovm, Ausert_pr.iloc[:,29:], y=y_smoothed, \n                                    cols=selected_features, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' - SVM - selected features prediction',\n                                    dropNA=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:43:42.333107Z","iopub.execute_input":"2022-02-04T14:43:42.333613Z","iopub.status.idle":"2022-02-04T14:44:02.828607Z","shell.execute_reply.started":"2022-02-04T14:43:42.333577Z","shell.execute_reply":"2022-02-04T14:44:02.827786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM prediction looks pretty good.\n\nIf we try to execute *find_best_cols_comb* function using extra trees model, we won't get such significant improvement. With extra trees RMSE would improve only to 0.13-0.14 after deleting features. And it seems that improvement only takes place due to randomization of extra trees algorithm.  \nFor brevity we won't give here this execution.\n\nIf we try to test extra trees on features, selected with SVN, then results won't be greate too:","metadata":{}},{"cell_type":"code","source":"ovm_ext = OwnValueModel(ExtraTreesRegressor(random_state=1), \n                        step=10, \n                        lag_nums=[1,2,3,4,5], \n                        mode='avg', \n                        min_periods=None\n                       )\nscores, pred = plot_pred_all_splits(ovm_ext, Ausert_pr.iloc[:,29:], y=y_smoothed, \n                                    cols=selected_features, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' - ET - selected features prediction',\n                                    dropNA=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:44:02.830001Z","iopub.execute_input":"2022-02-04T14:44:02.830835Z","iopub.status.idle":"2022-02-04T14:45:45.218505Z","shell.execute_reply.started":"2022-02-04T14:44:02.830788Z","shell.execute_reply":"2022-02-04T14:45:45.217679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Own Value Lags Selection","metadata":{}},{"cell_type":"markdown","source":"Now let's choose own value lags (previous values of target value) using grid search.","metadata":{}},{"cell_type":"code","source":"param = [{'step':[5,10,12,14,16,18,20,23,25,28,32,35,38,40,45,50,60], 'lag_nums':[[1]], 'mode':['avg']},\n         {'step':[1], 'lag_nums':[[1,2,3,4,5,6,10,15,20,30,50,60]], 'mode':['avg', 'lag', 'lag and dif']},         \n         {'step':[2,3], 'lag_nums':[[1,2,3,4,5,6,7,8,10,15,20,25]], 'mode':['avg', 'lag', 'lag and dif']},         \n         {'step':[4,5,6], 'lag_nums':[[1,2,3,4,5,6,7,10,15]], 'mode':['avg', 'lag', 'lag and dif']},         \n         {'step':[7,10,14], 'lag_nums':[[1,2,3,4,5]], 'mode':['avg', 'lag', 'lag and dif']}\n        ]\n\ngrid = GridSearchCV(ovm, \n                    param, \n                    cv=tscv, \n                    scoring='neg_root_mean_squared_error',\n                    n_jobs=-1)\ngrid.fit(Ausert_pr[selected_features], y=y_smoothed)\nprint('best score RMSE = ')\ngrid.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:45:45.219862Z","iopub.execute_input":"2022-02-04T14:45:45.220114Z","iopub.status.idle":"2022-02-04T14:52:51.037683Z","shell.execute_reply.started":"2022-02-04T14:45:45.220082Z","shell.execute_reply":"2022-02-04T14:52:51.03663Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:52:51.039249Z","iopub.execute_input":"2022-02-04T14:52:51.039574Z","iopub.status.idle":"2022-02-04T14:52:51.046454Z","shell.execute_reply.started":"2022-02-04T14:52:51.03953Z","shell.execute_reply":"2022-02-04T14:52:51.045648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very strange, model achieved the best results exactly with those parameters values, that we've chosen at the beginning.  \nThis may mean that our selected features are optimized for these paramerers values.","metadata":{}},{"cell_type":"markdown","source":"## Features Lags Selection","metadata":{}},{"cell_type":"markdown","source":"Now let's find the best combinations of rain, temperature and volume lags.\n\nFor this we will need a simple transformer, that selects columns by their names:","metadata":{}},{"cell_type":"code","source":"class SelectFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.features]","metadata":{"code_folding":[0,1],"execution":{"iopub.status.busy":"2022-02-04T14:52:51.047653Z","iopub.execute_input":"2022-02-04T14:52:51.047905Z","iopub.status.idle":"2022-02-04T14:52:51.058354Z","shell.execute_reply.started":"2022-02-04T14:52:51.047876Z","shell.execute_reply":"2022-02-04T14:52:51.057613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rain Lags Selection","metadata":{}},{"cell_type":"markdown","source":"Earlier we've began with these rain lags:","metadata":{}},{"cell_type":"code","source":"al_rain = AddLags(predict_time_features=False,\n                 cols=rain_cols, \n                 step=5, \n                 lag_nums=[1,2,3,4,5,6,9,12], \n                 mode='avg'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:52:51.059708Z","iopub.execute_input":"2022-02-04T14:52:51.060582Z","iopub.status.idle":"2022-02-04T14:52:51.073075Z","shell.execute_reply.started":"2022-02-04T14:52:51.060532Z","shell.execute_reply":"2022-02-04T14:52:51.072127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make a pipeline for rain lags selection and then select lags with grid search.\n\nWe will use our selected features plus we will try to add new different rain lags.","metadata":{}},{"cell_type":"code","source":"ovm = OwnValueModel(SVR(C=50, gamma=0.0005), \n                    step=10, \n                    lag_nums=[1,2,3,4,5], \n                    mode='avg', \n                    min_periods=None\n                   )","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:52:51.074448Z","iopub.execute_input":"2022-02-04T14:52:51.075335Z","iopub.status.idle":"2022-02-04T14:52:51.085336Z","shell.execute_reply.started":"2022-02-04T14:52:51.075283Z","shell.execute_reply":"2022-02-04T14:52:51.084435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(rain_cols + selected_features)),\n    ('AddRainLags', al_rain),   \n    ('SelectFeatures2', SelectColsFromN(len(rain_cols))),\n    ('OVM', ovm)\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:52:51.086981Z","iopub.execute_input":"2022-02-04T14:52:51.087323Z","iopub.status.idle":"2022-02-04T14:52:51.095291Z","shell.execute_reply.started":"2022-02-04T14:52:51.087276Z","shell.execute_reply":"2022-02-04T14:52:51.094329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The cell is commented, because I have not been able to remove the output of thousands of messages Performance Warning: \n# Data Frame is highly fragmented\n# These messages are not displayed on the local computer, although the same versions of pandas and sklearn are installed\n# If you just run fit and predict for pipeline, then there are no warnings. \n# There are warnings only if you use grid search for this pipeline\n\n# param = [\n#     {'AddRainLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110,200,300],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110,200,300],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110,200,300],\n#                               [1,2,3,4,5,6,7,8,9,10,11,15,20,50,80,120,150,200,250,300,320]\n#                              ],\n#      'AddRainLags__step':[1],\n#      'AddRainLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddRainLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110]\n#                              ],\n#      'AddRainLags__step':[2],\n#      'AddRainLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddRainLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,3,4,7,9,11,20,40],\n#                               [2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,4,5,9,11,15,40]\n#                              ],\n#      'AddRainLags__step':[6,7,8],\n#      'AddRainLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddRainLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,30],\n#                               [1,2,4,5,9,11,20,30],\n#                               [1,2,3,5,7,11,15,30]\n#                              ],\n#      'AddRainLags__step':[10],\n#      'AddRainLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },    \n#     {'AddRainLags__lag_nums':[[1,2,3,4,5,6,7,8,9,11,15,20],\n#                               [1,2,4,5,7,8,11,15,20],\n#                               [1,2,3,5,6,8,9,15,20]\n#                              ],\n#      'AddRainLags__step':[11],\n#      'AddRainLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     }\n   \n# ]\n\n# df = Ausert_pr.copy()\n\n# grid = RandomizedSearchCV(pipe, \n#                         param, \n#                         cv=tscv, \n#                         scoring='neg_root_mean_squared_error',\n#                         n_jobs=-1,\n#                         n_iter=2,\n#                         random_state=14)\n# grid.fit(df, y=y_smoothed)\n# grid.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:52:51.097233Z","iopub.execute_input":"2022-02-04T14:52:51.097955Z","iopub.status.idle":"2022-02-04T14:53:19.284481Z","shell.execute_reply.started":"2022-02-04T14:52:51.097902Z","shell.execute_reply":"2022-02-04T14:53:19.283622Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.best_params_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've achieved RMSE = 0.1262  \nWith parameters for AddRainLags:  \nstep: 10  \nmode: 'avg'  \nlag_nums: [1, 2, 3, 4, 5, 7, 9, 11, 15, 20, 30]\n\nRMSE increased a little bit after adding new rain lags.  \nThat's okay, we hope that after second features selection RMSE will be better that our current best value (0.1129).\n\nLet's update *al_rain* and use best lags combination.  \nNote that we've deleted duplicated lags numbers, because the same lags have been already added for rain.","metadata":{}},{"cell_type":"code","source":"al_rain = AddLags(predict_time_features=False,\n                 cols=rain_cols, \n                 step=10, \n                 lag_nums=[4, 5, 7, 9, 11, 15, 20, 30], \n                 mode='avg'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:39:51.274453Z","iopub.execute_input":"2022-01-31T18:39:51.275416Z","iopub.status.idle":"2022-01-31T18:39:51.284244Z","shell.execute_reply.started":"2022-01-31T18:39:51.275366Z","shell.execute_reply":"2022-01-31T18:39:51.283318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Temperature Lags Selection","metadata":{}},{"cell_type":"code","source":"al_temper = AddLags(predict_time_features=False,\n                   cols=temper_cols, \n                   step=5, \n                   lag_nums=[1,2,3,4,5,6,9,12], \n                   mode='avg'\n                )                                      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(rain_cols + temper_cols + selected_features)),\n    ('AddRainLags', al_rain),  \n    ('AddTempLags', al_temper), \n    ('SelectFeatures2', SelectColsFromN(len(rain_cols + temper_cols))),\n    ('OVM', ovm)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The cell is commented, because I have not been able to remove the output of thousands of messages Performance Warning: \n# Data Frame is highly fragmented\n# These messages are not displayed on the local computer, although the same versions of pandas and sklearn are installed\n# If you just run fit and predict for pipeline, then there are no warnings. \n# There are warnings only if you use grid search for this pipeline\n\n# param = [\n#     {'AddTempLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110,200,300],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110,200,300],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110,200,300],\n#                               [1,2,3,4,5,6,7,8,9,10,11,15,20,50,80,120,150,200,250,300,320]\n#                              ],\n#      'AddTempLags__step':[1],\n#      'AddTempLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddTempLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110]\n#                              ],\n#      'AddTempLags__step':[2],\n#      'AddTempLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddTempLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,3,4,7,9,11,20,40],\n#                               [2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,4,5,9,11,15,40]\n#                              ],\n#      'AddTempLags__step':[6,7,8],\n#      'AddTempLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddTempLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,30],\n#                               [1,2,4,5,9,11,20,30],\n#                               [1,2,3,5,7,11,15,30]\n#                              ],\n#      'AddTempLags__step':[10],\n#      'AddTempLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },    \n#     {'AddTempLags__lag_nums':[[1,2,3,4,5,6,7,8,9,11,15,20],\n#                               [1,2,4,5,7,8,11,15,20],\n#                               [1,2,3,5,6,8,9,15,20]\n#                              ],\n#      'AddTempLags__step':[11],\n#      'AddTempLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     }\n   \n# ]\n\n# grid = RandomizedSearchCV(pipe, \n#                         param, \n#                         cv=tscv, \n#                         scoring='neg_root_mean_squared_error',\n#                         n_jobs=-1,\n#                         n_iter=100,\n#                         random_state=14)\n# grid.fit(Ausert_pr, y=y_smoothed)\n# grid.best_score_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.best_params_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've achieved RMSE = 0.1184  \nWith parameters for AddTempLags:  \nstep: 1  \nmode: 'lag'  \nlag_nums: [1, 5, 110, 200, 300]","metadata":{}},{"cell_type":"code","source":"al_temper = AddLags(predict_time_features=False,\n                   cols=temper_cols, \n                   step=1, \n                   lag_nums=[1, 5, 110, 200, 300], \n                   mode='lag'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:39:51.285492Z","iopub.execute_input":"2022-01-31T18:39:51.285855Z","iopub.status.idle":"2022-01-31T18:39:51.297229Z","shell.execute_reply.started":"2022-01-31T18:39:51.285827Z","shell.execute_reply":"2022-01-31T18:39:51.29625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Volume Lags Selection","metadata":{}},{"cell_type":"code","source":"al_vol = AddLags(predict_time_features=False,\n                 cols=volume_cols_aug, \n                 step=7, \n                 lag_nums=[1,2,3,4,5,6,7,9], \n                 mode='avg'\n                )                                      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(rain_cols + temper_cols + volume_cols_aug+ selected_features)),\n    ('AddRainLags', al_rain),  \n    ('AddTempLags', al_temper), \n    ('AddVolLags', al_vol), \n    ('SelectFeatures2', SelectColsFromN(len(rain_cols + temper_cols + volume_cols_aug))),\n    ('OVM', ovm)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The cell is commented, because I have not been able to remove the output of thousands of messages Performance Warning: \n# Data Frame is highly fragmented\n# These messages are not displayed on the local computer, although the same versions of pandas and sklearn are installed\n# If you just run fit and predict for pipeline, then there are no warnings. \n# There are warnings only if you use grid search for this pipeline\n\n# param = [\n#     {'AddVolLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110,200,300],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110,200,300],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110,200,300],\n#                               [1,2,3,4,5,6,7,8,9,10,11,15,20,50,80,120,150,200,250,300,320]\n#                              ],\n#      'AddVolLags__step':[1],\n#      'AddVolLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddVolLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110]\n#                              ],\n#      'AddVolLags__step':[2],\n#      'AddVolLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddVolLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,3,4,7,9,11,20,40],\n#                               [2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,4,5,9,11,15,40]\n#                              ],\n#      'AddVolLags__step':[5,6,8],\n#      'AddVolLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddVolLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,30],\n#                               [1,2,4,5,9,11,20,30],\n#                               [1,2,3,5,7,11,15,30]\n#                              ],\n#      'AddVolLags__step':[10],\n#      'AddVolLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },    \n#     {'AddVolLags__lag_nums':[[1,2,3,4,5,6,7,8,9,11,15,20],\n#                               [1,2,4,5,7,8,11,15,20],\n#                               [1,2,3,5,6,8,9,15,20]\n#                              ],\n#      'AddVolLags__step':[11],\n#      'AddVolLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     }\n   \n# ]\n\n# grid = RandomizedSearchCV(pipe, \n#                         param, \n#                         cv=tscv, \n#                         scoring='neg_root_mean_squared_error',\n#                         n_jobs=-1,\n#                         n_iter=100,\n#                         random_state=14)\n# grid.fit(Ausert_pr, y=y_smoothed)\n# grid.best_score_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.best_params_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've achieved RMSE = 0.1303  \nWith parameters for AddVolLag:  \nstep: 2  \nmode: 'avg'  \nlag_nums: [1, 5, 110]","metadata":{}},{"cell_type":"code","source":"al_vol = AddLags(predict_time_features=False,\n                 cols=volume_cols_aug, \n                 step=2, \n                 lag_nums=[1,5,110], \n                 mode='avg'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:39:51.298301Z","iopub.execute_input":"2022-01-31T18:39:51.298505Z","iopub.status.idle":"2022-01-31T18:39:51.309083Z","shell.execute_reply.started":"2022-01-31T18:39:51.298482Z","shell.execute_reply":"2022-01-31T18:39:51.308238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Selection 2","metadata":{}},{"cell_type":"markdown","source":"After we added new rain, temperature and volume lags, RMSE increased from 0.1128 to 0.1303.  \nLets try to use *find_best_cols_comb* for the second time, and see, if RMSE is improved.\n\nFor start combination we will use *selected_features* (achieved before adding new lags).","metadata":{}},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(rain_cols + temper_cols + volume_cols_aug + selected_features)),\n    ('AddRainLags', al_rain),  \n    ('AddTempLags', al_temper), \n    ('AddVolLags', al_vol), \n    ('SelectFeatures2', SelectColsFromN(len(rain_cols + temper_cols + volume_cols_aug)))\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:42:35.548752Z","iopub.execute_input":"2022-01-31T18:42:35.549073Z","iopub.status.idle":"2022-01-31T18:42:35.554605Z","shell.execute_reply.started":"2022-01-31T18:42:35.549034Z","shell.execute_reply":"2022-01-31T18:42:35.55378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert_pr2 = pipe.fit_transform(Ausert_pr)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:42:39.832509Z","iopub.execute_input":"2022-01-31T18:42:39.833055Z","iopub.status.idle":"2022-01-31T18:42:39.941125Z","shell.execute_reply.started":"2022-01-31T18:42:39.833007Z","shell.execute_reply":"2022-01-31T18:42:39.940389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cols = find_best_cols_comb(X=Ausert_pr2,\n                               y=y_smoothed,\n                               model=ovm,\n                               cols=Ausert_pr2.columns.tolist(),\n                               start_comb=Ausert_pr2.columns.tolist(),\n                               cv=tscv\n                              )","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features2 = find_best_cols_comb(X=Ausert_pr2,\n                                         y=y_smoothed,\n                                         model=ovm,\n                                         cols=Ausert_pr2.columns.tolist(),\n                                         start_comb=new_cols,\n                                         cv=tscv\n                                        )","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE improved to 0.0909!","metadata":{}},{"cell_type":"markdown","source":"Let's update our *get_pipe* function:","metadata":{}},{"cell_type":"markdown","source":"### Pipeline without Hydrometry","metadata":{}},{"cell_type":"code","source":"def get_pipe(predict_rain, predict_temp, predict_vol):\n    pipe = Pipeline([\n        ('AddFeatures', AddFeatures()),\n        ('AddRainLags', AddLags(predict_time_features=predict_rain, \n                                cols=rain_cols, \n                                step=5, \n                                lag_nums=[1,2,3,4,5,6,9,12], \n                                mode='avg'\n                                )\n        ),\n        ('AddRainLags2', AddLags(predict_time_features=predict_rain,\n                                 cols=rain_cols, \n                                 step=10, \n                                 lag_nums=[4,5,7,9,11,15,20,30], \n                                 mode='avg'\n                                ) \n        ), \n        ('AddTempLags', AddLags(predict_time_features=predict_temp, \n                                cols=temper_cols, \n                                step=5, \n                                lag_nums=[1,2,3,4,5,6,9,12], \n                                mode='avg'\n                                )\n        ), \n        ('AddTempLags2', AddLags(predict_time_features=predict_temp,\n                                 cols=temper_cols, \n                                 step=1, \n                                 lag_nums=[1,5,110,200,300], \n                                 mode='lag'\n                                 ) \n        ),\n        ('AddVolLags', AddLags(predict_time_features=predict_vol, \n                               cols=volume_cols_aug, \n                               step=7, \n                               lag_nums=[1,2,3,4,5,6,7,9], \n                               mode='avg'                                 \n                               )\n        ), \n        ('AddVolLags2', AddLags(predict_time_features=predict_vol,\n                                cols=volume_cols_aug, \n                                step=2, \n                                lag_nums=[1,5,110], \n                                mode='avg'\n                               ) \n        ) \n        ])\n    return pipe","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-01-31T18:42:45.304143Z","iopub.execute_input":"2022-01-31T18:42:45.304419Z","iopub.status.idle":"2022-01-31T18:42:45.315339Z","shell.execute_reply.started":"2022-01-31T18:42:45.304389Z","shell.execute_reply":"2022-01-31T18:42:45.314441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_no_pred = get_pipe(predict_rain=False, \n                                predict_temp=False, \n                                predict_vol=False\n                               )","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:42:48.121979Z","iopub.execute_input":"2022-01-31T18:42:48.122906Z","iopub.status.idle":"2022-01-31T18:42:48.126672Z","shell.execute_reply.started":"2022-01-31T18:42:48.122872Z","shell.execute_reply":"2022-01-31T18:42:48.125979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's update *Ausert_pr*:","metadata":{}},{"cell_type":"code","source":"Ausert_pr = pipe_prepare_no_pred.fit_transform(Ausert_sc)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:42:49.992205Z","iopub.execute_input":"2022-01-31T18:42:49.99251Z","iopub.status.idle":"2022-01-31T18:42:50.244709Z","shell.execute_reply.started":"2022-01-31T18:42:49.992464Z","shell.execute_reply":"2022-01-31T18:42:50.243651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction plot:","metadata":{}},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(ovm, Ausert_pr, y=y_smoothed, \n                                    cols=selected_features2, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' - SVM - selected features prediction',\n                                    dropNA=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hydrometry adding","metadata":{}},{"cell_type":"markdown","source":"Now let's add hydrometry, and see, if RMSE improves.","metadata":{}},{"cell_type":"code","source":"al_hydro = AddLags(predict_time_features=False,\n                 cols=hydro_cols, \n                 step=7, \n                 lag_nums=[1,2,3,4,5,6,7,9], \n                 mode='avg'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:42:58.069726Z","iopub.execute_input":"2022-01-31T18:42:58.069982Z","iopub.status.idle":"2022-01-31T18:42:58.075451Z","shell.execute_reply.started":"2022-01-31T18:42:58.069954Z","shell.execute_reply":"2022-01-31T18:42:58.074331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(hydro_cols + selected_features2)),\n    ('AddHydroLags', al_hydro),  \n    ('SelectFeatures2', SelectColsFromN(len(hydro_cols))),\n    ('OVM', ovm)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The cell is commented, because I have not been able to remove the output of thousands of messages Performance Warning: \n# Data Frame is highly fragmented\n# These messages are not displayed on the local computer, although the same versions of pandas and sklearn are installed\n# If you just run fit and predict for pipeline, then there are no warnings. \n# There are warnings only if you use grid search for this pipeline\n\n# param = [\n#     {'AddHydroLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110,200,300],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110,200,300],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110,200,300],\n#                               [1,2,3,4,5,6,7,8,9,10,11,15,20,50,80,120,150,200,250,300,320]\n#                              ],\n#      'AddHydroLags__step':[1],\n#      'AddHydroLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddHydroLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,2,3,4,5,7,11,15,20,70,90,110],\n#                               [2,3,4,5,7,9,11,15,20,40,70,90,110],\n#                               [1,3,4,5,9,11,20,40,90,110],\n#                               [1,2,4,5,9,11,20,40,70,110],\n#                               [1,5,110]\n#                              ],\n#      'AddHydroLags__step':[2],\n#      'AddHydroLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddHydroLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,3,4,7,9,11,20,40],\n#                               [2,3,4,5,7,9,11,15,20,40],\n#                               [1,2,4,5,9,11,15,40]\n#                              ],\n#      'AddHydroLags__step':[5,6,8],\n#      'AddHydroLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },\n#     {'AddHydroLags__lag_nums':[[1,2,3,4,5,7,9,11,15,20,30],\n#                               [1,2,4,5,9,11,20,30],\n#                               [1,2,3,5,7,11,15,30]\n#                              ],\n#      'AddHydroLags__step':[10],\n#      'AddHydroLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     },    \n#     {'AddHydroLags__lag_nums':[[1,2,3,4,5,6,7,8,9,11,15,20],\n#                               [1,2,4,5,7,8,11,15,20],\n#                               [1,2,3,5,6,8,9,15,20]\n#                              ],\n#      'AddHydroLags__step':[11,12,13,14,16,20],\n#      'AddHydroLags__mode':['avg', 'lag', 'lag and avg', 'all']\n#     }\n   \n# ]\n\n# grid = RandomizedSearchCV(pipe, \n#                         param, \n#                         cv=tscv, \n#                         scoring='neg_root_mean_squared_error',\n#                         n_jobs=-1,\n#                         n_iter=100,\n#                         random_state=14)\n# grid.fit(Ausert_pr, y=y_smoothed)\n# grid.best_score_","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.best_params_","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've achieved RMSE = 0.0969  \nWith parameters for AddHydroLags:  \nstep: 2  \nmode: 'lag'  \nlag_nums: [1, 5, 110]","metadata":{}},{"cell_type":"code","source":"al_hydro = AddLags(predict_time_features=False,\n                 cols=hydro_cols, \n                 step=2, \n                 lag_nums=[1,5,110], \n                 mode='lag'\n                )                                      ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:43:01.856573Z","iopub.execute_input":"2022-01-31T18:43:01.856813Z","iopub.status.idle":"2022-01-31T18:43:01.861835Z","shell.execute_reply.started":"2022-01-31T18:43:01.856787Z","shell.execute_reply":"2022-01-31T18:43:01.860901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now try to delete unimportant features using pipeline with hydrometry.","metadata":{}},{"cell_type":"code","source":"pipe = Pipeline([\n    ('SelectFeatures', SelectFeatures(hydro_cols + selected_features2)),\n    ('AddHydroLags', al_hydro),  \n    ('SelectFeatures2', SelectColsFromN(len(hydro_cols)))\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:43:49.828027Z","iopub.execute_input":"2022-01-31T18:43:49.828624Z","iopub.status.idle":"2022-01-31T18:43:49.833535Z","shell.execute_reply.started":"2022-01-31T18:43:49.828586Z","shell.execute_reply":"2022-01-31T18:43:49.832361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert_pr_h = pipe.fit_transform(Ausert_pr)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:43:51.492952Z","iopub.execute_input":"2022-01-31T18:43:51.493209Z","iopub.status.idle":"2022-01-31T18:43:51.505242Z","shell.execute_reply.started":"2022-01-31T18:43:51.49318Z","shell.execute_reply":"2022-01-31T18:43:51.504671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cols = find_best_cols_comb(X=Ausert_pr_h,\n                               y=y_smoothed,\n                               model=ovm,\n                               cols=Ausert_pr_h.columns.tolist(),\n                               start_comb=Ausert_pr_h.columns.tolist(),\n                               cv=tscv\n                              )","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features_h = find_best_cols_comb(X=Ausert_pr_h,\n                                          y=y_smoothed,\n                                          model=ovm,\n                                          cols=Ausert_pr_h.columns.tolist(),\n                                          start_comb=new_cols,\n                                          cv=tscv\n                                         )","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After deleting unimportant features RMSE improved to 0.0829. It's a good result.\n\nSo we can see that after adding hydrometry RMSE improves.\n\nBy question is: will RMSE with hydrometry be better when we predict depth to groundwater on predicted hydrometry data? We'll see.","metadata":{}},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(ovm, Ausert_pr_h, y=y_smoothed, \n                                    cols=selected_features_h, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' - SVM - selected features wiht hydro prediction',\n                                    dropNA=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline with Hydrometry","metadata":{}},{"cell_type":"code","source":"def get_pipe_with_hydro(predict_rain, predict_temp, predict_vol, predict_hydro):\n    pipe = get_pipe(predict_rain, predict_temp, predict_vol)\n    pipe.steps.append(['AddHydroLags', AddLags(predict_time_features=predict_hydro,\n                                               cols=hydro_cols, \n                                               step=2, \n                                               lag_nums=[1,5,110], \n                                               mode='lag'\n                                              )  \n                      ])\n    return pipe","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2022-01-31T18:43:57.335423Z","iopub.execute_input":"2022-01-31T18:43:57.335795Z","iopub.status.idle":"2022-01-31T18:43:57.341816Z","shell.execute_reply.started":"2022-01-31T18:43:57.335758Z","shell.execute_reply":"2022-01-31T18:43:57.341048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filling Nulls of Depth to Groundwater SAL","metadata":{}},{"cell_type":"markdown","source":"Now, as we've chosen the model for predicting Depth_to_Groundwater_SAL, let's fill it's null value second time (earlier we've filled nulls with linear interpolation).\n\nLet's print nulls periods:","metadata":{}},{"cell_type":"code","source":"cols = rain_cols + temper_cols + volume_cols + [col_pred] + hydro_cols","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:43:59.528522Z","iopub.execute_input":"2022-01-31T18:43:59.529562Z","iopub.status.idle":"2022-01-31T18:43:59.533828Z","shell.execute_reply.started":"2022-01-31T18:43:59.529522Z","shell.execute_reply":"2022-01-31T18:43:59.532984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser[cols].T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(11,6))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=True)\nfig.suptitle('Blues are nulls periods', fontsize=18);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-01-31T18:43:59.913424Z","iopub.execute_input":"2022-01-31T18:43:59.913908Z","iopub.status.idle":"2022-01-31T18:44:00.631761Z","shell.execute_reply.started":"2022-01-31T18:43:59.913874Z","shell.execute_reply":"2022-01-31T18:44:00.631031Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will try to use as train set not only data from 2011, but earlier dates too.\n\nBut we won't use data till April 2007, because Depth_to_Groundwater_SAL is empty at that dates.  \nSo we won't fill the first period (since 2006 till the beginning of 2007).\n\nAlso we won't fill nulls in Hydrometry_Piaggione yet.\n\nWe will fill nulls in Depth_to_Groundwater_SAL based on model without hydrometry, because Hydrometry_Piaggione has null in dates, where filling of Depth_to_Groundwater_SAL is needed.","metadata":{}},{"cell_type":"code","source":"old = Auser.copy()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:00.63305Z","iopub.execute_input":"2022-01-31T18:44:00.633236Z","iopub.status.idle":"2022-01-31T18:44:00.63705Z","shell.execute_reply.started":"2022-01-31T18:44:00.633213Z","shell.execute_reply":"2022-01-31T18:44:00.636143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Short periods (few days long) we will fill with linear interpolation, as earlier:","metadata":{}},{"cell_type":"code","source":"Auser.loc['2009-08-11':'2016-09-03', col_pred] = Auser.loc['2009-08-11':'2016-09-03', col_pred].interpolate()\nAuser.loc['2016-11-02':'2019-04-18', col_pred] = Auser.loc['2016-11-02':'2019-04-18', col_pred].interpolate()\nAuser.loc['2020-06-04':, col_pred] = Auser.loc['2020-06-04':, col_pred].interpolate()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:01.264686Z","iopub.execute_input":"2022-01-31T18:44:01.265063Z","iopub.status.idle":"2022-01-31T18:44:01.28114Z","shell.execute_reply.started":"2022-01-31T18:44:01.265027Z","shell.execute_reply":"2022-01-31T18:44:01.280312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print null periods of Depth_to_Groundwater_SAL once more.\n\nNow we can see, that short periods already have been filled:","metadata":{}},{"cell_type":"code","source":"# Diagram of nulls periods\n\n# Transpose\ndf = Auser[[col_pred]].T.isna()\n# Change type of index for xticks lables formatting\ndf.columns = df.columns.astype('str')\nfig, ax = plt.subplots(figsize=(11,0.2))\nsns.heatmap(df, cmap='Blues', cbar=False, xticklabels = 365, yticklabels=False);","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-01-31T18:44:02.222769Z","iopub.execute_input":"2022-01-31T18:44:02.223824Z","iopub.status.idle":"2022-01-31T18:44:02.403222Z","shell.execute_reply.started":"2022-01-31T18:44:02.223783Z","shell.execute_reply":"2022-01-31T18:44:02.402316Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is pipeline without hydrometry.  \nNote, that we need to add scale transformer as first step:","metadata":{}},{"cell_type":"code","source":"pipe_prepare_pred_sc = get_pipe(predict_rain=False, \n                             predict_temp=False, \n                             predict_vol=False\n                            )\npipe_prepare_pred_sc.steps.append(['SelectFeatures', SelectFeatures(selected_features2)])\npipe_prepare_pred_sc.steps.append(['OVM', ovm])\npipe_prepare_pred_sc.steps.insert(0, ['Scale', ScaleDF()])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:03.930696Z","iopub.execute_input":"2022-01-31T18:44:03.931121Z","iopub.status.idle":"2022-01-31T18:44:03.935253Z","shell.execute_reply.started":"2022-01-31T18:44:03.931074Z","shell.execute_reply":"2022-01-31T18:44:03.934798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with warnings.catch_warnings():  # this section is here for ignoring SettingWithCopyWarning\n    warnings.simplefilter(\"ignore\")\n\n    replace_nulls_with_prediction(Auser.iloc[460:1318], col_pred, \n                                  cols_features=rain_cols + temper_cols + volume_cols, \n                                  model=pipe_prepare_pred_sc)\n\n    replace_nulls_with_prediction(Auser.iloc[460:3958], col_pred, \n                                  cols_features=rain_cols + temper_cols + volume_cols, \n                                  model=pipe_prepare_pred_sc)\n\n    replace_nulls_with_prediction(Auser.iloc[460:], col_pred, \n                                  cols_features=rain_cols + temper_cols + volume_cols, \n                                  model=pipe_prepare_pred_sc)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:05.250429Z","iopub.execute_input":"2022-01-31T18:44:05.250812Z","iopub.status.idle":"2022-01-31T18:44:08.670183Z","shell.execute_reply.started":"2022-01-31T18:44:05.250769Z","shell.execute_reply":"2022-01-31T18:44:08.669213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot prediction of null periods\nfig, ax = plt.subplots(figsize=(15, 4))\nax.plot(Auser.index, Auser[col_pred], label='predicted', c='xkcd:cherry')  \nax.plot(old.index, old[col_pred], label='real', c='xkcd:true blue') \nax.legend()\nax.set_title(col_pred + ' with filled nulls', fontdict={'fontsize':16});  ","metadata":{"code_folding":[0],"execution":{"iopub.status.busy":"2022-01-31T18:44:08.672093Z","iopub.execute_input":"2022-01-31T18:44:08.672389Z","iopub.status.idle":"2022-01-31T18:44:08.965815Z","shell.execute_reply.started":"2022-01-31T18:44:08.672353Z","shell.execute_reply":"2022-01-31T18:44:08.964476Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Set Selection","metadata":{}},{"cell_type":"markdown","source":"Heretofore we use only data from 2011 as traing set.\n\nNow let's check RMSE for other training sets. We'll check also training set from 2007 and 2012.  \nMeanwhile cross-validation splits will stay the same.\n\nWe will use pipeline without hydrometry, because Hydrometry_Piaggione has null in data before 2011.","metadata":{}},{"cell_type":"code","source":"Auser2007 = Auser.loc['2007-04-06':].copy()\nAuser2011 = Auser.loc['2011-01-01':].copy()\nAuser2012 = Auser.loc['2012-01-01':].copy()\n\nAusert7 = Auser2007.iloc[:-365].copy()\nAusert11 = Auser2011.iloc[:-365].copy()\nAusert12 = Auser2012.iloc[:-365].copy()\n\nsdf = ScaleDF()\nAusert7_sc = sdf.fit_transform(Ausert7)\nAusert11_sc = sdf.fit_transform(Ausert11)\nAusert12_sc = sdf.fit_transform(Ausert12)\n\ny_smoothed7 = Ausert7[col_pred].rolling(7, center=True, min_periods=1).mean()\ny_smoothed11 = Ausert11[col_pred].rolling(7, center=True, min_periods=1).mean()\ny_smoothed12 = Ausert12[col_pred].rolling(7, center=True, min_periods=1).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:18.115789Z","iopub.execute_input":"2022-01-31T18:44:18.116976Z","iopub.status.idle":"2022-01-31T18:44:18.144552Z","shell.execute_reply.started":"2022-01-31T18:44:18.116917Z","shell.execute_reply":"2022-01-31T18:44:18.143537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe(predict_rain=False, \n                             predict_temp=False, \n                             predict_vol=False\n                            )\npipe_prepare_pred.steps.append(['SelectFeatures', SelectFeatures(selected_features2)])\npipe_prepare_pred.steps.append(['OVM', ovm])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:20.585326Z","iopub.execute_input":"2022-01-31T18:44:20.586143Z","iopub.status.idle":"2022-01-31T18:44:20.591712Z","shell.execute_reply.started":"2022-01-31T18:44:20.586107Z","shell.execute_reply":"2022-01-31T18:44:20.590717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert7_sc, \n                                    y=y_smoothed7, \n                                    cols=Ausert.columns, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' prediction - train set from 2007' ,\n                                    dropNA=False)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert11_sc, \n                                    y=y_smoothed11, \n                                    cols=Ausert.columns, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' prediction - train set from 2011' ,\n                                    dropNA=False)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert12_sc, \n                                    y=y_smoothed12, \n                                    cols=Ausert.columns, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred + ' prediction - train set from 2012' ,\n                                    dropNA=False)","metadata":{"scrolled":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that train set starting from 2011 shows the best result.\n\nSo will use train set starting from 2011.\n\nThere are few nulls left in Hydrometry_Piaggione:","metadata":{}},{"cell_type":"code","source":"Auser2011['Hydrometry_Piaggione'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:31.313324Z","iopub.execute_input":"2022-01-31T18:44:31.313974Z","iopub.status.idle":"2022-01-31T18:44:31.320854Z","shell.execute_reply.started":"2022-01-31T18:44:31.313938Z","shell.execute_reply":"2022-01-31T18:44:31.319755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's interpolate them lineary:","metadata":{}},{"cell_type":"code","source":"Auser2011['Hydrometry_Piaggione'].interpolate(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:35.297786Z","iopub.execute_input":"2022-01-31T18:44:35.298612Z","iopub.status.idle":"2022-01-31T18:44:35.303884Z","shell.execute_reply.started":"2022-01-31T18:44:35.298572Z","shell.execute_reply":"2022-01-31T18:44:35.302964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ausert = Auser2011.iloc[:-365].copy() # train data","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:36.481791Z","iopub.execute_input":"2022-01-31T18:44:36.482087Z","iopub.status.idle":"2022-01-31T18:44:36.486698Z","shell.execute_reply.started":"2022-01-31T18:44:36.482052Z","shell.execute_reply":"2022-01-31T18:44:36.486139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdf = ScaleDF()\nAusert_sc = sdf.fit_transform(Ausert)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T18:44:36.79024Z","iopub.execute_input":"2022-01-31T18:44:36.79093Z","iopub.status.idle":"2022-01-31T18:44:36.798079Z","shell.execute_reply.started":"2022-01-31T18:44:36.790899Z","shell.execute_reply":"2022-01-31T18:44:36.797609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on predicted data","metadata":{}},{"cell_type":"markdown","source":"We've completed all preliminary steps: we've chosen the model, features, train set.\n\nNow we need to choose models for predicting rain, temperature, volume, hydrometry.\n\nAs for rain and temprerature, let's leave prediction to the weather stations, which can model the movement of cyclones and anticyclones. We will just take yearly average for rain and temperature.\n\nBelow is model that returns year average.","metadata":{}},{"cell_type":"code","source":"def del_29Feb(df):\n    return df[~((df.index.month == 2) & (df.index.day == 29))]\n\ndef del_53th_weeks(series):\n    \n    # Removes the 53th weeks from the time series with weekly granularity\n    # Index series should be dates with weekly frequency: freq = 'W-SUN' - week ending on Sunday\n    # Before deleting 53 weeks, their value is averaged with the following weeks \n    # (with the first weeks of the year)\n    # The series must have a name other than Date, year. The field must be numeric.\n    \n    df = pd.DataFrame(series, index=series.index, copy=True)\n    df['year'] = df.index.year\n    df['Date'] = df.index\n    \n    # For each row calculates week number\n    week_number = df.groupby('year')['Date'].rank()\n    \n    # Finds indexes of 53th weeks, which should be deleted\n    ix53 = week_number[week_number == 53].index\n    \n    if len(ix53) == 0:\n        return series\n        \n    # For 53th weeks we add 1 day to the Date field. Because we resampled the data \n    # to the end of the week (the whole week was resamplesd to Sunday), then if we add 1 day, \n    # then this date will be taken into account as next week \n    # when resampling to the week frequency\n    df.loc[ix53, 'Date'] = df.loc[ix53, 'Date'] + Day()\n    \n    df.set_index('Date', inplace=True)\n    \n    # Resample to week frequency. So values of 53th weeks will merge (average will be taken) \n    # with 52th weeks. There will be NaNs on 53th weeks.\n    res = df.resample('W-SUN').mean() # неделя группируется в последний день недели - воскресенье\n    \n    # Delete 53th weeks. (with NaNs)\n    res.drop(ix53, axis=0, inplace=True) \n    \n    return res.iloc[:, 0]\n\nclass MeanModel(BaseEstimator):\n        \n    # Predicts the value by its average values for all years.\n    # y is a column with the value to be predicted, index of y should contain dates\n    # For fit X is completely ignored - you can pass anything in X for fit method \n    # For predict, only the length is taken from X - to determine prediction length\n    \n    def __init__(self, slen=365, adjust_first_elements=False, freq='D'):\n        self.slen = slen\n        self.adjust_first_elements = adjust_first_elements\n        self.freq = freq\n    def fit(self, X, y):\n\n        if len(y) < self.slen:\n            print('Передан массив длиной менее длины сезона')\n            return -1        \n        \n        # Save the date, from which prediction will start\n        self.pred_start_date = y.index.max() + Day()       \n        \n        # Save the value of the last element of train set for the case if predicted data\n        # should be adjusted to the last values of train set\n        self.last_train_element = y.iloc[-1]        \n\n        if self.slen == 52:\n            df = del_53th_weeks(y)\n            \n        if self.slen == 365:\n            df = del_29Feb(y)\n        \n        # Save the lenth of the train set\n        self.len_train = len(df)\n\n        # Reseting index because calculations will be done based on index \n        # and it should start from 0\n        df = df.reset_index()\n        \n        # Save column name of the data\n        self.col = df.columns[1]\n\n        # Calculate ordinal number of week in year/day in year - not calendar, \n        # but 52 weeks/365 days beginning from start date\n        df['week_ord'] = df.index % self.slen\n\n        # Calculate average values by week/day taking in account all years in train set\n        self.mean_by_week = df.groupby(['week_ord'])[self.col].mean()\n        \n        return self\n        \n    def predict(self, X):\n        \n        n_preds = len(X)\n        \n        # Create DataFrame with dates for prediction\n        time_pred = pd.DataFrame({'Date': \n                                  pd.date_range(start=self.pred_start_date, \n                                                periods=n_preds, \n                                                freq=self.freq\n                                               )\n                                 })        \n        # Create DataFrame of lenght len(train set) + len(prediction) \n        # and fill it with numbers 0, 1, 2...\n        num = pd.DataFrame(range(self.len_train + n_preds))\n\n        # Calculate ordinal number for week/day for this new DataFrame: 0,...,51,0,...51,...\n        num['week_ord'] = num.index % self.slen\n\n        # Join num and mean_by_week ON n.week_ord = m.index. \n        # In mean_by_week index is unique key (0,...,51)\n        num = num.merge(self.mean_by_week, left_on='week_ord', right_index=True).sort_index()\n\n        # Join prediction to result DataFrame.\n        # The last n_preds elements - is prediction\n        time_pred[self.col] =  np.array(num.iloc[-n_preds:, 2])\n\n        if self.adjust_first_elements:\n            p10 = time_pred[self.col].iloc[:10]\n            # Differnence between prediction and last real value\n            delta = self.last_train_element - p10\n            # Generate numbers 1, 0.9, 0.8, .... 0.1, \n            # and then multiplicate the difference by these numbers \n            coef = np.arange(1, 0, -0.1)        \n            delta_coef = delta * coef\n            # Replace first 10 elements in prediction\n            time_pred.loc[:9, self.col] = p10 + delta_coef   \n                \n        time_pred.set_index('Date', inplace=True)\n\n        return time_pred.iloc[:, 0]","metadata":{"code_folding":[0,3,41],"execution":{"iopub.status.busy":"2022-01-31T18:44:44.784127Z","iopub.execute_input":"2022-01-31T18:44:44.784451Z","iopub.status.idle":"2022-01-31T18:44:44.809953Z","shell.execute_reply.started":"2022-01-31T18:44:44.784415Z","shell.execute_reply":"2022-01-31T18:44:44.809185Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Volume Prediction","metadata":{}},{"cell_type":"markdown","source":"We will predict volume as classic time series.  \nWe will assume that volume does not depend on rain and temperature.  \nWe will predict volume depending only on seasonality and it's own previous values.\n\nAt first let's plot Volume_POL.","metadata":{}},{"cell_type":"code","source":"col_pred_vol = 'Volume_POL'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,3))\nax.plot(Ausert[col_pred_vol])\nfig.suptitle(col_pred_vol, fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see some strange thickness of the line.  \nLet's plot some selected period:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,3))\nax.plot(Ausert.loc['2015-01', col_pred_vol], marker='o')\nfig.suptitle(col_pred_vol + ' - January 2015', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the is a spike every 7 days.  \nLet's smoothe the data - calculate the moving average on 7 days - and print the plot again.","metadata":{}},{"cell_type":"code","source":"y_vol = Ausert[col_pred_vol].rolling(7, min_periods=1).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,3))\nax.plot(y_vol)\nfig.suptitle(col_pred_vol, fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will test model on smoothed volume.","metadata":{}},{"cell_type":"markdown","source":"It seems that Volume_POL has an upward trend and a seasonal pattern: at winter time line is usually higher than at summer. (In summer the water treatment facilities take away more water than in winter).\n\nLet's take a first difference.  \nBut because volume changes only from week to week, and not daily (if not taken into account every 7 days spike at unsmoothe volume), we won't take difference of neighboring days, but we will take difference with step = 7 days.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,4))\nax.plot(y_vol.diff(7))\nfig.suptitle(col_pred_vol + ' 1st difference (step = 7 days)', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that if we resample data to weekly frequency, and then take first difference, than results will be almost the same:","metadata":{}},{"cell_type":"code","source":"y_vol_week = y_vol.resample('W-SUN').mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,4))\nax.plot(y_vol_week.diff(1))\nfig.suptitle(col_pred_vol + ' 1st difference after resampling', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's resample data to weekly frequency, then take first difference, and then season difference:","metadata":{}},{"cell_type":"code","source":"y_vol_d = y_vol_week.diff(1).diff(52).dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,4))\nax.plot(y_vol_d)\nfig.suptitle(col_pred_vol + ' 1st and year difference', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Differenced data looks more stationary then data wihtout differencing.\n\nNow let's plot autocorrelations diagrams.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,3))\nplot_acf(y_vol_d, lags=110, ax=ax);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,3))\nplot_pacf(y_vol_d, lags=110, ax=ax);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing these plots, we can suggest that if we try apply ARIMA model, than we should check AR and MA parameters up to 4.\n\nBesides the ARIMA model, we are going to try some other models, such as:\n1. Simple yearly average;\n2. Random Forest;\n3. Extra Trees;\n4. SVM;\n5. OVM (previously defined user-class model);\n6. OVM with previously droped seasonaltiy pattern.\n\nFor all these model we will apply the first difference.\n\nFor ARIMA we will first resample data for weekly frequency.\n\nBelow is sklearn-interface wrapper for stamodels SARIMAX class:","metadata":{}},{"cell_type":"code","source":"class ARIMAModel(BaseEstimator):\n    \n    # Wrapper for statsmodels SARIMAX\n    # y should have date as index\n    # in fit X is completely ignored - you can pass anything in X for fit method\n    # for predict, only the length is taken from X - to determine prediction length  \n    \n    def __init__(self, order, seasonal_order, slen=52, freq='W-SUN'):\n        self.order = order\n        self.seasonal_order = seasonal_order        \n        self.slen = slen\n        self.freq = freq\n    def fit(self, X, y):\n\n        if len(y) < self.slen:\n            print('The data length is less than the season length')\n            return -1\n        \n        # Save the date, from which prediction will start\n        self.pred_start_date = y.index.max() + Day()       \n                \n        # Save train set length\n        self.len_train = len(y)\n                \n        self.model = SARIMAX(y, order=self.order, seasonal_order=self.seasonal_order).fit(low_memory=True)\n        \n        return self\n        \n    def predict(self, X):\n        \n        n_preds = len(X)\n        time_pred = self.model.predict(start = self.len_train, end = self.len_train + n_preds - 1)\n        time_pred.index = pd.date_range(start=self.pred_start_date, periods=n_preds, freq=self.freq)\n                \n        return time_pred","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below are usefull classes for differencing, resampling, season decomposing.\n\nThese classes are actually transformers, not models. But they transform not only y, but X too. And because of this they cannot be implemented as sklearn-transformers.","metadata":{}},{"cell_type":"code","source":"class DeltaModel(BaseEstimator):\n    \n    # Calculates first difference for y\n    # between the current value and the previous value delta_step days ago.\n    # Deletes rows with NaNs if features or calculated difference\n    # Trains the model using difference instead of y.\n    # Predicts difference. \n    # Then calculates the original y prediction based on predicted difference\n    \n    def __init__(self, model=RandomForestRegressor(random_state=7), delta_step=7):\n        self.model = model\n        self.delta_step = delta_step\n    def fit(self, X, y):\n        df = X.copy()\n        df['y'] = y\n        df['delta'] = y - y.shift(self.delta_step)\n\n        # Delete NaNs\n        df.dropna(inplace=True)\n        X_nona = df[X.columns]\n\n        self.model.fit(X_nona, df['delta'])\n        \n        # The last delta_step elements of y of train set\n        self.last_train_val = np.array(y.iloc[-self.delta_step:])\n        \n    def predict(self, X):\n        delta_pred = self.model.predict(X)\n        \n        res = np.zeros((len(X)))\n\n        i = 0\n        while i*self.delta_step <= len(X):\n\n            if i == 0:        \n                sev_delta_curr = np.array(delta_pred[:self.delta_step])\n                # Previous values from train set + current difference values (for delta_step days)\n                res[i*self.delta_step : (i+1)*self.delta_step] = self.last_train_val + sev_delta_curr\n            else:\n                sev_col_sum_prev = np.array(res[(i-1)*self.delta_step : i*self.delta_step])\n                sev_delta_curr = np.array(delta_pred[i*self.delta_step : (i+1)*self.delta_step])\n                if sev_col_sum_prev.shape[0] > sev_delta_curr.shape[0]:\n                    # For the last iteration of the loop, we trim the array\n                    sev_col_sum_prev = sev_col_sum_prev[0:sev_delta_curr.shape[0]]\n                # Previous values from train set + current difference values (for delta_step days)\n                res[i*self.delta_step : (i+1)*self.delta_step] = sev_col_sum_prev + sev_delta_curr\n            i = i + 1\n\n        pred = pd.DataFrame(res)\n\n        pred['roll'] = pred[0].rolling(self.delta_step).mean()\n\n        pred.iloc[:6, 1]  = pred.iloc[:6, 0]\n        \n        return np.array(pred['roll'])","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResampleModel(BaseEstimator):\n\n    # Resamples data to freq frequency (to week frequency, for example)\n    # Trains the model using resampled data\n    # Predicts resampled data\n    # Then resamples predicted data back to day frequency\n\n    def __init__(self, model, freq='W-SUN'):\n        self.model = model\n        self.freq = freq\n        \n    def fit(self, X, y):\n        \n        df = X.copy()\n        df['y'] = y\n\n        # Resampling. If freq='W-SUN', we are grouping week to final week day - Sunday.\n        df_res = df.resample(self.freq).mean()\n        \n        # Delete NaN\n        df_res.dropna(inplace=True)\n        X_nona = df_res[X.columns]\n        \n        self.model.fit(X_nona, df_res['y'])\n        \n        self.y_name = y.name\n        \n        return self\n        \n    def predict(self, X):\n        \n        X_res = X.resample(self.freq).mean()\n        \n        X_res[self.y_name] = self.model.predict(X_res)\n                    \n        # Resample prediction back to day frequency\n        pred_day = X_res[self.y_name].resample('D').bfill() \n        \n        # Leave only those dates, that were in the test set\n        pred_day = pd.merge(X, pred_day, how='left', left_index=True, right_index=True).bfill()\n                \n        return pred_day[self.y_name]","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeasonDecomposeModel(BaseEstimator):\n    \n    # Removes the yearly averages from y, thus removing the seasonal component.\n    # Trains the model using data without a seasonal component.\n    # Predicting data without a seasonal component.\n    # Then adds the yearly average to the predictions\n\n    def __init__(self, model, slen=52, freq='W-SUN'):\n        self.model = model\n        self.slen = slen\n        self.freq = freq  \n        \n    def fit(self, X, y):         \n        \n        if self.slen == 52:\n            df = del_53th_weeks(y)\n            \n        if self.slen == 365:\n            df = del_29Feb(y)\n            \n        # Save train set length\n        self.len_train = len(df)\n        \n        df = df.reset_index()\n        \n        # Column name of y\n        self.col = df.columns[1]\n\n        # Calculate ordinal number of week in year/day in year - not calendar, \n        # but 52 weeks/365 days beginning from start date\n        df['week_ord'] = df.index % self.slen\n\n        # Calculate average values by week/day taking in account all years in train set\n        self.mean_by_week = df.groupby(['week_ord'])[self.col].mean()        \n\n        # Join average values to data\n        df = pd.merge(df, self.mean_by_week, left_on='week_ord', right_index=True).sort_index()\n        df.set_index('Date', inplace=True)\n        year_mean = df.iloc[:, 2]\n        # Return back 29th February/53th week\n        year_mean = year_mean.resample(self.freq).pad()\n        \n        # Substract average values\n        y_dec = y - year_mean\n        \n        # Assing a name for the series - for correct further processing\n        y_dec.name = y.name\n        \n        self.model.fit(X, y_dec)\n        \n    def predict(self, X):\n        \n        pred_dec = self.model.predict(X)      \n        \n        n_preds = len(X)        \n\n        # Create DataFrame of lenght len(train set) + len(prediction) \n        # and fill it with numbers 0, 1, 2...\n        num = pd.DataFrame(range(self.len_train + n_preds))\n\n        # Calculate ordinal number for week/day for this new DataFrame: 0,...,51,0,...51,...\n        num['week_ord'] = num.index % self.slen\n\n        # Join num and mean_by_week ON n.week_ord = m.index. \n        # In mean_by_week index is unique key (0,...,51)        \n        num = num.merge(self.mean_by_week, left_on='week_ord', right_index=True).sort_index()\n\n        # Join prediction to result DataFrame\n        res = np.array(pred_dec) + np.array(num.iloc[-n_preds:, 2])       \n        \n        return res","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For brevity, I will give here only the results of grid search for Volume_POL:\n- OVM with season decompose showed the best results:\n    - RMSE = 655\n    - OVM parameters: model=SVR(C=1, gamma=0.01), step=5, lag_nums=[1,2,3,4,5], mode='all'\n- Simple yearly mean showed RMSE = 705\n\nFor other volumes OVM with season decompose showed acceptable results too.","metadata":{}},{"cell_type":"markdown","source":"Below is shortened example of grid search for Volume_POL.","metadata":{}},{"cell_type":"code","source":"# Leave only week_sin, week_cos in X\naf = AddFeatures()\nAusert_pr_vol = af.fit_transform(Ausert)[['week_sin', 'week_cos']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExecuteModel(BaseEstimator):\n    \n    # Wrapper for using different models as parameters in grid search\n    \n    def __init__(self, model):\n        self.model = model\n        \n    def fit(self, X, y):\n        self.model.fit(X, y)        \n        return self\n        \n    def predict(self, X):        \n        return self.model.predict(X)","metadata":{"code_folding":[0],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"em = ExecuteModel(model)\n\nparam = [\n    {'model':[\n        # Delta - Mean\n        DeltaModel(\n            MeanModel(), \n            delta_step=7\n        ),        \n        # Delta - RF\n        DeltaModel(\n            RandomForestRegressor(random_state=1), \n            delta_step=7\n        ),\n        # Delta - ET\n        DeltaModel(\n            ExtraTreesRegressor(random_state=1), \n            delta_step=7\n        ),\n        # Delta - SVM\n        DeltaModel(\n            SVR(gamma=0.01, C=1), \n            delta_step=7\n        ),\n        \n        \n        ##################### OVM ####################\n        \n        ########## Delta - OVM - SVM ##########\n        DeltaModel(\n                    OwnValueModel(\n                                SVR(gamma=0.01, C=1), \n                                step=40, \n                                lag_nums=[3,4,5,6,7,8], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                SVR(gamma=0.01, C=1), \n                                step=10, \n                                lag_nums=[1,2,3,4,5], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                SVR(gamma=0.01, C=1), \n                                step=5, \n                                lag_nums=[1,2,3,4,5], \n                                mode='all', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        \n        ########## Delta - OVM - RF ##########\n        DeltaModel(\n                    OwnValueModel(\n                                RandomForestRegressor(random_state=1), \n                                step=40, \n                                lag_nums=[3,4,5,6,7,8], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                RandomForestRegressor(random_state=1), \n                                step=10, \n                                lag_nums=[1,2,3,4,5], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                RandomForestRegressor(random_state=1), \n                                step=5, \n                                lag_nums=[1,2,3,4,5], \n                                mode='all', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        \n        ########## Delta - OVM - ET ##########\n        DeltaModel(\n                    OwnValueModel(\n                                ExtraTreesRegressor(random_state=1), \n                                step=40, \n                                lag_nums=[3,4,5,6,7,8], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                ExtraTreesRegressor(random_state=1), \n                                step=10, \n                                lag_nums=[1,2,3,4,5], \n                                mode='avg', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),\n        DeltaModel(\n                    OwnValueModel(\n                                ExtraTreesRegressor(random_state=1), \n                                step=5, \n                                lag_nums=[1,2,3,4,5], \n                                mode='all', \n                                min_periods=None\n                                ), \n                    delta_step=7\n        ),    \n        \n        ##################### Season - OVM ####################\n        \n        ########## Delta - Season - OVM - SVM ##########\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    SVR(gamma=0.01, C=1), \n                                                    step=40, \n                                                    lag_nums=[3,4,5,6,7,8], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),        \n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    SVR(gamma=0.01, C=1), \n                                                    step=10, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    SVR(gamma=0.01, C=1), \n                                                    step=5, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='all', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        \n        ########## Delta - Season - OVM - RF ##########\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    RandomForestRegressor(random_state=1), \n                                                    step=40, \n                                                    lag_nums=[3,4,5,6,7,8], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),        \n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    RandomForestRegressor(random_state=1), \n                                                    step=10, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    RandomForestRegressor(random_state=1), \n                                                    step=5, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='all', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        \n        ########## Delta - Season - OVM - ET ##########\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    ExtraTreesRegressor(random_state=1), \n                                                    step=40, \n                                                    lag_nums=[3,4,5,6,7,8], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),        \n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    ExtraTreesRegressor(random_state=1), \n                                                    step=10, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='avg', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        DeltaModel(\n                   SeasonDecomposeModel(\n                                        OwnValueModel(\n                                                    ExtraTreesRegressor(random_state=1), \n                                                    step=5, \n                                                    lag_nums=[1,2,3,4,5], \n                                                    mode='all', \n                                                    min_periods=None\n                                                    ), \n                                        slen=365,\n                                        freq='D'\n                                       ),\n                    delta_step=7\n        ),\n        \n        ##################### ARIMA ####################\n        \n        ResampleModel(\n                  ARIMAModel(\n                      order=(1, 1, 0), seasonal_order=(1, 1, 0, 52), slen=52, freq='W-SUN')\n        ),\n        ResampleModel(\n                  ARIMAModel(\n                      order=(0, 1, 1), seasonal_order=(1, 1, 0, 52), slen=52, freq='W-SUN')\n        ),\n        ResampleModel(\n                  ARIMAModel(\n                      order=(1, 1, 1), seasonal_order=(1, 1, 0, 52), slen=52, freq='W-SUN')\n        )        \n             \n             ]}\n]","metadata":{"code_folding":[2],"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(em, \n                    param, \n                    cv=tscv, \n                    scoring='neg_root_mean_squared_error',\n                    n_jobs=-1)\ngrid.fit(Ausert_pr_vol, y=y_vol)","metadata":{"code_folding":[],"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_grid_results(grid)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = grid.best_estimator_\nscores, pred = plot_pred_all_splits(ExecuteModel(model), Ausert_pr_vol, \n                                    y=y_vol, \n                                    cols=Ausert_pr_vol.columns, cv=tscv, mark_pred_start=True,\n                                    plot_title=col_pred_vol + ' - Delta - Season - OVM - SVM prediction' ,\n                                    dropNA=False)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we've chosen model for volume prediction:","metadata":{}},{"cell_type":"code","source":"volume_model = DeltaModel(\n                          SeasonDecomposeModel(                                               \n                                               OwnValueModel(\n                                                             lag_nums=[1,2,3,4,5],\n                                                             min_periods=None,\n                                                             model=SVR(C=1,gamma=0.01),\n                                                             step=5\n                                                             ),\n                                               slen=365,                 \n                                               freq='D'\n                                             )\n                         )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Value Prediction","metadata":{}},{"cell_type":"markdown","source":"Let's update *get_pipe* function - we will add models for features prediction:","metadata":{}},{"cell_type":"code","source":"def get_pipe(predict_rain, predict_temp, predict_vol):\n    pipe = Pipeline([\n        ('AddFeatures', AddFeatures()),\n        ('AddRainLags', AddLags(predict_time_features=predict_rain, \n                                cols=rain_cols, \n                                step=5, \n                                lag_nums=[1,2,3,4,5,6,9,12], \n                                mode='avg',\n                                model=MeanModel(adjust_first_elements=False),\n                                features=['week'] # no matter which field to specify here,\n                                                  # it only shouldn't have nulls\n                                )\n        ),\n        ('AddTempLags', AddLags(predict_time_features=predict_temp, \n                                cols=temper_cols, \n                                step=5, \n                                lag_nums=[1,2,3,4,5,6,9,12], \n                                mode='avg',\n                                model=MeanModel(adjust_first_elements=True),\n                                features=['week'] # no matter which field to specify here\n                                                  # it only shouldn't have nulls\n                                )\n        ),\n        ('AddVolLags', AddLags(predict_time_features=predict_vol, \n                               cols=volume_cols_aug, \n                               step=7, \n                               lag_nums=[1,2,3,4,5,6,7,9], \n                               mode='avg',\n                               model=volume_model,\n                               features=['week_sin', 'week_cos']\n                               )\n        ),\n        ('AddRainLags2', AddLags(predict_time_features=predict_rain,\n                                 cols=rain_cols, \n                                 step=10, \n                                 lag_nums=[      4,5,7,9,11,15,20,30],\n                                 mode='avg',\n                                 model=MeanModel(adjust_first_elements=False),\n                                 features=['week'] # no matter which field to specify here,\n                                                   # it only shouldn't have nulls\n                                ) \n        ),  \n        ('AddTempLags2', AddLags(predict_time_features=predict_temp,\n                                 cols=temper_cols, \n                                 step=1, \n                                 lag_nums=[1,5,110,200,300], \n                                 mode='lag',\n                                 model=MeanModel(adjust_first_elements=True),\n                                 features=['week'] # no matter which field to specify here\n                                                   # it only shouldn't have nulls\n                                 ) \n        ), \n        ('AddVolLags2', AddLags(predict_time_features=predict_vol,\n                                cols=volume_cols_aug, \n                                step=2, \n                                lag_nums=[1,5,110], \n                                mode='avg',\n                                model=volume_model,\n                                features=['week_sin', 'week_cos']\n                               ) \n        ) \n        ])\n    return pipe","metadata":{"code_folding":[13,42,52]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pipe_pred(predict_rain, predict_temp, predict_vol):\n    pipe = get_pipe(predict_rain, predict_temp, predict_vol)\n    pipe.steps.append(['SelectFeatures', SelectFeatures(selected_features2)])\n    pipe.steps.append(['OVM', ovm])    \n    return pipe","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate whole prediction pipeline, icluding both feature prediction and target value prediction.","metadata":{}},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred(predict_rain=True, \n                                  predict_temp=True, \n                                  predict_vol=True\n                                 )\nplot_title = col_pred + ' - prediction on predicted features'\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert_sc, y=y_smoothed, \n                                    cols=Ausert_sc.columns.tolist(), cv=tscv, \n                                    mark_pred_start=True, plot_title=plot_title,\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE is signgificantly higher than for prediction on real rain, temperature and volume.  \nBut results are not too bad.\n\nLet's try to add hydrometry to our model, and check, would it improve our model, or not.","metadata":{}},{"cell_type":"markdown","source":"## Hydrometry Prediction","metadata":{}},{"cell_type":"markdown","source":"For predicting hydrometry we will use the same model, as we used for predicting depth to groundwater.  \nWe won't spend time on model parameters tuning.","metadata":{}},{"cell_type":"code","source":"col_pred_h = 'Hydrometry_Monte_S_Quirico'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred(predict_rain=False, \n                                  predict_temp=False, \n                                  predict_vol=False\n                                 )\nplot_title = col_pred_h + ' - prediction'\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert_sc, y=Ausert[col_pred_h], \n                                    cols=Ausert_sc.columns.tolist(), cv=tscv, \n                                    mark_pred_start=True, plot_title=plot_title,\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_pred_h = 'Hydrometry_Piaggione'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred(predict_rain=False, \n                                  predict_temp=False, \n                                  predict_vol=False\n                                 )\nplot_title = col_pred_h + ' - prediction'\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert_sc, y=Ausert[col_pred_h], \n                                    cols=Ausert_sc.columns.tolist(), cv=tscv, \n                                    mark_pred_start=True, plot_title=plot_title,\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For hydrometry our model shows good results.\n\nNow let's try to add hydrometry to our pipeline.","metadata":{}},{"cell_type":"code","source":"def get_pipe_pred_with_hydro(predict_rain, predict_temp, predict_vol, predict_hydro):\n    \n    pipe = get_pipe(predict_rain, predict_temp, predict_vol)\n    pipe.steps.append(['AddHydroLags', \n                       AddLags(predict_time_features=predict_hydro, \n                               cols=hydro_cols, \n                               step=2, \n                               lag_nums=[1,5,110], \n                               mode='lag',\n                               model=OwnValueModel(\n                                   lag_nums=[1,2,3,4,5], \n                                   min_periods=None, \n                                   mode='avg',\n                                   model=SVR(C=50, gamma=0.0005), \n                                   step=10\n                               ),\n                               features=selected_features2\n                            )\n    ])    \n    \n    pipe.steps.append(['SelectFeatures', SelectFeatures(selected_features_h)])\n    pipe.steps.append(['OVM', ovm])\n    \n    return pipe","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred_with_hydro(\n                                  predict_rain=True, \n                                  predict_temp=True, \n                                  predict_vol=True,\n                                  predict_hydro=True\n                                 )\nplot_title = col_pred + ' - prediction on predicted features, on predicted hydro'\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Ausert_sc, y=y_smoothed, \n                                    cols=Ausert_sc.columns.tolist(), cv=tscv, \n                                    mark_pred_start=True, plot_title=plot_title,\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE became higher after adding hydrometry.  \nSo we won't add hydrometry.","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation on Test Set","metadata":{}},{"cell_type":"markdown","source":"Now let's evaluate our pipeline on test set: on period since July 2019 till June 2020.\n\nFor this we should add scaling as first step.","metadata":{}},{"cell_type":"code","source":"def get_pipe_pred_sc(predict_rain, predict_temp, predict_vol):\n    pipe = get_pipe(predict_rain, predict_temp, predict_vol)\n    pipe.steps.append(['SelectFeatures', SelectFeatures(selected_features2)])\n    pipe.steps.append(['OVM', ovm])\n    pipe.steps.insert(0, ['Scale', ScaleDF()])\n    return pipe","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target value including period July 2019 - June 2020:","metadata":{}},{"cell_type":"code","source":"y_smoothed_all = Auser2011[col_pred].rolling(7, center=True, min_periods=1).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross validation splits that will use period July 2019 - June 2020 for validation:","metadata":{}},{"cell_type":"code","source":"tscv4 = TimeSeriesSplit(n_splits=4, test_size=test_size)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will pass to pipeline dataframe *Auser2011* which includes test set (July 2019 - June 2020)^","metadata":{}},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred_sc(predict_rain=True, \n                                     predict_temp=True, \n                                     predict_vol=True\n                                    )\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Auser2011, \n                                    y=y_smoothed_all, \n                                    cols=Auser2011.columns, cv=tscv4, mark_pred_start=True,\n                                    plot_title='Test set evaluation on predicted features',\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_prepare_pred = get_pipe_pred_sc(predict_rain=False, \n                                     predict_temp=False, \n                                     predict_vol=False\n                                    )\nscores, pred = plot_pred_all_splits(pipe_prepare_pred, Auser2011, \n                                    y=y_smoothed_all, \n                                    cols=Auser2011.columns, cv=tscv4, mark_pred_start=True,\n                                    plot_title='Test set evaluation on real feature values',\n                                    dropNA=False)","metadata":{"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that RMSE on test set is higher than on validation set when using the predicted features. Most likely the reason for this is that rain and temperature is far from it's mean values in 2020.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this notebook we've constructed an automated pipeline for adding lags of features and lags of target value, for predicting features and predicting target value.  \nOur pipeline can be used both for predicting future water level based on given future values of rain, temperature, volume and based only on current values of rain, temperature, volume, water level. (I.e. our pipeline can predict future rain, temperature and volume just before water level prediction)\n\nWe've used grid search for lag's selection. We could do it because our pipeline allowed to pass list of features lags as parameters.\n\nWe've implemented automated features selection: we've created a function that tried to drop feature one by one, compared RMSE including this feature and not including, and then left only those features which presence improved RMSE.\n\nWe've tried the following models:\n- Random Forest\n- Extra Trees\n- SVN\n- KNN\n- Linear Regression  \n\n**And the best results were shown by SVN.**\n\nWe've compared results for prediction based on actual future values of rain, temperature, volume and for prediction based on predicted values of rain, temperature, volume.\nOf course, prediction based on actual features values showed better results.\n\nFor test set:  \n**RMSE** for prediction based on **actual feature** values = **0.13** meters.  \n**RMSE** for prediction based on **predicted feature** values = **0.36** meters.  \nWhereas standard deviation for water level is 0.64 and mean value is 5.7 (meters from the ground floor).","metadata":{}},{"cell_type":"markdown","source":"***Thanks for reading!***\n\n***I will be glad for any comment:)***","metadata":{}}]}