{"cells":[{"metadata":{"_uuid":"a771d2fc-7cef-4efd-af64-f789780d824a","_cell_guid":"44db0080-4659-4566-863c-a2c63b6dd163","trusted":true},"cell_type":"markdown","source":"# ACEA WATER ANALYTICS"},{"metadata":{"_uuid":"5bd64019-d167-4dcf-888c-23c42230a853","_cell_guid":"4683223a-08c3-410b-89bc-3bf8edb68004","trusted":true},"cell_type":"markdown","source":"How-to Multivariate Time series analysis."},{"metadata":{"_uuid":"0cfc46c6-12e3-41fe-b23d-a394b200d75a","_cell_guid":"adba8472-41cd-4641-bcc1-87b758186d6d","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n!pip install odfpy \nfrom statsmodels.tsa.stattools import adfuller\nimport missingno as msno\nimport pickle \n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport tensorflow as tf\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nfrom keras import backend as K\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import PCA\n\nfrom statsmodels.tsa.vector_ar.var_model import VAR","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e231893-ae0b-4ece-969e-c144ebb2a0ef","_cell_guid":"a0bae19f-b546-40df-9613-76ca43055180","trusted":true},"cell_type":"markdown","source":"# RIVER"},{"metadata":{"_uuid":"2b798887-f818-488e-83a8-c8bf00fdd992","_cell_guid":"197b73b1-aff4-4f8c-a923-74b04744b8c6","trusted":true},"cell_type":"code","source":"river = pd.read_csv('/kaggle/input/acea-water-prediction/River_Arno.csv') \nriver.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d8f80c-6bba-4888-8072-c9ad43ead1c4","_cell_guid":"0fca835f-2cd7-4fd0-8970-bf81994b20f7","trusted":true},"cell_type":"code","source":"river.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"107d0bad-0b87-4302-a977-3e693921b600","_cell_guid":"28a36eaa-7fa6-4c10-a91f-5683f279dae9","trusted":true},"cell_type":"markdown","source":"## EDA"},{"metadata":{"_uuid":"ac19e5de-0c87-4d43-b297-a900911f6502","_cell_guid":"31ca70a5-d943-4dd0-adde-07fa8c2f1ab1","trusted":true},"cell_type":"markdown","source":"Checking amount of na values and removing them."},{"metadata":{"_uuid":"1ea914ba-09b0-47a5-95cc-8322dd618234","_cell_guid":"a92f8414-86bb-4605-af64-5bec02ecc5ee","trusted":true},"cell_type":"code","source":"river.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d9cf1d-4271-4d69-bb28-e9ae3e7aa424","_cell_guid":"02c5b914-1c77-4406-a226-6bb251a268c0","trusted":true},"cell_type":"code","source":"msno.bar(river)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29dadeaf-a46c-4c2a-8fdf-ee455db0d97d","_cell_guid":"0285ece0-792f-4bdd-a668-9874aebe1c39","trusted":true},"cell_type":"code","source":"river = river.dropna().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2cc872f-2159-42b7-96ed-46bd5fd0a1ba","_cell_guid":"c589b022-d595-45f9-bd9f-44d73b7baf73","trusted":true},"cell_type":"code","source":"river.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8c64dd4-09ac-4526-973c-0a2be7ac2216","_cell_guid":"7e49e0c9-c38f-411c-8974-cbcdf3860b11","trusted":true},"cell_type":"code","source":"river['Date'] = pd.to_datetime(river['Date'], dayfirst=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcfb873d-a59b-4a44-a1af-98f05deb3293","_cell_guid":"4f7774dd-7998-4528-83f5-4c904df39e2e","trusted":true},"cell_type":"markdown","source":"Checking for non-stationarity in data and removing it.\nNon stationary data create worse performing models in time series"},{"metadata":{"_uuid":"7669bdd0-e600-44ff-9cc5-8d19cee9afd8","_cell_guid":"da3c01ee-3eb8-4df8-8ff0-04d7ddbb510e","trusted":true},"cell_type":"code","source":"for i in range(river.shape[1]):\n    if i > 2:\n        print(adfuller(river.iloc[:,i])[1])\n        if adfuller(river.iloc[:,i])[1] > 0.05:\n            print('{} has p value > 0.05'.format(river.columns.values[i]))\n#column corrrosponding to temperature has non stationarity that needs to be removed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133bb2c2-b91e-445c-a13c-5ed2808445b4","_cell_guid":"ae4bed71-c2a8-48ed-b813-ee78ba0c674a","trusted":true},"cell_type":"code","source":"pd.plotting.autocorrelation_plot(river.iloc[:,-2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91ab4737-4372-4114-b8a3-c3f4498972f7","_cell_guid":"d1a9ebfd-df2e-4545-ab49-c83b89ca035b","trusted":true},"cell_type":"markdown","source":"In autocorrelation plot, it can be observed there is a cycly for roughly an year for temp."},{"metadata":{"_uuid":"d4ab370c-a0db-4223-b6c1-3daedb0617c5","_cell_guid":"2212c392-f0b2-47ae-8d16-fed4c3f30e21","trusted":true},"cell_type":"code","source":"river.iloc[:,-2] = river.iloc[:,-2].diff(1).fillna(river.iloc[:,-2].diff(1)[365])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a162bbef-047f-4ba0-b79f-31e14783e347","_cell_guid":"3ee4b5b6-2900-448b-a424-2e80d019c458","trusted":true},"cell_type":"code","source":"for i in range(river.shape[1]):\n    if i > 2:\n        print(adfuller(river.iloc[:,i])[1])\n        if adfuller(river.iloc[:,i])[1] > 0.05:\n            print('{} has p value > 0.05'.format(river.columns.values[i]))\n#column corrrosponding to temperature has non stationarity that needs to be removed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f550363b-c045-42d1-b0a4-57cf8357dae5","_cell_guid":"08ede81a-dcaf-4bc2-8be7-8958fe93e1fa","trusted":true},"cell_type":"code","source":"correlation = river.corr()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns, annot=True)\nplt.title(\"Correlation among the variables\", size=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67262575-7328-460f-a7e5-88fb34d90cb7","_cell_guid":"8602503b-3022-441b-8223-ae9056b07c93","trusted":true},"cell_type":"markdown","source":"## Features with > 0.4 corr"},{"metadata":{"_uuid":"c95d94cd-7ed7-4c03-bf37-56909845e4b1","_cell_guid":"f740ce6f-e636-412a-be45-92575a666771","trusted":true},"cell_type":"markdown","source":"features with low correlation will not be used. As can be observed, certain Features have correlation below 0.4 and other have above 0.4. Only ones with 0.4 and above will be used"},{"metadata":{"_uuid":"e46b2e9d-dbe8-4064-8878-b113146803f8","_cell_guid":"3610a81c-c63f-4845-adc6-265aed705284","trusted":true},"cell_type":"markdown","source":"## Feature selection methods"},{"metadata":{"_uuid":"317f1eb2-6764-4b0a-aac1-fa6d9b0e7772","_cell_guid":"e7d63650-60c5-4ade-9014-dc1b9e1ee372","trusted":true},"cell_type":"markdown","source":"following cell contains code for various feature selection methods"},{"metadata":{"_uuid":"1885d4ab-4862-44d5-9a9f-3a868d23e6a9","_cell_guid":"6cba3ba5-2833-417a-aae0-b45ab16a3b9b","trusted":true},"cell_type":"code","source":"#PCA \ndef pca(X,Y, col):\n    scaler_rain = StandardScaler()\n    scaled_feat = scaler_rain.fit_transform(X)\n    pca = PCA(n_components=4)\n    components = pca.fit_transform(scaled_feat)\n    col_name = ['PCA1', 'PCA2', 'PCA3', 'PCA4']\n    return [components, Y, col_name]\n\n#TREE FEATURE IMPORTANCE\ndef tree_feat_imp(X,Y, col):\n    estimator = RandomForestRegressor(n_estimators=500, random_state=1)\n    estimator.fit(X, X[:,-1])\n    id = list(estimator.feature_importances_.argsort()[-5:])\n    features = X[:,id]\n    names = river.iloc[:,2:].loc[:,list(col)].columns.values\n    print('METHOD - random forest feature importance')\n    print('important features : {}'.format(names[estimator.feature_importances_.argsort()[:-6:-1]]))\n    col_name = names[estimator.feature_importances_.argsort()[:-6:-1]]\n    return [features, Y, col_name]\n\n#RFE\ndef rfe(X, Y, col):\n    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=1), n_features_to_select=5)\n    fit = rfe.fit(X, Y)\n    names = river.iloc[:,2:].loc[:,list(col)].columns.values\n    print('METHOD - rfe')\n    col_name = []\n    for i in range(fit.support_.shape[0]):\n        if fit.support_[i]:\n            col_name.append(names[i])\n    print(col_name)\n    features = river.iloc[:,2:].loc[:,list(col)].loc[:,list(fit.support_)].copy().to_numpy()\n    return [features, Y, col_name]\n\n#F REGRESSION\ndef f_reg(X,Y, col):\n    f_test,_ = f_regression(X,Y)\n    id = list(abs(f_test).argsort()[:-6:-1])\n    features = X[:,id]\n    names = river.iloc[:,2:].loc[:,list(col)].columns.values\n    print('METHOD - f regression')\n    print('important features : {}'.format(names[abs(f_test).argsort()[:-6:-1]]))\n    col_name = names[abs(f_test).argsort()[:-6:-1]]\n    return [features, Y, col_name]\n\n#MUTUAL INFO REGRESSION\ndef mutual_info(X,Y, col):\n    mi = mutual_info_regression(X,Y)\n    id = list(mi.argsort()[:-6:-1])\n    features = X[:,id]\n    names = river.iloc[:,2:].loc[:,list(col)].columns.values\n    print('METHOD - mutual info regression')\n    print('important features : {}'.format(names[mi.argsort()[:-6:-1]]))\n    col_name = names[mi.argsort()[:-6:-1]]\n    return [features, Y, col_name]\n\n#PERMUTATION IMPORTANCE\ndef pi(X,Y, col):\n    estimator = RandomForestRegressor().fit(X,Y)\n    result = permutation_importance(estimator, X, Y, n_repeats=10)\n    id = list(result.importances_mean.argsort()[:-6:-1])\n    features = X[:,id]\n    names = river.iloc[:,2:].loc[:,list(col)].columns.values\n    print('METHOD - permutation importance')\n    print('important features : {}'.format(names[result.importances_mean.argsort()[:-6:-1]]))\n    col_name = names[result.importances_mean.argsort()[:-6:-1]]\n    return [features, Y, col_name]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2596248-518d-4988-9bb5-b80fc87377b8","_cell_guid":"6146d50f-36a1-4057-9bd9-7e11886d248f","trusted":true},"cell_type":"code","source":"def select_features(method=dict()):\n    col = (river.iloc[:,1:].corrwith(river['Hydrometry_Nave_di_Rosano'], axis=0) > 0.4)\n    features = river.iloc[:,2:].loc[:,list(col)].copy().to_numpy()\n    target = river['Hydrometry_Nave_di_Rosano'].copy().to_numpy()\n    #following are various feature selection methods\n    method['0.4_corr'] = [features,target, river.iloc[:,2:].loc[:,list(col)].columns.values]\n    # '0.4_corr' contains features with more than 0.4 correlation with Hydrometry\n    method['pca'] = pca(features, target, col)\n    #'pca' contains features with 4 pca components, acounting for 90% variance\n    method['tree'] = tree_feat_imp(features, target, col)\n    #'tree' uses tree based models to select top 5 features\n    method['rfe'] = rfe(features, target, col)\n    #'rfe' uses recursive feature elimination to select top 5 features\n    method['f_reg'] = f_reg(features,target, col)\n    #selecting features using f regression\n    method['mutual_info'] = mutual_info(features, target, col)\n    #selecting features using mutual info\n    method['perm_imp'] = pi(features, target, col)\n    #selecting features using permutation importance\n    return method","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b91984c3-9e88-42b3-83c4-5672060ee806","_cell_guid":"7489e77d-7435-4f0f-b0bf-9f00dbb0ff8a","trusted":true},"cell_type":"code","source":"feat_sel_method = select_features()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3863c2e3-89a3-4e95-b54b-a0b9ff742bfd","_cell_guid":"edecaf53-5aea-4da1-bd90-3297fedb273b","trusted":true},"cell_type":"markdown","source":"## Models"},{"metadata":{"_uuid":"8e2d4203-5188-48b7-a068-08a964f9da98","_cell_guid":"aa6c56c5-df59-4f54-8144-f2e492d6c024","trusted":true},"cell_type":"markdown","source":"### VAR"},{"metadata":{"_uuid":"72afc37a-9a27-461f-aae4-b1d64ddcff94","_cell_guid":"941b8b7c-7252-4642-87c0-081437d252ea","trusted":true},"cell_type":"markdown","source":"following loop contains training loop for VAR model"},{"metadata":{"_uuid":"50c05b59-ccfa-41c3-8531-72c52c72c7bd","_cell_guid":"f963fd2b-d403-4dd0-83e2-c08cc62a81a3","trusted":true},"cell_type":"code","source":"def VAR_training(feat_sel_dict):\n    \n    comparison={}\n    for key in feat_sel_dict.keys():\n        print('Method - {}'.format(key))\n        x , xtest, y, ytest = train_test_split(feat_sel_dict[key][0],feat_sel_dict[key][1]\n                                              , test_size=0.5, shuffle=False)\n\n        scaler = StandardScaler()\n        x = scaler.fit_transform(x)\n        xtest = scaler.transform(xtest)\n\n        xtrain = pd.DataFrame(data=x, columns = feat_sel_dict[key][2])\n\n        model = VAR(xtrain)\n        result = model.fit(maxlags=30, ic='aic')\n        print('lag - {}'.format(result.k_ar))\n        lag = result.k_ar\n\n        model_lag = VAR(xtrain)\n        result_lag = model_lag.fit(lag)\n        result_lag.summary()\n\n        xtest = pd.DataFrame(data=xtest, columns = feat_sel_dict[key][2])\n\n        pred=[]\n        for i in range(ytest.shape[0]-lag):\n            val = result_lag.forecast(xtest.values[i:i+lag], steps=1)\n            #print(val)\n            pred.append(val)\n\n        arr = np.array(pred)\n        #arr shape : [no of samples ,1,no. of features]\n        arr = np.squeeze(arr, axis=1)\n\n        ypred = scaler.inverse_transform(arr)\n        rounded = ypred[:,-1].round(2)\n\n        rmse = tf.keras.metrics.RootMeanSquaredError()\n        rmse.update_state(ytest[lag:], rounded)\n\n        print('RMSE - {}'.format(rmse.result().numpy()))\n\n        mae = tf.keras.losses.MeanAbsoluteError()\n        mae = mae(ytest[lag:], rounded).numpy()\n\n        print('MAE - {}'.format(mae))\n        \n        comparison[key] = {'RMSE':rmse.result().numpy(), 'lag':lag,\n                          'forecast':pd.DataFrame({'pred': rounded,'test': ytest[lag:]})}\n        \n    return comparison","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c80f162-1115-415a-bf58-fbbd86fa235c","_cell_guid":"9b814300-a773-44d5-bf06-302a170f0856","trusted":true},"cell_type":"code","source":"var = VAR_training(feat_sel_method)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5b5b14d-c3e5-4323-8730-8dc171244709","_cell_guid":"5140e141-586a-40d4-9233-7087b1a5bcfd","trusted":true},"cell_type":"markdown","source":"### LSTM"},{"metadata":{"_uuid":"c84dda58-7159-4d41-8d1f-154463ec3d8b","_cell_guid":"c9f0528e-09ad-4a29-9e7a-41f138b9326a","trusted":true},"cell_type":"markdown","source":"following cell contains trianing loop for lstm model."},{"metadata":{"_uuid":"e5b24799-bdbf-4ce0-89eb-75be7a23c215","_cell_guid":"742a28f0-9eed-47d0-b126-ed6509cfffe7","trusted":true},"cell_type":"code","source":"''' model_type - 'feature_corr', 'pca_corr', 'tree_corr', 'rfe_corr', 'f_regression_corr', \n'mutual_info_corr', 'permutation_imp_corr'\n\nThis partitions features and targets in train/val/test set\n \n'''\ndef training(X,Y,model_type, save=False):\n\n    x, xtest, y, ytest = train_test_split(X, Y, test_size=0.5,\n                                                shuffle=False)\n\n    scaler = StandardScaler()\n    x = scaler.fit_transform(x)\n\n    xtrain, xval, ytrain, yval = train_test_split(x, x[:,-1], test_size=0.1,\n                                                shuffle=False)\n    xtest = scaler.transform(xtest)\n\n    models = {}\n    models[model_type] = {}\n    for lag in range(1, 8):\n        train_generator = TimeseriesGenerator(xtrain, ytrain, length=lag, sampling_rate=1, batch_size=32)\n        val_generator = TimeseriesGenerator(xval, yval, length=lag, sampling_rate=1, batch_size=32)\n        test_generator = TimeseriesGenerator(xtest, ytest, length=lag, sampling_rate=1, batch_size=1)\n\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.LSTM(64, activation='relu', input_shape=(lag, xtrain.shape[1]), return_sequences=False))\n        model.add(tf.keras.layers.Dropout(0.3))\n        model.add(tf.keras.layers.Dense(1))\n\n        #model.summary()\n\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                        patience=2,\n                                                        mode='min'\n        )\n\n\n        model.compile(loss=tf.losses.MeanSquaredError(),\n                    optimizer=tf.optimizers.Adam(),\n                    metrics=[tf.keras.metrics.RootMeanSquaredError(),\n                            tf.metrics.MeanAbsoluteError()]\n        )\n\n\n        history = model.fit(train_generator,\n                          epochs=30,\n                          validation_data=val_generator,\n                          shuffle=False,\n                          callbacks=[early_stopping],\n                          verbose=0)\n        print('------------- EVALUATION - {}-----------------'.format(lag))\n        ypred = model.predict(test_generator)\n        pred = np.repeat(ypred, xtrain.shape[1], axis=1)\n        scaled = scaler.inverse_transform(pred)\n        rounded = scaled[:,-1].round(2)\n\n        test = ytest[-(ytest.shape[0]-lag):]\n\n        rmse = tf.keras.metrics.RootMeanSquaredError()\n        rmse.update_state(test, rounded)\n\n        print('RMSE = {}'.format(rmse.result().numpy()))\n\n        mae = tf.keras.losses.MeanAbsoluteError()\n        mae = mae(test, rounded).numpy()\n\n        print('MAE = {}'.format(mae))\n        \n        if save == True:\n\n            models[model_type][lag] = {}\n            models[model_type][lag]['prediction_metric'] = rmse.result().numpy()\n            models[model_type][lag]['history'] = history.history\n            models[model_type][lag]['forecast'] = pd.DataFrame({'pred': rounded, \n                                                                'test': test})\n            return models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e872fefd-dcf1-4882-85ea-1d179bec28ca","_cell_guid":"1c529ca1-501e-4511-8e2e-153a925d6ae8","trusted":true},"cell_type":"code","source":"for key in feat_sel_method.keys():\n    print('_____'+key+'_______')\n    training(feat_sel_method[key][0], feat_sel_method[key][1], key)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b097f03-4442-41e5-ad33-3e3fecff653c","_cell_guid":"283a8ac8-e9f2-48a1-9dd5-ac22e331911e","trusted":true},"cell_type":"code","source":"lstm = training(feat_sel_method['rfe'][0], feat_sel_method['rfe'][1],\n                'rfe', True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5442b75-2c84-48c9-bae2-9b06e59d8649","_cell_guid":"789c6382-c52e-4d3d-baae-1363439e87c0","trusted":true},"cell_type":"code","source":"lstm['rfe'][1]['forecast'].plot(y=['test', 'pred'], figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eccbbe82-791c-43b1-879a-85fd18503804","_cell_guid":"d9ed9bd1-72fc-4da8-9701-5e248eef754c","trusted":true},"cell_type":"code","source":"var['tree']['forecast'].plot(y=['test', 'pred'], figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8843e1b-2392-40d8-9088-ead72afeb5e1","_cell_guid":"99e7228f-e28c-489e-a861-ce979fc394b4","trusted":true},"cell_type":"markdown","source":"Given the fact that Model - VAR has lower rmse. Additionally, in graphical representation it can be observed that var predicts peak values better than lstm, var model will be used for forecasting of final test set."},{"metadata":{"_uuid":"0a80deb9-05c2-47cd-9a21-0680c5bfc0ed","_cell_guid":"c6a044b5-befd-434f-875f-acce96fd805f","trusted":true},"cell_type":"markdown","source":"# Final TESTING"},{"metadata":{"_uuid":"72f5ac62-bb3e-43a9-8439-56c27ae0231d","_cell_guid":"f15e638e-05a6-4572-9535-53de12d93e5b","trusted":true},"cell_type":"markdown","source":"training a var model with combined data"},{"metadata":{"_uuid":"59544cfa-b649-4926-9888-770879cffef0","_cell_guid":"b64fb876-cc31-411e-a144-81135c9e2637","trusted":true},"cell_type":"code","source":"def final_VAR_model(feat_sel_dict):\n    \n    for key in feat_sel_dict.keys():\n        print('Method - {}'.format(key))\n    \n        scaler = StandardScaler()\n        X = scaler.fit_transform(feat_sel_dict[key][0])\n\n        xtrain = pd.DataFrame(data=X, columns = feat_sel_dict[key][2])\n\n        model = VAR(xtrain)\n        result = model.fit(maxlags=30, ic='aic')\n        print('lag - {}'.format(result.k_ar))\n        lag = result.k_ar\n\n        model_lag = VAR(xtrain)\n        result_lag = model_lag.fit(lag)\n        result_lag.summary()\n        \n        if key == 'tree':\n            result_lag.save('river_var_rmse_0.3_lag_3.pkl')\n            pickle.dump(scaler, open('river_scaler_var_tree.pkl', 'wb'))#saving scaler to file \n            #scaler name contains details of relevant model and feature selection methods","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e455de7-3a4c-4731-84e0-e557ea3cac83","_cell_guid":"1af71d32-7b54-414e-9cd8-40c6b3f11c6d","trusted":true},"cell_type":"code","source":"final_VAR_model(feat_sel_method)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e412dea-8251-4e86-a64c-b49d72cf0c63","_cell_guid":"359abc33-a44a-453d-88cd-74833f7819cb","trusted":true},"cell_type":"markdown","source":"## Future Forecast"},{"metadata":{"_uuid":"2348c4df-c2e5-44e5-b693-eca06c34d331","_cell_guid":"8ae3d000-a5d4-4aa2-ac1a-cec425a829d6","trusted":true},"cell_type":"code","source":"#loading model and running it on test set\nimport statsmodels.api as sm\nmodel = sm.load('var_rmse_0.3_lag_3.pickle') #model path\nmodel.summary()#check if model loaded correctly\n\n# 1. load test set of river dataframe in test\ntest \n\n# 2. run cell containing code of different feature selection methods\ncol = (test.iloc[:,1:].corrwith(test['Hydrometry_Nave_di_Rosano'], axis=0) > 0.4)\nfeatures = test.iloc[:,2:].loc[:,list(col)].copy().to_numpy()#selects all columns except index and datatime\ntarget = test['Hydrometry_Nave_di_Rosano'].copy().to_numpy()\nX,Y, col_name = tree_feat_imp(features,target)\n\n# 3. scale data using same scaler\nscaler = load(open('scaler.pkl', 'rb'))\nX = scaler.transform(X)\n\n# 4. preparing input for VAR model\nxtest = pd.DataFrame(data=X, columns = col_name)\n\nlag = 3 # lag of saved model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91059404-226d-4c53-9ced-83449f4ac17b","_cell_guid":"2d8c1b93-a53f-4ccf-86a8-50743ec7932e","trusted":true},"cell_type":"code","source":"#define how far in future do you want the forcasts for\n# EX : for a week in future, steps = 7\nsteps = \nforecast = model.forecast(xtest.values[i:i+lag], steps=)\n\nypred = scaler.inverse_transform(forecast)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a324a49b-42a4-4631-b947-48e3d2630ac6","_cell_guid":"b82be82e-3e6a-4f0f-9232-c6298d4db23c","trusted":true},"cell_type":"code","source":"'''\nin case you want to predict data for next day\ngiven the data of selected features for past number of days\n\nlag = number for days\n\n'''\npred=[]\nfor i in range(ytest.shape[0]-lag):\n    val = model.forecast(xtest.values[i:i+lag], steps=1)\n    pred.append(val)\n\narr = np.array(pred)\n#arr shape : [no of samples ,1,no. of features]\narr = np.squeeze(arr, axis=1)\n\nypred = scaler.inverse_transform(arr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"207e25bd-a727-4f87-8015-2d2f20cf61bd","_cell_guid":"44339687-507c-4016-ae0d-e3b48bb71f46","trusted":true},"cell_type":"code","source":"rounded = ypred[:,-1].round(2)\n\nrmse = tf.keras.metrics.RootMeanSquaredError()\n\nrmse.update_state(target[lag:], rounded)\n\nprint('RMSE - {}'.format(rmse.result().numpy()))\n\nmae = tf.keras.losses.MeanAbsoluteError()\nmae = mae(target[lag:], rounded).numpy()\n\nprint('MAE - {}'.format(mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"070c19df-0170-4e6e-ba4f-e9181ed0365f","_cell_guid":"63f8aa35-2316-4d32-a036-69db7b15c250","trusted":true},"cell_type":"markdown","source":"# LAKE"},{"metadata":{"_uuid":"c0abee9a-f2e7-4731-9b9a-05608239bb28","_cell_guid":"ea2c7b04-bd82-40a8-94ed-338e54932f76","trusted":true},"cell_type":"code","source":"lake = pd.read_csv('/kaggle/input/acea-water-prediction/Lake_Bilancino.csv') \nlake.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ba0070-60a9-40c3-a819-e1a9c83ceabe","_cell_guid":"5c4173f8-90c3-4abe-9798-a010b94d9221","trusted":true},"cell_type":"code","source":"lake.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"_uuid":"ab7e0566-3daf-4fd3-ab3d-8617e3f5f444","_cell_guid":"eefbbad4-6a41-4022-8cda-ff2d4e67413e","trusted":true},"cell_type":"code","source":"lake.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40ab98e4-8c46-4765-af24-e1a887e7c50b","_cell_guid":"ec7e9eda-5e07-442e-86de-93c46a329e5a","trusted":true},"cell_type":"code","source":"msno.bar(lake)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"511f991e-700b-4e45-aa1c-1ab59f010a99","_cell_guid":"dd28f8fc-c224-4bd0-8d13-249e514eb293","trusted":true},"cell_type":"code","source":"lake = lake.dropna().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lake.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lake['Date'] = pd.to_datetime(lake['Date'], dayfirst=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(lake.shape[1]):\n    if i > 2:\n        print(adfuller(lake.iloc[:,i])[1])\n        if adfuller(lake.iloc[:,i])[1] > 0.05:\n            print('-'*20)\n            print('{} has p value > 0.05'.format(lake.columns.values[i]))\n            print('-'*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = lake.corr()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns, annot=True)\nplt.title(\"Correlation among the variables\", size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA \ndef pca(X,Y):\n    scaler_rain = StandardScaler()\n    scaled_feat = scaler_rain.fit_transform(X)\n    pca_var = PCA(n_components=3)\n    var = pca_var.fit(scaled_feat)\n    pca = PCA(n_components=3)\n    components = pca.fit_transform(scaled_feat)\n    col_name = ['PCA1', 'PCA2', 'PCA3']\n    print('METHOD - PCA')\n    print('feature variance = {}'.format(sum(var.explained_variance_ratio_.round(2))))\n    return [components, Y, col_name]\n\n#TREE FEATURE IMPORTANCE\ndef tree_feat_imp(X,Y):\n    estimator = RandomForestRegressor(n_estimators=500, random_state=1)\n    estimator.fit(X, X[:,-1])\n    id = list(estimator.feature_importances_.argsort()[:-4:-1])\n    features = X[:,id]\n    names = lake.iloc[:,2:].columns.values\n    print('METHOD - random forest feature importance')\n    print('important features : {}'.format(names[estimator.feature_importances_.argsort()[:-4:-1]]))\n    col_name = names[estimator.feature_importances_.argsort()[:-4:-1]]\n    return [features, Y, col_name]\n\n#RFE\ndef rfe(X, Y):\n    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=1), n_features_to_select=3)\n    fit = rfe.fit(X, Y)\n    names = lake.iloc[:,2:].columns.values\n    print('METHOD - rfe')\n    col_name = []\n    for i in range(fit.support_.shape[0]):\n        if fit.support_[i]:\n            col_name.append(names[i])\n    print(col_name)\n    features = lake.iloc[:,2:].loc[:,list(fit.support_)].copy().to_numpy()\n    return [features, Y, col_name]\n\n#F REGRESSION\ndef f_reg(X,Y):\n    f_test,_ = f_regression(X,Y)\n    id = list(abs(f_test).argsort()[:-4:-1])\n    features = X[:,id]\n    names = lake.iloc[:,2:].columns.values\n    print('METHOD - f regression')\n    print('important features : {}'.format(names[abs(f_test).argsort()[:-4:-1]]))\n    col_name = names[abs(f_test).argsort()[:-4:-1]]\n    return [features, Y, col_name]\n\n#MUTUAL INFO REGRESSION\ndef mutual_info(X,Y):\n    mi = mutual_info_regression(X,Y)\n    id = list(mi.argsort()[:-4:-1])\n    features = X[:,id]\n    names = lake.iloc[:,2:].columns.values\n    print('METHOD - mutual info regression')\n    print('important features : {}'.format(names[mi.argsort()[:-4:-1]]))\n    col_name = names[mi.argsort()[:-4:-1]]\n    return [features, Y, col_name]\n\n#PERMUTATION IMPORTANCE\ndef pi(X,Y):\n    estimator = RandomForestRegressor().fit(X,Y)\n    result = permutation_importance(estimator, X, Y, n_repeats=10)\n    id = list(result.importances_mean.argsort()[:-4:-1])\n    features = X[:,id]\n    names = lake.iloc[:,2:].columns.values\n    print('METHOD - permutation importance')\n    print('important features : {}'.format(names[result.importances_mean.argsort()[:-4:-1]]))\n    col_name = names[result.importances_mean.argsort()[:-4:-1]]\n    return [features, Y, col_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lake.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_features(y):\n    \n    method = {}\n    \n    features = lake.iloc[:,2:].copy().to_numpy()\n    target = lake[y].copy().to_numpy()\n    #following are various feature selection methods\n    method['all'] = [features,target, lake.iloc[:,2:].columns.values]\n    # '0.4_corr' contains features with more than 0.4 correlation with Hydrometry\n    #method['pca'] = pca(features, target)\n    #'pca' contains features with 4 pca components, acounting for 90% variance\n    method['tree'] = tree_feat_imp(features, target)\n    #'tree' uses tree based models to select top 5 features\n    method['rfe'] = rfe(features, target)\n    #'rfe' uses recursive feature elimination to select top 5 features\n    method['f_reg'] = f_reg(features,target)\n    #selecting features using f regression\n    method['mutual_info'] = mutual_info(features, target)\n    #selecting features using mutual info\n    method['perm_imp'] = pi(features, target)\n    #selecting features using permutation importance\n    return method","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flow = select_features('Flow_Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level = select_features('Lake_Level')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def VAR_training(feat_sel_dict, ycol):\n    \n    comparison={}\n\n    for key in feat_sel_dict.keys():\n        print('Method - {}'.format(key))\n        x , xtest, y, ytest = train_test_split(feat_sel_dict[key][0],feat_sel_dict[key][1]\n                                              , test_size=0.5, shuffle=False)\n\n        scaler = StandardScaler()\n        x = scaler.fit_transform(x)\n        xtest = scaler.transform(xtest)\n\n        xtrain = pd.DataFrame(data=x, columns = feat_sel_dict[key][2])\n\n        model = VAR(xtrain)\n        result = model.fit(maxlags=30, ic='aic')\n        print('lag - {}'.format(result.k_ar))\n        lag = result.k_ar\n\n        model_lag = VAR(xtrain)\n        result_lag = model_lag.fit(lag)\n        result_lag.summary()\n\n        xtest = pd.DataFrame(data=xtest, columns = feat_sel_dict[key][2])\n        if ycol == 'Flow_Rate':\n            idx = xtest.columns.get_loc('Flow_Rate')\n        else:\n            idx = xtest.columns.get_loc('Lake_Level')\n        \n        pred=[]\n        for i in range(ytest.shape[0]-lag):\n            val = result_lag.forecast(xtest.values[i:i+lag], steps=1)\n            #print(val)\n            pred.append(val)\n\n        arr = np.array(pred)\n        #arr shape : [no of samples ,1,no. of features]\n        arr = np.squeeze(arr, axis=1)\n\n        ypred = scaler.inverse_transform(arr)\n        rounded = ypred[:,idx].round(2)\n\n        rmse = tf.keras.metrics.RootMeanSquaredError()\n        rmse.update_state(ytest[lag:], rounded)\n\n        print('RMSE - {}'.format(rmse.result().numpy()))\n\n        mae = tf.keras.losses.MeanAbsoluteError()\n        mae = mae(ytest[lag:], rounded).numpy()\n\n        print('MAE - {}'.format(mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_flow = VAR_training(flow, 'Flow_Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_level = VAR_training(level, 'Lake_Level')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' model_type - 'feature_corr', 'pca_corr', 'tree_corr', 'rfe_corr', 'f_regression_corr', \n'mutual_info_corr', 'permutation_imp_corr'\n\nThis partitions features and targets in train/val/test set\n \n'''\ndef training(X,Y,model_type,ycol,save=False):\n\n    x, xtest, y, ytest = train_test_split(X, Y, test_size=0.5,\n                                                shuffle=False)\n\n    scaler = StandardScaler()\n    x = scaler.fit_transform(x)\n\n    xtrain, xval, ytrain, yval = train_test_split(x, y, test_size=0.1,\n                                                shuffle=False)\n    xtest = scaler.transform(xtest)\n    \n    if ycol == 'Flow_Rate':\n        if model_type in ['all','rfe']:\n            idx = -1\n        else:    \n            idx = 1 \n    else:\n        idx = -2\n    \n    \n    models = {}\n    models[model_type] = {}\n    for lag in range(1, 30):\n        train_generator = TimeseriesGenerator(xtrain, ytrain, length=lag, sampling_rate=1, batch_size=32)\n        val_generator = TimeseriesGenerator(xval, yval, length=lag, sampling_rate=1, batch_size=32)\n        test_generator = TimeseriesGenerator(xtest, ytest, length=lag, sampling_rate=1, batch_size=1)\n\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.LSTM(64, activation='relu', input_shape=(lag, xtrain.shape[1]), return_sequences=False))\n        model.add(tf.keras.layers.Dropout(0.3))\n        model.add(tf.keras.layers.Dense(1))\n\n        #model.summary()\n\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                        patience=2,\n                                                        mode='min'\n        )\n\n\n        model.compile(loss=tf.losses.MeanSquaredError(),\n                    optimizer=tf.optimizers.Adam(),\n                    metrics=[tf.keras.metrics.RootMeanSquaredError(),\n                            tf.metrics.MeanAbsoluteError()]\n        )\n\n\n        history = model.fit(train_generator,\n                          epochs=30,\n                          validation_data=val_generator,\n                          shuffle=False,\n                          callbacks=[early_stopping],\n                          verbose=0)\n        \n        \n        print('------------- EVALUATION - {}-----------------'.format(lag))\n        ypred = model.predict(test_generator)\n        pred = np.repeat(ypred, xtrain.shape[1], axis=1)\n        scaled = scaler.inverse_transform(pred)    \n        rounded = scaled[:,idx].round(2)\n\n        test = ytest[-(ytest.shape[0]-lag):]\n\n        rmse = tf.keras.metrics.RootMeanSquaredError()\n        rmse.update_state(test, rounded)\n\n        print('RMSE = {}'.format(rmse.result().numpy()))\n\n        mae = tf.keras.losses.MeanAbsoluteError()\n        mae = mae(test, rounded).numpy()\n\n        print('MAE = {}'.format(mae))\n        \n        if save == True:\n\n            models[model_type][lag] = {}\n            models[model_type][lag]['prediction_metric'] = rmse.result().numpy()\n            models[model_type][lag]['history'] = history.history\n            models[model_type][lag]['forecast'] = pd.DataFrame({'pred': rounded, \n                                                                'test': test})\n            return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in flow.keys():\n    print(i)\n    print(flow[i][0][:5], flow[i][1][:3], flow[i][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in ['all', 'tree', 'rfe']:\n    print('_____'+key+'_______')\n    training(flow[key][0], flow[key][1], key, 'Flow_Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in ['all', 'tree', 'rfe']:\n    print('_____'+key+'_______')\n    training(level[key][0], level[key][1], key, 'Lake_Level')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}