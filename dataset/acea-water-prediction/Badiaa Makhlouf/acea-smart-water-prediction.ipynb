{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing all the required libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install minepy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n%matplotlib inline\nimport missingno as msno\nfrom datetime import datetime\nfrom numpy.random import multivariate_normal as mvnrnd\nfrom scipy.stats import wishart\nfrom scipy.stats import invwishart\nfrom numpy.linalg import inv as inv\nimport scipy.io\nimport time\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom minepy import MINE\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom sklearn.linear_model import RidgeCV\nfrom math import sqrt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom pykalman import KalmanFilter\nfrom sklearn.ensemble import RandomForestRegressor\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data import\n\nWe use the data of Aquifer_Auser as an example to demonstrate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/acea-water-prediction/Aquifer_Auser.csv\")\ndf['date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d/%m/%Y\"))\ntarget_variable =['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_LT2']\ncolumns_name = df.columns.values.tolist()\nrain_list = [ a for a in columns_name if a.startswith('Rain')]\nTemp_list = [ a for a in columns_name if a.startswith('Temperature')]\nDepth_list = [ a for a in columns_name if a.startswith('Depth')]\nn_row = df.shape[0]\nmsno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rain_list+Temp_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the figure above, we can see that there are a huge number of missing values, and the density of these values is quite large. For the variables that will be used as predicted values, many sparse missing values are scattered in them. We decided to remove the rows with large density of missing values first, and then fill in the sparse missing values."},{"metadata":{},"cell_type":"markdown","source":"We found that there are some continuous zero values in the temperature variable and depth variable, which is quite abnormal. To prevent these zero values from being caused by measurement errors, we turn them into missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(Depth_list)):\n        df[df[[Depth_list[i]]]==0]=np.nan\n    \nfor i in range(len(Temp_list)):\n        df[df[[Temp_list[i]]]==0]=np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created some graphs to observe the missing value distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing value in rainfall variables \nfor i in range(len(rain_list)):\n    nullnum = df[['Date',rain_list[i]]].isnull().sum(axis=1).to_numpy()\n    plt.plot(np.array(list(range(n_row))),nullnum,label=rain_list[i])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing value in Depth variables\nfor i in range(len(Depth_list)):\n    nullnum = df[['Date',Depth_list[i]]].isnull().sum(axis=1).to_numpy()\n    plt.plot(np.array(list(range(n_row))),nullnum,label=Depth_list[i])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing value in temperature variables\nfor i in range(len(Temp_list)):\n    nullnum = df[['Date',Temp_list[i]]].isnull().sum(axis=1).to_numpy()\n    plt.plot(np.array(list(range(n_row))),nullnum,label=Temp_list[i])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#according to the four graphs above, we select all the data between the 4685th row and the 7000th row.\ndfsecond = df[4685:7000]\nmsno.matrix(dfsecond)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nRainfall variables and temperature variables are important independent variables in this dataset, \nbut their influence on the Depth_to_Groundwater variable may lag behind. So we need to find how many days will it take for their impact on the target variables to be reflected in the value. \nTake 'Depth_to_Groundwater_CoS' as excample, shift the data in this variable 31 times and create a variable after each shift."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfTestLag = dfsecond[rain_list+Temp_list]\nfor i in range(0,31):\n    dfTestLag['CoS'+str(i)] = dfsecond['Depth_to_Groundwater_CoS'].shift(-1*i)\n\ntargetlistCoS = ['CoS0','CoS1','CoS2', 'CoS3', 'CoS4', 'CoS5', 'CoS6', 'CoS7', 'CoS8', 'CoS9', 'CoS10', 'CoS11',\n                 'CoS12', 'CoS13', 'CoS14', 'CoS15', 'CoS16', 'CoS17', 'CoS18', 'CoS19', 'CoS20', 'CoS21', 'CoS22', \n                 'CoS23', 'CoS24', 'CoS25', 'CoS26', 'CoS27','CoS28','CoS29','CoS30']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use ridgeCV regression to fit the independent variables to 31 new variables to create 31 models,\nthen we test the rmse of each model, draw a plot, and choose the point with the smallest rmse value \nto determine the lag value."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfTestLag = dfTestLag.ffill().bfill()\nCoSList =[]\n\nfor i in range(len(targetlistCoS)):\n    YCoS =dfTestLag [targetlistCoS[i]]\n    X = dfTestLag[rain_list+Temp_list]\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,YCoS,test_size=0.2,random_state=i)\n    ridgecv = RidgeCV(alphas=[0.01, 0.1, 0.5, 1, 5, 7, 10, 30,100, 200])\n    model = ridgecv.fit(X_train1, y_train1)\n    y_pred1 = model.predict(X_test1)\n    rms1 = sqrt(mean_squared_error(y_test1, y_pred1))\n    CoSList.append(rms1)\n\ndfLT2 = pd.DataFrame(CoSList,columns=['rms'])\nplt.title(\"CoS lag\")\nplt.ylabel(\"RMS\")\nplt.xlabel('lag')\nplt.plot(dfLT2['rms'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used the same method to test the lag value of other target variables, then we chose '28' as the lag value of target variables."},{"metadata":{},"cell_type":"markdown","source":"Bayesian Temporal Matrix Factorization (BTMF)\n\nWe chose BTMF method to fill in the missing values of Depth_to_Groundwater variables. The BTMF method has better performance in filling the missing values of long-term time series data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfmeasure = dfsecond[['Depth_to_Groundwater_LT2',\n 'Depth_to_Groundwater_SAL',\n 'Depth_to_Groundwater_PAG',\n 'Depth_to_Groundwater_CoS',\n 'Depth_to_Groundwater_DIEC']]\ndfdens = dfmeasure[['Depth_to_Groundwater_LT2',\n 'Depth_to_Groundwater_SAL',\n 'Depth_to_Groundwater_PAG',\n 'Depth_to_Groundwater_CoS',\n 'Depth_to_Groundwater_DIEC']]\ndfdens['Depth_to_Groundwater_LT2'] = dfdens['Depth_to_Groundwater_LT2'].interpolate()\ndfdens['Depth_to_Groundwater_SAL'] = dfdens['Depth_to_Groundwater_SAL'].interpolate()\ndfdens['Depth_to_Groundwater_PAG'] = dfdens['Depth_to_Groundwater_PAG'].interpolate()\ndfdens['Depth_to_Groundwater_CoS'] = dfdens['Depth_to_Groundwater_CoS'].interpolate()\ndfdens['Depth_to_Groundwater_DIEC'] = dfdens['Depth_to_Groundwater_DIEC'].interpolate()\ndfdens = dfdens.ffill().bfill()\ndfdens = np.delete(dfdens.to_numpy().T,range(len(dfsecond)-(len(dfsecond)//28)*28),axis = 1)\ndfdealMis = np.delete(dfmeasure.fillna(0).to_numpy().T,range(len(dfsecond)-(len(dfsecond)//28)*28),axis = 1)\n\ndef kr_prod(a, b):\n    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)\n\ndef cov_mat(mat):\n    dim1, dim2 = mat.shape\n    new_mat = np.zeros((dim2, dim2))\n    mat_bar = np.mean(mat, axis = 0)\n    for i in range(dim1):\n        new_mat += np.einsum('i, j -> ij', mat[i, :] - mat_bar, mat[i, :] - mat_bar)\n    return new_mat\n\ndef ten2mat(tensor, mode):\n    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n\ndef mat2ten(mat, tensor_size, mode):\n    index = list()\n    index.append(mode)\n    for i in range(tensor_size.shape[0]):\n        if i != mode:\n            index.append(i)\n    return np.moveaxis(np.reshape(mat, list(tensor_size[index]), order = 'F'), 0, mode)\n\ndef mnrnd(M, U, V):\n    \"\"\"\n    Generate matrix normal distributed random matrix.\n    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n    \"\"\"\n    dim1, dim2 = M.shape\n    X0 = np.random.rand(dim1, dim2)\n    P = np.linalg.cholesky(U)\n    Q = np.linalg.cholesky(V)\n    return M + np.matmul(np.matmul(P, X0), Q.T)\n\ndef BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2):\n    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n    W = init[\"W\"]\n    X = init[\"X\"]\n    \n    d = time_lags.shape[0]\n    dim1, dim2 = sparse_mat.shape\n    pos = np.where((dense_mat != 0) & (sparse_mat == 0))\n    position = np.where(sparse_mat != 0)\n    binary_mat = np.zeros((dim1, dim2))\n    binary_mat[position] = 1\n    \n    beta0 = 1\n    nu0 = rank\n    mu0 = np.zeros((rank))\n    W0 = np.eye(rank)\n    tau = 1\n    alpha = 1e-6\n    beta = 1e-6\n    S0 = np.eye(rank)\n    Psi0 = np.eye(rank * d)\n    M0 = np.zeros((rank * d, rank))\n    \n    W_plus = np.zeros((dim1, rank))\n    X_plus = np.zeros((dim2, rank))\n    X_new_plus = np.zeros((dim2 + 1, rank))\n    A_plus = np.zeros((rank, rank, d))\n    mat_hat_plus = np.zeros((dim1, dim2 + 1))\n    for iters in range(maxiter1):\n        W_bar = np.mean(W, axis = 0)\n        var_mu_hyper = (dim1 * W_bar)/(dim1 + beta0)\n        var_W_hyper = inv(inv(W0) + cov_mat(W) + dim1 * beta0/(dim1 + beta0) * np.outer(W_bar, W_bar))\n        var_Lambda_hyper = wishart(df = dim1 + nu0, scale = var_W_hyper, seed = None).rvs()\n        var_mu_hyper = mvnrnd(var_mu_hyper, inv((dim1 + beta0) * var_Lambda_hyper))\n        \n        var1 = X.T\n        var2 = kr_prod(var1, var1)\n        var3 = tau * np.matmul(var2, binary_mat.T).reshape([rank, rank, dim1]) + np.dstack([var_Lambda_hyper] * dim1)\n        var4 = (tau * np.matmul(var1, sparse_mat.T)\n                + np.dstack([np.matmul(var_Lambda_hyper, var_mu_hyper)] * dim1)[0, :, :])\n        for i in range(dim1):\n            inv_var_Lambda = inv(var3[:, :, i])\n            W[i, :] = mvnrnd(np.matmul(inv_var_Lambda, var4[:, i]), inv_var_Lambda)\n        if iters + 1 > maxiter1 - maxiter2:\n            W_plus += W\n        \n        Z_mat = X[np.max(time_lags) : dim2, :]\n        Q_mat = np.zeros((dim2 - np.max(time_lags), rank * d))\n        for t in range(np.max(time_lags), dim2):\n            Q_mat[t - np.max(time_lags), :] = X[t - time_lags, :].reshape([rank * d])\n        var_Psi = inv(inv(Psi0) + np.matmul(Q_mat.T, Q_mat))\n        var_M = np.matmul(var_Psi, np.matmul(inv(Psi0), M0) + np.matmul(Q_mat.T, Z_mat))\n        var_S = (S0 + np.matmul(Z_mat.T, Z_mat) + np.matmul(np.matmul(M0.T, inv(Psi0)), M0) \n                 - np.matmul(np.matmul(var_M.T, inv(var_Psi)), var_M))\n        Sigma = invwishart(df = nu0 + dim2 - np.max(time_lags), scale = var_S, seed = None).rvs()\n        A = mat2ten(mnrnd(var_M, var_Psi, Sigma).T, np.array([rank, rank, d]), 0)\n        if iters + 1 > maxiter1 - maxiter2:\n            A_plus += A\n\n        Lambda_x = inv(Sigma)\n        var1 = W.T\n        var2 = kr_prod(var1, var1)\n        var3 = tau * np.matmul(var2, binary_mat).reshape([rank, rank, dim2]) + np.dstack([Lambda_x] * dim2)\n        var4 = tau * np.matmul(var1, sparse_mat)\n        for t in range(dim2):\n            Mt = np.zeros((rank, rank))\n            Nt = np.zeros(rank)\n            if t < np.max(time_lags):\n                Qt = np.zeros(rank)\n            else:\n                Qt = np.matmul(Lambda_x, np.matmul(ten2mat(A, 0), X[t - time_lags, :].reshape([rank * d])))\n            if t < dim2 - np.min(time_lags):\n                if t >= np.max(time_lags) and t < dim2 - np.max(time_lags):\n                    index = list(range(0, d))\n                else:\n                    index = list(np.where((t + time_lags >= np.max(time_lags)) & (t + time_lags < dim2)))[0]\n                for k in index:\n                    Ak = A[:, :, k]\n                    Mt += np.matmul(np.matmul(Ak.T, Lambda_x), Ak)\n                    A0 = A.copy()\n                    A0[:, :, k] = 0\n                    var5 = (X[t + time_lags[k], :] \n                            - np.matmul(ten2mat(A0, 0), X[t + time_lags[k] - time_lags, :].reshape([rank * d])))\n                    Nt += np.matmul(np.matmul(Ak.T, Lambda_x), var5)\n            var_mu = var4[:, t] + Nt + Qt\n            if t < np.max(time_lags):\n                inv_var_Lambda = inv(var3[:, :, t] + Mt - Lambda_x + np.eye(rank))\n            else:\n                inv_var_Lambda = inv(var3[:, :, t] + Mt)\n            X[t, :] = mvnrnd(np.matmul(inv_var_Lambda, var_mu), inv_var_Lambda)\n        mat_hat = np.matmul(W, X.T)\n        \n        X_new = np.zeros((dim2 + 1, rank))\n        if iters + 1 > maxiter1 - maxiter2:\n            X_new[0 : dim2, :] = X.copy()\n            X_new[dim2, :] = np.matmul(ten2mat(A, 0), X_new[dim2 - time_lags, :].reshape([rank * d]))\n            X_new_plus += X_new\n            mat_hat_plus += np.matmul(W, X_new.T)\n        \n        tau = np.random.gamma(alpha + 0.5 * sparse_mat[position].shape[0], \n                              1/(beta + 0.5 * np.sum((sparse_mat - mat_hat)[position] ** 2)))\n        rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n        if (iters + 1) % 200 == 0 and iters < maxiter1 - maxiter2:\n            print('Iter: {}'.format(iters + 1))\n            print('RMSE: {:.6}'.format(rmse))\n            print()\n\n    W = W_plus/maxiter2\n    X_new = X_new_plus/maxiter2\n    A = A_plus/maxiter2\n    mat_hat = mat_hat_plus/maxiter2\n    if maxiter1 >= 100:\n        final_mape = np.sum(np.abs(dense_mat[pos] - mat_hat[pos])/dense_mat[pos])/dense_mat[pos].shape[0]\n        final_rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n        print('Imputation MAPE: {:.6}'.format(final_mape))\n        print('Imputation RMSE: {:.6}'.format(final_rmse))\n        print()\n    \n    return mat_hat, W, X_new, A\n\n\nsparse_mat = dfdealMis\ndense_mat = dfdens\nimport time\nstart = time.time()\ndim1, dim2 = sparse_mat.shape\nrank = 10\ntime_lags = np.array([1, 2, (len(dfsecond)//28)])\ninit = {\"W\": 0.1 * np.random.rand(dim1, rank), \"X\": 0.1 * np.random.rand(dim2, rank)}\nmaxiter1 = 1100\nmaxiter2 = 100\na,b,c,d = BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2)\nend = time.time()\nprint('Running time: %d seconds'%(end - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After imputation we got 'a', which is the array contains all 'Depth_to_Groundwater' variable values \nafter filling in the missing values.\nWe use this array to replace the original data of 'Depth_to_Groundwater' variable in the dataset.\n1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.delete(a,-1,axis = 1)\ndfRainfall = dfsecond[rain_list].to_numpy()\ndfRainfall = np.delete(dfRainfall,range(len(dfsecond)-(len(dfsecond)//28)*28),axis = 0)\ndfTemp = dfsecond[Temp_list].to_numpy()\ndfTemp = np.delete(dfTemp,range(len(dfsecond)-(len(dfsecond)//28)*28),axis = 0)\npdate = pd.DataFrame(dfsecond['date'].values.astype('float32'), columns=['Date'])\ndfDate = pdate['Date'].to_numpy()\ndfDate = np.delete(dfDate,range(len(dfsecond)-(len(dfsecond)//28)*28),axis = 0)\ndfDate = dfDate.reshape(-1,1)\na = a.T\nwholedata = np.hstack((a,dfRainfall,dfTemp,dfDate))\nwholelist = Depth_list+rain_list+Temp_list+['date']\nnewFrame = DataFrame(wholedata,index=None,columns = wholelist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA\n\nWe found there are too many rainfall variables, so we decided to use PCA to reduce the number of these variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"test=newFrame[rain_list]\ntest = test.ffill().bfill()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\npca = PCA(n_components=10)\npca.fit(test)\nv = pca.explained_variance_ratio_.round(2)\n\nax.bar(range(1,11),v)\nplt.xlabel(\"PCA\")\nplt.title(\"variance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We chose the first two PCA to replace the rainfall variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"Xpca= PCA(n_components=2).fit_transform(test)\npf = pd.DataFrame(Xpca, columns=['PCA1','PCA2'])\nnewFrame['PCA1'] = pf['PCA1']\nnewFrame['PCA2'] = pf['PCA2']\nfor i in range(len(rain_list)):\n    newFrame = newFrame.drop(rain_list[i],axis=1)\nnewFrame = newFrame.ffill().bfill()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use \".shift(-28)\" to creat there new variables, which are the target variables for the prediction model of this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_target = []\nfor i in range(len(target_variable)):\n    newFrame[target_variable[i]+'28'] = newFrame[target_variable[i]].shift(-28)\n    lag_target.append(target_variable[i]+'28')\nnewFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation Matrix of the new dataset \n\nWe checked MIC values between all the variables, and delete those variables had higher MIC values with some other variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def MIC_matirx(dataframe, mine):\n\n    data_array = np.array(dataframe)\n    n = len(data_array[0, :])\n    output = np.zeros([n, n])\n\n    for i in range(n):\n        for j in range(n):\n            mine.compute_score(data_array[:, i], data_array[:, j])\n            output[i, j] = mine.mic()\n            output[j, i] = mine.mic()\n    mic_value = pd.DataFrame(output)\n    return mic_value\n\n\nmine = MINE(alpha=0.6, c=15)\nMatrix_mic_value = MIC_matirx(newFrame, mine)\n\ndef HeatMap(DataFrame):\n    %matplotlib inline\n    colormap = plt.cm.RdBu\n    plt.figure(figsize=(14,12))\n    plt.title('MIC', y=1.05, size=15)\n    sns.heatmap(DataFrame.astype(float),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n    plt.show()\nHeatMap(Matrix_mic_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newFrame.columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the MIC matrix above, we can delete columns named 'Temperature_Ponte_a_Moriano' and 'Temperature_Lucca_Orto_Botanico'."},{"metadata":{"trusted":true},"cell_type":"code","source":"newFrame = newFrame.drop([ 'Temperature_Ponte_a_Moriano','Temperature_Lucca_Orto_Botanico'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seasonal and Trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfseason = pd.Series(newFrame['Depth_to_Groundwater_LT2'].tolist(),index = newFrame['date'].tolist())\n\n\ndecomposition = seasonal_decompose(dfseason, model='additive',period = 365,two_sided = False)\ndecomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the ADF to check the Stationarity of the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"adfuller(dfseason)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfseasonshift = dfseason.shift(-1)\ndfseasondff = dfseason-dfseasonshift\ndfseasondff = dfseasondff.dropna(inplace=False)\nadfuller(dfseasondff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(dfseasondff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(dfseasondff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_name = newFrame.columns.values.tolist()\n\nDepth_list = [ a for a in columns_name if a.startswith('Depth')]\nTemp_list = [ a for a in columns_name if a.startswith('Temperature')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kalman filter"},{"metadata":{},"cell_type":"markdown","source":"Errors such as measurement errors will add some noise to the data. Which will affect the accuracy of prediction.\nWe used Kalman filter to remove this noise in Depth_to_Groundwater variables and temperature variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Kalman1D(data,damping=1):\n    observation_covariance = damping\n    first_value = data[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    first_value\n    kf = KalmanFilter(\n            initial_state_mean=first_value,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(data)\n    return pred_state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dffull = newFrame[:len(newFrame)-28]\norenArray = dffull['Temperature_Orentano'].to_numpy()\norenkal = Kalman1D(orenArray,0.1)\nplt.plot(np.array(list(range(len(newFrame)-28))),orenArray,label='measured')\nplt.plot(np.array(list(range(len(newFrame)-28))),orenkal,label='kal')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The orange line show the temperature value after the noise is eliminated.\nWe can see that it has become smoother than the blue line which contains the original value."},{"metadata":{"trusted":true},"cell_type":"code","source":"dffullkal = dffull.drop(Depth_list+Temp_list,axis =1 )\nfor i in range(len(Depth_list)):\n    DepthArray = dffull[Depth_list[i]].to_numpy()\n    Depthkal = Kalman1D(DepthArray,0.1)\n    kallist = map(lambda x: x[0], Depthkal)\n    Depthkalseries = pd.Series(kallist)\n    dffullkal[Depth_list[i]] = Depthkalseries\n\nfor i in range(len(Temp_list)):\n    TempArray = dffull[Temp_list[i]].to_numpy()\n    Tempkal = Kalman1D(TempArray,0.1)\n    kallist = map(lambda x: x[0], Tempkal)\n    Tempkalseries = pd.Series(kallist)\n    dffullkal[Temp_list[i]] = Tempkalseries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dffullkal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{},"cell_type":"markdown","source":"Recurrent Neural Network (RNN) is a neural network used to process sequence data. Compared with the general neural network, it can process the data of the sequence change.\nLong short-term memory (Long short-term memory, LSTM) is a special RNN, mainly to solve the problem of gradient disappearance and gradient explosion in the training process of long sequences. Simply put, LSTM can perform better in longer sequences than ordinary RNNs."},{"metadata":{"trusted":true},"cell_type":"code","source":"class lstm_reg(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n        super(lstm_reg, self).__init__()\n        \n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers,dropout = 0.3) \n        self.reg = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x, _ = self.rnn(x) \n        s, b, h = x.shape\n        x = x.view(s*b, h) \n        x = self.reg(x)\n        x = x.view(s, b, -1)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dffullkal.columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take 'Depth_to_Groundwater_SAL28' as the example.\nif we want to predict the value of 'Depth_to_Groundwater_SAL28', the features we need is 'date','PCA1','PCA2','Depth_to_Groundwater_SAL', 'Temperature_Orentano','Temperature_Monte_Serra'."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_test = int(((len(dffullkal)-28)/28)//5*28)\nn_train = len(dffullkal)-28 - n_test\n\ndftrain = dffullkal[:n_train]\ndftest = dffullkal[n_train:len(dffullkal)-28]\n\n\ndftrainX = dftrain[['date','PCA1','PCA2','Depth_to_Groundwater_SAL', 'Temperature_Orentano','Temperature_Monte_Serra']]\nn_feature = len(dftrainX.columns.values.tolist())\ndflistX = np.reshape(dftrainX.values.tolist(),(28,-1,n_feature))\n\ndftrainY = dftrain['Depth_to_Groundwater_SAL28']\ndflistY = np.reshape(dftrainY.values.tolist(),(28,-1,1))\ndflistX = dflistX.astype('float32')\ndflistY = dflistY.astype('float32')\ntensorx = torch.from_numpy(dflistX)\ntensory = torch.from_numpy(dflistY)\n\nnet = lstm_reg(n_feature, 100)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\nfor e in range(100):\n    var_x = Variable(tensorx)\n    var_y = Variable(tensory)\n\n    out = net(var_x)\n    loss = criterion(out, var_y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.1)\n\n    optimizer.step()\n    if (e + 1) % 10 == 0: \n            print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.data))\n                \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftestX = dftest[['date','PCA1','PCA2','Depth_to_Groundwater_SAL', 'Temperature_Orentano','Temperature_Monte_Serra']]\ndftestlistX = np.reshape(dftestX.values.tolist(),(28,-1,n_feature))\n\ndftestY = dftest['Depth_to_Groundwater_SAL28']\ndftestlistY = np.reshape(dftestY.values.tolist(),(28,-1,1))\ndftestlistX = dftestlistX.astype('float32')\ndftestlistY = dftestlistY.astype('float32')\ntensortestx = torch.from_numpy(dftestlistX)\ntensortesty = torch.from_numpy(dftestlistY)\ntestvar_x = Variable(tensortestx)\ntestvar_y = Variable(tensortesty)\n\nnettest = net.eval()\npred_teste = nettest(testvar_x)\nloss = criterion(pred_teste, testvar_y)\nprint('Epoch: {}, Loss: {:.5f}'.format('mse', loss.data))\n\na = nn.L1Loss()\nmaeloss = a(pred_teste, testvar_y)\nprint('Epoch: {}, Loss: {:.5f}'.format('mae', maeloss.data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Method"},{"metadata":{},"cell_type":"markdown","source":"referance table\n\nWe used the missingno library to visualize the missing values of each table, and make a line chart to observe the distribution of missing values, and finally determine the range of data used to build the prediction model in each table.\nWe refer to the introduction of each table in the 'datasets_description.xlsx' to determine the output of each table and the variables(except for the outputs themselves) that may be used to predict these outputs.\nWe make this information into a table so that it can be used when needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"referdata = {'table':['Aquifer_Auser','Aquifer_Doganella','Aquifer_Luco',\n                     'Aquifer_Petrignano','Lake_Bilancino','River_Arno',\n                      'Water_Spring_Amiata','Water_Spring_Lupa','Water_Spring_Madonna_di_Canneto'],\n       'start':[4685,3075,6540,1000,1000,2250,5600,600,1600],#Get data begin from which row\n       'end':[7000,3950,6950,5223,6000,3450,7487,4199,2500],#Stop getting data after reaching which row\n        'feature':[['Rain','Temperature','date'],['Rain','Temperature','date'],\n                  ['Rain','Temperature','date'],['Rain','Temperature','date'],\n                  ['Rain','Temperature','date'],['Rain','Temperature','date'],\n                  ['Rain','Temperature','date'],['Rain','date'],['Rain','Temperature','date']]}\nreferdf = DataFrame(referdata)\nreferdf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create a class to collect all the methods of processing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class data_cook():\n    \n    \n\n    def __init__(self, dataframe, start,end,target_variable):\n        '''\n        target_variable contains the name of all the variables which cound be used as target variable in this table.\n        '''\n        \n        self.dfsecond = dataframe[start:end]# the start and end number could be check in the table 'referdf'.\n        self.columns_name = dataframe.columns.values.tolist()\n        self.Rain_list = [ a for a in self.columns_name if a.startswith('Rain')]\n        self.Depth_list = [ a for a in self.columns_name if a.startswith('Depth')]\n        self.Temp_list = [ a for a in self.columns_name if a.startswith('Temperature')]\n        self.Flow_list = [ a for a in self.columns_name if a.startswith('Flow')]\n        self.n_row = dataframe.shape[0]\n        self.target_list = target_variable\n    \n    def BasicInformation(self):\n        msno.matrix(self.dfsecond)\n        print(self.columns_name)\n        \n    #BTMF\n    def FillNullBTMF(self,nullValue_list):\n        '''\n        the nullValue_list contain the columns' name which we want to fill the null value.\n        '''\n        for i in range(len(self.Depth_list)):\n            self.dfsecond[self.dfsecond[[self.Depth_list[i]]]==0]=np.nan\n    \n        for i in range(len(self.Temp_list)):\n            self.dfsecond[self.dfsecond[[self.Temp_list[i]]]==0]=np.nan\n        \n        for i in range(len(self.Flow_list)):\n            self.dfsecond[self.dfsecond[[self.Flow_list[i]]]==0]=np.nan\n        \n        dfmeasure = self.dfsecond[nullValue_list]\n        dfdens = dfmeasure[nullValue_list]\n        for i in range(len(nullValue_list)):\n            dfdens[nullValue_list[i]] = dfdens[nullValue_list[i]].interpolate()\n\n        dfdens = dfdens.ffill().bfill()\n        dfdens = np.delete(dfdens.to_numpy().T,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 1)\n        dfdealMis = np.delete(dfmeasure.fillna(0).to_numpy().T,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 1)\n\n        def kr_prod(a, b):\n            return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)\n\n        def cov_mat(mat):\n            dim1, dim2 = mat.shape\n            new_mat = np.zeros((dim2, dim2))\n            mat_bar = np.mean(mat, axis = 0)\n            for i in range(dim1):\n                new_mat += np.einsum('i, j -> ij', mat[i, :] - mat_bar, mat[i, :] - mat_bar)\n            return new_mat\n\n        def ten2mat(tensor, mode):\n            return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n\n        def mat2ten(mat, tensor_size, mode):\n            index = list()\n            index.append(mode)\n            for i in range(tensor_size.shape[0]):\n                if i != mode:\n                    index.append(i)\n            return np.moveaxis(np.reshape(mat, list(tensor_size[index]), order = 'F'), 0, mode)\n\n        def mnrnd(M, U, V):\n            \"\"\"\n            Generate matrix normal distributed random matrix.\n            M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n            \"\"\"\n            dim1, dim2 = M.shape\n            X0 = np.random.rand(dim1, dim2)\n            P = np.linalg.cholesky(U)\n            Q = np.linalg.cholesky(V)\n            return M + np.matmul(np.matmul(P, X0), Q.T)\n\n        def BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2):\n            \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n            W = init[\"W\"]\n            X = init[\"X\"]\n\n            d = time_lags.shape[0]\n            dim1, dim2 = sparse_mat.shape\n            pos = np.where((dense_mat != 0) & (sparse_mat == 0))\n            position = np.where(sparse_mat != 0)\n            binary_mat = np.zeros((dim1, dim2))\n            binary_mat[position] = 1\n\n            beta0 = 1\n            nu0 = rank\n            mu0 = np.zeros((rank))\n            W0 = np.eye(rank)\n            tau = 1\n            alpha = 1e-6\n            beta = 1e-6\n            S0 = np.eye(rank)\n            Psi0 = np.eye(rank * d)\n            M0 = np.zeros((rank * d, rank))\n\n            W_plus = np.zeros((dim1, rank))\n            X_plus = np.zeros((dim2, rank))\n            X_new_plus = np.zeros((dim2 + 1, rank))\n            A_plus = np.zeros((rank, rank, d))\n            mat_hat_plus = np.zeros((dim1, dim2 + 1))\n            for iters in range(maxiter1):\n                W_bar = np.mean(W, axis = 0)\n                var_mu_hyper = (dim1 * W_bar)/(dim1 + beta0)\n                var_W_hyper = inv(inv(W0) + cov_mat(W) + dim1 * beta0/(dim1 + beta0) * np.outer(W_bar, W_bar))\n                var_Lambda_hyper = wishart(df = dim1 + nu0, scale = var_W_hyper, seed = None).rvs()\n                var_mu_hyper = mvnrnd(var_mu_hyper, inv((dim1 + beta0) * var_Lambda_hyper))\n\n                var1 = X.T\n                var2 = kr_prod(var1, var1)\n                var3 = tau * np.matmul(var2, binary_mat.T).reshape([rank, rank, dim1]) + np.dstack([var_Lambda_hyper] * dim1)\n                var4 = (tau * np.matmul(var1, sparse_mat.T)\n                        + np.dstack([np.matmul(var_Lambda_hyper, var_mu_hyper)] * dim1)[0, :, :])\n                for i in range(dim1):\n                    inv_var_Lambda = inv(var3[:, :, i])\n                    W[i, :] = mvnrnd(np.matmul(inv_var_Lambda, var4[:, i]), inv_var_Lambda)\n                if iters + 1 > maxiter1 - maxiter2:\n                    W_plus += W\n\n                Z_mat = X[np.max(time_lags) : dim2, :]\n                Q_mat = np.zeros((dim2 - np.max(time_lags), rank * d))\n                for t in range(np.max(time_lags), dim2):\n                    Q_mat[t - np.max(time_lags), :] = X[t - time_lags, :].reshape([rank * d])\n                var_Psi = inv(inv(Psi0) + np.matmul(Q_mat.T, Q_mat))\n                var_M = np.matmul(var_Psi, np.matmul(inv(Psi0), M0) + np.matmul(Q_mat.T, Z_mat))\n                var_S = (S0 + np.matmul(Z_mat.T, Z_mat) + np.matmul(np.matmul(M0.T, inv(Psi0)), M0) \n                         - np.matmul(np.matmul(var_M.T, inv(var_Psi)), var_M))\n                Sigma = invwishart(df = nu0 + dim2 - np.max(time_lags), scale = var_S, seed = None).rvs()\n                A = mat2ten(mnrnd(var_M, var_Psi, Sigma).T, np.array([rank, rank, d]), 0)\n                if iters + 1 > maxiter1 - maxiter2:\n                    A_plus += A\n\n                Lambda_x = inv(Sigma)\n                var1 = W.T\n                var2 = kr_prod(var1, var1)\n                var3 = tau * np.matmul(var2, binary_mat).reshape([rank, rank, dim2]) + np.dstack([Lambda_x] * dim2)\n                var4 = tau * np.matmul(var1, sparse_mat)\n                for t in range(dim2):\n                    Mt = np.zeros((rank, rank))\n                    Nt = np.zeros(rank)\n                    if t < np.max(time_lags):\n                        Qt = np.zeros(rank)\n                    else:\n                        Qt = np.matmul(Lambda_x, np.matmul(ten2mat(A, 0), X[t - time_lags, :].reshape([rank * d])))\n                    if t < dim2 - np.min(time_lags):\n                        if t >= np.max(time_lags) and t < dim2 - np.max(time_lags):\n                            index = list(range(0, d))\n                        else:\n                            index = list(np.where((t + time_lags >= np.max(time_lags)) & (t + time_lags < dim2)))[0]\n                        for k in index:\n                            Ak = A[:, :, k]\n                            Mt += np.matmul(np.matmul(Ak.T, Lambda_x), Ak)\n                            A0 = A.copy()\n                            A0[:, :, k] = 0\n                            var5 = (X[t + time_lags[k], :] \n                                    - np.matmul(ten2mat(A0, 0), X[t + time_lags[k] - time_lags, :].reshape([rank * d])))\n                            Nt += np.matmul(np.matmul(Ak.T, Lambda_x), var5)\n                    var_mu = var4[:, t] + Nt + Qt\n                    if t < np.max(time_lags):\n                        inv_var_Lambda = inv(var3[:, :, t] + Mt - Lambda_x + np.eye(rank))\n                    else:\n                        inv_var_Lambda = inv(var3[:, :, t] + Mt)\n                    X[t, :] = mvnrnd(np.matmul(inv_var_Lambda, var_mu), inv_var_Lambda)\n                mat_hat = np.matmul(W, X.T)\n\n                X_new = np.zeros((dim2 + 1, rank))\n                if iters + 1 > maxiter1 - maxiter2:\n                    X_new[0 : dim2, :] = X.copy()\n                    X_new[dim2, :] = np.matmul(ten2mat(A, 0), X_new[dim2 - time_lags, :].reshape([rank * d]))\n                    X_new_plus += X_new\n                    mat_hat_plus += np.matmul(W, X_new.T)\n\n                tau = np.random.gamma(alpha + 0.5 * sparse_mat[position].shape[0], \n                                      1/(beta + 0.5 * np.sum((sparse_mat - mat_hat)[position] ** 2)))\n                rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n                if (iters + 1) % 200 == 0 and iters < maxiter1 - maxiter2:\n                    print('Iter: {}'.format(iters + 1))\n                    print('RMSE: {:.6}'.format(rmse))\n                    print()\n\n            W = W_plus/maxiter2\n            X_new = X_new_plus/maxiter2\n            A = A_plus/maxiter2\n            mat_hat = mat_hat_plus/maxiter2\n            if maxiter1 >= 100:\n                final_mape = np.sum(np.abs(dense_mat[pos] - mat_hat[pos])/dense_mat[pos])/dense_mat[pos].shape[0]\n                final_rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos]) ** 2)/dense_mat[pos].shape[0])\n                print('Imputation MAPE: {:.6}'.format(final_mape))\n                print('Imputation RMSE: {:.6}'.format(final_rmse))\n                print()\n\n            return mat_hat, W, X_new, A\n\n        sparse_mat = dfdealMis\n        dense_mat = dfdens\n        if (np.isnan(sparse_mat).any()==False):\n            self.dfsecond = self.dfsecond.reset_index(drop = True)\n            pdate = pd.DataFrame(self.dfsecond['date'].values.astype('float32'), columns=['Datefloat'])\n            self.dfsecond['Datefloat'] = pdate['Datefloat']\n            return self.dfsecond\n        start = time.time()\n        dim1, dim2 = sparse_mat.shape\n        rank = 10\n        time_lags = np.array([1, 2, (len(self.dfsecond)//28)])\n        init = {\"W\": 0.1 * np.random.rand(dim1, rank), \"X\": 0.1 * np.random.rand(dim2, rank)}\n        maxiter1 = 1100\n        maxiter2 = 100\n        a,b,c,d = BTMF(dense_mat, sparse_mat, init, rank, time_lags, maxiter1, maxiter2)\n        end = time.time()\n        print('Running time: %d seconds'%(end - start))\n\n        a = np.delete(a,-1,axis = 1)\n        dfRainfall = self.dfsecond[self.Rain_list].to_numpy()\n        dfRainfall = np.delete(dfRainfall,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 0)\n        dfFlow = self.dfsecond[self.Flow_list].to_numpy()\n        dfFlow = np.delete(dfFlow,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 0)\n        dfTemp = self.dfsecond[self.Flow_list].to_numpy()\n        dfTemp = np.delete(dfTemp,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 0)\n        pdate = pd.DataFrame(self.dfsecond['date'].values.astype('float32'), columns=['Datefloat'])\n        dfDate = pdate['Datefloat'].to_numpy()\n        dfDate = np.delete(dfDate,range(len(self.dfsecond)-(len(self.dfsecond)//28)*28),axis = 0)\n        dfDate = dfDate.reshape(-1,1)\n        a = a.T\n        wholedata = np.hstack((a,dfRainfall,dfTemp,dfDate))\n        wholelist = self.Depth_list+self.Rain_list+self.Temp_list+['Datefloat']\n        newFrame = DataFrame(wholedata,index=None,columns = wholelist)\n        self.dfsecond = newFrame\n        self.dfsecond = self.dfsecond.reset_index(drop = True)\n        return self.dfsecond\n\n    def PCA_trans(self,feature_list):\n        if len(feature_list)<=2:\n            return self.dfsecond\n        test=self.dfsecond[feature_list].ffill().bfill()\n        Xpca= PCA(n_components=2).fit_transform(test)\n        pf = pd.DataFrame(Xpca, columns=['PCA1','PCA2'])\n        self.dfsecond['PCA1'] = pf['PCA1']\n        self.dfsecond['PCA2'] = pf['PCA2']\n        for i in range(len(feature_list)):\n            self.dfsecond = self.dfsecond.drop(feature_list[i],axis=1)\n        self.dfsecond = self.dfsecond.ffill().bfill()\n        self.PCA_list = ['PCA1','PCA2']\n        return self.dfsecond\n    \n    \n\n    #use.shift(-28) made target variable\n    def target_made(self,potential_list):\n        self.lag_target=[]\n        '''\n        potential_list contains the name of all the variables which cound be seen as output in this table.\n        '''\n        for i in range(len(potential_list)):\n            name = potential_list[i]+'28'\n            self.dfsecond[name] = self.dfsecond[potential_list[i]].shift(-28)\n            self.lag_target.append(name)      \n        return self.dfsecond\n\n    \n    def MICMethod(self):\n        '''\n        use MICMethod to delete those variables which have higher MIC values with some other variables.\n        '''\n        mine = MINE(alpha=0.6, c=15)\n        deldep_feature =[]\n        deltem_feature =[]\n        delflow_feature = []\n        \n        if((len(self.Depth_list)!=0)&(self.target_list[0] not in self.Depth_list)):\n            dataDepth = self.dfsecond[self.Depth_list]\n            data_array = np.array(dataDepth)\n            n = len(data_array[0, :])\n            for i in range(n):\n                for j in range(n):\n                    mine.compute_score(data_array[:, i], data_array[:, j])\n                    if((mine.mic()>=0.9)&(i!=j)):\n                        if (self.Depth_list[j] not in deldep_feature):\n                            deldep_feature.append(self.Depth_list[i])\n                            break\n        \n        if(len(self.Temp_list)!=0):\n            dataTem = self.dfsecond[self.Temp_list]\n            data_array = np.array(dataTem)\n            n = len(data_array[0, :])\n            for i in range(n):\n                for j in range(n):\n                    mine.compute_score(data_array[:, i], data_array[:, j])\n                    if((mine.mic()>=0.9)&(i!=j)):\n                        if (self.Temp_list[j] not in deltem_feature):\n                            deltem_feature.append(self.Temp_list[i])\n                            break\n        \n        if((len(self.Flow_list)!=0)&(self.target_list[0] not in self.Flow_list)):\n            dataflow = self.dfsecond[self.Flow_list]\n            data_array = np.array(dataflow)\n            n = len(data_array[0, :])\n            for i in range(n):\n                for j in range(n):\n                    mine.compute_score(data_array[:, i], data_array[:, j])\n                    if((mine.mic()>=0.9)&(i!=j)):\n                        if (self.Flow_list[j] not in delflow_feature):\n                            delflow_feature.append(self.Flow_list[i])\n                            break\n        \n        if len(self.PCA_list): \n            datapca = self.dfsecond[['PCA1','PCA2']]\n            data_array = np.array(datapca)\n            mine.compute_score(data_array[:, 0], data_array[:, 1])\n            if(mine.mic()>=0.9):\n                delpca_feature = ['PCA2']\n        self.dfsecond = self.dfsecond.drop(deldep_feature+deltem_feature+delpca_feature+delflow_feature,axis =1)\n        \n        for i in range(len(deldep_feature)):\n            self.Depth_list.remove(deldep_feature[i])\n        for i in range(len(deltem_feature)):\n            self.Temp_list.remove(deltem_feature[i])\n        for i in range(len(delflow_feature)):\n            self.Flow_list.remove(delflow_feature[i])\n        self.PCA_list = ['PCA1']  \n        return self.dfsecond\n    \n    def KalmanCook(self):\n        '''\n        used Kalman filter to remove noise in Depth_to_Groundwater,flow, and temperature variables.\n        '''\n        def Kalman1D(data,damping=1):\n            observation_covariance = damping\n            first_value = data[0]\n            transition_matrix = 1\n            transition_covariance = 0.1\n            first_value\n            kf = KalmanFilter(\n                    initial_state_mean=first_value,\n                    initial_state_covariance=observation_covariance,\n                    observation_covariance=observation_covariance,\n                    transition_covariance=transition_covariance,\n                    transition_matrices=transition_matrix\n                )\n            pred_state, state_cov = kf.smooth(data)\n            return pred_state\n        \n        dfreborn = self.dfsecond.drop(self.Depth_list+self.Temp_list+self.Flow_list,axis =1)\n        for i in range(len(self.Depth_list)):\n            tryArray = self.dfsecond[self.Depth_list[i]].to_numpy()\n            trykal = Kalman1D(tryArray,0.1)\n            kallist = map(lambda x: x[0], trykal)\n            trykalseries = pd.Series(kallist)\n            dfreborn[self.Depth_list[i]] = trykalseries\n\n        for i in range(len(self.Temp_list)):\n            tryArray = self.dfsecond[self.Temp_list[i]].to_numpy()\n            trykal = Kalman1D(tryArray,0.1)\n            kallist = map(lambda x: x[0], trykal)\n            trykalseries = pd.Series(kallist)\n            dfreborn[self.Temp_list[i]] = trykalseries\n            \n        for i in range(len(self.Flow_list)):\n            tryArray = self.dfsecond[self.Flow_list[i]].to_numpy()\n            trykal = Kalman1D(tryArray,0.1)\n            kallist = map(lambda x: x[0], trykal)\n            trykalseries = pd.Series(kallist)\n            dfreborn[self.Flow_list[i]] = trykalseries\n        self.dfsecond = dfreborn\n        return self.dfsecond,self.lag_target\n    \n    \n    def LSTMGo(self,target_variable):\n        '''\n        Target_variable is the name of the variable which would be used as dependent variable in the LSTM model.\n        This method will print out the results of the training phase and the test phase, and return the forcast\n        results of the last 28 days.\n        '''\n        self.target_variable = target_variable\n        self.n_test = int(((len(self.dfsecond)-28)/28)//5*28)\n        self.n_train = int(((len(self.dfsecond)-28)/28)//5*4*28)\n\n        self.dftrain = self.dfsecond[:self.n_train]\n        self.dftest = self.dfsecond[self.n_train:self.n_test+self.n_train]\n        \n        if(target_variable.startswith('Depth')):\n            fake_target = [a for a in self.Depth_list if target_variable.startswith(a)][0]\n        else:\n            fake_target = [a for a in self.Flow_list if target_variable.startswith(a)][0]\n        \n        self.feature_name = ['Datefloat']+self.PCA_list+self.Temp_list+[fake_target]\n        dftrainX = self.dftrain[self.feature_name]\n        n_feature = len(dftrainX.columns.values.tolist())\n        dflistX = np.reshape(dftrainX.values.tolist(),(28,-1,n_feature))\n\n        dftrainY = self.dftrain[target_variable]\n        dflistY = np.reshape(dftrainY.values.tolist(),(28,-1,1))\n        dflistX = dflistX.astype('float32')\n        dflistY = dflistY.astype('float32')\n        tensorx = torch.from_numpy(dflistX)\n        tensory = torch.from_numpy(dflistY)\n\n        net = lstm_reg(n_feature, 100)\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\n        for e in range(100):\n            var_x = Variable(tensorx)\n            var_y = Variable(tensory)\n\n            out = net(var_x)\n            loss = criterion(out, var_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.1)#gradient clipping, used to avoid Exploding Gradients \n\n            optimizer.step()\n            if (e + 1) % 10 == 0: \n                    print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.data))\n        \n    \n        dftestX = self.dftest[self.feature_name]\n        n_feature = len(dftestX.columns.values.tolist())\n        dftestlistX = np.reshape(dftestX.values.tolist(),(28,-1,n_feature))\n\n        dftestY = self.dftest[self.target_variable]\n        dftestlistY = np.reshape(dftestY.values.tolist(),(28,-1,1))\n        dftestlistX = dftestlistX.astype('float32')\n        dftestlistY = dftestlistY.astype('float32')\n        tensortestx = torch.from_numpy(dftestlistX)\n        tensortesty = torch.from_numpy(dftestlistY)\n        testvar_x = Variable(tensortestx)\n        testvar_y = Variable(tensortesty)\n\n        nettest = net.eval()\n        pred_teste = nettest(testvar_x)\n        loss = criterion(pred_teste, testvar_y)\n        print('Epoch: {}, Loss: {:.5f}'.format('mse', loss.data))\n\n        a = nn.L1Loss()\n        maeloss = a(pred_teste, testvar_y)\n        print('Epoch: {}, Loss: {:.5f}'.format('mae', maeloss.data))\n        \n        dfpre = self.dfsecond.tail(28)\n        dfpreX = dfpre[self.feature_name]\n        n_feature = len(dfpreX.columns.values.tolist())\n        dfprelistX = np.reshape(dfpreX.values.tolist(),(28,-1,n_feature))\n        dfprelistX = dfprelistX.astype('float32')\n        tensorprex = torch.from_numpy(dfprelistX)\n        prevar_x = Variable(tensorprex)\n        preY= net(prevar_x)\n        return preY\n        \n\nclass lstm_reg(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n            super(lstm_reg, self).__init__()\n\n            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,dropout = 0.3) \n            self.reg = nn.Linear(hidden_size, output_size)\n\n        def forward(self, x):\n            x, _ = self.rnn(x) \n            s, b, h = x.shape\n            x = x.view(s*b, h) \n            x = self.reg(x)\n            x = x.view(s, b, -1)\n            return x\n\n        def output_y_hc(self, x, hc):\n            y, hc = self.rnn(x, hc)  # y, (h, c) = self.rnn(x)\n            s, b, h = y.size()\n            y = y.view(s*b, h)\n            y = self.reg(y)\n            y = y.view(s, b, -1)\n            return y, hc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2- Random Forest**"},{"metadata":{},"cell_type":"markdown","source":"**2.1- Predicting the Depth_to_Groundwater_SAL 28**\n\ndftrain containing the training dataset while dftest containing the testing dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nn_train = len(dffullkal)-28 - n_test\n\ndftrain = dffullkal[:n_train]\ndftest = dffullkal[n_train:len(dffullkal)-28]\n\ndftrainX = dftrain[['date','PCA1','PCA2','Depth_to_Groundwater_SAL', 'Temperature_Orentano','Temperature_Monte_Serra']]\nn_feature = len(dftrainX.columns.values.tolist())\ndflistX = np.reshape(dftrainX.values.tolist(),(28,-1,n_feature))\n\ndftrainY = dftrain['Depth_to_Groundwater_SAL28']\ndflistY = np.reshape(dftrainY.values.tolist(),(28,-1,1))\ndflistX = dflistX.astype('float32')\ndflistY = dflistY.astype('float32')\ntensorx = torch.from_numpy(dflistX)\ntensory = torch.from_numpy(dflistY)\ndftestX = dftest[['date','PCA1','PCA2','Depth_to_Groundwater_SAL', 'Temperature_Orentano','Temperature_Monte_Serra']]\ndftestlistX = np.reshape(dftestX.values.tolist(),(28,-1,n_feature))\n\ndftestY = dftest['Depth_to_Groundwater_SAL28']\ndftestlistY = np.reshape(dftestY.values.tolist(),(28,-1,1))\ndftestlistX = dftestlistX.astype('float32')\ndftestlistY = dftestlistY.astype('float32')\ntensortestx = torch.from_numpy(dftestlistX)\ntensortesty = torch.from_numpy(dftestlistY)\ntestvar_x = Variable(tensortestx)\ntestvar_y = Variable(tensortesty)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(dftrainX, dftrainY);\npredictions = rf.predict(dftestX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE\nprint('RMSE for predicting the Depth_to_Groundwater_SAL28 using the Random Forest')\nRMSE= mean_squared_error(dftestY, predictions, squared=False)\nprint(RMSE)\n#MAE\nprint('MAE for predicting the Depth_to_Groundwater_SAL28 using the Random Forest')\nMAE=mean_absolute_error(dftestY, predictions)\nprint(MAE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(dftestY,predictions,'bo')\nplt.grid()\nplt.xlabel('True')\nplt.ylabel('Predicted')\nplt.title('Predicting the Depth_to_Groundwater_SAL 28')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3- The Steepest Descent algorithm**\n\n**3.1- Predicting the Depth_to_Groundwater_SAL28**"},{"metadata":{"trusted":true},"cell_type":"code","source":"w_hat_old = np.ones(len(dffullkal.columns))\nrd = [random.randint(1,100) for i in range(1,9)]\nw_hat =np.array(rd) # generating the same random value\ny_hat_trainSAL= np.dot(dftrainX,w_hat_old)\n# calculating the gradient \nX_train_transposeSAL = dftrainX.T\ngradient_w_hat = -2*np.dot(dftrainX.T,dftrainY) + 2*np.dot(dftrainX.T,np.dot(dftrainX, w_hat))\n# hessian matrix at point w_hat \nHessian= 4*np.dot( X_train_transposeSAL,dftrainX)\niterations=0\nmax_iterations = 1e4\nold_error=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%\n# using the Steepest Descent algorithm \nw_hat_old = np.ones(len(dftrainX.columns))\nrd = [random.randint(1,100) for i in range(1,9)]\nw_hat =np.array(rd) # generating the same random value\ny_hat_trainSAL= np.dot(dftrainX,w_hat_old)\n# calculating the gradient \nX_train_transposeSAL = dftrainX.T\ngradient_w_hat = -2*np.dot(dftrainX.T,dftrainY) + 2*np.dot(dftrainX.T,np.dot(dftrainX, w_hat))\n# hessian matrix at point w_hat \nHessian= 4*np.dot( X_train_transposeSAL,dftrainX)\niterations=0\nmax_iterations = 1e4\nold_error=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while np.linalg.norm(w_hat- w_hat_old) > 1e-8 and iterations < max_iterations:\n    iterations += 1\n    #old_error += [train_error]                                           \n    w_hat_old=w_hat\n    gamma=np.linalg.norm(gradient_w_hat)**2/np.dot(np.dot(gradient_w_hat.T,Hessian),gradient_w_hat)\n    w_hat = w_hat - gamma * gradient_w_hat # update the guess \n    #train_error=np.linalg.norm(np.dot(X_train,w_hat)- y_train)**2\n    #hyp=np.dot(X_train, w_hat)\n    #v = -y_train+hyp\n    gradient_w_hat = -2*np.dot(dftrainX.T,dftrainY ) + 2*np.dot(dftrainX.T,np.dot(dftrainX, w_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_trainSAL= np.dot(dftrainX,w_hat)\n\ny_hat_testSAL = np.dot(dftestX,w_hat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MAE\nprint('MAE for predicting the Depth_to_Groundwater_SAL28 using the Steepest Descent in testing phase')\nMAE=mean_absolute_error(dftestY, y_hat_testSAL)\nprint(MAE)\n\n#RMSE\nprint('RMSE for predicting the Depth_to_Groundwater_SAL28 using the Steepest Descent in testing phase')\nRMSE=mean_squared_error(dftestY, y_hat_testSAL, squared=False)\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(dftestY,y_hat_testSAL,'bo')\nplt.grid()\nplt.xlabel('True')\nplt.ylabel('Predicted')\nplt.title('Depth_to_Groundwater_SAL28 prediction with Steepest Descent Algorithm ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# recommend"},{"metadata":{},"cell_type":"markdown","source":"We recommend using the LSTM model, which has more stable results and higher accuracy for large amounts of data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}