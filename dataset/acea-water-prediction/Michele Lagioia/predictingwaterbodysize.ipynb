{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nThis competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody.The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided). \n\nThe goal is to create four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) to predict the amount of water in each unique waterbody for a set time interval.  \nThe predictive power of the models shall be evaluated with both Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).\n\nA typical pplication scenario is to predict in advance low or high levels of water availability of waterbody in order to start remediation actions as soon as possible to reduce water consumption such as communications to citizens or water rationing.\n\n\n### Outcome of this notebook\nThis notebook provides with extensive data cleaning and exploratory data analysis on all the variables of the 9 waterbodies. \nA mathematical model forecasting waterbodies’ water availability in terms of groundwater depth for **aquifer “Petrignano”**. The time interval is defined as **days**. Due to lack of time, the same model or other models were not explored and applied to other waterbodies.\nThis notebook contains two parts: \n\n1.\t**Explore and Clean Data**: All individual waterbodies’ variables are analyzed in depth in order to clean them from missing or erroneous values. Historical time intervals with missing target variables or insufficient independent variables have been removed. Moreover, the variable “depth at Podere Casetta” had missing values for the latest period (2020). Since it was a target variable for “Aquifer Luco”, I preferred not to remove this time interval, in order to later create a model on the most recent data. So, I used an autoregressive model to predict the variable in 2020 based on historical values of the same variable. Similarly, for water spring “Madonna di Canneto” the rainfall “Settefrati”, an important independent variable, had missing values for the entire 2019-2020. Removing this variable may have been compromised the utility of the model, therefore, I have used machine learning to reconstruct such a variable from other variables. \n2.\t**Machine Learning for Prediction**: The objective of this part is to produce prediction models by relying on cleaned data from the previous part and by using a standard process for machine learning on time series data. The process considers training (to cross validate and choose parameters for each of 3 different algorithms), validation (to select the best algorithm) and test data (to achieve the final scores). Because of lack of time, only one waterbody was modeled: aquifer “Petrignano”. The achieved MSE and MAE scores for one of the two target variables (`depth_to_groundwater_p24`) was 0.14 for 1-day prediction, raising up to 0.37 for a 30-days prediction, while MAE went from 0.10 to 0.30 for the same prediction interval. The scores were obtained by executing the model on *previously unseen* test data from June 2019 to June 2020. Considering that the variable had an excursion from 24.5 to 27.5 in the same period, the mean error scores show good prediction performance.\nFuture work should apply the same process to other waterbodies, so that each different kind/category of waterbody (Aquifer, Water Spring, River and Lake) has its own model, each applicable to the single waterbody (Auser, Amiata, Petrignano, Doganella, Luco, Madonna di Canneto, Lupa, Arno, Bilancino).\n","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"## Load data and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install odfpy\n#!pip install pystan==2.19.1.1\n#!pip install fbprophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nfrom scipy import stats\n%matplotlib inline\n\n\ndata_descr = pd.read_excel(\"/kaggle/input/acea-water-prediction/datasets_description.ods\", engine=\"odf\")\n\n### There is no output information for databases \"Madonna di Canneto\" and \"Lupa\". So, let's add it manually. \n### Moreover, let's tranform the \"Output\" into a list of lower-case target variables to avoid problems with case sensitivity later on.\n\ndata_descr[\"Output\"][data_descr[\"Database\"]==\"Water_Spring_Lupa\"] = \"Flow_Rate_Lupa\"\ndata_descr[\"Output\"][data_descr[\"Database\"]==\"Water_Spring_Madonna_di_Canneto\"]= \"Flow_Rate_Madonna_di_Canneto\"\n\n\ndata_descr[\"Output\"] = data_descr[\"Output\"].apply(lambda x: x.lower())\ndata_descr[\"Output\"] = data_descr[\"Output\"].apply(lambda x: x.replace(\" \",\"\").split(\",\"))\n\ndata_descr = data_descr.reindex(columns = ['type','name']+data_descr.columns.tolist() )\ndata_descr['type']=\"\"\ndata_descr['name']=\"\"\n\nfor ii, data in data_descr.iterrows(): \n    name = data[\"Database\"]\n    if name.startswith(\"Water\"):\n        data[\"type\"] = \"Water_Spring\"\n        data[\"name\"] = name.replace(\"Water_Spring_\",\"\").strip()\n    else:\n        data[\"type\"], data[\"name\"] = name.strip().split(\"_\")\n    \ndata_descr.drop(columns=[\"Database\"],inplace=True)\ndata_descr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the database information more closely by translating it into human readable format"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_description(db,description,output):\n    \"\"\"\n    Print each row of the database in human readable format.\n    \"\"\"\n    print('\\033[1m%s\\033[0m\\n'%db)\n    print('%s\\n'%description)\n    print('Output:\\n')\n    for param in str(output).split(\",\"):\n        print(\"\\u2022 %s\\n\"%param)\n\nfor ind, series in data_descr.iterrows():\n    args = list(series)\n    print_description(args[0],args[1],args[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the name of the datasets, let's use the names to load the datasets into nested dictionaries in order to preserve the following data hierarchy:\n\n* 1: Waterbodies \n    * 2: Waterbody Type ([\"Aquifer\",\"Water_Spring\",\"River\",\"Lake\"]) \n        * 3: Waterbody Name\n\nWhen we load the data, let's transform \"Date\" columns into datetime so that it is easier to process it further"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract waterbody types, names, data, and fill the data container\ndf = {} # initialize data container\nfor ii, info in data_descr.iterrows():\n    waterbody_type = info[\"type\"]\n    waterbody_name = info[\"name\"]\n    df.setdefault(waterbody_type, {})[waterbody_name] = pd.read_csv('/kaggle/input/acea-water-prediction/%s_%s.csv'%(waterbody_type,waterbody_name))\n    df[waterbody_type][waterbody_name].columns = [x.lower() for x in df[waterbody_type][waterbody_name].columns] # transfrom variables in lower case\n    df[waterbody_type][waterbody_name][\"date\"] = pd.to_datetime(df[waterbody_type][waterbody_name][\"date\"], dayfirst=True)\n    \nprint(\"Example of data structure:\")    \nprint(\"df:\\t\\t\"+str(df.keys()))   \nprint(\"df['Aquifer']:\\t\"+str(df[\"Aquifer\"].keys()))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create utility functions and variables useful later on:\n- a utility function `getWaterbody()` that gets waterbody type and variable name from a variable\n- a utility function `getAllVariables()` that returns all variables of a given type e.g. (\"temperature\")\n- a utility function `getIndependentVariables()` that for a given waterbody returns the independent variables including \n- a utility function `getTargetVariables()` that for a given waterbody returns the target variables including date\n- a variable `global_targets` that contains all the target variable types"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getWaterbody(var_name):\n    # If more waterbodies have the same variable, the first waterbody is returned\n    # E.g.: getWaterbody(\"temperature_firenze\")\n    for ii,info in data_descr.iterrows():\n        if var_name in df[info[\"type\"]][info[\"name\"]].columns:\n            return(info[\"type\"],info[\"name\"])\n    raise ValueError('There is no variable \"%s\".'%var_name)\n\ndef getAllVariables(var_type):\n    # E.g.: getAllVariables(\"temperature\")\n    variables = pd.DataFrame(columns=[\"type\",\"name\",\"variable\"])\n    for ii, info in data_descr.iterrows():\n        w_type = info[\"type\"]\n        w_name = info[\"name\"]\n        w_vars = list(df[w_type][w_name].columns)\n        w_vars = [var for var in w_vars if var.startswith(var_type)]\n        variables_ii = pd.DataFrame({\"type\":w_type,\"name\":w_name,\"variable\":w_vars})\n        variables = pd.concat([variables,variables_ii],axis=0,sort=False)   \n    variables.reset_index(drop=True,inplace=True)\n    return variables\n\ndef getIndependentVariables(w_type,w_name,var_start_name=None):\n    var_names = []\n    for var_name in df[w_type][w_name].columns:\n        if not var_name.startswith(tuple(global_targets)):\n            if var_start_name:\n                if var_name.startswith(var_start_name):\n                    var_names.append(var_name)                    \n            else:\n                var_names.append(var_name)\n    return var_names \n\ndef getTargetVariables(w_type,w_name):\n    var_names = []\n    data_sub = data_descr[ (data_descr[\"type\"]==w_type) & (data_descr[\"name\"]==w_name)].copy()\n    return list(data_sub[\"Output\"])[0]\n\n# In order to get the global targets let's identify the variabels used as targets\n\ntemp = list(data_descr[\"Output\"].values)\nglobal_targets = []\nfor tt in temp:\n    if type(tt)==list:\n        global_targets = global_targets + tt\n    else:\n        global_targets.append(tt)        \nglobal_targets = list(set([x.split(\"_\")[0] for x in global_targets]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean-up for dates \n- Remove missing dates\n- Reorder datasets by date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for ii, info in data_descr.iterrows():\n    w_type =  info[\"type\"]\n    w_name =  info[\"name\"]\n    n_unique = len(df[w_type][w_name][\"date\"].unique())\n    n_tot = len(df[w_type][w_name][\"date\"])\n    if n_unique!= n_tot:\n        print(\"%s %s:\\t%d unique over %d total date values\"%(w_type,w_name,n_unique,n_tot))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Something going on with **dates** of water spring \"Madonna_di_Canneto\".\n\nLet's remove duplicates and/or missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"w_type = \"Water_Spring\"\nw_name = \"Madonna_di_Canneto\"\n\ndf_sub = df[w_type][w_name]\ndf_sub[\"date\"][df_sub[\"date\"].duplicated()]\ndf_sub.drop(df_sub[df_sub[\"date\"].isnull() ].index,inplace=True)\n\n\nn_unique = len(df[w_type][w_name][\"date\"].unique())\nn_tot = len(df[w_type][w_name][\"date\"])\nprint(\"%s %s:\\t%d unique over %d total date values\"%(w_type,w_name,n_unique,n_tot))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's sort datasets by dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"for ii, info in data_descr.iterrows():\n    w_type = info[\"type\"]\n    w_name = info[\"name\"]\n    df[w_type][w_name].sort_values(by=\"date\",inplace=True)\n    df[w_type][w_name].reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore and Clean Data\nSo now that we have organized the datasets, let's look at the input and output variables for each waterbody.\nThe objective of this section is to explore each dataset and achieve the following results:\n- Correct missing and faulty values\n- Remove variables that are redundant or with not enough samples for the prediction task\n\nSpecifically, we will see that: \n\n1. Some target variables not requested as an output are present in some datasets. SO, let's remove them.\n2. Some target variables started to be recorded only recently, while the dataset goes further in the past. Since, this is a necessary target variable, let's focus our attention to the most recent data from where the first depth to groundwater data was recorded. This applies, for example, to:\n    - \"Depth\" variables for \"Aquifer\": \"Doganella\", \"Auser\",\"Luco\"\n    - All \"Flow\" rate variables for \"Water_Spring\"\n3. For a given waterbody, Temperature and Rainfall at *some locations* are sporadically missing, so we can take the locaiton with most values and fill the missing values with values with the location of the same waterbody with the highest correlation as long as higher than 0.90. Examples: \n    - \"Aquifer Doganella\": Temperature \"Monteporzio\" \"Velletri\" \n    - \"Aquifer Auser\": Rainfall \"Piaggione\" \"Borgo a Mozzano\"\n4. For a given waterbody, Temperature and Rainfall at *all locations* are completely missing at the same time, so we look for correlated variables from other waterbodies. So, in this case we should replace the last part of temperature and rainfall with other temperature/rainfall sensors mostly correlated with them. This applies to: \n    - \"Water_Spring Madonna_di_Canneto\"\n    - Temperature for \"Aquifer Auser\"\n    - Temperature for \"Aquifer Petrignano\"\n5. When an independent variable (e.g. temeprature) is  fully available at one location, but missing in others, let's drop the others to avoid multicollinearity, as long as te correlation between the two is higher than 0.90. \n    - \"Aquifer Auser\": leave only temperature \"Lucca_orto_botanico\""},{"metadata":{},"cell_type":"markdown","source":"## General Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Let's remove data of the same type of the generally used output variables but not requested as target variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Target variables removed:\\n\")\nfor ii, data in data_descr.iterrows():\n    w_type = data[\"type\"]\n    w_name = data[\"name\"]\n    targets = data[\"Output\"] # transforms in list of strings\n    columns = []\n    for var_name in df[w_type][w_name].columns:\n        if var_name.startswith(tuple(global_targets)) & (var_name not in targets):\n            print(\"\\t\\t\"+var_name)\n            columns.append(var_name)\n    df[w_type][w_name].drop(columns=columns,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the impact of missing values on datasets. We create a function`plotMissingValues()` that helps inspecting missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\n\n\n\ndef plotMissingValues(waterbody_type,waterbody_name=None, ax=None):\n    # setup colors \n    myColors = ((0.0, 0.8, 0.0, 1.0),(0.8, 0.0, 0.0, 1.0)) # green and red\n    cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n\n    if waterbody_name:\n        nrows = 1\n        waterbody_names = [waterbody_name]\n    else:    \n        waterbody_names = df[waterbody_type].keys()\n        nrows = len(waterbody_names)\n    \n    if not ax:\n        fig, ax = plt.subplots(nrows=nrows, ncols=1,figsize=(15,5*nrows))\n        \n    ii = 0\n    nintervals = 20\n    resol = 30 # how many parts is an interval (indicates resoltuion of each missing line)  \n    for waterbody_name in waterbody_names:\n        axi = ax[ii] if type(ax)==np.ndarray else ax #  ensure the case of only one axes is dealt with\n        data = df[waterbody_type][waterbody_name].T.isnull()\n        ndata = data.shape[1]\n        for irow, row in data.iterrows(): # to avoid sporadic nulls are not visible, let's increase resolution\n            data.loc[irow,:] = np.convolve([True]*int(ndata/nintervals/resol), row.values,\"same\")\n        sns.heatmap(data,cmap=cmap,cbar=False,ax=axi)\n        axi.set_title(waterbody_type + \" \" + waterbody_name)\n        axi.set_yticks(range(df[waterbody_type][waterbody_name].shape[1])) # <--- set the ticks first\n        axi.set_yticklabels(df[waterbody_type][waterbody_name].columns,va=\"top\");\n        x_ind = np.linspace(0,df[waterbody_type][waterbody_name].shape[0]-1,nintervals+1,dtype=int) # 20 intervals\n        print()\n        axi.set_xticks(x_ind)\n        axi.set_xticklabels(df[waterbody_type][waterbody_name][\"date\"].dt.date.iloc[x_ind],rotation=45);\n        axi.set_xlabel(\"\")\n        axi.grid(True,color=\"white\")\n        ii = ii +1\n    \n    if type(ax)==np.ndarray:\n        fig.tight_layout(); #fig.suptitle(waterbody_type, fontsize=16,y=1.00);\n    \nplotMissingValues(\"Aquifer\",\"Doganella\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that target variables have no values for some historical dates (in the example above, \"*depth_to_groundwater*\" in *Aquifer Doganella* has no target variables before 2013). Since we cannot reliably reconstruct target variables for these dates, we will not to use such dates for building prediction models. \n\nTherefore, for each waterbody let's filter only the most recent data since when target variables are  generally available. Let's apply this filter to all the waterbodies' data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for ii, info in data_descr.iterrows():\n    w_type = info[\"type\"]\n    w_name = info[\"name\"]\n    columns = list(df[w_type][w_name].columns)\n    columns = [col in info[\"Output\"] for col in columns]\n    #columns = df[w_type][w_name].columns[columns] # columns of targets\n    indstart= np.where((~df[w_type][w_name].iloc[:,columns].isnull()).any(axis=1))[0][0]\n    df[w_type][w_name] = df[w_type][w_name].iloc[indstart:]\n    df[w_type][w_name].reset_index(drop=True,inplace=True)\n\nprint(\"Removed historical dates with no target variables. Example of results on water springs.\") \nplotMissingValues(\"Water_Spring\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check correlations between input variables so to identify possible multicollinearity."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotCorrMatrix(waterbody_type,waterbody_name,var_names=None,ax=None,**kwargs):\n    df_sub = df[waterbody_type][waterbody_name].copy(deep=True)\n    if var_names == None:\n        colnames = df_sub.columns\n        if \"Date\" in colnames: colnames.remove(\"Date\")\n    else:\n        colnames = []\n        for name in var_names:\n            colnames = colnames+list(df_sub.columns[np.where(df_sub.columns.str.startswith(name))[0]])\n    nvars = len(colnames)\n    if nvars<2: # if not enough variables to correlate\n        print(\"Not enough variables to print\")\n        return None\n    if ax==None:\n        plt.figure(num=None, figsize=(nvars, nvars )) #, dpi=80, facecolor='w', edgecolor='k')\n        ax = plt.gca()\n        \n    corrMatrix = df_sub[colnames].corr(method=\"pearson\")\n    corrMatrix = corrMatrix.where(np.tril(np.ones(corrMatrix.shape),-1).astype(np.bool))\n    corrMatrix.drop(corrMatrix.columns[-1],axis=1,inplace=True)\n    corrMatrix.drop(corrMatrix.index[0],axis=0,inplace=True)\n    sns.heatmap(corrMatrix, annot=True,ax=ax,**kwargs)\n    plt.setp(ax.get_xticklabels(), rotation=70);\n\nplotCorrMatrix(\"Aquifer\",\"Auser\",[\"temperature\",\"rainfall\"],vmin=-1, vmax=1,cmap=\"coolwarm\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables of the same type, for example `temperature`, are strogly correlated with each other. \nIn presence of multi-collinearity, the prediction power diminishes. Therefore it is a good practice to remove input variables too strongly correlated between each others.\nOn the other hand such correlated variables can be used to reconstruct missing values among each others.i"},{"metadata":{},"cell_type":"markdown","source":"## Temperature\nNow le's look at the temperature variables for all the waterbodies. \nThe main purpose is exploration and data-cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_WaterbodyVariable(waterbody_type,variable_name,waterbody_name=None,date_interval=None,**kwargs):\n    \"\"\"\n    date_interval: tuple of start and end dates with format: '%Y-%m-%d', e.g.: (\"2018-01-29\", \"2020-02-13\")\n    \"\"\"\n    \n    nrows = len(df[waterbody_type])\n    if waterbody_name:\n        nrows = 1\n        \n    fig, ax = plt.subplots(nrows=nrows, ncols=1,figsize=(14,nrows*5))\n    \n    df_type = df[waterbody_type].copy()\n    if waterbody_name:\n        df_type = { waterbody_name: df_type[waterbody_name] }\n\n    for ii, waterbody_name in enumerate(df_type):\n        df_sub = df_type[waterbody_name]\n        colnames = list(df_sub.columns[np.where(df_sub.columns.str.startswith(variable_name))[0]])\n        if len(colnames)==0: # if there is no variable, just go to next dataframe\n            continue\n        colnames.append(\"date\")\n        df_sub = df_sub[colnames]\n        if date_interval is not None:\n            date_start,date_end = date_interval\n            date_start =  datetime.strptime(date_start, '%Y-%m-%d').date()\n            date_end =  datetime.strptime(date_end, '%Y-%m-%d').date()\n            df_sub = df_sub[(df_sub[\"date\"].dt.date>= date_start) & (df_sub[\"date\"].dt.date<= date_end)]\n        axi = ax[ii] if type(ax)==np.ndarray else ax #  ensure the case of only one axes is dealt with\n        df_sub.plot(x=\"date\",ax=axi,grid=True,**kwargs)\n        axi.set_title(waterbody_type+\" \"+waterbody_name); axi.set_xlabel(\"\"); \n        #axi.set_xticks(df_sub[\"Date\"].index)\n        #axi.set_xticklabels(df_sub[\"Date\"].dt.date,rotation=45);\n        df_sub[\"date\"].dt.date\n        plt.setp(axi.get_xticklabels(), rotation=60)\n        axi.legend(loc = \"center left\", bbox_to_anchor = (1.0, 1.0))\n    fig.tight_layout()\n\n\ndate_start = \"2019-01-01\"  # required format: '%Y-%m-%d'\ndate_end = \"2019-12-31\"\nplot_WaterbodyVariable(\"Aquifer\",\"temperature\",\"Petrignano\",date_interval=(date_start,date_end),marker=\".\",linestyle=\"-\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_VarDistribution(waterbody_type,waterbody_name,variable_name,bins=50,stacked=True):\n\n    df_sub = df[waterbody_type][waterbody_name].copy()\n\n    colnames = list(df_sub.columns[np.where(df_sub.columns.str.startswith(variable_name))[0]])\n    df_sub = df_sub[colnames]\n\n    nrows = df_sub.shape[1]\n    if stacked:\n        fig, axs = plt.subplots(nrows=nrows, ncols=1,figsize=(20,nrows*5),sharex=False)\n    else:\n        fig = plt.figure(figsize=(20,5))\n        ax = plt.gca()\n\n    ii = 0\n    for name, series in df_sub.iteritems():\n        if stacked:\n            ax = axs[ii] if type(axs)==np.ndarray else axs\n        series.plot.hist(bins=bins,grid=True,alpha=0.5,ax=ax,legend=True)            \n        ax.set_title(waterbody_type+\" \"+waterbody_name+\" \"+name)\n        ii= ii +1\n    plt.setp(ax.get_xticklabels(), rotation=60)\n    fig.tight_layout()    \n\n\nplot_VarDistribution(\"Aquifer\",\"Luco\",\"temperature\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some **temperature** sensors look faulty because report 0 values. So let's identify them and set their 0 values to 'missing'. \n\nBut not all the \"temperature\" variables need this treatment.\nAs discriminator, we will use the margin of error (at 95% confidence) between the mean count of the adjacent bins and the bin with 0.\nWe use 50 bins, but other references can be used as well (e.g. nbins = 1% of number of samples)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\nconf_lev = 0.95\nzscore = scipy.stats.norm.ppf( 1-(1-conf_lev)/2) \n\n\nfor ii, w_type in enumerate(df):\n    for jj, w_name in enumerate(df[w_type]):\n        df_sub = df[w_type][w_name]#.copy()\n        for name, series in df_sub.iteritems():\n            if name.startswith('temperature'):\n                n,bins = np.histogram(series.dropna(),bins=50) #n, bins = get_hist(axs[ii])\n                ind0 = np.digitize(0,bins)-1 # index of the bin containing 0\n                bins_diff = np.diff(n) # difference among adjacent bins\n                moe = np.ceil(zscore*np.std(bins_diff)/np.sqrt(len(bins_diff))) # margin of error\n                expected = (n[ind0-1]+n[ind0+1])/2\n                check_ok = ((n[ind0]>=(expected-moe)) & (n[ind0]<=(expected+moe)) ) or (n[ind0]<20)\n                if ~check_ok: # if number of zero values is abnormal, tranbsform all 0 to nan \n                    series[series==0.00]=np.nan\n                    print(\"%s %s: removing %d values for %s as they are not in the confidence interval.\" %(w_type,w_name,n[ind0],name.replace(\"Temperature_\",\"sensor \")))\n            else:\n                continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a given waterbody, temperature values at *some locations* are sporadically missing, so we can take the location with most values and fill the missing values with values of similar varibales nearby the same waterbody. \n\nWe use the function `fixMissingValues()` to fix missing values of one series, with another series. \n\nWe use the function `getVarCorrList()` to get the list of correlation values between variables of the same type.\nLet's quickly look at what these functions do, before using them to clean up the data.\n\nWe use the function `cleanAndGetVariables` to find variable with least percentage of missing values in a waterbody and we replace its missing values with the most correlated variable in the SAME waterbody if the correlation is above a certain threshold, in this case 90%."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fixMissingValues(var1,var2):\n    \"\"\"\n    Fixes missing values of a variable (var1) with the values taken from another variable (var2).\n    Method: normalization and substitution\n    Returns the number of values for variable that were fixed\n    If the variables have the same name, the function does not fix anything.\n    \"\"\"\n    \n    if var1 == var2:\n        return 0\n        \n    w_type1,w_name1 = getWaterbody(var1)\n    w_type2,w_name2 = getWaterbody(var2)\n    \n    series1 = df[w_type1][w_name1][[\"date\", var1]].copy()\n    series2 = df[w_type2][w_name2][[\"date\",var2]].copy()\n\n\n    mean1 = series1.mean(numeric_only=True)[0]\n    mean2 = series2.mean(numeric_only=True)[0]\n    std1 = series1.std(numeric_only=True)[0]\n    std2 = series2.std(numeric_only=True)[0]\n    \n    data = series1.merge(series2,on=\"date\",how=\"left\")\n\n    jj = 0\n    for ii, val in data[var1].iteritems(): #in data.iterrows():\n        if np.isnan(val):\n            if (not np.isnan(data.loc[ii,var2])):\n                data.loc[ii,var1] = (data[var2][ii] - mean2) * std1/std2 + mean1\n                jj = jj +1\n    df[w_type1][w_name1][var1] = data[var1].values\n    return jj\n\n\n\n### Show example\n\nplt.figure(figsize=(15,5))\nax = plt.gca()\nvar1 = \"temperature_abbadia_s_salvatore\"\nvar2 =  \"temperature_le_croci\"#\"temperature_laghetto_verde\"\n\n## dates used only for plotting\nstart_date = datetime.strptime(\"2015-06-01\", '%Y-%m-%d').date() # datetime.strptime(\"2015-06-01\", '%Y-%m-%d').date()\nend_date = datetime.strptime(\"2016-03-07\", '%Y-%m-%d').date()#datetime.strptime(\"2016-03-07\", '%Y-%m-%d').date()\n\n# plot original series not yet fixed\nw_type1, w_name1 = getWaterbody(var1)\nw_type2, w_name2 = getWaterbody(var2)\nseries1old = df[w_type1][w_name1][[\"date\", var1 ]].copy()\nseries2 = df[w_type2][w_name2][[\"date\", var2 ]].copy()\nseries1old = series1old[(series1old[\"date\"].dt.date>start_date) & (series1old[\"date\"].dt.date<end_date)]\nseries2 = series2[(series2[\"date\"].dt.date>start_date) & (series2[\"date\"].dt.date<end_date)]\n\nax.plot(series2[\"date\"],series2[var2],\"b\",label = \"original %s\"%var2)\nax.plot(series1old[\"date\"],series1old[var1],\"k\",label= \"original %s\"%var1)\n\n\n# FIX MISSING VALUES\nfixMissingValues(var1,var2);\n\n\n# plot fixed series\nseries1new = df[w_type1][w_name1][[\"date\", var1 ]].copy()\nseries1new = series1new[(series1new[\"date\"].dt.date>start_date) & (series1new[\"date\"].dt.date<end_date)]\nax.plot(series1new[\"date\"],series1new[var1],\"--k\",label= \"fixed missing %s\"%var1,lw=2)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.setp(plt.gca().get_xticklabels(), rotation=60);\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getGlobalCorrList(var_type,isMirrored=True,removeDuplicates=False,date_interval=None):\n    \"\"\"\n    Get list of correlations for one variable across different waterbodies\n    isMirrored: reports the same correlation value for each of the two combination of variables\n    removeDuplicates: avoids reporting cases where the same variable is used in multiple waterbodies\n    \"\"\"\n\n    import itertools\n\n    \n    varList = getAllVariables(var_type)\n    varList.reset_index(drop=True,inplace=True)# ensure indices are unique\n    \n    \n    indices = list(itertools.combinations(varList.index,2))\n\n    corrList = pd.DataFrame(columns = ['type1', 'name1', 'variable1', 'type2', 'name2', 'variable2', 'corr_value'],index=range(len(indices)))\n\n    if date_interval is not None:\n        date_start,date_end = date_interval\n        date_start =  datetime.strptime(date_start, '%Y-%m-%d').date()\n        date_end =  datetime.strptime(date_end, '%Y-%m-%d').date()\n        \n    for ii in indices:\n        type1 = varList[\"type\"][ii[0]]\n        name1 = varList[\"name\"][ii[0]]\n        var1 = varList[\"variable\"][ii[0]]\n        type2 = varList[\"type\"][ii[1]]\n        name2 = varList[\"name\"][ii[1]]\n        var2 = varList[\"variable\"][ii[1]]\n        data1 = df[type1][name1][[\"date\",var1]].copy()\n        data2 = df[type2][name2][[\"date\",var2]].copy()\n        if date_interval is not None:\n            data1 = data1[(data1[\"date\"].dt.date>= date_start) & (data1[\"date\"].dt.date<= date_end)]\n            data2 = data2[(data2[\"date\"].dt.date>= date_start) & (data2[\"date\"].dt.date<= date_end)] \n        corrValue = data1.merge(data2,on=\"date\",how=\"outer\").corr().iloc[0,1]\n        if ~np.isnan(corrValue):\n            corrList_ii = pd.DataFrame({'type1': type1, 'name1':name1, 'variable1':var1, 'type2':type2, 'name2':name2, 'variable2':var2, 'corr_value':corrValue},index=[corrList.shape[0]])\n            corrList = pd.concat([corrList,corrList_ii],sort=False)\n\n    if isMirrored:\n        corrList2 = corrList.copy()\n        corrList2.columns = [col.replace(\"2\",\"0\").replace(\"1\",\"2\").replace(\"0\",\"1\") for col in corrList2.columns]\n        corrList = pd.concat([corrList,corrList2],sort=False)\n    \n    if removeDuplicates:\n        corrList.drop_duplicates(subset=[\"variable1\",\"variable2\"], keep='first', inplace=True)\n           \n    corrList.reset_index(drop=True,inplace=True)\n    \n    return corrList\n\n### Show example\n#list1 = getGlobalCorrList(\"rainfall\",isMirrored=True,date_interval=(\"2020-01-01\",\"2020-06-30\"))\n#list1 = list1.loc[list1[\"variable1\"]==\"rainfall_settefrati\",:]\n#list1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, **temperatures** for different locations of **different** waterbodies are strongly correlated. This means that we can use them interchangeably.\nNow, let's look at which variables contain most missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMissingValues(var_name):\n\n    report_table = pd.DataFrame(columns=[\"waterbody\",\"variable\",\"missing\",\"total\",\"missing%\"])\n    for ii, waterbody_type in enumerate(df):\n        for jj, waterbody_name in enumerate(df[waterbody_type]):\n            df_sub = df[waterbody_type][waterbody_name].copy()\n            ind = np.where(df_sub.columns.str.startswith(var_name))[0]\n            df_sub = df_sub.iloc[:,ind]\n            if len(df_sub.columns)==0:\n                continue\n            #df_sub.columns=df_sub.columns.str.replace(var_name+\"_\",\"\")\n            statsd = pd.DataFrame({\"waterbody\":waterbody_type+\" \"+waterbody_name,\"variable\":list(df_sub.columns),\"missing\":np.nan,\"total\":df_sub.shape[0],\"missing%\":np.nan})\n            statsd[\"missing\"] = df_sub.isnull().sum().to_frame().values\n            statsd[\"missing%\"] = np.round(statsd[\"missing\"]/statsd[\"total\"]*100,1)\n            report_table = pd.concat([statsd,report_table],axis=0,sort=False)   \n    report_table = report_table.reindex(columns=[\"waterbody\",\"variable\",\"missing\",\"total\",\"missing%\"] )  \n    report_table.sort_values(by=['waterbody','missing%'],ascending=True,inplace=True) # sort in ascending order for each waterbody\n    report_table.reset_index(drop=True,inplace=True)\n    return report_table\n\ngetMissingValues(\"temperature\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanAndGetVariables(var_type,var_name=None,corr_threshold=0.9,verbose=False):\n    \"\"\"\n    For each waterbody find variable with least percentage of missing values\n    If no missing values, the variable is clean enough and does not need any further treatment\n    Otherwise find the most correlated variable in the SAME AQUIFER (if any) and use it to replace missing values of the first variable \n    Return: \n        - list of selected variables\n        - list of complete waterbodies (waterbodies that have at least one complete variable)\n    \"\"\"\n\n    corrList = getGlobalCorrList(var_type,isMirrored=True,removeDuplicates=True)\n    report_table = getMissingValues(var_type)\n\n    #pdb.set_trace()\n    if var_name is not None:\n        report_table = report_table.loc[report_table[\"variable\"]==var_name,:]\n    subtable = report_table[report_table[\"missing\"]==0.0]\n    completeWaterbodies = list(set(subtable[\"waterbody\"]))\n    selectedVariables = list(subtable[\"variable\"])\n    iid_remaining = np.array([wb not in completeWaterbodies for _,wb in report_table[\"waterbody\"].iteritems()])\n    report_table = report_table[iid_remaining]\n    report_table.reset_index(drop=True,inplace=True)\n\n    # for each waterbody, take the varibale with least missing values and fix it with any other variable of the same type \n    fixing = [True]\n    while ((report_table.shape[0]>0) & np.any(fixing)):\n        report_table.reset_index(drop=True,inplace=True)\n        corrList.reset_index(drop=True,inplace=True)\n        variables = report_table[\"variable\"][report_table.groupby(\"waterbody\")[\"missing%\"].idxmin()]\n        # for each waterbody fix the variable with less missing values, with the next varibale with highest correlation within the boundary\n        fixing = []\n        for ii,var_type in variables.iteritems():\n            if verbose==True:\n                missing = report_table[\"missing%\"][report_table[\"variable\"]==var_type].values[0]\n                print(\"%s: %.1f%% missing\"%(var_type,missing))\n            fixing.append(False)\n            #pdb.set_trace()\n            if (var_type in corrList[\"variable1\"].values): # if there are missing values and there is another variable of the same type\n                corrmax = corrList[corrList[\"variable1\"]==var_type][\"corr_value\"].max()\n                if corrmax >= corr_threshold: # if varibale with the maximum correlation is higher than threshold\n                    fixing[-1] = True # so we managed to fix\n                    kk = corrList[corrList[\"variable1\"]==var_type][\"corr_value\"].idxmax() # take the value with the highest correlation\n                    var_type2 = corrList[\"variable2\"].loc[kk]\n                    nfixed = fixMissingValues(var_type,var_type2)\n                    if nfixed>0:\n                        print(\"   -> fixed %d values of %s with %s.\"%(nfixed,var_type,var_type2) )\n                        jj = np.where(report_table[\"variable\"]==var_type)[0]\n                        report_table[\"missing\"].iloc[jj] = report_table[\"missing\"].iloc[jj] - nfixed\n                        report_table[\"missing%\"].iloc[jj] = float(report_table[\"missing\"].iloc[jj] / report_table[\"total\"].iloc[jj] *100)\n                    corrList.drop(index=kk,inplace=True) # drop this correlation value since already used to avoid using always the same variable\n\n        # remove the waterbody from the table of waterbodies to check        \n        subtable = report_table[report_table[\"missing\"]==0.0]\n        completeWaterbodies = list(set(completeWaterbodies + list(subtable[\"waterbody\"])))\n        selectedVariables = list(set(selectedVariables + list(subtable[\"variable\"])))\n        iid_remaining = np.array([wb not in completeWaterbodies for _,wb in report_table[\"waterbody\"].iteritems()])\n        report_table = report_table[iid_remaining]\n        report_table.reset_index(drop=True,inplace=True)\n        \n    return(completeWaterbodies,selectedVariables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"completeWaterbodies,selectedVariables = cleanAndGetVariables(\"temperature\",corr_threshold=0.9,verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally for each waterbody, we fill missing data with values form other sensors (when available), then we remove the other sensor data to avoid collinearity in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Auser\"].drop(columns=[\"temperature_monte_serra\",\"temperature_ponte_a_moriano\"],inplace=True)\ndf[\"Aquifer\"][\"Luco\"].drop(columns=[\"temperature_mensano\",\"temperature_siena_poggio_al_vento\",\"temperature_pentolina\"],inplace=True)\ndf[\"Aquifer\"][\"Petrignano\"].drop(columns=[\"temperature_petrignano\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Amiata\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fill sporadic missing values with interpolated values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Amiata\"].copy()\ndata[\"temperature_laghetto_verde_interpolated\"] = data[\"temperature_laghetto_verde\"].interpolate(method='linear')\ndata[\"temperature_s_fiora_interpolated\"] = data[\"temperature_s_fiora\"].interpolate(method='linear')\ndata.loc[700:750,:].plot(x=\"date\",y=[\"temperature_laghetto_verde_interpolated\",\"temperature_laghetto_verde\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Water_Spring\"][\"Amiata\"][\"temperature_laghetto_verde\"] = data[\"temperature_laghetto_verde_interpolated\"].values\ndf[\"Water_Spring\"][\"Amiata\"][\"temperature_s_fiora\"] = data[\"temperature_s_fiora_interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Lupa\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Since this water spring does not include any temperature data, we attach temperature from a closeby location, where location is chosen based on correlation of the rainfall variable in the last period (2020). "},{"metadata":{"trusted":true},"cell_type":"code","source":"endDate = df[\"Water_Spring\"][\"Lupa\"][\"date\"].values[-1].astype(str)[:10]\nstartDate = \"2020-01-01\"\ncorrList = getGlobalCorrList(\"rainfall\",isMirrored=True,removeDuplicates=True,date_interval=(startDate,endDate))\ncorrList = corrList.loc[corrList[\"variable1\"]==\"rainfall_terni\",[\"type2\",\"name2\",\"variable2\",\"corr_value\"]].sort_values(by=\"corr_value\",ascending=False)\ncorrList.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is no temperature data for location \"*Croce Arcana*\", we take the temperature from the second most rainfall-correlated location, which is \"*Bastia_Umbra*\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#startDate = df[\"Water_Spring\"][\"Lupa\"][\"date\"].values[0].astype(str)[:10]\ndata2 = df[\"Aquifer\"][\"Petrignano\"][[\"date\",\"temperature_bastia_umbra\"]].copy()\ndf[\"Water_Spring\"][\"Lupa\"] = df[\"Water_Spring\"][\"Lupa\"].merge(data2,on=\"date\",how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"River\",\"Arno\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"River\"][\"Arno\"].copy()\nind_start = data.loc[~data[\"temperature_firenze\"].isnull(),:].index[0]\ndf[\"River\"][\"Arno\"] = data.loc[ind_start:,:]\ndata = df[\"River\"][\"Arno\"].copy()\ndata[\"temperature_firenze_interpolated\"] = data[\"temperature_firenze\"].interpolate(method='linear')\ndata.iloc[810:860,:].plot(x=\"date\",y=[\"temperature_firenze_interpolated\",\"temperature_firenze\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"River\"][\"Arno\"][\"temperature_firenze\"] = data[\"temperature_firenze_interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Lake\",\"Bilancino\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Lake\"][\"Bilancino\"].copy()\ndata[\"temperature_le_croci_interpolated\"] = data[\"temperature_le_croci\"].interpolate(method='linear')\ndata.iloc[120:200,:].plot(x=\"date\",y=[\"temperature_le_croci_interpolated\",\"temperature_le_croci\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Lake\"][\"Bilancino\"][\"temperature_le_croci\"] = data[\"temperature_le_croci_interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rainfall\nLet's analyze and correct rainfall data for all waterbodies."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Amiata\"].copy()\nvariable_name = \"rainfall_castel_del_piano\"\n\nfig, axs = plt.subplots(nrows=2,ncols=2,figsize=(20,12));\nrolling = pd.concat([data[[\"date\"]],data[[variable_name]].rolling(30).mean()],axis=1,sort=False)\nrolling.plot(x=\"date\",y=variable_name,ax=axs[0,0],title=\"30-days rolling mean\",grid=True);\ndata[variable_name].plot.hist(bins=np.arange(0,10,0.25),grid=True,alpha=0.5,ax=axs[0,1],legend=False,title=\"rainfall data distribution\") ;    \n#data[variable_name].plot(kind='kde',ax=axs[0,1]);\n\n# show yearly seasonality over months\ngroups = pd.crosstab(data[\"date\"].dt.year, data[\"date\"].dt.month, data[variable_name], aggfunc=\"mean\",rownames=[\"Year\"],colnames=[\"Month\"])\ngroups.mean().plot(ax=axs[1,0],kind=\"bar\",title=\"average across months\",grid=True)\n\n# create a boxplot of yearly data\ngroups.T.boxplot(ax=axs[1,1])\naxs[1,1].set_title(\"boxplot of average monthly rainfall over years\");\n\nplt.tight_layout()\n\"\"\"\ndatafull = data.copy()\ndatafull[\"year\"] = datafull[\"date\"].dt.year\ndatafull[\"day\"] = datafull[\"date\"].dt.dayofyear\ndatafull=datafull.pivot(columns=\"year\",index=\"day\",values=variable_name)\ndatafull = datafull.mean(0)\ndatafull.plot(x=\"index\",ax=axs[2],style=[\"-o\"],grid=True);\naxs[2].set_xticks(datafull.index.astype(int))\naxs[2].set_xticklabels(datafull.index.astype(int));\n\"\"\"\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that data is sparse, and the greatest majority of data falls at 0 value. \n\nMoreover, as expected, there is yearly seasonality, since months have different average rainfall.\n\nAverage monthly rainfall in 2020 has decreased with respect to 2019."},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Doganella"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Doganella\")\nplot_WaterbodyVariable(\"Aquifer\",\"rainfall\",waterbody_name=\"Doganella\",date_interval=(\"2014-12-19\",\"2015-02-28\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Doganella\",\"rainfall\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For **Aquifer Doganella** there are sporadically missing rainfall data, so we fill them (where possible) with interpolation over a week's time window. We then substitute missing values for each location with non-missing values of the other location, and vice-versa. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanAndGetVariables(\"rainfall\",var_name=\"rainfall_monteporzio\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Doganella\"].loc[df[\"Aquifer\"][\"Doganella\"][\"rainfall_monteporzio\"]<0,\"rainfall_monteporzio\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_velletri\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Doganella\"].loc[df[\"Aquifer\"][\"Doganella\"][\"rainfall_velletri\"]<0,\"rainfall_velletri\"]=0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average for how many consecutive days does it rain? This info will help us with filling data through interpolation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import more_itertools\n\nrainyDays = np.where(df[\"Aquifer\"][\"Doganella\"][\"rainfall_velletri\"]!=0)[0]\n# now let's group consecutive days of rain\nnRainyDays = [len(list(group)) for group in more_itertools.consecutive_groups(rainyDays)]\nnp.median(nRainyDays)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we interpolate to fill remaining missing values. \nWe use **linear** interpolation because it is more suitable to approximate spiky data like rainfall.\n\nWe use a window of two days because we'have just seen that the median duration for rain is 2 days.\n\nFinally we fill the remaining missing values with zeros, since no-rain is the most probable outcome for a day."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Aquifer\"][\"Doganella\"].copy()\n\ndata[\"rainfall_velletri_interpolated\"] = data[\"rainfall_velletri\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_monteporzio_interpolated\"] = data[\"rainfall_monteporzio\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_velletri_interpolated\"].fillna(0,inplace=True)\ndata[\"rainfall_monteporzio_interpolated\"].fillna(0,inplace=True)\n\ndata.loc[900:1052,:].plot(x=\"date\",y=[\"rainfall_monteporzio_interpolated\",\"rainfall_monteporzio\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Aquifer\"][\"Doganella\"][\"rainfall_velletri\"] = data[\"rainfall_velletri_interpolated\"].values\ndf[\"Aquifer\"][\"Doganella\"][\"rainfall_monteporzio\"] = data[\"rainfall_monteporzio_interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Auser"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Auser\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only missing values are at locations *Piaggione* and *Monte Serra*.\n\nWe fix *Piaggione* using other locations' values."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanAndGetVariables(\"rainfall\",var_name=\"rainfall_piaggione\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Auser\"].loc[df[\"Aquifer\"][\"Auser\"][\"rainfall_piaggione\"]<0,\"rainfall_piaggione\"]=0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Aquifer\"][\"Auser\"].copy()\ndata[\"rainfall_monte_serra_interpolated\"] = data[\"rainfall_monte_serra\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_monte_serra_interpolated\"].fillna(0,inplace=True)\ndf[\"Aquifer\"][\"Auser\"][\"rainfall_monte_serra\"] = data[\"rainfall_monte_serra_interpolated\"].values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Luco"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Luco\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax= plt.gca()\nplotCorrMatrix(\"Aquifer\",\"Luco\",var_names=\"rainfall\",vmin=-1, vmax=1,cmap=\"coolwarm\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation matrices we see that some rainfall at some locations is quite correlated with rainfall at other locations (correlation>80%). So we can fix missing values of one location with missing values at another location."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanAndGetVariables(\"rainfall\",var_name=\"rainfall_monticiano_la_pineta\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_monticiano_la_pineta\"]<0,\"rainfall_monticiano_la_pineta\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_ponte_orgia\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_ponte_orgia\"]<0,\"rainfall_ponte_orgia\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_scorgiano\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_scorgiano\"]<0,\"rainfall_scorgiano\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_pentolina\",corr_threshold=0.8,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_pentolina\"]<0,\"rainfall_pentolina\"]=0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Montalcinello*,*Simignano* and *Sovicille* are locations with few missing values, but that can be still partially reconstructed. \n\nSo we interpolate up to 2 days (avearage duration of rainfall) for each location.\nThen, for the remaining missing values, we use other locations' values, but this time we are more tolerant to correlation values above 75% (below previous 80% threshold).\n\nOn the other hand, we do not spend more time on *Pentolina* and *Arbia Piena* *Ponte Orgia* as they miss large parts of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Aquifer\"][\"Luco\"].copy()\ndata.loc[1620:1670,:].plot(x=\"date\",y=[\"rainfall_sovicille\"],style=[\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\n#data.loc[2100:2300,:].plot(x=\"date\",y=[\"rainfall_simignano_interpolated\",\"rainfall_simignano\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Aquifer\"][\"Luco\"].copy()\n\ndata[\"rainfall_montalcinello_interpolated\"] = data[\"rainfall_montalcinello\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_simignano_interpolated\"] = data[\"rainfall_simignano\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_sovicille_interpolated\"] = data[\"rainfall_sovicille\"].interpolate(method='linear',limit=2)\ndata[\"rainfall_sovicille_interpolated\"].fillna(0,inplace=True)\n#data[\"rainfall_scorgiano_interpolated\"] = data[\"rainfall_scorgiano\"].interpolate(method='linear',limit=2)\n\n\ndata.loc[2100:2300,:].plot(x=\"date\",y=[\"rainfall_simignano_interpolated\",\"rainfall_simignano\"],style=[\".-\",\".-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Aquifer\"][\"Luco\"][\"rainfall_montalcinello\"] = data[\"rainfall_montalcinello_interpolated\"].values\ndf[\"Aquifer\"][\"Luco\"][\"rainfall_simignano\"] = data[\"rainfall_simignano_interpolated\"].values\ndf[\"Aquifer\"][\"Luco\"][\"rainfall_sovicille\"] = data[\"rainfall_sovicille_interpolated\"].values\n#df[\"Aquifer\"][\"Luco\"][\"rainfall_scorgiano\"] = data[\"rainfall_scorgiano_interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanAndGetVariables(\"rainfall\",var_name=\"rainfall_simignano\",corr_threshold=0.75,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_simignano\"]<0,\"rainfall_simignano\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_montalcinello\",corr_threshold=0.75,verbose=False)\ndf[\"Aquifer\"][\"Luco\"].loc[df[\"Aquifer\"][\"Luco\"][\"rainfall_montalcinello\"]<0,\"rainfall_montalcinello\"]=0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Luco\")\nplot_WaterbodyVariable(\"Aquifer\",\"rainfall_simignano\",waterbody_name=\"Luco\",date_interval=(\"2013-01-01\",\"2014-02-28\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove variables where there are too litlle values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Luco\"].drop(columns=[\"rainfall_siena_poggio_al_vento\",\"rainfall_mensano\",\"rainfall_scorgiano\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Petrignano"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Petrignano\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are missing values, there is not much we can do about cleaning values at this location. So, we just drop the first part of the data, so that our data includes both temperature and rainfall."},{"metadata":{"trusted":true},"cell_type":"code","source":"indstart= np.where(~df[\"Aquifer\"][\"Petrignano\"][\"rainfall_bastia_umbra\"].isnull())[0][0]\ndf[\"Aquifer\"][\"Petrignano\"] = df[\"Aquifer\"][\"Petrignano\"].iloc[indstart:]\ndf[\"Aquifer\"][\"Petrignano\"].reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Water Spring Amiata"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Amiata\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fix missing values with rainfall values at other locations, as long as correlation is higher than 80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanAndGetVariables(\"rainfall\",var_name=\"rainfall_abbadia_s_salvatore\",corr_threshold=0.8,verbose=False)\ndf[\"Water_Spring\"][\"Amiata\"].loc[df[\"Water_Spring\"][\"Amiata\"][\"rainfall_abbadia_s_salvatore\"]<0,\"rainfall_abbadia_s_salvatore\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_s_fiora\",corr_threshold=0.8,verbose=False)\ndf[\"Water_Spring\"][\"Amiata\"].loc[df[\"Water_Spring\"][\"Amiata\"][\"rainfall_s_fiora\"]<0,\"rainfall_s_fiora\"]=0.\ncleanAndGetVariables(\"rainfall\",var_name=\"rainfall_laghetto_verde\",corr_threshold=0.8,verbose=False)\ndf[\"Water_Spring\"][\"Amiata\"].loc[df[\"Water_Spring\"][\"Amiata\"][\"rainfall_laghetto_verde\"]<0,\"rainfall_laghetto_verde\"]=0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since *Vetta Amiata* is correlated less than 80% with any other location, let's clean the remaining sporadic values by using interpolation. \nLet's first calculate what is the average duration (in days) of rain, so that we use the proper window."},{"metadata":{"trusted":true},"cell_type":"code","source":"import more_itertools\n\nrainyDays = np.where(df[\"Water_Spring\"][\"Amiata\"][\"rainfall_vetta_amiata\"]!=0)[0]\n# now let's group consecutive days of rain\nnRainyDays = [len(list(group)) for group in more_itertools.consecutive_groups(rainyDays)]\ndata = df[\"Water_Spring\"][\"Amiata\"].copy()\ndata[\"rainfall_vetta_amiata\"] = data[\"rainfall_vetta_amiata\"].interpolate(method='linear',limit=int(np.median((nRainyDays))))\ndata[\"rainfall_vetta_amiata\"].fillna(0,inplace=True)\ndf[\"Water_Spring\"][\"Amiata\"][\"rainfall_vetta_amiata\"] = data[\"rainfall_vetta_amiata\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Amiata\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Water Spring Lupa"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Lupa\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_WaterbodyVariable(\"Water_Spring\",\"rainfall\",waterbody_name=\"Lupa\",date_interval=(\"2019-06-01\",\"2020-06-30\"))#,marker=\".\",linestyle=\"\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rainfall data looks aggregated in two different ways, first aggregated monthly and then aggregated daily (only in 2020). \nSo we monthly-aggregate 2020 data to make it consistent with the previous data. Then we smooth the full dataset in order to make it less jumpy."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Lupa\"][[\"date\",\"rainfall_terni\"]].copy()\n\ndata[\"year\"] = data[\"date\"].dt.year\ndata[\"month\"] = data[\"date\"].dt.month\ndata = data.merge(data.groupby([\"year\",\"month\"]).mean().reset_index(),on=[\"year\",\"month\"],how=\"left\",suffixes=('_old', '_new'))\n\nplt.figure(figsize=(15,5))\nax = plt.gca()\ndata.loc[3500:5000,:].plot(x=\"date\",y=\"rainfall_terni_old\",grid=True,figsize=(15,5),style=[\"-k\"],alpha=0.5,ax=ax)\n\n\nmsk = data['date'].map(lambda x: x.day) == 15\ndata.loc[~msk,\"rainfall_terni_new\"]=np.nan\ndata[\"rainfall_terni_new\"] = data[\"rainfall_terni_new\"].interpolate(method=\"akima\")\ndata.loc[3500:5000,:].plot(x=\"date\",y=\"rainfall_terni_new\",grid=True,figsize=(15,5),style=\"-b\",ax=ax)\n\ndf[\"Water_Spring\"][\"Lupa\"][\"rainfall_terni\"] = data[\"rainfall_terni_new\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Water Spring Madonna di Canneto"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Madonna_di_Canneto\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrList = getGlobalCorrList(\"rainfall\",removeDuplicates=True)\ncorrList = getGlobalCorrList(\"rainfall\",isMirrored=True,removeDuplicates=True)\ncorrList = corrList.loc[corrList[\"variable1\"]==\"rainfall_settefrati\",[\"type2\",\"name2\",\"variable2\",\"corr_value\"]].sort_values(by=\"corr_value\",ascending=False)\ncorrList.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is only one rainfall variable, and it is missing data from the beginning of 2019 onwards. This values will negatively impact the forecast. \n\nWe also see that there is no variable highly correlated, so we need to predict the value based on rainfall from mulltiple other locations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the dates where the rainfal needs to be predicted\n\n\ndf[\"Water_Spring\"][\"Madonna_di_Canneto\"] = df[\"Water_Spring\"][\"Madonna_di_Canneto\"].loc[~df[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"date\"].isnull(),:] # remove dates where null\n\nindices = np.where(df[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"rainfall_settefrati\"].isnull())[0]\n\nstartDate = df[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"date\"].iloc[indices[0]]\nendDate = df[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"date\"].iloc[indices[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Madonna_di_Canneto\"][[\"date\",\"rainfall_settefrati\"]].copy()\ndata_train = data.loc[data[\"date\"].dt.date<startDate,:]\ndata_pred = data.loc[(data[\"date\"].dt.date>=startDate) & (data[\"date\"].dt.date<=endDate),:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge all rainfall data into one training dataset\ndata = df[\"Water_Spring\"][\"Madonna_di_Canneto\"][[\"date\",\"rainfall_settefrati\"]].copy()\ndata_train = data.loc[data[\"date\"].dt.date<startDate,:]\ndata_pred = data.loc[(data[\"date\"].dt.date>=startDate) & (data[\"date\"].dt.date<=endDate),:]\n\ntrain_index = data_train.index\npred_index = data_pred.index\n\n#select rainfall locations that can cover the missing rainfall data in Madonna di Canneto   \nndates = len(indices) # number of days with missing values\nfor ii,row in corrList.iterrows():\n    var_name = row[\"variable2\"]\n    if row[\"corr_value\"]>=0.25: # taregt variable has correlation of at least 25%\n        data = df[row[\"type2\"]][row[\"name2\"]][[\"date\",var_name]].copy()\n        data4train = data.loc[(data[\"date\"].dt.date<startDate),[\"date\",var_name] ]\n        data4pred = data.loc[(data[\"date\"].dt.date<=endDate) & (data[\"date\"].dt.date>=startDate),[\"date\",var_name]]\n        if data4pred[var_name].isnull().sum()<.05*ndates: # variable has less than 5% of missing values in the prediction date range\n            if data4train[var_name].isnull().sum()<.1*ndates: # variable has less than 10% of missing values in the training date range\n                data_train = data_train.merge(data4train,on=\"date\",how=\"left\")\n                data_pred = data_pred.merge(data4pred,on=\"date\",how=\"outer\")\n\ndata_train.index =  train_index\ndata_pred.index =  pred_index\n\n# interpolate for missing values up to 3 days\ndata_train = data_train.interpolate(method=\"linear\",limit=3)\ndata_pred = data_pred.interpolate(method=\"linear\",limit=3)\n#data_rain = data_rain.iloc[:,np.where(~data_rain.isnull().any())[0]] \n\n# scale dates into cyclical features in range 0-1\nday = pd.DatetimeIndex(data_train[\"date\"]).dayofyear.values\ndata_train[\"day_y\"] = np.sin(2*np.pi*day/365.25)\ndata_train[\"day_x\"] = np.cos(2*np.pi*day/365.25)\ndata_train[\"day_y\"] = (data_train[\"day_y\"]+1)/2\ndata_train[\"day_x\"] = (data_train[\"day_x\"]+1)/2\ndata_train.drop(\"date\",axis=1,inplace=True)\n\nday = pd.DatetimeIndex(data_pred[\"date\"]).dayofyear.values\ndata_pred[\"day_y\"] = np.sin(2*np.pi*day/365.25)\ndata_pred[\"day_x\"] = np.cos(2*np.pi*day/365.25)\ndata_pred[\"day_y\"] = (data_pred[\"day_y\"]+1)/2\ndata_pred[\"day_x\"] = (data_pred[\"day_x\"]+1)/2\ndata_pred.drop(\"date\",axis=1,inplace=True)\n\ndata_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's setup the train and test sets for the machine learning job"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def selectFeatures(data,vif_thr=1.5):\n    ## uses variance inflation factor to select most relevenat features\n    from statsmodels.stats.outliers_influence import variance_inflation_factor \n\n    vif_thr = 1.5 # the maximum VIF acceptable for an indepndent variable to have no collinearity with another variable\n\n    data_sel = data.copy()\n    # method is to repeatedly compute VIF for every variable and remove the highest VIF, until only varibales with VIF<=vif_thr are left  \n    while True:\n\n        vif_data = pd.DataFrame() \n        vif_data[\"feature\"] = data_sel.columns \n\n        # calculating VIF for each independent variable \n        vif_data[\"VIF\"] = [variance_inflation_factor(data_sel.values, i) for i in range(len(data_sel.columns))] \n\n        vif_iimax = vif_data[\"VIF\"].idxmax()\n        if vif_data.loc[vif_iimax,\"VIF\"]<=vif_thr:\n            break\n        data_sel.drop(columns=[vif_data.loc[vif_iimax,\"feature\"]],inplace=True)\n    return(data_sel)\n\n\ndef print_results(results,verbose=False):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n    print('BEST SCORE: {}\\n'.format(round(results.best_score_,3)))\n\n    if verbose:\n        means = results.cv_results_['mean_test_score']\n        stds = results.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, results.cv_results_['params']):\n            print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n        \n\ndef evaluate_model(name, model, features, labels,pr_plot=False):\n    '''\n    Evaluates and prints model performance indicators:\n    - prediction runtime \n    - accuracy\n    - precision\n    - recall\n    '''\n    from time import time\n    from sklearn.metrics import accuracy_score, precision_score, recall_score\n    from sklearn.metrics import precision_recall_curve\n    start = time()\n    predictions = model.predict(features)\n    end = time()\n    accuracy = round(accuracy_score(labels, predictions), 3)\n    precision = round(precision_score(labels, predictions), 3)\n    recall = round(recall_score(labels, predictions), 3)\n    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,accuracy,precision,recall,\n                                                                                round((end - start)*1000, 1)))\n    if pr_plot:\n        probs = model.predict_proba(features)\n        precision, recall, thresholds = precision_recall_curve(labels, probs[:,1])\n        plt.plot(thresholds,precision[:-1],label=\"precision\")\n        plt.plot(thresholds,recall[:-1],label=\"recall\")\n        plt.xlabel(\"binary threshold\")\n        plt.legend()\n        plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data_train.drop('rainfall_settefrati', axis=1) # remove the label column from the dataset\nlabels = data_train['rainfall_settefrati'] # use only the label column\n\nX_train, X_val, y_train, y_val = train_test_split(features,labels,test_size=0.2,shuffle=True,random_state=42) # 60% training\n\nX_test = data_pred.drop('rainfall_settefrati', axis=1) # features used for final prediction\n\n\n## select most relevant features based on variance inflation\ncolnames = [x for x in X_train.columns if x not in [\"day_x\",\"day_y\"]]\nX_sel = selectFeatures(X_train[colnames],vif_thr=1.5)\nX_train = pd.concat([X_sel,X_train[[\"day_x\",\"day_y\"]]],axis=1,sort=False)\n\nX_val = X_val[X_train.columns]\nX_test = X_test[X_train.columns]\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the data has many zero values (no rain), we first need to predict the occurrence of rain as a binary classificaiton problem. \nThen we can predict amount of rain only for days when rain is predicted. \n\nIf we use only a prediction model without a classification model, the accuracy would be much worse."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select data of rain vs no rain\nrain_thrs = 0.0 # threshold for rain (can be changed depending on how noisy are the sensors, here we assume no noise)\n\n# TRAIN\ncolnames = [x for x in X_train.columns if x not in [\"day_x\",\"day_y\"]]\nX_train_bin = X_train.copy()\nX_train_bin[colnames] = (X_train_bin[colnames]>rain_thrs).astype(int)\ny_train_bin = (y_train>rain_thrs).astype(int)\n# VALIDATION\ny_val_bin = (y_val>rain_thrs).astype(int)\nX_val_bin = (X_val>rain_thrs).astype(int)\n# TEST\nX_test_bin = (X_test>rain_thrs).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We try a multi-layer perceptron and a logistic regression classifier model, using cross-validation to select the best learning parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nparameters = {\n    'C': [.001,.01,.1,1,10]\n}\n\ncv = GridSearchCV(logreg,parameters,cv=5)\ncv.fit(X_train,y_train_bin.values.ravel())\nprint_results(cv)\n\nbinary_model = cv.best_estimator_\npredictions = binary_model.predict(X_val_bin)\nevaluate_model(type(cv.estimator).__name__, binary_model, X_val_bin, y_val_bin,pr_plot=True) # evaluate the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\n\nmlp = MLPClassifier(early_stopping=False)\nparameters = {\n    'hidden_layer_sizes': [(10,1),(30,1),(10,2),(30,2)],\n    'activation': [\"relu\",\"tanh\"],\n    'learning_rate': [\"constant\",\"adaptive\"] # \"invscaling\" takes a large jump at the beginning and then slightly decreases it while we get closer to the final model, \"adaptive\" eeps learning_rate constant as long as the training loss keeps decreasing otherwise it decreases learning rate \n}\n\ncv = GridSearchCV(mlp,parameters,cv=5)\ncv.fit(X_train,y_train_bin.values.ravel())\n\nprint_results(cv)\n\nbinary_model = cv.best_estimator_\nevaluate_model(type(cv.estimator).__name__, binary_model, X_val_bin, y_val_bin,pr_plot=True) # evaluate the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The multi-layer perceptron with **binary threshold 0.45** which has more than 70% recall and precision. \n\nThen, let's implement a gradient boosting regression, after having scaled the data using boxcox transofrmation with lambda=0 (equivalent to natural logaritm)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale data\ncolnames = [x for x in X_train.columns if x not in [\"day_x\",\"day_y\"]]\n\nX_train_scaled = X_train.copy() \nX_val_scaled = X_val.copy()\nX_test_scaled = X_test.copy()\nfor col in colnames:\n    X_train_scaled[col] = stats.boxcox(X_train_scaled.loc[:,col].values+1,lmbda=0) \n    X_val_scaled[col] = stats.boxcox(X_val_scaled.loc[:,col].values+1,lmbda=0) \n    X_test_scaled[col] = stats.boxcox(X_test_scaled.loc[:,col].values+1,lmbda=0) \n\ny_train_scaled = pd.Series(stats.boxcox(y_train.values+1,0),index=y_train.index)\ny_val_scaled = pd.Series(stats.boxcox(y_val.values+1,0),index=y_val.index)\n\n# Generate datasets for regression\n\ny_train_rain = y_train_scaled[y_train_scaled>rain_thrs]\nX_train_rain = X_train_scaled.loc[y_train_rain.index,:]\n\ny_val_rain = y_val_scaled[y_val_scaled>rain_thrs]\nX_val_rain = X_val_scaled.loc[y_val_rain.index,X_train_rain.columns]\n\nX_test_rain = X_test_scaled.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\ngbr = GradientBoostingRegressor()\n\nparameters = {\n    'n_estimators': [100,200,300],\n    'max_depth': [1, 3 ,5], # depth=1 is called \"decision stump\"\n    'learning_rate': [0.001,0.01, 0.1]\n}\n\n\ncv = GridSearchCV(gbr,parameters,cv=5,scoring='neg_mean_squared_error',return_train_score=True)\ncv.fit(X_train_rain,y_train_rain.values.ravel())\n\nprint_results(cv)\n\nprediction_model = cv.best_estimator_\ny_pred = prediction_model.predict(X_val_rain)\nprint(\"R2 score: %.2f\" % r2_score(y_val_rain, y_pred))\nprint(\"MSE : %.4f\" % mean_squared_error(y_val_rain, y_pred))\n\n\n#residuals = (y_pred-y_test_rain)/y_test_rain\n#plt.plot(residuals.values,label=\"residuals\")\nplt.plot(y_pred,label=\"predictions\")\nplt.plot(y_val_rain.values,label=\"true\")\nplt.legend()\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is finally time to predict. \n\nFirst let's retrain the models on the whole data, since before we did not train on 20% of training data (due to cross-validation) and on validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train,X_val],axis=0,sort=False)\ny_train_bin = pd.concat([y_train_bin,y_val_bin],axis=0,sort=False)\nX_train_rain = pd.concat([X_train_rain,X_val_rain],axis=0,sort=False)\ny_train_rain = pd.concat([y_train_rain,y_val_rain],axis=0,sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_model.fit(X_train,y_train_bin.values.ravel())\nprediction_model.fit(X_train_rain,y_train_rain.values.ravel())\nevaluate_model(type(binary_model).__name__, binary_model, X_train, y_train_bin,pr_plot=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_threshold = 0.45\ny_pred = pd.Series(binary_model.predict_proba(X_test_bin)[:,1]>bin_threshold,index=X_test_bin.index)\n\nX_test_rain = X_test_rain.loc[y_pred,X_train_rain.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rain = pd.Series(prediction_model.predict(X_test_rain),index=X_test_rain.index)\npred_rain = np.exp(pred_rain)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = y_pred.astype(float)\ny_pred.loc[pred_rain.index] = pred_rain.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Water_Spring\"][\"Madonna_di_Canneto\"].loc[y_pred.index,\"rainfall_settefrati\"] = y_pred\nplot_WaterbodyVariable(\"Water_Spring\",\"rainfall\",waterbody_name=\"Madonna_di_Canneto\",date_interval=(\"2018-01-01\",\"2019-12-31\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the algorithm does not predict any large peak in the last quarter of the year (as in 2019), however it predicts rain for a large amount of days.  "},{"metadata":{},"cell_type":"markdown","source":"### River Arno"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"River\",\"Arno\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove old data with missing values, as it is not relevant for the forecasting"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices= np.where(~df[\"River\"][\"Arno\"].loc[:,\"rainfall_le_croci\"].isnull())[0]\ndf[\"River\"][\"Arno\"] = df[\"River\"][\"Arno\"].iloc[indices,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove locations where there  is no recent data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"River\"][\"Arno\"].drop(columns=[\"rainfall_stia\",\"rainfall_incisa\",\"rainfall_montevarchi\",\"rainfall_s_savino\",\"rainfall_laterina\",\"rainfall_bibbiena\",\"rainfall_camaldoli\",\"rainfall_consuma\",\"rainfall_vernio\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lake Bilancino"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Lake\",\"Bilancino\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove old data with missing values, as it is not relevant for the forecasting"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices= np.where(~df[\"Lake\"][\"Bilancino\"].loc[:,\"rainfall_le_croci\"].isnull())[0]\ndf[\"Lake\"][\"Bilancino\"] = df[\"Lake\"][\"Bilancino\"].iloc[indices,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Volume\nThe volume is the **OUT volume**, i.e. the volume that is taken from the pumps (and then sent to the water treatment plant).\nThe convention of the sign of volume is different for different companies, therefore there  is need to uniform the volumes to a single positive sign.\nFurthermore we analyze the data and clean it for all the waterbodies."},{"metadata":{"trusted":true},"cell_type":"code","source":"for ii, info in data_descr.iterrows():\n    w_type = info[\"type\"]\n    w_name = info[\"name\"]\n    variables = getIndependentVariables(w_type,w_name,\"volume\")\n    for vv in variables:\n        df[w_type][w_name][vv] = np.abs(df[w_type][w_name][vv]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Petrignano"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Petrignano\")\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Petrignano\",date_interval=(\"2019-01-01\",\"2019-12-31\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Petrignano\",\"volume\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aquifer Petrignano has some volume values that go abruptly to zero, so let's deal with them by conveting them to Na "},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = getIndependentVariables(\"Aquifer\",\"Petrignano\",\"volume\")\nseries = df[\"Aquifer\"][\"Petrignano\"][variable]\nindwhere = np.where(series==0)[0]\nseries.iloc[indwhere]=np.nan\ndf[\"Aquifer\"][\"Petrignano\"][variable] = series\nplot_VarDistribution(\"Aquifer\",\"Petrignano\",\"volume\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's interpolate for missing values, taking care of filling only when there are not more than 15 days of missing data. Otherwise we have not enough confidence to interpolate"},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = getIndependentVariables(\"Aquifer\",\"Petrignano\",\"volume\")\ndf[\"Aquifer\"][\"Petrignano\"][variable] = df[\"Aquifer\"][\"Petrignano\"][variable].interpolate(method='spline', order=1, axis=0, limit=15)\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Petrignano\",date_interval=(\"2019-01-01\",\"2019-12-31\"))#,marker=\".\",linestyle=\"\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Doganella"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Doganella\")\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Doganella\",date_interval=(\"2019-09-01\",\"2019-10-31\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Doganella\",\"volume\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For \"Doganella\", it looks there are periods when the wells get depleted (e.g. Oct 2019).\nGenerally, there are no particular observations to make.\nThere are possibilities to fix only sporadic data, bu data prior to the second half of 2016 cannot be reconstructed."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Aquifer\"][\"Doganella\"].copy()\nfor pozzoname, pozzodata in data.loc[:,data.columns.str.startswith(\"volume\")].iteritems():\n     df[\"Aquifer\"][\"Doganella\"][pozzoname] = pozzodata.interpolate(method='akima',limit=21)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Auser"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Auser\")\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Auser\")#,date_interval=(\"2013-12-15\",\"2014-01-15\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Auser\",\"volume\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wells have always a level different from zero, apart from two wells, where data starts to be recorded only from January 2014. \nTherefore we need to transform such values to \"not available\""},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = getIndependentVariables(\"Aquifer\",\"Auser\",\"volume\")\ndata_sel = df[\"Aquifer\"][\"Auser\"][[\"volume_csal\",\"volume_csa\"]].copy()\ndata_sel.where(data_sel!=0.,inplace=True)\ndf[\"Aquifer\"][\"Auser\"][[\"volume_csal\",\"volume_csa\"]] = data_sel\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Auser\",date_interval=(\"2013-12-15\",\"2014-01-15\"))#,marker=\".\",linestyle=\"\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Luco"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Luco\")\nplot_WaterbodyVariable(\"Aquifer\",\"volume\",waterbody_name=\"Luco\",date_interval=(\"2018-11-21\",\"2018-11-30\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Luco\",\"volume\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Between decmber 2016 and February 2017, the volume of well 4 goes unexpectedly close to 0, while normally its level is  within wells 1 and 2. However, we have no information to establish that the values are somehow wrong.\nThe data seems to have weekly pattern. Since the volume of water taken is constant along the week (apart from a minimum on Sunday, we might conclude that the volume is artificially controlled."},{"metadata":{},"cell_type":"markdown","source":"## Depth to Groundwater\n\nThis is the target variable, so let's see how it changes. First we need to take absolute values to uniform across the emasurements of waterbodies.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for ii, info in data_descr.iterrows():\n    w_type = info[\"type\"]\n    w_name = info[\"name\"]\n    variables = getTargetVariables(w_type,w_name)\n    for vv in variables:\n        df[w_type][w_name][vv] = np.abs(df[w_type][w_name][vv]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Auser\")\nplot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Auser\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Auser\",\"depth\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Auser\",date_interval=(\"2020-06-01\",\"2020-09-30\"))#,marker=\".\",linestyle=\"\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In June 2020 there are strong daily spikes, that look quite irrealistic since the depth changes from a few meters to 0 and then goes back to a few meters abruptedly. Such values look more data errors than realistic ones.\nIn order to clean up the data, we transform such \"zero\" sensor readings to \"missing\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = getIndependentVariables(\"Aquifer\",\"Auser\",\"depth\")\ndata_sel = df[\"Aquifer\"][\"Auser\"][[\"depth_to_groundwater_lt2\",\"depth_to_groundwater_sal\",\"depth_to_groundwater_cos\"]].copy()\ndata_sel.where(data_sel!=0.,inplace=True)\ndf[\"Aquifer\"][\"Auser\"][[\"depth_to_groundwater_lt2\",\"depth_to_groundwater_sal\",\"depth_to_groundwater_cos\"]] = data_sel\nplot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Auser\",date_interval=(\"2020-06-01\",\"2020-09-30\"))#,marker=\".\",linestyle=\"\",alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values are not sporadic but last longer. So, in order to predict missing values from istorical observations, we ned to use established auto-regressive techniques. \n\nThe first step is to de-seasonalize the data. So, let's get a data interval sufficiently long where there are no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getLongestValidRange(data,minlen):\n    \"\"\"\n    Returns a list of sub-series, each containing an interval of non missing values larger than minlen\n    \"\"\"\n    \n    chunks = []\n    series = data.copy()\n    while True:\n        null_ii= np.where(series.isnull())[0] # print(null_ii)\n        if len(null_ii)==0:\n            if len(series)>minlen:\n                chunks.append(series)\n            return chunks        \n        \n        #ii_diff = np.diff(null_ii) # difference between consecutive indices of null values\n        null_ii = np.insert(null_ii, 0,-1)\n        ii_diff = np.diff(null_ii) # difference between consecutive indices of null values\n        \n        mm = np.argmax(ii_diff) # index of the maximum value of the differences\n        i_start = null_ii[mm]+1\n        i_end = null_ii[mm+1]#-1 # print(\"start: %s, end: %s\"%(i_start,i_end))\n        chunk = series.iloc[i_start:i_end].copy()\n        if len(chunk)>minlen:\n            chunks.append(chunk)\n            series.iloc[i_start:i_end+1] = np.nan\n        else:\n            return chunks\nresults = getLongestValidRange(df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_lt2\"].copy(),365*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = len(results)\nfig, axs = plt.subplots(nrows=nrows, ncols=1,figsize=(20,nrows*5),sharex=False)\nfor ii,result in enumerate(results):\n    df[\"Aquifer\"][\"Auser\"].loc[result.index,[\"date\",\"depth_to_groundwater_lt2\"]].plot(x=\"date\",grid=True,ax = axs[ii])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, these are the two chunks of 2-years data without missing values. They can be used to check for seasonality. There is yearly maximum in October and yearly minimum in April. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndef seasonalDecompose(observations,dates, period, plot=True,model=\"additive\",extrapolate_trend=0,two_sided=True):\n    frame = pd.DataFrame()\n    if len(observations)>=2*period:\n        res =  seasonal_decompose(observations, period=period,model=model,two_sided=two_sided,extrapolate_trend=extrapolate_trend)#int(period/100) )#==\"freq\") # returns [observed, trend, seasonal, resid]\n        frame = pd.concat([dates, res.observed,res.trend,res.seasonal,res.resid],axis=1)\n        if plot:\n            fig, ax = plt.subplots(nrows=4, ncols=1,figsize=(15,12))\n            frame.plot(x=\"date\",y=frame.columns[1],ax=ax[0])\n            frame.plot(x=\"date\",y=\"trend\",ax=ax[1])\n            frame.plot(x=\"date\",y=\"seasonal\",ax=ax[2])\n            frame.plot(x=\"date\",y=\"resid\",marker=\".\",linestyle=\"\",alpha=0.5,ax=ax[3])\n            plt.tight_layout()\n    else:\n        print(\"There are only %d observations, but for a period of %d we need at least %d observations\" % (len(observations), period,2*period)  )\n    return frame\n\nperiod=365\ndates = df[\"Aquifer\"][\"Auser\"].loc[results[0].index,\"date\"]\nres = seasonalDecompose(results[0], dates, period, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(timeseries,window=12):\n    \"\"\"\n    Function that visually tests for stationariety of the time series. \n    It also performs \"Augmented Dickey-Fuller Test\", which is used to assess whether or not the time-series is stationary. \n    \"\"\"\n    \n    # Calculate rolling statistics\n    rolmean = timeseries.rolling(window=window).mean()\n    rolstd = timeseries.rolling(window=window).std()\n\n    # Plot rolling statistics:\n    plt.figure(figsize=(14,3))\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \n    # Perform Dickey-Fuller test:\n    from statsmodels.tsa.stattools import adfuller\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n\nseries = res.resid.dropna()\ntest_stationarity(series,window=period)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals are stationary (test statistic is lower than critical value). This means that such data can be used to predict the future. So, let's first remove the seasonal component."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSeasonality(series, samplePeriod) :\n    \"\"\"\n    From a long series (type pandas.Series), it returns seasonality described by the samplePeriod series (type pandas.Series).\n    The index of the samplePeriod matches the index of the series.\n    \n    Example:\n    a = pd.Series(np.array([1,2.02,3,4,69.1,21,34.21,4,23,2,32,21,63,54,45.8,53]),index=range(16))\n    b =  a.loc[3:9]\n    removeSeasonality(a, b) \n    \"\"\"\n    \n    max1 = samplePeriod.index[-1]\n    max2 = series.index[-1]#[\"depth_to_groundwater_lt2\"]\n    min1 = samplePeriod.index[0]\n    min2 = series.index[0]\n\n    nSamples = len(samplePeriod)#len(res.seasonal.index)\n    nForwardSamples = max2-max1\n    nForward = int(np.ceil((nForwardSamples+nSamples)/nSamples))\n\n    forwardSamples = np.tile(samplePeriod,nForward)[0:nForwardSamples+nSamples]\n\n    nBackwardSamples = min1-min2\n    nBackward = int(np.ceil((nBackwardSamples)/nSamples))\n    backwardSamples = np.tile(samplePeriod,nBackward)[-nBackwardSamples:]\n\n    seasonal = np.concatenate([backwardSamples,forwardSamples])\n    assert len(seasonal)==len(series), \"lengths are different, something is wrong\"\n\n    return seasonal\n\n    #plt.plot(a.loc[4800:5000])#.loc[min1:max1]);\n    #plt.plot(deseasonal.loc[4800:5000])\n\n    \nseasonal = getSeasonality(df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_lt2\"].copy(), res.seasonal[0:period])\ndeseasonal365 = df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_lt2\"] - seasonal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can interpolate the **deseasonalized** data. We use the \"akima\" spline method, which given the variability of the data, it reduces wiggles. We interpolate for not more than 3 weeks, in order to keep some degree of confidence."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\nax = plt.gca()\n\noriginal = pd.concat([df[\"Aquifer\"][\"Auser\"][\"date\"], deseasonal365], axis=1)\noriginal[deseasonal365.name] = original[deseasonal365.name].rolling(8).mean() # smoothing\ninterpolated = original.interpolate(method='akima')\ninterpolated.plot(x=\"date\",style=\"k\",ax=ax,alpha=0.5)\noriginal.plot(x=\"date\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we check for stationariety of residuals, in order to be sure that deseasonalization makes sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"period=365\nresults = getLongestValidRange(df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_lt2\"].copy(),period*2)\ndates = df[\"Aquifer\"][\"Auser\"].loc[res.seasonal.index,\"date\"]\nres = seasonalDecompose(results[0], dates, period, plot=True)\nseries = res.resid.dropna()\ntest_stationarity(series,window=period)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test statistic is lower than critical value at 1%. So, residuals are stationary.  Therefore we can proceed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_lt2\"] = interpolated[deseasonal365.name]+seasonal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's go to the other varibale: `depth_to_groundwater_sal`"},{"metadata":{"trusted":true},"cell_type":"code","source":"seasonal = getSeasonality(df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_sal\"], res.seasonal[0:period])\ndeseasonal365 = df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_sal\"] - seasonal\nfig = plt.figure(figsize=(15,4))\nax = plt.gca()\noriginal = pd.concat([df[\"Aquifer\"][\"Auser\"][\"date\"], deseasonal365], axis=1)\noriginal[deseasonal365.name] = original[deseasonal365.name].rolling(8).mean() # smoothing\ninterpolated = original.interpolate(method='akima')#,limit=32)\ninterpolated.plot(x=\"date\",style=\"k\",ax=ax,alpha=0.5)\noriginal.plot(x=\"date\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we add back the seasonal factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_sal\"] = interpolated[deseasonal365.name]+seasonal \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We repeat for `depth_to_groundwater_cos`"},{"metadata":{"trusted":true},"cell_type":"code","source":"period=365\ndf[\"Aquifer\"][\"Auser\"].plot(x=\"date\",y=\"depth_to_groundwater_cos\")\n\nseries = df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_cos\"]\ninterpolated = series.interpolate(method='akima',limit=7) # we first need to interpolate because we do not have a chunk sufficiently long\nresults = getLongestValidRange(interpolated,period*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = df[\"Aquifer\"][\"Auser\"].loc[results[0].index,\"date\"]\nres = seasonalDecompose(results[0], dates, period, plot=True)\nseries = res.resid.dropna()\ntest_stationarity(series,window=period)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seasonal = getSeasonality(df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_cos\"], res.seasonal[0:period])\ndeseasonal365 = df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_cos\"] - seasonal\nfig = plt.figure(figsize=(15,4))\nax = plt.gca()\noriginal = pd.concat([df[\"Aquifer\"][\"Auser\"][\"date\"], deseasonal365], axis=1)\noriginal[deseasonal365.name] = original[deseasonal365.name].rolling(7).mean() # smoothing\ninterpolated = original.interpolate(method='akima')\ninterpolated.plot(x=\"date\",style=\"k\",ax=ax,alpha=0.5)\noriginal.plot(x=\"date\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Auser\"][\"depth_to_groundwater_cos\"] = interpolated[deseasonal365.name]+seasonal ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Petrignano"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Petrignano\")\nplot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Petrignano\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Petrignano\",\"depth\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that the two target variables are correlated with each other.\nSo, in order to fill na, we use the substitutino method (already used for temperature and rainfall), which consists of substituting missing values of one variable with non-missing values of the other variable (properly normalized), and vice-versa. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fixMissingValues(\"depth_to_groundwater_p24\",\"depth_to_groundwater_p25\");\nfixMissingValues(\"depth_to_groundwater_p25\",\"depth_to_groundwater_p24\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's use interpolation to fill remaining missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"series = df[\"Aquifer\"][\"Petrignano\"][\"depth_to_groundwater_p25\"].copy()\ninterpolated = series.interpolate(method='akima',limit=7) # we first need to interpolate because we do not have a chunk sufficiently long\ndf[\"Aquifer\"][\"Petrignano\"][\"depth_to_groundwater_p25\"] = interpolated\n\nseries = df[\"Aquifer\"][\"Petrignano\"][\"depth_to_groundwater_p24\"].copy()\ninterpolated = series.interpolate(method='akima',limit=7) # we first need to interpolate because we do not have a chunk sufficiently long\ndf[\"Aquifer\"][\"Petrignano\"][\"depth_to_groundwater_p24\"] = interpolated\n\nplot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Petrignano\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aquifer Luco"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Aquifer\",\"Luco\")\nplot_WaterbodyVariable(\"Aquifer\",\"depth\",waterbody_name=\"Luco\",date_interval=(\"2014-01-01\",\"2015-01-01\"))#,marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Aquifer\",\"Luco\",\"depth\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable `depth_to_groundwater_podere_casetta` misses values during the most recent period (2020). \nSince we do want to use this interval as test-data, let's create **synthetic** taregt values for this interval by using auto-regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n##prophet reqiures a pandas df at the below config ( date column named as DS and the value column as Y)\n#ts.columns=['ds','y']\n#model = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \n#model.fit(ts) #fit the model with your dataframe\n\n\ndf_sub = df[\"Aquifer\"][\"Luco\"][[\"date\",\"depth_to_groundwater_podere_casetta\"]].copy()\ndf_sub.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(df_sub) #fit the model with your dataframe\nfuture = model.make_future_dataframe(periods=0)\nforecast = model.predict(future)\n\nfig1 = model.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seasonality changes over time, so we need to fix the dates of seasonality changes by visual inspection. \nThis should improve the quality of the prediction considering the available values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.plot import add_changepoints_to_plot\nnewchangepoints = df_sub.loc[df_sub[\"ds\"].isin(['2017-06-01','2018-01-01']),\"ds\"]\nchangepoints = pd.concat([model.changepoints,newchangepoints],sort=False)\nmodel = Prophet(changepoints=changepoints, yearly_seasonality=True,changepoint_prior_scale=0.8, interval_width = 0.95)\nforecast = model.fit(df_sub).predict(future)\nfig2 = model.plot(forecast)\na = add_changepoints_to_plot(fig2.gca(), model, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.figure(figsize=(15,5)).gca()\ndata = df[\"Aquifer\"][\"Luco\"][[\"date\",\"depth_to_groundwater_podere_casetta\"]].copy()\ndata.plot(x=\"date\",y=\"depth_to_groundwater_podere_casetta\",style=\"b.\",ax=ax)\nlocwhere = data[\"depth_to_groundwater_podere_casetta\"].isnull()\ndata.loc[locwhere,\"depth_to_groundwater_podere_casetta\"] = forecast.loc[locwhere,\"yhat\"].values\ndata.plot(x=\"date\",y=\"depth_to_groundwater_podere_casetta\",style=\"r-\",ax=ax,grid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Aquifer\"][\"Luco\"][\"depth_to_groundwater_podere_casetta\"] = data[\"depth_to_groundwater_podere_casetta\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Flow Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Madonna_di_Canneto\")\nplot_WaterbodyVariable(\"Water_Spring\",\"flow\",waterbody_name=\"Madonna_di_Canneto\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Water_Spring\",\"Madonna_di_Canneto\",\"flow\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_ii = np.where(df[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"date\"].isnull())[0]\ndf[\"Water_Spring\"][\"Madonna_di_Canneto\"].drop(index=null_ii,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many missing values and the distribution is very skewed, meaning that there are two different peaks. \nMoreover, there is no visible seasonality. \nThere are many spikes of one day, which might indicate either data errors or real behavior (for example, the flow rate might be controlled by a pump which shuts down for one day). However, the instructors said that they provided with raw data which need to be cleaned up, because they come from measures using sensors located in the field, exposed by their nature to natural weather, therefore subject to malfunction."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_WaterbodyVariable(\"Water_Spring\",\"flow\",waterbody_name=\"Madonna_di_Canneto\")#,date_interval=(\"2017-06-01\",\"2017-12-31\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Madonna_di_Canneto\"].copy()\ndata[\"smoothed\"] = data[\"flow_rate_madonna_di_canneto\"]\ndata[\"smoothed\"] = data[\"smoothed\"].rolling(window=3).median() # a median of 3 days removes one-day spikes\ndata[\"interpolated\"] = data[\"smoothed\"].interpolate(method='akima')\ndata.loc[1000:1330,:].plot(x=\"date\",y=[\"flow_rate_madonna_di_canneto\",\"interpolated\"],style=[\".\",\"-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Water_Spring\"][\"Madonna_di_Canneto\"][\"flow_rate_madonna_di_canneto\"] = data[\"interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_WaterbodyVariable(\"Water_Spring\",\"flow\",waterbody_name=\"Madonna_di_Canneto\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Water Sping Lupa"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Water_Spring\",\"Lupa\")\nplot_WaterbodyVariable(\"Water_Spring\",\"flow\",waterbody_name=\"Lupa\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Water_Spring\",\"Lupa\",\"flow\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the time series, it seems the flow rate measure is more stable than for other acquifers. It is visible an abrupt spike that goes to 0, which is most likely a sensor malfunction.  \n\nThere are also casual missing values, whihc can be easily fixed by interpolation."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Water_Spring\"][\"Lupa\"].copy()\ndata[\"fixed\"] = data[\"flow_rate_lupa\"]\ndata.loc[data[\"fixed\"]==0,\"fixed\"] = np.nan\n\ndata[\"interpolated\"] = data[\"fixed\"].interpolate(method='akima')\ndata.plot(x=\"date\",y=[\"flow_rate_lupa\",\"interpolated\"],style=[\".\",\"-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Water_Spring\"][\"Lupa\"][\"flow_rate_lupa\"] = data[\"interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lake Bilancino"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Lake\",\"Bilancino\")\nplot_WaterbodyVariable(\"Lake\",\"flow\",waterbody_name=\"Bilancino\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Lake\",\"Bilancino\",\"flow\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be a minimum flow rate of a few liters per second, however, there are some spikes during the year which look seasonal."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"Lake\"][\"Bilancino\"].copy()\ndata[\"interpolated\"] = data[\"flow_rate\"]\n#data[\"smoothed\"] = data[\"smoothed\"].rolling(window=3).median() # a median of 3 days removes one-day spikes\ndata[\"interpolated\"] = data[\"interpolated\"].interpolate(method='akima')\ndata.plot(x=\"date\",y=[\"interpolated\",\"flow_rate\"],style=[\"-\",\"-\"],grid=True,figsize=(15,5))#,legend=False)\n\ndf[\"Lake\"][\"Bilancino\"][\"flow_rate\"] = data[\"interpolated\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hydrometry\n### River Arno"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"River\",\"Arno\")\nplot_WaterbodyVariable(\"River\",\"hydrometry\",waterbody_name=\"Arno\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"River\",\"Arno\",\"hydrometry\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has a clear distribution (*gamma*), which means that there are no major outliers.  \nThere are some zero sensor readings to correct and some missing values to interpolate."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"River\"][\"Arno\"].copy()\ndata[\"fixed\"] = data[\"hydrometry_nave_di_rosano\"]\ndata.loc[data[\"fixed\"]==0,\"fixed\"] = np.nan\n\ndata[\"fixed\"] = data[\"fixed\"].interpolate(method='akima',limit=5)\ndata.loc[3000:4500,:].plot(x=\"date\",y=[\"hydrometry_nave_di_rosano\",\"fixed\"],style=[\".\",\"-\"],grid=True,figsize=(15,5))#,legend=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values for second half of 2018 are unavailable, so we use the same values form the previosu year, as the first half of the year looks pretty much the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[\"River\"][\"Arno\"].copy()\nstart_date08 = datetime.strptime(\"2008-07-01\", '%Y-%m-%d').date() # datetime.strptime(\"2015-06-01\", '%Y-%m-%d').date()\nend_date08 = datetime.strptime(\"2008-12-31\", '%Y-%m-%d').date()#datetime.strptime(\"2016-03-07\", '%Y-%m-%d').date()\nstart_date07 = datetime.strptime(\"2007-07-01\", '%Y-%m-%d').date() # datetime.strptime(\"2015-06-01\", '%Y-%m-%d').date()\nend_date07 = datetime.strptime(\"2007-12-31\", '%Y-%m-%d').date()#datetime.strptime(\"2016-03-07\", '%Y-%m-%d').date()\n\ndata07 = data.loc[((data[\"date\"].dt.date>start_date07) & (data[\"date\"].dt.date<end_date07)),\"hydrometry_nave_di_rosano\"]\ndata.loc[((data[\"date\"].dt.date>start_date08) & (data[\"date\"].dt.date<end_date08)),\"hydrometry_nave_di_rosano\"] = data07.values\ndata.loc[3000:4500,:].plot(x=\"date\",y=\"hydrometry_nave_di_rosano\",grid=True,figsize=(15,5))\n\ndf[\"River\"][\"Arno\"][\"hydrometry_nave_di_rosano\"] = data[\"hydrometry_nave_di_rosano\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lake Level\n### Lake Bilancino"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMissingValues(\"Lake\",\"Bilancino\")\nplot_WaterbodyVariable(\"Lake\",\"lake_level\",waterbody_name=\"Bilancino\")#,date_interval=(\"2020-06-01\",\"2020-09-30\"),marker=\".\",linestyle=\"\",alpha=0.5)\nplot_VarDistribution(\"Lake\",\"Bilancino\",\"lake_level\",stacked=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning for Prediction\n\n#### Data splitting and cross-validation\nFor each model we create train/validation/test splits. However, time series modeling is different from time-independent supervised learning. There are intrinsic interrelationships between the data points measured across time. This means that during training and testing, the temporal structure of the series needs to be maintained. Shuffling the data from different dates causes bias. The **rolling-window** (or walk-forward) validation is best suited for time series-based forecasting because it avoids such biases and facilitates updating the models as new data comes in.\n\n- **training** set will be used to select the best combination of parameters for a given algorithm and set of features. Thus, different hyperparameters will be chosen for the same algorithm in order to come up with the best model for the given algorithm. Within the training-set the rolling window method is used to select the best model.\n- **validation** set will be used to test models from different algorithms against each other and select a winner for each prediction time in the future. Before validating the data, the model is retrained on all the previous data (in order to reduce model variance, as described here https://stats.stackexchange.com/questions/395645/does-retraining-a-model-on-all-available-data-necessarily-yield-a-better-model).\n- **test** set will be used to finally get the final score for the preselected model.\n\n\nOn the training set, the best combination of parameters for a given algorithm is found using **blocked cross-validation**, which consists of leaving a gap between each training fold and its relative validation fold, as well as between different training folds. The approach is described in this paper: https://www.sciencedirect.com/science/article/abs/pii/S0304407600000300. This procedure will return a more reliable outcome because will avoid **data leakage** between different sets and different folds. In other words, it will incrrease independency of folds/sets. Thus, in the training set there is a series of training and validation folds, typically 5. The training prediction accuracy for a model is computed by averaging the scores obtained from training the model on each trainign fold and running it on the corresponding validation fold. \n\n\n#### Independent variables\n\nAs explained by the organizer in the competition forum, in the example below the first of the two options is preferred although the second option can also be tried.\n\nConsidering the aim of predicting the target variable at (t+1): \n\n**Option 1 (Future observations unknown)** - the information available for predicting the target at (t+1) is:\ntarget variable at times (t), (t-1), (t-2), etc…\nother variables (rainfall etc.) at times (t), (t-1), (t-2), etc… \n\n**Option 2 (Future observations known)** - the information available for predicting the target at (t+1) is: \ntarget variable at times (t), (t-1), (t-2), etc…\nother variables (rainfall etc.) at times (t+1), (t), (t-1), (t-2), etc…\n\nIn other words, although it is acceptable to use data for the other variables (temperature, rainfall etc) at time (t+1), the business needs first and foremost to rely on current/past observations, beause the objective is **pure forecasting**.\n\n\n#### Target variables\n\nIn order to ensure target variables are consistent, we ensure all targets are variations rather than absolute quantities. For example, \"depth to groundwater\" is converted to \"daily change of depth\".  \n\n\n#### Prediction intervals\nThe required prediction is of \"multi-step\" kind, meaning that multiple predictions, each for a different time interval in the future, will be made. The performance of the model will be evaluated, for each prediction interval, using MSE. For example, considering an interval of 15 days, we average the MSE for the 15 days to come up with the 15-days-interval's MSE. Then we compare all the intervals in order to verify up to which interval we can consider a forecast acceptable according to the mean MSE of each interval.\n\nBroadyl speaking, multi-step forecasts can be recursive or not. The *recursive multi-step* approach, consists of a single model that uses as input data both observations and predictions. The *non-recursive* approach, on the other hand, does not use any future prediction but only current and past observations, therefore each prediction interval needs a different model.\n\nAs mentioned by the organizers, the multi-step forecast **must not** be recursive \"*if you have data observations until 31.12.2020 (t) and you try to predict the “Depth_to_Groundwater” at date 30.01.2021 (t+1) you are allowed to use all data, also the Depth_to_Groundwater measured in November (t-1), October (t-2), September (t-3) etc. Please notice also that, if you are willing to predict Depth_to_Groundwater at date 28.02.2021(t+2), in order to understand how far the prediction can go, you can use exclusively the given data, this means you cannot include either your estimated “Depth_to_Groundwater” value at 30.01.2021 (t+1) or temperature/rainfall weather predictions for the month of February (t+2). We want models based exclusively on observed data, basically you cannot use inferred data to make further predictions.*\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BlockingTimeSeriesSplit():\n    def __init__(self, n_splits, train_ratio=0.7, gap_ratio=0.1):\n        # train_gap ratios, indicates the portions of samples going to train set and gap respectively, where the sum of samples is 1.0\n        self.n_splits = n_splits\n        assert train_ratio+gap_ratio<1\n        self.train_ratio = train_ratio\n        self.gap_ratio = gap_ratio\n\n    def get_n_splits(self, X, y, groups):\n        return self.n_splits\n    \n    def split(self, X, y=None, groups=None):\n        n_samples = len(X)\n        k_fold_size = n_samples // self.n_splits\n        indices = np.arange(n_samples) #X.index\n\n        for i in range(self.n_splits):\n            start = i * k_fold_size\n            stop = start + k_fold_size\n            mid = int(self.train_ratio * (stop - start)) + start\n            gap = int(self.gap_ratio * (stop - start))\n            yield indices[start: mid], indices[mid + gap: stop]\n            \n## function used to plot train/validation folds for the time series \ndef plot_cv_indices(cv, X,ax, lw=20):\n    \"\"\"\n    Create a sample plot for indices of a cross-validation object.\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\n    \"\"\"\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=None)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n    \n        # Visualize the results\n        cmap_cv = plt.cm.coolwarm\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv,vmin=-.2, vmax=1.2)\n        ax.set_yticklabels(\"\")\n        \n    return ax\n\n\n\ndef plot_scores(searchers,ax=None,label=None):\n    best_scores = np.empty(len(searchers))\n    for ii,y_name in enumerate(searchers.keys()):\n        best_scores[ii] = searchers[y_name].best_score_\n    if ax is None:\n        plt.plot(np.arange(len(searchers))+1,best_scores,marker=\".\",label=label)\n        plt.xlabel(\"+days (\"+\"_\".join(y_name.split(\"_\")[:-1])+\")\" )\n        plt.ylabel(searchers[y_name].refit)\n        plt.grid()\n        ax = plt.gca()\n    else:\n        ax.plot(np.arange(len(searchers))+1,best_scores,marker=\".\",label=label)\n    ax.legend()\n    return ax\n    \n\ndef build_multi_output(y,pred_int,X=None,as_variation=False):\n    \"\"\"\n    From a dataframe containing one or more output variables, it creates multiple lagged columns (pandas dataframe) to cover the desired prediction interval.\n    `pred_int` is the length of the prediction interval.\n    It returns the modified output y. If input matrix is also given, pred_int samples are removed from both input and output matrices in order to avoid missing values in the outputs due to lag.\n    If `as_variation` is True, the lagged targets are given as differences with respect to the initial target, otherwise they are absolute.\n    \"\"\"\n\n    assert type(y)== pd.core.frame.DataFrame, \"y must be of type dataframe\"\n    varnames = list(y.columns)\n    ymulti = pd.DataFrame(index=y.index)\n    \n    for varname in varnames:\n        for lag in np.arange(pred_int)+1:\n            if as_variation:\n                ymulti[varname+\"_+%s\"%str(lag).zfill(3)] = y[varname].shift(-lag) - y[varname]\n            else:\n                ymulti[varname+\"_+%s\"%str(lag).zfill(3)] = y[varname].shift(-lag)\n    \n    if X is not None:\n        # remove last rows with missing values due to lags\n        ymulti = ymulti.iloc[0:ymulti.shape[0]-pred_int,:]\n        assert type(X)== pd.core.frame.DataFrame, \"X must be of type dataframe\"\n        Xmulti = X.loc[ymulti.index,:]\n        return (ymulti,Xmulti)\n    else:\n        return ymulti\n\ndef fitMulti_GridSearchCV(X,y,gscv,X_y=None):\n    \"\"\"\n    creates and fits a gridSearchCV object for each column of the target dataframe y.\n    It returns a dictionary of fitted gridSearchCV objects, one for each column of the target dataframe y.\n    If X_y is given, it attaches the relative features X_y to the input features X.\n    The X_y features are selected by matching the part of their names after the last \"_\" with the same part of the name of the column y.\n    \"\"\"\n    import copy\n    \n    searchers = {}\n    mapper = {}\n    for iii,(y_name,y_data) in enumerate(y.iteritems()): # run a grid search for each class, so that we can get a score for each optimal model\n        this_gscv = copy.deepcopy(gscv)\n        mapper[iii] = y_name\n        print(\"searching best model for class %s\"%y_name,end=\"\\r\")\n        searchers[y_name] = this_gscv\n        if X_y is None:\n            X_sel = X\n        else:\n            X_y_sel = X_y.loc[:,X_y.columns.str.endswith(y_name.split(\"_\")[-1])]\n            X_sel = pd.concat([X,X_y_sel],sort=False,axis=1)\n            #print(list(X_sel.columns))\n        searchers[y_name].fit(X_sel, y_data.values.ravel());\n    if X_y is None:\n        feature_names = list(X.columns)\n    else:    \n        feature_names = list(X.columns)+[\"_\".join(fname.split(\"_\")[:-1]) for fname in list(X_y_sel.columns)]\n    return (searchers,mapper,feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup\nDefine the future prediction interval and define where the data will be stored."},{"metadata":{"trusted":true},"cell_type":"code","source":"### parameters for splitting data\n\nT_set = 365 # is the period for each validation and test set (in days), while the remaining data is used for training \nT_pred = 30 # maximum prediction interval in the future \n\n### Define where data will be stored\n### Hierarchy Aquifer name > target name > predition day\n\nresults_estimators = {} # save all the chosen estimators (one for each time)\nresults_RMSE = {} # store root mean squared error score\nresults_MAE = {} # store mean absolute error score\nresults_data = {} # store predicted vs true data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aquifer Petrignano"},{"metadata":{},"cell_type":"markdown","source":"### Prepare data"},{"metadata":{"trusted":true},"cell_type":"code","source":"wtype = \"Aquifer\"\nwname = \"Petrignano\"\n\nresults_estimators[wtype+\"_\"+wname] = {} # save all the chosen estimators (one for each time)\nresults_RMSE[wtype+\"_\"+wname] = {} # store root mean squared error score\nresults_MAE[wtype+\"_\"+wname] = {} # store mean absolute error score\nresults_data[wtype+\"_\"+wname] = {} # store predicted vs true data\n\ndata = df[wtype][wname].copy()\ndata.reset_index(drop=True,inplace=True) # index starts from 0\n\ntargets = getTargetVariables(wtype,wname)\nfeatures = data.columns[~data.columns.isin(targets)]\nX = data[features]\ny = data[targets]\nprint(\"Input samples:\")\ndisplay(X.head(3))\nprint(\"\\nOutput samples:\")\ndisplay(y.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create features and multiple targets\nIn order to improve the power of our model, we do the folling:\n- Add the current output as a feature.\n- Since the target variable is autoregressive, make output differences (differences between today and yesterday's depths) as new features.\n- Since we want to predict taregts at different times in the future, create multiple targets, one for each future prediction day and one for each output variable\n- Ensure that targets are differences rather than absolute values, so that prediction in the future is a difference with respect to current value. For example: the value of depth in 5 days is the difference between today's depth and the depth in 5 days. \n- Add seasonality features since target variables are seasonal (on yearly basis). They are: yearly seasonality of the target value (calculated on training set), target day of the year transformed into cyclical (with cosine/sin), new features by making the variables dependent on each time-shifted instance of the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"### CREATE Multiple targets\n# y = days x ntargets*T_pred\n# X = days x (nfeatures+ntargets)\n\ninput_vars = X.columns\n\n### Add current targets as features\nX = pd.concat([X,y],axis=1,sort=False)\n\n\n### Add target variables' historical mean and standard deviations as features\nrollvals = [5,10,15,20,30] # rolling windows\nfor tt in targets:\n    for rollval in rollvals:\n        rollmean = X[tt].rolling(rollval,min_periods=1).mean()\n        X.loc[:,tt+\"_mean\"+str(rollval).zfill(2)] = rollmean - rollmean.shift(rollval,fill_value=0) \n        X.loc[:,tt+\"_std\"+str(rollval).zfill(2)] = X[tt].rolling(rollval,min_periods=1).std().fillna(0).values  \n     \n\n### Add average daily temperature and volume\nvar_names = input_vars[input_vars.str.startswith((\"temperature\",\"rainfall\",\"volume\"))]\nrollvals = [1,5,10,15,20,30] # rolling values\nfor var_name in var_names:    \n    for rollval in rollvals:\n        ## calculate rolling average\n        X.loc[:,var_name+\"_mean\"+str(rollval).zfill(2)] = X.loc[:,var_name].rolling(rollval,min_periods=1).mean().values  \n\n#### SORT COLUMNS by column names ###\nX = X.reindex(sorted(X.columns), axis=1)\n\n#####################  CREATE MULTI_OUTPUTS ##################################\n\n# copy absolute values before it gets transformed\ny_absolute = y.copy()\n\n## Create multi-interval target predictions as variations with respect to current values\n(y_multi, X_multi) = build_multi_output(y,T_pred,X,as_variation=True)\n\n\n\n\n###################### ADD Seasonal values for the targets as Features ############################\nimax = data.shape[0] # assumes index starts from 0\ntraining_indices = X_multi.loc[:imax-2*T_set-1,:].index\ndata_seas = pd.concat([y_multi,X_multi.loc[training_indices,\"date\"]],sort=False,axis=1)\ndata_seas[\"dayofyear\"] = data_seas[\"date\"].dt.dayofyear\ndata_seas = data_seas.groupby(\"dayofyear\").mean()\n\ndates = X_multi[[\"date\"]].copy()\ndata = X_multi[[\"date\"]].copy()\nfor tt in targets:\n    for dd in range(1,T_pred+1):\n        tlabel = \"+\"+str(dd).zfill(3)\n        dates[tlabel] = pd.PeriodIndex(dates['date'], freq='D') + dd#).to_timestamp()\n        doy = dates[tlabel].apply(lambda x: x.dayofyear)\n        data[\"day_cos_\"+tlabel] =  (np.cos(2 * np.pi * doy.copy() /365.25)+1)/2\n        data[\"day_sin_\"+tlabel] =  (np.sin(2 * np.pi * doy.copy() /365.25)+1)/2\n        data[tt+\"_seas_\"+tlabel] = pd.merge(doy,data_seas[tt+\"_\"+tlabel],how=\"left\",left_on=tlabel,right_index=True)[tt+\"_\"+tlabel]\n\n# New seasonal features\nX_multi_y = data.drop(columns=\"date\").copy()\n\n        \n###############################################################\nprint(\"shape target matrix: \"+str(y_multi.shape))\nprint(\"shape input matrix: \"+str(X_multi.shape))\nprint(\"shape seasonal matrix: \"+str(X_multi_y.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create *training*, *validation* and *test* data sets as consecutive intervals. \nThen let's divide the trainign set in different *folds* in order to perform cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"imax = data.shape[0] # assumes index starts from 0\n\nX_test_multi = X_multi.loc[imax-T_set:,:]\ny_test_multi = y_multi.loc[imax-T_set:,:]\nX_test_multi_y = X_multi_y.loc[imax-T_set:,:]\n\nX_val_multi = X_multi.loc[imax-2*T_set:imax-T_set-1,:]\ny_val_multi = y_multi.loc[imax-2*T_set:imax-T_set-1,:]\nX_val_multi_y = X_multi_y.loc[imax-2*T_set:imax-T_set-1,:]\n\nX_train_multi = X_multi.loc[:imax-2*T_set-1,:]\ny_train_multi = y_multi.loc[:imax-2*T_set-1,:]\nX_train_multi_y = X_multi_y.loc[:imax-2*T_set-1,:]\n\nprint(\"Training set from %s to %s.\"%(X_train_multi[\"date\"].iloc[0].date(),X_train_multi[\"date\"].iloc[-1].date()))\nprint(\"Validation set from %s to %s.\"%(X_val_multi[\"date\"].iloc[0].date(),X_val_multi[\"date\"].iloc[-1].date()))\nprint(\"Test set from %s to %s.\"%(X_test_multi[\"date\"].iloc[0].date(),X_test_multi[\"date\"].iloc[-1].date()))\n\n\nbtscv = BlockingTimeSeriesSplit(n_splits=5,train_ratio=0.7,gap_ratio=0.05)#BlockingTimeSeriesSplit(n_splits=5)\nfig = plt.figure(figsize=(15, 5))\nax = fig.gca()\nax = plot_cv_indices(btscv, X_train_multi.values,ax)\n\nxticks = np.arange(0,X_train_multi.shape[0],200) #xticks = ax.get_xticks()\nax.set_xticks(xticks)\nax.set_xticklabels(X_train_multi.iloc[xticks,np.where(X_train_multi.columns==\"date\")[0][0]].dt.date,rotation=60);\nax.set_title(\"Training set's cross-validation folds\",size=14)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scale features and targets\nBy scaling features and target we do not expect any major improvement on decision tree or gradinet boosting algoirthms. \nWe cannot compare mean squared error (MSE) of scaled features with the same error on unscaled features. Since with scaling  the target variable gets expanded of one order of magnitude, the MSE becomes larger in absolute terms.\nOn the other hand, linear models, SVM and neural network are expected to perform much better, in terms of convergence time and accuracy, when variables are scaled.  \nIn order to scale the varibales in the **range 0-1**, we use the `MinMaxScaler` class, fitted on training data and then applied on validation and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n############################## SCALE TARGET-INDEPENDENT FEATURES ########################\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler_X = MinMaxScaler()\nwhichVars = ~ (\"date\" == X_train_multi.columns)\nX_train_multi.loc[:,whichVars] = scaler_X.fit_transform(X_train_multi.loc[:,whichVars])\nX_val_multi.loc[:,whichVars] = scaler_X.transform(X_val_multi.loc[:,whichVars])\nX_test_multi.loc[:,whichVars] = scaler_X.transform(X_test_multi.loc[:,whichVars])\n\n\n############################## SCALE TARGET-DEPENDENT FEATURES ########################\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler_Xy = MinMaxScaler()\nwhichVars = ~ (\"date\" == X_train_multi.columns)\nX_train_multi_y.loc[:,:] = scaler_Xy.fit_transform(X_train_multi_y)\nX_val_multi_y.loc[:,:] = scaler_Xy.transform(X_val_multi_y)\nX_test_multi_y.loc[:,:] = scaler_Xy.transform(X_test_multi_y)\n\n############################# SCALE TARGETS ###############################\nscaler_y = {}\nfor name,data in y_train_multi.iteritems():\n    scaler_y[name] = MinMaxScaler()\n    y_train_multi.loc[:,name] = scaler_y[name].fit_transform(y_train_multi[name].values.reshape(-1, 1))\n    y_val_multi.loc[:,name] = scaler_y[name].transform(y_val_multi[name].values.reshape(-1, 1))\n    y_test_multi.loc[:,name] = scaler_y[name].transform(y_test_multi[name].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Select target variable\ntarget = targets[0]\n\n## Initializes data stores\nresults_estimators[wtype+\"_\"+wname][target] = {} # save all the chosen estimators (one for each time)\nresults_RMSE[wtype+\"_\"+wname][target] = {} # store root mean squared error score\nresults_MAE[wtype+\"_\"+wname][target] = {} # store mean absolute error score\nresults_data[wtype+\"_\"+wname][target] = {} # store predicted vs true data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline model\nAs a baseline let's use a decision tree algorithm, which is fast at both training and prediction times. \nIt also allows to check how important are features"},{"metadata":{"trusted":true},"cell_type":"code","source":"############## PREPARE #######################\n\n## select features and outputs related only to this target\nothers = targets[:]\nothers.remove(target)\nsel_cols = X_train_multi.columns[~np.array([[x.startswith(other) for x in X_train_multi.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_temp = X_train_multi[sel_cols]\nX_temp = X_temp.drop(columns=\"date\").copy()\n\n## select outputs related only to this target\ny_temp = y_train_multi.copy()\ny_temp = y_temp.loc[:,y_temp.columns.str.startswith(target)]\n\nsel_cols = X_train_multi_y.columns[~np.array([[x.startswith(other) for x in X_train_multi_y.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_y_temp = X_train_multi_y[sel_cols]\n\n################### DEFINE MODEL ################\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(\n    criterion='mse', \n    splitter='random',#'best', \n    max_depth=None, \n    min_samples_split=2, \n    min_samples_leaf=1, \n    min_weight_fraction_leaf=0.0, \n    max_features=None, #('sqrt','log2')\n    random_state=None, \n    max_leaf_nodes=None, \n    min_impurity_decrease=0.0, \n    min_impurity_split=None, \n    ccp_alpha=0.0\n)\n\n## SEARCH OPTIMAL PARAMETERS\nparameters = {\n    'min_samples_split': (0.01,0.15,0.25),#(0.01,0.05,0.1,0.2,0.5,0.8) #Used to control over-fitting.\n    'splitter':(\"best\",\"random\")\n}\n\nopt_scoring = 'neg_mean_squared_error' # scoring used to select optimal parameters on the refit\n \ngscv = GridSearchCV(\n        estimator=model,\n        param_grid=parameters,\n        scoring=('r2','neg_mean_squared_error'),\n        n_jobs=-1,\n        refit=opt_scoring,\n        cv=btscv,  # change this to the splitter subject to test\n        verbose=0,\n        error_score='raise',#=-999,\n        return_train_score=True\n        )\n\n\nsearchersDTR, mapperDTR, feat_names = fitMulti_GridSearchCV(X_temp,y_temp,gscv,X_y_temp)\n\n\n############################ EVALUATE FEATURES #######################\n\nfeat_importances = pd.DataFrame(columns=feat_names,index=1+np.arange(y_temp.shape[1]))\nfeat_importances.index.name=\"days\"\nfor ii,y_name in enumerate(searchersDTR.keys()):\n    feat_importances.iloc[ii,:] = searchersDTR[y_name].best_estimator_.feature_importances_\n\nfig = plt.figure(figsize=(18,0.2*len(feat_names)))\nax = fig.gca()\nsns.heatmap(feat_importances.T.astype(float), vmin=0, vmax=1,cmap=\"coolwarm\",ax=ax)\nax.set_title(\"features importances\")\nax.set_yticks(range(feat_importances.shape[1])) # <--- set the ticks first\nax.set_yticklabels(feat_importances.columns,va=\"top\");\nplt.setp(ax.get_xticklabels(), rotation=70);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use Gradient Boosting Regression (GBR)\nGradient Bossting regressor is more performant .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel = GradientBoostingRegressor(\n    loss='ls', \n    learning_rate=0.1, \n    n_estimators=100, \n    subsample=1.0, \n    criterion='friedman_mse', \n    min_samples_split=2, \n    min_samples_leaf=1, \n    min_weight_fraction_leaf=0.0, \n    max_depth=3, \n    min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, \n    alpha=0.9, # tried (0.7,0.9,0.95) with no improvement \n    verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, \n    tol=0.0001, ccp_alpha=0.0)\n\nparameters = {\n    'learning_rate':(0.01,0.1,0.2), #(0.01,0.1,1.)\n    'n_estimators':(30,50,70), # (5,50,100,200),\n    'max_depth': (3,5,7)\n}\n\nopt_scoring = 'neg_mean_squared_error' # scoring used to select optimal parameters on the refit\n\ngscv = GridSearchCV(\n        estimator=model,\n        param_grid=parameters,\n        scoring=('r2','neg_mean_squared_error'),\n        n_jobs=-1,\n        refit=opt_scoring,\n        cv=btscv,  # change this to the splitter subject to test\n        verbose=0,\n        error_score='raise',#=-999,\n        return_train_score=True\n        )\n\n\nsearchersGBR, mapperGBR, feat_names = fitMulti_GridSearchCV(X_temp,y_temp,gscv,X_y_temp)\n\n\n\n############################ EVALUATE FEATURES #######################\n\nfeat_importances = pd.DataFrame(columns=feat_names,index=1+np.arange(y_temp.shape[1]))\nfeat_importances.index.name=\"days\"\nfor ii,y_name in enumerate(searchersGBR.keys()):\n    feat_importances.iloc[ii,:] = searchersGBR[y_name].best_estimator_.feature_importances_\n\nfig = plt.figure(figsize=(18,0.2*len(feat_names)))\nax = fig.gca()\nsns.heatmap(feat_importances.T.astype(float), vmin=0, vmax=1,cmap=\"coolwarm\",ax=ax)\nax.set_title(\"features importances\")\nax.set_yticks(range(feat_importances.shape[1])) # <--- set the ticks first\nax.set_yticklabels(feat_importances.columns,va=\"top\");\nplt.setp(ax.get_xticklabels(), rotation=70);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use Support Vector Machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR # support vector machine regression\n\nmodel = SVR(\n    kernel='rbf', degree=3, \n    gamma='scale', \n    coef0=0.0, tol=0.001, \n    C=1.0, \n    epsilon=0.01, # default was 0.1, tried: (0.001,0.01,0.1),\n    shrinking=True, cache_size=200, verbose=False, max_iter=- 1)\n\n\n\n## SEARCH OPTIMAL PARAMETERS\nparameters = {\n    'C':(1e-4,1e-3,0.01,0.1),\n    'kernel':('linear','rbf')\n}\n\nopt_scoring = 'neg_mean_squared_error' # scoring used to select optimal parameters on the refit\n \ngscv = GridSearchCV(\n        estimator=model,\n        param_grid=parameters,\n        scoring=('r2','neg_mean_squared_error'),\n        n_jobs=-1,\n        refit=opt_scoring,\n        cv=btscv,  # change this to the splitter subject to test\n        verbose=0,\n        error_score='raise',#=-999,\n        return_train_score=True\n        )\n\nsearchersSVM, mapperSVM, feat_names = fitMulti_GridSearchCV(X_temp,y_temp,gscv,X_y_temp)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Selection\nAmong the different selected best models, let's pick the best performing ones for each time interval."},{"metadata":{"trusted":true},"cell_type":"code","source":"searchers_list = {\"decision tree\": searchersDTR, \n                  \"gradient boosting\": searchersGBR,\n                  \"support vector machine\": searchersSVM,\n                 }\n\n######################################### COMPARE MODELS ##################################\nplt.figure(figsize=(15,5))\nax = plt.gca()\nfor name,ss in searchers_list.items():\n    ax = plot_scores(ss,label=name,ax=ax);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n## for each prediction time, choose best estimator\nMSE = {}\nbest_estimator = {}\n\nX_train_multi_y.columns = [col.replace(\"season_\",\"\") for col in X_train_multi_y.columns]\nX_val_multi_y.columns = [col.replace(\"season_\",\"\") for col in X_val_multi_y.columns]\n\n\n\n#### Prepare val data\nothers = targets[:]\nothers.remove(target)\nsel_cols = X_val_multi.columns[~np.array([[x.startswith(other) for x in X_val_multi.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_temp_val = X_val_multi[sel_cols]\nX_temp_val = X_temp_val.drop(columns=\"date\").copy()\ny_temp_val = y_val_multi.copy()\ny_temp_val = y_temp_val.loc[:,y_temp_val.columns.str.startswith(target)]\nsel_cols = X_val_multi_y.columns[~np.array([[x.startswith(other) for x in X_val_multi_y.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_y_temp_val = X_val_multi_y[sel_cols]\n\nfor tt in range(1,T_pred+1):\n    MSE[tt] = np.inf\n    name = target + \"_+\"+str(tt).zfill(3)\n    for ss in searchers_list.values():\n        \n        \n        ### fit estimator on training data\n        colnames = X_y_temp.columns[X_y_temp.columns.str.endswith(name.split(\"_\")[-1])]\n        X_train = pd.concat([X_temp, X_y_temp.loc[:,colnames]],sort=False,axis=1)\n        y_train = y_temp.loc[:,name].values.ravel()\n        ss[name].best_estimator_.fit(X_train,y_train)\n        \n        ### predict on validation data\n        colnames = X_y_temp_val.columns[X_y_temp_val.columns.str.endswith(name.split(\"_\")[-1])]\n        X_val = pd.concat([X_temp_val, X_y_temp_val.loc[:,colnames]],sort=False,axis=1)\n        y_pred = ss[name].best_estimator_.predict(X_val)\n        \n        ### calculate MSE as score\n        y_true = y_temp_val.loc[:,name].values#.ravel()\n        thisMSE = mean_squared_error(y_true, y_pred)  \n          \n        ### compare score with other algoithms\n        if thisMSE < MSE[tt]:\n            MSE[tt] = thisMSE\n            best_estimator[tt] = ss[name].best_estimator_\n\n    print(\"prediction on day +%d: lowest MSE = %.4f on scaled data with %s \" %(tt,MSE[tt],best_estimator[tt]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run best models on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Finally test on the most recent data\nX_test_multi_y.columns = [col.replace(\"season_\",\"\") for col in X_test_multi_y.columns]\n\n\n## Prepare\n#e.g.: wtype = \"Aquifer\"\n#e.g.: wname = \"Petrignano\"\n#e.g.: target = \"depth_to_groundwater_p24\"\n\n\n\n#### Prepare test data\nothers = targets[:]\nothers.remove(target)\nsel_cols = X_test_multi.columns[~np.array([[x.startswith(other) for x in X_test_multi.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_temp_test = X_test_multi[sel_cols]\ny_temp_test = y_test_multi.copy()\ny_temp_test = y_temp_test.loc[:,y_temp_test.columns.str.startswith(target)]\nsel_cols = X_test_multi_y.columns[~np.array([[x.startswith(other) for x in X_test_multi_y.columns] for other in others]).any(0)] # remove columns that have nothing to do with this target\nX_y_temp_test = X_test_multi_y[sel_cols]\n\n\nresults_estimators[wtype+\"_\"+wname][target] = best_estimator # save all the chosen estimators (one for each prediction day)\n\nfor tt in range(1,T_pred+1):\n    name = target + \"_+\"+str(tt).zfill(3) \n    \n    ### refit estimator on training+validation data\n    colnames = X_y_temp.columns[X_y_temp.columns.str.endswith(name.split(\"_\")[-1])]\n    X_train = pd.concat([X_temp, X_y_temp.loc[:,colnames]],sort=False,axis=1)\n    X_val = pd.concat([X_temp_val, X_y_temp_val.loc[:,colnames]],sort=False,axis=1)\n    \n    X_in = pd.concat([X_train,X_val],axis=0,sort=False)\n    y_train = y_temp.loc[:,name]\n    y_val = y_temp_val.loc[:,name]\n    y_in = pd.concat([y_train,y_val],axis=0,sort=False).values.ravel()\n    best_estimator[tt].fit(X_in,y_in)\n    \n    ### predict on test data\n    colnames = X_y_temp_test.columns[X_y_temp_test.columns.str.endswith(name.split(\"_\")[-1])]\n    X_test = pd.concat([X_temp_test, X_y_temp_test.loc[:,colnames]],sort=False,axis=1)\n    y_pred = best_estimator[tt].predict(X_test.drop(columns=\"date\"))\n    y_true = y_temp_test.loc[:,name].values#.ravel()\n    \n    ### scale back to original values\n    y_pred = scaler_y[name].inverse_transform(y_pred.reshape(-1,1))\n    \n    \n    ### reconstruct absolute values from differences\n    y_true = y_absolute[target].shift(-tt).loc[X_test.index]\n    y_pred = y_absolute.loc[X_test.index,target].values.reshape(-1,1) + y_pred\n\n    ### calculate MSE (on unscaled data) as score\n    RMSE_test = mean_squared_error(y_true, y_pred,squared=False)\n    MAE_test = mean_absolute_error(y_true, y_pred)\n\n    ### plot predicted vs true data\n    plt.figure(figsize=(15,5))\n    ax=plt.gca()\n    result = pd.DataFrame({'date':X_test[\"date\"].values,'true':y_true.ravel(), 'predicted':y_pred.ravel()})\n    result.plot(x=\"date\",style=\"-o\",ylabel=target,ax=ax,grid=True,\n                title=\"%s %s - %s prediction for day +%d\\ntest data scores with %s: RMSE = %.4f, MAE = %.4f\"%(wtype,wname,target,tt,best_estimator[tt],RMSE_test,MAE_test))\n    plt.show()\n    \n    ### store data\n    results_RMSE[wtype+\"_\"+wname][target][tt] = RMSE_test\n    results_MAE[wtype+\"_\"+wname][target][tt] = MAE_test\n    results_data[wtype+\"_\"+wname][target][tt] = result\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################### PLOT SCORES ##################################\nplt.figure(figsize=(15,5))\nax = plt.gca()\nax.plot(results_RMSE[wtype+\"_\"+wname][target].keys(),results_RMSE[wtype+\"_\"+wname][target].values(),label=\"RMSE\",lw=3,marker=\"o\")#,style=\"o-\")\nax.plot(results_MAE[wtype+\"_\"+wname][target].keys(),results_MAE[wtype+\"_\"+wname][target].values(),label=\"MAE\",lw=3,marker=\"o\")#,style=\"o-\")\nplt.grid()\nplt.legend()\nax.set_xlabel(\"prediction days\",size=14);\nax.set_title(target)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}