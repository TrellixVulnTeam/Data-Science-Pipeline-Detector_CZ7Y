{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Smart Water Analytics"},{"metadata":{},"cell_type":"markdown","source":"In this competition, Acea Group focuses on the water sector to preserve water bodies by forecasting the water level. Each dataset has a different kind of waterbody with its unique behavior and characteristics. Available Datasets are:\n\n1. Aquifer (Auser, Doganella, Luco, Petrignano)\n2. Water Spring (Amiata, Lupa, Madonna di Canneto)\n3. River (Arno)\n4. Lake (Bilancino)\nThe models' predictive power will be evaluated with both Mean Absolute Error (MAE) and Mean Square Error (MSE)."},{"metadata":{},"cell_type":"markdown","source":"# AQUIFER"},{"metadata":{},"cell_type":"markdown","source":"## 1. AUSER"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install statsmodels --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data= pd.read_csv(\"../input/acea-water-prediction/Aquifer_Auser.csv\",parse_dates=True)\nauser_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', auser_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different datatypes present in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Date column has data type 'object' which need to be changed to 'DateTime'"},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data['Date'] = pd.to_datetime(auser_data.Date, format = '%d/%m/%Y')\n\nauser_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Output Features/ Dependent variables are:\n\n1. Depth_to_Groundwater_SAL\n2. Depth_to_Groundwater_LT2\n3. Depth_to_Groundwater_CoS"},{"metadata":{},"cell_type":"markdown","source":"A time series is a series of data points indexed (or listed or graphed) in time order. Thus it is a sequence of discrete-time data. Since I want the “DATE” column as our index, but simply by reading, it is not doing it, so we have to add some extra parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data= auser_data.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we are plotting the Output variables of Auser_Aquifier. As we can observe there is huge amount of data which is missing from the intial years."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_auser= auser_data[[\"Depth_to_Groundwater_SAL\",\"Depth_to_Groundwater_LT2\",\"Depth_to_Groundwater_CoS\"]]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,7))\nsns.color_palette(\"husl\", 9)\n#sns.lineplot(data=df_auser)\ndf_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((auser_data.isnull() | auser_data.isna()).sum() * 100 / auser_data.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking at the percentage of missing values in each columns we will remove the columns with missing value % more than 50% because it might afftec the performance of our model.\n\nSo we will remove *Depth_to_Groundwater_DIEC* and *Depth_to_Groundwater_PAG*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data= auser_data.drop(columns=['Depth_to_Groundwater_DIEC','Depth_to_Groundwater_PAG'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the missing values from the data."},{"metadata":{},"cell_type":"markdown","source":"It is require that a row has at least 22 non-NaNs out of total 27 features. Keeping this threshold is giving the optimal number of records with minimal loss of data. We have adjusted the threshold throughout all of the 9 waterbodies dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data= auser_data.dropna(0,how='all',thresh=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have set the threshold as 22. This is because we want to remove the nan values with minimum loss of data."},{"metadata":{},"cell_type":"markdown","source":"Now we will interpolate the missing data.\n\nThe Series Pandas object provides an interpolate() function to interpolate missing values, and there is a nice selection of simple and more complex interpolation functions. We are using linear interpolation. This draws a straight line between available data, in this case on the first of the month, and fills in values at the chosen frequency from this line."},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data = auser_data.interpolate(method = 'linear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data = auser_data.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((auser_data.isnull() | auser_data.isna()).sum() * 100 / auser_data.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_plot= auser_data[[\"Depth_to_Groundwater_SAL\",\"Depth_to_Groundwater_LT2\",\"Depth_to_Groundwater_CoS\"]]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,7))\nsns.color_palette(\"husl\", 9)\n#sns.lineplot(data=plot, legend=\"full\", err_style=\"bars\")\nauser_plot.plot(linewidth=1, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(5, 2,figsize=(15,11))\naxs[0, 0].plot(auser_data[[\"Rainfall_Monte_Serra\"]])\naxs[0, 0].set_title('Rainfall_Monte_Serra')\naxs[0, 1].plot(auser_data[[\"Rainfall_Piaggione\"]])\naxs[0, 1].set_title('Rainfall_Piaggione')\naxs[1, 0].plot(auser_data[[\"Rainfall_Gallicano\"]])\naxs[1, 0].set_title('Rainfall_Gallicano')\naxs[1, 1].plot(auser_data[[\"Rainfall_Pontetetto\"]])\naxs[1, 1].set_title('Rainfall_Pontetetto')\naxs[2, 0].plot(auser_data[[\"Rainfall_Orentano\"]])\naxs[2, 0].set_title('Rainfall_Orentano')\naxs[2, 1].plot(auser_data[[\"Rainfall_Borgo_a_Mozzano\"]])\naxs[2, 1].set_title('Rainfall_Borgo_a_Mozzano')\naxs[3, 0].plot(auser_data[[\"Rainfall_Calavorno\"]])\naxs[3, 0].set_title('Rainfall_Calavorno')\naxs[3, 1].plot(auser_data[[\"Rainfall_Croce_Arcana\"]])\naxs[3, 1].set_title('Rainfall_Croce_Arcana')\naxs[4, 0].plot(auser_data[[\"Rainfall_Tereglio_Coreglia_Antelminelli\"]])\naxs[4, 0].set_title('Rainfall_Tereglio_Coreglia_Antelminelli')\naxs[4, 1].plot(auser_data[[\"Rainfall_Fabbriche_di_Vallico\"]])\naxs[4, 1].set_title('Rainfall_Fabbriche_di_Vallico')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Date', ylabel='Rainfall(mm)')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will plot correlation matrix for all the features/independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"white\")\n\n# Compute the correlation matrix\ncorr = auser_data.corr(method=\"pearson\")\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(auser_data.columns)):\n  result = adfuller(auser_data[auser_data.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(auser_data.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(auser_data.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As, we can see from the test that there is a Non-Stationarity within some features.So, now we will remove the Non- Stationarity by using Differencing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time series datasets may contain trends and seasonality, which may need to be removed prior to modeling. Differencing is a popular and widely used data transform for making time series data stationary.Now we will remove the Non- Stationarity by using First Order Differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data=auser_data-auser_data.shift(1)\nauser_data.dropna().plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data = auser_data.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(auser_data.columns)):\n  result = adfuller(auser_data[auser_data.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(auser_data.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(auser_data.columns[i]))\n    print(\" \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=auser_data\ntemp= temp.drop(columns=[\"Depth_to_Groundwater_SAL\",\"Depth_to_Groundwater_LT2\",\"Depth_to_Groundwater_CoS\"])\n\ndf= auser_data[[\"Depth_to_Groundwater_SAL\",\"Depth_to_Groundwater_LT2\",\"Depth_to_Groundwater_CoS\"]]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\nauser_data=new\nauser_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auser_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into X and Y matrix for further building the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(auser_data,[-3],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data in Training set and Testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will build the architecture of the LSTM RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=64, return_sequences=True, input_shape=(1, 21)))\n\n# Adding 2nd LSTM layer\nmodel.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.25))\n\n# Output layer\nmodel.add(Dense(units=3, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=250, validation_split=0.2, verbose=1, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"Auser_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will predict on Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mse = mean_squared_error(Y_train, train_pred)\nprint('Train MSE: %.3f' % train_mse)\n\ntest_mse = mean_squared_error(Y_test, test_pred)\nprint('Test MSE: %.3f' % test_mse)\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Aquifer_AUSER MSE= 0.012 and MAE=0.037 "},{"metadata":{},"cell_type":"markdown","source":"## DOGANELLA"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = pd.read_csv('../input/acea-water-prediction/Aquifer_Doganella.csv',parse_dates=True)\ndoganella","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', doganella.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((doganella.isnull() | doganella.isna()).sum() * 100 / doganella.index.size).round(2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = doganella.dropna(0,how ='all',thresh=7)\ndoganella","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((doganella.isnull() | doganella.isna()).sum() * 100 / doganella.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella_plot = doganella[['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2', 'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4', 'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6', 'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8', 'Depth_to_Groundwater_Pozzo_9']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=doganella_plot)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = doganella.set_index('Date')\ndoganella.index = pd.to_datetime(doganella.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = doganella.interpolate(method = 'time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = doganella.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((doganella.isnull() | doganella.isna()).sum() * 100 / doganella.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(doganella.columns)):\n  result = adfuller(doganella[doganella.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(doganella.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(doganella.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the Non-Stationarity present in some features by using the method of Differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella=doganella-doganella.shift(1)\ndoganella.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella = doganella.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(doganella.columns)):\n  result = adfuller(doganella[doganella.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(doganella.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(doganella.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=doganella\ntemp= temp.drop(columns=['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2', 'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4', 'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6', 'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8', 'Depth_to_Groundwater_Pozzo_9'])\n\ndf= doganella[['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2', 'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4', 'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6', 'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8', 'Depth_to_Groundwater_Pozzo_9']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\ndoganella=new\ndoganella.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doganella.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into X and Y matrix for further building the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(doganella,[-9],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_Train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_Test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=False, input_shape=(1,12)))\n\n\n# Output layer\nmodel.add(Dense(units=9, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_Train, Y_train, shuffle=True, epochs=50, validation_split=0.1, verbose=1, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"Doganella_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mse = mean_squared_error(Y_train, train_pred)\nprint('Train MSE: %.3f' % train_mse)\n\ntest_mse = mean_squared_error(Y_test, test_pred)\nprint('Test MSE: %.3f' % test_mse)\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Aquifer_DOGANELLA MSE= 0.513 and MAE=0.193"},{"metadata":{},"cell_type":"markdown","source":"## LUCO"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = pd.read_csv('../input/acea-water-prediction/Aquifer_Luco.csv',parse_dates=True)\nluco","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', luco.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((luco.isnull() | luco.isna()).sum() * 100 / luco.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = luco.dropna(0,how ='all',thresh=8)\nluco","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((luco.isnull() | luco.isna()).sum() * 100 / luco.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco_plot = luco[['Depth_to_Groundwater_Podere_Casetta']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\n#sns.lineplot(data=doganella_plot)\ndf_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = luco.set_index('Date')\nluco.index = pd.to_datetime(luco.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = luco.interpolate(method = 'time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = luco.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((luco.isnull() | luco.isna()).sum() * 100 / luco.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(luco.columns)):\n  result = adfuller(luco[luco.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(luco.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(luco.columns[i]))\n    print(\" \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the Non- Stationarity present in some features by using Differencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"luco.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco= luco-luco.shift(1)\nluco.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"luco = luco.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(luco.columns)):\n  result = adfuller(luco[luco.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(luco.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(luco.columns[i]))\n    print(\" \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=luco\ntemp= temp.drop(columns=['Depth_to_Groundwater_Podere_Casetta'])\n\ndf= luco[['Depth_to_Groundwater_Podere_Casetta']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\nluco= new\nluco.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"luco.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into features and output matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(luco,[-1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Training set and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(1, 20)))\n\n# Adding 2nd LSTM layer\n#model.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.3))\n\n# Output layer\nmodel.add(Dense(units=1, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=150, validation_split=0.2, verbose=1, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model weights\nmodel.save_weights(\"luco_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mse = mean_squared_error(Y_train, train_pred)\nprint('Train MSE: %.3f' % train_mse)\n\ntest_mse = mean_squared_error(Y_test, test_pred)\nprint('Test MSE: %.3f' % test_mse)\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Aquifer_LUCO MSE= 0.003 and MAE=0.018"},{"metadata":{},"cell_type":"markdown","source":"## PETRIGNANO"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = pd.read_csv('../input/acea-water-prediction/Aquifer_Petrignano.csv',parse_dates=True)\npetrignano","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', petrignano.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((petrignano.isnull() | petrignano.isna()).sum() * 100 / petrignano.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = petrignano.dropna(0,how ='all',thresh=3)\npetrignano","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have set the threshold as 3. This is because we want to remove the nan values with minimum loss of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((petrignano.isnull() | petrignano.isna()).sum() * 100 / petrignano.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano_plot = petrignano[['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=petrignano_plot)\n#df_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Futher, we will set the Date column as out index."},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = petrignano.set_index('Date')\npetrignano.index = pd.to_datetime(petrignano.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = petrignano.interpolate(method = 'time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = petrignano.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((petrignano.isnull() | petrignano.isna()).sum() * 100 / petrignano.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(petrignano.columns)):\n  result = adfuller(petrignano[petrignano.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(petrignano.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(petrignano.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the Non- Stationarity by using Differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano= petrignano-petrignano.shift(1)\npetrignano.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano = petrignano.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(petrignano.columns)):\n  result = adfuller(petrignano[petrignano.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(petrignano.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(petrignano.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=petrignano\ntemp= temp.drop(columns=['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25'])\n\ndf= petrignano[['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\npetrignano= new\npetrignano.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petrignano.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(petrignano,[-2],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Training set and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(1, 5)))\n\n# Adding 2nd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=8, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.3))\n\n# Output layer\nmodel.add(Dense(units=2, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=100, validation_split=0.1, verbose=1, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"petrignano_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mse = mean_squared_error(Y_train, train_pred)\nprint('Train MSE: %.3f' % train_mse)\n\ntest_mse = mean_squared_error(Y_test, test_pred)\nprint('Test MSE: %.3f' % test_mse)\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Aquifer_PETRIGNANO MSE= 0.021 and MAE=0.063"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, for the water body AQUIFER the predictive model that fits the best is LSTM RNN. The MSE and MAE scores obtained per water bodies are as follows:\n\n","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/results/W1_Updated.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WATER SPRING"},{"metadata":{},"cell_type":"markdown","source":"## AMIATA"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = pd.read_csv('../input/acea-water-prediction/Water_Spring_Amiata.csv',parse_dates=True)\namiata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', amiata.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((amiata.isnull() | amiata.isna()).sum() * 100 / amiata.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = amiata.dropna(0,how ='all',thresh=5)\namiata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have set the threshold as 5. This is because we want to remove the nan values with minimum loss of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((amiata.isnull() | amiata.isna()).sum() * 100 / amiata.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata_plot = amiata[['Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n 'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=amiata_plot)\n#df_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = amiata.set_index('Date')\namiata.index = pd.to_datetime(amiata.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = amiata.interpolate(method = 'time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = amiata.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"white\")\n\n# Compute the correlation matrix\ncorr = amiata.corr(method=\"pearson\")\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As depicted from the correlation plot above ther is an observer correlation between  Rainfall_Castel_del_Piano & Rainfall_Abbadia_S_Salvatore, Rainfall_S_Fiora, Rainfall_Laghetto_Verde and Rainfall_Vetta_Amiata.\n\nSo we will drop Rainfall_Castel_del_Piano in order to avoid any bais in out input data for smooth training of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp= amiata\ntemp= temp.drop(columns=['Rainfall_Castel_del_Piano'])\namiata= temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((amiata.isnull() | amiata.isna()).sum() * 100 / amiata.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(amiata.columns)):\n  result = adfuller(amiata[amiata.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(amiata.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(amiata.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the Non- Stationarity present in some features by using the method of Differencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata= amiata-amiata.shift(1)\namiata.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata = amiata.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(amiata.columns)):\n  result = adfuller(amiata[amiata.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(amiata.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(amiata.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=amiata\ntemp= temp.drop(columns=['Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n 'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta'])\n\ndf= amiata[['Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n 'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\namiata= new\namiata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amiata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(amiata,[-4],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Training set and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. XGBoost Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgr\nfrom sklearn.multioutput import MultiOutputRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training XGBoost with evaluation metric as MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_mae = xgr.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=3, eval_metric='mae', seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multioutputregressor = MultiOutputRegressor(xgr_mae)\nxgbr_1= multioutputregressor.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Training set Prediction\ntrain_pred = xgbr_1.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Testing set Prediction\ntest_pred= xgbr_1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Absolute Error\nprint(\"Training MAE\", mean_absolute_error(Y_train, train_pred))\nprint(\"Testing MAE\",mean_absolute_error(Y_test, test_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Water_Spring_AMIATA MAE=0.084"},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=8, return_sequences=True, input_shape=(1, 10)))\n\n# Adding 2nd LSTM layer\n#model.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.2))\n\n# Output layer\nmodel.add(Dense(units=4, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=50, validation_split=0.2, verbose=1, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"amiata_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mse = mean_squared_error(Y_train, train_pred)\nprint('Train MSE: %.3f' % train_mse)\n\ntest_mse = mean_squared_error(Y_test, test_pred)\nprint('Test MSE: %.3f' % test_mse)\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Water_Spring_AMIATA MSE= 0.052 and MAE=0.076"},{"metadata":{},"cell_type":"markdown","source":"## MADONNA_DI_CANNETO"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc = pd.read_csv('../input/acea-water-prediction/Water_Spring_Madonna_di_Canneto.csv',parse_dates=True)\nmdc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', mdc.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((mdc.isnull() | mdc.isna()).sum() * 100 / mdc.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc_plot = mdc[['Flow_Rate_Madonna_di_Canneto']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=mdc_plot)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc = mdc.dropna(subset = [\"Date\"])\nmdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc = mdc.set_index('Date')\nmdc.index = pd.to_datetime(mdc.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc = mdc.interpolate(method = 'linear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdc = mdc.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((mdc.isnull() | mdc.isna()).sum() * 100 / mdc.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(mdc.columns)):\n  result = adfuller(mdc[mdc.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(mdc.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(mdc.columns[i]))\n    print(\" \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=mdc\ntemp= temp.drop(columns=['Flow_Rate_Madonna_di_Canneto'])\n\ndf= mdc[['Flow_Rate_Madonna_di_Canneto']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\nmdc= new\nmdc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(mdc,[-1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Training set and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. XGBoost Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training XGBoost with evaluation metric as MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_mae = xgr.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=3, eval_metric='mae', seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr_1= xgr_mae.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Training set Prediction\ntrain_pred = xgbr_1.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Testing set Prediction\ntest_pred=xgbr_1.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Absolute Error\nprint(\"Training MAE\", mean_absolute_error(Y_train, train_pred))\nprint(\"Testing MAE\",mean_absolute_error(Y_test, test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for XGBoost implemented on Water_Spring_MADONNA_DI_CANNETO ha MAE=18.19"},{"metadata":{},"cell_type":"markdown","source":"#### 2. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(1, 2)))\n\n# Adding 2nd LSTM layer\n#model.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.2))\n\n# Output layer\nmodel.add(Dense(units=1, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=100, validation_split=0.2, verbose=1, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"mdc_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for LSTM RNN implemented on Water_Spring_MADONNA_DI_CANNETO has MAE=19.40"},{"metadata":{},"cell_type":"markdown","source":"## LUPA"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa = pd.read_csv('../input/acea-water-prediction/Water_Spring_Lupa.csv',parse_dates=True)\nlupa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', lupa.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((lupa.isnull() | lupa.isna()).sum() * 100 / lupa.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa_plot = lupa[['Flow_Rate_Lupa']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=lupa_plot)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa = lupa.dropna(subset = [\"Date\"])\nlupa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa = lupa.set_index('Date')\nlupa.index = pd.to_datetime(lupa.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa = lupa.interpolate(method = 'linear')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlupa = lupa.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((lupa.isnull() | lupa.isna()).sum() * 100 / lupa.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(lupa.columns)):\n  result = adfuller(lupa[lupa.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(lupa.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(lupa.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa= lupa-lupa.shift(1)\nlupa.dropna().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the first row because it consist of missing values or NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa = lupa.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confirming Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will run the AD Fuller Test on the data to confirm if the all the timeseries are stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(lupa.columns)):\n  result = adfuller(lupa[lupa.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(lupa.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(lupa.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=lupa\ntemp= temp.drop(columns=['Flow_Rate_Lupa'])\n\ndf= lupa[['Flow_Rate_Lupa']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\nlupa= new\nlupa.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lupa.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(lupa,[-1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. XGBoost Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgr\nfrom sklearn.multioutput import MultiOutputRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nTraining XGBoost with evaluation metric as MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_mae = xgr.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=3, eval_metric='mae', seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr_1= xgr_mae.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Training set Prediction\ntrain_pred = xgbr_1.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Testing set Prediction\ntest_pred=xgbr_1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Absolute Error\nprint(\"Training MAE\", mean_absolute_error(Y_train, train_pred))\nprint(\"Testing MAE\",mean_absolute_error(Y_test, test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Water_Spring_LUPA after implementing XGBoost is MAE(Mean Absolute Error)= 0.3"},{"metadata":{},"cell_type":"markdown","source":"#### 2. LSTM Model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(1, 1)))\n\n# Adding 2nd LSTM layer\n#model.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.2))\n\n# Output layer\nmodel.add(Dense(units=1, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_absolute_error')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=100, validation_split=0.2, verbose=1, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"lupa_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Water_Spring_LUPA after implementing LSTM RNN MAE=0.284"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, for the Water Body Water_Spring we can implement **XGBoost** model as it is best predictive model which performs well on all the three Water_Springs i.e. Amiata, Madonna di Canneto, Lupa."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/results/W2.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RIVER"},{"metadata":{},"cell_type":"markdown","source":"## ARNO"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno = pd.read_csv('../input/acea-water-prediction/River_Arno.csv',parse_dates=True)\narno","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', arno.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((arno.isnull() | arno.isna()).sum() * 100 / arno.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno_plot = arno[['Hydrometry_Nave_di_Rosano']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=arno_plot)\n#df_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno = arno.set_index('Date')\narno.index = pd.to_datetime(arno.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno = arno.interpolate(method = 'linear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno = arno.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((arno.isnull() | arno.isna()).sum() * 100 / arno.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(arno.columns)):\n  result = adfuller(arno[arno.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(arno.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(arno.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSince the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=arno\ntemp= temp.drop(columns=['Hydrometry_Nave_di_Rosano'])\n\ndf= arno[['Hydrometry_Nave_di_Rosano']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\narno= new\narno.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arno.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(arno,[-1],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. XGBoost Regression Model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training XGBoost with evaluation metric as MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_mae = xgr.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=3, eval_metric='mae', seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr_1= xgr_mae.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Training set Prediction\ntrain_pred = xgbr_1.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Testing set Prediction\ntest_pred=xgbr_1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Absolute Error\nprint(\"Training MAE\", mean_absolute_error(Y_train, train_pred))\nprint(\"Testing MAE\",mean_absolute_error(Y_test, test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### 2. LSTM Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(1,15)))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.25))\n\n# Output layer\nmodel.add(Dense(units=1, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=50, validation_split=0.2, verbose=1, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"arno_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for River_ARNO after implementing LSTM RNN is MAE=0.36"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, for the water body RIVER the predictive model that fits the best is LSTM RNN. The MAE scores obtained are as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/results/W3.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LAKE"},{"metadata":{},"cell_type":"markdown","source":"## BILANCINO"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Importing The Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nimport missingno as mn\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino = pd.read_csv('../input/acea-water-prediction/Lake_Bilancino.csv',parse_dates=True)\nbilancino","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine the shape or structure of the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', bilancino.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((bilancino.isnull() | bilancino.isna()).sum() * 100 / bilancino.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino = bilancino.dropna(0,how ='all',thresh=9)\nbilancino\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((bilancino.isnull() | bilancino.isna()).sum() * 100 / bilancino.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino_plot = bilancino[['Lake_Level','Flow_Rate']]\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(35,7))\nsns.color_palette(\"husl\", 9)\nsns.lineplot(data=bilancino_plot)\n#df_auser.plot(linewidth=2, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino = bilancino.set_index('Date')\nbilancino.index = pd.to_datetime(bilancino.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino = bilancino.interpolate(method = 'time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino = bilancino.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The percentage of missing values in dataset\")\n((bilancino.isnull() | bilancino.isna()).sum() * 100 / bilancino.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking The Stationarity"},{"metadata":{},"cell_type":"markdown","source":"In order to check the stationarity of the time series (i.e identify wether the time series is stationary or not) we perform Augmented Dickey-Fuller test (ADF Test.)\n\nFor AD Fuller test:\n\n1. Null Hypothesis - Series possesses a unit root and hence is not stationary.\n2. Alternate Hypothesis - Series is stationary\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nprint(\"AUGMENTED DICKEY FULLER TEST \\n\\n\")\nfor i in range(len(bilancino.columns)):\n  result = adfuller(bilancino[bilancino.columns[i]])\n\n  if result[1] > 0.05 :\n    print('{} - Series is NOT Stationary'.format(bilancino.columns[i]))\n    print(\" \")\n  else:\n    print('{} - Series is Stationary'.format(bilancino.columns[i]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the time Series is stationary we can proceed with the model building section."},{"metadata":{},"cell_type":"markdown","source":"### Building Predictive Models"},{"metadata":{},"cell_type":"markdown","source":"#### Split Data into Training and Testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=bilancino\ntemp= temp.drop(columns=['Lake_Level','Flow_Rate'])\n\ndf= bilancino[['Lake_Level','Flow_Rate']]\nnew= pd.merge(temp, df, left_index=True, right_index=True)\n\n# Update the main dataframe i.e. auser_data\nbilancino= new\nbilancino.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilancino.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = np.split(bilancino,[-2],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X\", X.shape)\nprint(\"Shape of Y\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into Training set and Testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of Y_train\", Y_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of Y_test\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Standard Scalar Scalar for scaling the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. XGBoost Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgr\nfrom sklearn.multioutput import MultiOutputRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training XGBoost with evaluation metric as MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_mae = xgr.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=3, eval_metric='mae', seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multioutputregressor = MultiOutputRegressor(xgr_mae)\nxgbr_1= multioutputregressor.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Training set Prediction\ntrain_pred = xgbr_1.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Testing set Prediction\ntest_pred=xgbr_1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Absolute Error\nprint(\"Training MAE\", mean_absolute_error(Y_train, train_pred))\nprint(\"Testing MAE\",mean_absolute_error(Y_test, test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Lake_BILANCINO implemented using XGBoost is MAE=2.101"},{"metadata":{},"cell_type":"markdown","source":"#### 1. LSTM Model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages from Keras for building model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Neural Network based on LSTM RNN\nmodel = Sequential()\n\n# Add 1st LSTM RNN layer\nmodel.add(LSTM(units=64, return_sequences=True, input_shape=(1, 6)))\n\n# Adding 2nd LSTM layer\nmodel.add(LSTM(units=32, return_sequences=True))\n\n# Adding 3rd LSTM layer\nmodel.add(LSTM(units=16, return_sequences=False))\n\n# Adding Dropout\nmodel.add(Dropout(0.25))\n\n# Output layer\nmodel.add(Dense(units=2, activation='linear'))\n\n# Compiling the Neural Network\nmodel.compile(optimizer = Adam(learning_rate=0.01), loss='mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, shuffle=True, epochs=150, validation_split=0.2, verbose=1, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"bilancino_M1.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Training Data: \",train_pred)\nprint(\"Actual Train Data: \",Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted on Test Data: \",test_pred)\nprint(\"Actual Test Data: \",Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\ntrain_mae = mean_absolute_error(Y_train, train_pred)\nprint('Train MAE: %.3f' % train_mae)\n\ntest_mae = mean_absolute_error(Y_test, test_pred)\nprint('Test MAE: %.3f' % test_mae)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, the final scores for Lake_BILANCINO implemented using LSTM RNN is MAE=4.515"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, for the water body RIVER the predictive model that fits the best is XGBoost. The MAE scores obtained are as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/results/W4_new.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table below represents the different models implmented on different water bodies and the ones highlighted are the models that best performed onb the specific dataset of the waterbodies."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/results/Final_1.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}