{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time Series Part-1 (data visualisation) is explained clearly in the below link kindly check and then continue here\n\n#### https://www.kaggle.com/nandha13/time-series-part-1-data-visualization","metadata":{"execution":{"iopub.status.busy":"2021-10-29T14:52:02.336137Z","iopub.execute_input":"2021-10-29T14:52:02.336482Z","iopub.status.idle":"2021-10-29T14:52:03.301186Z","shell.execute_reply.started":"2021-10-29T14:52:02.336398Z","shell.execute_reply":"2021-10-29T14:52:03.300289Z"}}},{"cell_type":"markdown","source":"# TimeSeries ðŸ“ˆ ARIMA, Prophet, ADF, PACF... ðŸ“š Beginner to Pro\n\nIn this project we will work on how to preprocess, how to review the data in time series","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:44.252419Z","iopub.execute_input":"2021-11-07T15:27:44.252822Z","iopub.status.idle":"2021-11-07T15:27:44.259827Z","shell.execute_reply.started":"2021-11-07T15:27:44.252775Z","shell.execute_reply":"2021-11-07T15:27:44.258465Z"}}},{"cell_type":"markdown","source":"add Codeadd Markdown\nWe also will be working on some commonly used timeseries topics\n\nACF/PACF\nARIMA\nAuto-ARIMA\nProphet\nAugumented Dickey Fuller (ADF)\nadd Codeadd Markdown","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:44.261062Z","iopub.status.idle":"2021-11-07T15:27:44.261741Z","shell.execute_reply.started":"2021-11-07T15:27:44.261485Z","shell.execute_reply":"2021-11-07T15:27:44.261513Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Petrignano.csv\")\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:26.841274Z","iopub.execute_input":"2021-11-07T15:28:26.841533Z","iopub.status.idle":"2021-11-07T15:28:26.87029Z","shell.execute_reply.started":"2021-11-07T15:28:26.841505Z","shell.execute_reply":"2021-11-07T15:28:26.86952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:26.872203Z","iopub.execute_input":"2021-11-07T15:28:26.872877Z","iopub.status.idle":"2021-11-07T15:28:26.882462Z","shell.execute_reply.started":"2021-11-07T15:28:26.872829Z","shell.execute_reply":"2021-11-07T15:28:26.881463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing rows having null values, from the below output we can see that similar kind of null values are in all the column\nalso resetting index","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:44.26783Z","iopub.status.idle":"2021-11-07T15:27:44.268606Z","shell.execute_reply.started":"2021-11-07T15:27:44.268263Z","shell.execute_reply":"2021-11-07T15:27:44.268288Z"}}},{"cell_type":"code","source":"df = df[df.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:26.884265Z","iopub.execute_input":"2021-11-07T15:28:26.884566Z","iopub.status.idle":"2021-11-07T15:28:26.896731Z","shell.execute_reply.started":"2021-11-07T15:28:26.884528Z","shell.execute_reply":"2021-11-07T15:28:26.895905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:26.898239Z","iopub.execute_input":"2021-11-07T15:28:26.898525Z","iopub.status.idle":"2021-11-07T15:28:27.550549Z","shell.execute_reply.started":"2021-11-07T15:28:26.898488Z","shell.execute_reply":"2021-11-07T15:28:27.549849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see Depth_to_Groundwater_P24 and Depth_to_Groundwater_P25 are highly correlated so we ca remove one of the feature.\n\nsimilarly for Temperature_Bastia_Umbra and Temperature_Petrignano we can remove one of the feature","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:56.254577Z","iopub.execute_input":"2021-11-07T15:27:56.254916Z","iopub.status.idle":"2021-11-07T15:27:56.262387Z","shell.execute_reply.started":"2021-11-07T15:27:56.254881Z","shell.execute_reply":"2021-11-07T15:27:56.25999Z"}}},{"cell_type":"code","source":"df = df.drop(['Depth_to_Groundwater_P24', 'Temperature_Petrignano'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.552985Z","iopub.execute_input":"2021-11-07T15:28:27.553769Z","iopub.status.idle":"2021-11-07T15:28:27.559283Z","shell.execute_reply.started":"2021-11-07T15:28:27.553719Z","shell.execute_reply":"2021-11-07T15:28:27.558559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Changing column name for our understanding","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:56.267731Z","iopub.status.idle":"2021-11-07T15:27:56.268542Z","shell.execute_reply.started":"2021-11-07T15:27:56.268154Z","shell.execute_reply":"2021-11-07T15:27:56.26818Z"}}},{"cell_type":"code","source":"df.columns = ['date', 'rainfall', 'depth_to_groundwater', 'temperature', 'drainage_volume', 'river_hydrometry']","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.562441Z","iopub.execute_input":"2021-11-07T15:28:27.562633Z","iopub.status.idle":"2021-11-07T15:28:27.569142Z","shell.execute_reply.started":"2021-11-07T15:28:27.562609Z","shell.execute_reply":"2021-11-07T15:28:27.568426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = ['depth_to_groundwater']\nfeatures = [feature for feature in df.columns if feature not in targets]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.571909Z","iopub.execute_input":"2021-11-07T15:28:27.572498Z","iopub.status.idle":"2021-11-07T15:28:27.58907Z","shell.execute_reply.started":"2021-11-07T15:28:27.572467Z","shell.execute_reply":"2021-11-07T15:28:27.588334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.590674Z","iopub.execute_input":"2021-11-07T15:28:27.591414Z","iopub.status.idle":"2021-11-07T15:28:27.606519Z","shell.execute_reply.started":"2021-11-07T15:28:27.591371Z","shell.execute_reply":"2021-11-07T15:28:27.605728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we can see date column is in Object type we have to change it to date time index for analysis","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:27:56.276734Z","iopub.status.idle":"2021-11-07T15:27:56.277403Z","shell.execute_reply.started":"2021-11-07T15:27:56.277157Z","shell.execute_reply":"2021-11-07T15:27:56.277182Z"}}},{"cell_type":"code","source":"from datetime import datetime,date\ndf['date'] = pd.to_datetime(df['date'], format = '%d/%m/%Y')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.607927Z","iopub.execute_input":"2021-11-07T15:28:27.608415Z","iopub.status.idle":"2021-11-07T15:28:27.639285Z","shell.execute_reply.started":"2021-11-07T15:28:27.608377Z","shell.execute_reply":"2021-11-07T15:28:27.638439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 25))\n\nfor i, column in enumerate(df.drop('date', axis=1).columns):\n    sns.lineplot(x=df['date'], y=df[column].fillna(method='ffill'), ax=ax[i], color='dodgerblue')\n    ax[i].set_title('Feature: {}'.format(column), fontsize=14)\n    ax[i].set_ylabel(ylabel=column, fontsize=14)\n                      \n    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])   ","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:27.640993Z","iopub.execute_input":"2021-11-07T15:28:27.641514Z","iopub.status.idle":"2021-11-07T15:28:29.37167Z","shell.execute_reply.started":"2021-11-07T15:28:27.641474Z","shell.execute_reply":"2021-11-07T15:28:29.37091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### The above steps are already detalied explined in my data visualisation notebook","metadata":{}},{"cell_type":"markdown","source":"# Time Series - Part 2 (Data Preprocessing)","metadata":{}},{"cell_type":"markdown","source":"#### Chronological Order and Equidistant Timestamps","metadata":{}},{"cell_type":"markdown","source":"Chronological Order of timeseries data must be checked and preprocessed. Similarly Equidistant of the date also needed to be check.\n\nIn order make in chronological order we  sort the data. similarly to check for equidistant we see the difference between the current and previous date or you can use a constant time and find the diffenrence in date","metadata":{}},{"cell_type":"code","source":"df = df.sort_values(by='date')\ndf['delta'] = df['date'] - df['date'].shift(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:29.372856Z","iopub.execute_input":"2021-11-07T15:28:29.373129Z","iopub.status.idle":"2021-11-07T15:28:29.380598Z","shell.execute_reply.started":"2021-11-07T15:28:29.373093Z","shell.execute_reply":"2021-11-07T15:28:29.379805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"shift function will shift the value of the date one record below its previous record based on the argumet given inside the shift function.\n\n ","metadata":{}},{"cell_type":"code","source":"df['delta'].sum(),df['delta'].count()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:29.382235Z","iopub.execute_input":"2021-11-07T15:28:29.382698Z","iopub.status.idle":"2021-11-07T15:28:29.390853Z","shell.execute_reply.started":"2021-11-07T15:28:29.382653Z","shell.execute_reply":"2021-11-07T15:28:29.390176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see that the equdistant between each record has been maintained ","metadata":{}},{"cell_type":"markdown","source":"#### Handling Missing Values","metadata":{}},{"cell_type":"markdown","source":"We can see there is still some null values in the features\n\nAlso we can see that  there are some zero values in river_hydrometry and drainage volume which we can repalce with nan values and filling them afterwords ","metadata":{}},{"cell_type":"code","source":"# setting rows, columns and size \nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n\n\n# hydrometry having original values and hydrometry having zoeros replaced with nan\nold_hydrometry = df['river_hydrometry'].copy()\ndf['river_hydrometry'] = df['river_hydrometry'].replace(0, np.nan)\n\n# ploting for old and new hydrometry\nsns.lineplot(x=df['date'], y=old_hydrometry, ax=ax[0], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['river_hydrometry'].fillna(np.inf), ax=ax[0], color='dodgerblue', label='modified')\nax[0].set_title('Feature: Hydrometry', fontsize=14)\nax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nold_drainage = df['drainage_volume'].copy()\ndf['drainage_volume'] = df['drainage_volume'].replace(0, np.nan)\n\nsns.lineplot(x=df['date'], y=old_drainage, ax=ax[1], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[1], color='dodgerblue', label='modified')\nax[1].set_title('Feature: Drainage', fontsize=14)\nax[1].set_ylabel(ylabel='Drainage', fontsize=14)\nax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:29.392266Z","iopub.execute_input":"2021-11-07T15:28:29.39281Z","iopub.status.idle":"2021-11-07T15:28:30.752637Z","shell.execute_reply.started":"2021-11-07T15:28:29.392772Z","shell.execute_reply":"2021-11-07T15:28:30.751894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see that zeros are replaced by null values which is mentioned in orange lines ","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=1,ncols=1,figsize=(16,5))\nsns.heatmap(df.T.isna(),cmap='Blues')\nax.set_title('Missing Values',fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:30.7563Z","iopub.execute_input":"2021-11-07T15:28:30.757043Z","iopub.status.idle":"2021-11-07T15:28:32.183292Z","shell.execute_reply.started":"2021-11-07T15:28:30.756989Z","shell.execute_reply":"2021-11-07T15:28:32.182581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmap gives insights on null values and we can see that  hydrometry having more null values","metadata":{}},{"cell_type":"markdown","source":"###### Missing values handling\n\n1. filling NaN values with zeros or Outliers.\n2. filling NaN values with last value\n3. filling NaN values with mean value\n4. filling NaN values with  .interpolate()","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4,ncols=1,figsize=(15,12))\n\n## we take np.inf to differentiate with \nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(0),ax = ax[0],color = 'darkorange',label = 'modified')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(np.inf),ax = ax[0],color = 'dodgerblue',label = 'original')\nax[0].set_title('filling Nan values with zeros')\n\nmean_drain = df['drainage_volume'].mean()\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(mean_drain),ax = ax[1],color = 'darkorange',label = 'modified')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(np.inf),ax = ax[1],color = 'dodgerblue',label = 'original')\nax[0].set_title('filling Nan values with meanvalue')\n\n\nlast_value = df['drainage_volume'].ffill()\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(last_value),ax = ax[2],color = 'darkorange',label = 'modified')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(np.inf),ax = ax[2],color = 'dodgerblue',label = 'original')\nax[0].set_title('filling Nan values with last value')\n\ninterpolate = df['drainage_volume'].interpolate()\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(interpolate),ax = ax[3],color = 'darkorange',label = 'modified')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].fillna(np.inf),ax = ax[3],color = 'dodgerblue',label = 'original')\nax[0].set_title('filling Nan values with interpolation')\n\nfor i in range(4):\n    ax[i].set_xlim([date(2019, 5, 1), date(2019, 10, 1)])\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:32.184461Z","iopub.execute_input":"2021-11-07T15:28:32.185551Z","iopub.status.idle":"2021-11-07T15:28:34.34454Z","shell.execute_reply.started":"2021-11-07T15:28:32.185512Z","shell.execute_reply":"2021-11-07T15:28:34.343856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see that best option is to interpolate ","metadata":{}},{"cell_type":"code","source":"df['drainage_volume'] = df['drainage_volume'].interpolate()\ndf['river_hydrometry'] = df['river_hydrometry'].interpolate()\ndf['depth_to_groundwater'] = df['depth_to_groundwater'].interpolate()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:34.34562Z","iopub.execute_input":"2021-11-07T15:28:34.345956Z","iopub.status.idle":"2021-11-07T15:28:34.355624Z","shell.execute_reply.started":"2021-11-07T15:28:34.345924Z","shell.execute_reply":"2021-11-07T15:28:34.354708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Smoothing data / Resampling","metadata":{}},{"cell_type":"markdown","source":"Resampling the data can provide additional information. there are two types of resampling\n\n-  Upsampling : upsampling is making weekly wise data into day wise data\n-  downsampling : downsapling is making day wise data into weekly or mothly data\n\nin this we will use .resample() method to resample","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=3,ncols=2,figsize = (16,12))\n\nsns.lineplot(x=df['date'],y=df['drainage_volume'],ax = ax[0,0], color = 'dodgerblue')\nax[0,0].set_title('drainage_volume',fontsize =14)\n\nresampling = df[['date','drainage_volume']].resample('7D',on='date').sum().reset_index(drop = False)\nsns.lineplot(x=resampling['date'],y=resampling['drainage_volume'],ax = ax[1,0], color = 'dodgerblue')\nax[1,0].set_title('weekly_drainage_volume',fontsize =14)\n\nresampling = df[['date','drainage_volume']].resample('M',on='date').sum().reset_index(drop = False)\nsns.lineplot(x=resampling['date'],y=resampling['drainage_volume'],ax = ax[2,0], color = 'dodgerblue')\nax[2,0].set_title('Monthly_drainage_volume',fontsize =14)\n\nfor i in range(3):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(x=df['date'],y=df['temperature'],ax = ax[0,1], color = 'dodgerblue')\nax[0,1].set_title('temperature',fontsize =14)\n\nresampling = df[['date','temperature']].resample('7D',on='date').mean().reset_index(drop = False)\nsns.lineplot(x=resampling['date'],y=resampling['temperature'],ax = ax[1,1], color = 'dodgerblue')\nax[1,1].set_title('weekly_temperature',fontsize =14)\n\nresampling = df[['date','temperature']].resample('M',on='date').mean().reset_index(drop = False)\nsns.lineplot(x=resampling['date'],y=resampling['temperature'],ax = ax[2,1], color = 'dodgerblue')\nax[2,1].set_title('Monthly_temperature',fontsize =14)\n\nfor i in range(3):\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:34.357307Z","iopub.execute_input":"2021-11-07T15:28:34.35793Z","iopub.status.idle":"2021-11-07T15:28:35.718898Z","shell.execute_reply.started":"2021-11-07T15:28:34.357878Z","shell.execute_reply":"2021-11-07T15:28:35.718089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see that down sampling weekly wise will make the analysis easy with less data loss","metadata":{}},{"cell_type":"code","source":"downsammple = df[['date',\n                 'depth_to_groundwater', \n                 'temperature',\n                 'drainage_volume', \n                 'river_hydrometry',\n                 'rainfall']].resample('7D',on='date').mean().reset_index(drop = False)\n\ndf = downsammple.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:35.720073Z","iopub.execute_input":"2021-11-07T15:28:35.720589Z","iopub.status.idle":"2021-11-07T15:28:35.732313Z","shell.execute_reply.started":"2021-11-07T15:28:35.720553Z","shell.execute_reply":"2021-11-07T15:28:35.731388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stationarity","metadata":{}},{"cell_type":"markdown","source":"Sationarity means dataset is sattionary when the data properties does not depend on time. the data with sesonality and trends are said to be non stationary. since the trends and sesonality affects the data at different at diffrent timeseries.\n\nStationarity defines the time series as\n\n* constant mean and mean is not time-dependent\n* constant variance and variance is not time dependent \n* constant covariance and covariance is not time dependent\n\n","metadata":{}},{"cell_type":"markdown","source":"This stationarity check can be done with three methods\n\n- Visually: by ploting in a graph and checking trends and seasonality\n- Statistical: spliting time series and camparing the mean and variance of the data\n- Staitstical test: Augmented Dickey Fuller test ","metadata":{}},{"cell_type":"code","source":"# rolling windo is the number of weeks per year\nrolling_window = 52\nf , ax = plt.subplots(nrows=2,ncols=1,figsize= (16,16))\n\nsns.lineplot(x=df['date'],y=df['drainage_volume'],ax= ax[0], color = 'dodgerblue')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].rolling(rolling_window).mean(),ax= ax[0], color = 'black',label = 'roling_mean')\nsns.lineplot(x=df['date'],y=df['drainage_volume'].rolling(rolling_window).std(),ax= ax[0], color = 'orange',label = 'roling_std')\nax[0].set_title('Depth to Groundwater: Non-stationary \\n non-constant mean & non-constant variance', fontsize=14)\nax[0].set_ylabel(ylabel='Drainage Volume', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(x=df['date'], y=df['temperature'], ax=ax[1], color='dodgerblue')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(rolling_window).mean(), ax=ax[1], color='black', label='rolling mean')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(rolling_window).std(), ax=ax[1], color='orange', label='rolling std')\nax[1].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\nax[1].set_ylabel(ylabel='Temperature', fontsize=14)\nax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:35.733633Z","iopub.execute_input":"2021-11-07T15:28:35.734464Z","iopub.status.idle":"2021-11-07T15:28:36.551557Z","shell.execute_reply.started":"2021-11-07T15:28:35.734421Z","shell.execute_reply":"2021-11-07T15:28:36.55089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see that features don't have constant mean and standard deviation","metadata":{}},{"cell_type":"markdown","source":"### Unit Root Test\n\nUnit Root test is a charachteristic of time series that make time series Non stationary, ADF is a Unit root test. A unit root is said to be exist in a time series if Value of alpha =1 in time series \n\nY\nt\n=\nÎ±\nY\nt\nâˆ’\n1\n+\nÎ²\nX\ne\n+\nÏµ","metadata":{}},{"cell_type":"markdown","source":"### Agumented Dicky Fuller(ADF)\n\nAgumeented Dicky Fuller is a type of Statistical test called as Unit root test. we can give them by.\n\n**Null Hypothesis(H0)**: Time Series has unit Root (Non- Stationary)\n**Alternate Hypothesis(H1)**: Time Series not has unit Root (Stationary)\n\nIf the null hypothesis can be rejected, we can conclude that the time series is stationary.\n\nThere are two ways to rejects the null hypothesis:\n\nOn the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n\n**p-value > significance level (default: 0.05)**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n**p-value <= significance level (default: 0.05)**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\nOn the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n\n**ADF statistic > critical value**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n**ADF statistic < critical value**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(df['depth_to_groundwater'].values)\nresult","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:36.552873Z","iopub.execute_input":"2021-11-07T15:28:36.553558Z","iopub.status.idle":"2021-11-07T15:28:36.587872Z","shell.execute_reply.started":"2021-11-07T15:28:36.553522Z","shell.execute_reply":"2021-11-07T15:28:36.587087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are going to check for each variable:\n\nThe p-value is less than 0.05\nCheck the range of the ADF statistic compared with critical_values","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=3,ncols=2,figsize = (15,9))\n\ndef visualisation(series,title,ax):\n    result = adfuller(series)\n    sig_lvl = 0.05\n    adf_stat = result[0]\n    p_val = result[1]\n    crit_val_1 = result[4]['1%']\n    crit_val_5 = result[4]['5%']\n    crit_val_10 = result[4]['10%']\n    if (p_val < sig_lvl) & ((adf_stat < crit_val_1)):\n        linecolor = 'forestgreen' \n    elif (p_val < sig_lvl) & (adf_stat < crit_val_5):\n        linecolor = 'orange'\n    elif (p_val < sig_lvl) & (adf_stat < crit_val_10):\n        linecolor = 'red'\n    else:\n        linecolor = 'purple'\n    sns.lineplot(x=df['date'],y=series,ax=ax,color=linecolor)\n    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n    ax.set_ylabel(ylabel=title, fontsize=14)\n    \nvisualisation(df['rainfall'].values, 'Rainfall', ax[0, 0])\nvisualisation(df['temperature'].values, 'Temperature', ax[1, 0])\nvisualisation(df['river_hydrometry'].values, 'River_Hydrometry', ax[0, 1])\nvisualisation(df['drainage_volume'].values, 'Drainage_Volume', ax[1, 1])\nvisualisation(df['depth_to_groundwater'].values, 'Depth_to_Groundwater', ax[2, 0])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:36.5892Z","iopub.execute_input":"2021-11-07T15:28:36.589686Z","iopub.status.idle":"2021-11-07T15:28:38.708529Z","shell.execute_reply.started":"2021-11-07T15:28:36.589649Z","shell.execute_reply":"2021-11-07T15:28:38.707858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the data is not staionary bh=ut we want to use ARIMA models. we can do that transforming the data.The two most common methods to transform series into stationarity ones are:\n\nTransformation: e.g. log or square root to stabilize non-constant variance\n\nDifferencing: subtracts the current value from the previous\n","metadata":{}},{"cell_type":"markdown","source":"### Transformation","metadata":{}},{"cell_type":"code","source":"df['depth_to_groundwater_log'] = np.log(abs(df['depth_to_groundwater']))\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\nvisualisation(df['depth_to_groundwater_log'], 'Transformed \\n Depth to Groundwater', ax[0])\n\nsns.distplot(df['depth_to_groundwater_log'], ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:38.709899Z","iopub.execute_input":"2021-11-07T15:28:38.710185Z","iopub.status.idle":"2021-11-07T15:28:39.294621Z","shell.execute_reply.started":"2021-11-07T15:28:38.710149Z","shell.execute_reply":"2021-11-07T15:28:39.293942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Differencing","metadata":{}},{"cell_type":"markdown","source":"Differencing can be done in different orders:\n\nFirst order differencing: linear trends with  \nz\ni\n=\ny\ni\nâˆ’\ny\ni\nâˆ’\n1\n \nSecond-order differencing: quadratic trends with  \nz\ni\n=\n(\ny\ni\nâˆ’\ny\ni\nâˆ’\n1\n)\nâˆ’\n(\ny\ni\nâˆ’\n1\nâˆ’\ny\ni\nâˆ’\n2\n)\n \nand so on...","metadata":{}},{"cell_type":"code","source":"# First Order Differencing\nts_diff = np.diff(df['depth_to_groundwater'])\ndf['depth_to_groundwater_diff_1'] = np.append([0], ts_diff)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 6))\nvisualisation(df['depth_to_groundwater_diff_1'], 'Differenced (1. Order) \\n Depth to Groundwater', ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:39.295676Z","iopub.execute_input":"2021-11-07T15:28:39.297187Z","iopub.status.idle":"2021-11-07T15:28:39.833816Z","shell.execute_reply.started":"2021-11-07T15:28:39.297144Z","shell.execute_reply":"2021-11-07T15:28:39.833088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\n","metadata":{}},{"cell_type":"code","source":"df['year'] = pd.DatetimeIndex(df['date']).year\ndf['month'] = pd.DatetimeIndex(df['date']).month\ndf['days'] = pd.DatetimeIndex(df['date']).day\ndf['day_of_year'] = pd.DatetimeIndex(df['date']).dayofyear\ndf['week_of_year'] = pd.DatetimeIndex(df['date']).weekofyear\ndf['quarter'] = df['month']%12//3+1","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:39.834888Z","iopub.execute_input":"2021-11-07T15:28:39.835257Z","iopub.status.idle":"2021-11-07T15:28:39.86964Z","shell.execute_reply.started":"2021-11-07T15:28:39.835224Z","shell.execute_reply":"2021-11-07T15:28:39.86871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['year','month','days','day_of_year','week_of_year','quarter']]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:39.870934Z","iopub.execute_input":"2021-11-07T15:28:39.871302Z","iopub.status.idle":"2021-11-07T15:28:39.893475Z","shell.execute_reply.started":"2021-11-07T15:28:39.871245Z","shell.execute_reply":"2021-11-07T15:28:39.891083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding Cyclic Features\n\nfrom the above we can see that new time features are cyclical, fo example if we take feature month. it cycles from 1 to 12 having a difference of  1 month for each month. if you consider for two year then diffrence between the year end month december and the next year starting month january will be 12-1 = 11 ","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=1,ncols=1,figsize=(20,3))\n\nsns.lineplot(x=df['date'],y=df['month'],color = 'dodgerblue')\nax.set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:39.897176Z","iopub.execute_input":"2021-11-07T15:28:39.897671Z","iopub.status.idle":"2021-11-07T15:28:40.263607Z","shell.execute_reply.started":"2021-11-07T15:28:39.897636Z","shell.execute_reply":"2021-11-07T15:28:40.26285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"month_in_year = 12\ndf['month_sin'] = np.sin(2*np.pi*df['month']/month_in_year)\ndf['month_cos'] = np.cos(2*np.pi*df['month']/month_in_year)\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\nsns.scatterplot(df.month_sin,df.month_cos)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:40.267978Z","iopub.execute_input":"2021-11-07T15:28:40.270183Z","iopub.status.idle":"2021-11-07T15:28:40.653306Z","shell.execute_reply.started":"2021-11-07T15:28:40.270141Z","shell.execute_reply":"2021-11-07T15:28:40.652559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TimeSeries Decomposition\n\nTimeseries Decomposition is making time series more understandable with the following terms Level,Trend,Sesonality,Noise. By decomposing the timeseries data.\n\nLevel: The average value in the series.\nTrend: The increasing or decreasing value in the series.\nSeasonality: The repeating short-term cycle in the series.\nNoise: The random variation in the series.\n\nDecomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.\n\nAll series have a level and noise. The trend and seasonality components are optional.\n\nIt is helpful to think of the components as combining either additively or multiplicatively:\n\nAdditive:  \ny\n(\nt\n)\n=\nL\ne\nv\ne\nl\n+\nT\nr\ne\nn\nd\n+\nS\ne\na\ns\no\nn\na\nl\ni\nt\ny\n+\nN\no\ni\ns\ne\n \nMultiplicative:  \ny\n(\nt\n)\n=\nL\ne\nv\ne\nl\nâˆ—\nT\nr\ne\nn\nd\nâˆ—\nS\ne\na\ns\no\nn\na\nl\ni\nt\ny\nâˆ—\nN\no\ni\ns\ne\n \nIn this case we are going to use function seasonal_decompose() from the statsmodels library.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ncore_columns =  [\n    'rainfall', 'temperature', 'drainage_volume', \n    'river_hydrometry', 'depth_to_groundwater'\n]\n\nfor column in core_columns:\n    decomp = seasonal_decompose(df[column], period=52, model='additive', extrapolate_trend='freq')\n    df[f\"{column}_trend\"] = decomp.trend\n    df[f\"{column}_seasonal\"] = decomp.seasonal","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:40.657697Z","iopub.execute_input":"2021-11-07T15:28:40.659935Z","iopub.status.idle":"2021-11-07T15:28:40.701439Z","shell.execute_reply.started":"2021-11-07T15:28:40.659891Z","shell.execute_reply":"2021-11-07T15:28:40.700664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=4,ncols=2,figsize=(16,10))\n\nfor i, j in enumerate(['temperature', 'depth_to_groundwater']):\n    result = seasonal_decompose(df[j],freq=52,model='additive',extrapolate_trend='freq')\n    ax[0,i].set_title('Decomposition of {}'.format(j),fontsize=16)\n    \n    result.observed.plot(ax=ax[0,i],legend=False,color='dodgerblue')\n    ax[0,i].set_ylabel('observed',fontsize=14)\n    \n    result.trend.plot(ax=ax[1,i], legend=False, color='dodgerblue')\n    ax[1,i].set_ylabel('Trend', fontsize=14)\n    \n    result.seasonal.plot(ax=ax[2,i], legend=False, color='dodgerblue')\n    ax[2,i].set_ylabel('Seasonal', fontsize=14)\n    \n    result.resid.plot(ax=ax[3,i], legend=False, color='dodgerblue')\n    ax[3,i].set_ylabel('Residual', fontsize=14)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:40.702653Z","iopub.execute_input":"2021-11-07T15:28:40.702956Z","iopub.status.idle":"2021-11-07T15:28:41.799079Z","shell.execute_reply.started":"2021-11-07T15:28:40.702905Z","shell.execute_reply":"2021-11-07T15:28:41.798332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lag","metadata":{}},{"cell_type":"markdown","source":"Lag is maily used find the correlation between the features","metadata":{}},{"cell_type":"code","source":"weeks_in_month = 4\n\nfor column in core_columns:\n    df[f'{column}_seasonal_shift_b_2m'] = df[f'{column}_seasonal'].shift(-2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_b_1m'] = df[f'{column}_seasonal'].shift(-1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_1m'] = df[f'{column}_seasonal'].shift(1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_2m'] = df[f'{column}_seasonal'].shift(2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_3m'] = df[f'{column}_seasonal'].shift(3 * weeks_in_month)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:41.800192Z","iopub.execute_input":"2021-11-07T15:28:41.800535Z","iopub.status.idle":"2021-11-07T15:28:41.821572Z","shell.execute_reply.started":"2021-11-07T15:28:41.800503Z","shell.execute_reply":"2021-11-07T15:28:41.820945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{"execution":{"iopub.status.busy":"2021-11-07T05:01:38.140231Z","iopub.execute_input":"2021-11-07T05:01:38.14052Z","iopub.status.idle":"2021-11-07T05:01:38.177518Z","shell.execute_reply.started":"2021-11-07T05:01:38.14049Z","shell.execute_reply":"2021-11-07T05:01:38.176577Z"}}},{"cell_type":"markdown","source":"lets plot the data ans extrct some knowledge","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=5,ncols=1,figsize=(20,15))\n\nfor i,j in enumerate(core_columns):\n    sns.lineplot(x=df['date'],y=df[j+'_seasonal'],ax=ax[i],color= 'dodgerblue')\n    ax[i].set_ylabel(j)\n    ax[i].set_xlim([date(2017,9,30),date(2020,6,30)])\nplt.tight_layout()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:41.822864Z","iopub.execute_input":"2021-11-07T15:28:41.823149Z","iopub.status.idle":"2021-11-07T15:28:43.395432Z","shell.execute_reply.started":"2021-11-07T15:28:41.823115Z","shell.execute_reply":"2021-11-07T15:28:43.394756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see \n\ndepth_to_groundwater: Reaches maximum on May and drops minimum on October\n\ntemperature: Reaches Maximum on October and drops Minimum on January\n\ndrainage_volume: Reaches Maximum on June and drops Minumum on July\n\nriver_hydrometry: Reaches its maximum around February/March and its minimum around September","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(nrows=2,ncols=1,figsize=(20,15))\n\nsns.heatmap(df[core_columns].corr(),vmin=-1,vmax=1,annot=True,cmap='coolwarm_r',ax=ax[0])\nax[0].set_title('Correlation Matrix of Core Features', fontsize=16)\n\nshifted_cols = [\n    'depth_to_groundwater_seasonal',         \n    'temperature_seasonal_shift_b_2m',\n    'drainage_volume_seasonal_shift_2m', \n    'river_hydrometry_seasonal_shift_3m'\n]\n\nsns.heatmap(df[shifted_cols].corr(),vmin=-1,vmax=1,annot=True,cmap='coolwarm_r',ax=ax[1])\nax[1].set_title('Correlation Matrix of shifted Features', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:43.396626Z","iopub.execute_input":"2021-11-07T15:28:43.397498Z","iopub.status.idle":"2021-11-07T15:28:44.350402Z","shell.execute_reply.started":"2021-11-07T15:28:43.39746Z","shell.execute_reply":"2021-11-07T15:28:44.347795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Autocorrelation Analysis","metadata":{}},{"cell_type":"markdown","source":"After the timeseries has been stationarised by differencing. we will be plotting for AR(Auto Correlation) and MA (Moving Average) in order to correct any auto correlation in the time series.\n\nAutocorrelation Function (ACF): P = Periods to lag for eg: (if P= 3 then we will use the three previous periods of our time series in the autoregressive portion of the calculation) P helps adjust the line that is being fitted to forecast the series. P corresponds with MA parameter\n\nPartial Autocorrelation Function (PACF): D = In an ARIMA model we transform a time series into stationary one(series without trend or seasonality) using differencing. D refers to the number of differencing transformations required by the time series to get stationary. D corresponds with AR parameter.\nAutocorrelation plots help in detecting seasonality.","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(df['depth_to_groundwater_diff_1'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:44.351789Z","iopub.execute_input":"2021-11-07T15:28:44.352081Z","iopub.status.idle":"2021-11-07T15:28:44.576519Z","shell.execute_reply.started":"2021-11-07T15:28:44.352045Z","shell.execute_reply":"2021-11-07T15:28:44.575859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n\nplot_acf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[0])\nplot_pacf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:28:44.577738Z","iopub.execute_input":"2021-11-07T15:28:44.579677Z","iopub.status.idle":"2021-11-07T15:28:45.030668Z","shell.execute_reply.started":"2021-11-07T15:28:44.579632Z","shell.execute_reply":"2021-11-07T15:28:45.029867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}