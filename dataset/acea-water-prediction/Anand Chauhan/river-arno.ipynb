{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# multivariate output multi-step 1d cnn\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import date,datetime\nimport itertools\nimport statsmodels.api as sm\nimport matplotlib\nfrom statsmodels.graphics.tsaplots import (plot_acf, plot_pacf, month_plot,\n                                           quarter_plot, seasonal_plot)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print('Datasets:')\nos.listdir('../input/acea-water-prediction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's get each dataset\nAquifer_Doganella = pd.read_csv('../input/acea-water-prediction/Aquifer_Doganella.csv', index_col='Date')\nAquifer_Auser = pd.read_csv('../input/acea-water-prediction/Aquifer_Auser.csv', index_col='Date')\nWater_Spring_Amiata = pd.read_csv('../input/acea-water-prediction/Water_Spring_Amiata.csv', index_col='Date')\nLake_Bilancino = pd.read_csv('../input/acea-water-prediction/Lake_Bilancino.csv', index_col='Date')\nWater_Spring_Madonna_di_Canneto = pd.read_csv('../input/acea-water-prediction/Water_Spring_Madonna_di_Canneto.csv', index_col='Date')\nAquifer_Luco = pd.read_csv('../input/acea-water-prediction/Aquifer_Luco.csv',index_col='Date')\nAquifer_Petrignano = pd.read_csv('../input/acea-water-prediction/Aquifer_Petrignano.csv', index_col='Date')\nWater_Spring_Lupa = pd.read_csv('../input/acea-water-prediction/Water_Spring_Lupa.csv', index_col='Date')\nRiver_Arno = pd.read_csv('../input/acea-water-prediction/River_Arno.csv', index_col='Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [Aquifer_Doganella,Aquifer_Auser,Water_Spring_Amiata,Lake_Bilancino,Water_Spring_Madonna_di_Canneto,\n Aquifer_Luco,Aquifer_Petrignano,Water_Spring_Lupa,River_Arno]\ndataset_names = ['Aquifer_Doganella',\n 'Aquifer_Auser',\n 'Water_Spring_Amiata',\n 'Lake_Bilancino',\n 'Water_Spring_Madonna_di_Canneto',\n 'Aquifer_Luco',\n 'Aquifer_Petrignano',\n 'Water_Spring_Lupa',\n 'River_Arno']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(datasets)):\n    print('{} :\\n{}\\nShape:{}'.format(dataset_names[i],datasets[i].dtypes.value_counts(),datasets[i].shape))\n    print('='*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_R = pd.read_csv('../input/acea-water-prediction/River_Arno.csv')\ndf_R['Date'] = pd.to_datetime(df_R.Date, format = '%d/%m/%Y')\n#df_R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_R.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_R['day'] = df_R['Date'].dt.day\ndf_R['month'] = df_R['Date'].dt.month\ndf_R['year'] = df_R['Date'].dt.year\nyears = df_R['year'].unique()\n#df_R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\nsns.boxplot(x='year', y='Hydrometry_Nave_di_Rosano', data=df_R, ax=axes[0])\nsns.boxplot(x='month', y='Hydrometry_Nave_di_Rosano', data=df_R.loc[~df_R.year.isin([1998, 2020]), :])\n\n# Set Title\naxes[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize=18);\naxes[0].tick_params(axis='x', labelrotation = 45)\naxes[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize=18)\n#plt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize=(16,7))\ndf_R['Hydrometry_Nave_di_Rosano'].replace(0, np.nan, inplace=True)\nsns.lineplot(x='Date',y='Hydrometry_Nave_di_Rosano',data=df_R)\n#fig.update_xaxes(rangeslider_visible=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_fill = ['Date', 'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero', 'Rainfall_Vernio',\n       'Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa',\n       'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina',\n       'Rainfall_Bibbiena', 'Rainfall_Camaldoli', 'Temperature_Firenze',\n       'Hydrometry_Nave_di_Rosano']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df_R[features_to_fill].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df['year'] = pd.DatetimeIndex(temp_df['Date']).year \ntemp_df['month'] = pd.DatetimeIndex(temp_df['Date']).month \n\n\nmonth_in_year = 12\ntemp_df['month_sin'] = np.sin(2*np.pi*temp_df.month/month_in_year)\ntemp_df['month_cos'] = np.cos(2*np.pi*temp_df.month/month_in_year)\n\n\ntemp_df['season'] = temp_df.month%12 // 3 + 1\n\ntemp_df['day_of_year'] = pd.DatetimeIndex(temp_df['Date']).dayofyear\n\ndays_in_year = 365.25\ntemp_df['day_of_year_sin'] = np.sin(2*np.pi*temp_df.day_of_year/days_in_year)\ntemp_df['day_of_year_cos'] = np.cos(2*np.pi*temp_df.day_of_year/days_in_year)\n\ntemp_df['week_of_year'] = pd.DatetimeIndex(temp_df['Date']).weekofyear\n\nweeks_in_year = 52.1429\ntemp_df['week_of_year_sin'] = np.sin(2*np.pi*temp_df.week_of_year/weeks_in_year)\ntemp_df['week_of_year_cos'] = np.cos(2*np.pi*temp_df.week_of_year/weeks_in_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare training, validation, and test data sets\n\nfeatures = ['month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'week_of_year_sin', 'week_of_year_cos', 'year', 'season']\n\nfor attribute in features_to_fill[1:]:\n    print(attribute)\n    target = attribute\n    X = temp_df[temp_df[attribute].notna()][features]#.reset_index(drop=True)\n    y = temp_df[temp_df[attribute].notna()][target]#.reset_index(drop=True)\n    print(X.shape)\n    print(y.shape)\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n\n    X_test = temp_df[features]#.reset_index(drop=True)\n    print(X_test.shape)\n    # Model\n    params = {'num_leaves': 32,\n              'objective': 'regression_l1',\n              'max_depth': 8,\n              'learning_rate': 0.05,\n              \"metric\": 'mae',\n              'seed' : 42\n            }\n\n    dtrain = lgb.Dataset(X_train, y_train)\n    dvalid = lgb.Dataset(X_valid, y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=False,  early_stopping_rounds=100)\n\n    y_pred_valid = clf.predict(X_valid)\n\n    old = temp_df[attribute].copy()\n    y_pred = clf.predict(X_test)\n    print(y_pred)\n    df_R[attribute] = np.where(df_R[attribute].isna(), y_pred, df_R[attribute])\n\n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 3))\n\n    sns.lineplot(x=df_R.Date, y=df_R[attribute], color='red',label='predicted')\n    sns.lineplot(x=df_R.Date, y=old.fillna(np.inf), color='blue', label = 'original')\n    ax.set_title('Fill NaN by Prediction', fontsize=14)\n    ax.set_ylabel(ylabel=attribute, fontsize=14)\n    ax.set_xlim([date(1998, 1, 1), date(2020, 6, 30)])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize=(16,7))\nsns.lineplot(x=df_R.Date,y = df_R.Hydrometry_Nave_di_Rosano)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_R['Hydrometry_Nave_di_Rosano']\nX = df_R.drop(['Hydrometry_Nave_di_Rosano','Date','month','year'],axis=1)\nprint(X)\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequence)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the sequence\n\t\tif end_ix > len(sequence)-1:\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = df_R['Hydrometry_Nave_di_Rosano']\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# univariate bidirectional lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Bidirectional\n\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(X, y, epochs=200, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = array(X[:len(X)-50])\nprint(x_input)\nx_input = x_input.reshape((len(X)-50, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\ndiff_results =[]\nfor i in range(0,len(yhat)):\n    diff_results.append(y - yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\n_, train_acc = model.evaluate(X, y, verbose=0)\n_, test_acc = model.evaluate(X, y, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot training history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Old code below------------------------------------------------>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X[:int(X.shape[0]*0.8)]\nX_test = X[int(X.shape[0]*0.8):]\ny_train = y[:int(X.shape[0]*0.8)]\ny_test = y[int(X.shape[0]*0.8):]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequence)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the sequence\n\t\tif end_ix > len(sequence)-1:\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = array(df_R['Hydrometry_Nave_di_Rosano'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nn_train = int(X.shape[0]*0.8)\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# multi-step data preparation\nfrom numpy import array\n \n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n\tX, y = list(), list()\n\tfor i in range(len(sequence)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps_in\n\t\tout_end_ix = end_ix + n_steps_out\n\t\t# check if we are beyond the sequence\n\t\tif out_end_ix > len(sequence):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps_in, n_steps_out = 3, 2\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=1000, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot training history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = y.reshape((y.shape[0], y.shape[1], n_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nn_steps = 3\n\nx_input = array(Y[0:300,0])\nx_input = x_input.reshape((100 , n_steps, n_features))\nyhat = model.predict(x_input, verbose=1)\nprint(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\ntestX = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = array(trainX)\nx_input = x_input.reshape((X_train.shape[0], n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom pandas import DataFrame\nfrom pandas import Series\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_error\n# date-time parsing function for loading the dataset\ndef parser(x):\n\treturn datetime.strptime(x, '%d/%m/%Y')\n \n# convert time series into supervised learning problem\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n \n# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn Series(diff)\n \n# transform series into train and test sets for supervised learning\ndef prepare_data(series, n_test, n_lag, n_seq):\n\t# extract raw values\n\traw_values = series.values\n\t# transform data to be stationary\n\tdiff_series = difference(raw_values, 1)\n\tdiff_values = diff_series.values\n\tdiff_values = diff_values.reshape(len(diff_values), 1)\n\t# rescale values to -1, 1\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\n\tscaled_values = scaler.fit_transform(diff_values)\n\tscaled_values = scaled_values.reshape(len(scaled_values), 1)\n\t# transform into supervised learning problem X, y\n\tsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\n\tsupervised_values = supervised.values\n\t# split into train and test sets\n\ttrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n\treturn scaler, train, test\n \n# fit an LSTM network to training data\ndef fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n\t# reshape training into [samples, timesteps, features]\n\tX, y = train[:, 0:n_lag], train[:, n_lag:]\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\n\t# design network\n\tmodel = Sequential()\n\tmodel.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n\tmodel.add(Dense(y.shape[1]))\n\tmodel.compile(loss='mean_squared_error', optimizer='adam',  metrics=['accuracy'])\n\t# fit network\n\tfor i in range(nb_epoch):\n\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n\t\tmodel.reset_states()\n\treturn model\n \n# make one forecast with an LSTM,\ndef forecast_lstm(model, X, n_batch):\n\t# reshape input pattern to [samples, timesteps, features]\n\tX = X.reshape(1, 1, len(X))\n\t# make forecast\n\tforecast = model.predict(X, batch_size=n_batch)\n\t# convert to array\n\treturn [x for x in forecast[0, :]]\n \n# evaluate the persistence model\ndef make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n\tforecasts = list()\n\tfor i in range(len(test)):\n\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n\t\t# make forecast\n\t\tforecast = forecast_lstm(model, X, n_batch)\n\t\t# store the forecast\n\t\tforecasts.append(forecast)\n\treturn forecasts\n \n# invert differenced forecast\ndef inverse_difference(last_ob, forecast):\n\t# invert first forecast\n\tinverted = list()\n\tinverted.append(forecast[0] + last_ob)\n\t# propagate difference forecast using inverted first value\n\tfor i in range(1, len(forecast)):\n\t\tinverted.append(forecast[i] + inverted[i-1])\n\treturn inverted\n \n# inverse data transform on forecasts\ndef inverse_transform(series, forecasts, scaler, n_test):\n\tinverted = list()\n\tfor i in range(len(forecasts)):\n\t\t# create array from forecast\n\t\tforecast = array(forecasts[i])\n\t\tforecast = forecast.reshape(1, len(forecast))\n\t\t# invert scaling\n\t\tinv_scale = scaler.inverse_transform(forecast)\n\t\tinv_scale = inv_scale[0, :]\n\t\t# invert differencing\n\t\tindex = len(series) - n_test + i - 1\n\t\tlast_ob = series.values[index]\n\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n\t\t# store\n\t\tinverted.append(inv_diff)\n\treturn inverted\n \n# evaluate the RMSE for each forecast time step\ndef evaluate_forecasts(test, forecasts, n_lag, n_seq):\n\tfor i in range(n_seq):\n\t\tactual = [row[i] for row in test]\n\t\tpredicted = [forecast[i] for forecast in forecasts]\n\t\trmse = math.sqrt(mean_squared_error(actual, predicted))\n\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n \n# plot the forecasts in the context of the original dataset\ndef plot_forecasts(series, forecasts, n_test):\n\t# plot the entire dataset in blue\n\t\n    pyplot.plot(series.values)\n\t# plot the forecasts in red\n\tfor i in range(len(forecasts)):\n\t\toff_s = len(series) - n_test + i - 1\n\t\toff_e = off_s + len(forecasts[i]) + 1\n\t\txaxis = [x for x in range(off_s, off_e)]\n\t\tyaxis = [series.values[off_s]] + forecasts[i]\n\t\tpyplot.plot(xaxis, yaxis, color='red')\n\t# show the plot\n\tpyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\nseries = pd.read_csv('../input/acea-water-prediction/River_Arno.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nseries.drop(series.iloc[:, 0:15], inplace = True, axis = 1)\n\n# configure\nn_lag = 1\nn_seq = 3\nn_test = 100\nn_epochs = 100\nn_batch = 1\nn_neurons = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare data\nscaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n# fit model\nmodel = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make forecasts\n\nforecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n# inverse transform forecasts and test\nforecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n##actual = [row[n_lag:] for row in test]\n##actual = inverse_transform(series, actual, scaler, n_test+2)\n# evaluate forecasts\n##evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n# plot forecasts\nfig = plt.subplots(figsize = (50,7))\nplot_forecasts(series, forecasts, n_test+2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}