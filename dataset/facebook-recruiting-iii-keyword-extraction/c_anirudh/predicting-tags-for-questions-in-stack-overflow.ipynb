{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Tags for Questions in Stack Overflow"},{"metadata":{},"cell_type":"markdown","source":"## Contents\n\n* [Loading Data](#loadingData)\n* [Data Preprocessing](#dataPreprocessing)\n    * [Basic Data Analysis on Tags](#tagAnalysis)\n* [Text Processing](#textProcessing)\n* [Supervised ML Models](#supervisedModels)\n    * [Logistic Regresssion](#logisticRegression)\n    * [SGD Classifier](#SGD)\n    * [Multilabel KNN](#MLKNN)\n* [Unsupervised Learning Algorithms](#unsupervised)\n    * [K-Means Clustering](#kmeans)\n    * [Mean Shift](#meanShift)\n* [References](#references)\n   "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"# Importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom skmultilearn.adapt import MLkNN\n\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, precision_score, recall_score, silhouette_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"loadingData\">\n<h2>Loading Data</h2>\n</div>"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Loading the data into a pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/facebook-recruiting-iii-keyword-extraction/Train.zip\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Prints the shape of the dataframe, i.e,  the number of rows and columns in the dataset\nprint(\"Dataframe shape : \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"'''\nSince the number of records in the data is very large(60,34,195) so \nlet's consider a small subset of data for faster computing.\n\n.iloc[] Purely integer-location based indexing for selection by position.\n'''\n\ndf = df.iloc[:10000, :] # selecting the first 10000 rows and all columns of the dataset\nprint(\"Shape of Dataframe after subsetting : \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"dataPreprocessing\">\n<h2>Data Preprocessing</h2>\n</div>"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"'''\nChecking for duplicates and removing them\n\nPandas duplicated() method helps in analyzing duplicate values only. It returns \na boolean series which is false only for Unique elements and first occurence of\nduplicate elements by default.\n\n'''\n\nduplicate_pairs = df.sort_values('Title', ascending=False).duplicated('Title')\nprint(\"Total number of duplicate questions : \", duplicate_pairs.sum())\ndf = df[~duplicate_pairs] # passing NOT of bool series to see unique values only \nprint(\"Dataframe shape after duplicate removal : \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Create a new column in the dataframe with the count of tags for each question\ndf[\"tag_count\"] = df[\"Tags\"].apply(lambda x : len(x.split()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"tagAnalysis\">\n<h3>Basic Data Analysis on Tags</h3>\n</div>"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"'''\nFrequency of tag_count\n\n.value_counts() Return a Series containing counts of unique values.\n'''\n\ndf[\"tag_count\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print( \"Maximum number of tags in a question: \", df[\"tag_count\"].max())\nprint( \"Minimum number of tags in a question: \", df[\"tag_count\"].min())\nprint( \"Average number of tags in a question: \", df[\"tag_count\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a graph showing the frequency of each count of tags\nsns.countplot(df[\"tag_count\"])\nplt.title(\"Number of tags in questions \")\nplt.xlabel(\"Number of Tags\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n1. Maximum number of tags in a question: **5**\n2. Minimum number of tags in a question: **1**\n3. Average number of tags per question: **2.92**\n4. Most of the questions have either 2 or 3 tags"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"'''\nCountVectorizer tokenizes the text along with performing very basic preprocessing like removing the\npunctuation marks, converting all the words to lowercase, etc.\n\nfit_transform()\nLearn the vocabulary dictionary and return document-term matrix.\n'''\n\nvectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_bow = vectorizer.fit_transform(df['Tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(\"Number of questions :\", tag_bow.shape[0])\nprint(\"Number of unique tags :\", tag_bow.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tags = vectorizer.get_feature_names()\nprint(\"Some of the tags :\", tags[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Frequency of each tag"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"'''\naxis=0 Column-wise sum\n.A1 returns a falttened numpy array\n'''\n\nfreq = tag_bow.sum(axis=0).A1\ntag_to_count_map = dict(zip(tags, freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"list = []\nfor key, value in tag_to_count_map.items():\n  list.append([key, value]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tag_df = pd.DataFrame(list, columns=['Tags', 'Counts'])\ntag_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\nplt.plot(tag_df_sorted['Counts'].values)\nplt.grid()\nplt.title(\"Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.grid()\nplt.title(\"Top 100 tags : Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.scatter(x=np.arange(0,100,5), y=tag_df_sorted['Counts'][0:100:5], c='g', label=\"quantiles with 0.05 intervals\")\nplt.scatter(x=np.arange(0,100,25), y=tag_df_sorted['Counts'][0:100:25], c='r', label = \"quantiles with 0.25 intervals\")\nfor x,y in zip(np.arange(0,100,25), tag_df_sorted['Counts'][0:100:25]):\n    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.01, y+30))\n\nplt.title('first 100 tags: Distribution of frequency of tags based on appeareance')\nplt.grid()\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(\"{} tags are used more than 25 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>25].shape[0]))\nprint(\"{} tags are used more than 50 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>50].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n1. 144 tags are used more than 25 times.\n2. 59 tags are used more than 50 times.\n3. C# is most frequently used tag 778 times.\n4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this problem."},{"metadata":{},"cell_type":"markdown","source":"Word map for most frequent Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_to_count_map\ntupl = dict(tag_to_count_map.items())\nword_cloud = WordCloud(width=1600,height=800,).generate_from_frequencies(tupl)\nplt.figure(figsize = (12,8))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n\n\"c#\", \"java\", \"php\", \"android\", \"javascript\", \"jquery\", \"C++\" are some of the most frequent tags."},{"metadata":{},"cell_type":"markdown","source":"Bar plot of top 20 tags"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"i=np.arange(20)\ntag_df_sorted.head(20).plot(kind='bar')\nplt.title('Frequency of top 20 tags')\nplt.xticks(i, tag_df_sorted['Tags'])\nplt.xlabel('Tags')\nplt.ylabel('Counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"textProcessing\">\n    <h2>Text Processing</h2>\n</div>"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"qus_list=[]\nqus_with_code = 0\nlen_before_preprocessing = 0 \nlen_after_preprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        qus_with_code+=1\n    len_before_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)</code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+ body\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    qus_list.append(question)\n    len_after_preprocessing += len(question)\ndf[\"question_with_title\"] = qus_list\navg_len_before_preprocessing=(len_before_preprocessing*1.0)/df.shape[0]\navg_len_after_preprocessing=(len_after_preprocessing*1.0)/df.shape[0]\nprint( \"Avg. length of questions(Title+Body) before preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) after preprocessing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (qus_with_code*100.0)/df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"preprocessed_df = df[[\"question_with_title\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\ny_multilabel = vectorizer.fit_transform(preprocessed_df['Tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def tags_to_consider(n):\n    tag_i_sum = y_multilabel.sum(axis=0).tolist()[0]\n    sorted_tags_i = sorted(range(len(tag_i_sum)), key=lambda i: tag_i_sum[i], reverse=True)\n    yn_multilabel = y_multilabel[:,sorted_tags_i[:n]]\n    return yn_multilabel\n\ndef questions_covered_fn(numb):\n    yn_multilabel = tags_to_consider(numb)\n    x = yn_multilabel.sum(axis=1)\n    return (np.count_nonzero(x==0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"questions_covered = []\ntotal_tags = y_multilabel.shape[1]\ntotal_qus = preprocessed_df.shape[0]\nfor i in range(100, total_tags, 100):\n    questions_covered.append(np.round(((total_qus-questions_covered_fn(i))/total_qus)*100,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(100,total_tags, 100),questions_covered)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number of questions covered partially\")\nplt.grid()\nplt.show()\nprint(questions_covered[9],\"% of questions covered by 1000 tags\")\nprint(\"Number of questions that are not covered by 100 tags : \", questions_covered_fn(1000),\"out of \", total_qus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"yx_multilabel = tags_to_consider(1000)\nprint(\"Number of tags in the subset :\", y_multilabel.shape[1])\nprint(\"Number of tags considered :\", yx_multilabel.shape[1],\"(\",(yx_multilabel.shape[1]/y_multilabel.shape[1])*100,\"%)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.2,random_state = 42)\nprint(\"Number of data points in training data :\", X_train.shape[0])\nprint(\"Number of data points in test data :\", X_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=0.009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\nX_train_multilabel = vectorizer.fit_transform(X_train['question_with_title'])\nX_test_multilabel = vectorizer.transform(X_test['question_with_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(\"Training data shape X : \",X_train_multilabel.shape, \"Y :\",y_train.shape)\nprint(\"Test data shape X : \",X_test_multilabel.shape,\"Y:\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def print_score(y_test, y_pred):\n    print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n    print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n    print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n    print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"supervisedModels\">\n    <h2>Supervised ML Models</h2>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"logisticRegression\">\n    <h3>Logistic Regression</h3>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using direct implementation of Logistic Regression\nclf2 = OneVsRestClassifier(LogisticRegression(penalty='l2'))\nclf2.fit(X_train_multilabel, y_train)\ny_pred2 = clf2.predict(X_test_multilabel)\nprint_score(y_test, y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" class LogisticRegression(object):\n    \n    def __init__(Logreg, alpha=0.01, n_iteration=100):   \n        Logreg.alpha = alpha                            \n        Logreg.n_iter = n_iteration\n        \n    def _sigmoid_function(Logreg, x): #This function is resonsible for calculating the sigmoid value with given parameter\n        value = 1 / (1 + np.exp(-x))\n        return value\n    def _cost_function(Logreg,h,theta, y): # The fuctions calculates the cost value\n        m = len(y)\n        cost = (1 / m) * (np.sum(-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))))\n        return cost\n    \n    def _gradient_descent(Logreg,X,h,theta,y,m): # This function calculates the theta value by gradient descent\n        gradient_value = np.dot(X.T, (h - y)) / m\n        theta -= Logreg.alpha * gradient_value\n        return theta\n\n    def predict(Logreg, X): # this function calls the max predict function to classify the individul feauter\n        X = np.insert(X, 0, 1, axis=1)\n        X_predicted = [max((Logreg._sigmoid_function(i.dot(theta)), c) for theta, c in Logreg.theta)[1] for i in X ]\n        return X_predicted\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logi = LogisticRegression(n_iteration=30000).fit(X_train_multilabel, y_train)\ny_pred1 = logi.predict(X_test_multilabel)\nprint_score(y_test, y_pred1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"SGD\">\n    <h3>SGD Classifier</h3>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\n# #hyper-parameter tuning of alpha\nstart = datetime.now()\nalpha = [10**-8, 10**-6, 10**-4, 10**-2]\nfor i in alpha:\n    classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=i, penalty='l1'), n_jobs=-1)\n    classifier.fit(X_train_multilabel, y_train)\n    predictions = classifier.predict(X_test_multilabel)\n    print(\"For alpha value = {}, Micro f1 score = {}\".format(i, f1_score(y_test, predictions, average = 'micro')))\n\nprint(\"Total Time taken = {}\".format(datetime.now() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training classifier with best alpha\nstart = datetime.now()\nclassifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=10**-4, penalty='l1'), n_jobs=-1)\nclassifier.fit(X_train_multilabel, y_train)\npredictions = classifier.predict(X_test_multilabel)\n\nprint_score(y_test, predictions)\n\nprint(\"Total training time: {}\".format(datetime.now() - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"MLKNN\">\n    <h3>Multilabel KNN</h3>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = MLkNN(k=10)\nknn.fit(X_train_multilabel, y_train)\ny_pred4 = knn.predict(X_test_multilabel)\nprint_score(y_test, y_pred4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names =[\"LR\",\"SGD\",\"KNN\"]\nvalues = [metrics.f1_score(y_test, y_pred2, average = 'micro')*100, metrics.f1_score(y_test, y_pred4, average = 'micro')*100, metrics.f1_score(y_test, predictions, average = 'micro')*100]\nplt.ylim(0,50)\nplt.bar(names,values)\nvalues","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"unsupervised\">\n    <h2>Unsupervised Learning Algorithms</h2>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"kmeans\">\n    <h3>K-Means clustering</h3>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sil = []\nkmax = 10\nfor k in range(2, kmax+1):\n   kmeans = KMeans(n_clusters = k).fit(X_train_multilabel)\n   labels = kmeans.labels_\n   sil.append(silhouette_score(X_train_multilabel, labels, metric = 'euclidean'))\n        \nmaxpos = sil.index(max(sil))\nn_clusters = maxpos + 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide into k groups using k-mean clustering\nmodel = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=100)\nmodel.fit(X_train_multilabel)\n    \n# plot the centroids\nplt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1],s=250, marker='*',c='red', edgecolor='black',label='centroids')\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"meanShift\">\n    <h3>Mean Shift</h3>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MeanShift\nms = MeanShift()\nms.fit(X_train_multilabel[:3000,].toarray())\ncluster_centers = ms.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_train_multilabel[:,0], X_train_multilabel[:,1], X_train_multilabel[:,2], marker='o')\nax.scatter(cluster_centers[:,0], cluster_centers[:,1], cluster_centers[:,2], marker='x', color='red', s=300, linewidth=5, zorder=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"references\">\n    <h2>References</h2>\n</div>\n\n- https://www.kaggle.com/vikashrajluhaniwal/multi-label-classification-for-tag-predictions\n- https://github.com/gauravtheP/Stackoverflow-Tag-Prediction/blob/master/SO-Tag-Prediction/SO-Tag-Prediction.ipynb"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}