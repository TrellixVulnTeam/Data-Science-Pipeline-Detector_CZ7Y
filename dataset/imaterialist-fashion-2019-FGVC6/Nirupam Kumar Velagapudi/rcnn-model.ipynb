{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing Modules\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\nfrom PIL import Image,ImageFont\n%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nimport sys\nimport random\nimport math\nimport numpy as np\nimport skimage.io\nfrom skimage.color import rgb2gray\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport cv2\nfrom scipy import ndimage\n\n# Root directory of the project\nROOT_DIR = os.path.abspath(\"../\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/imaterialist-fashion-2019-FGVC6/'\n#reading train csv file\ntrain_df = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_test_images = len(os.listdir(os.path.join(root_path,'test')))\nnum_train_images = len(os.listdir(os.path.join(root_path,'train')))\nprint(\"Number of images in test set: {}\".format(num_test_images))\nprint(\"Number of images in train set: {}\".format(num_train_images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_class_per_image = np.round(train_df.shape[0]/num_train_images, 2)\nprint(\"Average number of classes per image: {}\".format(avg_class_per_image))\nassert len(train_df[\"ImageId\"].value_counts()) == num_train_images\nprint(\"Every image has at least 1 class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading categories\nwith open(os.path.join(root_path, 'label_descriptions.json')) as f:\n    labels_data=json.load(f)\nlabels_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating the categories and attributes\ncategories = pd.DataFrame(labels_data['categories'])\nattributes = pd.DataFrame(labels_data['attributes'])\nprint(\"There are descriptions for\", categories.shape[0],\"categories and\", attributes.shape[0], \"attributes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories['supercategory'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes['supercategory'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating categories and attributes in train data\ntrain_df['hasAttributes'] = train_df.ClassId.apply(lambda x: x.find(\"_\") > 0)\ntrain_df['CategoryId'] = train_df.ClassId.apply(lambda x: x.split(\"_\")[0]).astype(int)\ntrain_df = train_df.merge(categories, left_on=\"CategoryId\", right_on=\"id\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fine_grained_obj_perc = np.round(train_df[\"hasAttributes\"].mean()*100, 1)\nprint(\"{}% of the objects are fine-grained.\".format(fine_grained_obj_perc))\nfine_grained_img_perc = np.round((train_df.groupby(\"ImageId\")[\"hasAttributes\"].sum() > 0).mean()*100, 1)\nprint(\"{}% of the images have at least one fine-grained object.\".format(fine_grained_img_perc))\nclass_df = train_df.groupby(\"CategoryId\").agg({\"ImageId\": \"count\"}).reset_index()\nclass_df = class_df.rename(columns={\"ImageId\": \"img_count\"})\nprint(\"Number of classes: {}\".format(class_df.shape[0]))\nprint(\"{} of the classes are fine-grained.\".format(train_df[train_df[\"hasAttributes\"] == True].CategoryId.nunique()))\nclass_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eda\ndef plot_function_for_supercategories(subset,title):\n    supercategory_names = np.unique(subset.supercategory)\n    plt.figure(figsize=(10, 6))\n    g = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\n    ax = g.axes\n    tl = [x.get_text() for x in ax.get_xticklabels()]    \n    ax.set_xticklabels(tl, rotation=45)\n    for p, label in zip(ax.patches, supercategory_names):\n        c = subset[(subset['supercategory'] == label)].shape[0]\n        ax.annotate(str(c), (p.get_x()+0.3, p.get_height() + 50))\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_function_for_supercategories(train_df[train_df.hasAttributes],'Supercategories with any attributes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_function_for_supercategories(train_df[~train_df.hasAttributes],'Supercategories with no attributes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"super_cat = list(train_df['supercategory'].unique())\nfig, axes = plt.subplots(6, 2, figsize=(25, 20))\nz=0\nfor i in range(0, 6):\n    for j in range(0, 2):\n        sns.countplot(y=\"name\", data=train_df[train_df.supercategory.isin([super_cat[z]])],ax = axes[i, j]).set(title = (super_cat[z]))\n        fig.tight_layout()\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## understanding and reading few images from train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading sample images from training data\nfor i in range(6):\n    id_image=train_df['ImageId'].iloc[np.random.randint(0,train_df.shape[0])]\n    print('Image ID:',id_image)\n    image = plt.imread(os.path.join(root_path,'train/',id_image))\n    plt.imshow(image)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = plt.imread(os.path.join(root_path,'train/','b98f08f330c23af5db1c62c2412592b4.jpg'))\ngray = rgb2gray(image)\nplt.imshow(gray, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nan_array = np.where(gray_r > gray_r.mean(), 0, 3)\ngray = an_array.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='binary_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# execution_path = '../input/imageai/resnet50_coco_best_v2.0.1.h5'\n# detector = ObjectDetection()\n# detector.setModelTypeAsRetinaNet()\n# detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n# detector.loadModel()\n# detections = detector.detectObjectsFromImage(input_image=os.path.join(root_path,'train/','b98f08f330c23af5db1c62c2412592b4.jpg'), output_image_path=os.path.join(root_path,'b98f08f330c23af5db1c62c2412592b4_detection.jpg'))\n# pic = plt.imread(os.path.join(root_path,'b98f08f330c23af5db1c62c2412592b4_detection.jpg'))\n# plt.imshow(pic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://www.github.com/matterport/Mask_RCNN.git\n# os.chdir('Mask_RCNN')\n# !rm -rf .git # to prevent an error when the kernel is committed\n# !rm -rf images assets # to prevent displaying images at the bottom of a kernel\n\n# # Root directory of the project\n# ROOT_DIR = os.path.abspath(\"../\")\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Import Mask RCNN\n# sys.path.append(ROOT_DIR)  # To find local version of the library\n# from mrcnn import utils\n# import mrcnn.model as modellib\n# from mrcnn import visualize\n# # Import COCO config\n# sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n# import coco\n\n# # Directory to save logs and trained model\n# MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n\n# # Local path to trained weights file\n# COCO_MODEL_PATH = os.path.join('', \"mask_rcnn_coco.h5\")\n\n# # Download COCO trained weights from Releases if needed\n# if not os.path.exists(COCO_MODEL_PATH):\n#     utils.download_trained_weights(COCO_MODEL_PATH)\n\n# # Directory of images to run detection on\n# IMAGE_DIR = os.path.join(ROOT_DIR, \"trump.jpg\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ProgressBar\n# import required packages\nfrom pathlib import Path\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *\nfrom progressbar import ProgressBar\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a folder for the mask images\nif  not os.path.isdir('../labels'):\n    os.makedirs('../labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path = Path(\"../input/imaterialist-fashion-2019-FGVC6\")\n# path_img = path+'/train'\n# path_lbl = root_path+Path(\"../labels\")\n# only the 27 apparel items, plus 1 for background\n# model image size 224x224\ncategory_num = 27 + 1\nsize = 224\n# get and show categories\n# with open(os.path.join(root_path,\"label_descriptions.json\")) as f:\n#     label_descriptions = json.load(f)\n# label_names = [x['name'] for x in label_descriptions['categories']]\n# print(label_names)\n# train dataframe\ndf = train_df[['ImageId', 'EncodedPixels', 'Height', 'Width','ClassId']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training image path and images\nfnames = get_image_files(os.path.join(root_path,'train'))\nprint(fnames[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# need a function to turn the run encoded pixels from train.csv into an image mask\n# there are multiple rows per image for different apparel items, this groups them into one mask\ndef make_mask_img(segment_df):\n    seg_width = segment_df.at[0, \"Width\"]\n    seg_height = segment_df.at[0, \"Height\"]\n    seg_img = np.full(seg_width*seg_height, category_num-1, dtype=np.int32)\n    for encoded_pixels, class_id in zip(segment_df[\"EncodedPixels\"].values, segment_df[\"ClassId\"].values):\n        pixel_list = list(map(int, encoded_pixels.split(\" \")))\n        for i in range(0, len(pixel_list), 2):\n            start_index = pixel_list[i] - 1\n            index_len = pixel_list[i+1] - 1\n            if int(class_id.split(\"_\")[0]) < category_num - 1:\n                seg_img[start_index:start_index+index_len] = int(class_id.split(\"_\")[0])\n    seg_img = seg_img.reshape((seg_height, seg_width), order='F')\n    return seg_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can look at an image to see how the processing works\n# the original image\nimg_file = fnames[500]\nimg = open_image(img_file)\nimg.show(figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert rows for this image into a numpy array mask\nimg_name = os.path.basename(img_file)\nimg_df = df[df.ImageId == img_name].reset_index()\n#img_df = img_df.iloc[0:1]\n#img_df = img_df[img_df.ClassId.astype(int) < category_num - 1].reset_index()\nimg_mask = make_mask_img(img_df)\nplt.imshow(img_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the numpy array into a three channel png that can be used in the standard SegmentationItemList\n# then write into the labels folder as png and show the image\n# all pixels have the category numbers, so it looks like a dark greyscale image\nimg_mask_3_chn = np.dstack((img_mask, img_mask, img_mask))\ncv2.imwrite('../labels/' + os.path.splitext(img_name)[0] + '_P.png', img_mask_3_chn)\npng = open_image('../labels/' + os.path.splitext(img_name)[0] + '_P.png')\npng.show(figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use fastai's open_mask for an easier-to-view image (and check it works...)\nmask = open_mask('../labels/' + os.path.splitext(img_name)[0] + '_P.png')\nmask.show(figsize=(5,5), alpha=1)\nprint(mask.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the same procedure for a sample of first 5000 images in dataset\nimages = df.ImageId.unique()[:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pbar = ProgressBar()\nfor img in pbar(images):\n    img_df = df[df.ImageId == img].reset_index()\n    img_mask = make_mask_img(img_df)\n    img_mask_3_chn = np.dstack((img_mask, img_mask, img_mask))\n    cv2.imwrite('../labels/' + os.path.splitext(img)[0] + '_P.png', img_mask_3_chn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before creating the databunch we need a function to find the mask images\n# also set the batch size, categories and wd\nget_y_fn = lambda x: Path(\"../labels\")/f'{Path(x).stem}_P.png'\nbs = 32\n#classes = label_names\ncodes = list(range(category_num))\nwd = 1e-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the databunch\nimages_df = pd.DataFrame(images)\nsrc = (SegmentationItemList.from_df(images_df, os.path.join(root_path,'train'))\n       .split_by_rand_pct()\n       .label_from_func(get_y_fn, classes=codes))\n\ndata = (src.transform(get_transforms(), size=size, tfm_y=True)\n       .databunch(bs=bs)\n       .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at a batch\ndata.show_batch(3, figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I create an accuracy metric which excludes the background pixels\ndef acc_fashion(input, target):\n    target = target.squeeze(1)\n    mask = target != category_num - 1\n    return (input.argmax(dim=1)==target).float().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learner, include where to save pre-trained weights (default is in non-write directory)\nlearn = unet_learner(data, models.resnet34, metrics=acc_fashion, wd=wd, model_dir=\"/kaggle/working/models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run learning rate finder\nlr_find(learn)\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set learning rate based on roughly the steepest part of the curve\nlr=1e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train for 10 cycles frozen\nlearn.fit_one_cycle(10, slice(lr), pct_start=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at some results\nlearn.show_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unfreeze earlier weights\nlearn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decrease the learning rate\nlrs = slice(lr/400,lr/4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train for 10 more cycles unfrozen\nlearn.fit_one_cycle(10, lrs, pct_start=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# more results\nlearn.show_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/imaterialist-fashion-2019-FGVC6/test/0046f98599f05fd7233973e430d6d04d.jpg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(learn.data.classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/imaterialist-fashion-2019-FGVC6/test/0146a53e12d690914995248fb6872121.jpg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x[2][27])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/personal-testing/1.jpeg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/personal-group-1/_G2A0656.JPG'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/personal-2/MicrosoftTeams-image (6).png'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/personal-2/MicrosoftTeams-image (7).png'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x[2][27])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnames = get_image_files('../input/personal-girl-power')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[0]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[1]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[2]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[3]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[4]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = gnames[5]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}