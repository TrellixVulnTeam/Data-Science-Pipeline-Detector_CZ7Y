{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport json\nfrom PIL import Image\n\nBASE_DIR = \"../input\"\n\nIMAGES_TRAIN_DIR = f\"{BASE_DIR}/train\"\nIMAGES_TEST_DIR = f\"{BASE_DIR}/test\"\nTRAIN_CSV = f\"{BASE_DIR}/train.csv\"\nLABEL_DESCRIPTIONS = f\"{BASE_DIR}/label_descriptions.json\"\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#print(os.listdir(\"../input\"))\ntrain_df = pd.read_csv(TRAIN_CSV)\n\nwith open(LABEL_DESCRIPTIONS) as f:\n    image_info = json.load(f)\ncategories = pd.DataFrame(image_info['categories'])\nattributes = pd.DataFrame(image_info['attributes'])\nprint(\"There are descriptions for\", categories.shape[0],\"categories and\", attributes.shape[0], \"attributes\")\n\ntrain_df['hasAttributes'] = train_df.ClassId.apply(lambda x: x.find(\"_\") > 0)\ntrain_df['CategoryId'] = train_df.ClassId.apply(lambda x: x.split(\"_\")[0]).astype(int)\ntrain_df = train_df.merge(categories, left_on=\"CategoryId\", right_on=\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both attributes and categories contain additional `supercategory` column, which might be the source of insights related to our data. Cloths from the same supercategory are similar in some sense. Attribute's supercategory denotes that it describes some specific property (I.e., length, style)."},{"metadata":{},"cell_type":"markdown","source":"# Supercategories"},{"metadata":{},"cell_type":"markdown","source":"Supercategories might be the key to answering following questions:\n1. How many mask annotations have any associated attributes?\n2. How often specific attributes (or attribute groups) appear within category's supercategory?\n3. Is there a way to filter train data somehow?\n4. etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fraction of mask annotations with any attributes within train data:\", train_df.hasAttributes.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = train_df[~train_df.hasAttributes]\nsupercategory_names = np.unique(subset.supercategory)\nplt.figure(figsize=(10, 10))\ng = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\nax = g.axes\ntl = [x.get_text() for x in ax.get_xticklabels()]    \nax.set_xticklabels(tl, rotation=90)\nfor p, label in zip(ax.patches, supercategory_names):\n    c = subset[(subset['supercategory'] == label)].shape[0]\n    ax.annotate(str(c), (p.get_x(), p.get_height() + 1000))\nplt.title(\"Supercategories with no attributes\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = train_df[train_df.hasAttributes]\nsupercategory_names = np.unique(subset.supercategory)\ng = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\nax = g.axes\ntl = [x.get_text() for x in ax.get_xticklabels()]    \nax.set_xticklabels(tl, rotation=90)\nfor p, label in zip(ax.patches, supercategory_names):\n    c = subset[(subset['supercategory'] == label)].shape[0]\n    ax.annotate(str(c), (p.get_x()+0.3, p.get_height() + 50))\nplt.title(\"Supercategories with any attributes\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from previous 2 plots, number of mask annotations with any attribute is relatively small. Only masks related to 4 supercategories (well, 3, if we neglect 3 mask annotations related to `garment parts`) have any associated attribute. For every other supercategory, we might ignore them."},{"metadata":{},"cell_type":"markdown","source":"Now we will take a closer look at train dataset and check how many categories are presented there. What supercategories are varied the most?"},{"metadata":{"trusted":true},"cell_type":"code","source":"supercategory_names = train_df[['supercategory', 'name']].groupby('supercategory').agg(\n    lambda x: x.unique().shape[0]).reset_index().sort_values(\"name\", ascending=False).set_index('name')\nsupercategory_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Relative counts for 3 supercategories (`decorations`, `garment parts`, `upperbody`) are shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def buildPlot(**kwargs):\n    data = kwargs['data']\n    g = sns.countplot(y=\"name\", data=data)\n    g.set_yticklabels(data['name'].unique())#, rotation=90)\n    \nidx = train_df.supercategory.isin(['decorations', 'garment parts', 'upperbody'])\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[4].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the dataset the most common `wholebody` cloth type is `dress`. `Shoe` dominates in 2nd most common supercategory (`legs and feet`), that means that it might be worth to search for shoes on the photos :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train_df.ImageId.unique().shape[0]\nprint(f\"There are {total} images in train dataset.\")\nimages_with_shoes = train_df[train_df.name==\"shoe\"].ImageId.unique().shape[0]\nimages_with_legs = train_df[train_df.supercategory==\"legs and feet\"].ImageId.unique().shape[0]\nprint(f\"However, only {images_with_legs} images have associated legs and feet annotation, and only {images_with_shoes} have any shoes on it.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[3].values)\ng = sns.FacetGrid(data=train_df[idx], row=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[2].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[1].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Connecting categories and attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract all available attributes and create separate table\ncat_attributes = []\nfor i in train_df[train_df.hasAttributes].index:\n    item = train_df.loc[i]\n    xs = item.ClassId.split(\"_\")\n    for a in xs[1:]:\n        cat_attributes.append({'ImageId': item.ImageId, 'category': int(xs[0]), 'attribute': int(a)})\ncat_attributes = pd.DataFrame(cat_attributes)\n\ncat_attributes = cat_attributes.merge(\n    categories, left_on=\"category\", right_on=\"id\"\n).merge(attributes, left_on=\"attribute\", right_on=\"id\", suffixes=(\"\", \"_attribute\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper objects and methods\nscat_x, count_x = np.unique(cat_attributes['supercategory'], return_counts=True)\ncategories_by_x = {\n    x: dict(cat_attributes[cat_attributes['supercategory'] == x][['name', 'category']].drop_duplicates().values)\n    for x in scat_x}\nscat_y, count_y = np.unique(cat_attributes['supercategory_attribute'], return_counts=True)\ncategories_by_y = {\n    y: dict(cat_attributes[cat_attributes['supercategory_attribute'] == y][['name_attribute', 'attribute']].drop_duplicates().values) \n    for y in scat_y}\nvals = cat_attributes.groupby(['category', 'attribute']).count().reset_index(drop=True).values[:,0]\nscale_min, scale_max = vals.min(), vals.max()\n\ndef get_scatter_data(x, y, cat, attr):\n    ids_x = {cat[k]: i for i, k in enumerate(cat)}\n    ids_y = {attr[k]: i for i, k in enumerate(attr)}\n    data = np.zeros((len(cat), len(attr)), dtype=np.uint)\n    for k, v in zip(x, y):\n        data[ids_x[k], ids_y[v]]+=1\n    ii, jj = np.where(data > 0)\n    sizes = [data[i, j] for i, j in zip(ii, jj)]\n    return ii, jj, sizes\n\ndef drawPunchcard(**kwargs):\n    data = kwargs['data']\n    x = data[\"category\"]\n    y = data[\"attribute\"]\n    supercategory_x = data[\"supercategory\"].values[0]\n    cat = categories_by_x[supercategory_x]\n    supercategory_y = data[\"supercategory_attribute\"].values[0]\n    attr = categories_by_y[supercategory_y]\n    ii, jj, sizes = get_scatter_data(\n        x, y, \n        cat, \n        attr)\n    g = sns.scatterplot(ii, jj, size=sizes, sizes=(20, 200), hue=np.log(sizes)+1)\n    g.set_xticks(np.arange(len(cat)))\n    g.set_xticklabels(list(cat), rotation=90)\n    g.set_yticks(np.arange(len(attr)))\n    g.set_yticklabels(list(attr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.color_palette(\"bright\")\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nwidth_ratios=[len(categories_by_x[x]) for x in categories_by_x]\nheight_ratios=[len(categories_by_y[x]) for x in categories_by_y]\ng = sns.FacetGrid(data=cat_attributes, col=\"supercategory\",  row=\"supercategory_attribute\", \n                  #margin_titles=True, \n                  gridspec_kws={'height_ratios': height_ratios, 'width_ratios': width_ratios},\n                  sharex=\"col\", sharey=\"row\",\n                  col_order=list(categories_by_x),\n                  row_order=list(categories_by_y))#.set_titles('{col_name}', '{row_name}')\ng = g.map_dataframe(drawPunchcard).set_titles('{col_name}', '{row_name}')\ng.fig.set_size_inches(10, 20) \nfor ax, cat_name in zip(g.axes, list(categories_by_y)):\n    ax[-1].set_ylabel(cat_name, labelpad=10, rotation=-90)\n    ax[-1].yaxis.set_label_position(\"right\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Closer look at train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"images = train_df[['ImageId', \"Width\", \"Height\"]].drop_duplicates()\nprint(\"Number of unique triplets (ImageId, Width, Height):\", images.shape[0])\nprint(\"Unique image names: \", images['ImageId'].unique().shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Check Width and Height correctness\nThere are no images with different width and height parameters in `train.csv` file. That doesn't mean that there are no errors. Just in case we'll check all dimensions for train images, and compare them with the ones provided in annotation file."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image_dimensions(path):\n    \"returns real width and height\"\n    with Image.open(path) as image:\n        dimensions = image.size\n    return dimensions\n\nimages_with_incorrect_size = {}\nfor ImageId, width, height in images.values:\n    image_path = os.path.join(IMAGES_TRAIN_DIR, ImageId)\n    (real_width, real_height) = read_image_dimensions(image_path)\n    if real_width != width or real_height!=height:\n        images_with_incorrect_size[ImageId] = (real_width, real_height)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of images with incorrect dimensions:\", len(images_with_incorrect_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we fix annotation file."},{"metadata":{"trusted":true},"cell_type":"code","source":"for ImageId in images_with_incorrect_size:\n    (width, height) = images_with_incorrect_size[ImageId]\n    idx = train_df['ImageId'] == ImageId\n    print(ImageId, train_df.loc[idx, \"Width\"].values[0], train_df.loc[idx, \"Height\"].values[0], \"Real dimensions:\", width, height)\n    train_df.loc[idx, \"Width\"] = width\n    train_df.loc[idx, \"Height\"] = height","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Check if there are any duplicates\nWe also check if there are any image masks with several accociated classes (we might want to ignore them while training our segmentation model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_df[[\"ImageId\", \"EncodedPixels\", \"ClassId\"]].drop_duplicates()\ngrouped_df = df.groupby([\"EncodedPixels\", \"ImageId\"]).count().reset_index()\ngrouped_df = grouped_df[grouped_df.ClassId > 1]\nprint(\"Number of images with duplicated EncodedPixels:\", grouped_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated_data = df[df.ImageId.isin(grouped_df.ImageId) & df.EncodedPixels.isin(grouped_df.EncodedPixels)].sort_values([\"ImageId\", \"EncodedPixels\"])\nduplicated_data.to_csv(\"images_with_duplicated_masks.csv\", index=None) # you can look at these images, if you want","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicates = dict()\nxlabels, ylabels = set(), set()\n\nfor (ImageId, EncodedPixels), x in duplicated_data.groupby([\"ImageId\", \"EncodedPixels\"]):\n    pair = tuple(sorted(x.ClassId.values))\n    s,e = pair\n    xlabels.add(s)\n    ylabels.add(e)\n    if not pair in duplicates:\n        duplicates[pair] = 0\n    duplicates[pair] +=1\n\nxlabels = {x: i for i, x in enumerate(sorted(xlabels))}\nylabels = {x: i for i, x in enumerate(sorted(ylabels))}\nmatrix = np.zeros((len(ylabels), len(xlabels)), dtype=np.int)\nfor (s, e) in duplicates:\n    matrix[ylabels[e], xlabels[s]] = duplicates[(s, e)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nannot = np.array([[(str(x) if x >0 else \"\") for x in line]for line in matrix])\nsns.heatmap(matrix, annot=annot, fmt=\"s\",xticklabels=sorted(list(xlabels)), yticklabels=sorted(list(ylabels)), square=True, cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most common duplicated mask is marked with image categories with ids 32 and 35."},{"metadata":{"trusted":true},"cell_type":"code","source":"categories[categories.id.isin((32, 35))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pockets and zipper are usually located nearby, and they are relatively small. There also might be pockets with zipper. Maybe not a mistake after all?"},{"metadata":{},"cell_type":"markdown","source":"# Masking"},{"metadata":{},"cell_type":"markdown","source":"For every mask in train dataset, we convert number of masks pixels to a fraction of total image pixels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sum_mask_pixels(encoded_pixels):\n    pixels = [np.int(x) for x in encoded_pixels.split(\" \")]\n    return np.sum(pixels[1::2])\n\ndef compute_mask_percentage(row):\n    s = sum_mask_pixels(row['EncodedPixels'])\n    return 1.0* s/row[\"Width\"]/row[\"Height\"]\n\ntrain_df['mask_fraction'] = train_df.EncodedPixels.apply(sum_mask_pixels).astype(np.float)\ntrain_df['mask_fraction'] = train_df['mask_fraction']/train_df[\"Width\"]/train_df[\"Height\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We named this parameter `mask_fraction`. We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\ng = sns.stripplot(y=\"mask_fraction\", data=train_df, x=\"supercategory\")\nlabels = [x.get_text() for x in g.get_xticklabels()]\ng = g.set_xticklabels(labels, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The biggest masks (`mask_fraction` > 0.7) are associated only with 3 supercategories - `lowerbody`, `upperbody` and `wholebody`. Masks associated with `neck`, `arms and hands`, `closures` are almost always small (< 0.1 of total image pixels).\n\nLet's take a look at some big masks (`mask_fraction` > 0.7):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_images(data=None,**kwargs):\n    plt.axis(\"off\")\n    path = os.path.join(IMAGES_TRAIN_DIR, data['ImageId'].values[0])\n    with Image.open(path) as image:\n        data = np.asarray(image)\n    plt.imshow(data)\n\nsubset = train_df[train_df.mask_fraction > 0.7]\ngrid = sns.FacetGrid(subset, col=\"name\", col_wrap=4)\ngrid.map_dataframe(draw_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These sample images are all clipped. They have particular clothes shown on it, but there is no human on the photo."},{"metadata":{},"cell_type":"markdown","source":"# Conclusions and other ideas\n1. It is quite clear that for the draft segmentation model it is sufficient to use mask's category identifier and to ignore mask's attributes.\n2. It is unclear (at least for me) if the train dataset attributes are given for all possible images, or are these attributes  provided for only a small subset of images. Do we have weakly-labeled data or fully-labeled data?\n3. Train dataset contains a very diverse collection of images, with a lot of small details. There are also images which contain only 1 apparel, already clipped. \n4. Almost a half of the images have no shoes annotation. It means that human body pictured on them (if there is any) is probably clipped. \n5. We can split train dataset to several parts. One of these parts might contain clipped clothes, the other might contain full human body. And maybe it will improve segmentation results for both subsets. But maybe not."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}