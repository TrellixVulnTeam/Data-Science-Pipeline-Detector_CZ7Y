{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is just for test.\n## This cannot make a complete model because of memory error.\n## Thank you."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.前回データの確認"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"../input/eda-for-biginner-updated-to-english-ver\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf = pd.read_csv(path+\"/traindf.csv\")\ntraindf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# おさらい"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = traindf[traindf[\"landmark_id\"]==7]\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in tmp[\"path\"]:\n    img = cv2.imread(a)\n    plt.figure()\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# おさらい2　import collectionを使って、各idの個数を数えた。それをcount数ごとに並べたのが、dfcnt"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcnt = pd.read_csv(path+\"/dfcnt.csv\")\ndfcnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(dfcnt[\"id\"],dfcnt[\"count\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# この時点で、landmark idの種類は全部で81313個、最小枚数は2枚であることがわかる(前回は138982が6272個に注目してた)ので、\n# 各landmark idごとに1枚訓練データ(traindata)、１枚検証データ(validation)にしてpytorchでモデルを作成することを考える。"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcnt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dfcntのidでfilteringして、一番上にきたやつをtrain data, 上から2つ目をvalidationとする\n## わかりやすくするため、１個で説明"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp1 = dfcnt[\"id\"].iloc[0]\ntmp1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmpdf1 = traindf[traindf[\"landmark_id\"]==tmp1]\ntmpdf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tlist = []\nvlist = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tlist.append(tmpdf1.iloc[0].values)\ntlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vlist.append(tmpdf1.iloc[1].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# これを繰り返す","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.path.exists(\"./tdf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tlist = []\nvlist = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists(\"./tdf.csv\")==False:\n    \n\n    \n\n    tmp1 = dfcnt[\"id\"].values #.valuesでnumpy. for文はnumpyのほうが早いときがある。\n\n    for a in tqdm(range(len(dfcnt))):\n\n        tmpdf1 = traindf[traindf.landmark_id.values==tmp1[a]]\n        tlist.append(tmpdf1.iloc[0].values)\n        vlist.append(tmpdf1.iloc[1].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf = pd.DataFrame(tlist,columns=tmpdf1.columns)\ntdf[\"repair_id\"]=np.arange(0,len(tdf),1)\ntdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vdf = pd.DataFrame(vlist,columns=tmpdf1.columns)\nvdf[\"repair_id\"]=np.arange(0,len(vdf),1)\nvdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists(\"./tdf.csv\"):\n    tdf = pd.read_csv(\"./tdf.csv\")\n    vdf = pd.read_csv(\"./vdf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf.to_csv(\"tdf.csv\",index=False)\nvdf.to_csv(\"vdf.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 全部やっても良いが、自分で作成するときは10枚くらいでテストするほうが効率的"},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf2 = tdf.iloc[:10,:]\nvdf2 = vdf.iloc[:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vdf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ここからpytorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision.models import resnet18\nfrom albumentations import Normalize, Compose\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\nimport multiprocessing as mp\n\n\n\nif torch.cuda.is_available():\n    device = 'cuda:0'\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = 'cpu'\nprint(f'Running on device: {device}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# このサイトがとても分かりやすく書いてくれているので、迷ったらここを見る\nhttps://qiita.com/takurooo/items/e4c91c5d78059f92e76d"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. transformの定義"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess = Compose([\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1)\n])\n\n# resnextなどのpre-trainモデルは全て、同じ方法で正規化された入力画像を使用しなければならない。それの変換をこの関数で行う。値はdefault。\n# Composeは今回あまり、意味をなさない\n# https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/albumentations%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81/ に詳細は書いてある","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 画像をどれだけ小さくするかの処理\nROWS = 32\nCOLS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GLDataset(Dataset):\n    \n    def __init__(self,img_pass,labels,preprocess=None):\n        self.img_pass = img_pass\n        self.labels = labels\n        self.preprocess = preprocess\n        \n    def __len__(self):\n        return len(self.img_pass)\n    \n    def __getitem__(self,idx):\n        \n        # ここからdatasetに食わせる前の前処理の記述。\n        \n        img_pass = self.img_pass[idx]\n        label = self.labels[idx]\n        \n        land = cv2.imread(img_pass)\n        land = cv2.resize(land,(ROWS,COLS),interpolation = cv2.INTER_CUBIC)\n        land = cv2.cvtColor(land,cv2.COLOR_BGR2RGB) # augmentを使うときにBGRからRGBにする必要があるのかもしれない。\n        \n        if self.preprocess is not None: # ここで、前処理を入れてnormalizationしている。\n                augmented = self.preprocess(image=land) # preprocessのimageをfaceで読む\n                land = augmented['image'] # https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/albumentations%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81/　に書いてある\n                \n        return {'landmarks': land.transpose(2, 0, 1), 'label': np.array(label, dtype=int)}  # pytorchはchannnl, x, yの形。これは辞書型で返している。(扱いやすいというだけかも。)\n        \n        \n        \n        \n        \n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1つ1つ追って、何やっているかを見ていく。"},{"metadata":{"trusted":true},"cell_type":"code","source":"land = cv2.imread(tdf2[\"path\"].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"land = cv2.resize(land,(ROWS,COLS),interpolation = cv2.INTER_CUBIC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"land = cv2.cvtColor(land,cv2.COLOR_BGR2RGB) # augmentを使うときにBGRからRGBにする必要があるのかもしれない。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented = preprocess(image=land) # preprocessのimageをfaceで読む\nland = augmented['image']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"land.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"land=land.transpose(2, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"land.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasetのinstance化"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instance化\ntrain_dataset = GLDataset(\n    img_pass=tdf2[\"path\"],\n    labels=tdf2[\"repair_id\"].to_numpy(),\n    preprocess=preprocess\n)\n\nval_dataset = GLDataset(\n    img_pass=vdf2[\"path\"],\n    labels=vdf2[\"repair_id\"].to_numpy(),\n    preprocess=preprocess\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 2\n\n#NUM_WORKERS = mp.cpu_count()\nNUM_WORKERS = mp.cpu_count() # ここを0にしないと動かない。cpuの仕様個数。←実は動くことが判明。classの中身次第！","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NUM_WORKERS = mp.cpu_count()\n#NUM_WORKERS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DataLoaderはimport torch.utils.data.Datasetでimport済みのもの\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False, #https://schemer1341.hatenablog.com/entry/2019/01/06/024605 を参考. idがわからなくなる\n    num_workers=NUM_WORKERS\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# reference) class化してすっきりする"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PytorchDataSet():\n    \n    def __init__(self,Dataset,train_imgpath,train_label,val_imgpath,val_label,batch_size,num_workers,shuffle=True):\n        \n        self.Dataset = Dataset\n        self.train_imgpath = train_imgpath\n        self.train_label = train_label\n        self.val_imgpath = val_imgpath\n        self.val_label = val_label\n        \n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.shuffle = shuffle\n               \n               \n        self.train_dataset = self.Dataset(\n            img_pass=self.train_imgpath,\n            labels=self.train_label.to_numpy(),\n            preprocess=preprocess\n)\n        \n        self.val_dataset = self.Dataset(\n            img_pass=self.val_imgpath,\n            labels=self.val_label.to_numpy(),\n            preprocess=preprocess\n        )\n        \n                ## DataLoaderはimport torch.utils.data.Datasetでimport済みのもの\n        self.train_dataloader = DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle, #https://schemer1341.hatenablog.com/entry/2019/01/06/024605 を参考. idがわからなくなる\n            num_workers=self.num_workers\n        )\n        self.val_dataloader = DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            num_workers=self.num_workers\n        )\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example\nbase = PytorchDataSet(GLDataset,tdf2[\"path\"],tdf2[\"repair_id\"],vdf2[\"path\"],vdf2[\"repair_id\"],BATCH_SIZE,NUM_WORKERS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# こうしておくと、例えば、全部の画像でやるときに便利。\n# 全部の画像でやるとき\n# base = PytorchDataSet(GLDataset,tdf[\"path\"],tdf[\"repair_id\"],vdf[\"path\"],vdf[\"repair_id\"],BATCH_SIZE,NUM_WORKERS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example\nfor a in base.train_dataloader:\n    print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 初日はここまでかな・・・"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ここからDeep learningの設定"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = resnet18(pretrained=True) #  pretrained = Trueはimagenetからpre-trainモデルを使う","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LandmarkClassifier(nn.Module): # nn.Moduleが入っているのが、モデルの定義っぽい\n    \n    def __init__(self, encoder, in_channels=3, num_classes=len(dfcnt)): \n        \n        super(LandmarkClassifier, self).__init__() # nn.Moduleの__init__を継承。https://blog.codecamp.jp/python-class-2\n        \n        self.encoder = encoder\n        \n        # Modify input layer. # 入口のチャンネル数を合わせる。defaultのResnetのclassは64 channelになっているので、それをin_channelsにする。\n        # ここの記述がencoderごとに代わるので、efficientnet使うときは変えなければいけない\n        \n        self.encoder.conv1 = nn.Conv2d(\n            in_channels,\n            64,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False\n        )\n        \n        # Modify output layer.# 出口の個数も合わせる。defaultのResnetの出口は、1000になっているので、num_classesに変更\n        # ここの記述がencoderごとに代わるので、efficientnet使うときは変えなければいけない\n        \n        self.encoder.fc = nn.Linear(512 * 1, num_classes)\n\n    def forward(self, x): # 呼び出されたときに、xの引数があると、sigmoidで返す。ex) 後ほどのclassifier(sample_batched[\"landmark\"]) みたいなところ。\n        \n        # sigmoidで返すときは以下の感じ　https://aidiary.hatenablog.com/entry/20180203/1517629555\n        # return torch.sigmoid(self.encoder(x))\n        \n        # 多値問題でも、softmaxはここでは使わない。nn.CrossEntropyに既に入っているため\n        return self.encoder(x)\n    \n    \n    \n    \n    \n    ### 以下、どこのパラメーターを最適化するか。簡易テスト用は真ん中。フルオプションは下。ここはご参考。###\n    \n    def freeze_all_layers(self):# 中間層のパラメーター(重みづけ)を全部変えない。\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n    def freeze_middle_layers(self):\n        self.freeze_all_layers()\n        \n        for param in self.encoder.conv1.parameters():# 最初のパラメーター(重みづけ)を変える。\n            param.requires_grad = True\n            \n        for param in self.encoder.fc.parameters():# 最後の(重みづけ)を変える。\n            param.requires_grad = True\n\n    def unfreeze_all_layers(self):# 中間層のパラメーター(重みづけ)を全部変える。\n        for param in self.encoder.parameters():\n            param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclassifier = LandmarkClassifier(encoder=encoder, in_channels=3, num_classes=len(dfcnt)) # classifierはDeepfakeClassifierのインスタンス化\n\n# classifier.to(device) # ここがKerasとは違うところ。GPUに送りますよーという意味。今回はcpuなので、pass.\n\nclassifier.train() # ここがKerasとは違うところ。 訓練モードの場合 classifier.train() , 推論の場合 classifier.eval() と書く。Normalizeのプロセスなどが違うらしい。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 今回は全部最適化\nclassifier.unfreeze_all_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n# 多値分類はcrossentropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()),lr=1e-5)\noptimizer = optim.Adam(classifier.parameters(),lr=1e-6)\n#optimizer = optim.SGD(classifier.parameters(),lr=1e-5)\n\n# conv層(model.features)のパラメータを固定し、全結合層(model.classifier)のみをfinetuneする設定です。\n# 全層のうち、requires_gradがTrueの層のみをfinetuneするという意味です。\n# lr : 学習率","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 効率的なやり方\n## Process 1 : Simpleにしてできるところからやる。例えば1つだけやる\n## Process 2 : いったんまとめてみる\n## Process 3 : for文にして回してみる\n## Process 4 : 汎用性を持たせる (数字 → 文字化)\n## Process 5 : さらに汎用性を持たせる (関数化)\n## Process 6 : class化する"},{"metadata":{},"cell_type":"markdown","source":"# モデルの最適化は複雑なので、1epochごとに、1つ1つ見ていく\n# まずはtrain"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 読めないので、以下で確認。\nfor a in train_dataloader:\n    print(a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in train_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    print(y_pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_pred[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lossを算出 (crossentropyloss)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in train_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    label = a[\"label\"]\n    \n    loss = criterion(y_pred, label)\n    print(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in train_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    label = a[\"label\"]\n    \n    loss = criterion(y_pred, label)\n    print(loss.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in train_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    label = a[\"label\"]\n    \n    loss = criterion(y_pred, label)\n    print(loss.item())\n    \n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad() # おきまり。初期化。\n    loss.backward() # 後方にバック\n    optimizer.step() # 重みづけの最適化","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# このあと、epochごとにまとめることを考えると、loss.itemの一番最後を保存。"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.train()\ntrainloss = []\n\nfor a in train_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    \n    \n    label2 = a[\"label\"]\n    \n    loss = criterion(y_pred, label2)\n    print(loss.item()) # itemは0次元tensorから整数を出す場合。\n    \n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad() # おきまり。初期化。\n    loss.backward() # 後方にバック\n    optimizer.step() # 重みづけの最適化\n    \ntrainloss.append(loss.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 関数化"},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainmodel(train_dataloader):\n    classifier.train()\n    \n    for a in train_dataloader:\n        y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n        label = a[\"label\"]\n\n        loss = criterion(y_pred, label)\n\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad() # おきまり。初期化。\n        loss.backward() # 後方にバック\n        optimizer.step() # 重みづけの最適化\n        \n    return(loss.item())\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# validationを同様に作る。"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.eval() # ここが推論なので、変えなきゃダメ。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.eval() # ここが推論なので、変えなきゃダメ。\nval_loss = []\n\nfor a in val_dataloader:\n    y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n    \n   \n    label = a[\"label\"]\n    \n    lossval = criterion(y_pred, label)\n    print(lossval.item()) # itemは0次元tensorから整数を出す場合。\n\nval_loss.append(lossval.item())\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 関数化"},{"metadata":{"trusted":true},"cell_type":"code","source":"def valmodel(val_dataloader):\n    \n    classifier.eval()\n    \n    for a in val_dataloader:\n\n        y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n        label = a[\"label\"]\n\n        loss = criterion(y_pred, label)\n        \n    return(loss.item())\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# くっつけてepochを回す"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\ntrainloss = []\nvalloss = []\n\n\nfor epoch in range(epochs):\n    \n    tloss_tmp =[]\n    vloss_tmp =[]\n\n    \n    ####### train #######\n    classifier.train()\n    \n\n    for a in train_dataloader:\n        y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n        \n        # y_pred = torch.sigmoid(y_pred) 本来違うのだけどなぜかこれを入れた方がスコアが良い\n\n        y_pred2 = y_pred.squeeze(dim=-1)\n        label2 = a[\"label\"].squeeze(dim=-1)\n\n        loss = criterion(y_pred2, label2)\n        tloss_tmp.append(loss.item())\n        # print(loss.item()) # itemは0次元tensorから整数を出す場合。\n\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad() # おきまり。初期化。\n        loss.backward() # 後方にバック\n        optimizer.step() # 重みづけの最適化\n\n    trainloss.append(tloss_tmp[-1])\n    \n    #######validation#######\n    \n    classifier.eval() # ここが推論なので、変えなきゃダメ。\n    \n\n    for a in val_dataloader:\n        y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n        \n        # y_pred = torch.sigmoid(y_pred) 本来違うのだけどなぜかこれを入れた方がスコアが良い\n\n\n        y_pred2 = y_pred.squeeze(dim=-1)\n        label2 = a[\"label\"].squeeze(dim=-1)\n\n        lossval = criterion(y_pred2, label2)\n        vloss_tmp.append(lossval.item())\n        # print(lossval.item()) # itemは0次元tensorから整数を出す場合。\n\n    valloss.append(vloss_tmp[-1])\n    \n    print(str(epoch) + \"_end\")\n\n    \n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 関数化した版"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloss = []\nvalloss = []\n\n\nfor epoch in range(epochs):\n    \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))\n    \n    print(str(epoch) + \"_end\")\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(epochs)\nplt.scatter(x,trainloss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(x,valloss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# validationlossが下がったら、モデルをsaveする記述を追加。"},{"metadata":{"trusted":true},"cell_type":"code","source":"savename=\"resnet18.pth\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloss = []\nvalloss = []\n\nbestloss = None\n\nfor epoch in range(epochs):\n    \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))\n    \n    print(str(epoch) + \"_end\")\n    \n    #######modelをsave#######\n    \n    if bestloss is None:\n        bestloss = valloss[-1]\n        state = {\n                'state_dict': classifier.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n                \"bestloss\":bestloss,\n            }\n\n        torch.save(state, savename)\n        \n        print(\"save the first model\")\n    \n    elif valloss[-1] < bestloss:\n        \n        bestloss = valloss[-1]\n        state = {\n                'state_dict': classifier.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n            \"bestloss\":bestloss,\n            }\n\n        torch.save(state, savename)\n        \n        print(\"found a better point\")\n    \n    else:\n        pass\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 関数化"},{"metadata":{"trusted":true},"cell_type":"code","source":"def savemodel(bestloss,valloss):\n    \n    #######modelをsave#######\n    \n    if bestloss is None:\n        bestloss = valloss[-1]\n        state = {\n                'state_dict': classifier.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n                \"bestloss\":bestloss,\n            }\n\n        torch.save(state, savename)\n        \n        print(\"save the first model\")\n    \n    elif valloss[-1] < bestloss:\n        \n        bestloss = valloss[-1]\n        state = {\n                'state_dict': classifier.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n                \"bestloss\":bestloss,\n            }\n\n        torch.save(state, savename)\n        \n        print(\"found a better point\")\n    \n    else:\n        pass\n    \n    return bestloss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloss = []\nvalloss = []\n\nbestloss = None\n\nfor epoch in range(epochs):\n    \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))\n    \n    print(str(epoch) + \"_end\")\n    \n    bestloss= savemodel(bestloss,valloss)\n    \n    print(bestloss)\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x,trainloss)\nplt.scatter(x,valloss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference : ここまでをclass化してすっきりする→汎用性をもたせる"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MakingModel():\n    \n    def __init__(self,train_dataloader,val_dataloader,epochs,savename,bestloss=None):\n        \n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.epochs = epochs\n        \n        if bestloss is None:\n            self.bestloss = bestloss\n        else:\n            self.bestloss = None\n            \n        self.savename = savename\n\n        self.trainloss=[]\n        self.valloss=[]\n        \n        self.makemodel()\n        \n    \n    \n    def trainmodel(self):\n        \n        classifier.train()\n\n        for a in self.train_dataloader:\n            y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n            label = a[\"label\"]\n\n            loss = criterion(y_pred, label)\n\n            # Zero gradients, perform a backward pass, and update the weights.\n            optimizer.zero_grad() # おきまり。初期化。\n            loss.backward() # 後方にバック\n            optimizer.step() # 重みづけの最適化\n\n        return(loss.item())\n    \n    \n    def valmodel(self):\n        classifier.eval()\n\n        for a in self.val_dataloader:\n            y_pred = classifier(a['landmarks']) # onenoteでいうoutput = model(train_x)\n            label = a[\"label\"]\n\n            loss = criterion(y_pred, label)\n\n\n        return(loss.item())\n    \n    \n    \n    def savemodel(self):\n    \n        #######modelをsave#######\n\n        if self.bestloss is None:\n            self.bestloss = self.valloss[-1]\n            state = {\n                    'state_dict': classifier.state_dict(),\n                    'optimizer_dict': optimizer.state_dict(),\n                    \"bestloss\":self.bestloss,\n                }\n\n            torch.save(state, self.savename)\n\n            print(\"save the first model\")\n\n        elif self.valloss[-1] < self.bestloss:\n\n            self.bestloss = self.valloss[-1]\n            state = {\n                    'state_dict': classifier.state_dict(),\n                    'optimizer_dict': optimizer.state_dict(),\n                    \"bestloss\":self.bestloss,\n                }\n\n            torch.save(state, self.savename)\n\n            print(\"found a better point\")\n\n        else:\n            pass\n\n        return self.bestloss\n    \n    \n    def makemodel(self):\n\n        for epoch in range(self.epochs):\n\n            self.trainloss.append(self.trainmodel())\n            self.valloss.append(self.valmodel())\n\n            print(str(epoch) + \"_end\")\n\n            self.bestloss= self.savemodel()\n\n            print(self.bestloss)\n\n\n\n\n    \n    \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = MakingModel(train_dataloader,val_dataloader,epochs,\"test.pth\",bestloss=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(tmp.epochs)\nplt.scatter(x,tmp.trainloss)\nplt.scatter(x,tmp.valloss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp.savename","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ここからは推論"},{"metadata":{},"cell_type":"markdown","source":"# 基本は、model作成と同じだが、ラベルがないので、それようのclassを作る"},{"metadata":{},"cell_type":"markdown","source":"testのpassを出す"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = []\n\nimport os\nfor dirname, _, filenames in os.walk('../input/landmark-recognition-2020/test'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        test_path.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testid = [s.split(\"/\")[-1] for s in test_path]\ntestid = [s.split(\".\")[0] for s in testid]\ntestid[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf = pd.DataFrame()\ntestdf[\"id\"] = testid\ntestdf[\"path\"] = test_path\ntestdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GLDataset_inf(Dataset):\n    \n    def __init__(self,testid,img_pass,preprocess=None):\n        self.testid = testid\n        self.img_pass = img_pass\n        self.preprocess = preprocess\n        \n    def __len__(self):\n        return len(self.img_pass)\n    \n    def __getitem__(self,idx):\n        \n        # ここからdatasetに食わせる前の前処理の記述。\n        \n        img_id = self.testid[idx]\n        \n        img_pass = self.img_pass[idx]\n        \n        land = cv2.imread(img_pass)\n        land = cv2.resize(land,(ROWS,COLS),interpolation = cv2.INTER_CUBIC)\n        land = cv2.cvtColor(land,cv2.COLOR_BGR2RGB) # augmentを使うときにBGRからRGBにする必要があるのかもしれない。\n        \n        if self.preprocess is not None: # ここで、前処理を入れてnormalizationしている。\n                augmented = self.preprocess(image=land) # preprocessのimageをfaceで読む\n                land = augmented['image'] # https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/albumentations%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81/　に書いてある\n                \n        return {\"id\":img_id,'landmarks': land.transpose(2, 0, 1)}  # pytorchはchannnl, x, yの形。これは辞書型で返している。(扱いやすいというだけかも。)\n        \n        \n        \n        \n        \n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf2 = testdf.iloc[:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# インスタンス化(とりあえず10枚)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = GLDataset_inf(\n    testdf2[\"id\"],\n    testdf2[\"path\"],\n    preprocess=preprocess\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader化"},{"metadata":{"trusted":true},"cell_type":"code","source":"## DataLoaderはimport torch.utils.data.Datasetでimport済みのもの\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False, #https://schemer1341.hatenablog.com/entry/2019/01/06/024605 を参考. idがわからなくなる\n    num_workers=0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# torchモデルのload ※ おそらく必要ないが、notebookわけたときように。"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = \"resnet18.pth\"\n\nencoder = resnet18(pretrained=False) # 学習済みモデルをインターネットからひっぱるわけじゃないので、false\nclassifier = LandmarkClassifier(encoder=encoder, in_channels=3, num_classes=len(dfcnt)) # classifierはDeepfakeClassifierのインスタンス化\n\n# classifier.to(device) # ここがKerasとは違うところ。GPUに送りますよーという意味\n\nstate = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\nclassifier.load_state_dict(state['state_dict'])\n\nclassifier.eval() # 推論なので、eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"presub = []\n\nfor a in tqdm(test_dataloader):\n    y_pred = classifier(a['landmarks'])\n    cnt = F.softmax(y_pred).cpu().detach().numpy()\n    \n    y_pred = y_pred.cpu().detach().numpy()\n    \n    \n    for b in range(len(y_pred)):\n        tmp = np.argmax(y_pred[b])\n        conf = cnt[b][tmp]\n        \n        presub.append([a[\"id\"][b],tmp,conf])\n    \n    \n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"presubdf = pd.DataFrame(presub,columns=[\"id\",\"landid\",\"conf\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"presubdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#全テストデータで実行"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = GLDataset_inf(\n    testdf[\"id\"],\n    testdf[\"path\"],\n    preprocess=preprocess\n)\n\n## DataLoaderはimport torch.utils.data.Datasetでimport済みのもの\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False, #https://schemer1341.hatenablog.com/entry/2019/01/06/024605 を参考. idがわからなくなる\n    num_workers=NUM_WORKERS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"presub = []\n\nfor a in tqdm(test_dataloader):\n    y_pred = classifier(a['landmarks'])\n    cnt = F.softmax(y_pred).cpu().detach().numpy()\n    \n    y_pred = y_pred.cpu().detach().numpy()\n    \n    \n    for b in range(len(y_pred)):\n        tmp = np.argmax(y_pred[b])\n        conf = cnt[b][tmp]\n        \n        presub.append([a[\"id\"][b],tmp,conf])\n    \n\n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"presubdf = pd.DataFrame(presub,columns=[\"testid\",\"repair_id\",\"conf\"])\npresubdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# repairidの補正"},{"metadata":{"trusted":true},"cell_type":"code","source":"mdf = pd.merge(presubdf,tdf,on=\"repair_id\",how=\"left\")\nmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = []\n\nfor a in range(len(mdf)):\n    \n    tmp = str(mdf[\"landmark_id\"].iloc[a]) + \" \" + str(mdf[\"conf\"].iloc[a])\n    sub.append(tmp)\n\nmdf[\"submission\"]=sub\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissiondf = mdf[[\"testid\",\"submission\"]]\nsubmissiondf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissiondf.columns=[\"id\",\"landmarks_pytorch\"]\nsubmissiondf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample submissionの形式に。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/landmark-recognition-2020/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.merge(sample,submissiondf,on=\"id\",how=\"left\")\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"landmarks\"] = submission[\"landmarks_pytorch\"]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.iloc[:,:2]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# memo : メモリが足りなくて、全部学習できない。\n# memo2 : 学習済みresnet 18はinternet onだと実行できない。→　やり方はあるが、\n# モデル構築と、推論はnotebookを分けてもよい。"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}