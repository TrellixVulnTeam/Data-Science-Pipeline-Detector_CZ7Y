{"cells":[{"metadata":{"_uuid":"38a80fc1-6e46-4636-95c9-5f39e4ca0ac6","_cell_guid":"cec45659-28a4-4f1a-a61d-816c81520426","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport matplotlib.image as mpimg\nimport cv2\nimport seaborn as sn\nimport torch\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"615816fb-bfcb-4b1b-bd69-79fbd2255f4e","_cell_guid":"80eac2f1-7c04-4fe5-b46d-7d29472a3d92","trusted":true},"cell_type":"code","source":"directory = '../input/landmark-recognition-2020/'\ntrain_dir = '../input/landmark-recognition-2020/train/*/*/*/*'\ntest_dir = '../input/landmark-recognition-2020/test/*/*/*/*'\noutput_dir ='../output/kaggle/working/'\nimage_dir_train='../input/landmark-recognition-2020/train/'\nimage_dir_test='../input/landmark-recognition-2020/test/'\nos.listdir(directory)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c75ffd7-5e29-465a-bb3a-764ae9a4e20a","_cell_guid":"a888e8d0-70c3-487d-a91f-2f8eaba28236","trusted":true},"cell_type":"code","source":"test = pd.read_csv(os.path.join(directory,'sample_submission.csv'))\ntest['image_']=test.id.str[0]+\"/\"+test.id.str[1]+\"/\"+test.id.str[2]+\"/\"+test.id+\".jpg\"\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c1c4ec-0940-47fb-838a-f9e996c8502d","_cell_guid":"bf1b2d05-eed4-405d-8d7f-c67f6b376f9a","trusted":true},"cell_type":"code","source":"# train_images = glob.glob(train_dir)\n# test_images = glob.glob(test_dir)\n# print('Training images : ',len(train_images))\n# print('Testing images : ',len(test_images))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a103f094-9610-4bad-b8b4-23476b70e738","_cell_guid":"51c49bd3-99df-4000-957d-0f3d95557749","trusted":true},"cell_type":"code","source":"# assert test.shape[0]==len(test_images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70a58328-f68b-46a8-aa52-9587de150d7e","_cell_guid":"46f501c6-5f4d-4f74-9efa-ef85519415ac","trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(directory,'train.csv'))\ntrain[\"image_\"] = train.id.str[0]+\"/\"+train.id.str[1]+\"/\"+train.id.str[2]+\"/\"+train.id+\".jpg\"\ntrain[\"target_\"] = train.landmark_id.astype(str)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Threshold_count = 150\n\nvalid_landmark_df = pd.DataFrame(train['landmark_id'].value_counts()).reset_index()\nvalid_landmark_df.columns =  ['landmark_id', 'count_']\nlist_valid_landmarks = list(valid_landmark_df[valid_landmark_df.count_ >= Threshold_count]['landmark_id'].unique())\n\n#or\n# y = train.landmark_id.values\n# valid_landmark_count = Counter(y).most_common(1000)\n# print(valid_landmark_count[:10])\n# list_valid_landmarks = [landmark[0] for landmark in valid_landmark_count]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain= train[train.landmark_id.isin(list_valid_landmarks)]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"984f6a07-56d2-4f82-b9f1-a78a4f9042fd","_cell_guid":"ee549b2e-6653-45b2-8e7f-bf57a6842f22","trusted":true},"cell_type":"markdown","source":"# **Visualize train and test images**","execution_count":null},{"metadata":{"_uuid":"f60e1bd4-e3b4-4c84-931b-1f4d565de4a8","_cell_guid":"acbd0939-3ce0-48ef-8a4b-4d57a016f9d4","trusted":true},"cell_type":"code","source":"# for img in range(2):\n#     image = mpimg.imread(train_images[img])\n#     print(image.shape)\n#     plt.imshow(image)\n#     plt.axis('Off')\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99555618-463b-46ac-97e0-ae81afa2af87","_cell_guid":"aff95257-4b02-4465-9e05-b2cb50bca0c5","trusted":true},"cell_type":"code","source":"# for img in range(2):\n#     image = cv2.imread(test_images[img])\n#     print(image.shape)\n#     plt.imshow(image)\n#     plt.axis('Off')\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec213e09-79a0-41d0-aa72-4fe21b7eed96","_cell_guid":"108f55a1-e4fc-4e00-b955-5be460e15c76","trusted":true},"cell_type":"markdown","source":"### create DataSet","execution_count":null},{"metadata":{"_uuid":"7d3c8818-070b-43ea-a377-56b993d02378","_cell_guid":"aa37b0b8-b6de-4ecf-9dc1-682f97db29a0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision.transforms import transforms\n\nTRAIN_BS = 32\nTEST_BS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [ s for s in train_images if '.jpg' not in s]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [ s for s in test_images if '.jpg' not in s]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class createDataset(Dataset):\n    def __init__(self, transform, image_dir, df, train_type = True):        \n        self.df = df \n        self.image_dir = image_dir    \n        self.transform = transform\n        self.train_type=train_type\n        \n    def __len__(self):\n        return self.df.shape[0] \n    \n    def __getitem__(self,idx):\n        image_id = self.df.iloc[idx].id\n        image_name = f\"{self.image_dir}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg\"\n        self.image = Image.open(image_name).convert('RGB')               \n        self.image = self.transform(self.image)\n#         print(self.image)\n        \n#         self.Y = self.df.iloc[idx].landmark_id\n        self.Y = torch.Tensor([self.df.iloc[idx].landmark_id]).type(torch.LongTensor)        \n        if(self.train_type):\n            return {'image':self.image, \n                    'label':self.df.iloc[idx].landmark_id}\n        else:\n            return {'image':self.image}         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# idx=10\n\n# # test_data = createDataset(transform = transformations , df = test , image_dir = image_dir_test , train = False )\n\n# image_id = train.iloc[idx].id\n# image_name = f\"{image_dir_train}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg\"\n# image = Image.open(image_name)                 \n# # image = image)\n# print(image)\n\n# #         Y = self.df.iloc[idx].landmark_id\n# Y = torch.Tensor([train.iloc[idx].landmark_id]).type(torch.LongTensor)\n# image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = (0.485, 0.456, 0.406)\nstd =  (0.229,0.225,0.224)\ntransformations = transforms.Compose([ transforms.Resize((64,64)),\n#                                     transforms.Resize((128,128),interpolation=Image.NEAREST),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean,std)\n                                     ]\n                                    )                               \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\nle = label_encoder.fit(train.landmark_id.values)\nunique_classes = len(le.classes_)\nprint('Total number of classes ', unique_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = createDataset(transform = transformations , df = train , image_dir = image_dir_train , train_type = True )\ntrain_loader = DataLoader(dataset = train_data, batch_size = TRAIN_BS, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = createDataset(transform = transformations , df = test , image_dir = image_dir_test , train_type = False )\ntest_loader = DataLoader(dataset = test_data, batch_size = TEST_BS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch\n# import efficientnet_pytorch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class EfficientNetEncoderHead(nn.Module):\n#     def __init__(self, depth, num_classes):\n#         super(EfficientNetEncoderHead, self).__init__()\n#         self.depth = depth\n#         self.base = efficientnet_pytorch.EfficientNet.from_pretrained(f'efficientnet-b{self.depth}')\n#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n#         self.output_filter = self.base._fc.in_features\n#         self.classifier = nn.Linear(self.output_filter, num_classes)\n#     def forward(self, x):\n#         x = self.base.extract_features(x)\n#         x = self.avg_pool(x).squeeze(-1).squeeze(-1)\n#         x = self.classifier(x)\n#         return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = EfficientNetEncoderHead(depth=0, num_classes=unique_classes)\n# model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport torch.nn as nn\nimport torch\n# model = torchvision.models.resnet50(pretrained=True)\n\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_name('efficientnet-b1')\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfor param in model.parameters():\n    param.requires_grad = False\n    \n\nmodel._fc = nn.Linear(model._fc.in_features, unique_classes)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 1\nloss_list = []\nactivation = nn.Softmax(dim=1)\nfor epochs in range(n_epochs):    \n    for i, data_x_y in enumerate(tqdm(train_loader)):\n#         model.train()\n        x= data_x_y['image']\n        y=data_x_y['label']        \n        optimizer.zero_grad()\n        yhat =  model(x.cuda())\n        loss = criterion(yhat, y.cuda())\n        loss.backward()\n        optimizer.step()\n        loss_list.append(loss.item())\n    print('Epoch ', epochs, 'loss : ',loss.item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Someone help me resolving this issue","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for x_test, y_test in test_loader:        \n    x_test = x_test.to('cuda')\n    y_test=y_test.to('cuda')\n\n    model.eval()\n    z = model(x_test)\n    score, label = torch.max(z,1)\n    correct += (label == y_test).sum().item()\n    test_list.append(y_test)\n    lables_list(label)\n    scores_list.append(score)        \naccuracy = accuracy/ n_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}