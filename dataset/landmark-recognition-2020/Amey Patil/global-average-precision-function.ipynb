{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/landmark-recognition-2020/train.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.unique(np.array(df['landmark_id'], dtype=int))\nprint(labels.size)\nnp.sort(labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"Global Average Precision for Google Landmark Recognition 2020\nhttps://www.kaggle.com/c/landmark-recognition-2020/overview\"\"\"\n\"\"\"\nThanks to https://github.com/yisaienkov/evaluations/blob/master/evaluations/kaggle_2020/global_average_precision.py\n\"\"\"\n\nfrom typing import Dict, Tuple, Any\n\n\ndef global_average_precision_score(\n        y_true: Dict[Any, Any],\n        y_pred: Dict[Any, Tuple[Any, float]]\n) -> float:\n    \"\"\"\n    Compute Global Average Precision score (GAP)\n    Parameters\n    ----------\n    y_true : Dict[Any, Any]\n        Dictionary with query ids and true ids for query samples\n    y_pred : Dict[Any, Tuple[Any, float]]\n        Dictionary with query ids and predictions (predicted id, confidence\n        level)\n    Returns\n    -------\n    float\n        GAP score\n    Examples\n    --------\n    >>> from evaluations.kaggle_2020 import global_average_precision_score\n    >>> y_true = {\n    ...         'id_001': 123,\n    ...         'id_002': None,\n    ...         'id_003': 999,\n    ...         'id_004': 123,\n    ...         'id_005': 999,\n    ...         'id_006': 888,\n    ...         'id_007': 666,\n    ...         'id_008': 666,\n    ...         'id_009': None,\n    ...         'id_010': 666,\n    ...     }\n    >>> y_pred = {\n    ...         'id_001': (123, 0.15),\n    ...         'id_002': (123, 0.10),\n    ...         'id_003': (999, 0.30),\n    ...         'id_005': (999, 0.40),\n    ...         'id_007': (555, 0.60),\n    ...         'id_008': (666, 0.70),\n    ...         'id_010': (666, 0.99),\n    ...     }\n    >>> global_average_precision_score(y_true, y_pred)\n    0.5479166666666666\n    \"\"\"\n    indexes = list(y_pred.keys())\n    indexes.sort(\n        key=lambda x: -y_pred[x][1],\n    )\n    queries_with_target = len([i for i in y_true.values() if i is not None])\n    correct_predictions = 0\n    total_score = 0.\n    for i, k in tqdm(enumerate(indexes, 1)):\n        relevance_of_prediction_i = 0\n        if y_true[k] == y_pred[k][0]:\n            correct_predictions += 1\n            relevance_of_prediction_i = 1\n        precision_at_rank_i = correct_predictions / i\n        total_score += precision_at_rank_i * relevance_of_prediction_i\n\n    return 1 / queries_with_target * total_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate some random predictions on 3 classes\nnp.random.seed(2020)\nypred = np.random.choice([1,2,3], 10)\nytrue = np.random.choice([1,2,3], 10)\nconf = np.random.random(10)\n\n\nglobal_average_precision_score(y_true = {str(idx): ytrue[idx] for idx in range(10)}, y_pred = {str(idx): (ypred[idx], conf[idx]) for idx in range(10)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = {df['id'].iloc[idx]: df['landmark_id'].iloc[idx] for idx in tqdm(range(len(df)))}\ndummy_labels = {df['id'].iloc[idx]: (np.random.choice(labels, 1), np.random.random(1)) for idx in tqdm(range(len(df)))}\nglobal_average_precision_score(train_labels, dummy_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}