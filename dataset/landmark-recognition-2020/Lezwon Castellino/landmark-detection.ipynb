{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# !pip install colabcode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from colabcode import ColabCode\n# ColabCode()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/landmark-recognition-2020/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n\ntrain_files = Path('../input/landmark-recognition-2020/train').glob('**/*.jpg')\ntest_files = Path('../input/landmark-recognition-2020/test').glob('**/*.jpg')\n# test_files = glob.glob('../input/landmark-recognition-2020/test/*/*/*/*.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create folds\nfrom sklearn import model_selection\n\ndf[\"kfold\"] = -1    \ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.landmark_id.values\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[v_, 'kfold'] = f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall typing -y\n!pip install -qU git+https://github.com/PyTorchLightning/pytorch-lightning.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchtoolbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport cv2\nimport albumentations\nimport torch\nimport numpy as np\nimport io\nfrom torch.utils.data import Dataset\nfrom torchtoolbox.transform import Cutout\n\n# Making the dataset class for training and testing Flower images\n\nclass Landmark_detection_Dataset(Dataset):\n    def __init__(self, id , classes , image , img_height , img_width, mean , std , is_valid):\n        self.id = id\n        self.classes = classes\n        self.image = image\n        self.is_valid = is_valid\n        if self.is_valid == 1: # transforms for validation images\n            self.aug = albumentations.Compose([\n               albumentations.Resize(img_height , img_width, always_apply = True) ,\n               albumentations.Normalize(mean , std , always_apply = True) \n            ])\n        else:                  # transfoms for training images \n            self.aug = albumentations.Compose([\n                albumentations.Resize(img_height , img_width, always_apply = True) ,\n                albumentations.Normalize(mean , std , always_apply = True),\n                albumentations.ShiftScaleRotate(shift_limit = 0.0625,\n                                                scale_limit = 0.1 ,\n                                                rotate_limit = 5,\n                                                p = 0.9)\n            ]) \n        \n    def __len__(self):\n        return len(self.id)\n    \n    def __getitem__(self, index):\n        id = self.id[index]\n        \n        # converting jpg format of images to numpy array\n        img = np.array(Image.open('../input/landmark-recognition-2020/train/'+\n                                  self.image[index][0]+'/'+\n                                  self.image[index][1]+'/'+\n                                  self.image[index][2]+'/'+\n                                  self.image[index]+'.jpg')) \n        \n        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n        img = self.aug(image = img)['image']\n        img = np.transpose(img , (2,0,1)).astype(np.float32) # 2,0,1 because pytorch excepts image channel first then dimension of image\n       \n        return torch.tensor(img, dtype = torch.float),torch.tensor(self.classes[index], dtype = torch.long)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = 0\ndf_train = df[df.kfold != fold].reset_index(drop=True)\ndf_valid = df[df.kfold == fold].reset_index(drop=True)\n\n# prepare transforms standard to MNIST\ntrain_data = Landmark_detection_Dataset(id = [i for i in range(len(df_train))], \n                                         classes = df_train['landmark_id'], \n                                         image = df_train['id'], \n                                         img_height = 224 , img_width = 224, \n                                         mean = (0.485, 0.456, 0.406),\n                                         std = (0.229, 0.224, 0.225) , is_valid = 0)\n\nval_data = Landmark_detection_Dataset(id = [i for i in range(len(df_valid))], \n                                       classes = df_valid['landmark_id'], \n                                       image = df_valid['id'], \n                                       img_height = 224 , img_width = 224, \n                                       mean = (0.485, 0.456, 0.406),\n                                       std = (0.229, 0.224, 0.225) , is_valid = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nidx = 1000 # taking index for 10000th image out of 51000 images\nimg = val_data[idx][0]\n\nprint(val_data[idx][1]) # val_dataset label is one Hot encoded\n\nnpimg = img.numpy()\nplt.imshow(np.transpose(npimg, (1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_data, batch_size=16, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=16, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels\nimport pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn import functional as F\nfrom torchvision.datasets import MNIST\nfrom torchvision import datasets, transforms\n\nno_of_outputs_classes_for_our_dataset = len(df['landmark_id'].unique())\n\nclass ResNet34(pl.LightningModule):\n    \n    def __init__(self):\n        super(ResNet34, self).__init__()\n        self.model =  pretrainedmodels.__dict__['resnet34'](pretrained='imagenet')\n\n        self.last_dense_layer = torch.nn.Linear(self.model.last_linear.in_features, no_of_outputs_classes_for_our_dataset)\n        \n        self.loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        batch_size ,_,_,_ = x.shape     #taking out batch_size from input image\n        x = self.model.features(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)     # then reshaping the batch_size\n        x = self.last_dense_layer(x)\n        return x\n    \n    \n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        predictions = self.forward(x)\n        loss = self.loss(predictions, y)\n\n        logs = {'train_loss': loss}\n        return {'loss': loss, 'log': logs}\n    \n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n        predictions = self.forward(x)\n        loss = self.loss(predictions, y)\n        return {'val_loss': loss}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        return {'val_loss': avg_loss}\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.last_dense_layer.parameters(), lr=1e-3)\n        return optimizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_lightning import Trainer, seed_everything\nseed_everything(0)\n\nmodel = ResNet34()                 \n\ntrainer = Trainer(tpu_cores=8, progress_bar_refresh_rate=20, max_epochs=10)\ntrainer.fit(model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchcontrib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for Stochastic Weight Averaging in PyTorch\nfrom torchcontrib.optim import SWA\n\nbase_optimizer = torch.optim.Adam(model.last_dense_layer.parameters(), lr=1e-4)\n\noptimizer = SWA(base_optimizer, swa_start=5, swa_freq=5, swa_lr=0.05)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}