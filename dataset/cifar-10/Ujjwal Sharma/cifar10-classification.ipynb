{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport keras\nimport keras as ks\nimport matplotlib\nfrom matplotlib import pyplot\nfrom keras.datasets import cifar10\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train,Y_train),(X_test,Y_test) = cifar10.load_data()\n\n# Ananlyzing the shape of the data(Should be the first step of training)\nprint(\"shape of X_train and Y_train \"+ str(X_train.shape)+\" \"+str(Y_train.shape))\nprint(\"shape of X_test and Y_test \"+ str(X_test.shape)+\" \"+str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the first 5 images and their classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(X_train[i])\n    print(Y_train[i])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function 1 : load() --> X_train,Y_train,X_test,Y_test\n### As this is Categorical Data, we use one_hot_encoding on it\nThis creates multiple columns of different categories (As this data is in 0-9 form). So, this is performed on Y_test and Y_train.<br> This is done using to_categorical of keras.utils<br> So, finally load function to load and encode data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load():\n    (X_train,Y_train),(X_test,Y_test) = cifar10.load_data()\n    Y_train=ks.utils.to_categorical(Y_train)\n    Y_test=ks.utils.to_categorical(Y_test)\n    return X_train,Y_train,X_test,Y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function 2: normalize(train,test) --> trainNorm,testNorm\n### Normalizing Images\nWe need the data to be scaled properly (0-255). So, we normalize it from 0-1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(train,test):\n    trainNorm=train/255.0\n    testNorm=test/255.0\n    return trainNorm,testNorm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function 3: model()--> model\nWithout Dropout => 70%<br>\nNow we define model and change it as per our needs<br>\nAdded Dropout!<br>\nAdded Adam Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def getmodel():\n    # Adding sequential to add layers sequentially as per our needs.\n    model = ks.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    \n    opt = Adam(learning_rate=0.01,name=\"Adam\")\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we fit our model and code for visualizing it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_diagnostics(history):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainModel():\n    X_train,Y_train,X_test,Y_test=load()\n    X_train,X_test=normalize(X_train,X_test)\n    model=getmodel()\n    history=model.fit(X_train,Y_train,epochs=100,batch_size=64, validation_data=(X_test, Y_test), verbose=0)\n    summarize_diagnostics(history)\n    _, acc = model.evaluate(X_test, Y_test, verbose=0)\n    print('> %.3f' % (acc * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}