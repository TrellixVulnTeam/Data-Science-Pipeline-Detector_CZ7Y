{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading in the train_df\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\ntrain_df[train_df.Id == 'de4c8260-cd87-431c-b425-70c9c44596d0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#filenames of all the training json files \nfnames = os.listdir('../input/coleridgeinitiative-show-us-the-data/train')\nlen(fnames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmydataframe = pd.DataFrame()\n\nparagraph_num = []\n\nfor file in tqdm(fnames):\n    #print('-'*52)\n    paper_path = os.path.join('../input/coleridgeinitiative-show-us-the-data/train', file)\n    \n    paper = pd.read_json(paper_path)\n    paper_id = file.replace('.json','')\n    \n    find = train_df[train_df.Id == paper_id].dataset_label.unique()\n   \n    section_count = 0 \n    small = []\n    #print(paper)\n    for text in paper.iterrows(): #each section\n        section_count += 1\n        text_df = text[1]\n        for find_label in find:\n            if find_label.lower() in text_df.text.lower():\n                for sent in text_df.text.split('.'): #for each sentence in section\n                    if find_label.lower() in sent.lower():\n                        mydataframe = mydataframe.append({'Id':paper_id,'label_found':find_label.lower(),'section_title':text_df.section_title,'section_num':section_count,'total_section':len(paper),'sentence':sent,}, ignore_index=True)\n                small.append(text_df.text[0])\n    if small == []:#if i havnt found any find labels in the whole paper\n        mydataframe = mydataframe.append({'Id':paper_id,'label_found':find_label.lower(),'section_title':'NONE','section_num':'NONE','total_section':len(paper),'sentence':'NONE',}, ignore_index=True)\n \n    paragraph_num.append(small)\n    \nmydataframe\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#how many papers did i not find the label in the text\ncount = 0 \nfor i,v in enumerate(paragraph_num):\n    if v == []:\n        count += 1\n        \ncount","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removes the label from the sentence \nc_sent = []\nfor row in mydataframe.iterrows():\n    temp = row[1].sentence.lower()\n    for word in row[1].label_found.split(' '):\n        temp = temp.replace(word,'')\n    c_sent.append(temp)\n    \n    \nmydataframe['clean_sentence'] = c_sent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_files_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_files_path = '../input/coleridgeinitiative-show-us-the-data/test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\ndef text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = text.replace('-',' ')\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport json\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.cleaned_label.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mydataframe[mydataframe.section_num == 'NONE'].Id.unique()\ntrain_df[train_df.Id == 'de4c8260-cd87-431c-b425-70c9c44596d0']\nempty = mydataframe[mydataframe.section_num == 'NONE'].Id.unique()\nl = train_df.Id.apply(lambda x: x in empty)\ndidnt_find_df = train_df[l]#.groupby('cleaned_label').count()\ndidnt_find_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = 0\nfor i in range(len(didnt_find_df)):\n    if didnt_find_df.iloc[i].cleaned_label in didnt_find_df.iloc[i].text:\n        train_df[train_df.Id == didnt_find_df.iloc[i].Id]\n        paper_path = os.path.join('../input/coleridgeinitiative-show-us-the-data/train', str(didnt_find_df.iloc[i].Id)+'.json')\n        paper = pd.read_json(paper_path)\n        #print(len(paper))\n        sent\n        c += 1\n    else:\n        pass\n        #print(didnt_find_df.iloc[i].Id)\n        #print(train_df[train_df.Id == didnt_find_df.iloc[i].Id])\n        #print(list(train_df[train_df.Id == didnt_find_df.iloc[i].Id].text))\n        #break\n #       print({'Id':didnt_find_df.iloc[i].Id,'label_found':didnt_find_df.iloc[i].cleaned_label,'section_title':text_df.section_title,'section_num':section_count,'total_section':len(paper),'sentence':sent})#, ignore_index=True)\n        #mydataframe\nc\n#NOAA water-level station","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nmydataframe = pd.DataFrame()\n\npaper_datasets = []\nfnames = os.listdir('../input/coleridgeinitiative-show-us-the-data/test')\nfor file in tqdm(fnames):\n    #print('-'*52)\n    paper_path = os.path.join('../input/coleridgeinitiative-show-us-the-data/test', file)\n    \n    paper = pd.read_json(paper_path)\n    paper_id = file.replace('.json','')\n    \n    find = train_df.dataset_label.unique()\n   \n    section_count = 0 \n    small = []\n    #print(paper)\n    for text in paper.iterrows(): #each section\n        section_count += 1\n        text_df = text[1]\n        for find_label in find:\n            if find_label.lower() in text_df.text.lower():\n                small.append(find_label.lower())\n    if small == []:#if i havnt found any find labels in the whole paper\n        small.append('data set')\n    else:\n        small = \" | \".join(set(small))\n \n    paper_datasets.append({'Id':paper_id, 'PredictionString':small})\nsub_df = pd.DataFrame(paper_datasets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('./submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('./submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}