{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## literal matching","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#jsonモジュールのインポート\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_text_from_json(dataInd,fileId):\n    filename = \"/kaggle/input/coleridgeinitiative-show-us-the-data/\" + dataInd + \"/\" + fileId + \".json\"\n    \n    fd = open(filename, mode='r')\n    data = json.load(fd)\n    fd.close()\n    json_text = ''\n    for sections in data:\n        json_text = json_text + ' ' + sections.get('text')\n    \n    return json_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    text = ''.join([k if k not in string.punctuation else ' ' for k in text])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\ntrain_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_text = []\n\n# 学習データを取り込み\nfor Id in train_df[\"Id\"]:\n    training_text.append(create_text_from_json(\"train\", Id))\n\ntrain_df['text'] = training_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_text = []\n# 学習データを取り込み\nfor Id in sample_submission_df[\"Id\"]:\n    submit_text.append(create_text_from_json(\"test\", Id))\n\nsample_submission_df['text'] = submit_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\nexisting_labels = set(temp_1 + temp_2 + temp_3)\n\nid_list = []\nlables_list = []\nfor index, row in sample_submission_df.iterrows():\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 提出用データ作成\nmy_submission = pd.DataFrame()\nmy_submission['Id'] = id_list\nmy_submission['PredictionString1'] = lables_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del training_text\ndel submit_text\ndel id_list\ndel lables_list\ndel sample_text\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## simple transformers","metadata":{}},{"cell_type":"code","source":"!pip install '../input/simpletransformers0272/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '../input/tokenizers-070/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl' -q\n!pip install '../input/simpletransformers-0323-pypi/transformers-2.11.0-py3-none-any.whl' -q\n!pip install '../input/simpletransformers-0323-pypi/simpletransformers-0.32.3-py3-none-any.whl' -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.classification import MultiLabelClassificationModel\nimport logging","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ログの設定\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"work_df = pd.get_dummies(train_df['cleaned_label']) \nlabel_list = list(work_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del work_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_textdf_from_json(dataInd,fileId):\n    filename = \"/kaggle/input/coleridgeinitiative-show-us-the-data/\" + dataInd + \"/\" + fileId + \".json\"\n    \n    fd = open(filename, mode='r')\n    data = json.load(fd)\n    fd.close()\n    text_list = []\n    for sections in data:\n        text_list.append(sections.get('text'))\n    text_df = pd.DataFrame(columns=['Id','text'])\n    text_df['text'] = text_list\n    text_df['Id'] = fileId\n    \n    return text_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = pd.DataFrame()\ndistinct_train_df['Id'] = train_df['Id']\ndistinct_train_df = distinct_train_df.drop_duplicates(subset=[\"Id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_df = pd.DataFrame(columns=['Id','text'])\nfor index, row in distinct_train_df.iterrows():\n    fileId = row['Id']\n    text_df = text_df.append(create_textdf_from_json('train',fileId),ignore_index=True)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = pd.merge(distinct_train_df, text_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df['text'] = distinct_train_df['text'].apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"work_label_df = pd.DataFrame()\nfor label in label_list:\n    match_list = []\n    for index, row in distinct_train_df.iterrows():\n        match_list.append(1 if label in row['text'] else 0)\n    work_label_df[label] = match_list\n\ndistinct_train_df['label'] = work_label_df.values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df['match_count'] = distinct_train_df['label'].sum()\ndistinct_train_df = distinct_train_df.query('match_count > 0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del match_list\ndel work_label_df\ndel text_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopword_list = list(set([stopword for stopword in clean_text(' '.join(existing_labels)).split()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(json_text):\n    json_text = remove_stopwords(json_text)\n    \n    for label in label_list:\n        json_text.replace(label,'')\n\n    for stopword in stopword_list:\n        json_text.replace(stopword,'')\n\n    return json_text.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df['text'] = distinct_train_df['text'].apply(text_preprocessing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = distinct_train_df.query('text != \"\"')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = distinct_train_df.drop('match_count', axis=1)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df = pd.DataFrame(columns=['Id','text'])\nfor index, row in sample_submission_df.iterrows():\n    fileId = row['Id']\n    sub_text_df = sub_text_df.append(create_textdf_from_json('train',fileId),ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df['text'] = sub_text_df['text'].apply(text_cleaning)\nsub_text_df['text'] = sub_text_df['text'].apply(text_preprocessing)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df = sub_text_df.query('text != \"\"')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del stopword_list\ndel existing_labels\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tail_label(df):\n    \"\"\"\n    Give tail label colums of the given target dataframe\n    \n    args\n    df: pandas.DataFrame, target label df whose tail label has to identified\n    \n    return\n    tail_label: list, a list containing column name of all the tail label\n    \"\"\"\n    columns = df.columns\n    n = len(columns)\n    irpl = np.zeros(n)\n    for column in range(n):\n        irpl[column] = df[columns[column]].value_counts()[1]\n    irpl = max(irpl)/irpl\n    mir = np.average(irpl)\n    tail_label = []\n    for i in range(n):\n        if irpl[i] > mir:\n            tail_label.append(columns[i])\n    return tail_label\n\ndef get_index(df):\n  \"\"\"\n  give the index of all tail_label rows\n  args\n  df: pandas.DataFrame, target label df from which index for tail label has to identified\n    \n  return\n  index: list, a list containing index number of all the tail label\n  \"\"\"\n  tail_labels = get_tail_label(df)\n  index = set()\n  for tail_label in tail_labels:\n    sub_index = set(df[df[tail_label]==1].index)\n    index = index.union(sub_index)\n  return list(index)\n\ndef get_minority_instace(X, y):\n    \"\"\"\n    Give minority dataframe containing all the tail labels\n    \n    args\n    X: pandas.DataFrame, the feature vector dataframe\n    y: pandas.DataFrame, the target vector dataframe\n    \n    return\n    X_sub: pandas.DataFrame, the feature vector minority dataframe\n    y_sub: pandas.DataFrame, the target vector minority dataframe\n    \"\"\"\n    index = get_index(y)\n    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n    return X_sub, y_sub\n\ndef nearest_neighbour(X):\n    \"\"\"\n    Give index of 5 nearest neighbor of all the instance\n    \n    args\n    X: np.array, array whose nearest neighbor has to find\n    \n    return\n    indices: list of list, index of 5 NN of each element in X\n    \"\"\"\n    nbs=NearestNeighbors(n_neighbors=5,metric='euclidean',algorithm='kd_tree').fit(X)\n    euclidean,indices= nbs.kneighbors(X)\n    return indices\n\ndef MLSMOTE(X,y, n_sample):\n    \"\"\"\n    Give the augmented data using MLSMOTE algorithm\n    \n    args\n    X: pandas.DataFrame, input vector DataFrame\n    y: pandas.DataFrame, feature vector dataframe\n    n_sample: int, number of newly generated sample\n    \n    return\n    new_X: pandas.DataFrame, augmented feature vector data\n    target: pandas.DataFrame, augmented target vector data\n    \"\"\"\n    indices2 = nearest_neighbour(X)\n    n = len(indices2)\n    new_X = np.zeros((n_sample, X.shape[1]))\n    target = np.zeros((n_sample, y.shape[1]))\n    for i in range(n_sample):\n        reference = random.randint(0,n-1)\n        neighbour = random.choice(indices2[reference,1:])\n        all_point = indices2[reference]\n        nn_df = y[y.index.isin(all_point)]\n        ser = nn_df.sum(axis = 0, skipna = True)\n        target[i] = np.array([1 if val>2 else 0 for val in ser])\n        ratio = random.random()\n        gap = X.loc[reference,:] - X.loc[neighbour,:]\n        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n    new_X = pd.DataFrame(new_X, columns=X.columns)\n    target = pd.DataFrame(target, columns=y.columns)\n    new_X = pd.concat([X, new_X], axis=0)\n    target = pd.concat([y, target], axis=0)\n    return new_X, target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df_add,_ = MLSMOTE(distinct_train_df,distinct_train_df['label'],len(distinct_train_df) // 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = distinct_train_df.append(distinct_train_df_add, ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distinct_train_df = distinct_train_df.sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# モデルの作成\nmodel = MultiLabelClassificationModel('albert', '../input/pretrained-albert-pytorch/albert-base-v2', \n                                      num_labels=len(label_list),\n                                      use_cuda=False, \n                                      args={'reprocess_input_data': False, \n                                            'overwrite_output_dir': True, \n                                            'train_batch_size': 8, \n                                            'num_train_epochs': 1})\n\nmodel.train_model(distinct_train_df.drop('Id',axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Prediction_list = []\nfor index, row in sub_text_df.iterrows():\n    predictions, raw_outputs = model.predict([row['text']])\n    Prediction_list.append('|'.join(np.array(label_list)[predictions==1].tolist()))\n\nsub_text_df['PredictionString2'] = Prediction_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df = (sub_text_df.groupby('Id')['PredictionString2']\n          .apply(list)\n          .apply(lambda x:sorted(x))\n          .apply('|'.join)\n         )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df = sub_text_df.reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def splitAndJoin(text):\n    text = '|'.join(list(set(text.split('|'))))\n    text = re.sub('^\\|','',text)\n    text = re.sub('\\|$','',text)\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text_df['PredictionString2'] = sub_text_df['PredictionString2'].apply(splitAndJoin)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.merge(my_submission, sub_text_df, how='left')\nmy_submission.fillna('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2つのモデルの結果をマージ\nmy_submission['PredictionString'] = np.where(my_submission['PredictionString1'] == '', \n                                             my_submission['PredictionString2'], \n                                             my_submission['PredictionString1'])\nmy_submission = my_submission.drop('PredictionString1', axis=1)\nmy_submission = my_submission.drop('PredictionString2', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}