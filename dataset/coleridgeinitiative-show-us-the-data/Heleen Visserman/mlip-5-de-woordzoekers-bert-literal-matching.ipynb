{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\n\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition.","metadata":{}},{"cell_type":"code","source":"MAX_SAMPLE = None # set a small number for experimentation, set None for production.\nPREDICTION_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:47:16.059684Z","iopub.execute_input":"2021-06-13T14:47:16.060141Z","iopub.status.idle":"2021-06-13T14:47:16.064284Z","shell.execute_reply.started":"2021-06-13T14:47:16.060097Z","shell.execute_reply":"2021-06-13T14:47:16.063507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install packages","metadata":{}},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:47:16.075277Z","iopub.execute_input":"2021-06-13T14:47:16.07581Z","iopub.status.idle":"2021-06-13T14:48:38.011973Z","shell.execute_reply.started":"2021-06-13T14:47:16.075778Z","shell.execute_reply":"2021-06-13T14:48:38.010752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#new\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nrandom.seed(123)\nnp.random.seed(456)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-13T14:48:38.013872Z","iopub.execute_input":"2021-06-13T14:48:38.014147Z","iopub.status.idle":"2021-06-13T14:48:38.022944Z","shell.execute_reply.started":"2021-06-13T14:48:38.014119Z","shell.execute_reply":"2021-06-13T14:48:38.021921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\nds = pd.read_csv(train_path)\nds = ds[:MAX_SAMPLE] #picks the number of samples from the trainingset\n\ntrain, test = train_test_split(ds , test_size=0.20, random_state=42)\n#append all texts after each other\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in ds['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:48:38.0243Z","iopub.execute_input":"2021-06-13T14:48:38.024614Z","iopub.status.idle":"2021-06-13T14:49:04.436419Z","shell.execute_reply.started":"2021-06-13T14:48:38.024587Z","shell.execute_reply":"2021-06-13T14:49:04.435517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\nfor paper_id in sample_submission['Id']: #TODO make sure this uses the test data createdabove.\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f: #Training with testdata? or maybe they don't use the test part of the papers dictionary\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:04.437615Z","iopub.execute_input":"2021-06-13T14:49:04.437867Z","iopub.status.idle":"2021-06-13T14:49:04.455331Z","shell.execute_reply.started":"2021-06-13T14:49:04.437842Z","shell.execute_reply":"2021-06-13T14:49:04.454345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show statistic about the data","metadata":{}},{"cell_type":"code","source":"print(f\"Number of different datasets: {len(ds['dataset_label'].unique())}\")\nprint(f\"Size of the dataset: {len(ds['dataset_label'])}\")\n\nprint(\"\\nFrequency table\")\nprint(ds['dataset_label'].value_counts())\nprint(\"\\n\")\nds['dataset_label'].value_counts().plot()\n\na = 4","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:04.45922Z","iopub.execute_input":"2021-06-13T14:49:04.459508Z","iopub.status.idle":"2021-06-13T14:49:04.622013Z","shell.execute_reply.started":"2021-06-13T14:49:04.45948Z","shell.execute_reply":"2021-06-13T14:49:04.621238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Literal matching","metadata":{}},{"cell_type":"markdown","source":"### Create a knowledge bank","metadata":{}},{"cell_type":"markdown","source":"Extra_data reads out the extra data sets that are retrieved from a notebook by Ken Miller (https://www.kaggle.com/mlconsult/score-57ish-with-additional-govt-datasets)","metadata":{}},{"cell_type":"code","source":"\nall_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \n# Load in extra labels\nextra_data = pd.read_csv('../input/bigger-govt-dataset-list/data_set_26897.csv')\n    \n# for x, row in extra_data.iterrows():\n#     all_labels.add(str(row['title']).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:04.624588Z","iopub.execute_input":"2021-06-13T14:49:04.624854Z","iopub.status.idle":"2021-06-13T14:49:04.722001Z","shell.execute_reply.started":"2021-06-13T14:49:04.624827Z","shell.execute_reply":"2021-06-13T14:49:04.721227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Matching on test data","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:04.722991Z","iopub.execute_input":"2021-06-13T14:49:04.723247Z","iopub.status.idle":"2021-06-13T14:49:04.728654Z","shell.execute_reply.started":"2021-06-13T14:49:04.723224Z","shell.execute_reply":"2021-06-13T14:49:04.727683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"literal_preds = []\n\nfor paper_id in test['Id'][:PREDICTION_SIZE]:\n# for paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels)) #separates the different datasets for the papers using |\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:04.729833Z","iopub.execute_input":"2021-06-13T14:49:04.730062Z","iopub.status.idle":"2021-06-13T14:49:20.46857Z","shell.execute_reply.started":"2021-06-13T14:49:04.73004Z","shell.execute_reply":"2021-06-13T14:49:20.467601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"literal_preds[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:20.469738Z","iopub.execute_input":"2021-06-13T14:49:20.469989Z","iopub.status.idle":"2021-06-13T14:49:20.475948Z","shell.execute_reply.started":"2021-06-13T14:49:20.469964Z","shell.execute_reply":"2021-06-13T14:49:20.474961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert prediction","metadata":{"trusted":true}},{"cell_type":"markdown","source":"### Paths and Hyperparameters","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = '../input/split8020seed42/output'\nTEST_INPUT_SAVE_PATH = './input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '../input/split8020seed42/train_ner.json'\nVAL_PATH = '../input/split8020seed42/train_ner.json'\n\nPREDICTION_SAVE_PATH = './pred'\nPREDICTION_FILE = 'test_predictions.txt'","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:20.477435Z","iopub.execute_input":"2021-06-13T14:49:20.47783Z","iopub.status.idle":"2021-06-13T14:49:20.487862Z","shell.execute_reply.started":"2021-06-13T14:49:20.477771Z","shell.execute_reply":"2021-06-13T14:49:20.487065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform data to NER format","metadata":{}},{"cell_type":"markdown","source":"Group by publication, training labels should have the same form as expected output.","metadata":{}},{"cell_type":"markdown","source":"We split the full training data into a training and validation part, using train_test_split(). ","metadata":{}},{"cell_type":"code","source":"train_full = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n\nds = pd.read_csv(train_path)\nds = ds[:MAX_SAMPLE]\n\ntrain, test = train_test_split(ds , test_size=0.20, random_state=42) #TODO make sure this also happens for literal matching\n\nprint(f'No. grouped training rows: {len(train)}', f'No. grouped test rows: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:20.489005Z","iopub.execute_input":"2021-06-13T14:49:20.489323Z","iopub.status.idle":"2021-06-13T14:49:20.916481Z","shell.execute_reply.started":"2021-06-13T14:49:20.489297Z","shell.execute_reply":"2021-06-13T14:49:20.915517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:20.917771Z","iopub.execute_input":"2021-06-13T14:49:20.918037Z","iopub.status.idle":"2021-06-13T14:49:20.924341Z","shell.execute_reply.started":"2021-06-13T14:49:20.91801Z","shell.execute_reply":"2021-06-13T14:49:20.923266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We moved the ner format making to the BERT training notebook. This notebook returns train and test set.","metadata":{}},{"cell_type":"code","source":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in test['Id'][:PREDICTION_SIZE]:\n# for paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ] #This is tricky for URLs, as this may lead to very short sentences containing 'google', thus below sentences shorter than 10 characters are filtered out.\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study', 'dataset', 'et al.', 'https', 'http'])] #only adds sentence if it contains word like data and study ??\n    \n# TODO Check which words are often found in sentences with datasets.    \n    \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:20.925723Z","iopub.execute_input":"2021-06-13T14:49:20.925999Z","iopub.status.idle":"2021-06-13T14:49:27.464214Z","shell.execute_reply.started":"2021-06-13T14:49:20.925973Z","shell.execute_reply":"2021-06-13T14:49:27.463349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Do predict and collect results","metadata":{}},{"cell_type":"code","source":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\" #TODO Change the path to the new testset retrieved from the train notebook.\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:27.466308Z","iopub.execute_input":"2021-06-13T14:49:27.466698Z","iopub.status.idle":"2021-06-13T14:49:27.473235Z","shell.execute_reply.started":"2021-06-13T14:49:27.466657Z","shell.execute_reply":"2021-06-13T14:49:27.47084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:27.474897Z","iopub.execute_input":"2021-06-13T14:49:27.475504Z","iopub.status.idle":"2021-06-13T14:49:28.261196Z","shell.execute_reply.started":"2021-06-13T14:49:27.475455Z","shell.execute_reply":"2021-06-13T14:49:28.260044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_predict():\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:28.262844Z","iopub.execute_input":"2021-06-13T14:49:28.26313Z","iopub.status.idle":"2021-06-13T14:49:28.268672Z","shell.execute_reply.started":"2021-06-13T14:49:28.263089Z","shell.execute_reply":"2021-06-13T14:49:28.267519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:28.270001Z","iopub.execute_input":"2021-06-13T14:49:28.270254Z","iopub.status.idle":"2021-06-13T14:55:53.728496Z","shell.execute_reply.started":"2021-06-13T14:49:28.27023Z","shell.execute_reply":"2021-06-13T14:55:53.727256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Restore Dataset labels from predictions","metadata":{}},{"cell_type":"code","source":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.729643Z","iopub.status.idle":"2021-06-13T14:55:53.730146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_dataset_labels = [] # store all dataset labels for each publication\n\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.730952Z","iopub.status.idle":"2021-06-13T14:55:53.731413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert_dataset_labels","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.732231Z","iopub.status.idle":"2021-06-13T14:55:53.732689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter based on Jaccard score and clean","metadata":{}},{"cell_type":"code","source":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.733447Z","iopub.status.idle":"2021-06-13T14:55:53.733892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filtered_bert_labels\n# final_predictions","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.734634Z","iopub.status.idle":"2021-06-13T14:55:53.73507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aggregate final predictions and write submission file","metadata":{}},{"cell_type":"code","source":"#TODO Score implementeren om weer te geven wat goed werkt. coledrige uses FBeta score. Performance, Confusion matrix could be useful. \n#TODO Find a way to check whether we are right. Can be done using the calculation of TP, TN, FP and FN.","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.735843Z","iopub.status.idle":"2021-06-13T14:55:53.736292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CURRENT PROBLEM\n# BERT does not predict every time, but Literal matching does, so all predictions are from literal matching and thus BERT prediction is not used\n\nfinal_predictions = []\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n    if literal_match:\n        final_predictions.append(literal_match)\n    else:\n        final_predictions.append(bert_pred)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.737009Z","iopub.status.idle":"2021-06-13T14:55:53.73747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = test[:PREDICTION_SIZE]\n\nsample_submission['PredictionString'] = final_predictions\nsample_submission['LiteralPrediction'] = literal_preds\nsample_submission['BertPrediction'] = filtered_bert_labels\nsample_submission['adni'] = filtered_bert_labels[1] * PREDICTION_SIZE\nsample_submission\n\n# filtered_bert_labels","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.738211Z","iopub.status.idle":"2021-06-13T14:55:53.738654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO Score implementeren om weer te geven wat goed werkt. coledrige uses FBeta score. Performance, Confusion matrix could be useful. \n#TODO Find a way to check whether we are right. Can be done using the calculation of TP, TN, FP and FN.\n# TP = 0\n# FP = 0\n# TN = 0\n# FN = 0\n\ndef check_match(predictions,cleaned_label):\n    for pred in predictions:\n#         result = 0\n#         print(f\"predict:{totally_clean_text(pred)} \\tcorrect:{cleaned_label}\")\n        t = totally_clean_text(pred)\n        if t and t in cleaned_label:\n            return True\n    return False\n#     return result >= min(len(predictions)-5,1)\n        \ndef calc_accuracy(row_name, sample_submission):\n    preds = [check_match(row[row_name].split('|'), row[\"cleaned_label\"]) for k,row in sample_submission.iterrows()]\n\n    return len(list(filter(lambda x: x,preds))) / PREDICTION_SIZE\n    \nacc_bert = calc_accuracy(\"BertPrediction\", sample_submission)\nacc_literal = calc_accuracy(\"LiteralPrediction\", sample_submission)\nacc_combined = calc_accuracy(\"PredictionString\", sample_submission)\nadni = calc_accuracy(\"adni\", sample_submission)\n\nprint(acc_bert,acc_literal,acc_combined,adni)\n# for k, row in sample_submission.iterrows():\n#     id = row['Id']\n#     predictions = row['PredictionString'].split('|')\n# #     print(predictions)\n    \n#     for pred in predictions:\n#         if totally_clean_text(pred) in sample_submission[\"cleaned_label\"]:\n#             row[\"Match\"] = True\n#         else:\n#             row[\"Match\"] = False\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.739498Z","iopub.status.idle":"2021-06-13T14:55:53.739935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(f'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:55:53.74065Z","iopub.status.idle":"2021-06-13T14:55:53.741093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}