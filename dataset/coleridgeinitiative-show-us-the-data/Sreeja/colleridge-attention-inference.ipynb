{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the inference using the model created in the following notebook.\nhttps://www.kaggle.com/sreejaej/attention-is-all-you-need-score-of-0-532\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:08:44.471996Z","iopub.execute_input":"2021-06-01T17:08:44.472418Z","iopub.status.idle":"2021-06-01T17:08:44.478394Z","shell.execute_reply.started":"2021-06-01T17:08:44.472384Z","shell.execute_reply":"2021-06-01T17:08:44.477022Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport torch.nn as nn \nimport torch\nfrom transformers import BertTokenizer\nfrom nltk.corpus import stopwords\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.086145Z","iopub.execute_input":"2021-06-17T15:10:47.086955Z","iopub.status.idle":"2021-06-17T15:10:47.092118Z","shell.execute_reply.started":"2021-06-17T15:10:47.086905Z","shell.execute_reply":"2021-06-17T15:10:47.091251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQ=500","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.093706Z","iopub.execute_input":"2021-06-17T15:10:47.094255Z","iopub.status.idle":"2021-06-17T15:10:47.106752Z","shell.execute_reply.started":"2021-06-17T15:10:47.094218Z","shell.execute_reply":"2021-06-17T15:10:47.105633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\ntrain\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.229907Z","iopub.execute_input":"2021-06-17T15:10:47.230528Z","iopub.status.idle":"2021-06-17T15:10:47.324769Z","shell.execute_reply.started":"2021-06-17T15:10:47.230489Z","shell.execute_reply":"2021-06-17T15:10:47.323584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['cleaned_label'] = train['cleaned_label'].apply(lambda x : x.strip())\nknown_labels = np.unique ( list( train['cleaned_label'].values ))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.326538Z","iopub.execute_input":"2021-06-17T15:10:47.326861Z","iopub.status.idle":"2021-06-17T15:10:47.371979Z","shell.execute_reply.started":"2021-06-17T15:10:47.32683Z","shell.execute_reply":"2021-06-17T15:10:47.370883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''known_labels_final=[]\nfor label1 in known_labels:\n    ignore=False\n    for label2 in known_labels:\n        \n        if (label1==label2):\n            continue\n        elif ( label1 in label2):\n            ignore=True\n            print( \"ignoring\", label1,\"coz of\", label2 )\n            break\n    if ( ignore == False ) :\n        known_labels_final.append(label1)\n\nknown_labels_final'''\n     ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.373943Z","iopub.execute_input":"2021-06-17T15:10:47.374315Z","iopub.status.idle":"2021-06-17T15:10:47.38044Z","shell.execute_reply.started":"2021-06-17T15:10:47.374281Z","shell.execute_reply":"2021-06-17T15:10:47.379563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\ntest = pd.read_csv(test_path)\ntest=test.set_index(['Id'])\ntest","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.381875Z","iopub.execute_input":"2021-06-17T15:10:47.382171Z","iopub.status.idle":"2021-06-17T15:10:47.404542Z","shell.execute_reply.started":"2021-06-17T15:10:47.382142Z","shell.execute_reply":"2021-06-17T15:10:47.403496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nstopwords_set = set(stopwords.words('english')) \ndef clean_text(input_words):\n    #return [   re.sub('[^A-Za-z0-9]+', ' ', str(r).lower()).strip() for r in input_words if not r.lower() in stopwords_set  ]\n    return [   re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(r).lower()).strip() for r in input_words  ]\n\n\n\ndef process_row_with_bert(words, tokenizer, stopwords_set):  \n        \n    encoded_input_word_list = clean_text(words)\n    encoded_input_word_list = list(map(tokenizer.convert_tokens_to_ids, tokenizer.tokenize ( \" \".join(encoded_input_word_list)) ))\n    \n\n    if ( len(encoded_input_word_list) < MAX_SEQ ):\n        encoded_input_word_list = encoded_input_word_list + list(np.zeros( MAX_SEQ - len(encoded_input_word_list)) )\n    else :\n        encoded_input_word_list = encoded_input_word_list[0:MAX_SEQ]\n    \n     \n    return encoded_input_word_list\n\ndef process_data_with_bert (df, tokenizer) :\n    max_word_count = 0 \n    text_array = [] \n    code_array=[]\n     \n    stopwords_set = set(stopwords.words('english')) \n    \n    df_final = pd.DataFrame(columns = ['Id', 'text'])\n    \n    for index, id in tqdm ( enumerate ( df.index) )  :\n        with open(paper_test_folder+'/'+ id +'.json', 'r') as f:\n                #Load the json \n                paper = json.load(f)\n                \n                #Convert the relevant sentences into a single paragraph\n                section_sentences =[]\n                for section_index in range ( 0, len(paper)):\n                    section_sentences = section_sentences +  re.split('[.;\\nâ€¢]',paper[section_index].get('text'))   \n                section_sentences = \" [sep] \".join(section_sentences) \n                sentence_words = section_sentences.split() + ['[sep]']\n\n                # Chunk the paragraph into size of 500. Todo:Relook at this \n                n = int(0.7*MAX_SEQ) \n                word_chunks = [sentence_words[i:i+n+20] for i in range(0, len(sentence_words), n)] \n                word_chunks[-1] = word_chunks[-1] + list(np.empty( MAX_SEQ - len(word_chunks[-1]), dtype=str) )\n                     \n                # Add the words in the labels to the output wordlist\n                #labels = df.loc[id]\n\n                # Encode/Embed the words for processing\n                df_new = pd.DataFrame(columns = ['Id', 'text', 'known_labels'])\n                for i in range ( 0, len(word_chunks)):\n                    encoded_input_word_list = process_row_with_bert(word_chunks[i], tokenizer, stopwords_set )\n                    word_chunk_sentence = \" \".join( clean_text( word_chunks[i] ) ) \n                    labels = [known_label for known_label in known_labels  if \" \".join(clean_text( known_label.split() )) in word_chunk_sentence.lower() ] \n                    df_temp = pd.DataFrame ({\"Id\":id, \"org_text\": word_chunk_sentence, \"text\":[encoded_input_word_list], \"known_labels\": \"|\".join(labels) }, index=[i])\n                    df_new =df_new.append(df_temp, ignore_index=True)\n                    #print(df_new)\n\n        df_final = df_final.append(df_new, ignore_index=True)\n\n    return df_final","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.406101Z","iopub.execute_input":"2021-06-17T15:10:47.406476Z","iopub.status.idle":"2021-06-17T15:10:47.429208Z","shell.execute_reply.started":"2021-06-17T15:10:47.406442Z","shell.execute_reply":"2021-06-17T15:10:47.428382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/\")\ntokenizer.add_tokens('[cls]')\ntokenizer.add_tokens('[sep]')\n\ndf_test = process_data_with_bert(test, tokenizer)  \n   ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:47.430512Z","iopub.execute_input":"2021-06-17T15:10:47.430997Z","iopub.status.idle":"2021-06-17T15:10:50.559099Z","shell.execute_reply.started":"2021-06-17T15:10:47.430961Z","shell.execute_reply":"2021-06-17T15:10:50.558303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.560784Z","iopub.execute_input":"2021-06-17T15:10:50.561099Z","iopub.status.idle":"2021-06-17T15:10:50.594608Z","shell.execute_reply.started":"2021-06-17T15:10:50.561067Z","shell.execute_reply":"2021-06-17T15:10:50.593493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[ df_test['known_labels'] != \"\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.596465Z","iopub.execute_input":"2021-06-17T15:10:50.596792Z","iopub.status.idle":"2021-06-17T15:10:50.631155Z","shell.execute_reply.started":"2021-06-17T15:10:50.596763Z","shell.execute_reply":"2021-06-17T15:10:50.630073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn \n\nclass FFN(nn.Module):\n    def __init__(self, state_size=200):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n\n        self.lr1 = nn.Linear(state_size, state_size)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(state_size, state_size)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\nclass ColleridgeTransformerModel(nn.Module):\n\n    def __init__(self, embed_dim = 200, max_seq=MAX_SEQ, input_vocab_len=30523 , output_vocab_len= 30523): #HDKIM 100->MAX_SEQ\n        super(ColleridgeTransformerModel, self).__init__()\n        self.embed_dim = embed_dim\n        self.embedding = nn.Embedding(input_vocab_len+1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq, embed_dim)\n        self.e_embedding = nn.Embedding(output_vocab_len+1, embed_dim)\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        self.ffn = FFN(embed_dim)\n        self.pred = nn.Linear(embed_dim, output_vocab_len+1)\n        self.softmax = nn.LogSoftmax(dim=2)\n    \n    def forward(self, x, generated_words):\n        device = x.device       \n        \n        # Embed the question\n        x = self.embedding(x)\n        \n        # Calculate the position id and embed it\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        \n        #Add it to the poisition id\n        pos_x = self.pos_embedding(pos_id)\n        x = x + pos_x\n        \n        # This is the target_id/query ( right shifted input )\n        e = self.e_embedding(generated_words) \n        pos_e_id = torch.arange(e.size(1)).unsqueeze(0).to(device)\n        pos_e = self.pos_embedding(pos_e_id)\n        e = e + pos_e\n         \n        # Send the query key and value to the multi attention network \n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n        att_output, att_weight = self.multi_att(x, x, x)\n        x = self.layer_normal(att_output + x)\n        \n        \n        for i in range ( 0, 1):\n            \n            e = e.permute(1, 0, 2)\n              \n            att_mask = future_mask(e.size(0)).to(device) \n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, e, e, attn_mask=att_mask)\n\n            e = self.layer_normal(att_output + e)\n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, x, x)\n\n            # Normalization \n            att_output = self.layer_normal(att_output + e)\n\n            # Reshape the output\n            att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n\n            # Send the output to FFN.\n            e = self.ffn(att_output)\n\n            # \n            e = self.layer_normal(e + att_output)\n        \n        # \n        e = self.pred(e)\n        e = self.softmax(e)\n        e = e.permute(0, 2, 1)\n        return e, att_weight","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.632463Z","iopub.execute_input":"2021-06-17T15:10:50.632774Z","iopub.status.idle":"2021-06-17T15:10:50.655243Z","shell.execute_reply.started":"2021-06-17T15:10:50.632747Z","shell.execute_reply":"2021-06-17T15:10:50.653901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xfrmer_model = ColleridgeTransformerModel(  )\nxfrmer_model.load_state_dict(torch.load(\"../input/coleridgemodel/xform-463-fold0.model\", map_location=torch.device(device) ))\nxfrmer_model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.656694Z","iopub.execute_input":"2021-06-17T15:10:50.657004Z","iopub.status.idle":"2021-06-17T15:10:50.946725Z","shell.execute_reply.started":"2021-06-17T15:10:50.656976Z","shell.execute_reply":"2021-06-17T15:10:50.9458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass ColleridgeDatasetEval(Dataset):\n    def __init__(self, df, max_seq=MAX_SEQ):\n        super(ColleridgeDatasetEval, self).__init__()\n        self.max_seq = max_seq\n        self.samples = df\n\n    def __len__(self):\n        return len(self.samples.index)\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.samples.iloc[index]['text'], dtype=torch.int, device=device).view(-1, 1)\n        y = torch.tensor(np.zeros(MAX_SEQ), dtype=torch.int, device=device).view(-1, 1)\n        y[0] = 30522\n        target_id = y[:-1]\n        label = self.samples.iloc[index]['known_labels']\n        \n        return  x.squeeze(-1), target_id.squeeze(-1), label, self.samples.iloc[index]['org_text']","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.947944Z","iopub.execute_input":"2021-06-17T15:10:50.948224Z","iopub.status.idle":"2021-06-17T15:10:50.956192Z","shell.execute_reply.started":"2021-06-17T15:10:50.948197Z","shell.execute_reply":"2021-06-17T15:10:50.955453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ColleridgeDatasetEval(df_test)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n\nnum=-1\nxfrmer_model.to(device)\n\ndf_eval = pd.DataFrame(columns = ['id', 'predicted', 'known_labels'])\nfor item in dataloader:\n    stop = False \n    i = 0\n    num = num+1\n    while ( i < 498 ):\n        output,_ = xfrmer_model( item[0].to(device).long(),item[1].to(device).long()  )\n        y = torch.argmax( output , dim=1)\n        z = y[0][i]\n        item[1][0][i+1]=z\n        i = i+1\n        if ( z == 0.0):\n            break\n    y = [30522] + y.tolist()[0]\n    \n    predicted_text = tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens( [int(z) for z in y if z not in [30522, 0.0, 0]] ))\n    complete_text= item[3][0]\n\n    for index, label in enumerate ( predicted_text.split('[sep]') )   :  \n            if(label in complete_text and label !=\"\"):\n                    #workaround-tobe fixed later\n                    '''if( label not in item[2] ):\n                        print(label)'''\n                    \n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [label], \"known_labels\": [label]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n            else:\n                    #workaround-tobe fixed later\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [\"\"] , \"known_labels\": [\"\"] }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n            \n    \n    '''if(predicted_text==\"[sep]\"):\n        for known_label1 in item[2][0].split(\"|\"):\n            ignore=False\n            for known_label2 in item[2][0].split(\"|\"):\n                if( known_label1 in known_label2 and known_label1 != known_label2):\n                    #print(known_label1, known_label2)\n                    ignore=True\n                    continue\n\n            if (ignore==False):\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [known_label1], \"known_labels\": [known_label1]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n    else:\n    \n        for index, label in enumerate ( predicted_text.split('[sep]') )   :  \n\n            if(label in complete_text and label !=\"\"):\n                    #workaround-tobe fixed later\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [label], \"known_labels\": [label]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n\n            elif label != \"\":\n                for known_label1 in item[2][0].split(\"|\"):\n                    ignore=False\n                    for known_label2 in item[2][0].split(\"|\"):\n                        if( known_label1 in known_label2 and known_label1 != known_label2):\n                            #print(known_label1, known_label2)\n                            ignore=True\n                            continue\n\n                    if (ignore==False):\n                            df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [known_label1], \"known_labels\": [known_label1]  }, index=[i])\n                            df_eval =df_eval.append(df_temp, ignore_index=True) \n            '''\n\ndf_eval","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:10:50.958714Z","iopub.execute_input":"2021-06-17T15:10:50.959088Z","iopub.status.idle":"2021-06-17T15:12:16.804471Z","shell.execute_reply.started":"2021-06-17T15:10:50.959058Z","shell.execute_reply":"2021-06-17T15:12:16.803417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_eval[df_eval['known_labels']!=\"\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.805865Z","iopub.execute_input":"2021-06-17T15:12:16.806322Z","iopub.status.idle":"2021-06-17T15:12:16.819533Z","shell.execute_reply.started":"2021-06-17T15:12:16.806259Z","shell.execute_reply":"2021-06-17T15:12:16.818533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp1=df_eval.groupby('id').apply(lambda row:  \"|\".join( np.unique( row['predicted'].values ) ) ) \ndf_temp2=df_eval.groupby('id').apply(lambda row:  \"|\".join(np.unique((\"|\".join(row['known_labels'].values)).split(\"|\")))  ) \n\n\ndf_final_result = pd.DataFrame(columns = ['Id', 'PredictionString'])\ndf_final_result['Id'] = df_temp1.index\ndf_final_result['PredictionString1'] = list(map(lambda i: \"|\".join( clean_text(df_temp1[i].split(\"|\")) ) , range(0,len(df_temp1))))\ndf_final_result['PredictionString2'] = list(map(lambda i: \"|\".join( clean_text(df_temp2[i].split(\"|\")) ) , range(0,len(df_temp2))))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.820834Z","iopub.execute_input":"2021-06-17T15:12:16.821215Z","iopub.status.idle":"2021-06-17T15:12:16.845475Z","shell.execute_reply.started":"2021-06-17T15:12:16.821183Z","shell.execute_reply":"2021-06-17T15:12:16.844368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_final_result['PredictionString'] = list(map( lambda x: \"|\".join ( np.unique(x.split(\"|\"))[1:] ) , (df_final_result['PredictionString1']+\"|\"+df_final_result['PredictionString2']) ))\ndf_final_result['PredictionString'] = list(map( lambda x: \"|\".join ( np.unique(x.split(\"|\")) ) , (df_final_result['PredictionString1'] )))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.846973Z","iopub.execute_input":"2021-06-17T15:12:16.847311Z","iopub.status.idle":"2021-06-17T15:12:16.852368Z","shell.execute_reply.started":"2021-06-17T15:12:16.847249Z","shell.execute_reply":"2021-06-17T15:12:16.851395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_final_result['PredictionString'] =df_final_result['PredictionString'] + \"|\" + df_final_result['PredictionString'] ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.853331Z","iopub.execute_input":"2021-06-17T15:12:16.853624Z","iopub.status.idle":"2021-06-17T15:12:16.867195Z","shell.execute_reply.started":"2021-06-17T15:12:16.853596Z","shell.execute_reply":"2021-06-17T15:12:16.866431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final_result['PredictionString'] .iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.868212Z","iopub.execute_input":"2021-06-17T15:12:16.868671Z","iopub.status.idle":"2021-06-17T15:12:16.882774Z","shell.execute_reply.started":"2021-06-17T15:12:16.868641Z","shell.execute_reply":"2021-06-17T15:12:16.881922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final_result['PredictionString'] =df_final_result['PredictionString'].apply(  lambda x: \"|\".join( string for string in x.split(\"|\")\\\n                                                                                    if string != \"\") )\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.883849Z","iopub.execute_input":"2021-06-17T15:12:16.884329Z","iopub.status.idle":"2021-06-17T15:12:16.89799Z","shell.execute_reply.started":"2021-06-17T15:12:16.884281Z","shell.execute_reply":"2021-06-17T15:12:16.897162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final_result['PredictionString'] =df_final_result['PredictionString']+\"|\"+df_final_result['PredictionString']","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.899476Z","iopub.execute_input":"2021-06-17T15:12:16.899771Z","iopub.status.idle":"2021-06-17T15:12:16.914149Z","shell.execute_reply.started":"2021-06-17T15:12:16.899743Z","shell.execute_reply":"2021-06-17T15:12:16.912717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final_result[['Id','PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.915987Z","iopub.execute_input":"2021-06-17T15:12:16.916489Z","iopub.status.idle":"2021-06-17T15:12:16.940175Z","shell.execute_reply.started":"2021-06-17T15:12:16.916442Z","shell.execute_reply":"2021-06-17T15:12:16.93893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final_result[['Id','PredictionString']].to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:12:16.943948Z","iopub.execute_input":"2021-06-17T15:12:16.944291Z","iopub.status.idle":"2021-06-17T15:12:16.954407Z","shell.execute_reply.started":"2021-06-17T15:12:16.944241Z","shell.execute_reply":"2021-06-17T15:12:16.953197Z"},"trusted":true},"execution_count":null,"outputs":[]}]}