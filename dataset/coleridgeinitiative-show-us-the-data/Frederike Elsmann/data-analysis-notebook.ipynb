{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Analysis  \nAnalysis of the given dataset, including bag of word for sentences in which dataset labels are mentioned and N-Grams. ","metadata":{}},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport json\nimport re\nimport os\n\nINPUT_PATH = '/kaggle/input/coleridgeinitiative-show-us-the-data'\nWORKING_PATH = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:53:11.518968Z","iopub.execute_input":"2021-06-12T11:53:11.519516Z","iopub.status.idle":"2021-06-12T11:53:11.524817Z","shell.execute_reply.started":"2021-06-12T11:53:11.519431Z","shell.execute_reply":"2021-06-12T11:53:11.523732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code of this cell base on https://www.kaggle.com/hsbota/simple-baseline-finding-labels-in-text-hbot\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\n\ndef read_doc(doc_id, split:str = 'train'):\n    path = os.path.join(INPUT_PATH, split, f'{doc_id}.json')\n    return json.loads(open(path).read())\n\n    \ndef row_to_doc_text_uncleaned(row: pd.Series, split:str = 'train') -> str:\n    \n    doc_id = row['Id']\n    doc_title = row['pub_title'] if 'pub_title' in row else ''\n    \n    doc_content = read_doc(doc_id, split)\n    doc_text = ' '.join([section['section_title'] + ':. ' + \n                         section['text'] for section in doc_content])\n    \n    return doc_text","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:53:11.530024Z","iopub.execute_input":"2021-06-12T11:53:11.530458Z","iopub.status.idle":"2021-06-12T11:53:11.539793Z","shell.execute_reply.started":"2021-06-12T11:53:11.530369Z","shell.execute_reply":"2021-06-12T11:53:11.53862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'))\ntrain_df['pub_text'] = train_df.apply(row_to_doc_text_uncleaned, split='train', axis='columns')\nprint(train_df.nunique())\nprint(\"Sample of the dataset, including the raw text\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:53:11.54101Z","iopub.execute_input":"2021-06-12T11:53:11.541409Z","iopub.status.idle":"2021-06-12T11:54:20.416291Z","shell.execute_reply.started":"2021-06-12T11:53:11.54138Z","shell.execute_reply":"2021-06-12T11:54:20.415242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Amount of unique Datset labels: \", len(train_df['dataset_label'].value_counts().index))\nprint(\"Top 30 Mentioned Datasets: \")\ndataset_label_counts = train_df['dataset_label'].value_counts(dropna=False)\ndf_dataset_counts = pd.DataFrame(dataset_label_counts)\ndf_dataset_counts = df_dataset_counts.reset_index()\ndf_dataset_counts.columns = ['dataset_label','dataset_frequency'] \nprint(df_dataset_counts.head(30))\n\nprint(\"Interessting observation: SARS-CoV-2 genome sequence is treated as a Dataset \")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:54:20.418858Z","iopub.execute_input":"2021-06-12T11:54:20.419284Z","iopub.status.idle":"2021-06-12T11:54:20.444409Z","shell.execute_reply.started":"2021-06-12T11:54:20.419239Z","shell.execute_reply":"2021-06-12T11:54:20.443289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract sentences that contain the labeled dataset","metadata":{}},{"cell_type":"code","source":"# text is not cleaned, i.e. punctation, capitals and further are preserved\n\nmentions_sentences = []\nmentions_ds = []\nmentions_id = []\nmentions_index = []\nmentions_three_gram = []\nmentions_two_gram = []\nmentions_one_gram = []\nmentions_nthree_gram = []\nmentions_ntwo_gram = []\nmentions_none_gram = []\nnot_found_label = []\nnot_found_id = []\n\n\nfor i in range(len(train_df.index)):\n    # split after sentences end (\".\", \"!\" or \"?\" is followed by whitespace and captial letter)\n    text = re.split(\"(?<=[\\.\\!\\?])\\s*(?=[A-Z])\", train_df['pub_text'][i])\n    for sentence in text: \n        # if the according label is substring of the sentence\n        if  train_df['dataset_label'][i] in sentence:\n            # add sentence,label and Id to database\n            mentions_sentences.append(sentence)\n            mentions_ds.append(train_df['dataset_label'][i])\n            mentions_id.append(train_df['Id'][i])\n            \n            # create seperate strings for each word\n            word_list = re.sub('[\\,\\;\\:\\.\\!\\?]', '', sentence).split()\n            dataset_strings = train_df['dataset_label'][i].split()\n            indicies = []\n            #find indicies of mentioned database in sentence\n            for dataset_string in dataset_strings:\n                # if words of dataset are found as whole strings\n                if dataset_string not in word_list:\n                    #see if they can be found as sub strings\n                    for idx,string in enumerate(word_list):\n                        if  dataset_string in string:\n                            indicies.append(idx)\n                else:\n                    indicies.append(word_list.index(dataset_string))\n            # save index of first dataset word \n            mentions_index.append(indicies[0])\n            \n            # create 3-gram\n            if indicies[0]>2:\n                three_gram= word_list[indicies[0]-3] + \" \" +word_list[indicies[0]-2] + \" \" + word_list[indicies[0]-1]\n                mentions_three_gram.append(three_gram)\n            else: \n                mentions_three_gram.append(None)\n            \n            # create 2-gram\n            if indicies[0]>1:\n                two_gram= word_list[indicies[0]-2] + \" \" + word_list[indicies[0]-1]\n                mentions_two_gram.append(two_gram)\n            else: \n                mentions_two_gram.append(None)\n                \n            # create 1-gram\n            if indicies[0]>0:\n                one_gram=  word_list[indicies[0]-1]\n                mentions_one_gram.append(one_gram)\n            else: \n                mentions_one_gram.append(None)\n                \n            # create -3-gram\n            if indicies[-1]<len(word_list)-3:\n                nthree_gram= word_list[indicies[-1]+1] + \" \" +word_list[indicies[-1]+2] + \" \" + word_list[indicies[-1]+3]\n                mentions_nthree_gram.append(nthree_gram)\n            else: \n                mentions_nthree_gram.append(None)\n            \n            # create -2-gram\n            if indicies[-1]<len(word_list)-2:\n                ntwo_gram= word_list[indicies[-1]+1] + \" \" +word_list[indicies[-1]+2] \n                mentions_ntwo_gram.append(ntwo_gram)\n            else: \n                mentions_ntwo_gram.append(None)\n                \n            # create -1-gram\n            if indicies[-1]<len(word_list)-1:\n                none_gram= word_list[indicies[-1]+1] \n                mentions_none_gram.append(none_gram)\n            else: \n                mentions_none_gram.append(None)\n\n            \nmentions_dict = dict([('sentences',  mentions_sentences), ('dataset_labels', mentions_ds), ('id',mentions_id), ('index',mentions_index),('3-Gram', mentions_three_gram), ('2-Gram', mentions_two_gram), ('1-Gram', mentions_one_gram),('-3-Gram', mentions_nthree_gram), ('-2-Gram', mentions_ntwo_gram), ('-1-Gram', mentions_none_gram)])\nmentions_df = pd.DataFrame(mentions_dict, columns = ['sentences','dataset_labels','id','index', '3-Gram','2-Gram','1-Gram','-3-Gram','-2-Gram','-1-Gram'])\nmentions_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:55:26.437334Z","iopub.execute_input":"2021-06-12T11:55:26.437732Z","iopub.status.idle":"2021-06-12T11:57:33.02978Z","shell.execute_reply.started":"2021-06-12T11:55:26.437691Z","shell.execute_reply":"2021-06-12T11:57:33.028781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example sentences that mention dataset label ","metadata":{}},{"cell_type":"code","source":"pd.options.display.max_colwidth = 500\nmentions_df.sample(n = 15)[['sentences', 'dataset_labels']]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:57:33.031545Z","iopub.execute_input":"2021-06-12T11:57:33.031835Z","iopub.status.idle":"2021-06-12T11:57:33.046604Z","shell.execute_reply.started":"2021-06-12T11:57:33.031806Z","shell.execute_reply":"2021-06-12T11:57:33.045861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words for all words in extracted sentences","metadata":{}},{"cell_type":"code","source":"cleaned = ''.join([clean_text(sentence) for sentence in mentions_sentences])\nword_list = cleaned.split()    \nword_dict= dict([('word', word_list)])\nword_df= pd.DataFrame (word_dict, columns = ['word'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:57:33.047772Z","iopub.execute_input":"2021-06-12T11:57:33.048199Z","iopub.status.idle":"2021-06-12T11:57:34.947598Z","shell.execute_reply.started":"2021-06-12T11:57:33.048157Z","shell.execute_reply":"2021-06-12T11:57:34.946441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Words in mention sentences: \",len(word_df.index))\nword_counts = word_df['word'].value_counts(dropna=False)\ndf_word_counts = pd.DataFrame(word_counts)\ndf_word_counts = df_word_counts.reset_index()\ndf_word_counts.columns = ['unique_words', 'word_frequency'] \nprint(\"Unique words: \", len(df_word_counts.index))\ndf_word_counts.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:57:34.94893Z","iopub.execute_input":"2021-06-12T11:57:34.949259Z","iopub.status.idle":"2021-06-12T11:57:35.558466Z","shell.execute_reply.started":"2021-06-12T11:57:34.949228Z","shell.execute_reply":"2021-06-12T11:57:35.557633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset labels and N-Grams ","metadata":{}},{"cell_type":"code","source":"mentions_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:57:35.560506Z","iopub.execute_input":"2021-06-12T11:57:35.560769Z","iopub.status.idle":"2021-06-12T11:57:35.659434Z","shell.execute_reply.started":"2021-06-12T11:57:35.560743Z","shell.execute_reply":"2021-06-12T11:57:35.658293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Amount of papers searched: \", len(train_df.index),\" \\n\")\nprint(\"Amount of mention sentences: \", len(mentions_df.index),\" \\n\")\nprint(\"Top 10 Dataset_labels in mention sentences: \")\nprint(mentions_df['dataset_labels'].value_counts().nlargest(10),\" \\n\" )\nprint(\"Top 10 Index of first word of Dataset: \")\nprint(mentions_df['index'].value_counts().nlargest(10),\" \\n\")\nprint(\"Top 20 3_Grams:\")\nprint(mentions_df['3-Gram'].value_counts().nlargest(20),\" \\n\")\nprint(\"Top 20 2_Grams:\")\nprint(mentions_df['2-Gram'].value_counts().nlargest(20),\" \\n\")\nprint(\"Top 20 1_Grams:\")\nprint(mentions_df['1-Gram'].value_counts().nlargest(20),\" \\n\")\nprint(\"Top 20 -3_Grams:\")\nprint(mentions_df['-3-Gram'].value_counts().nlargest(20),\" \\n\")\nprint(\"Top 20 -2_Grams:\")\nprint(mentions_df['-2-Gram'].value_counts().nlargest(20),\" \\n\")\nprint(\"Top 20 -1_Grams:\")\nprint(mentions_df['-1-Gram'].value_counts().nlargest(20),\" \\n\")\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:58:25.473818Z","iopub.execute_input":"2021-06-12T11:58:25.474226Z","iopub.status.idle":"2021-06-12T11:58:25.648252Z","shell.execute_reply.started":"2021-06-12T11:58:25.47419Z","shell.execute_reply":"2021-06-12T11:58:25.647219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save mentions dataframe as cvs\nmentions_path = os.path.join(WORKING_PATH, 'mentions.csv')\nwords_path =  os.path.join(WORKING_PATH, 'bag_of_words.csv')\nmentions_df.to_csv(mentions_path)\nword_df.to_csv(words_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:57:35.866318Z","iopub.status.idle":"2021-06-12T11:57:35.866787Z"},"trusted":true},"execution_count":null,"outputs":[]}]}