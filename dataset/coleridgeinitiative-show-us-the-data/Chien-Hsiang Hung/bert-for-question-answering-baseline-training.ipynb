{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ¤— Bert for Question Answering Baseline: Training\n\nThis code is adapted from my work in the Tweet Sentiment Extraction Competition.\n\nIt tackles the task as a Question Answering one, where the question is implicit and can be understood as : \"Which datasets are mentionned ?\"\n\n\nThe approach is quite naÃ¯ve and has a lot of flaws. Feel free to ask any question in the comments.\n\nInference Kernel : https://www.kaggle.com/theoviel/bert-for-question-answering-baseline-inference","metadata":{}},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport gc\nimport glob\nimport json\nimport time\nimport copy\nimport torch\nimport random\nimport datetime\nimport tokenizers\nimport numpy as np\nimport transformers\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tokenizers import *\nfrom transformers import *\nfrom functools import partial\nfrom tqdm.notebook import tqdm\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import GroupKFold","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:33:05.696908Z","iopub.execute_input":"2021-06-01T07:33:05.697323Z","iopub.status.idle":"2021-06-01T07:33:05.707095Z","shell.execute_reply.started":"2021-06-01T07:33:05.697286Z","shell.execute_reply":"2021-06-01T07:33:05.706003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Params","metadata":{}},{"cell_type":"code","source":"SEED = 2020\n\nDATA_PATH = \"../input/coleridgeinitiative-show-us-the-data/\"\nDATA_PATH_TRAIN = DATA_PATH + 'train/'\nDATA_PATH_TEST = DATA_PATH + 'test/'\n\nNUM_WORKERS = 4\n\nVOCABS = {\n    \"bert-base-uncased\": \"../input/vocabs/bert-base-uncased-vocab.txt\",\n}\n\nMODEL_PATHS = {\n    'bert-base-uncased': '../input/bertconfigs/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/',\n    'bert-large-uncased-whole-word-masking-finetuned-squad': '../input/bertconfigs/wwm_uncased_L-24_H-1024_A-16/wwm_uncased_L-24_H-1024_A-16/',\n    'albert-large-v2': '../input/albert-configs/albert-large-v2/albert-large-v2/',\n    'albert-base-v2': '../input/albert-configs/albert-base-v2/albert-base-v2/',\n    'distilbert': '../input/albert-configs/distilbert/distilbert/',\n}\n\nFL_TH = 0.75\nDATA_PREPARING = False\nREMOVE_LONG = False","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:33:05.790946Z","iopub.execute_input":"2021-06-01T07:33:05.791651Z","iopub.status.idle":"2021-06-01T07:33:05.79922Z","shell.execute_reply.started":"2021-06-01T07:33:05.791604Z","shell.execute_reply":"2021-06-01T07:33:05.798433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\nInstead of having labels at article level, I refine the definition to section level. This allows for less sparsity. ","metadata":{}},{"cell_type":"code","source":"def load_text(id_, root=\"\"):\n    with open(os.path.join(root, id_ + \".json\")) as f:\n        text = json.load(f)\n    return text\n\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\n\ndef create_data(df):\n    new_df = []\n\n    for idx in tqdm(range(len(df))):\n        article = load_text(df['Id'][idx], DATA_PATH_TRAIN)\n        id_, pub_title, dataset_title, dataset_label, cleaned_label = df.iloc[idx]\n        \n        \n        for i, section in enumerate(article):\n            text = section['text']\n            title = section['section_title']\n            \n            cleaned_text = clean_text(section['text'])\n            \n            found = cleaned_label in cleaned_text\n    \n            dic = {\n                \"id\": [id_], \n                \"section_id\": [i],\n                \"pub_title\": [pub_title], \n                \"dataset_title\": [dataset_title], \n                \"dataset_label\": [dataset_label], \n                \"cleaned_label\": [cleaned_label],\n                \"text\": [text],\n                \"cleaned_text\": [cleaned_text],\n                \"label_found\": [found],\n            }\n            new_df.append(pd.DataFrame.from_dict(dic))\n            \n    return pd.concat(new_df).reset_index(drop=True)\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:33:05.800968Z","iopub.execute_input":"2021-06-01T07:33:05.801382Z","iopub.status.idle":"2021-06-01T07:33:05.818694Z","shell.execute_reply.started":"2021-06-01T07:33:05.801352Z","shell.execute_reply":"2021-06-01T07:33:05.817732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# train\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n\ntrain = pd.read_csv(train_path)\nprint('train size before agg.:', len(train))\n\n# Group by publication, training labels should have the same form as expected output.\ntrain = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()    \nprint('train size after agg.:', len(train))\n\ntrain = train.sort_values(by=['Id'])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:33:05.82Z","iopub.execute_input":"2021-06-01T07:33:05.820403Z","iopub.status.idle":"2021-06-01T07:33:06.702949Z","shell.execute_reply.started":"2021-06-01T07:33:05.820374Z","shell.execute_reply":"2021-06-01T07:33:06.701943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pseudo","metadata":{}},{"cell_type":"code","source":"# train\npseudo_train_path = '../input/coleridge-pseudolabels/submission.csv'\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n\npseudo_train = pd.read_csv(pseudo_train_path)\nprint('pseudo_train size before drop_duplicates.:', len(pseudo_train))\n\npseudo_train = pseudo_train.drop_duplicates(subset='Id')\nprint('pseudo_train size after drop_duplicates:', len(pseudo_train), '\\n')\n\npseudo_train = pseudo_train.sort_values(by=['Id']).reset_index(drop=True)\npseudo_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:33:06.704258Z","iopub.execute_input":"2021-06-01T07:33:06.704553Z","iopub.status.idle":"2021-06-01T07:33:06.807319Z","shell.execute_reply.started":"2021-06-01T07:33:06.704523Z","shell.execute_reply":"2021-06-01T07:33:06.806127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train + Pseudo","metadata":{}},{"cell_type":"code","source":"if DATA_PREPARING:\n    \n    final_predictions = []\n    for pred_match, perd_mlm in tqdm(zip(train['cleaned_label'], pseudo_train['PredictionString'])):\n        if pred_match:\n            labels = pred_match.split('|')\n\n            # literal_preds + pred_mlm_labels\n            if perd_mlm:\n                filtered_labels = labels\n                labels_mlm = perd_mlm.split('|')\n                for label_mlm in labels_mlm:\n                    if all(jaccard_similarity(label_mlm, got_label) < FL_TH for got_label in labels):\n                        filtered_labels.append(label_mlm)\n\n            # literal_preds\n            else: filtered_labels = labels\n\n        # pred_mlm_labels\n        elif perd_mlm:\n            filtered_labels = perd_mlm.split('|')\n\n        # ''\n        else:\n            filtered_labels = []\n\n        final_predictions.append('|'.join(filtered_labels))\n\n    train['cleaned_label'] = final_predictions\n\n    print(f'FL_TH = {FL_TH}: \\n{final_predictions[:4]}')\n    del pseudo_train, final_predictions\n\nelse:\n    del train, pseudo_train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:33:06.809981Z","iopub.execute_input":"2021-06-01T07:33:06.810253Z","iopub.status.idle":"2021-06-01T07:33:07.046616Z","shell.execute_reply.started":"2021-06-01T07:33:06.810227Z","shell.execute_reply":"2021-06-01T07:33:07.045558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare For create_data","metadata":{}},{"cell_type":"code","source":"if DATA_PREPARING:\n    \n    train['cleaned_label_list'] = train['cleaned_label'].apply(lambda label_str: label_str.split('|'))\n\n    train_temp = copy.deepcopy(train)\n\n    for index, row in tqdm(train.iterrows()):\n        label_list = row['cleaned_label_list']\n        if len(label_list) == 1:\n            train_temp['cleaned_label'][index] = label_list[0]\n        elif len(label_list) > 1:\n            train_temp['cleaned_label'][index] = label_list[0]\n            for label in label_list[1:]:\n                row['cleaned_label'] = label\n                train_temp = train_temp.append(row, ignore_index=True)\n\n    train = train_temp.drop('cleaned_label_list', axis=1)\n    del train_temp","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:33:07.04939Z","iopub.execute_input":"2021-06-01T07:33:07.049729Z","iopub.status.idle":"2021-06-01T07:34:18.702154Z","shell.execute_reply.started":"2021-06-01T07:33:07.049699Z","shell.execute_reply":"2021-06-01T07:34:18.700737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_PREPARING:\n    \n    new_df = create_data(train)  # Quite slow, could be sped-up with multi-processing.\n\n    del train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:34:18.704483Z","iopub.execute_input":"2021-06-01T07:34:18.704944Z","iopub.status.idle":"2021-06-01T07:54:07.105353Z","shell.execute_reply.started":"2021-06-01T07:34:18.704896Z","shell.execute_reply":"2021-06-01T07:54:07.100711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_PREPARING:\n    sns.countplot(x=new_df['label_found'])\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T07:54:07.127233Z","iopub.execute_input":"2021-06-01T07:54:07.127637Z","iopub.status.idle":"2021-06-01T07:54:07.368484Z","shell.execute_reply.started":"2021-06-01T07:54:07.127604Z","shell.execute_reply":"2021-06-01T07:54:07.367452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A lot of sections have no label found, I won't use them for training.","metadata":{}},{"cell_type":"code","source":"if DATA_PREPARING:\n    print(f'len(new_df) b/ removing label_not_found = {len(new_df)}')\n\n    df = new_df[new_df['label_found']].reset_index(drop=True)\n    print(f'len(df) b/ removing long texts = {len(df)}')\n    del new_df\n\n    if REMOVE_LONG:\n        df['length'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n        df = df[df['length'] < 3000]  # remove too long texts\n        print(f'len(df) a/ removing long texts = {len(df)}')\n\n    df.to_csv(\"df_train.csv\", index=False)  # saving, just in case\n    \nelse: df = None\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:54:07.369785Z","iopub.execute_input":"2021-06-01T07:54:07.370077Z","iopub.status.idle":"2021-06-01T07:54:59.440068Z","shell.execute_reply.started":"2021-06-01T07:54:07.370049Z","shell.execute_reply":"2021-06-01T07:54:59.439036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## External Data","metadata":{}},{"cell_type":"code","source":"def find_data_sets_id(publication_id: int) -> list:\n    data_set_ids = []\n    for class_ in data_set_citations:\n        if class_['publication_id'] == publication_id:\n            data_set_ids.append(class_['data_set_id'])\n    return data_set_ids\n\n\ndef find_data_set_citations_mention_list(publication_id: int) -> str:\n    mention_list = []\n    for class_ in data_set_citations:\n        if class_['publication_id'] == publication_id:\n            cleaned_labels = list( map(clean_text, class_['mention_list']) )\n            mention_list.append( '|'.join(cleaned_labels) )\n    return '|'.join( [label for label in mention_list if label != ''] )\n\n\ndef find_data_sets_title(data_set_id: int) -> str:\n    for class_ in data_sets:\n        if class_['data_set_id'] == data_set_id:\n            return class_['title']\n\n\ndef RichContextDF(publications) -> pd.DataFrame:\n    '''\n    Use publications.json do generate a training set DF\n    '''\n    data_sets_title = []\n    cleande_labels = []\n    cleaned_text = []\n    for class_ in tqdm(publications):\n        publication_id = class_['publication_id']\n        \n        # to get data_sets_title\n        data_sets_title_temp = []\n        for data_sets_id in find_data_sets_id(publication_id):\n            data_sets_title_temp.append( find_data_sets_title(data_sets_id) )\n        data_sets_title.append('||'.join(data_sets_title_temp))\n        \n        # to get cleande_labels\n        cleande_labels.append( find_data_set_citations_mention_list(publication_id) )\n        \n        # to get text\n        with open(f'../input/rich-context-competition-train-testtargz/train_test/files/text/{publication_id}.txt', 'r') as f:\n            text = f.readlines()\n        text = [line[:-1] for line in text]\n        text = list(map(clean_text, text))\n        text = ' '.join(text)        \n        cleaned_text.append(text)\n\n    return pd.DataFrame({\n        'dataset_title': data_sets_title,\n        'cleaned_label': cleande_labels,\n        'cleaned_text': cleaned_text,\n    })","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:54:59.442024Z","iopub.execute_input":"2021-06-01T07:54:59.442618Z","iopub.status.idle":"2021-06-01T07:54:59.456763Z","shell.execute_reply.started":"2021-06-01T07:54:59.44257Z","shell.execute_reply":"2021-06-01T07:54:59.455878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_PREPARING:\n    with open(f'../input/rich-context-competition-train-testtargz/train_test/publications.json', 'r') as f:\n        publications = json.load(f)\n    with open(f'../input/rich-context-competition-train-testtargz/train_test/data_set_citations.json', 'r') as f:\n        data_set_citations = json.load(f)\n    with open(f'../input/rich-context-competition-train-testtargz/train_test/data_sets.json', 'r') as f:\n        data_sets = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:54:59.458163Z","iopub.execute_input":"2021-06-01T07:54:59.458465Z","iopub.status.idle":"2021-06-01T07:55:00.790426Z","shell.execute_reply.started":"2021-06-01T07:54:59.458438Z","shell.execute_reply":"2021-06-01T07:55:00.789265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_PREPARING:\n    RichContext_train = RichContextDF(publications)\n    RichContext_train = RichContext_train[ RichContext_train['cleaned_label'] != '' ]\n    \nelse: RichContext_train = None\n\nRichContext_train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:55:00.795453Z","iopub.execute_input":"2021-06-01T07:55:00.795827Z","iopub.status.idle":"2021-06-01T07:56:02.410501Z","shell.execute_reply.started":"2021-06-01T07:55:00.795795Z","shell.execute_reply":"2021-06-01T07:56:02.409448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train + Pseudo + External","metadata":{}},{"cell_type":"code","source":"# prepare for required format\n\nif DATA_PREPARING:\n    RichContext_train['cleaned_label_list'] = RichContext_train['cleaned_label'].apply(lambda label_str: label_str.split('|'))\n\n    RichContext_train_temp = copy.deepcopy(RichContext_train)\n\n    for index, row in tqdm( RichContext_train.iterrows() ):\n        label_list = row['cleaned_label_list']\n        if len(label_list) == 1:\n            RichContext_train_temp['cleaned_label'][index] = label_list[0]\n        elif len(label_list) > 1:\n            RichContext_train_temp['cleaned_label'][index] = label_list[0]\n            for label in label_list[1:]:\n                row['cleaned_label'] = label\n                RichContext_train_temp = RichContext_train_temp.append(row, ignore_index=True)\n\n    RichContext_train = RichContext_train_temp.drop('cleaned_label_list', axis=1)\n    del RichContext_train_temp\n\nRichContext_train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:56:02.412266Z","iopub.execute_input":"2021-06-01T07:56:02.412582Z","iopub.status.idle":"2021-06-01T07:57:07.116982Z","shell.execute_reply.started":"2021-06-01T07:56:02.412553Z","shell.execute_reply":"2021-06-01T07:57:07.116008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_found and length\n\nif DATA_PREPARING:\n    label_found = []\n    for index, row in RichContext_train[['cleaned_label', 'cleaned_text']].iterrows():\n        cleaned_label = row['cleaned_label']\n        cleaned_text = row['cleaned_text']\n        found = cleaned_label in cleaned_text\n        label_found.append(found)\n    RichContext_train['label_found'] = label_found\n    del label_found\n    print(f'len(RichContext_train) b/ removing label_not_found = {len(RichContext_train)}')\n    \n    RichContext_train = RichContext_train[RichContext_train['label_found']].reset_index(drop=True)\n    print(f'len(RichContext_train) b/ removing long texts = {len(RichContext_train)}')\n\n    RichContext_train['length'] = RichContext_train['cleaned_text'].apply(lambda x: len(x.split()))\n\n    RichContext_train.to_csv(\"RichContext_train.csv\", index=False)\n    \nRichContext_train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:57:07.118923Z","iopub.execute_input":"2021-06-01T07:57:07.119358Z","iopub.status.idle":"2021-06-01T07:57:59.798199Z","shell.execute_reply.started":"2021-06-01T07:57:07.119311Z","shell.execute_reply":"2021-06-01T07:57:59.796796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATA_PREPARING:\n    df = df.append(RichContext_train, ignore_index=True)\n    del RichContext_train\n    df.to_csv(\"df_train_final.csv\", index=False)\n    \nelse: df = pd.read_csv('../input/bert-for-question-answering-baseline-training/df_train_final.csv')\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:57:59.800185Z","iopub.execute_input":"2021-06-01T07:57:59.800645Z","iopub.status.idle":"2021-06-01T07:59:23.909455Z","shell.execute_reply.started":"2021-06-01T07:57:59.800599Z","shell.execute_reply":"2021-06-01T07:59:23.908606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training a Model","metadata":{}},{"cell_type":"markdown","source":"# Utils","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model.\n\n    Args:\n        model (torch model): Model to save the weights of.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to save to. Defaults to \"\".\n    \"\"\"\n\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))\n\n\ndef count_parameters(model, all=False):\n    \"\"\"\n    Count the parameters of a model.\n\n    Args:\n        model (torch model): Model to count the parameters of.\n        all (bool, optional):  Whether to count not trainable parameters. Defaults to False.\n\n    Returns:\n        int: Number of parameters.\n    \"\"\"\n\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.910973Z","iopub.execute_input":"2021-06-01T07:59:23.911245Z","iopub.status.idle":"2021-06-01T07:59:23.921052Z","shell.execute_reply.started":"2021-06-01T07:59:23.911219Z","shell.execute_reply":"2021-06-01T07:59:23.919634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"class EncodedText:\n    def __init__(self, ids, offsets):\n        self.ids = ids\n        self.offsets = offsets\n\n\ndef create_tokenizer_and_tokens(config):\n    if \"roberta\" in config.selected_model:\n        raise NotImplementedError\n        \n    elif \"albert\" in config.selected_model:\n        raise NotImplementedError\n        \n    else:\n        tokenizer = BertWordPieceTokenizer(\n            MODEL_PATHS[config.selected_model] + 'vocab.txt',\n            lowercase=config.lowercase,\n        )\n\n        tokens = {\n            'cls': tokenizer.token_to_id('[CLS]'),\n            'sep': tokenizer.token_to_id('[SEP]'),\n            'pad': tokenizer.token_to_id('[PAD]'),\n        }\n    \n    return tokenizer, tokens","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.922403Z","iopub.execute_input":"2021-06-01T07:59:23.922723Z","iopub.status.idle":"2021-06-01T07:59:23.938814Z","shell.execute_reply.started":"2021-06-01T07:59:23.922681Z","shell.execute_reply":"2021-06-01T07:59:23.937795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Locating labels","metadata":{}},{"cell_type":"code","source":"def locate_label_string(text, label):\n    \"\"\"\n    Finds the label in the text\n    \"\"\"\n    len_label = len(label) - 1\n\n    candidates_idx = [i for i, e in enumerate(text) if e == label[1]]\n    for idx in candidates_idx:\n        if \" \" + text[idx: idx + len_label] == label:\n            idx_start = idx\n            idx_end = idx + len_label\n            break\n\n    assert (\n        text[idx_start:idx_end] == label[1:]\n    ), f'\"{text[idx_start: idx_end]}\" instead of \"{label}\" in \"{text}\"'\n\n    char_targets = np.zeros(len(text))\n    char_targets[idx_start:idx_end] = 1\n\n    return idx_start, idx_end, char_targets\n\n\ndef locate_label_tokens(offsets, char_targets):\n    \"\"\"\n    Finds the tokens corresponding to the found labels\n    \"\"\"\n    target_idx = []\n    for idx, (offset1, offset2) in enumerate(offsets):\n        if sum(char_targets[offset1:offset2]) > 0:\n            target_idx.append(idx)\n\n    if not len(target_idx):\n        for idx, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(idx)\n\n    return target_idx[0], target_idx[-1]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.940501Z","iopub.execute_input":"2021-06-01T07:59:23.941063Z","iopub.status.idle":"2021-06-01T07:59:23.958338Z","shell.execute_reply.started":"2021-06-01T07:59:23.941028Z","shell.execute_reply":"2021-06-01T07:59:23.957178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process sample","metadata":{}},{"cell_type":"code","source":"def process_data(\n    text,\n    label,\n    tokenizer,\n    tokens,\n    max_len=100,\n    model_name=\"bert\",\n):\n    \"\"\"\n    Prepares the data for the question answering task.\n    Adapted from Abishek's work on the Tweet Sentiment extraction competition, \n    check his work for more details !\n    \"\"\"\n    target_start, target_end = 0, 0\n    text = \" \" + \" \".join(str(text).split())\n    label = \" \" + \" \".join(str(label).split())\n\n    if label != \" \":\n        idx_start, idx_end, char_targets = locate_label_string(\n            text, label\n        )\n\n    tokenized = tokenizer.encode(text)\n    input_ids_text = tokenized.ids[1:-1]\n\n    # print(input_ids_text, len(input_ids_text))\n\n    offsets = tokenized.offsets[1:-1]\n\n    if label != \" \":\n        target_start, target_end = locate_label_tokens(offsets, char_targets)\n\n    if target_end >= max_len - 2:  # target is too far in the sentence, we crop its beginning.\n        n_tok_to_crop = target_start - max_len // 2\n        new_str_start = offsets[n_tok_to_crop][0]\n\n        input_ids_text = input_ids_text[n_tok_to_crop:]\n\n        offsets = [tuple(t) for t in np.array(offsets[n_tok_to_crop:]) - new_str_start]\n        text = text[new_str_start:]\n\n        target_start -= n_tok_to_crop\n        target_end -= n_tok_to_crop\n\n    input_ids = (\n        [tokens[\"cls\"]]\n        + input_ids_text[:max_len - 2]\n        + [tokens[\"sep\"]]\n    )\n\n    if \"roberta\" in model_name:\n        token_type_ids = [0] * len(input_ids)\n    else:\n        token_type_ids = [1] * len(input_ids)\n\n    text_offsets = [(0, 0)] + offsets[:max_len - 2] + [(0, 0)]\n\n    target_start += 1\n    target_end += 1\n\n    # target_end = min(target_end, max_len - 1)\n\n    assert len(input_ids) == len(token_type_ids) and len(input_ids) == len(text_offsets), (len(input_ids), len(text_offsets))  # noqa\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        text_offsets = text_offsets + ([(0, 0)] * padding_length)\n\n    return {\n        \"ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"targets_start\": target_start,\n        \"targets_end\": target_end,\n        \"text\": text,\n        \"label\": label,\n        \"offsets\": text_offsets,\n    }","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.959646Z","iopub.execute_input":"2021-06-01T07:59:23.96003Z","iopub.status.idle":"2021-06-01T07:59:23.976193Z","shell.execute_reply.started":"2021-06-01T07:59:23.959996Z","shell.execute_reply":"2021-06-01T07:59:23.974969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class SectionDataset(Dataset):\n    def __init__(\n        self,\n        df,\n        tokenizer,\n        tokens,\n        max_len=512,\n        model_name=\"bert\",\n    ):\n        self.tokens = tokens\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.model_name = model_name\n\n        self.texts = df[\"cleaned_text\"].values\n        self.labels = df[\"cleaned_label\"].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        data = process_data(\n            self.texts[idx],\n            self.labels[idx],\n            self.tokenizer,\n            self.tokens,\n            max_len=self.max_len,\n            model_name=self.model_name,\n        )\n\n        return {\n            \"ids\": torch.tensor(data[\"ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            \"target_start\": torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            \"target_end\": torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            \"text\": data[\"text\"],\n            \"label\": data[\"label\"],\n            \"offsets\": torch.tensor(data[\"offsets\"], dtype=torch.long),\n        }","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.977652Z","iopub.execute_input":"2021-06-01T07:59:23.977991Z","iopub.status.idle":"2021-06-01T07:59:23.992993Z","shell.execute_reply.started":"2021-06-01T07:59:23.977959Z","shell.execute_reply":"2021-06-01T07:59:23.991987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"TRANSFORMERS = {\n    \"roberta-base\": (RobertaModel, \"roberta-base\"),\n    'albert-base-v2': (AlbertModel, 'albert-base-v2'),\n    'albert-large-v2': (AlbertModel, 'albert-large-v2'),\n    'albert-xlarge-v2': (AlbertModel, 'albert-xlarge-v2'),\n    'albert-xxlarge-v2': (AlbertModel, 'albert-xxlarge-v2'),\n    \"bert-base-uncased\": (BertModel, \"bert-base-uncased\"),\n    \"bert-base-cased\": (BertModel, \"bert-base-cased\"),\n    \"bert-large-uncased-whole-word-masking\": (BertModel, \"bert-large-uncased-whole-word-masking\"),\n    \"distilbert-base-uncased-distilled-squad\": (\n        DistilBertModel,\n        \"distilbert-base-uncased-distilled-squad\"\n    )\n}\n\n\nclass QATransformer(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.name = model\n\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        model_class, pretrained_weights = TRANSFORMERS[model]\n\n        self.transformer = model_class.from_pretrained(\n            pretrained_weights, output_hidden_states=True\n        )\n\n        self.nb_features = self.transformer.pooler.dense.out_features\n\n        self.logits = nn.Sequential(\n            nn.Linear(self.nb_features, self.nb_features),\n            nn.Tanh(),\n            nn.Linear(self.nb_features, 2),\n        )\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n        logits = self.logits(features)\n\n        start_logits, end_logits = logits[:, :, 0], logits[:, :, 1]\n\n        return start_logits, end_logits","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:23.994174Z","iopub.execute_input":"2021-06-01T07:59:23.994678Z","iopub.status.idle":"2021-06-01T07:59:24.007647Z","shell.execute_reply.started":"2021-06-01T07:59:23.994643Z","shell.execute_reply":"2021-06-01T07:59:24.006584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"def jaccard_similarity(s1, s2): # could be wrong, see CPMP's thread\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")\n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\n\ndef jaccard_similarity(str1, str2): # the right one\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef compute_score(y_true, y_pred, beta=0.5):\n    \"\"\"\n    From https://www.kaggle.com/tungmphung/coleridge-initiative-local-score-computation/\n    \"\"\"\n    TP, FP, FN = 0, 0, 0\n\n    for truth, pred in zip(y_true, y_pred):\n        true_datasets = truth.split('|')\n        # Predicted strings for each publication are sorted alphabetically\n        # and processed in that order.\n        pred_datasets = sorted(pred.split('|'))\n\n        for true_dataset in true_datasets:\n            if len(pred_datasets):\n                match_scores = [jaccard_similarity(true_dataset, pred_dataset)\n                                for pred_dataset in pred_datasets]\n                # The prediction with the highest score for a given ground truth\n                # is matched with that ground truth.\n                match_index = np.argmax(match_scores)\n\n                if match_scores[match_index] >= 0.5:\n                    # Any matched predictions where the Jaccard score meets or\n                    # exceeds the threshold of 0.5 are counted as true positives (TP),\n                    TP += 1\n                else:\n                    # the remainder as false positives (FP).\n                    FP += 1\n\n                del(pred_datasets[match_index])\n            else:\n                # Any ground truths with no nearest predictions are counted as\n                # false negatives (FN).\n                FN += 1\n        # Any unmatched predictions are counted as false positives (FP).\n        FP += len(pred_datasets)\n\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f_score = (1 + beta**2)*(precision*recall)/((beta**2)*precision + recall)\n\n    return f_score","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.009149Z","iopub.execute_input":"2021-06-01T07:59:24.009679Z","iopub.status.idle":"2021-06-01T07:59:24.028541Z","shell.execute_reply.started":"2021-06-01T07:59:24.009645Z","shell.execute_reply":"2021-06-01T07:59:24.027244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preds from probas","metadata":{}},{"cell_type":"code","source":"def get_string_from_idx(text, idx_start, idx_end, offsets):\n    \"\"\"\n    Uses the offsets to retrieve the predicted string based on the start and end indices\n    \"\"\"\n    if idx_end < idx_start:\n        idx_end = idx_start\n\n    predicted_string = \"\"\n    for i in range(idx_start, idx_end + 1):\n        predicted_string += text[offsets[i][0]: offsets[i][1]]\n        if i + 1 < len(offsets) and offsets[i][1] < offsets[i + 1][0]:\n            predicted_string += \" \"\n\n    return predicted_string\n\ndef get_pred_from_logits(data, start_logits, end_logits, from_proba=False):\n    if not from_proba:\n        start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n        end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n\n    offsets = data[\"offsets\"].cpu().numpy()\n\n    preds = []\n    for i in range(len(start_logits)):\n        start_idx = np.argmax(start_logits[i])\n        end_idx = np.argmax(end_logits[i])\n        preds.append(get_string_from_idx(data[\"text\"][i], start_idx, end_idx, offsets[i]))\n\n    return preds","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.03055Z","iopub.execute_input":"2021-06-01T07:59:24.030908Z","iopub.status.idle":"2021-06-01T07:59:24.049259Z","shell.execute_reply.started":"2021-06-01T07:59:24.03087Z","shell.execute_reply":"2021-06-01T07:59:24.047975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss","metadata":{}},{"cell_type":"code","source":"def ce_loss(\n    pred, truth, smoothing=False, trg_pad_idx=-1, eps=0.1\n):\n    \"\"\"\n    Computes the cross entropy loss with label smoothing\n\n    Args:\n        pred (torch tensor): Prediction\n        truth (torch tensor): Target\n        smoothing (bool, optional): Whether to use smoothing. Defaults to False.\n        trg_pad_idx (int, optional): Indices to ignore in the loss. Defaults to -1.\n        eps (float, optional): Smoothing coefficient. Defaults to 0.1.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    truth = truth.contiguous().view(-1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n\n    if smoothing:\n        n_class = pred.size(1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n\n    loss = -one_hot * F.log_softmax(pred, dim=1)\n\n    if trg_pad_idx >= 0:\n        loss = loss.sum(dim=1)\n        non_pad_mask = truth.ne(trg_pad_idx)\n        loss = loss.masked_select(non_pad_mask)\n\n    return loss.sum()\n\n\ndef qa_loss_fn(start_logits, end_logits, start_positions, end_positions, config):\n    \"\"\"\n    Loss function for the question answering task.\n    It is the sum of the cross entropy for the start and end logits\n\n    Args:\n        start_logits (torch tensor): Start logits\n        end_logits (torch tensor): End logits\n        start_positions (torch tensor): Start ground truth\n        end_positions (torch tensor): End ground truth\n        config (dict): Dictionary of parameters for the CE Loss.\n\n    Returns:\n        torch tensor: Loss value\n    \"\"\"\n    bs = start_logits.size(0)\n\n    start_loss = ce_loss(\n        start_logits,\n        start_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n    )\n\n    end_loss = ce_loss(\n        end_logits,\n        end_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n    )\n\n    total_loss = start_loss + end_loss\n\n    return total_loss / bs\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.050973Z","iopub.execute_input":"2021-06-01T07:59:24.051368Z","iopub.status.idle":"2021-06-01T07:59:24.064083Z","shell.execute_reply.started":"2021-06-01T07:59:24.051331Z","shell.execute_reply":"2021-06-01T07:59:24.062938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trim tensors\n\nHelps for speedup","metadata":{}},{"cell_type":"code","source":"def trim_tensors(tokens, input_ids, model_name='bert', min_len=10):\n    \"\"\"\n    Trim tensors so that within a batch, padding is shortened.\n    This speeds up training for RNNs and Transformers\n\n    Arguments:\n        tokens {torch tensor} -- Text tokens\n\n    Keyword Arguments:\n        min_len {int} -- Minimum length to trim to (default: {10})\n\n    Returns:\n        torch tensor -- trimmed tokens\n    \"\"\"\n    pad_token = 1 if \"roberta\" in model_name else 0\n    max_len = max(torch.max(torch.sum((tokens != pad_token), 1)), min_len)\n    return tokens[:, :max_len], input_ids[:, :max_len]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.065759Z","iopub.execute_input":"2021-06-01T07:59:24.066111Z","iopub.status.idle":"2021-06-01T07:59:24.082132Z","shell.execute_reply.started":"2021-06-01T07:59:24.066079Z","shell.execute_reply":"2021-06-01T07:59:24.081015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit function","metadata":{}},{"cell_type":"code","source":"def fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_config,\n    epochs=5,\n    batch_size=8,\n    weight_decay=0,\n    warmup_prop=0.0,\n    lr=5e-4,\n    cfg=dict()\n):\n\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS, shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n\n    opt_params = []\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    for n, p in model.named_parameters():\n        wd = 0 if any(nd in n for nd in no_decay) else weight_decay\n        opt_params.append(\n            {\"params\": [p], \"weight_decay\": wd, \"lr\": lr}\n        )\n\n    optimizer = AdamW(opt_params, lr=lr, betas=(0.5, 0.999))\n\n    n_steps = epochs * len(train_loader)\n    num_warmup_steps = int(warmup_prop * n_steps)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, n_steps\n    )\n\n    total_steps = 0\n    for epoch in range(epochs):\n        model.train()\n        start_time = time.time()\n\n        optimizer.zero_grad()\n        avg_loss = 0\n\n        for step, data in enumerate(train_loader):\n            total_steps += 1\n\n            ids, token_type_ids = trim_tensors(\n                data[\"ids\"], data[\"token_type_ids\"], model.name\n            )\n\n            start_logits, end_logits = model(ids.cuda(), token_type_ids.cuda())\n\n            loss = qa_loss_fn(\n                start_logits,\n                end_logits,\n                data[\"target_start\"].cuda(),\n                data[\"target_end\"].cuda(),\n                config=loss_config,\n            )\n\n            avg_loss += loss.item() / len(train_loader)\n            loss.backward()\n\n            nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n\n        model.eval()\n        avg_val_loss = 0.\n        preds, truths = [], []\n\n        with torch.no_grad():\n\n            for data in val_loader:\n                ids, token_type_ids = trim_tensors(\n                    data[\"ids\"], data[\"token_type_ids\"], model.name\n                )\n\n                start_logits, end_logits = model(ids.cuda(), token_type_ids.cuda())\n\n                loss = qa_loss_fn(\n                    start_logits.detach(),\n                    end_logits.detach(),\n                    data[\"target_start\"].cuda(),\n                    data[\"target_end\"].cuda(),\n                    config=loss_config,\n                )\n\n                avg_val_loss += loss.item() / len(val_loader)\n\n                preds += get_pred_from_logits(data, start_logits, end_logits)\n                truths += data['label']\n\n        score = compute_score(truths, preds)\n\n        dt = time.time() - start_time\n        lr = scheduler.get_last_lr()[0]\n        print(f\"Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t\", end=\"\")\n        print(\n            f\"loss={avg_loss:.3f} \\t val_loss={avg_val_loss:.3f} \\t val_score={score:.4f}\"\n        )\n        \n        save_model_weights(model, f\"{cfg['selected_model']}_{cfg['fold']}_{epoch}.pt\", cp_folder=\"\")\n\n    del loss, data, avg_val_loss, avg_loss, train_loader, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return preds\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.084051Z","iopub.execute_input":"2021-06-01T07:59:24.084345Z","iopub.status.idle":"2021-06-01T07:59:24.106023Z","shell.execute_reply.started":"2021-06-01T07:59:24.084317Z","shell.execute_reply":"2021-06-01T07:59:24.105077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## $k$-fold","metadata":{}},{"cell_type":"code","source":"def k_fold(config, df, save=True):\n    tokenizer, tokens = create_tokenizer_and_tokens(config)\n    \n    time = re.sub(' ', '_', str(datetime.datetime.now())[:16])\n    score = 0\n    \n    gkf = GroupKFold(n_splits=config.k)\n    folds = list(gkf.split(X=df, groups=df['dataset_title']))\n    \n    pred_oof = [''] * len(df)\n    \n    for fold, (train_idx, val_idx) in enumerate(folds):\n        if fold in config.selected_folds:\n            print(f\"\\n-------------   Fold {fold + 1} / {len(folds)}  -------------\\n\")\n            seed_everything(config.seed + fold)\n\n            model = QATransformer(config.selected_model).cuda()\n            model.zero_grad()\n\n            train_dataset = SectionDataset(\n                df.iloc[train_idx], \n                tokenizer, \n                tokens, \n                max_len=config.max_len, \n                model_name=config.selected_model\n            )\n\n            val_dataset = SectionDataset(\n                df.iloc[val_idx], \n                tokenizer, \n                tokens,\n                max_len=config.max_len, \n                model_name=config.selected_model\n            )\n\n            n_parameters = count_parameters(model)\n\n            print(f\"    -> {len(train_dataset)} training texts\")\n            print(f\"    -> {len(val_dataset)} validation texts\")\n            print(f\"    -> {n_parameters} trainable parameters\\n\")  \n\n            preds = fit(\n                model, \n                train_dataset, \n                val_dataset, \n                config.loss_config,\n                epochs=config.epochs, \n                batch_size=config.batch_size, \n                weight_decay=config.weight_decay, \n                lr=config.lr, \n                warmup_prop=config.warmup_prop,\n                cfg={'selected_model': config.selected_model,\n                    'fold': fold}\n            )\n\n            for i, idx in enumerate(val_idx):\n                pred_oof[idx] = preds[i]\n\n            if save:\n                save_model_weights(model, f'{config.selected_model}_{fold + 1}.pt', cp_folder=\"\")\n\n            del model, train_dataset, val_dataset\n            torch.cuda.empty_cache()\n            gc.collect()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T07:59:24.107881Z","iopub.execute_input":"2021-06-01T07:59:24.108504Z","iopub.status.idle":"2021-06-01T07:59:24.124928Z","shell.execute_reply.started":"2021-06-01T07:59:24.108459Z","shell.execute_reply":"2021-06-01T07:59:24.123835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"class Config:\n    # General\n    k = 5\n    seed = 2021\n    selected_folds = [0] # 0\n\n    # Texts\n    max_len = 256\n    \n    # Architecture\n    selected_model = \"bert-base-uncased\"\n    lowercase = True\n    \n    # Loss function\n    loss_config = {\n        \"smoothing\": False,\n        \"eps\": 0.1,\n    }\n    \n    # Training\n    batch_size = 16\n    batch_size_val = batch_size * 2\n    weight_decay = 1.\n    \n    epochs = 5\n    lr = 5e-5\n    warmup_prop = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:59:24.126599Z","iopub.execute_input":"2021-06-01T07:59:24.127099Z","iopub.status.idle":"2021-06-01T07:59:24.140681Z","shell.execute_reply.started":"2021-06-01T07:59:24.127065Z","shell.execute_reply":"2021-06-01T07:59:24.139917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"df_train.csv\")\n\nk_fold(\n    Config,\n    df,\n    save=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T07:59:24.142538Z","iopub.execute_input":"2021-06-01T07:59:24.143414Z","iopub.status.idle":"2021-06-01T07:59:29.641518Z","shell.execute_reply.started":"2021-06-01T07:59:24.143352Z","shell.execute_reply":"2021-06-01T07:59:29.639762Z"},"trusted":true},"execution_count":null,"outputs":[]}]}