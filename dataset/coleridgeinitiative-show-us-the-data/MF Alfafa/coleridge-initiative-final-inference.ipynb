{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# additional python packages\n!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install -q ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install -q ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install -q ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-25T05:38:02.088978Z","iopub.execute_input":"2021-09-25T05:38:02.08963Z","iopub.status.idle":"2021-09-25T05:39:41.633016Z","shell.execute_reply.started":"2021-09-25T05:38:02.089479Z","shell.execute_reply":"2021-09-25T05:39:41.631776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport simplejson\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\n# dataset manipulation\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pytorch\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n\nfrom typing import List\nimport string\nfrom functools import partial\n\nimport pickle\nfrom joblib import Parallel, delayed\n\nfrom collections import defaultdict, Counter\nimport gc\n\n# tf\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# language preprocessing\nimport nltk\n\nfrom typing import *\n\n# spacy\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\n\n# set seed\nsns.set()\nrandom.seed(123)\nnp.random.seed(456)\ntorch.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nprint('packages loaded')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T05:39:54.737754Z","iopub.execute_input":"2021-09-25T05:39:54.738064Z","iopub.status.idle":"2021-09-25T05:40:09.271126Z","shell.execute_reply.started":"2021-09-25T05:39:54.738017Z","shell.execute_reply":"2021-09-25T05:40:09.269939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n\nCOMPUTE_CV = False\n\nif len(sample_submission)>4: COMPUTE_CV = False\nif COMPUTE_CV:\n    print('this submission notebook will compute CV score but commit notebook will not')\nelse:\n    print('this submission notebook will only be used to submit result')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:37.882455Z","iopub.execute_input":"2021-09-25T01:45:37.882741Z","iopub.status.idle":"2021-09-25T01:45:37.896349Z","shell.execute_reply.started":"2021-09-25T01:45:37.882706Z","shell.execute_reply":"2021-09-25T01:45:37.895528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\n\nif COMPUTE_CV:\n    sample_submission = train\n    paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n    test_files_path = paper_test_folder\nelse:\n    sample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n    paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\n    test_files_path = paper_test_folder\n\nadnl_govt_labels_path = '../input/bigger-govt-dataset-list/data_set_800.csv'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:37.898369Z","iopub.execute_input":"2021-09-25T01:45:37.898657Z","iopub.status.idle":"2021-09-25T01:45:38.029529Z","shell.execute_reply.started":"2021-09-25T01:45:37.898623Z","shell.execute_reply":"2021-09-25T01:45:38.028767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SAMPLE = 0\n\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.030769Z","iopub.execute_input":"2021-09-25T01:45:38.031112Z","iopub.status.idle":"2021-09-25T01:45:38.042692Z","shell.execute_reply.started":"2021-09-25T01:45:38.031077Z","shell.execute_reply":"2021-09-25T01:45:38.041832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for paper_id in tqdm(sample_submission['Id']):\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.044241Z","iopub.execute_input":"2021-09-25T01:45:38.044602Z","iopub.status.idle":"2021-09-25T01:45:38.088786Z","shell.execute_reply.started":"2021-09-25T01:45:38.044567Z","shell.execute_reply":"2021-09-25T01:45:38.088087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional goverent dataset","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.0899Z","iopub.execute_input":"2021-09-25T01:45:38.090141Z","iopub.status.idle":"2021-09-25T01:45:38.094897Z","shell.execute_reply.started":"2021-09-25T01:45:38.090109Z","shell.execute_reply":"2021-09-25T01:45:38.093929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp3 = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\n\ntmp3_ = [x for x in tmp3['cleaned_label'].unique() if len(str(x).split()) > 0]\ntmp3_ += [x for x in tmp3['dataset_title'].unique()]\ntmp3 = [clean_text(x) for x in np.unique(tmp3_)]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.096764Z","iopub.execute_input":"2021-09-25T01:45:38.097218Z","iopub.status.idle":"2021-09-25T01:45:38.177308Z","shell.execute_reply.started":"2021-09-25T01:45:38.097182Z","shell.execute_reply":"2021-09-25T01:45:38.176585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp8 = pd.read_csv('../input/ci-ext-datasets-found-in-train-v2/train_ext_data.csv')\ntmp8['ext_cleaned_label'] = tmp8['ext_cleaned_label'].apply(lambda x: x.split('|'))\nall_labels = []\nfor labels in tmp8['ext_cleaned_label'].values:\n    for l in labels:\n        all_labels.append(l)\ntmp8 = list(np.unique(all_labels))\ntmp8 = pd.DataFrame(tmp8, columns=['title'])\ntmp8.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.178775Z","iopub.execute_input":"2021-09-25T01:45:38.179052Z","iopub.status.idle":"2021-09-25T01:45:38.374316Z","shell.execute_reply.started":"2021-09-25T01:45:38.179018Z","shell.execute_reply":"2021-09-25T01:45:38.373661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp8_ = []\nfor l in tmp8['title'].values:\n    if l not in tmp3:\n        tmp8_.append(l)\n        \nprint(len(tmp8_))\ntmp8_ = pd.DataFrame(tmp8_, columns=['title'])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.377876Z","iopub.execute_input":"2021-09-25T01:45:38.37959Z","iopub.status.idle":"2021-09-25T01:45:38.386754Z","shell.execute_reply.started":"2021-09-25T01:45:38.379562Z","shell.execute_reply":"2021-09-25T01:45:38.385387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_datasets = ['about', 'climatologists', 'control', 'exploration', 'defense', \n                'american community', 'american landscape', 'current population survey',\n                'gulf of maine', 'argonne national laboratory s greet', \n                'annual wholesale trade',\n                'bird conservation areas', 'bird incidental take', 'new housing', 'business patterns',\n                'create', 'federal aid to states', 'freedom of information act', 'fruit and vegetable prices',\n                'guidance navigation and control', 'high school and beyond', 'human resource management', \n                'housing unit estimates', 'international data base', 'labor market analysts', 'major land uses',\n                'mars exploration program', 'new residential construction', 'oxygen delivery system',\n                'pilot boarding areas', 'profiles in science', 'state fact sheets', 'summary of business',\n                'tsunamis general', 'virtual grower', # 0.620\n                \n                'advanced monthly', \n                'advanced telecommunications', \n                'agricultural productivity',\n                'annual survey', \n                'breeding bird', \n                'bridged race population estimates', \n                'building permits survey',\n                'census of governments', \n                'clinical laboratory', 'coastal energy facilities', \n                'commodity costs and returns',\n                'comprehensive environmental', 'county typology codes', 'delta cost project', \n                'endangered species act',\n                'energy policy act', \n                'fertilizer', 'geostationary', 'landfire', 'occupational projections', \n                'marine mammal protection act', \n                'meat price', 'medication therapy', 'mexican american', \n                'milk cost',\n                'animal health', 'weather', 'national environmental policy', 'national outbreak', 'natural amenities scale',\n                'office', 'services file', 'stores', 'right whale', 'shuttle radar', 'solar dynamics',\n                'business owners', 'expedition', 'usa'\n               ]\nfor l in not_datasets:\n    tmp8_ = tmp8_[~tmp8_['title'].str.contains(l)]\n    \ntmp8_.loc[tmp8_['title'].str.contains('national assessment of educational progress'), 'title'] = 'national assessment of educational progress'\ntmp8_.loc[tmp8_['title'].str.contains('national postsecondary student aid study'), 'title'] = 'national postsecondary student aid study'\ntmp8_.loc[tmp8_['title'].str.contains('nursing home compare'), 'title'] = 'nursing home compare'\ntmp8_.loc[tmp8_['title'].str.contains('private school universe survey'), 'title'] = 'private school universe survey'\ntmp8_.loc[tmp8_['title'].str.contains('program for international student assessment'), 'title'] = 'program for international student assessment'\ntmp8_.loc[tmp8_['title'].str.contains('progress in international reading literacy study'), 'title'] = 'progress in international reading literacy study'\ntmp8_.loc[tmp8_['title'].str.contains('schools and staffing survey'), 'title'] = 'schools and staffing survey'\n\ntmp8_ = list(tmp8_['title'].unique())\nprint(len(tmp8_))","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.387875Z","iopub.execute_input":"2021-09-25T01:45:38.388251Z","iopub.status.idle":"2021-09-25T01:45:38.52953Z","shell.execute_reply.started":"2021-09-25T01:45:38.388219Z","shell.execute_reply":"2021-09-25T01:45:38.528623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_datasets = np.unique(tmp3 + tmp8_)\nall_datasets = np.unique([clean_text(x) for x in all_datasets])\nprint(len(all_datasets))\nall_datasets[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.530736Z","iopub.execute_input":"2021-09-25T01:45:38.531602Z","iopub.status.idle":"2021-09-25T01:45:38.547582Z","shell.execute_reply.started":"2021-09-25T01:45:38.531561Z","shell.execute_reply":"2021-09-25T01:45:38.546623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\ndef text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n    \ndef read_json_pub(filename, train_data_path=paper_train_folder, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.549181Z","iopub.execute_input":"2021-09-25T01:45:38.549494Z","iopub.status.idle":"2021-09-25T01:45:38.562898Z","shell.execute_reply.started":"2021-09-25T01:45:38.54944Z","shell.execute_reply":"2021-09-25T01:45:38.561808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Literal prediction","metadata":{}},{"cell_type":"code","source":"literal_preds = []\nto_append = []\nfor index, row in tqdm(sample_submission.iterrows()):\n    to_append = [row['Id'],'']\n    large_string = str(read_json_pub(row['Id'], test_files_path))\n    clean_string = text_cleaning(large_string)\n    for row2 in all_datasets:\n        query_string = str(row2)\n        if query_string in clean_string:\n            if to_append[1] != '' and clean_text(query_string) not in to_append[1]:\n                to_append[1] = to_append[1] + '|' + clean_text(query_string)\n            if to_append[1] == '':\n                to_append[1] = clean_text(query_string)\n    literal_preds.append(*to_append[1:])\nliteral_preds[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.565851Z","iopub.execute_input":"2021-09-25T01:45:38.566095Z","iopub.status.idle":"2021-09-25T01:45:38.700407Z","shell.execute_reply.started":"2021-09-25T01:45:38.566071Z","shell.execute_reply":"2021-09-25T01:45:38.699498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLM Roberta prediction","metadata":{}},{"cell_type":"code","source":"# Auxiliary functions\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.719783Z","iopub.execute_input":"2021-09-25T01:45:38.720222Z","iopub.status.idle":"2021-09-25T01:45:38.736086Z","shell.execute_reply.started":"2021-09-25T01:45:38.720058Z","shell.execute_reply":"2021-09-25T01:45:38.735361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths and Hyperparameters\nMAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/output']\nTEST_INPUT_SAVE_PATH = './input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/train_ner.json']\nVAL_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/train_ner.json']\n\nPREDICTION_SAVE_PATH = './pred'\nPREDICTION_FILE = 'test_predictions.txt'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.750864Z","iopub.execute_input":"2021-09-25T01:45:38.751256Z","iopub.status.idle":"2021-09-25T01:45:38.757119Z","shell.execute_reply.started":"2021-09-25T01:45:38.751222Z","shell.execute_reply":"2021-09-25T01:45:38.756427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.758309Z","iopub.execute_input":"2021-09-25T01:45:38.758818Z","iopub.status.idle":"2021-09-25T01:45:38.778058Z","shell.execute_reply.started":"2021-09-25T01:45:38.758782Z","shell.execute_reply":"2021-09-25T01:45:38.776853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 5] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study', 'from'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.77939Z","iopub.execute_input":"2021-09-25T01:45:38.779692Z","iopub.status.idle":"2021-09-25T01:45:38.834373Z","shell.execute_reply.started":"2021-09-25T01:45:38.779656Z","shell.execute_reply":"2021-09-25T01:45:38.833185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_os_env(\n    pretrained_path,\n    train_path,\n    val_path\n):\n    os.environ[\"MODEL_PATH\"] = f\"{pretrained_path}\"\n    os.environ[\"TRAIN_FILE\"] = f\"{train_path}\"\n    os.environ[\"VALIDATION_FILE\"] = f\"{val_path}\"\n    \n    os.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\n    os.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.836157Z","iopub.execute_input":"2021-09-25T01:45:38.836768Z","iopub.status.idle":"2021-09-25T01:45:38.842367Z","shell.execute_reply.started":"2021-09-25T01:45:38.836732Z","shell.execute_reply":"2021-09-25T01:45:38.841559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:38.843635Z","iopub.execute_input":"2021-09-25T01:45:38.844104Z","iopub.status.idle":"2021-09-25T01:45:39.662269Z","shell.execute_reply.started":"2021-09-25T01:45:38.844069Z","shell.execute_reply":"2021-09-25T01:45:39.661381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_predict():\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:39.663962Z","iopub.execute_input":"2021-09-25T01:45:39.664227Z","iopub.status.idle":"2021-09-25T01:45:39.668767Z","shell.execute_reply.started":"2021-09-25T01:45:39.664201Z","shell.execute_reply":"2021-09-25T01:45:39.667986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_bert_outputs = []\nfor i in range(1):\n    print(f'Prediction Bert model {i}')\n    set_os_env(PRETRAINED_PATH[i], TRAIN_PATH[i], VAL_PATH[i])\n    \n    bert_outputs = []\n\n    for batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n        # write data rows to input file\n        with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n            for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n                json.dump(row, f)\n                f.write('\\n')\n\n        # remove output dir\n        !rm -r \"$OUTPUT_DIR\"\n\n        # do predict\n        bert_predict()\n\n        # read predictions\n        with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n            this_preds = f.read().split('\\n')[:-1]\n            bert_outputs += [pred.split() for pred in this_preds]\n        break\n    final_bert_outputs.append(bert_outputs)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:45:39.670105Z","iopub.execute_input":"2021-09-25T01:45:39.670603Z","iopub.status.idle":"2021-09-25T01:47:21.432577Z","shell.execute_reply.started":"2021-09-25T01:45:39.670572Z","shell.execute_reply":"2021-09-25T01:47:21.431635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\ndel test_rows","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.434314Z","iopub.execute_input":"2021-09-25T01:47:21.434638Z","iopub.status.idle":"2021-09-25T01:47:21.440114Z","shell.execute_reply.started":"2021-09-25T01:47:21.4346Z","shell.execute_reply":"2021-09-25T01:47:21.439307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_dataset_labels = []\n\nfor i in range(1):\n    \n    bert_dataset_labels = [] # store all dataset labels for each publication\n\n    for length in paper_length:\n        labels = set()\n        for sentence, pred in zip(test_sentences[:length], final_bert_outputs[i][:length]):\n            curr_phrase = ''\n            for word, tag in zip(sentence, pred):\n                if tag == 'B': # start a new phrase\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n                    curr_phrase = word\n                elif tag == 'I' and curr_phrase: # continue the phrase\n                    curr_phrase += ' ' + word\n                else: # end last phrase (if any)\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n            # check if the label is the suffix of the sentence\n            if curr_phrase:\n                labels.add(curr_phrase)\n                curr_phrase = ''\n\n        # record dataset labels for this publication\n        bert_dataset_labels.append(labels)\n\n        del test_sentences[:length], final_bert_outputs[i][:length]\n    final_dataset_labels.append(bert_dataset_labels)\n    \nfinal_dataset_labels[0][:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.44152Z","iopub.execute_input":"2021-09-25T01:47:21.442029Z","iopub.status.idle":"2021-09-25T01:47:21.46181Z","shell.execute_reply.started":"2021-09-25T01:47:21.441993Z","shell.execute_reply":"2021-09-25T01:47:21.460903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter based on Jaccard","metadata":{}},{"cell_type":"code","source":"final_xlm_roberta_labels = []\nfor bert_dataset_labels in final_dataset_labels:\n    filtered_bert_labels = []\n    for labels in bert_dataset_labels:\n        filtered = []\n\n        for label in sorted(labels, key=len):\n            label = clean_text(label)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.4 for got_label in filtered):\n                filtered.append(label)\n\n        filtered_bert_labels.append('|'.join(filtered))\n    final_xlm_roberta_labels.append(filtered_bert_labels)\ndel filtered_bert_labels\n\nprint(final_xlm_roberta_labels[0][:5])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.466226Z","iopub.execute_input":"2021-09-25T01:47:21.466418Z","iopub.status.idle":"2021-09-25T01:47:21.475013Z","shell.execute_reply.started":"2021-09-25T01:47:21.466394Z","shell.execute_reply":"2021-09-25T01:47:21.474212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spicy prediction","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:               return re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\ndef clean_texts(texts: List[str]) -> List[str]: return [ clean_text(text) for text in texts ] \n\ndef read_json(index: str, test_train) -> Dict:\n    filename = f\"../input/coleridgeinitiative-show-us-the-data/{test_train}/{index}.json\"\n    with open(filename) as f:\n        json = simplejson.load(f)\n    return json\n        \ndef json2text(index: str, test_train) -> str:\n    json  = read_json(index, test_train)\n    texts = [\n        row[\"section_title\"] + \" \" + row[\"text\"] \n        for row in json\n    ]\n    text  = \" \".join(texts)\n    return text\n\ndef filename_to_index(filename):\n    return re.sub(\"^.*/|\\.[^.]+$\", '', filename)\n\ndef glob_to_indices(globpath):\n    return list(map(filename_to_index, glob.glob(globpath)))\n\n# Inspired by: https://www.kaggle.com/hamditarek/merge-multiple-json-files-to-a-dataframe\ndef dataset_df(test_train=\"test\"):\n    indices = glob_to_indices(f\"../input/coleridgeinitiative-show-us-the-data/{test_train}/*.json\")    \n    texts   = Parallel(-1)( \n        delayed(json2text)(index, test_train)\n        for index in indices  \n    )\n    df = pd.DataFrame([\n        { \"id\": index, \"text\": text}\n        for index, text in zip(indices, texts)\n    ])\n    df.to_csv(f\"{test_train}.json.csv\", index=False)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.476647Z","iopub.execute_input":"2021-09-25T01:47:21.477745Z","iopub.status.idle":"2021-09-25T01:47:21.488982Z","shell.execute_reply.started":"2021-09-25T01:47:21.477718Z","shell.execute_reply":"2021-09-25T01:47:21.488292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers = {}\nfor paper_id in sample_submission['Id'].values:\n    with open(f'../input/coleridgeinitiative-show-us-the-data/test/{paper_id}.json', 'r') as f:\n        sections = json.load(f)\n        paper = ''\n        for section in sections:\n            paper = paper + section['text'] + ' .'\n    papers[paper_id] = paper\n    del paper","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.490295Z","iopub.execute_input":"2021-09-25T01:47:21.490603Z","iopub.status.idle":"2021-09-25T01:47:21.510236Z","shell.execute_reply.started":"2021-09-25T01:47:21.490564Z","shell.execute_reply":"2021-09-25T01:47:21.509615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load spacy classifier model\nwith open('../input/coleridge-spacy-classifier/spacy_model.pickle', 'rb') as f:\n    nlp = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:21.51264Z","iopub.execute_input":"2021-09-25T01:47:21.512823Z","iopub.status.idle":"2021-09-25T01:47:23.438627Z","shell.execute_reply.started":"2021-09-25T01:47:21.512803Z","shell.execute_reply":"2021-09-25T01:47:23.437853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.tokenize import sent_tokenize\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split()\n    l2 = s2.split()    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\nstart_time = time.time()\ncolumn_names = [\"Id\", \"PredictionString\"]\nsubmission = pd.DataFrame(columns = column_names)\n\nno_delete = ['study', 'dataset', 'model','survey','data','adni','codes', 'genome', 'program','assessment','database','census','initiative','gauge','system','stewardship','surge']\n\nspacy_predictions = []\nfor index, row in sample_submission.iterrows():\n    to_append=[row['Id'],'']\n    passage = papers[row['Id']]\n    passage=passage.replace(\"'s\",\"s\")\n    passage=passage.replace(\"-\",\" \")\n    passage=passage.replace(\",\",\" \")\n    \n    ######## ACRONYMS\n    for match in re.finditer(r\"(\\(([A-Z]{2,})\\))\", passage):\n    #for match in re.finditer(r\"(\\((.*?)\\))\", data):\n        caps=[]\n        start_index = match.start()\n        abbr = match.group(1)\n        size = len(abbr)\n        words = passage[:start_index].split()[-size:]\n        for word in words:\n            if word[0].isupper():\n                caps.append(word)\n        definition = \" \".join(caps)\n        if sum(1 for c in definition if c.isupper()) < 15:\n            words = [word for word in no_delete if word in definition.lower()]\n            doc=nlp(definition)\n            score=doc.cats['POSITIVE']\n            if len(words)>0 and  score > .99:\n                if to_append[1]!='' and definition not in to_append[1]:\n                    to_append[1] = to_append[1]+'|'+definition+'|'+abbr\n                    to_append[1] = to_append[1]+'|'+abbr\n                if to_append[1]=='':\n                    to_append[1] = definition\n                    to_append[1] = to_append[1]+'|'+abbr\n                            \n    #### cap word sequence\n    if to_append[1]=='':        \n        mylist=re.findall('([A-Z][\\w-]*(?:\\s+[A-Z][\\w-]*)+)', remove_stopwords(passage))\n        mylist = list(dict.fromkeys(mylist))\n        for match in mylist:\n            upper_score=sum(1 for c in match if c.isupper())\n            if upper_score < 15:\n                words = [word for word in no_delete if word in match.lower()]\n                doc=nlp(match)\n                score=doc.cats['POSITIVE']\n                if len(words)>0 and len(match.split())>=2 and score > .99:\n                    if to_append[1]!='' and match not in to_append[1]:\n                        to_append[1] = to_append[1]+'|'+match\n                    if to_append[1]=='':\n                        to_append[1] = match\n            \n    ###### remove similar jaccard\n    got_label=to_append[1].split('|')\n    filtered=[]\n    filtered_labels = ''\n    for label in sorted(got_label, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < .5 for got_label in filtered):\n            filtered.append(label)\n            if filtered_labels!='':\n                filtered_labels=filtered_labels+'|'+label\n            if filtered_labels=='':\n                filtered_labels=label\n    \n    to_append[1] = filtered_labels  \n    \n    spacy_predictions.append(to_append[1])\n    \nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nspacy_predictions[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:32.248787Z","iopub.execute_input":"2021-09-25T01:47:32.249517Z","iopub.status.idle":"2021-09-25T01:47:32.921435Z","shell.execute_reply.started":"2021-09-25T01:47:32.249474Z","shell.execute_reply":"2021-09-25T01:47:32.920547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom transformer model","metadata":{}},{"cell_type":"code","source":"model_path = '../input/ci-transformers-model-v2/model/sent_transformer'\ntokenizer_path = '../input/ci-transformers-model-v2/tokenizer.pickle'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:40.339937Z","iopub.execute_input":"2021-09-25T01:47:40.340554Z","iopub.status.idle":"2021-09-25T01:47:40.346149Z","shell.execute_reply.started":"2021-09-25T01:47:40.340519Z","shell.execute_reply":"2021-09-25T01:47:40.344309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" build transformer model\"\"\"\n\nmaxlen = 500\nnum_classes = 2\nvocab_size = 32824\n\nembed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(2, activation=\"softmax\")(x)\n\nmodel_t = keras.Model(inputs=inputs, outputs=outputs)\nmodel_t.load_weights(model_path)\nmodel_t.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n                patience=0, verbose=1, mode='auto', baseline=None, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:41.556954Z","iopub.execute_input":"2021-09-25T01:47:41.557838Z","iopub.status.idle":"2021-09-25T01:47:45.551397Z","shell.execute_reply.started":"2021-09-25T01:47:41.557787Z","shell.execute_reply":"2021-09-25T01:47:45.550645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\n\n# prepare list of dataset titles to match\ntrain_df = pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n\nds_titles = all_datasets\n\nfiltered = []\nlabels = ds_titles\nfor label in sorted(labels, key=len):\n    label = clean_text(label)\n    if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.2 for got_label in filtered):\n        filtered.append(label)\n        \nds_titles = np.array(filtered)\nds_titles.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:45.553108Z","iopub.execute_input":"2021-09-25T01:47:45.553377Z","iopub.status.idle":"2021-09-25T01:47:45.624053Z","shell.execute_reply.started":"2021-09-25T01:47:45.55333Z","shell.execute_reply":"2021-09-25T01:47:45.623464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = ''\nwith open(tokenizer_path, \"rb\") as openfile:\n    tokenizer = pickle.load(openfile)\n            \ntest_data_path = '../input/coleridgeinitiative-show-us-the-data/test'\ntest_sentences = {}\ncandidate_threshold = 0.3\nacceptance_score = 80\n\ndef read_json_pub(Id):\n    filename = os.path.join(test_data_path, Id+'.json')\n    with open(filename) as f:\n        json_pub = json.load(f)\n    return json_pub\n\ntransformers_preds = []\nfor index, row in tqdm(sample_submission.iterrows(), total = sample_submission.shape[0]):\n    # Load text\n    raw_text = read_json_pub(row['Id'])\n    text = '\\n'.join([z for y in raw_text for z in y.values()])\n\n    # split and clean sentences\n    sentences = nltk.sent_tokenize(re.sub(r'\\.?\\n', '. ', text))\n    sentences = [re.sub(r\"[^a-z ]+\",\"\", s.lower()) for s in sentences]\n    \n    # tokenize\n    tokens = tokenizer.texts_to_sequences(sentences)\n    tokens = tf.keras.preprocessing.sequence.pad_sequences(\n        tokens, maxlen=maxlen, padding='pre',)\n\n    # Predict candidates sentences that may contain DS references\n    y_pred = model_t.predict(tokens, batch_size=32)\n    sent_candidates = np.array(sentences)[y_pred[:,1] > candidate_threshold]\n    test_sentences[row['Id']] = sent_candidates\n\n    ds_candidates = set()\n    for sent in sent_candidates:\n        scores = [fuzz.partial_ratio(sent.lower(), title) for title in ds_titles]\n        best_fit_title_index = np.argmax(scores)\n        if max(scores) > acceptance_score:\n            ds_candidates.add(ds_titles[np.argmax(scores)])\n    prediction_string = ' | '.join(ds_candidates)\n    transformers_preds.append(prediction_string)\n    \ntransformers_preds[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:45.625155Z","iopub.execute_input":"2021-09-25T01:47:45.625412Z","iopub.status.idle":"2021-09-25T01:47:48.593989Z","shell.execute_reply.started":"2021-09-25T01:47:45.625361Z","shell.execute_reply":"2021-09-25T01:47:48.593167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aggregate all predictions","metadata":{}},{"cell_type":"code","source":"final_predictions = []\nfor bert_pred, trans_pred, spacy_pred, literal_match in zip(\n    final_xlm_roberta_labels[0], \n    transformers_preds, \n    spacy_predictions,\n    literal_preds\n):        \n    pred1 = [x for x in bert_pred.split('|') if x not in ['']]\n    pred2 = [x for x in trans_pred.split('|') if x not in ['']]\n    pred3 = [x for x in spacy_pred.split('|') if x not in ['']]\n\n    labels = np.unique(pred1+pred2+pred3)\n    if len(labels)>0:\n        filtered = []\n        for label in tqdm(sorted(labels, key=len)):\n            label = clean_text(label)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.4 for got_label in filtered):\n                filtered.append(label)\n\n        final_predictions.append('|'.join(filtered))\n    else:\n        final_predictions.append(literal_match)        \n\nfinal_predictions[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:48.596043Z","iopub.execute_input":"2021-09-25T01:47:48.596313Z","iopub.status.idle":"2021-09-25T01:47:48.627305Z","shell.execute_reply.started":"2021-09-25T01:47:48.596279Z","shell.execute_reply":"2021-09-25T01:47:48.626535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['PredictionString'] = final_predictions\nsample_submission[['Id', 'PredictionString']].to_csv('submission.csv', index=False)\n\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:47:48.628646Z","iopub.execute_input":"2021-09-25T01:47:48.628905Z","iopub.status.idle":"2021-09-25T01:47:48.645415Z","shell.execute_reply.started":"2021-09-25T01:47:48.628872Z","shell.execute_reply":"2021-09-25T01:47:48.644621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}