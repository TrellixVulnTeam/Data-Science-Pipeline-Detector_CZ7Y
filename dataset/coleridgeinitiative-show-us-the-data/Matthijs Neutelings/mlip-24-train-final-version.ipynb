{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n\nNote that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction.","metadata":{}},{"cell_type":"markdown","source":"## Install packages","metadata":{}},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T09:24:34.389905Z","iopub.execute_input":"2021-06-14T09:24:34.390278Z","iopub.status.idle":"2021-06-14T09:25:06.820554Z","shell.execute_reply.started":"2021-06-14T09:24:34.390244Z","shell.execute_reply":"2021-06-14T09:25:06.819382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#install translator. Make sure internet is enabled\n!pip install google_trans_new\nfrom google_trans_new import google_translator\n\n#packages to find synonyms\nimport nltk\nfrom nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:25:06.826258Z","iopub.execute_input":"2021-06-14T09:25:06.826642Z","iopub.status.idle":"2021-06-14T09:25:15.494746Z","shell.execute_reply.started":"2021-06-14T09:25:06.826607Z","shell.execute_reply":"2021-06-14T09:25:15.493889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"#imports\nimport os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#set seed for reproducability\nrandom.seed(123)\nnp.random.seed(456)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:25:15.496336Z","iopub.execute_input":"2021-06-14T09:25:15.497069Z","iopub.status.idle":"2021-06-14T09:25:15.587555Z","shell.execute_reply.started":"2021-06-14T09:25:15.497022Z","shell.execute_reply":"2021-06-14T09:25:15.586781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:25:15.74777Z","iopub.execute_input":"2021-06-14T09:25:15.748049Z","iopub.status.idle":"2021-06-14T09:25:16.491951Z","shell.execute_reply.started":"2021-06-14T09:25:15.748022Z","shell.execute_reply":"2021-06-14T09:25:16.490709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.\nADD_SYNONYMS = False\nADD_TRANSLATIONS = True\nEQUALIZE_COUNTS =  True\nassert EQUALIZE_COUNTS == True or (ADD_SYNONYMS == False and ADD_TRANSLATIONS == False), \"for the translations and synonyms to work label counts have to be equalised\"","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:30:50.758765Z","iopub.execute_input":"2021-06-14T09:30:50.759111Z","iopub.status.idle":"2021-06-14T09:30:50.764625Z","shell.execute_reply.started":"2021-06-14T09:30:50.759081Z","shell.execute_reply":"2021-06-14T09:30:50.763525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n\ndf = pd.read_csv(train_path)\ndf = df[:MAX_SAMPLE]\nprint(f'No. raw training rows: {len(df)}')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:25:15.599443Z","iopub.execute_input":"2021-06-14T09:25:15.599948Z","iopub.status.idle":"2021-06-14T09:25:15.746285Z","shell.execute_reply.started":"2021-06-14T09:25:15.599903Z","shell.execute_reply":"2021-06-14T09:25:15.745112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove papers with dataset title Our World in Data COVID-19 dataset --> this will be in the validation set\nCOVID_papers = df[df['dataset_title'] == \"Our World in Data COVID-19 dataset\"]\nCOVID_papers_IDS = set(COVID_papers['Id'])\ndf = df[~df['Id'].isin(COVID_papers_IDS)]\n\n\n#leave out 1% to test on\ntrain_ids, validate_ids = train_test_split(df['Id'].unique(), test_size=0.01, random_state=42)\nprint(train_ids.shape, validate_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:31:17.15901Z","iopub.execute_input":"2021-06-14T09:31:17.159415Z","iopub.status.idle":"2021-06-14T09:31:17.179921Z","shell.execute_reply.started":"2021-06-14T09:31:17.159371Z","shell.execute_reply":"2021-06-14T09:31:17.178749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Group by publication, training labels should have the same form as expected output.","metadata":{}},{"cell_type":"code","source":"papers = {}\nfor paper_id in df['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:31:19.774777Z","iopub.execute_input":"2021-06-14T09:31:19.775145Z","iopub.status.idle":"2021-06-14T09:31:28.12492Z","shell.execute_reply.started":"2021-06-14T09:31:19.775116Z","shell.execute_reply":"2021-06-14T09:31:28.124005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train papers with titles\ntrain = df[df['Id'].isin(train_ids)]\ntrain['New_label'] = train['dataset_label']\npapers = {your_key: papers[your_key] for your_key in train_ids}\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:31:28.127151Z","iopub.execute_input":"2021-06-14T09:31:28.127507Z","iopub.status.idle":"2021-06-14T09:31:28.148682Z","shell.execute_reply.started":"2021-06-14T09:31:28.12747Z","shell.execute_reply":"2021-06-14T09:31:28.147838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_translations(labels):\n    import time\n    translator = google_translator()\n    new_labels = []\n    for label in labels:\n        result = translator.translate(label, lang_src = 'en', lang_tgt='ug') #you can choose other languages for different sentences\n        result = translator.translate(result, lang_src = 'ug', lang_tgt='zh')\n        result = translator.translate(result, lang_src = 'zh', lang_tgt='en')\n        new_labels.append(result)\n        time.sleep(1)\n    return new_labels\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:29:14.65253Z","iopub.execute_input":"2021-06-14T09:29:14.652961Z","iopub.status.idle":"2021-06-14T09:29:14.66022Z","shell.execute_reply.started":"2021-06-14T09:29:14.652925Z","shell.execute_reply":"2021-06-14T09:29:14.658649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_synonyms(labels):\n    \n    stop_words = ['in', 'of', 'and']\n    new_labels = []\n    for label in labels:\n        string=label\n        words = string.split()\n        for word in words:\n            if word in stop_words: #We don't want synonyms of stop words\n                continue\n            synonyms = []\n            for syn in wordnet.synsets(word): \n                for l in syn.lemmas():\n                    synonyms.append(l.name()) #synonyms contains all synonyms of a word\n            synonyms = [syn for syn in synonyms if not syn.lower() in word.lower() and not word.lower() in syn.lower()] #remove synonyms that are part of the word (or vice versa)\n            if len(synonyms)>0:   #if we have any synonyms:\n                rep = synonyms[0] #replace out word with the first synonym\n                st=string.replace(word,rep, 1)\n                string = st\n        new_labels.append(string)\n    return new_labels","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:29:16.357191Z","iopub.execute_input":"2021-06-14T09:29:16.35757Z","iopub.status.idle":"2021-06-14T09:29:16.366921Z","shell.execute_reply.started":"2021-06-14T09:29:16.357536Z","shell.execute_reply":"2021-06-14T09:29:16.365362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function that counts the distribution of datasets among papers\ndef get_init_dist(labels):\n    print(labels)\n    init_dist = []\n    for label in labels:\n        init_dist.append([len(set(train[train['dataset_title'] == label]['Id'])), label]) #find papers where label occurs\n    init_dist = sorted(init_dist)\n    return init_dist","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:29:32.448429Z","iopub.execute_input":"2021-06-14T09:29:32.44881Z","iopub.status.idle":"2021-06-14T09:29:32.454694Z","shell.execute_reply.started":"2021-06-14T09:29:32.448776Z","shell.execute_reply":"2021-06-14T09:29:32.453535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function that replaces dataset labels in text to straighten the skewed distribution. \n#after running this function each dataset title will occur approximately the same number of times across all papers.\ndef make_occurences_equal(init_dist):\n    no_datasets = sum([x[0] for x in init_dist])/len(init_dist) #total number of datasets\n    low_mentions = 0\n    high_mentions = len(init_dist)-1\n    high_mention_papers = train[train['dataset_title']==init_dist[high_mentions][1]] \n    start_index = 0\n    while high_mentions > low_mentions:\n        #print(\"new: \", init_dist[low_mentions][1], \" old: \", init_dist[high_mentions][1])\n        Ids = list(set(train[train['dataset_title']==init_dist[high_mentions][1]]['Id']))\n        while init_dist[low_mentions][0] < no_datasets and init_dist[high_mentions][0] > no_datasets:\n            Id = Ids[start_index]\n            text = papers[Id]\n            train_rows = train[train[\"Id\"]==Id]\n            labels_in_text = train_rows[train_rows[\"dataset_title\"]==init_dist[high_mentions][1]]['dataset_label']\n            labels_sorted = sorted(labels_in_text, key=len, reverse=True)\n            low_mention_title = init_dist[low_mentions][1]\n            for label in labels_sorted:\n                for section in papers[Id]: \n                    section['text'] = section['text'].replace(label, low_mention_title) #replace high mention title with low mention title\n                train.loc[train[(train[\"Id\"] == Id) & (train['dataset_label']==label)].index, \"New_label\"] = low_mention_title\n            init_dist[low_mentions][0] += 1\n            init_dist[high_mentions][0] -= 1\n            start_index += 1 \n        if init_dist[low_mentions][0] >= no_datasets:\n            low_mentions += 1\n        else:\n            high_mentions -= 1\n            high_mention_papers = train[train['dataset_title']==init_dist[high_mentions][1]]\n            start_index = 0\n    return init_dist\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:29:36.694549Z","iopub.execute_input":"2021-06-14T09:29:36.694908Z","iopub.status.idle":"2021-06-14T09:29:36.709107Z","shell.execute_reply.started":"2021-06-14T09:29:36.694879Z","shell.execute_reply":"2021-06-14T09:29:36.70789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all dataset labels\nall_labels = list(set(train['dataset_title']))\n#print(all_labels)\nif ADD_SYNONYMS:\n    all_labels = list(set(all_labels).union(set(add_synonyms(all_labels))))\n    #print(all_labels)\nif ADD_TRANSLATIONS:\n    all_labels = list(set(all_labels).union(set(add_translations(all_labels))))\n    #print(all_labels)\nif EQUALIZE_COUNTS: \n    init_dist = get_init_dist(all_labels)\n    #print(init_dist)\n    init_dist = make_occurences_equal(init_dist)\n    #print(init_dist)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T09:31:28.149898Z","iopub.execute_input":"2021-06-14T09:31:28.150318Z","iopub.status.idle":"2021-06-14T09:32:46.609904Z","shell.execute_reply.started":"2021-06-14T09:31:28.150287Z","shell.execute_reply":"2021-06-14T09:32:46.607886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#group same paper Ids together\ntrain = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join,\n    'New_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:42:17.847302Z","iopub.execute_input":"2021-06-14T07:42:17.847635Z","iopub.status.idle":"2021-06-14T07:42:18.297263Z","shell.execute_reply.started":"2021-06-14T07:42:17.847588Z","shell.execute_reply":"2021-06-14T07:42:18.296387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform data to NER format","metadata":{}},{"cell_type":"code","source":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions\n\ndef tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n    sentence_words = sentence.split()\n    labels = sorted(labels, key=len, reverse=True)\n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = ['O'] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = 'B'\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = 'I'\n\n        return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = ['O'] * len(sentence_words)\n        return False, list(zip(sentence_words, nes))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:37:53.809374Z","iopub.execute_input":"2021-06-14T07:37:53.809717Z","iopub.status.idle":"2021-06-14T07:37:53.821407Z","shell.execute_reply.started":"2021-06-14T07:37:53.809687Z","shell.execute_reply":"2021-06-14T07:37:53.820534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt_pos, cnt_neg, cnt_rnd = 0, 0 ,0 # number of sentences that contain/not contain labels\nner_data = []\n\n# pbar = tqdm(total=len(train))\nfor i, id, dataset_label in train[['Id', 'New_label']].itertuples():\n    # paper\n    paper = papers[id]\n    \n    # labels\n    labels = dataset_label.split('|')\n    labels = [clean_training_text(label) for label in labels]\n    \n    # sentences\n    sentences = set([clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.') \n                ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    \n    # positive sample\n    for sentence in sentences:\n        is_positive, tags = tag_sentence(sentence, labels)\n        if is_positive:\n            cnt_pos += 1\n            ner_data.append(tags)\n        elif any(word in sentence.lower() for word in ['data', 'study']): \n            ner_data.append(tags)\n            cnt_neg += 1\n        else:\n            p = random.uniform(0,1)\n            if p < 0.01:\n                ner_data.append(tags)\n                cnt_rnd += 1\n                \n    \nprint(cnt_pos)\nprint(cnt_neg)\nprint(cnt_rnd)\n    # process bar\n#     pbar.update(1)\n#     pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n\n# shuffling\nrandom.shuffle(ner_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T11:18:30.851053Z","iopub.execute_input":"2021-06-09T11:18:30.851469Z","iopub.status.idle":"2021-06-09T11:20:52.977349Z","shell.execute_reply.started":"2021-06-09T11:18:30.851428Z","shell.execute_reply":"2021-06-09T11:20:52.976336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"write data to file.","metadata":{}},{"cell_type":"code","source":"with open('train_ner.json', 'w') as f:\n    for row in ner_data:\n        words, nes = list(zip(*row))\n        row_json = {'tokens' : words, 'tags' : nes}\n        json.dump(row_json, f)\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T11:22:00.929756Z","iopub.execute_input":"2021-06-09T11:22:00.930096Z","iopub.status.idle":"2021-06-09T11:22:32.516427Z","shell.execute_reply.started":"2021-06-09T11:22:00.930065Z","shell.execute_reply":"2021-06-09T11:22:32.515558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune a BERT model for NER","metadata":{}},{"cell_type":"code","source":"!python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n--model_name_or_path 'bert-base-cased' \\\n--train_file './train_ner.json' \\\n--validation_file './train_ner.json' \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 8 \\\n--save_steps 15000 \\\n--output_dir './output' \\\n--report_to 'none' \\\n--seed 123 \\\n--do_train ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T11:23:05.861953Z","iopub.execute_input":"2021-06-09T11:23:05.862274Z","iopub.status.idle":"2021-06-09T13:37:38.365172Z","shell.execute_reply.started":"2021-06-09T11:23:05.862242Z","shell.execute_reply":"2021-06-09T13:37:38.364278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the tuning finishes, we should find our model in './output'.","metadata":{}}]}