{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b76f41dc-eac5-89dc-48f1-0ca4fb5a7c71"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport itertools \nimport csv\nimport collections\nimport matplotlib.pyplot as plt\n\nsns.set_context(\"paper\")\n%matplotlib inline\n\nRES_DIR = \"../input/\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb34c73c-dcba-aad3-43ae-39c252ee2ae5"},"outputs":[],"source":"# Load train data (skips the content column)\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags'])\n        data['category'] = cat\n        train_data.append(data)\n    \n    return pd.concat(train_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bfda00f-6c8d-67ba-b03f-4632d550354b"},"outputs":[],"source":"train_data = load_train_data()\ntrain_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56fdadce-563d-0068-284b-638bfec125bf"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db674632-6e9b-4651-90cf-b736648359f6"},"outputs":[],"source":"# Summary about tags\ntag_lists = [t.strip().split() for t in train_data['tags'].values]\nall_tags = list(itertools.chain(*tag_lists))\ntag_list_size = np.array([len(x) for x in tag_lists])\nprint(\"\"\"The corpus is composed by {} questions. Overall {} tags have been used, of which {} unique ones. \nAverage number of tags per question {:.2f} (min={}, max={}, std={:.2f})\"\"\".format(\n    len(train_data),\n    len(all_tags), len(set(all_tags)),\n    tag_list_size.mean(), \n    min(tag_list_size), max(tag_list_size),\n    tag_list_size.std()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4935b7e7-7791-2501-cb99-30eb1be87472"},"outputs":[],"source":"# Utility function to return top occuring tags in the passed df\ndef get_top_tags(df, n=None):\n    tags = list(itertools.chain(*[t.strip().split() for t in df['tags'].values]))\n    top_tags = collections.Counter(list(tags)).most_common(n)\n    tags, count = zip(*top_tags)\n    return tags, count"},{"cell_type":"markdown","metadata":{"_cell_guid":"de9e98aa-6ab5-e4b5-9bba-f91240fc80d8"},"source":"### Shared Tags\n\nExplore tags that are used in more than one categories\n\nI build the needed DataFrame myself. Using [sklearn CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) might be a quicker and maybe more reusable solution, but I find the following lines short and intuitive enough for the task."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3ddfcc6-5a73-1be8-a30c-ce8fe538ec50"},"outputs":[],"source":"# Created DataFrame indexed on tags\ntags_df = pd.DataFrame(index=set(itertools.chain(*tag_lists)))\n# For each category create a column and update the flag to tag count\nfor i, (name, group) in enumerate(train_data.groupby('category')):\n    tags_df[name] = 0\n    tmp_index, count = get_top_tags(group)\n    tmp = pd.Series(count, index=tmp_index)\n    tags_df[name].update(tmp)\n# Number of categories for which a tag appeared at least 1 time\ntags_df['categories_appears'] = tags_df.apply(lambda x: x.astype(bool).sum(), axis=1)\ntags_df['categories_appears'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8fde8789-02a2-6ca9-cdc0-0cf58535bcdf"},"outputs":[],"source":"# List of tags ordered by number of categories in which they appear, with total count for each\ntags_df.sort_values('categories_appears', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f48a0e9a-1f16-d025-eec3-ec41fd0fcda8"},"outputs":[],"source":"d = tags_df.unstack().reset_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b01e2f9c-0e9d-438b-6e12-3dc7aaf503e1"},"outputs":[],"source":"d.columns = ['source', 'target', 'weight']\nimport networkx as nx"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c5d0da-566d-a797-ef5b-bef775c89429"},"outputs":[],"source":"d = d[d.weight > 10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af85fc33-0584-beb4-747c-c44a2ce820c3"},"outputs":[],"source":"g = nx.from_pandas_dataframe(d, 'source', 'target', ['weight'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8f14efd-707b-323d-d5a9-ad2f8561b97e"},"outputs":[],"source":"nx.node_connectivity(g)bnvhn"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fdd44da8-7491-0889-ea70-285c1a33e9f2"},"outputs":[],"source":"import matplotlib.pyplot as plt\nplt.figure(figsize=[50,50])\nnx.draw_networkx(g,alpha=0.1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73ca470f-3ee3-0d71-8507-e24e642008d2"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}