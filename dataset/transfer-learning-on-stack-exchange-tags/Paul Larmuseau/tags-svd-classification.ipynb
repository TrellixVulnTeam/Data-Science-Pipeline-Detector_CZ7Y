{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2a018561-c0aa-7efa-f9f0-95beb50e0d91"},"source":"Ok , this is IMHO the fasted way to classify all documents"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"376a3953-871e-3a6e-85d6-fc16e243c4ad"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport itertools \nimport csv\nimport collections\nimport matplotlib.pyplot as plt\n\nsns.set_context(\"paper\")\n%matplotlib inline\n\nRES_DIR = \"../input/\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9453439e-96e4-a5da-b9fc-f9a6af6dd25f"},"outputs":[],"source":"# Load train data (skips the content column)\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags'])\n        data['category'] = cat\n        train_data.append(data)\n    \n    return pd.concat(train_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47363d23-3ba7-f7d4-aacb-5dd8bebb6389"},"outputs":[],"source":"train_data = load_train_data()\ntrain_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"580f9f85-38d9-a29a-07e6-06d92d9bc197"},"outputs":[],"source":"# Distribution of questions by category\nax = train_data['category'].value_counts().plot(kind='bar')\nax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=30)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df1c1d2f-f413-7611-c609-cf86d0da1f96"},"outputs":[],"source":"# Summary about tags\ntag_lists = [t.strip().split() for t in train_data['tags'].values]\ntag_lists2 = [t.strip().split() for t in train_data['title'].values]\nall_tags = list(itertools.chain(*tag_lists,*tag_lists2))\ntag_list_size = np.array([len(x) for x in tag_lists])\nprint(\"\"\"The corpus is composed by {} questions. Overall {} tags have been used, of which {} unique ones. \nAverage number of tags per question {:.2f} (min={}, max={}, std={:.2f})\"\"\".format(\n    len(train_data),\n    len(all_tags), len(set(all_tags)),\n    tag_list_size.mean(), \n    min(tag_list_size), max(tag_list_size),\n    tag_list_size.std()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86cb0538-77ff-93d0-f7a3-550b4f45282c"},"outputs":[],"source":"# Distribution of number of tags per question\nsns.distplot(tag_list_size, kde=False)\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"392a42b7-287c-66af-d33d-eb65cb06dfc2"},"outputs":[],"source":"# Utility function to return top occuring tags in the passed df\ndef get_top_tags(df, n=None):\n    itag_lists = [t.strip().split() for t in df['tags'].values]\n    itag_lists2 = [t.strip().split() for t in df['title'].values]\n    tags = list(itertools.chain(*itag_lists,*itag_lists2))\n    top_tags = collections.Counter(list(tags)).most_common(n)\n    tags, count = zip(*top_tags)\n    return tags, count\n# Utility function to return top occuring tags in the passed df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36f713d3-ef34-58dc-0571-16b056ca2ba6"},"outputs":[],"source":"# Created DataFrame indexed on tags\ntags_df = pd.DataFrame(index=set(itertools.chain(*tag_lists,*tag_lists2)))\n# For each category create a column and update the flag to tag count\nfor i, (name, group) in enumerate(train_data.groupby('category')):\n    tags_df[name] = 0\n    tmp_index, count = get_top_tags(group)\n    tmp = pd.Series(count, index=tmp_index)\n    tags_df[name].update(tmp)\n# Number of categories for which a tag appeared at least 1 time\ntags_df['categories_appears'] = tags_df.apply(lambda x: x.astype(bool).sum(), axis=1)\ntags_df['categories_appears'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a813edd-89df-eb56-1892-c4623b31bbf6"},"outputs":[],"source":"#import the test data\ntest = pd.read_csv(\"../input/test.csv\")\ntest.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2fa116b-ba78-3dad-5341-e249bb36c98d"},"outputs":[],"source":"# viewing the table of tags\nA=tags_df\nA"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d80cbf66-e6ec-be73-45ec-43ab09db6b70"},"outputs":[],"source":"#Solving the question with a Singular Value Decomposition, \n#this is the core function"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35112bb5-1642-4756-ca22-d6a3f0b9cbef"},"outputs":[],"source":"from numpy.linalg import inv\nU,s,V=np.linalg.svd(A,full_matrices=False)\n# reconstruct\nS=np.diag(s)\n\niS=inv(S)\nUS=np.dot(U,iS)\nUS\n# A fill up with US matrix\nUS_df=pd.DataFrame(data=US, index=tags_df.index, columns=tags_df.columns)\n# with this simple math i know all the relations between all the tags and the documents\n# "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62fbc30d-32a1-d406-83df-faf9943747f9"},"outputs":[],"source":"#learn how to use dataframes...  and yes the algorithm knows extreme tourism antarctica has something to do with travel...\ndf1=US_df['extreme-tourism':'extreme-tourism':]\ndf2=US_df['antarctica':'antarctica':]\nframes = [df1,df2]\nQtemp=pd.concat(frames).sum()\nnp.dot(Qtemp,V)/np.dot(np.abs(Qtemp),np.abs(V))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1777e418-e6e9-3aea-07ca-b828e2e3ca23"},"outputs":[],"source":"columns = ['biology','cooking','crypto','diy','robotics','travel']\n#,'categories_appears']\ndata = {'biology': [0],'cooking': [0],'crypto': [0],'diy': [0],'robotics': [0],'travel': [0],'categories_appears': [0]}\nnewDF = pd.DataFrame(data, columns=columns,index = ['blanco'])\n#print(newDF)\nfor xya in range(0,8926):\n    temptxt = test['title'][xya] + test['content'][xya]\n    tempspl = temptxt.strip().split() \n    Qtemp=newDF\n    for sword in tempspl:\n        if sword in US_df.index:\n            #print(US_df.loc[sword:sword,:])\n            Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n            #print(Qtemp)\n    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))\n    tempprnt=''\n    for xyb in range(0,5):\n        if simila[xyb]>0.89 or simila[xyb]==np.amax(simila[0:5]):\n            tempprnt+=columns[xyb]+' '\n    \n    print(test['id'][xya],',',tempprnt)\n\n    \n    \n "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}