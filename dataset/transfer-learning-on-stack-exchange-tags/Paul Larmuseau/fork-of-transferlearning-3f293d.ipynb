{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b76f41dc-eac5-89dc-48f1-0ca4fb5a7c71"},"outputs":[],"source":"\ndel testimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport itertools \nimport csv\nimport collections\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\n\n\nsns.set_context(\"paper\")\n%matplotlib inline\n\nRES_DIR = \"../input/\"\n# Load train data (skips the content column)\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags','content'])\n        data['category'] = cat\n        train_data.append(data)\n    \n    return pd.concat(train_data)\ndata = load_train_data()\nprint(data.head())\n\n\ndata=data.sample(3000)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb34c73c-dcba-aad3-43ae-39c252ee2ae5"},"outputs":[],"source":"uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\"\n\n# This could take a while\ndata[\"title\"] = data[\"title\"].map(stripTagsAndUris)\ndata[\"content\"] = data[\"content\"].map(stripTagsAndUris)\nimport string\nprint(data.head())\n\ndef removePunctuation(x):\n    # Lowercasing all words\n    x = x.lower()\n    # Removing non ASCII chars\n    #x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    # Removing (replacing with empty spaces actually) all the punctuations\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n\n# point questionmarks etc\ndata[\"title\"] = data[\"title\"].map(removePunctuation)\ndata[\"content\"] = data[\"content\"].map(removePunctuation)\n#data = clean_dataframe(data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bfda00f-6c8d-67ba-b03f-4632d550354b"},"outputs":[],"source":"from gensim.parsing import PorterStemmer\nglobal_stemmer = PorterStemmer()\n \nclass StemmingHelper(object):\n    \"\"\"\n    Class to aid the stemming process - from word to stemmed form,\n    and vice versa.\n    The 'original' form of a stemmed word will be returned as the\n    form in which its been used the most number of times in the text.\n    \"\"\"\n \n    #This reverse lookup will remember the original forms of the stemmed\n    #words\n    word_lookup = {}\n \n    @classmethod\n    def stem(cls, word):\n        \"\"\"\n        Stems a word and updates the reverse lookup.\n        \"\"\"\n \n        #Stem the word\n        stemmed = global_stemmer.stem(word)\n \n        #Update the word lookup\n        if stemmed not in cls.word_lookup:\n            cls.word_lookup[stemmed] = {}\n        cls.word_lookup[stemmed][word] = (\n            cls.word_lookup[stemmed].get(word, 0) + 1)\n \n        return stemmed\n \n    @classmethod\n    def original_form(cls, word):\n        \"\"\"\n        Returns original form of a word given the stemmed version,\n        as stored in the word lookup.\n        \"\"\"\n \n        if word in cls.word_lookup:\n            return max(cls.word_lookup[word].keys(),\n                       key=lambda x: cls.word_lookup[word][x])\n        else:\n            return word\n        \n#data['title']=data['title'].replace('?',' ?')\ndata.head(5)# Summary about tags"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97e65fb7-23b1-3bc4-a877-5892418cd7d0"},"outputs":[],"source":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\ndata['tot']=data['title']+' '+data['content']\n#create sklearn pipeline, fit all, and predit test data\nclf = Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), \n                ('svd', TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)), \n                ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), \n                ('svm', SVC(C=10.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None))])\nclf.fit(data['tot'], data['tags'])\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"646459aa-4ba6-9805-24d8-5e4d8f9e6452"},"outputs":[],"source":"test = pd.read_csv(\"../input/test.csv\")\ntest['tags'] = ''\ntest['category'] = 'physics'\n\ndata=test.sample(3000)\ndata[\"title\"] = data[\"title\"].map(removePunctuation)\ndata[\"content\"] = data[\"content\"].map(removePunctuation)\ndata['tot']=data['title']+' '+data['content']\nt_labels = clf.predict(data['tot'])\nprint(t_labels)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}