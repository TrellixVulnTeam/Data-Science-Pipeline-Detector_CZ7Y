{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9df1ac46-a694-f128-e21a-1d125935b9e8"},"source":"I am still puzzling on this contest\nIMHO there is no 'perfect' solution in that sense, that all submission that have a 'bugfree' should be good...\n\nTake for instance this SVD attack to the problem\n*Do you train a 'full matrix' on one copy of  the titles.words* or do you train a matrix taking with 10 copies columnwize of the matrix such that  you get a resolution of 60 axis, instead of the '6-axis solution' ?\n*Do you use that result on the titles* ? Or do you use that result on the content ? the answer changes each time, and all should be accounted as good..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba0f4ab9-c447-3871-6809-8f2b4c1dc44e"},"outputs":[],"source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\nimport numpy as np\nimport seaborn as sns\nimport itertools \nimport csv\nimport collections\nimport matplotlib.pyplot as plt\n\nsns.set_context(\"paper\")\n%matplotlib inline\n\nRES_DIR = \"../input/\"\n# Load train data (skips the content column)\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags'])\n        data['category'] = cat\n        train_data.append(data)\n    \n    return pd.concat(train_data)\ntrain_data = load_train_data()\n#import the test data\ntest = pd.read_csv(\"../input/test.csv\")\ntrain_data.head()\ntest.head()\nuri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\"\n# This could take a while\ntrain_data[\"title\"] = train_data[\"title\"].map(stripTagsAndUris)\ntest[\"content\"] = test[\"content\"].map(stripTagsAndUris)\ntrain_data.head()\ntest.head()\ndef removePunctuation(x):\n    # Lowercasing all words\n    x = x.lower()\n    # Removing non ASCII chars\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    # Removing (replacing with empty spaces actually) all the punctuations\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n\ntrain_data[\"title\"] = train_data[\"title\"].map(removePunctuation)\ntest[\"title\"] = test[\"title\"].map(removePunctuation)\ntest[\"content\"] = test[\"content\"].map(removePunctuation)\n\ntrain_data.head()\ntest.head()\n\nstops = set(stopwords.words(\"english\"))\ndef removeStopwords(x):\n    # Removing all the stopwords\n    filtered_words = [word for word in x.split() if word not in stops]\n    return \" \".join(filtered_words)\n\ntrain_data[\"title\"] = train_data[\"title\"].map(removeStopwords)\ntest[\"title\"] = test[\"title\"].map(removeStopwords)\ntest[\"content\"] = test[\"content\"].map(removeStopwords)\n\n\n# Summary about tags\ntag_lists = [t.split() for t in train_data['tags'].values]\ntag_lists2 = [t.split() for t in train_data['title'].values]\nall_tags = list(itertools.chain(*tag_lists,*tag_lists2))\ntag_list_size = np.array([len(x) for x in tag_lists])\nprint(\"\"\"The corpus is composed by {} questions. Overall {} tags have been used, of which {} unique ones. \nAverage number of tags per question {:.2f} (min={}, max={}, std={:.2f})\"\"\".format(\n    len(train_data),\n    len(all_tags), len(set(all_tags)),\n    tag_list_size.mean(), \n    min(tag_list_size), max(tag_list_size),\n    tag_list_size.std()))\n\n# Utility function to return top occuring tags in the passed df\ndef get_top_tags(df, n=None):\n    itag_lists = [t.split() for t in df['tags'].values]\n    itag_lists2 = [t.split() for t in df['title'].values]\n    tags = list(itertools.chain(*itag_lists,*itag_lists2))\n    top_tags = collections.Counter(list(tags)).most_common(n)\n    tags, count = zip(*top_tags)\n    return tags, count\n# Utility function to return top occuring tags in the passed df\n\n# Created DataFrame indexed on tags\ntags_df = pd.DataFrame(index=set(itertools.chain(*tag_lists,*tag_lists2)))\n# For each category create a column and update the flag to tag count\nfor i, (name, group) in enumerate(train_data.groupby('category')):\n    tags_df[name] = 0\n    tmp_index, count = get_top_tags(group)\n    tmp = pd.Series(count, index=tmp_index)\n    tags_df[name].update(tmp)\n# Number of categories for which a tag appeared at least 1 time\ntags_df['categories_appears'] = tags_df.apply(lambda x: x.astype(bool).sum(), axis=1)\ntags_df['categories_appears'].value_counts()\n# viewing the table of tags\n\nA=tags_df.drop('categories_appears',axis=1)\n#lets try to , smooth big numbers,normalize \n#A = np.log(A+1)\n#B = (A - A.mean()) / (A.max() - A.min())\nB=A\nfrom scipy.cluster.vq import whiten\nB=whiten(A)\nprint(B)\nfrom numpy.linalg import inv\nU,s,V=np.linalg.svd(B,full_matrices=False)\n# reconstruct\nS=np.diag(s)\n\niS=inv(S)\nUS=np.dot(U,iS)\n\n# A fill up with US matrix\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03323f30-eac2-fc44-fd50-92a22b9ce30c"},"outputs":[],"source":"US=np.dot(U,iS)\nUS_df=pd.DataFrame(data=US, index=A.index, columns=A.columns)\nprint(US_df.shape)\n# with this simple math i know all the relations between all the tags and the documents\n# \n#learn how to use dataframes...  and yes the algorithm knows extreme tourism antarctica has something to do with travel...\ndf1=US_df['extreme-tourism':'extreme-tourism':]\ndf2=US_df['antarctica':'antarctica':]\nframes = [df1,df2]\nQtemp=pd.concat(frames).sum()\nprint(np.dot(Qtemp,V)/np.dot(np.abs(Qtemp),np.abs(V)))\n\ncolumns = ['biology','cooking','crypto','diy','robotics','travel']\n#,'categories_appears']\ndata = {'biology': [0],'cooking': [0],'crypto': [0],'diy': [0],'robotics': [0],'travel': [0],'categories_appears': [0]}\nnewDF = pd.DataFrame(data, columns=columns,index = ['blanco'])\n\n\nprint(test.head())\nfor xyz in range (1,10):\n    zoekwoorden=test['title'][xyz]\n    tempspl = zoekwoorden.strip().split()\n    Qtemp=newDF\n    for sword in tempspl:\n        if sword in US_df.index:\n           #print(US_df.loc[sword:sword,:])\n           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n    #print(Qtemp)\n    #print(Qtemp.sum())\n    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n    #print(simila)\n    tempprnt=''\n    for xyb in range(0,5):\n        if simila[xyb]>85:\n            #or simila[xyb]==np.amax(simila[0:5]):\n            tempprnt+=columns[xyb]+' '\n    print('title map:',xyz,tempprnt)\n\nfor xyz in range (1,10):\n    zoekwoorden=test['content'][xyz]\n    tempspl = zoekwoorden.strip().split()\n    Qtemp=newDF\n    for sword in tempspl:\n        if sword in US_df.index:\n           #print(US_df.loc[sword:sword,:])\n           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n    #print(Qtemp)\n    #print(Qtemp.sum())\n    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n    #print(simila)\n    tempprnt=''\n    for xyb in range(0,5):\n        if simila[xyb]>85:\n            #or simila[xyb]==np.amax(simila[0:5]):\n            tempprnt+=columns[xyb]+' '\n    print('content mapp:',xyz,tempprnt)\n    \nfor xyz in range (1,10):\n    zoekwoorden=test['content'][xyz]+' '+test['title'][xyz]\n    tempspl = zoekwoorden.strip().split()\n    Qtemp=newDF\n    for sword in tempspl:\n        if sword in US_df.index:\n           #print(US_df.loc[sword:sword,:])\n           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n    #print(Qtemp)\n    #print(Qtemp.sum())\n    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n    #print(simila)\n    tempprnt=''\n    for xyb in range(0,5):\n        if simila[xyb]>85:\n            #or simila[xyb]==np.amax(simila[0:5]):\n            tempprnt+=columns[xyb]+' '\n    print('content/title mapp:',xyz,tempprnt)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}