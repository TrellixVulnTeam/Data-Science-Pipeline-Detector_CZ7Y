{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The problem statement is to train the model on topics related to biology, cooking, robotics, travel, diy and crypto and use this model to predict the tags of topics related to Physics.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Challenge","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The challenge of this competition is that it violates the basic assumption of machine learning which is that the train and test data should come from the same distribution and here the training data and the test data are completely different from each other.\n\nSince one title can belong to multiple categories at the same time this problem is a multilabel classification problem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Solution Approach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On a high level the solution approach would look like this:\n\n1. Data Ingestion\n2. Data Cleaning\n3. Data Preprocessing\n4. Exploratory Data Analysis (EDA)\n5. Feature Extraction\n6. Model Training\n7. Hyperparameter Tuning\n8. Model Evaluation\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import the necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nfrom scipy import sparse\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport ast\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('../input/transfer-learning-on-stack-exchange-tags')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Ingestion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"All the six data files biology, cooking, crypto, diy, robotics and travel were imported into a pandas dataframe and later all these six dataframes were appended into a single dataframe for analysis and model training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biology_data=pd.read_csv('biology.csv.zip')\ncooking_data=pd.read_csv('cooking.csv.zip')\ncrypto_data=pd.read_csv('crypto.csv.zip')\ndiy_data=pd.read_csv('diy.csv.zip')\nrobotics_data=pd.read_csv('robotics.csv.zip')\ntravel_data=pd.read_csv('travel.csv.zip')\ntest_data=pd.read_csv('test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data=combined_data.append([biology_data,cooking_data,crypto_data,diy_data,robotics_data,travel_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The text data consists of html tags and some leading and trailing spaces and hence we will write some helper function to clean our pandas dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html_tags(text):\n    \"\"\"Remove html tags from a string\"\"\"\n    import re\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_space(text):\n    \"Remove spaces from the text\"\n    s=text\n    s=s.strip()\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title']=combined_data['title'].apply(lambda x: remove_html_tags(x))\ncombined_data['content']=combined_data['content'].apply(lambda x: remove_html_tags(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title']=combined_data['title'].apply(lambda x: remove_space(x))\ncombined_data['content']=combined_data['content'].apply(lambda x: remove_space(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data=combined_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before doing further analysis and model training it is important to do some preprocessing. The following preprocessing will be done on the data\n\n1. Tokenization\n2. Stopwords Removal\n3. Remove Punctuation\n4. Lemmmatization\n5. Lowercase all the words in the text\n6. Combine the text of title and content column and train the model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(tokens):\n    from nltk.stem.snowball import SnowballStemmer\n    stemmer = SnowballStemmer(\"english\")\n    stemmed=[stemmer.stem(x) for x in tokens]\n    return stemmed\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text):\n    from nltk.tokenize import sent_tokenize, word_tokenize \n    return word_tokenize(text)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(tokens):\n    words = [word for word in tokens if word.isalpha()]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(tokens):\n    from nltk.corpus import stopwords\n    stop_words = stopwords.words('english')\n    extra_words = ['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly',\n              'across', 'actually', 'after', 'afterwards', 'again', 'against',\n              \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along',\n              'already', 'also', 'although', 'always', 'am', 'among', 'amongst',\n              'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone',\n              'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear',\n              'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as',\n              'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away',\n              'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes',\n              'becoming', 'been', 'before', 'beforehand', 'behind', 'being',\n              'believe', 'below', 'beside', 'besides', 'best', 'better',\n              'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\",\n              \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause',\n              'causes', 'certain', 'certainly', 'changes', 'clearly', 'co',\n              'com', 'come', 'comes', 'concerning', 'consequently', 'consider',\n              'considering', 'contain', 'containing', 'contains',\n              'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd',\n              'definitely', 'described', 'despite', 'did', \"didn't\",\n              'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done',\n              'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight',\n              'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially',\n              'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone',\n              'everything', 'everywhere', 'ex', 'exactly', 'example', 'except',\n              'f', 'far', 'few', 'fifth', 'first', 'five', 'followed',\n              'following', 'follows', 'for', 'former', 'formerly', 'forth',\n              'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets',\n              'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got',\n              'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly',\n              'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\",\n              'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter',\n              'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him',\n              'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit',\n              'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if',\n              'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed',\n              'indicate', 'indicated', 'indicates', 'inner', 'insofar',\n              'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\",\n              \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps',\n              'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later',\n              'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\",\n              'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks',\n              'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean',\n              'meanwhile', 'merely', 'might', 'more', 'moreover', 'most',\n              'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely',\n              'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither',\n              'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody',\n              'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing',\n              'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often',\n              'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only',\n              'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our',\n              'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own',\n              'p', 'particular', 'particularly', 'per', 'perhaps', 'placed',\n              'please', 'plus', 'possible', 'presumably', 'probably',\n              'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're',\n              'really', 'reasonably', 'regarding', 'regardless', 'regards',\n              'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw',\n              'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing',\n              'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves',\n              'sensible', 'sent', 'serious', 'seriously', 'seven', 'several',\n              'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so',\n              'some', 'somebody', 'somehow', 'someone', 'something', 'sometime',\n              'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry',\n              'specified', 'specify', 'specifying', 'still', 'sub', 'such',\n              'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th',\n              'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats',\n              'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence',\n              'there', \"there's\", 'thereafter', 'thereby', 'therefore',\n              'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\",\n              \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this',\n              'thorough', 'thoroughly', 'those', 'though', 'three', 'through',\n              'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took',\n              'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying',\n              'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless',\n              'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used',\n              'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value',\n              'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants',\n              'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\",\n              'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\",\n              'whatever', 'when', 'whence', 'whenever', 'where', \"where's\",\n              'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon',\n              'wherever', 'whether', 'which', 'while', 'whither', 'who',\n              \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will',\n              'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder',\n              'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you',\n              \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n              'yourself', 'yourselves', 'z', 'zero', '','is','based','aa','aaa','aac','aad','aav','ab','aa','aa aa',\n 'aa ab',\n 'aa batteri',\n 'aa lt',\n 'aaa',\n 'aabb',\n 'aabb aabb',\n 'aabbcc',\n 'aac',\n 'aad',\n 'aasa',\n 'aav',\n 'ab']\n    \n    new_stop=stop_words + extra_words\n    new_stop=list(set(new_stop))\n    filtered_words=[word for word in tokens if word not in new_stop]\n    return filtered_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_word(tokens):\n    words = [word.lower() for word in tokens]\n    return words\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title_words']=combined_data['title'].apply(lambda x: tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['content_words']=combined_data['content'].apply(lambda x: tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title_words']=combined_data['title_words'].apply(lambda x: remove_punctuation(x))\ncombined_data['content_words']=combined_data['content_words'].apply(lambda x: remove_punctuation(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title_words']=combined_data['title_words'].apply(lambda x: lower_word(x))\ncombined_data['content_words']=combined_data['content_words'].apply(lambda x: lower_word(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title_words']=combined_data['title_words'].apply(lambda x: remove_stopwords(x))\ncombined_data['content_words']=combined_data['content_words'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['text']=combined_data['title_words']+ combined_data['content_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#combined_data.loc[0,'title_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['text']=combined_data['text'].apply(lambda x: lemmatization(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['text']=combined_data['text'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################### Save Combined Data ############################################################\n#combined_data.to_csv('combined_data_preprocessed.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before training the model it is important to understand and explore the data. The following analysis will be done:\n\n1. No. of datapoints corresponding to each category\n2. Distribution of tags\n3. Number of unique tags\n4. Number of questions covered by taking x% of sample","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Datapoints corresponding to each category ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biology_data=biology_data.drop_duplicates(subset=['title'],) #### Removing rows with duplicate titles\ntravel_data=travel_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\ncooking_data=cooking_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\nrobotics_data=robotics_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\ndiy_data=diy_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\ncrypto_data=crypto_data.drop_duplicates(subset=['title'],)      #### Removing rows with duplicate titles\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datapoints=[]\ndatapoints.extend((biology_data.shape[0],travel_data.shape[0],cooking_data.shape[0],robotics_data.shape[0],diy_data.shape[0],\n                  crypto_data.shape[0]))\ntopics=['biology','travel','cooking','robotics','diy','crypto']\ntopic_count=pd.DataFrame({'topics':topics,'datapoints':datapoints})\ntopic_count['percentage']=(topic_count['datapoints']/topic_count['datapoints'].sum())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_count.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=topic_count['topics'],y=topic_count['datapoints'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=topic_count['topics'],y=topic_count['percentage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following observations can be drawn from the above plots:\n\n1. diy has the highest number of datapoints (30%)\n2. Robotics has the lowest number of datapoints (3.2 %)\n\nThis makes our problem statement harder because the language and vocabulary of robotics would have been closer to physics but the number of training datapoints are very less\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Distribution of tags","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_count = combined_data[\"tags\"].apply(lambda x: len(x.split(\" \"))) # counting the number of tags for each datapoint\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['Tags_Count'] = tags_count\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum number of tags per question = \"+str(max(combined_data['Tags_Count'])))\nprint(\"Minimum number of tags per question = \"+str(min(combined_data['Tags_Count'])))\nprint(\"Avg number of tags per question = \"+str(sum(combined_data['Tags_Count'])/len(combined_data['Tags_Count'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_per_tag=combined_data['Tags_Count'].value_counts()\nquestions_per_tag=pd.DataFrame(questions_per_tag)\nquestions_per_tag.reset_index(level=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_per_tag=questions_per_tag.rename(columns={'index':'tag_count','Tags_Count':'question_count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_per_tag['percentage']=(questions_per_tag['question_count']/questions_per_tag['question_count'].sum())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_per_tag.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=questions_per_tag['tag_count'],y=questions_per_tag['question_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=questions_per_tag['tag_count'],y=questions_per_tag['percentage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot the following observations could be drawn\n\n1. Only 8% of questions contains 5 tags\n2. Majority of questions 30% contains 2 tags\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tag Overlap ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['text_original']=combined_data['title_words'] + combined_data['content_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['tags_words']=combined_data['tags'].apply(lambda x: x.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.drop(['text_original'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(combined_data)):\n    #print(i)\n    tag_words=combined_data.loc[i,'tags_words']\n    title_words=combined_data.loc[i,'title_words']\n    common_title_words=set(tag_words)&set(title_words)\n    combined_data.loc[i,'title_overlap']=len(common_title_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(combined_data)):\n    #print(i)\n    tag_words=combined_data.loc[i,'tags_words']\n    content_words=combined_data.loc[i,'content_words']\n    common_content_words=set(tag_words)&set(content_words)\n    combined_data.loc[i,'content_overlap']=len(common_content_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['title_overlap_percent']=(combined_data['title_overlap']/combined_data['Tags_Count'])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data['content_overlap_percent']=(combined_data['content_overlap']/combined_data['Tags_Count'])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average Title Overlap = {}\".format(combined_data.title_overlap_percent.mean()))\nprint(\"Average Content Overlap = {}\".format(combined_data.content_overlap_percent.mean()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of Unique Tags ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(\" \"))\ntagcount = vectorizer.fit_transform(combined_data['tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of datapoints = {}\".format(tagcount.shape[0]))\nprint(\"Total number of unique tags = {}\".format(tagcount.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vectorizer.get_feature_names()[0:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most frequent tags ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#top 10 highest occurring tags\ncol_sum = tagcount.sum(axis = 0).A1 \nfeat_count = dict(zip(vectorizer.get_feature_names(), col_sum))\nfeat_count_sorted = dict(sorted(feat_count.items(), key = lambda x: x[1], reverse = True))\ncount_data = {\"Tags\":list(feat_count_sorted.keys()), \"Count\": list(feat_count_sorted.values())}\ncount_df = pd.DataFrame(data = count_data)\ncount_df[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df['Percentage']=(count_df['Count']/count_df['Count'].sum())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df=count_df[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 4))\nsns.barplot(x=count_df['Tags'],y=count_df['Count'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 4))\nsns.barplot(x=count_df['Tags'],y=count_df['Percentage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph depicts the percentage of occurence of top 10 tags\n\n1. The highest occuring tag electrical belongs to only 4489 (2 %) of the questions\n2. Tags like i.e. visas, air-travel, usa, schengen and uk belong to travel which is not a science related topic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Questions covered by Top n tags ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(\" \"), binary = True)\nlabels = vectorizer.fit_transform(combined_data['tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_sum = labels.sum(axis = 0).A1   \ncol_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_tags = np.argsort(-col_sum)  \nsorted_tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_n_tags(n):\n    multilabel_yn = labels[:,sorted_tags[:n]] \n    return multilabel_yn\n\ndef questionsCovered(n):\n    multilabel_yn = top_n_tags(n)\n    NonZeroQuestions = multilabel_yn.sum(axis = 1)  \n    return np.count_nonzero(NonZeroQuestions), NonZeroQuestions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questionsExplained = []\nnumberOfTags = []\nfor i in range(500,4268,500):\n    questionsExplained.append(round((questionsCovered(i)[0]/labels.shape[0])*100,2))\n    numberOfTags.append(i)\n    \nplt.figure(figsize = (16, 8))\nplt.plot(numberOfTags, questionsExplained)\nplt.title(\"Number of Tags VS Percentage of Questions Explained(%)\", fontsize=20)\nplt.xlabel(\"Number of Tags\", fontsize=15)\nplt.ylabel(\"Percentage of Questions Explained(%)\", fontsize=15)\nplt.scatter(x = numberOfTags, y = questionsExplained, c = \"blue\", s = 70)\nfor x, y in zip(numberOfTags, questionsExplained):\n    plt.annotate(s = '({},{}%)'.format(x, y), xy = (x, y), fontweight='bold', fontsize = 12, xytext=(x+70, y-0.3), rotation = -20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following import conclusions can be drawn:\n1. 90 % of the questions are covered if we take top 500 labels\n2. 96 % of the questions are covered if we take top 1000 labels\n\nBased on the fact that we have limited computation power it would make sense to take top 500 labels and train our multilabel classification problem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sumOfRows = questionsCovered(500)[1]\nRowIndicesZero = np.where(sumOfRows == 0)[0]  #this contains indices of all the questions for which the tags are removed\ndata_new = combined_data.drop(labels = RowIndicesZero, axis = 0)\ndata_new.reset_index(drop = True, inplace = True)\nprint(\"Size of new data = \",data_new.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing tags from data\ndata_tags = top_n_tags(500)\ndf = pd.DataFrame(data_tags.toarray())\nTagsDF_new = df.drop(labels = RowIndicesZero, axis = 0)\nTagsDF_new.reset_index(drop = True, inplace = True)\nprint(\"Size of new data = \",TagsDF_new.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that after taking the top 500 labels we are covering 78276 (90%) questions out of 86968 questions    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train and Validation Split","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will split our dataset into train and validation set and after that we will perform feature extraction using TF-IDF technique in order to feed our data to the machine learning model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"allTags = sparse.csr_matrix(TagsDF_new.values)\nx_train, x_test, y_train, y_test = train_test_split(data_new, allTags, test_size=0.20, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train data shape \",x_train.shape)\nprint(\"Train label shape\", y_train.shape)\nprint(\"Test data shape \",x_test.shape)\nprint(\"Test label shape\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction - Using TF-IDF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to train our multilabel classification model it is important to extract features from our text so that we can feed it into our classification algorithm.\n\nWe will use TF-IDF method to extract features from our dataset\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=10000, ngram_range = (1,3), tokenizer = lambda x: x.split(\" \"))\nTrainData = vectorizer.fit_transform(x_train['text'])\nTestData = vectorizer.transform(x_test['text'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sparse.save_npz(\"FinalTrain.npz\", TrainData)       ####### Saving Training and Test data in sparse format for later use  \n#sparse.save_npz(\"FinalTest.npz\", TestData)\n#sparse.save_npz(\"FinalTrainLabels.npz\", y_train)\n#sparse.save_npz(\"FinalTestLabels.npz\", y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FinalTrain = sparse.load_npz(\"FinalTrain.npz\")      ####### Loading Training and Test Data for training\n#FinalTest = sparse.load_npz(\"FinalTest.npz\")\n#FinalTrainLabels = sparse.load_npz(\"FinalTrainLabels.npz\")\n#FinalTestLabels = sparse.load_npz(\"FinalTestLabels.npz\")\n\nFinalTrain=TrainData\nFinalTest=TestData\nFinalTrainLabels=y_train\nFinalTestLabels=y_test\n\nprint(\"Dimension of train data = \",TrainData.shape)\nprint(\"Dimension of test data = \",TestData.shape)\nprint(\"Dimension of train labels \",y_train.shape)\nprint(\"Dimension of Test labels \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training - Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now since we have splitted our dataset into train and validation set and also we have extracted tf-idf features from our dataset we are ready to train our model\n\nSince it is a multilabel classification problem we will use Logistic Regression with one vs rest strategy in order to train our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier= OneVsRestClassifier(LogisticRegression(C=0.9,penalty='l1',solver='saga'), n_jobs=-1)\nclassifier.fit(FinalTrain, FinalTrainLabels)\npredictions = classifier.predict(FinalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_train=classifier.predict(FinalTrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Accuracy :\",accuracy_score(FinalTrainLabels,prediction_train))\nprint(\"Train Macro f1 score :\",f1_score(FinalTrainLabels, prediction_train, average = 'macro'))\nprint(\"Train Micro f1 scoore :\",f1_score(FinalTrainLabels, prediction_train, average = 'micro'))\nprint(\"Train Classification Report :\\n\",classification_report(FinalTrainLabels, prediction_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation Accuracy :\",accuracy_score(FinalTestLabels,predictions))\nprint(\"Validation Macro f1 score :\",f1_score(FinalTestLabels, predictions, average = 'macro'))\nprint(\"Validation Micro f1 scoore :\",f1_score(FinalTestLabels, predictions, average = 'micro'))\nprint(\"Validation Classification Report :\\n\",classification_report(FinalTestLabels, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Save Model for later use #################################################\n##filename = 'best_model_l1_saga_f1_0.47.sav'\n#joblib.dump(classifier, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training- Naive Bayes\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_1= OneVsRestClassifier(MultinomialNB(alpha=0.35), n_jobs=-1)\nclassifier_1.fit(FinalTrain, FinalTrainLabels)\npredictions_1 = classifier_1.predict(FinalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_train_1=classifier_1.predict(FinalTrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Accuracy :\",accuracy_score(FinalTrainLabels,prediction_train_1))\nprint(\"Train Macro f1 score :\",f1_score(FinalTrainLabels, prediction_train_1, average = 'macro'))\nprint(\"Train Micro f1 scoore :\",f1_score(FinalTrainLabels, prediction_train_1, average = 'micro'))\nprint(\"Train Classification Report :\\n\",classification_report(FinalTrainLabels, prediction_train_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation Accuracy :\",accuracy_score(FinalTestLabels,predictions_1))\nprint(\"Validation Macro f1 score :\",f1_score(FinalTestLabels, predictions_1, average = 'macro'))\nprint(\"Validation Micro f1 scoore :\",f1_score(FinalTestLabels, predictions_1, average = 'micro'))\nprint(\"Validation Classification Report :\\n\",classification_report(FinalTestLabels, predictions_1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating predictions on Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['title']=test_data['title'].apply(lambda x: remove_html_tags(x))\ntest_data['content']=test_data['content'].apply(lambda x: remove_html_tags(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['title']=test_data['title'].apply(lambda x: remove_space(x))\ntest_data['content']=test_data['content'].apply(lambda x: remove_space(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['title_words']=test_data['title'].apply(lambda x: tokenize(x))\n\ntest_data['content_words']=test_data['content'].apply(lambda x: tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['title_words']=test_data['title_words'].apply(lambda x: remove_punctuation(x))\ntest_data['content_words']=test_data['content_words'].apply(lambda x: remove_punctuation(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['title_words']=test_data['title_words'].apply(lambda x: lower_word(x))\ntest_data['content_words']=test_data['content_words'].apply(lambda x: lower_word(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['title_words']=test_data['title_words'].apply(lambda x: remove_stopwords(x))\ntest_data['content_words']=test_data['content_words'].apply(lambda x: remove_stopwords(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.reset_index(drop=True,inplace=True)\ntest_data['text']=test_data['title_words']+ test_data['content_words']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['text']=test_data['text'].apply(lambda x: lemmatization(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data['text']=test_data['text'].apply(lambda x: ' '.join(x))\n#test_data.to_csv('test_data_preprocessed.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.to_csv('test_data_preprocessed.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#test_data['title_words']=test_data['title_words'].apply(lambda x: ast.literal_eval(x))\n#test_data['content_words']=test_data['content_words'].apply(lambda x: ast.literal_eval(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#test_data['text']=test_data['title_words']*3 + test_data['content_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#vectorizer = TfidfVectorizer(max_features=50000, ngram_range = (1,3), tokenizer = lambda x: x.split(\" \"))\n#TrainData = vectorizer.fit_transform(x_train['text'])\ntest_data_features = vectorizer.transform(test_data['text'])   ##### We will use the vectoriser which we fit on training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_test=classifier.predict(test_data_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_test_df=pd.DataFrame(predictions_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"predictions_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Column Names ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer_label = CountVectorizer(tokenizer = lambda x: x.split(\" \"), binary = True)\nnew_labels = vectorizer_label.fit_transform(combined_data['tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(vectorizer_label.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top_label_indices=sorted_tags[0:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top_500=[vectorizer_label.get_feature_names()[i] for i in top_label_indices]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions Probability","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_probability=classifier.predict_proba(test_data_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_probability=pd.DataFrame(predictions_probability)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_probability.columns=top_500","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions_probability.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for c in predictions_probability.columns.values.tolist():\n    predictions_probability[c]=np.where(predictions_probability[c] >= 0.03,1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare test data for Submission","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cols_test = predictions_probability.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bt = predictions_probability.apply(lambda x: x > 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result=bt.apply(lambda x: list(cols_test[x.values]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result=pd.DataFrame(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result.columns=['tag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result['tag']=result['tag'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_data.reset_index(drop=True,inplace=True)\nresult.reset_index(drop=True,inplace=True)\nfinal_result=pd.concat([test_data,result],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_result.tag.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_result.loc[final_result['tag']==\"electrical\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission=final_result[['id','tag']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.columns=['id','tags']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('twelth_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the above analysis we conclude that the model performs satisfactory on Train and Validation set but the performance on test set is low.\n\n1. Train Set: 0.50 (Micro F1 score)\n2. Validation Set: 0.48 (Micro F1 score)\n3. Test Set (Kaggle Submission): 0.011 (Micro F1 score)\n\nThe major reason for low F1 score on test set could be that the domain and vocabulary of the test set is very different from the one on which the model was trained on.\n\nAlthough the score is less but by doing some more extensive feature engineering and hyperparameter tuning we can improve the performance of the model on test set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Next Steps","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Currently we have used TF-IDF for feature extraction and Logistic Regression for training a classifier model\n\nIn future we can improve the performance of the model by trying different techniques of feature extraction and model training\n\nFeature Extraction Techniques\n1. Increase the vocabulary of the TF-IDF feature extractor \n2. Extract POS tags from the text and use those POS tags as features to train our model\n3. Perform LDA and use topic vectors as features to train the multilabel classification model\n4. Train word2vec and doc2vec model on the data and use it to extract features from the data \n5. Extract features using Pretrained Models like Universal Sentence Encoder (USE) and BERT\n6. Dimension Reduction techniques like PCA\n7. During EDA we observed that content is matching with the tags more than title and hence we can give more weightage to content words as compared to title word and then train the model\n\nModel Training techniques\n\n1. Tree based models like Random Forest and XGBoost\n2. Neural Network Models like CNN, RNN and DNN\n3. Support Vector Machine (SVM)\n\n\nImportant Note: Currently in our multilabel classification problem we have made the assumption that all the labels are independent of each other. This assumption needs to be validated and accordingly the techniques needs to be modified  \n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}