{"cells":[{"metadata":{"_uuid":"5bddb72b2dd0b17eee0525665395d289148328aa"},"cell_type":"markdown","source":"\n\nThis is a simple exploratory study - Instacart Market Basket Analysis"},{"metadata":{"_uuid":"65e11c0f5aaf952307f722cd9f758b15adfc3d05"},"cell_type":"markdown","source":"\nMarket Basket Analysis is one of the key techniques used by large retailers to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions. To put it another way, it allows retailers to identify relationships between the items that people buy.    \n\nAssociation Rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.  \n\nAn example of Association Rules  \n\nAssume there are 100 customers  \n10 of them bought milk, 8 bought butter and 6 bought both of them.  \nbought milk => bought butter  \nsupport = P(Milk & Butter) = 6/100 = 0.06  \nconfidence = support/P(Butter) = 0.06/0.08 = 0.75  \nlift = confidence/P(Milk) = 0.75/0.10 = 7.5  \nNote: this example is extremely small. In practice, a rule needs the support of several hundred transactions, before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.  \n\nfor more information kindly take a look at this article explaining an UCI Machine Learning repository data [link](https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)\n\nFor those who do not have much hands-on experience in python, can go to this link to work through an amazing tutorial to understand Association Rules and Market Basket Analysis in XLminer [link](https://www.analyticsvidhya.com/blog/2014/08/effective-cross-selling-market-basket-analysis/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"615a1fea9d6baa6f3d6d1dccf6bc400df4b24b4f"},"cell_type":"markdown","source":"Typically, In most of the problems in Kaggle, we are given with 1 or 2 files to deal with. However, this problem has muliple files associated with it and took few minutes for me to understand the problem thoroughly. Nevertheless, I will break down the problem piece by piece and will try to explain the reader in the most comprehensive and easy-to-follow manner.   \n\nKaggle states the problem as follows:\n\nInstacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.  \n\nIn this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. \n\nThis can be simplified as, if a customer named \"Adam\" has bought the product \"Apple\" in his previous orders, what are the chances that he might buy the product \"Apple\" again in his next orders. Or, Predict the probability that Adam (decision : buy / dont buy) tries or buys the product \"Apple\" for the first time in his next order based on his previous buying behaviour / Transactions. \n "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Reading the files\norders = pd.read_csv(\"../input/orders.csv\")\ndepartments = pd.read_csv(\"../input/departments.csv\")\nproducts = pd.read_csv(\"../input/products.csv\")\norder_prod_train = pd.read_csv(\"../input/order_products__train.csv\")\norder_prod_prior = pd.read_csv(\"../input/order_products__prior.csv\")\naisles = pd.read_csv(\"../input/aisles.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bc567cdddf84a19b0ae66440eab75ab4f9cfa3b"},"cell_type":"markdown","source":"I would like to explain the relationship between the files and how different columns mean to us. Having a good knowledge in RDBMS concepts would help anyone understand this. Firstly, I would like to like to explain the files \"orders.csv\", \"order_products__train.csv\", \"orders_products__prior.csv\".\n\nBasically, these transactions are structured as follows:  \n\n*  A customer has a unique customer id and under each customer (or customer ID) , there can be multiple orders (or order ID's) with each order has its own unique id.\n\n* An order may contain one or more products and each product has its own unique product ID. Also, A product can be bought multiple number of times by multiple number of customers.Hence, a product or product ID can occur in multiple orders under multiple customers. Hope this makes sense!!!!\n"},{"metadata":{"trusted":true,"_uuid":"55ff2891565bc230c19c4436df5e87bab7bc95f2"},"cell_type":"code","source":"datafiles = [orders, departments, products, order_prod_train, order_prod_prior,aisles]\n\nfor var in datafiles:\n    print(var.info())\n    print(\"-\"*100)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57699bba3ffe865eda20c45d205da4b61b6836e"},"cell_type":"markdown","source":"\n\n\nLet's Take a look at the orders.csv (already loaded into dataframe \"orders\")"},{"metadata":{"trusted":true,"_uuid":"20314681b79789bf7d12cee144ca7d331d6b5dc0"},"cell_type":"code","source":"orders.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc6bfaf4bb7350fdf35eadcc72e1359d46c55d59"},"cell_type":"markdown","source":"As you can see that the orders.csv file contains the all the data pertaining to an order such as the user who have placeed the order, the day of the week the order was placed,order_hour_of_the_day, eval set,and days_since_last_order.. By the time, you should have already noticed that order_id is not arranged in any specific sorting (ascending or descent order from the numerical perspective). However, the rows are sorted as per th user_id (customer) and again the rows are further sorted using the order_number\n\nORDER_NUMBER COLUMN:-\nYou may get confused with the order_id and order number. I will explain the difference below using the kaggle data description:-\n\n*\"The dataset for this competition is a relational set of files describing customers' orders over time. The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between **4 and 100 of their orders**, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders. For more information, see the blog post accompanying its public release.\"*\n\nIt might be a bit confusing. what is \"4 and 100 of their orders\". Let's put it this way!!!\n\nJust for the sake of better understanding and explanation,, I am assuming that Instacart only has 100 customers (**ASSUMPTION**). Out of these 100 customers, there might be *25 customers* who might have totally made** 4 orders** throughout his entire shopping history with Instacart. Another set of *25 customers* might have made **15 orders** totally, and another *25 customers* might have made **60 orders**, and the last set of* 25 customers* might have made **100 orders** totally  \n\nHere the minimum is 4 and the maximum is 100. Now we can translate this to our real dataset. \nIn our dataset, no Instacart customers have made less than 4 orders or have made more than 100 orders throughtout their shopping history with Instacart. Let's verify if that's true with our dataset\n\nEVAL_SET COLUMN:\n\neval_set has 3 values (categorical values namely - prior, train, test). The latest (last) order of every customer was taken and split into train and test. See the below example for brevity"},{"metadata":{"trusted":true,"_uuid":"746fd2d0e90bf3ffb5646bcc47aba2ee23f1d206"},"cell_type":"code","source":"orders.head(40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7694af2d00e0c72c0804edeea0414c9ccd528cd5"},"cell_type":"markdown","source":"Kindly note the row indices 10, 25, 38. You can see those 3 different rows corresponds to 3 different user id (customer) and they are the last or the latest order. Rest other rows corresponding to any user goes into the prior dataset.\n\nNow, you would have guessed what the \"order_products__train.csv\" and \"order_products__prior.csv\" would contain. "},{"metadata":{"trusted":true,"_uuid":"afa292c4d681dffd9614d32ba30fd48aa708ba4e"},"cell_type":"code","source":"#order_products__prior.csv contains all the prior data\norder_prod_prior.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7276237a1bb4a9e5388f23ad2eac613a5d4e06b7"},"cell_type":"code","source":"#order_products__train.csv conatins all the train data\norder_prod_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4c0587d264253f18c572ccd82e0a2374839a4cb"},"cell_type":"markdown","source":"**Caution: Don't get confused with the 3 previous generated tables (notice the difference between order_id and user_id).**\n\nI am going to use one of the most powerful yet simple functions in pandas - pd.groupby(). Inorder to know more about groupby and its related methods kindly look into the documentation. [link](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html) "},{"metadata":{"trusted":true,"_uuid":"db05d08742c38cb3a98be29ba710c9e94cca8635"},"cell_type":"code","source":"import IPython\n \n# Grouping by one factor\ndf_id = orders.groupby('user_id')\n \n# Getting all methods from the groupby object:\nattr = [method for method in dir(df_id)\n if callable(getattr(df_id, method)) & ~method.startswith('_')]\n \n# Printing the result\nprint(IPython.utils.text.columnize(attr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8089ecd7f925f9720785eb76da3f6c7cfcfa556a"},"cell_type":"markdown","source":"There are 59 methods that you could call on a groupby object!!!!! I am super excited to get started!!!! Let's Dive deep!!!!"},{"metadata":{"trusted":true,"_uuid":"4f98788be62e3f9ffab4458af7a289c536dc270d"},"cell_type":"markdown","source":"we will first verify the kaggle's statement whether the no of orders cap ranges from 4 to 100"},{"metadata":{"trusted":true,"_uuid":"aaa237b3e12a1bce4edb6b4fca7c8fe0ed1343ee"},"cell_type":"code","source":"#group by \"user_id\" and take the max value of \"order_number\" column for each group in user_id and aggregate it.\ncap = orders.groupby(\"user_id\")[\"order_number\"].aggregate(np.max).reset_index()\ncap = cap.order_number.value_counts()\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(15,12))\nsns.barplot(cap.index, cap.values)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Max order number', fontsize=12)\nplt.xticks(rotation= 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35cf17d95aa72f2bd08fe985febc8dc124018fa"},"cell_type":"markdown","source":"The distribution seems to be right skewed!!!. no of customers drops as the maximum order number increases."},{"metadata":{"trusted":true,"_uuid":"de8578693a8aab718ca66f185689b736c6c934e5"},"cell_type":"code","source":"#Let's check how many customers are there totally\nprint(\"There are {} customers\".format(sum(orders.groupby(\"eval_set\")[\"user_id\"].nunique().values[1:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c2c92f53733c827d20f6f64efb8f313d8c89a0"},"cell_type":"code","source":"cap = orders.groupby(\"eval_set\").size()\nplt.figure(figsize = (10,7))\nsns.barplot(cap.index, cap.values, palette = \"coolwarm\")\nplt.ylabel(\"No of Orders\", fontsize = 14)\nplt.xlabel(\"Dataset\",fontsize = 14)\nplt.title(\"number of unique orders in different dataset\", fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf16512636f89132864aefbfc0a680fe767b10df"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"4a26158e366fdaeaebc64e184888999607ce0353"},"cell_type":"markdown","source":"It can be noticed that prior datset has the most orders because the partition is done in such a way that only the last order of customers are split into train and test, whereas, rest all the orders goes into prior dataset  \n\nLet's start the customer buying patterns with respect to time and day"},{"metadata":{"trusted":true,"_uuid":"48d10eff0ce363531ee2ab23c14b61edadced19b"},"cell_type":"markdown","source":"<font size = \"5\"> **Day of the week - Shopping Behaviour** </font>"},{"metadata":{"trusted":true,"_uuid":"c840171eecd177edfdce8cecb7d740a565b2f04e"},"cell_type":"code","source":"cap = orders.groupby(\"order_dow\")[\"order_id\"].size()\n#sns.barplot(cap.index, cap.values, color = color[9], alpha = 0.8)\nplt.figure(figsize=(10,8))\nax = sns.barplot(cap.index, cap.values, alpha=0.8, color=color[9])\nax.set_xlabel('Day of the week', fontsize = 10)\nax.set_ylabel('Total number of orders placed during the day',fontsize = 10)\nax.set_xticklabels([\"Sat\", \"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\"], fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf2398ce1963413512fa8045abe5e1c54b55a9e2"},"cell_type":"markdown","source":"<font size = \"3\">During Saturdays and Sundays, the volume of orders placed are considerably higher than other day, with volume of orders placed during Wednesdays being the lowest!!!</font>"},{"metadata":{"_uuid":"c9c9e29c3dfd68a7f2aaa20fff834f376444fd22"},"cell_type":"markdown","source":"<font size = \"5\"> **Hour of the Day Break down - Order Pattern** </font>"},{"metadata":{"trusted":true,"_uuid":"de8b48370e0a148a1830bbf49a4bd004e297007f"},"cell_type":"code","source":"cap = orders.groupby(\"order_hour_of_day\")[\"order_id\"].size()\n#sns.barplot(cap.index, cap.values, color = color[9], alpha = 0.8)\nplt.figure(figsize=(10,8))\nax = sns.barplot(cap.index, cap.values, alpha=0.8, color=color[0])\nax.set_xlabel('Hour of the Day', fontsize = 12)\nax.set_ylabel('Hourly total number of orders', fontsize = 12)\nax.set_title('Hour of the day - Order Pattern', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ceb06dd55fe85ba834161260daa9c1a179256f9"},"cell_type":"markdown","source":"<font size = \"4\"> **Days since last order ** - Let's visualize the ordering pattern of customers </font>"},{"metadata":{"trusted":true,"_uuid":"c3467394119f96a7d5e7b6650b5b037557accae8"},"cell_type":"code","source":"plt.figure(figsize = (12,9))\nsns.countplot(\"days_since_prior_order\",data = orders, color = color[2])\nplt.ylabel('Frequency', fontsize=14)\nplt.xlabel('Days since prior order', fontsize=14)\nplt.xticks(rotation = 90)\nplt.title(\"Order Patterns of customers\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d72343266387ae21bcece8c1f48ea652c4a2f7"},"cell_type":"markdown","source":"One of the best skill that anyone could gain by having profound interest in data visualization is the art of storytelling with the data. In the above chart,Overall trend goes downward as the days increases.However, you can see the bars at 7.0, 14.0, 21.0, 28.0 has a sudden surge in the frequency of orders. This might be very easy to interpret as most people replenish their stock, once in a week, or once in 2 weeks or once in a month ( in this case, you can notice the bar at 30.0 days). \n\nHowever, one dominant insight that we could grab out of the chart is that*** most customer place their orders once in 7 days and once in 30 days. ***"},{"metadata":{"_uuid":"992a7e85eca20b020ca62c7ce746c56817cd4335"},"cell_type":"markdown","source":"<font size = \"5\">Hour of the day Vs Day of the week HEATMAP</font>"},{"metadata":{"_uuid":"c768a929dbae4d6512f9cced09304e6fed667d55"},"cell_type":"markdown","source":"Though we have generated separate vizzes for the Hour of the day and Day of the week, it would be interesting to know how volume of orders move across the hours throughout the weekdays."},{"metadata":{"trusted":true,"_uuid":"791f97a243c6ddbd19b8f02337d0317d6b862820"},"cell_type":"code","source":"df = orders.groupby([\"order_dow\", \"order_hour_of_day\"])[\"order_number\"].agg(\"count\").reset_index(name = \"count\")\npivoted_df = df.pivot('order_dow', 'order_hour_of_day', 'count')\n#pivoted_df.head()\nplt.figure(figsize=(14,8))\nsns.heatmap(pivoted_df)\nplt.title(\"Frequency of Orders - Day of week Vs Hour of day\")\nplt.ylabel(\"Day of the week\", fontsize = 14)\nplt.xlabel(\"Hour of the Day\", fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a49af36e77ffde1aac1152ae308906cffa8b7608"},"cell_type":"markdown","source":"Woaw!!! There's so much traffic and huge number of orders being placed on Saturday noon to Saturday Evenings and during Sunday Mornings!!!. It is very obvious that most people would like to shop during weekends after tiring weekdays."},{"metadata":{"trusted":true,"_uuid":"6ff08291ddae79eeb1fd7e2c785ba456c7c15ca8"},"cell_type":"markdown","source":"To proceed our exploratory journey with other 3 data files, it is very useful to see the whole picture rather than seeing them as train, prior etc. Hence, Now to facilitate the EDA, I am going to combine both \"order_products__prior.csv\" and \"order_products__prior.csv\" data files into one single dataframe"},{"metadata":{"trusted":true,"_uuid":"43b803a886d6cfa22a093157a379d0a7f803bc5b"},"cell_type":"code","source":"concat = pd.concat([order_prod_prior, order_prod_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44b6cfe439694a442bfe3e8f675e91d0bb2b0cf6"},"cell_type":"code","source":"#checking and validating the concatenation of the two dataset\nlen(concat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3a3c2a7bb7ad436d1874e27c714d06e43f0f22a"},"cell_type":"code","source":"len(order_prod_prior) + len(order_prod_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29e906ef307932f4604989240cc82c25ea2da741"},"cell_type":"markdown","source":"Let's now go ahead with merging the data files products.csv, aisles.csv, departments.csv with the concatenated dataset(order_train and order_prior). While merging the dataset, there are various types of joins such as inner join, outer join, left join, right join etc. However, we have to be wary while making the join as some rows may get deleted or ignored (for example, inner join will only keep the rows that are mutually present in both the datsets. Here, I ma going to merge with left join as we want to retain all the rows in the concat and simultaneously fill with the information from other datasets. For more information on joins, kindly look into this [http://www.sql-join.com/sql-join-types](http://www.sql-join.com/sql-join-types)"},{"metadata":{"trusted":true,"_uuid":"3adfb3aa2be0e55e08dd11d572110877faa43dfb"},"cell_type":"code","source":"#left joining the dataset so that none of the rows in the concat dataset gets deleted or ignored during the join\nconcat = pd.merge(concat, products, on='product_id', how='left')\nconcat = pd.merge(concat, aisles, on='aisle_id', how='left')\nconcat = pd.merge(concat, departments, on='department_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90302cb52ca41226d83911cddfd5666f312f2125"},"cell_type":"markdown","source":"Now we have two solid and complete datasets to work on and are able to see the clear picture of how datasets and variables within them are interconnected to each other"},{"metadata":{"trusted":true,"_uuid":"f281e9ca32270dcacff19e60f5dd6ac193241f3d"},"cell_type":"code","source":"orders.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaeb5c7f1dbf7a7f4f3c65c4b9d2251675e1e087"},"cell_type":"code","source":"concat.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6d81fb3c98447c58e2c581ba91a0b3d298f201"},"cell_type":"markdown","source":"Firstly, we will find out what products customers are buying and replenish frequently."},{"metadata":{"trusted":true,"_uuid":"ad946ddbcebc623993b4cceca06723de9f9e4e03"},"cell_type":"code","source":"cap = concat.groupby(\"product_name\").size().sort_values(ascending = False)[:20]\nprint(cap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb09195b67f5f0a074dee606dd9724a479ac47bf"},"cell_type":"code","source":"plt.figure(figsize= (14,8))\nsns.barplot(cap.index, cap.values, color=color[1])\nplt.ylabel(\"Frequency\", fontsize = 12)\nplt.xlabel(\"Products\", fontsize = 12)\nplt.xticks(rotation = 90, fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06f9e9e045e74c7d19b0e873e04dcd9a4a6d949b"},"cell_type":"markdown","source":"Most frequently purchased products are Perishables (Fruits, Vegetables, Diary products)  \nAs we can expect from the above diagrams, the most visited aisles and departments would be the ones with perishables"},{"metadata":{"trusted":true,"_uuid":"0506a67163e029d1a6de446c3c8debac52fe4fc2"},"cell_type":"code","source":"cap = concat.groupby(\"aisle\").size().sort_values(ascending = False)[:20]\nprint(cap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfcd1587dda3ec7f827df52b71c420214f0c92c0"},"cell_type":"code","source":"plt.figure(figsize= (14,8))\nsns.barplot(cap.index, cap.values, color=color[3])\nplt.ylabel(\"Frequency\", fontsize = 12)\nplt.xlabel(\"Products\", fontsize = 12)\nplt.xticks(rotation = 90, fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e57ccd23f7500a3bfaf2aca309a885a69b7595b"},"cell_type":"code","source":"cap = concat.groupby(\"department\").size().sort_values(ascending = False)[:20]\nprint(cap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"760ae662bb9e5b61384e0b6c195c9bde3836089b"},"cell_type":"code","source":"plt.figure(figsize= (14,8))\nsns.barplot(cap.index, cap.values, color=color[8])\nplt.ylabel(\"Frequency\", fontsize = 12)\nplt.xlabel(\"Products\", fontsize = 12)\nplt.xticks(rotation = 90,fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3157fe1112937d3b7fb3de48fb65747da9725258"},"cell_type":"markdown","source":"Next, it would be great to see which department dominates the Instacart by variety of products!!! "},{"metadata":{"trusted":true,"_uuid":"b2b907e39e9451ed43fce5e6317a69fb28074f5b"},"cell_type":"code","source":"concat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b9f0abde18dfe457209d47040dbfd1d5713307f"},"cell_type":"code","source":"items  = pd.merge(left =pd.merge(left=products, right=departments, how='left'), right=aisles, how='left')\nitems.head()\ngroup_val = items.groupby(\"department\")[\"product_id\"].count().sort_values(ascending = False)\nplt.figure(figsize=(12,12))\nlabels = (np.array(group_val.index))\nsizes = (np.array((group_val / group_val.sum())*100))\nplt.pie(sizes, labels=labels, \n        autopct='%1.1f%%', startangle=200)\nplt.title(\"Departments distribution\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a114063c4908d66f386170ce4774edf028cc312"},"cell_type":"markdown","source":"Personal care department is the one which is dominating and has wide range of cosmetics products stacked in the department!!!. "},{"metadata":{"_uuid":"0e92043fefbdc31dc21f8f23cf2720e9ff0f3e13"},"cell_type":"markdown","source":"Mostly in any product based or service based organization, most priority and resources on its most revenue generating products or line of services. Though we dont know the price and the profit margins of these products, we assume the revenue goes up as the number of products sold grows up!!!!"},{"metadata":{"trusted":true,"_uuid":"61e2c74d4748b11c51a38de1c78451b07ce2ceac"},"cell_type":"code","source":"users_flow = orders[['user_id', 'order_id']].merge(concat[['order_id', 'product_id']],how='inner', left_on='order_id', right_on='order_id')\nusers_flow = users_flow.merge(items, how='inner', left_on='product_id',right_on='product_id')\ngrouped = users_flow.groupby(\"department\")[\"order_id\"].count().reset_index(name = \"Total_orders\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e9125013278e526658f9fb3bafdc35483e46e38"},"cell_type":"code","source":"grouped.sort_values(by = \"Total_orders\",ascending=False, inplace=True)\ngrouped = grouped.reset_index(drop = True)\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped['department'], grouped['Total_orders'].values, alpha=0.8, color=color[0])\nplt.ylabel('Sales', fontsize=12)\nplt.xlabel('Department', fontsize=12)\nplt.title(\"Departmentwise Sales\", fontsize=15)\nplt.xticks(rotation='vertical')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98587aca38d729231b095f2943b6fee9a923ed3d"},"cell_type":"markdown","source":"All the food and related products are the forefront drivers of the Instacart revenue. Without Perishables, I suppose Instacart's success would perish!!!!!"},{"metadata":{"trusted":true,"_uuid":"79c22e0cb54fd099a4fbc57dee8f8e4c10638283"},"cell_type":"code","source":"orders.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf59c4eedb8372bb6bd5a5315c144c4e7fe2abc6"},"cell_type":"code","source":"concat.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd207e3c180b0cfae1e1c7eb06c3cb1b032593f6"},"cell_type":"markdown","source":"<font size = \"5\">In Progress</font>"},{"metadata":{"trusted":true,"_uuid":"b7913449bf8e132cd5feda75af7c48720b5db873"},"cell_type":"code","source":"concat_orders = pd.merge(orders, concat, on = \"order_id\", how = \"right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd16da1544216aacb179f52dcf732ee22c7a260c"},"cell_type":"code","source":"concat_orders.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f1c094ce0dcfbe08956b45e171087f4b6203c84"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ab240acfb538579e7910875df0218926af3ac4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}