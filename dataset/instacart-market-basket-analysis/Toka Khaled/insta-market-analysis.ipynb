{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prediciting your Future order!\n> This notebook is implementing a predictive analysis model, that predicts the products ordered in users' future order based on each purchasing history.","metadata":{}},{"cell_type":"markdown","source":"## Model Usage\n> **How this model will make better customer shopping expireince?**\n\n> Customers tend to do things quick and in an easy way. Thus can integrate this model with instacart's shopping software to initially autofill user's basket once openned the application.","metadata":{}},{"cell_type":"markdown","source":"## 1. Reading Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# garbage collector to free up memory\nimport gc\ngc.enable()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-05T13:37:42.01977Z","iopub.execute_input":"2022-06-05T13:37:42.020233Z","iopub.status.idle":"2022-06-05T13:37:42.052618Z","shell.execute_reply.started":"2022-06-05T13:37:42.020142Z","shell.execute_reply":"2022-06-05T13:37:42.051647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orders = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/orders.csv' )\norder_products_train = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/order_products__train.csv')\norder_products_prior = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/order_products__prior.csv')\nproducts = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/products.csv')\naisles = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/aisles.csv')\ndepartments = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/departments.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:37:42.056888Z","iopub.execute_input":"2022-06-05T13:37:42.05715Z","iopub.status.idle":"2022-06-05T13:38:02.03054Z","shell.execute_reply.started":"2022-06-05T13:37:42.057105Z","shell.execute_reply":"2022-06-05T13:38:02.029523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(train_data):\n    \n#  iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n    start_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n        end_mem = train_data.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:02.032423Z","iopub.execute_input":"2022-06-05T13:38:02.032801Z","iopub.status.idle":"2022-06-05T13:38:02.052429Z","shell.execute_reply.started":"2022-06-05T13:38:02.032718Z","shell.execute_reply":"2022-06-05T13:38:02.051356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(order_products_prior)\nreduce_mem_usage(order_products_train)\nreduce_mem_usage(products)\nreduce_mem_usage(orders)\nreduce_mem_usage(departments)\nreduce_mem_usage(aisles)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:02.05586Z","iopub.execute_input":"2022-06-05T13:38:02.056873Z","iopub.status.idle":"2022-06-05T13:38:04.10226Z","shell.execute_reply.started":"2022-06-05T13:38:02.05682Z","shell.execute_reply":"2022-06-05T13:38:04.101266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get shape of each df\nprint(f\" aisles : {aisles.shape} \\n depts : {departments.shape} \\n order_prod_prior : {order_products_prior.shape} \\n order_products_train : {order_products_train.shape} \\n orders : {orders.shape} \\n products : {products.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:04.104012Z","iopub.execute_input":"2022-06-05T13:38:04.106984Z","iopub.status.idle":"2022-06-05T13:38:04.112951Z","shell.execute_reply.started":"2022-06-05T13:38:04.106936Z","shell.execute_reply":"2022-06-05T13:38:04.11191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions to be able to correclty calculate the avergae of hours\n# The problem is that if we deal with hours and average them normally, the average of 1:00 and 23:00 will be 12 not 0:00\nimport datetime\nimport math\n\ndef datetime_to_radians(x):\n    # radians are calculated using a 24-hour circle, not 12-hour, starting at north and moving clockwise\n    seconds_from_midnight = 3600 * x\n    radians = float(seconds_from_midnight) / float(12 * 60 * 60) * 2.0 * math.pi\n    return radians\n\ndef average_angle(angles):\n    # angles measured in radians\n    x_sum = np.sum(np.sin(angles))\n    y_sum = np.sum(np.cos(angles))\n    x_mean = x_sum / float(len(angles))\n    y_mean = y_sum / float(len(angles))\n    return np.arctan2(x_mean, y_mean)\n\ndef radians_to_time_of_day(x):\n    # radians are measured clockwise from north and represent time in a 24-hour circle\n    seconds_from_midnight = int(float(x) / (2.0 * math.pi) * 12.0 * 60.0 * 60.0)\n    hour = seconds_from_midnight // 3600 % 24\n    minute = (seconds_from_midnight % 3600) // 60\n    second = seconds_from_midnight % 60\n    return datetime.time(hour, minute, second)\n    \ndef average_times_of_day(x):\n    # input datetime.datetime array and output datetime.time value\n    angles = [datetime_to_radians(y) for y in x]\n    avg_angle = average_angle(angles)\n    return radians_to_time_of_day(avg_angle)\n\ndef day_to_radians(day):\n    radians = float(day) / float(7) * 2.0 * math.pi\n    return radians\ndef radians_to_days(x):\n    day = int(float(x) / (2.0 * math.pi) * 7) % 7\n    return day\ndef average_days(x):\n    angles = [day_to_radians(y) for y in x]\n    avg_angle = average_angle(angles)\n    return radians_to_days(avg_angle)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:04.114746Z","iopub.execute_input":"2022-06-05T13:38:04.115298Z","iopub.status.idle":"2022-06-05T13:38:04.131782Z","shell.execute_reply.started":"2022-06-05T13:38:04.115254Z","shell.execute_reply":"2022-06-05T13:38:04.130519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Predictor Features","metadata":{}},{"cell_type":"markdown","source":"### 2.1 User predictors\n","metadata":{}},{"cell_type":"code","source":"# We keep only the prior orders\nusers = orders[orders['eval_set'] == 'prior']\nusers['days_since_prior_order'].dropna()\n\n# We group orders by user_id & calculate the variables based on different user_id\nusers = users.groupby('user_id').agg(\n    \n user_orders= ('order_number' , max),\n user_period=('days_since_prior_order', sum),\n user_mean_days_since_prior = ('days_since_prior_order','mean')\n    \n)\nusers.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:04.133339Z","iopub.execute_input":"2022-06-05T13:38:04.134457Z","iopub.status.idle":"2022-06-05T13:38:04.537046Z","shell.execute_reply.started":"2022-06-05T13:38:04.134411Z","shell.execute_reply":"2022-06-05T13:38:04.535805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### calculate three more new variables:\n\nuser_total_products: Total numbers of basket items included in user's orders\n\nuser_reorder_ratio: Ratio a user orders reorordered products.\n\nuser_distinct_products: Total number of distinct products ordered by a user","metadata":{}},{"cell_type":"code","source":"# We create a new table \"orders_products\" which contains the tables \"orders\" and \"order_products_prior\"\norders_products =pd.merge(orders , order_products_prior, on='order_id', how='inner')\n\n# Getting the number of products in each basket(order)\ngroupedorders_products = orders_products.groupby(['order_id']).agg(\n    basket_size = ('product_id', 'count')\n).reset_index()\norders_products = orders_products.merge(groupedorders_products, on='order_id', how='left')\norders_products.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:04.538801Z","iopub.execute_input":"2022-06-05T13:38:04.539418Z","iopub.status.idle":"2022-06-05T13:38:22.702539Z","shell.execute_reply.started":"2022-06-05T13:38:04.539372Z","shell.execute_reply":"2022-06-05T13:38:22.701275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orders_products['p_reordered']= orders_products['reordered']==1\norders_products['non_first_order']= orders_products['order_number']>1\n\nus=orders_products\n\n# We group orders_products by user_id & calculate the variables based on different user_id\nus=orders_products.groupby('user_id').agg(\n    \n     user_total_products =('user_id','count') ,\n     p_reordered =('p_reordered', sum) ,\n     non_first_order =('non_first_order', sum),\n     user_distinct_products=('product_id','nunique')\n\n).reset_index()\n#    us['user_reorder_ratio'] = sum(reordered == 1) / sum(order_number > 1)\nus['user_reorder_ratio']=us['p_reordered']/us['non_first_order']\n\ndel us[\"p_reordered\"],us[\"non_first_order\"]\ndel orders_products['p_reordered' ],orders_products['non_first_order']\n\nus.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:22.704471Z","iopub.execute_input":"2022-06-05T13:38:22.704798Z","iopub.status.idle":"2022-06-05T13:38:34.801757Z","shell.execute_reply.started":"2022-06-05T13:38:22.704734Z","shell.execute_reply":"2022-06-05T13:38:34.800702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Then we combine the users and us tables ussing inner_join() function and we calculate the final variable:\n\nuser_average_basket: Average number of basket items per order per user","metadata":{}},{"cell_type":"code","source":"users =pd.merge(users,us ,on='user_id',  how='inner')\n\n# We calculate the user_average_basket variable\nusers['user_average_basket'] = users['user_total_products'] / users['user_orders']\nusers.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:34.806255Z","iopub.execute_input":"2022-06-05T13:38:34.806504Z","iopub.status.idle":"2022-06-05T13:38:34.886042Z","shell.execute_reply.started":"2022-06-05T13:38:34.806473Z","shell.execute_reply":"2022-06-05T13:38:34.884924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now identify the future order per user and add them in the users table. The future orders are indicated as train and test in the eval_set variable. As a result, we will know what is the order_id of the future order per user, whether this order belongs in the train or test set, and the time in days since the last order.","metadata":{}},{"cell_type":"code","source":"# we exclude prior orders and thus we keep only train and test orders\nus = orders[orders['eval_set'] != 'prior']\nus['time_since_last_order'] = us['days_since_prior_order']\nus['future_order_dow'] = us['order_dow']\nus['future_order_hour_of_day'] = us['order_hour_of_day']\n\nus = us[['user_id','order_id','eval_set','time_since_last_order', 'future_order_dow', 'future_order_hour_of_day']]\n\n# We combine users and us tables and store the results into the users table\nusers_features = pd.merge(users , us, on='user_id', how='inner') \n\n# We delete the us table\ndel us, users\n\nusers_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:34.8882Z","iopub.execute_input":"2022-06-05T13:38:34.888592Z","iopub.status.idle":"2022-06-05T13:38:35.007609Z","shell.execute_reply.started":"2022-06-05T13:38:34.888535Z","shell.execute_reply":"2022-06-05T13:38:35.006413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Product dependent Features\n* Total number of orders per product\n* Avg position in cart for the product","metadata":{}},{"cell_type":"code","source":"prod_features = orders_products.groupby(['product_id']).agg(\n    prod_freq = ('order_id', 'count'),\n    prod_avg_position = ('add_to_cart_order', 'mean')\n    ,prod_avg_hour = ('order_hour_of_day', average_times_of_day),\n    prod_avg_dow = ('order_dow', average_days)\n).reset_index()\n\nprod_features.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:41:49.676074Z","iopub.execute_input":"2022-06-05T13:41:49.677083Z","iopub.status.idle":"2022-06-05T13:43:22.638512Z","shell.execute_reply.started":"2022-06-05T13:41:49.677036Z","shell.execute_reply":"2022-06-05T13:43:22.637357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Probability a product is reordered after the first order\n* On average how many times a product has been purchased by the users who purchased it at least once","metadata":{}},{"cell_type":"code","source":"non_first_order = orders_products['order_number'] != 1\n\ngroupedorders_products = orders_products[non_first_order].groupby(['product_id']).agg(\n    prod_reorder_ratio = ('reordered', 'mean')\n).reset_index()\n\nprod_features = prod_features.merge(groupedorders_products, on='product_id', how='left')\n\n# Group by users who have bought it more than once\n# get the count of orders each user bought having the product. \ngroupedorders_products = orders_products[non_first_order].groupby(['product_id', 'user_id']).agg(\n    user_prod_freq = ('order_id', 'count')\n).reset_index()\n\n# get the avg # of orders the user will buy having that product after buying it for the first time\ngroupedorders_products = groupedorders_products.groupby(['product_id']).agg(\n    user_prod_avg_freq = ('user_prod_freq', 'mean')\n).reset_index()\n\nprod_features = prod_features.merge(groupedorders_products, on='product_id', how='left')\ndel groupedorders_products, non_first_order\n\nprod_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:43:22.640998Z","iopub.execute_input":"2022-06-05T13:43:22.64132Z","iopub.status.idle":"2022-06-05T13:43:41.615445Z","shell.execute_reply.started":"2022-06-05T13:43:22.641275Z","shell.execute_reply":"2022-06-05T13:43:41.614334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 User x Product Predictors \n\n##### We now create predictors that indicate how a user behaves towards a specific product. We store these predictors in the data table, which is also the final table that we create. Towards this end, we use both prd and users tables. \n##### We start by calculating four new variables:\nup_orders:The total times a user ordered a product\n\nup_first_order: What was the first time a user purchased a product\n\nup_last_order: What was the last time a user purchased a product\n\nup_average_cart_position: The average position in a user's cart of a product","metadata":{}},{"cell_type":"code","source":"# We create the data table starting from the orders_products table \ndata = orders_products\n\ndata = data.groupby(['user_id','product_id']).agg(\n\n up_orders= ('product_id', 'count'),\n up_first_order=('order_number', min),\n up_last_order = ('order_number',max),\n up_average_cart_position = ('add_to_cart_order','mean')\n ,up_avg_hour = ('order_hour_of_day', average_times_of_day),\n up_avg_dow = ('order_dow', average_days)\n).reset_index()\n \ndel orders_products\ndata.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:44:31.841175Z","iopub.execute_input":"2022-06-05T13:44:31.841828Z","iopub.status.idle":"2022-06-05T14:07:25.374634Z","shell.execute_reply.started":"2022-06-05T13:44:31.841737Z","shell.execute_reply":"2022-06-05T14:07:25.373386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Then We compine the data table with the prd and users tables and we calculate the final three variables.\n\nup_order_rate: Percentage of user’s orders that include a specific product\n\nup_orders_since_last_order: Number of orders since user’s last order of a product\n\nup_order_rate_since_first_order: Pecentage of orders since first order of a product in which a user purchased this product","metadata":{}},{"cell_type":"code","source":"\n# up_order_rate = up_orders / user_total_products\ndata = data.merge(users_features[['user_id','user_orders']], on='user_id' , how='left')\ndata['up_order_rate'] = data['up_orders']/data['user_orders']\n\n# up_orders_since_last_order = user_last_order - user_last_ordered_that_product\ndata['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n\n# From the moment the user know about the product, how frequent he then bought it in his next orders?\n# up_order_rate_since_first_order = up_orders / (user_orders - up_first_order + 1)\n# The + 1 is added since order_numbering starts from 1 not 0\ndata['up_order_rate_since_first_order'] = data['up_orders']/(data['user_orders'] - data['up_first_order'] + 1)\ndel data['user_orders']\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:07:25.37726Z","iopub.execute_input":"2022-06-05T14:07:25.37771Z","iopub.status.idle":"2022-06-05T14:07:28.292342Z","shell.execute_reply.started":"2022-06-05T14:07:25.377665Z","shell.execute_reply":"2022-06-05T14:07:28.291313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It seems that user_id = 1 is always ordering product_id = 196 in all his orders","metadata":{}},{"cell_type":"markdown","source":"#### Merging product, user, product-user features","metadata":{}},{"cell_type":"code","source":"# Merging user and product features with the final features dataframe\ndata = data.merge(users_features, on='user_id', how='left').merge(prod_features, on='product_id', how='left')\ndel users_features, prod_features\n\norder_products_future = order_products_train.merge(orders, on='order_id', how='left')\norder_products_future = order_products_future[['user_id', 'product_id', 'reordered']]\ndata = data.merge(order_products_future, on=['user_id', 'product_id'], how='left')\n\n# Set 0 to Product who didn't exists in the future order so model can predict them as Not in future order.\ndata['reordered'].fillna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:07:28.294242Z","iopub.execute_input":"2022-06-05T14:07:28.294878Z","iopub.status.idle":"2022-06-05T14:07:55.16112Z","shell.execute_reply.started":"2022-06-05T14:07:28.294833Z","shell.execute_reply":"2022-06-05T14:07:55.160135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Time-dependent Predictors\n#### 2.4.1 User-dependent\n- On avg the hour the user buys this product at - The future order hour.\n- On avg which day of week the user buys this product at - The future order's day of week.\n#### 2.4.2 product-dependent\n- On avg the hour the product is most bought at - The future order hour.\n- On avg which day of week the product is most bought at - The future order's day of week\n- Frequency of buying this product at the future's order hour of day. [Not yet implemented]\n- Frequency of buying this product on the future's order day of week. [Not yet implemented]","metadata":{}},{"cell_type":"code","source":"'''\nCalculates the difference between 2 values from a looping sequence\ndist(X, Y) = min { X-Y, N-(X-Y) }\n'''\ndef diff_bet_time(arr1, arr2, max_value=23):\n    arr1 = pd.to_datetime(arr1, format='%H')\n    arr2 = pd.to_datetime(arr2, format='%H:%M:%S')\n    arr_diff = np.abs(arr1.dt.hour-arr2.dt.hour)\n    return np.minimum(arr_diff, max_value- (arr_diff-1))\n\n'''\nCalculates the difference between 2 values from a looping sequence\ndist(X, Y) = min { X-Y, N-(X-Y) }\n'''\ndef diff_bet_dow(arr1, arr2, max_value=6):\n    arr_diff = np.abs(arr1-arr2)\n    return np.minimum(arr_diff, max_value- (arr_diff-1))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:07:55.164046Z","iopub.execute_input":"2022-06-05T14:07:55.164402Z","iopub.status.idle":"2022-06-05T14:07:55.172524Z","shell.execute_reply.started":"2022-06-05T14:07:55.164357Z","shell.execute_reply":"2022-06-05T14:07:55.171377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['up_hour_diff'] = diff_bet_time(data['future_order_hour_of_day'], data['up_avg_hour'])\ndata['up_dow_diff'] = diff_bet_dow(data['future_order_dow'], data['up_avg_dow'])\n\ndata['prod_hour_diff'] = diff_bet_time(data['future_order_hour_of_day'], data['prod_avg_hour'], )\ndata['prod_dow_diff'] = diff_bet_dow(data['prod_avg_dow'], data['future_order_dow'])\n\ndel data['prod_avg_dow'], data['prod_avg_hour'], data['future_order_hour_of_day'], data['up_avg_hour'], data['future_order_dow'], data['up_avg_dow']\n# del data['future_order_hour_of_day'], data['future_order_dow']","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:10.243425Z","iopub.execute_input":"2022-06-05T14:11:10.243703Z","iopub.status.idle":"2022-06-05T14:11:10.248226Z","shell.execute_reply.started":"2022-06-05T14:11:10.243674Z","shell.execute_reply":"2022-06-05T14:11:10.247275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Saving features in a csv file\n# data.to_csv('./features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.039111Z","iopub.status.idle":"2022-06-05T13:38:35.03984Z","shell.execute_reply.started":"2022-06-05T13:38:35.039526Z","shell.execute_reply":"2022-06-05T13:38:35.039556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the X, y","metadata":{}},{"cell_type":"code","source":"# To saveup memory, delete any dataframe we won't use next\ndel order_products_prior, order_products_train, products, orders, departments, aisles","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:25.909488Z","iopub.execute_input":"2022-06-05T14:11:25.912Z","iopub.status.idle":"2022-06-05T14:11:25.926163Z","shell.execute_reply.started":"2022-06-05T14:11:25.911951Z","shell.execute_reply":"2022-06-05T14:11:25.924922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting data to train, valid and test\n- Note that we don't have y_test, since in the data given we don't know what products will be in the test future orders.\n- To have an direction on how our model performs, will create a validation set.\n- Train on train set, observe performance and tune parameters on valid set.\n","metadata":{}},{"cell_type":"code","source":"# Splitting data to train and test\nX_train = data[data['eval_set'] == 'train']\ny_train = X_train['reordered']\nX_test = data[data['eval_set'] == 'test']\ndel data","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:21.55157Z","iopub.execute_input":"2022-06-05T14:11:21.551877Z","iopub.status.idle":"2022-06-05T14:11:25.902834Z","shell.execute_reply.started":"2022-06-05T14:11:21.551844Z","shell.execute_reply":"2022-06-05T14:11:25.901806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint('Class distribution before splitting')\npos_count = np.sum(X_train['reordered']==1)\nneg_count = np.sum(X_train['reordered']==0)\nprint('positive ratio: ', pos_count)\nprint('negative count: ', neg_count)\nprint('positive ratio: ', pos_count/(pos_count+neg_count))\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\nprint('Class distribution of Train set')\ntrain_pos_count = np.sum(X_train['reordered']==1)\ntrain_neg_count = np.sum(X_train['reordered']==0)\nprint('positive count: ', train_pos_count)\nprint('negative count: ', train_neg_count)\nprint('positive ratio: ', train_pos_count/(train_pos_count+train_neg_count))\n\nprint('Class distribution of Validation set')\nval_pos_count = np.sum(X_val['reordered']==1)\nval_neg_count = np.sum(X_val['reordered']==0)\nprint('positive count: ', val_pos_count)\nprint('negative count: ', val_neg_count)\nprint('positive ratio: ', val_pos_count/(val_pos_count+val_neg_count))\n\n# Removing eval_set and the target variable from features\nX_train_non_pred_vars = X_train[['product_id', 'order_id', 'user_id']]\nX_train.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\nX_val_non_pred_vars = X_val[['product_id', 'order_id', 'user_id']]\nX_val.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\nX_test_non_pred_vars = X_test[['product_id', 'order_id', 'user_id']]\nX_test.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\n# Drop features I suspect redundant or of no significant importance as the feature importance graph says\nX_train.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\nX_test.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\nX_val.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\n\n# Dropping the time dependent features 'up_dow_diff','prod_dow_diff','up_hour_diff','prod_hour_diff'\n# We had a strong intuition that these time features will be significant in the prediction,\n# However, they were the least feature's importance used by the model.\n# This is why We commented them across the code.","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:27.44096Z","iopub.execute_input":"2022-06-05T14:11:27.44125Z","iopub.status.idle":"2022-06-05T14:11:35.092468Z","shell.execute_reply.started":"2022-06-05T14:11:27.441221Z","shell.execute_reply":"2022-06-05T14:11:35.091465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:35.094694Z","iopub.execute_input":"2022-06-05T14:11:35.095334Z","iopub.status.idle":"2022-06-05T14:11:35.10328Z","shell.execute_reply.started":"2022-06-05T14:11:35.095275Z","shell.execute_reply":"2022-06-05T14:11:35.102285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:37.014484Z","iopub.execute_input":"2022-06-05T14:11:37.014957Z","iopub.status.idle":"2022-06-05T14:11:37.031313Z","shell.execute_reply.started":"2022-06-05T14:11:37.014906Z","shell.execute_reply":"2022-06-05T14:11:37.030245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"markdown","source":"Notes from xgb documentation\n> * learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n> * max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n> * subsample: percentage of samples used per tree. Low value can lead to underfitting.\n> * colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n> * n_estimators: number of trees you want to build.\n> * objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\nXGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n> * gamma: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n> * alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n> * lambda: L2 regularization on leaf weights and is smoother than L1 regularization.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n# Standarization didn't affect the score much\nscaler = preprocessing.StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\nX_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\nprint(np.shape(X_train))\nprint(np.shape(X_test))\nprint(np.shape(X_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:42.998623Z","iopub.execute_input":"2022-06-05T14:11:42.9993Z","iopub.status.idle":"2022-06-05T14:11:50.406307Z","shell.execute_reply.started":"2022-06-05T14:11:42.999264Z","shell.execute_reply":"2022-06-05T14:11:50.40519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = y_train.values.reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.055203Z","iopub.status.idle":"2022-06-05T13:38:35.055966Z","shell.execute_reply.started":"2022-06-05T13:38:35.055613Z","shell.execute_reply":"2022-06-05T13:38:35.055646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n\n# class myCallback(tf.keras.callbacks.Callback):\n#         # Define the correct function signature for on_epoch_end\n#         def on_epoch_end(self, epoch, logs={}):\n#             if logs.get('recall') is not None and logs.get('recall') > 0.8: \n#                 print(\"\\nReached 80% recall so cancelling training!\") \n                \n#                 # Stop training once the above condition is met\n#                 self.model.stop_training = True\n\n# callbacks = myCallback()\n# # Build the classification model\n# model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n#                                     tf.keras.layers.Dense(128, activation=tf.nn.relu),\n#                                     tf.keras.layers.Dense(128, activation=tf.nn.relu),\n#                                     tf.keras.layers.Dense(1, activation=tf.nn.softmax)])\n# model.compile(optimizer='adam', \n#                   loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n#                   metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n\n# history = model.fit(x=X_train, y=y_train, epochs=10, callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.057436Z","iopub.status.idle":"2022-06-05T13:38:35.058269Z","shell.execute_reply.started":"2022-06-05T13:38:35.057937Z","shell.execute_reply":"2022-06-05T13:38:35.057967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.059724Z","iopub.status.idle":"2022-06-05T13:38:35.060632Z","shell.execute_reply.started":"2022-06-05T13:38:35.060279Z","shell.execute_reply":"2022-06-05T13:38:35.060326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import metrics\n\n# Training the model with features except the product_id, user_id, order_id columns\nclf = xgb.XGBClassifier(objective='binary:logistic', colsample_bytree = 0.4, learning_rate = 0.1,\n                max_depth = 5, reg_lambda = 5.0, n_estimators = 100)\nclf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:11:50.408846Z","iopub.execute_input":"2022-06-05T14:11:50.40915Z","iopub.status.idle":"2022-06-05T14:27:29.207669Z","shell.execute_reply.started":"2022-06-05T14:11:50.409102Z","shell.execute_reply":"2022-06-05T14:27:29.20665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectFromModel\n# Visualizing the Feature importance \nprint(clf.feature_importances_)\n\nxgb.plot_importance(clf)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:29.210142Z","iopub.execute_input":"2022-06-05T14:27:29.21094Z","iopub.status.idle":"2022-06-05T14:27:29.694674Z","shell.execute_reply.started":"2022-06-05T14:27:29.210892Z","shell.execute_reply":"2022-06-05T14:27:29.693696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing PR-Curve ","metadata":{}},{"cell_type":"code","source":"# keep probabilities for the positive outcome only\ny_test_prob = clf.predict_proba(X_test)[:, 1]\ny_val_prob = clf.predict_proba(X_val)[:, 1]\ny_train_prob = clf.predict_proba(X_train)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:29.697642Z","iopub.execute_input":"2022-06-05T14:27:29.698331Z","iopub.status.idle":"2022-06-05T14:27:56.506359Z","shell.execute_reply.started":"2022-06-05T14:27:29.69827Z","shell.execute_reply":"2022-06-05T14:27:56.505233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis function maximizes a metric, while keeping another metric above a given threshold.\n'''\ndef maximize_metric_keep_metric(metric1_list, metric2_list, metric2_thresh=0.3):\n    for idx in range(len(metric1_list)):\n        if(metric2_list[idx] > metric2_thresh):\n            return idx\n    return -1","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:56.508077Z","iopub.execute_input":"2022-06-05T14:27:56.508363Z","iopub.status.idle":"2022-06-05T14:27:56.514382Z","shell.execute_reply.started":"2022-06-05T14:27:56.508322Z","shell.execute_reply":"2022-06-05T14:27:56.513456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# Choosing Threshold that maximizes the f1_score\nprecision, recall, thresholds = precision_recall_curve(y_val, y_val_prob)\nf1_scores = 2*recall*precision/(recall+precision)\nopt_indx = np.argmax(f1_scores)\nprint(\"Maximuim f1_score for the positive class: \", f1_scores[opt_indx])\nprint(\"Correspoding precision: \", precision[opt_indx])\nprint(\"Correspoding recall: \", recall[opt_indx])\nprint(\"Correspoding Threshold: \", thresholds[opt_indx])\nbest_thresh = thresholds[opt_indx]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:56.515992Z","iopub.execute_input":"2022-06-05T14:27:56.516501Z","iopub.status.idle":"2022-06-05T14:27:57.305555Z","shell.execute_reply.started":"2022-06-05T14:27:56.516458Z","shell.execute_reply":"2022-06-05T14:27:57.304438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choosing Threshold that maximizes recall, while keeping precision above 0.3\nopt_indx = maximize_metric_keep_metric(metric1_list=recall, metric2_list=precision, metric2_thresh=0.3)\nprint(\"Max recall for the positive class: \", recall[opt_indx])\nprint(\"Correspoding precision: \", precision[opt_indx])\nprint(\"Correspoding f1_score: \", f1_scores[opt_indx])\nprint(\"Correspoding Threshold: \", thresholds[opt_indx])\nbest_thresh = thresholds[opt_indx]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:57.307509Z","iopub.execute_input":"2022-06-05T14:27:57.307864Z","iopub.status.idle":"2022-06-05T14:27:57.949498Z","shell.execute_reply.started":"2022-06-05T14:27:57.307803Z","shell.execute_reply":"2022-06-05T14:27:57.948472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the precision-recall curves\nno_skill = len(y_val[y_val==1]) / len(y_val)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(recall, precision, marker='.', label='Logistic')\nplt.plot(recall[opt_indx], precision[opt_indx], marker='o', color='k', label='optimum threshold')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:27:57.95156Z","iopub.execute_input":"2022-06-05T14:27:57.95208Z","iopub.status.idle":"2022-06-05T14:28:01.88181Z","shell.execute_reply.started":"2022-06-05T14:27:57.952033Z","shell.execute_reply":"2022-06-05T14:28:01.880907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dynamic Thresholding","metadata":{}},{"cell_type":"code","source":"def recommended_basket(user_prods):\n    user_basket_size = int(np.floor(user_prods['user_average_basket'].iloc[0]))\n    user_prods = user_prods.sort_values(by=['reordered_prop'], ascending=False)\n    user_prods.iloc[:user_basket_size]['reordered'] = 1\n    user_prods.iloc[user_basket_size:]['reordered'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:09:22.37654Z","iopub.status.idle":"2022-06-05T14:09:22.377358Z","shell.execute_reply.started":"2022-06-05T14:09:22.377027Z","shell.execute_reply":"2022-06-05T14:09:22.377078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Changing probabilities to crisp predicted values, useing the threshold obtained from the ROC-curve\ny_test_preds = y_test_prob>best_thresh\ny_val_preds = y_val_prob>best_thresh\ny_train_preds = y_train_prob>best_thresh","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:28:01.886903Z","iopub.execute_input":"2022-06-05T14:28:01.887212Z","iopub.status.idle":"2022-06-05T14:28:01.902803Z","shell.execute_reply.started":"2022-06-05T14:28:01.887171Z","shell.execute_reply":"2022-06-05T14:28:01.901598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint('-----------------CLASSIFICATION REPORT--------------------')\nprint(\"Train positive class count: \", y_train.sum())\nprint(\"Train negative class count: \", y_train.shape[0] - y_train.sum())\nprint(\"Train Set tn, fp, fn, tp:\",confusion_matrix(y_train, y_train_preds).ravel())\nprint(\"Train Set report:\",classification_report(y_train, y_train_preds))\n\nprint(\"Validation positive class count: \", y_val.sum())\nprint(\"Validation negative class count: \", y_val.shape[0] - y_val.sum())\nprint(\"Validation Set tn, fp, fn, tp:\",confusion_matrix(y_val, y_val_preds).ravel())\nprint(\"Validation Set report:\",classification_report(y_val, y_val_preds))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:28:01.906603Z","iopub.execute_input":"2022-06-05T14:28:01.907338Z","iopub.status.idle":"2022-06-05T14:28:58.451521Z","shell.execute_reply.started":"2022-06-05T14:28:01.907305Z","shell.execute_reply":"2022-06-05T14:28:58.45048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> * By forming the X, and y in this way, data is sparse. It's very skewed to the negative class. Thus accuracy is not a good measure for how the model is performing.\n> * First, we've found that there's alot of false negatives, so we considered to decrease it, or increase the recall of the positive class.\n> * This means, we want to reduce the number of products the model say user won't predict in the future while he/she will actually does. On the other side, it's okay to allow some false positives, when the model recommends a products the user won't buy in the next order.\n> * Our aim is to change the threshold to maximize the recall, while keeping the precision above a certain threshold. \n> * Since skewed class distribution we will consider PR-curve not ROC-curve.","metadata":{}},{"cell_type":"markdown","source":"#### Preparing the sumission file","metadata":{}},{"cell_type":"code","source":"import csv\n\n# Append prediction to test_order details\ntest_orders = X_test_non_pred_vars[['user_id','order_id','product_id']]\ntest_orders['reordered'] = y_test_preds\n\n# Extracting orders who have no predicted products\nempty_orders = test_orders.groupby(['order_id']).agg(\n    count_reorders = ('reordered', 'sum')\n).reset_index()\nempty_orders = empty_orders[empty_orders['count_reorders'] == 0]\n\n# For orders who have predicted products \n# Extract the products predicted to be in the future order\ntest_orders = test_orders[test_orders['reordered'] == 1]\n# For each order group its predicted products together into a list \ntest_orders = test_orders.groupby('order_id')['product_id'].apply(list).reset_index(name='products')\n\ntest_orders.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.08518Z","iopub.status.idle":"2022-06-05T13:38:35.085943Z","shell.execute_reply.started":"2022-06-05T13:38:35.085611Z","shell.execute_reply":"2022-06-05T13:38:35.085644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# csv header\nheaderNames = ['order_id', 'products', 'user_id']\nrows = []\n\nfor index, row in test_orders.iterrows():\n    products = ' '.join(str(product_id) for product_id in row['products']) \n    rows.append( \n        {'order_id': str(row['order_id']),\n         'products': products})\n\nfor index, row in empty_orders.iterrows():\n    rows.append( \n        {'order_id': str(row['order_id']),\n         'products': 'None'})\n    \nwith open('./submissions.csv', 'w', encoding='UTF-8', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=headerNames)\n    writer.writeheader()\n    writer.writerows(rows)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:38:35.087508Z","iopub.status.idle":"2022-06-05T13:38:35.088351Z","shell.execute_reply.started":"2022-06-05T13:38:35.088016Z","shell.execute_reply":"2022-06-05T13:38:35.088046Z"},"trusted":true},"execution_count":null,"outputs":[]}]}