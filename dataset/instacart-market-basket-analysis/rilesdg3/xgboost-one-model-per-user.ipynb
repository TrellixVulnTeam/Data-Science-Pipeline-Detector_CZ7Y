{"nbformat_minor":1,"cells":[{"cell_type":"markdown","execution_count":null,"source":"The model uses all historical data for the user.\nThe best lb score was ~.26 using only the 'days_since_last_ordered'\nI think by using xgboosts \"update\" function the model could be updated every time a user places a new order","outputs":[],"metadata":{"_cell_guid":"4fc2b61c-1b5b-4615-9c7c-105a50a113b3","_execution_state":"idle","_uuid":"b39d52d075c3d768868d4024ca58ea2af0f70a29"}},{"cell_type":"code","outputs":[],"source":"\n__docformat__ = 'restructedtext en'\n\nimport timeit\nimport time\n\nimport os\nimport threading\n\n\nimport pandas as pd\nimport numpy as np\n\nimport scipy as sci\nimport sklearn as sk\nfrom sklearn import metrics\nimport statsmodels.sandbox.stats.runs as statruns\nimport scipy.stats as sci\nfrom sklearn import preprocessing\n\nimport xgboost as xgb\n\n\n''' inportant link bout the data https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b '''\n\n\naisle = pd.read_csv('../input/aisles.csv')\ndepartments = pd.read_csv('../input/departments.csv')\norder_products_prior = pd.read_csv('../input/order_products__prior.csv')#Order history of customer\norder_products_train = pd.read_csv('../input/order_products__train.csv')\norders = pd.read_csv('../input/orders.csv')#tells to which set (prior, train, test) an order belongs\nproducts = pd.read_csv('../input/products.csv')\n\n'''Find missing values '''\naisle.isnull().values.any()\ndepartments.isnull().values.any()\norder_products_prior.isnull().values.any()\norder_products_train.isnull().values.any()\norders.isnull().values.any()\nproducts.isnull().values.any()\ntest=orders[orders.eval_set=='test']\n'''Seems like the only NaN is from days_since_prior_order and that is because they are new customers\nand this is their first order'''\norders[orders.days_since_prior_order.isnull()]\n\n''' # of unique clients '''\norders.user_id.nunique()\n\n'''merge order_product_prior and orders so information is in one data frame '''\nmerged_prior=pd.merge(left=order_products_prior,right=orders, on='order_id')\nmerged_train=pd.merge(left=order_products_train,right=orders, on='order_id')\n\n'''order df so orders are in crohnological order, order_number represents the number\nof orders the custerm has placed '''\nmerged_prior.sort_values(by='order_number', axis=0, inplace=True)\nmerged_train.sort_values(by='order_number', axis=0, inplace=True)\n\nmergedall=pd.concat([merged_prior,merged_train,test])\nmerged_prior=mergedall","execution_count":null,"metadata":{"_cell_guid":"ebf1d91d-ee62-43bb-b357-eab1d33c899c","_execution_state":"idle","collapsed":true,"_uuid":"17c33c45d22d0d9e08ff0e04de40e9716576224b","trusted":false}},{"cell_type":"markdown","execution_count":null,"source":"The \"merged_prior=mergedall`\" is like this so I did not have to go back and change it in every spot in the script\n","outputs":[],"metadata":{"_cell_guid":"1273e814-4f89-4360-8fde-b5afebdc9111","_execution_state":"idle","_uuid":"a1853f81a2782d7f1cddbf8a20542328ff795c8f"}},{"cell_type":"code","outputs":[],"source":"def load_data_XG(FnlBinDailyData):\n    \"\"\" Loads the dataset\n\n    :type dataset: Pandas Dataframe\n    :param dataset: A dataset with the variables and the labels\n    \"\"\"\n    \n    Vars=FnlBinDailyData.columns[4:FnlBinDailyData.columns.size-1]#have -1 becauase order_id is last and that is not a feature\n    \n    D = xgb.DMatrix(FnlBinDailyData[Vars],label=FnlBinDailyData['reordered'].values)\n       \n    rval = [D]\n    \n    return D\n","execution_count":null,"metadata":{"_cell_guid":"9b447a80-19a0-4562-a0f8-ccf0fc265f19","_execution_state":"idle","collapsed":true,"_uuid":"0b1a84762dfca35dd368dfa7b05408d96c85037c","trusted":false}},{"cell_type":"code","outputs":[],"source":"def xgBoost(mydata,test,thresh):\n    \"\"\" Loads the dataset\n\n    :type mydata: Pandas Dataframe\n    :param mydata: The training dataset with the variables and the labels\n    \n    :type test: Pandas Dataframe\n    :param test: The varibles to be used for predicting \n    \n    :type thresh: An int\n    :param thresh: The threshhold value at which we say the product is in the order\n    \"\"\"\n    # prepare dict of params for xgboost to run with\n    xgb_params = {\n        #'n_trees': 50, \n        'eta': .1,\n        'max_depth': 6,\n        'subsample': 0.76,\n        'objective': 'reg:logistic',\n        'eval_metric': 'logloss',\n        'lambda': 10,\n        'gamma': .7,\n        'colsample_bytree':.95,\n        'silent': 1,\n        'alpha': 2e-05\n    }\n    \n    #for when it breaks we know what user it was on.... becuase it always breaks the first\n    #couple of times\n    print('xgb user_id ',mydata.user_id[0])\n    \n    # form DMatrices for Xgboost training\n    dtrain = load_data_XG(mydata)#xgb.DMatrix(mydata, mydata.y)\n    \n    #because in some orders there is less than 5 products\n    if(dtrain.get_label().size>15):\n        # xgboost, cross-validation\n        cv_result = xgb.cv(xgb_params, dtrain,num_boost_round=50, # increase to have better results (~700)\n                           nfold=5,early_stopping_rounds=30,\n                           show_stdv=False\n                          )\n           \n        num_boost_rounds = len(cv_result)\n        print(num_boost_rounds)\n        \n        \n        # train model\n        model = xgb.train(dict(xgb_params, silent=0), dtrain,num_boost_round=num_boost_rounds)\n    else:\n        model = xgb.train(dict(xgb_params, silent=0),dtrain)\n    \n    # now fixed, correct calculation\n    pred=model.predict(dtrain)\n    for w in enumerate(pred):\n        if(pred[w[0]]>thresh):\n            pred[w[0]]=1\n        else:\n            pred[w[0]]=0    \n    \n    print('XGB Score ', f1_score(dtrain.get_label(), pred), ' User_id ', mydata.user_id[0])\n    \n    dtest = xgb.DMatrix(test[model.feature_names])\n    y_pred=model.predict(dtest)\n    pred=y_pred\n    \n    for w in enumerate(pred):\n        if(pred[w[0]]>thresh):\n            pred[w[0]]=1\n        else:\n            pred[w[0]]=0   \n    \n    popo=pd.concat([pd.DataFrame(test.product_id.values), pd.DataFrame(pred),pd.DataFrame(test.order_id.values)],axis=1,ignore_index=True)\n    popo.rename(columns={0:'product_id',1:'pred',2:'order_id'},inplace = True)\n    \n    d=dict()\n    for row in popo.itertuples():\n        if row.pred ==1:\n            try:\n                d[row.order_id] += ' ' + str(row.product_id)\n            except:\n                d[row.order_id] = str(row.product_id)\n    if(len(d)==0):\n        d[row.order_id]='None'\n        print('None')\n    return test.order_id.unique(), d","execution_count":null,"metadata":{"_cell_guid":"6ef002c8-c566-4e16-99e1-19c39bb2da21","_execution_state":"idle","collapsed":true,"_uuid":"16cc34f4c41f18fd68a7d19d22f60f97023fa4cf","trusted":false}},{"cell_type":"code","outputs":[],"source":"#Sort by most active users in the test set\ndfindexer=merged_prior.groupby('user_id').product_id.count().sort_values(ascending=False).index#.intersection(test.user_id.unique())\nmyseries=pd.Series(index=range(test.user_id.nunique()))\nindexcount=np.int(0)\nfor a in dfindexer:\n    if(a in test.user_id.unique()):\n        myseries.set_value(indexcount,a,takeable=True)\n        indexcount+=1\ndfindexer=pd.Index(myseries)\n\n#The code above is slow so I saved the results and just reload them\n#myseries.to_csv('dfindexer.csv')\n#dfindexer=pd.read_csv('dfindexer.csv',header=-1)\ndfindexer=pd.Index(dfindexer[0])\norder_id_splits=pd.DataFrame(index=range(np.int((dfindexer.size-1)/10)+1), \n                             columns=['g0','g1','g2','g3','g4',\n                                      'g5','g6','g7','g8','g9'])","execution_count":null,"metadata":{"_cell_guid":"6f81f3fe-451b-4a60-8842-fcea3edbc6d7","_uuid":"521d83618785350a6a6e350395fa48a920b1410d","collapsed":true,"trusted":false}},{"cell_type":"code","outputs":[],"source":"count=np.int(0)\ncount1=np.int(0)\nwhile count<np.int((dfindexer.size-1)/10):    \n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g0'),dfindexer[count1],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g1'),dfindexer[count1+1],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g2'),dfindexer[count1+2],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g3'),dfindexer[count1+3],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g4'),dfindexer[count1+4],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g5'),dfindexer[count1+5],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g6'),dfindexer[count1+6],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g7'),dfindexer[count1+7],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g8'),dfindexer[count1+8],takeable=True)\n    order_id_splits.set_value(count,order_id_splits.columns.get_loc('g9'),dfindexer[count1+9],takeable=True)\n    count1=count1+10\n    count=count+1\n \norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g0'),dfindexer[dfindexer.size-10],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g1'),dfindexer[dfindexer.size-9],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g2'),dfindexer[dfindexer.size-8],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g3'),dfindexer[dfindexer.size-7],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g4'),dfindexer[dfindexer.size-6],takeable=True) \norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g5'),dfindexer[dfindexer.size-5],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g6'),dfindexer[dfindexer.size-4],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g7'),dfindexer[dfindexer.size-3],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g8'),dfindexer[dfindexer.size-2],takeable=True)\norder_id_splits.set_value(count,order_id_splits.columns.get_loc('g9'),dfindexer[dfindexer.size-1],takeable=True)\n\ngrouped = merged_prior.groupby(['user_id','order_id']).product_id.count()\naver_order_size=pd.DataFrame(index=range(merged_prior.user_id.nunique()), columns=['user_id','avg_num_prods_in_cart'])\nidcount=np.int(0)\n","execution_count":null,"metadata":{"_cell_guid":"7b7acfaa-40a7-46ce-af9f-e334ba9e857b","_execution_state":"idle","collapsed":true,"_uuid":"69ce80cbb0585874990545e1d72a32cc20fd1583","trusted":false}},{"cell_type":"code","outputs":[],"source":"#Get the size of the dataframe fro the user_id I am modeling\ndef getlen(ids):\n    count=0\n    idcount=0\n    for userid in ids:\n        d=merged_prior[merged_prior.user_id==userid]\n        print('count', count,' idcount ', idcount)\n        for e in d.order_number.unique():#iterate over order numbers\n            if(e==1):\n                daset=d[d.order_number==e].apply(set)\n                count=count+len(daset.product_id)\n            else:\n                daset1=d[d.order_number==e].apply(set)\n                count=count+len(daset.product_id)+len(daset.product_id.symmetric_difference(daset1.product_id))\n                daset.product_id.update(daset1.product_id)\n        \n        idcount=idcount+1\n    return count\n\n","execution_count":null,"metadata":{"_cell_guid":"1d3bcba5-decf-4dc9-8f88-e84978ebd803","_uuid":"d0d1b8728356883e9d390534bc659b15168243ec","collapsed":true,"trusted":false}},{"cell_type":"markdown","execution_count":null,"source":"fastest way to add data to a dataframe\ndf.set_value()","outputs":[],"metadata":{"_cell_guid":"92cab515-f5e6-418c-80b5-38ec8c635f6c","_uuid":"ade35c5bff00424c6902aede192073d5d1c6533e","collapsed":true}},{"cell_type":"code","outputs":[],"source":"#Builds the dataframe with all the order history for the user_id we are trying to predict\nmerged_prior.sort_values(by=['user_id','order_number'], inplace =True)\ndef buildDf(ids):\n    df=pd.DataFrame(index=range(getlen(ids)),columns=['reordered','product_in_order','user_id','order_number','product_id',\n                                                      'days_since_last_ordered',\n                                                      'order_id'])#\n    idcount=0\n    dfit=np.int(0)\n    we=dict()\n    order_id_col=0\n    days_since_prior_order_col=1\n    reorded_col=2\n    num_time_buy_product_col=3\n    num_orders_since_last_purchase_col=4\n    for userid in ids:\n        t0=time.time()\n        #could set index to user_id and sort the below??\n        b=merged_prior[merged_prior.user_id==userid]\n        d=b.order_number.unique()\n        print('idcount ', idcount)\n        for e in d:#iterate over order numbers\n            for c in b[b.order_number==e].itertuples():#iterate over products\n                '''has product ever been purchased before\n                if yes, then we are reordering the product'''\n                if(c.product_id in we):                    \n                    '''I do not add becuase for predictin I do not want to say it is in the order becuase I do not really know '''\n                    num_times_buy_product=we[c.product_id][len(we[c.product_id])-1][num_time_buy_product_col]\n                    '''I know this is a new order and I want to take that into considaration '''\n                    num_orders_since_last_purchase=we[c.product_id][len(we[c.product_id])-1][num_orders_since_last_purchase_col]+1\n                    '''I know the days since prior order so I  want to add that in '''\n                    this_days_since_prior_order=we[c.product_id][len(we[c.product_id])-1][days_since_prior_order_col]+c.days_since_prior_order\n                    we[c.product_id].append([e,this_days_since_prior_order,c.reordered,num_times_buy_product,1])\n                else:\n                    if(c.product_id>0):\n                        we[c.product_id]=[[e,0,c.reordered,1,0]]#first time buying product\n            \n            for key in we:\n                #are we reordering a product in this order, yes == 1, no == 0\n                if(key in b[b.order_number==e].product_id.values):#'''product is in the order'''\n                    if(we[key][len(we[key])-1][reorded_col]>0):#c.days_since_prior_order>0):\n                        df.set_value(dfit,df.columns.get_loc('user_id'),c.user_id,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('order_number'),e,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_id'),key,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_in_order'),1,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('reordered'),we[key][len(we[key])-1][reorded_col],takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('days_since_last_ordered'),c.days_since_prior_order,takeable=True)\n                        f.set_value(dfit,df.columns.get_loc('order_id'),c.order_id,takeable=True)\n                        dfit=dfit+1\n                        \n                        '''Update we to reflect that the product was in this order '''\n                        we[key][len(we[key])-1]=[e,c.days_since_prior_order,we[key][len(we[key])-1][reorded_col],we[key][len(we[key])-1][num_time_buy_product_col]+1,1]\n                        \n                else:#'''product not in order '''                    \n                    if(c.days_since_prior_order>0):#we[key][len(we[key])-1][days_since_prior_order_col]>0):                   \n                        #dslo=last[last.product_id==key].days_since_last_ordered.values[0]+c.days_since_prior_order\n                        dic=we[key][len(we[key])-1][days_since_prior_order_col]+c.days_since_prior_order\n                        num_times_buy_product=we[key][len(we[key])-1][num_time_buy_product_col]\n                        num_orders_since_last_purchase=we[key][len(we[key])-1][num_orders_since_last_purchase_col]+1\n                        we[key].append([e,dic,0,num_times_buy_product,num_orders_since_last_purchase])\n                        df.set_value(dfit,df.columns.get_loc('user_id'),c.user_id,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('order_number'),e,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_id'),key,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_in_order'),0,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('reordered'),0,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('days_since_last_ordered'),dic,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('order_id'),c.order_id,takeable=True)\n                        dfit=dfit+1\n                    else:#First order by user\n                        df.set_value(dfit,df.columns.get_loc('user_id'),c.user_id,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('order_number'),e,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_id'),key,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('product_in_order'),0,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('reordered'),0,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('days_since_last_ordered'),0,takeable=True)\n                        df.set_value(dfit,df.columns.get_loc('order_id'),c.order_id,takeable=True)\n                        dfit=dfit+1\n        \n        we.clear()\n        idcount=idcount+1\n        t1=time.time()\n        print('len time ', t1-t0)\n    return df\n","execution_count":null,"metadata":{"_cell_guid":"27fbca24-9d95-4c63-ba10-0e7a3f646488","_execution_state":"idle","collapsed":true,"_uuid":"e2ddf13873368204a28eddb275c20c7de83f091c","trusted":false}},{"cell_type":"code","outputs":[],"source":"#Puts it all together\ndef xg(splits,filename,thresh):\n    \n    daindex=pd.Index(merged_prior[merged_prior.eval_set=='test'].order_id.unique(),name='order_id')\n    preddf=pd.DataFrame(index=daindex,columns=['products'])\n    for split in enumerate(splits):\n        mydf=buildDf([split[1]])\n        mydf.dropna(inplace=True)\n        mydf=mydf.astype(np.int64)\n        for a in mydf.user_id.unique():\n            fg=mydf[mydf.user_id==a]\n            pred_order_id,predvalues=xgBoost(fg[fg.order_number<fg.order_number.max()], fg[fg.order_number==fg.order_number.max()],thresh)\n            preddf.set_value(daindex.get_loc(pred_order_id[0]),preddf.columns.get_loc('products') , predvalues.get(pred_order_id[0]), takeable=True)\n        print('number of models built ', split[0])\n    preddf.dropna(inplace=True) \n    preddf.to_csv(filename)","execution_count":null,"metadata":{"_cell_guid":"179ce1dd-e981-4ee0-be7e-f14fd8ec6f29","_uuid":"b506f037aa91694566e8b1ebb5050aa4f956feaf","collapsed":true,"trusted":false}},{"cell_type":"code","outputs":[],"source":"#Calls the function that puts it all together.\n#I have divided the dataset into 10 groups. I run two seperate instances of the script\n#each instance contains 5 sets of user_id, When everything is done each instance saves\n#the predicult results for its 5 groups of user_ids and then I merge the two .csv's by\n#hand and then submit\n\njobNumber=1\n\nfilenames=['set0.csv','set1.csv','set2.csv','set3.csv','set4.csv','set5.csv','set6.csv','set7.csv',\n           'set8.csv','set9.csv','final0.csv','final1.csv']\n\nif(jobNumber==1):\n    mythread0=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g0,'filename':filenames[0],'thresh':.22})\n    mythread0.start()\n     mythread1=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g1,'filename':filenames[1],'thresh':.22})\n     mythread1.start()\n     mythread2=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g2,'filename':filenames[2],'thresh':.22})\n     mythread2.start()\n     mythread3=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g3,'filename':filenames[3],'thresh':.22})\n     mythread3.start()\n     mythread4=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g4,'filename':filenames[4],'thresh':.22})\n     mythread4.start()\n     \n     mythread0.join()\n     mythread1.join()\n     mythread2.join()\n     mythread3.join()\n     mythread4.join()\n\n    csv0=pd.read_csv(filenames[0])\n    csv1=pd.read_csv(filenames[1])\n    csv2=pd.read_csv(filenames[2])\n    csv3=pd.read_csv(filenames[3])\n    csv4=pd.read_csv(filenames[4])\n    pd.concat([csv0,csv1,csv2,csv3,csv4]).to_csv(filenames[10])\nelse:\n    mythread0=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g5,'filename':filenames[5],'thresh':.22})\n    mythread0.start()\n    mythread1=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g6,'filename':filenames[6],'thresh':.22})\n    mythread1.start()\n    mythread2=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g7,'filename':filenames[7],'thresh':.22})\n    mythread2.start()\n    mythread3=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g8,'filename':filenames[8],'thresh':.22})\n    mythread3.start()\n    mythread4=threading.Thread(target=xg,kwargs={'splits':order_id_splits.g9,'filename':filenames[9],'thresh':.22})\n    mythread4.start()\n    \n    mythread0.join()\n    mythread1.join()\n    mythread2.join()\n    mythread3.join()\n    mythread4.join()\n\n    csv0=pd.read_csv(filenames[5])\n    csv1=pd.read_csv(filenames[6])\n    csv2=pd.read_csv(filenames[7])\n    csv3=pd.read_csv(filenames[8])\n    csv4=pd.read_csv(filenames[9])\n    pd.concat([csv0,csv1,csv2,csv3,csv4]).to_csv(filenames[11])\n\n","execution_count":null,"metadata":{"_cell_guid":"a8eeca70-2015-4d25-9adf-95fd81d3ed91","_uuid":"1a5dcfc7e349b27e9e6bc6e963ef5cf94fbe6be4","collapsed":true,"trusted":false}},{"cell_type":"code","outputs":[],"source":"","execution_count":null,"metadata":{"_cell_guid":"547c4b06-8eb3-47d2-b92e-a578b3fe8a95","_uuid":"46ba19f6199a5a7a3bc3e73c3b1749d3dd48526f","collapsed":true,"trusted":false}}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","nbconvert_exporter":"python","name":"python","file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}