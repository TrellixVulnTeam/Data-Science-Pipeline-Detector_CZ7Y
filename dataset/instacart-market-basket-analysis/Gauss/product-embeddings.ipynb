{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Product Emebddings for Recommendation Systems\n\n-----\n\n## Instacart Grocery Dataset\n\nSource: https://www.kaggle.com/c/instacart-market-basket-analysis\n\n- Instacart is an online grocery delivery service\n- They have made available 3M grocery orders for over 200K users\n- They provide between 4 to 100 orders for each user and each order contains the sequence of products purchased\n- We also have a brief description of the products\n\nGoals:\n- We will use this data to generate product embeddings - dense continuous representations of discrete tokens\n- We will apply methods from Natural Language Processing to analyze product baskets\n----","metadata":{}},{"cell_type":"markdown","source":"## **1. Import Libraries**","metadata":{}},{"cell_type":"code","source":"## data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom zipfile import ZipFile # read zip files directly\nimport gc # garbage collection\nimport pickle # save python objects\nimport random \n\n# parallel processing\nimport multiprocessing\nfrom joblib import delayed, Parallel\n\n# cool progress bar\nimport tqdm\nimport time\n\n# modeling and evaluation\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### tensorflow related packages and functions\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import Input, Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import make_sampling_table, skipgrams\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import History\nfrom tensorflow.keras import optimizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# global parameters\n#===============================================\n\n# show entire value of cell in pandas\npd.set_option('display.max_colwidth', 1000)\npd.set_option('display.max_columns', 500)\n\n# number of cpus\ncpus = multiprocessing.cpu_count()\nf\"Number of CPUs: {cpus}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test for gpu\nprint(tf.test.is_gpu_available(\n    cuda_only=False, min_cuda_compute_capability=None\n))\nprint(tf.test.gpu_device_name())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## 2. Raw data","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set data directory\ndata_dir =  \"/kaggle/input/instacart-market-basket-analysis/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.1 Product info**","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"# product file\nwith ZipFile(data_dir + \"products.csv.zip\") as z:\n    with z.open(\"products.csv\") as f:\n        products = pd.read_csv(f)\nprint(products.shape)\nproducts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# department file\nwith ZipFile(data_dir + \"departments.csv.zip\") as z:\n    with z.open(\"departments.csv\") as f:\n        dept = pd.read_csv(f)\nprint(dept.shape)\ndept","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aisle file\nwith ZipFile(data_dir + \"aisles.csv.zip\") as z:\n    with z.open(\"aisles.csv\") as f:\n        aisle = pd.read_csv(f)\nprint(aisle.shape)\naisle.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge all files to a single product file\nproducts = pd.merge(products, aisle, on = \"aisle_id\", how = \"left\")\nproducts = pd.merge(products, dept, on = \"department_id\", how = \"left\")\nprint(products.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# departments with the most products\nproducts[\"department\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aisles with the most products\nproducts[\"aisle\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"## 2.2 **Orders data**","metadata":{}},{"cell_type":"code","source":"# orders file\nwith ZipFile(data_dir + \"orders.csv.zip\") as z:\n    with z.open(\"orders.csv\") as f:\n        orders = pd.read_csv(f)\nprint(orders.shape)\norders","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove test data\norders = orders.loc[orders[\"eval_set\"] != \"test\", :]\nprint(orders.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aggregate to user level\nusers = orders.groupby(\"user_id\").agg({\"order_number\": \"max\"})\nusers = users.reset_index()\nprint(users.shape)\nusers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================================================\n# split train-val-test\n# =========================================================\n\n# training set: 60%\n# validation set: 20%\n# test set: 20%\nusers[\"eval\"] = np.random.choice([\"train\", \"test\", \"val\"], size = users.shape[0], p = [0.6, 0.2, 0.2])\nusers[\"eval\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================================================\n# merge with orders data\n# =========================================================\norders = pd.merge(orders, users, on = [\"user_id\", \"order_number\"], how = \"left\")\nprint(orders.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set missing eval to prior and delete eval_set column\norders.loc[pd.isnull(orders[\"eval\"]), \"eval\"] = \"prior\"\norders.drop(labels = \"eval_set\", axis = 1, inplace = True)\nprint(orders[\"eval\"].value_counts())\norders.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 **Prior orders**","metadata":{}},{"cell_type":"code","source":"# orders file\nwith ZipFile(data_dir + \"order_products__prior.csv.zip\") as z:\n    with z.open(\"order_products__prior.csv\") as f:\n        prior = pd.read_csv(f)\nprint(prior.shape)\nprior","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# product frequency to check top-products\nprod_freq = prior[\"product_id\"].value_counts()\nprint(prod_freq.shape)\nprod_freq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep products bought at least 200 times\nmin_freq = 200\nprod_freq = prod_freq[prod_freq >= min_freq]\nprint(prod_freq.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# subset prior data\nprior = prior.loc[prior[\"product_id\"].isin(list(prod_freq.index)), :]\nprint(prior.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregate to order-level from order-product level","metadata":{}},{"cell_type":"code","source":"# function to aggregate\nf = {\"product_id\": lambda g: \" \".join(g),\n    \"add_to_cart_order\": [\"count\"]}\n\n# format product-ids to string\nprior[\"product_id\"] = prior[\"product_id\"].astype(str)\n\n# roll-up\nprior_orders = prior.groupby([\"order_id\"]).agg(f)\nprint(prior_orders.shape)\nprior_orders.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =================================================\n# reset column levels and rename\n# =================================================\n\nprior_orders.columns = prior_orders.columns.droplevel(1)\nprior_orders.rename(columns = {\"add_to_cart_order\" : \"num_products\"}, inplace = True)\nprior_orders = prior_orders.loc[prior_orders[\"num_products\"] > 1, :]\nprior_orders.reset_index(inplace = True)\nprint(prior_orders.shape)\nprior_orders.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 **Train orders**","metadata":{}},{"cell_type":"code","source":"# Note:\n## - We have made our own training and test data\n## - This dataset will eventually be appended to the larger prior orders","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train orders file\nwith ZipFile(data_dir + \"order_products__train.csv.zip\") as z:\n    with z.open(\"order_products__train.csv\") as f:\n        train = pd.read_csv(f)\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retain only frequently sold products\ntrain = train.loc[train[\"product_id\"].isin(list(prod_freq.index)), :]\nprint(train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to aggregate\nf = {\"product_id\": lambda g: \" \".join(g),\n    \"add_to_cart_order\": [\"count\"]}\ntrain[\"product_id\"] = train[\"product_id\"].astype(str)\ntrain_orders = train.groupby([\"order_id\"]).agg(f)\ntrain_orders.columns = train_orders.columns.droplevel(1)\ntrain_orders.reset_index(inplace = True)\ntrain_orders.rename(columns = {\"add_to_cart_order\" : \"num_products\"}, inplace = True)\ntrain_orders = train_orders.loc[train_orders[\"num_products\"] > 1, :] # retain baskets with more than one product\nprint(train_orders.shape)\ntrain_orders.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================================================\n# Combine training and prior orders\n# =========================================================\n\n\n# add identifier column to both data sets to append\n# orders-level data\nprior_orders[\"eval\"] = \"prior\"\ntrain_orders[\"eval\"] = \"train\"\n\n# order-product level data\nprior[\"eval\"] = \"prior\"\ntrain[\"eval\"] = \"train\"\n\n\n# append wide orders\nall_orders_wide = prior_orders.append(train_orders)\nall_orders_wide.reset_index(drop = True, inplace = True)\nprint(all_orders_wide.shape)\n\n# append long orders\nall_orders_long = prior.append(train)\nall_orders_long.reset_index(drop = True, inplace = True)\nprint(all_orders_long.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_orders_wide.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 3. Prepare data for modeling","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# merge orders-wide and orders to get train-test split\n# =========================================================\n\nall_orders_wide.drop(labels = \"eval\", axis = 1, inplace = True)\n\n# merge\norders_wide = pd.merge(all_orders_wide, orders[[\"order_id\", \"user_id\", \"eval\"]],\n                       on = \"order_id\", how = \"left\")\nprint(orders_wide.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# split train-val-test\n#===============================================\n\ntrain = orders_wide.loc[orders_wide[\"eval\"].isin([\"prior\", \"train\"]), :]\nval = orders_wide.loc[orders_wide[\"eval\"] == \"val\", :]\ntest = orders_wide.loc[orders_wide[\"eval\"] == \"test\", :]\nprint(\"train size:\", train.shape)\nprint(\"val size:\", val.shape)\nprint(\"test size:\", test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clear up memory\ndel all_orders_wide\ndel orders_wide\ngc.collect()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# randomly sample training data\n#===============================================\n\nsample_size = 1000000\ntrain = train.sample(n = sample_size)\ntrain = train.reset_index(drop = True)\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use keras tokenizer to split baskets into individual products\nvocab_size = 15000\ntokenizer = Tokenizer(num_words = vocab_size, lower = False)\n\n# fit the tokenizer orders\ntokenizer.fit_on_texts(list(train[\"product_id\"].values))\nprint(tokenizer.document_count)\nprint(len(tokenizer.word_counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map orders from product-ids to contiguous integers\ntrain_orders = tokenizer.texts_to_sequences(list(train[\"product_id\"].values))\nval_orders = tokenizer.texts_to_sequences(list(val[\"product_id\"].values))\ntest_orders = tokenizer.texts_to_sequences(list(test[\"product_id\"].values))\nprint(len(train_orders))\nprint(len(val_orders))\nprint(len(test_orders))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# manually inspect some data\nprint(train_orders[:3])\ntrain[\"product_id\"][0:3]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save pickle files\nwith open('/kaggle/working/train_orders.pkl', 'wb') as f:\n    pickle.dump(train_orders, f)\nf.close() \n    \n\nwith open('/kaggle/working/val_orders.pkl', 'wb') as f:\n    pickle.dump(val_orders, f)\nf.close()     \n\n    \nwith open('/kaggle/working/test_orders.pkl', 'wb') as f:\n    pickle.dump(test_orders, f)  \nf.close()     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"## **4. Setup Tensorflow**","metadata":{}},{"cell_type":"code","source":"# define vocab size - this is total number of unique products \nvocab_size = len(tokenizer.word_index) + 1 # we add one to account for products in test/val that were not in train. They are treated as \"UNK\". ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the sampling table for negative sampling\nsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size, sampling_factor = 0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1. Data generator - this is typically the tricky part in setting up a data pipeline for deep learning","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, order_data, batch_size = 64, cs = 15, ns = 20, shuffle = True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.order_data = order_data\n        self.shuffle = shuffle\n        self.cs = cs\n        self.ns = ns\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.order_data) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.order_data[k] for k in indexes]\n\n        # Generate data\n        X, dv = self.__data_generation(list_IDs_temp)\n\n        return X, dv\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.order_data))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        iv = []\n        dv =[]\n\n        # Generate data\n        for i, d in enumerate(list_IDs_temp):\n            # Store sample\n            couples, labels = skipgrams(d, vocabulary_size = vocab_size, window_size = self.cs,\n                                        negative_samples = self.ns, sampling_table = sampling_table)\n            iv = iv + couples\n            dv = dv + labels\n            \n        X = np.array(iv, dtype = \"int32\")\n        X = [X[:, 0], X[:, 1]]\n        return X, np.array(dv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# randomly sample orders for quick training\ntrain_orders = random.sample(train_orders, 300000)\nprint(len(train_orders))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize train and validation data generators with default setting\ntrain_gen = DataGenerator(train_orders)\nval_gen = DataGenerator(val_orders)\n\n# test data\ntest_gen = DataGenerator(test_orders, cs = 10, ns = 0, shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Set-up a simple model","metadata":{}},{"cell_type":"code","source":"# model input parameters\nemb_size = 32\nbatch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define input layers\ninput_target = Input((1, ))\ninput_context = Input((1, ))\n\n# target\ntarget = Embedding(input_dim = vocab_size, output_dim= emb_size, name = \"rho\")(input_target)\ntarget = Reshape(target_shape = (emb_size, 1))(target)\n\n# context\ncontext = Embedding(input_dim = vocab_size, output_dim= emb_size, name = \"alpha\")(input_context)\ncontext = Reshape(target_shape = (emb_size, 1))(context)\n\n# concatenate model inputs and outputs\ninput_model = [input_target, input_context]\noutput_embeddings = Dot(axes = 1)([target, context])\noutput_model = Flatten()(output_embeddings)\n\n# complete model\noutput_model = Dense(1, activation = \"sigmoid\")(output_model)\n\n# define as keras model\nemb_model = Model(inputs = input_model, outputs = output_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model optimizer and compile\nadam = optimizers.Adam()\nemb_model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"acc\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initial weights - rho\ninit_alpha = emb_model.get_layer(\"alpha\").get_weights()[0]\nprint(init_alpha.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit model\nhistory = History()\nt0 = time.time()\nemb_model.fit(x = train_gen, \n               epochs = 1,\n               validation_data = val_gen,\n               use_multiprocessing = True,\n               callbacks = [history])\nt1 = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save keras model\n# emb_model.save('/kaggle/working/emb_model/')\nemb_model = tf.keras.models.load_model('/kaggle/input/product-embeddings/emb_model/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Inspect model results","metadata":{}},{"cell_type":"code","source":"# initial weights\ninit_alpha","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final weights - target words\nfinal_alpa = emb_model.get_layer(\"alpha\").get_weights()[0]\nprint(final_alpa.shape)\nfinal_alpa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4. Extract results for downstream tasks","metadata":{}},{"cell_type":"code","source":"#===============================================\n# extract embeddings to data frame\n#===============================================\n\ndef EmbToDataFrame(ix_word, emb_mat, col_prefix = \"rho\"):\n    emb_df = {ix_word[i]: list(emb_mat[i-1]) for i in list(ix_word.keys())[:vocab_size]}\n    emb_df = pd.DataFrame.from_dict(emb_df, orient = \"columns\")\n    emb_df = emb_df.transpose().reset_index(drop = False)\n    emb_df.columns = [\"product_id\"] + [col_prefix + str(i + 1) for i in range(emb_df.shape[1] - 1)]\n    return emb_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reversed mapping of words to index\nix_word = tokenizer.index_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get data frame from alpha matrix\nalpha_df = EmbToDataFrame(ix_word, emb_mat = emb_model.get_layer(\"alpha\").get_weights()[0], col_prefix = \"alpha\")\ndisplay(alpha_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# similarity in alpha matrix\n#===============================================\n\nalpha_sim = cosine_similarity(alpha_df.iloc[:, 1:])\nalpha_sim = pd.DataFrame(alpha_sim)\nalpha_sim.reset_index(inplace = True, drop = True)\nalpha_sim.index = list(alpha_df[\"product_id\"].values)\nalpha_sim.columns = list(alpha_df[\"product_id\"].values)\ndisplay(alpha_sim.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute product similarity given the embeddings\ndef ComputeProductSimilarity(prod_id, alpha_sim, top = 5, include_prod_info = True):\n    sim = alpha_sim.loc[:, prod_id]\n    sim = sim.sort_values(ascending = False)\n    sim = sim[1:][0:top]\n    sim = pd.DataFrame({\"product_id\" : list(sim.index), \"score\": sim}, index = None)\n    if include_prod_info:\n        sim = products.loc[products[\"product_id\"].isin(sim[\"product_id\"]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]]\n    return sim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prod_id = \"100\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find most similar products to the following product\nprod_id = \"49332\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find most similar products to the following product\nprod_id = \"3151\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5. Visualize embeddings using T-SNE","metadata":{}},{"cell_type":"code","source":"#===============================================\n# prep data for t-sne\n#===============================================\n\nalpha_df[\"product_id\"] = alpha_df[\"product_id\"].astype(str)\nproducts[\"product_id\"] = products[\"product_id\"].astype(str)\n\n# relevant columns from product info\nprod_info_cols = [\"product_id\", \"product_name\", \"department\", \"aisle\"]\n\n# merge product information and product embeddings into a single data frame \nprod_vec_df = pd.merge(products[prod_info_cols], alpha_df, on = \"product_id\", how = \"inner\")\nprint(prod_vec_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# fit t-sne\n#===============================================\n\n# define model\ntsne = TSNE(n_components = 2, verbose = 1, perplexity = 35, n_iter = 400)\n\n# columns to fit on\nprod_vec_names = list(prod_vec_df.columns)[4:]\n\n# fit\nt0 = time.time()\ntsne_fit = tsne.fit_transform(prod_vec_df[prod_vec_names])\nt1 = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# create t-sne data frame for plotting\n#===============================================\n\ntsne_df = prod_vec_df[[\"product_name\", \"department\", \"aisle\"]]\n\n# extract t-sne dimensions\ntsne_df[\"x_tsne\"] = tsne_fit[:,0]\ntsne_df[\"y_tsne\"] = tsne_fit[:,1]\nprint(tsne_df.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===============================================\n# subset data for plot\n#===============================================\n\n# select only top departments\nselect_dept = [\"produce\", \"babies\", \"beverages\"]\ntsne_plot_df = tsne_df.loc[tsne_df[\"department\"].isin(select_dept), :]\nprint(tsne_plot_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ng = sns.scatterplot(x = \"x_tsne\", y = \"y_tsne\",\n              hue=\"department\",\n              data = tsne_plot_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}