{"nbformat_minor":2,"cells":[{"cell_type":"markdown","source":"We are going to build a segmentation of InstaCart users to then use the segment as a factor to basic classifier model that predicts is a user will buy a product in their next order. Another option is identifying products that are likely to be purchased in next order that have not been purchased before to increase the sample for general model. Finally, we can build a separate model for each cluster.\n\nI decided to do it to try a couple of algorithms in practice, specifically I am going to use TruncatedSVD for dimension reduction, IsolationForest for excluding outliers and KMeans for clusterization itself.\n\nI found that it would be interesting to share my results with the community. I am very happy to get feedback on my approach, please comment if you have any questions or suggestions. Let's start!","metadata":{"_cell_guid":"f2044c8d-7b5c-4d13-b1bc-3732f41bc40d","_uuid":"bcb383e6ac0b5affde0037d5758096d3ca8345e2"}},{"cell_type":"markdown","source":"First we load the data.","metadata":{"_cell_guid":"a32e753b-8b27-464a-bad6-75c3cab1914d","_uuid":"a444e10f4b03ce24c1a9af6d87574af543a4ad43"}},{"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\n\ninput_folder = '../input/'\n\nproducts = pd.read_csv(input_folder + 'products.csv', index_col='product_id')\norders = pd.read_csv(input_folder + 'orders.csv', usecols=['order_id','user_id','eval_set'], index_col='order_id')\nitem_prior = pd.read_csv(input_folder + 'order_products__prior.csv', usecols=['order_id','product_id'], index_col=['order_id','product_id'])","execution_count":null,"metadata":{"_cell_guid":"618aa22c-78ba-4e88-a11d-abc519ce9fb6","ExecuteTime":{"end_time":"2017-07-10T21:36:36.640628Z","start_time":"2017-07-10T21:36:22.123881Z"},"_uuid":"354d92bbb133e53a67c7fb3a576470c9db88603e"}},{"cell_type":"markdown","source":"Now let's extract what we need - data which customers have bought which products.","metadata":{"_cell_guid":"e457a273-4b14-4247-83ef-5890461ed3ed","_uuid":"bf5bd9a642d0e9b5c1631b8ad704c0a6658b7444"}},{"cell_type":"code","outputs":[],"source":"# basic prior products table\nuser_product = orders.join(item_prior, how='inner').reset_index().groupby(['user_id','product_id']).count()\nuser_product = user_product.reset_index().rename(columns={'order_id':'prior_order_count'})","execution_count":null,"metadata":{"_cell_guid":"e89f9533-4b63-4157-93e7-6d675309108f","ExecuteTime":{"end_time":"2017-07-10T21:37:05.838074Z","start_time":"2017-07-10T21:36:36.64363Z"},"_uuid":"1708477e5783706d6484e3110f9ed16a99d951d5"}},{"cell_type":"markdown","source":"I am going to translate it into a sparse matrix to then apply a dimension reduction algorithm.","metadata":{"_cell_guid":"868c9432-f07e-4463-a8da-70772e903fa2","_uuid":"3a96730df2731177b5acde9c1eb13634adae231f"}},{"cell_type":"code","outputs":[],"source":"from scipy.sparse import csr_matrix\nuser_product_sparse = csr_matrix((user_product['prior_order_count'], (user_product['user_id'], user_product['product_id'])), shape=(user_product['user_id'].max()+1, user_product['product_id'].max()+1), dtype=np.uint16)","execution_count":null,"metadata":{"_cell_guid":"cdfb8859-bc53-4c65-9624-af646ccfcb14","ExecuteTime":{"end_time":"2017-07-10T21:37:10.785663Z","start_time":"2017-07-10T21:37:05.849087Z"},"_uuid":"69aef6d82344cbbc358b90ff09f1f9274f877ddb"}},{"cell_type":"markdown","source":"Now let's apply a singular value decomposition with 10 components. In short, this algorithm will reduce the dimension of our problem and instead of dealing with ~50k factors (products) it will determine a new feature space that will explain the most variance in our data.","metadata":{"_cell_guid":"99655f49-d591-413b-9b5f-81e34cf98e26","_uuid":"87fbf35b9bd8e338e80716735bc1bdb12aed4283"}},{"cell_type":"code","outputs":[],"source":"from sklearn.decomposition import TruncatedSVD\ndecomp = TruncatedSVD(n_components=10, random_state=101)\nuser_reduced = decomp.fit_transform(user_product_sparse)\n\nprint(decomp.explained_variance_ratio_[:10], decomp.explained_variance_ratio_.sum())","execution_count":null,"metadata":{"_cell_guid":"c88700a4-084a-4090-8a5c-f197cdabc8dd","ExecuteTime":{"end_time":"2017-07-10T21:40:01.923423Z","start_time":"2017-07-10T21:39:56.433918Z"},"_uuid":"1fecc46cf939ac138a82475b0fa5d3da2f34636c"}},{"cell_type":"markdown","source":"As we can see in the output, our new 10 factors explain ~16% of total variance and the most important factor explains around 6% of variance. Not bad for a reduction from 50k to 10 variables.","metadata":{"_cell_guid":"e5fd720b-e063-43f4-8705-466d0ba1f616","_uuid":"81298c0ed2cd4044fd6ea7c2caed8ce278c165e1"}},{"cell_type":"markdown","source":"The next step is to cluster the data, but before we need to do a couple of adjustments. First of all, let's scale it with StandardScaler to assure proper result of KMeans.","metadata":{"_cell_guid":"cfe7f2a2-be44-43b6-bbdc-9ec8dca0dd94","_uuid":"4a87735943035117eba85e642eb5da481fd68ec2"}},{"cell_type":"code","outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nuser_reduced_scaled = scaler.fit_transform(user_reduced)","execution_count":null,"metadata":{"_cell_guid":"5e791d51-7d4d-40d4-b055-3fe72205e775","ExecuteTime":{"end_time":"2017-07-10T21:37:18.938018Z","start_time":"2017-07-10T21:37:18.861961Z"},"collapsed":true,"_uuid":"9f98556bc840cf3219950e5f6e6f6d74b1da7853"}},{"cell_type":"markdown","source":"It's also a good idea to get rid of outliers before doing the clusterization, otherwise we have a good chances to get a separate class for each outlier and all the rest of users in single class, definitely something we don't really want. I am going to use IsolationForest algorithm with 5% set as a share of outliers I'd like to exclude at the step of KMeans model training.","metadata":{"_cell_guid":"10acc45b-b84b-46a5-804b-e3a5b657c54c","_uuid":"8e9af1e87d1a4ebbaf0401081f56bd68e6e8e028"}},{"cell_type":"code","outputs":[],"source":"from sklearn.ensemble import IsolationForest\nclf = IsolationForest(contamination=0.05, random_state=101)\nclf.fit(user_reduced_scaled)\noutliers = clf.predict(user_reduced_scaled)\n\nunique, counts = np.unique(outliers, return_counts=True)\ndict(zip(unique, counts))","execution_count":null,"metadata":{"_cell_guid":"cce08da8-4467-40e7-8853-4a221d8471a2","ExecuteTime":{"end_time":"2017-07-10T21:37:37.044127Z","start_time":"2017-07-10T21:37:18.94002Z"},"_uuid":"36a84e7fe3ce8b567db6e43f4721eb8cbc6778d3"}},{"cell_type":"markdown","source":"I have found this contamination parameter empirically, making sure we don't exclude too much data but the classes are more or less balanced. If someone has a good idea how to determine this paratemer using some algorithm - please let me know! Let's check how it looks like in 2D space of the first two factors (being most important ones).","metadata":{"_cell_guid":"341779ce-c4a4-4946-9d70-5717681f5334","_uuid":"dc8615b6a04ab4183785816698ad3069780e753a"}},{"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# red is an outlier, green is a regular observation\ncolor_map = np.vectorize({ -1: 'r', 1: 'g'}.get)\nplt.scatter(user_reduced_scaled[:,0], user_reduced_scaled[:,1], c=color_map(outliers), alpha=0.1)","execution_count":null,"metadata":{"_cell_guid":"d532429f-93d6-4b5e-96e5-1067830f485a","ExecuteTime":{"end_time":"2017-07-10T21:37:50.152876Z","start_time":"2017-07-10T21:37:37.046111Z"},"_uuid":"cd70a0f46528cb94427d6192982e808b0871535f"}},{"cell_type":"markdown","source":"It might look like we have excluded too many observations, but in reality the density is very different across our space and most of points remained untouched.","metadata":{"_cell_guid":"e207bc94-6abe-45e6-9eb3-bc2092124cbb","_uuid":"dfa747706942bd9037113b8bd779fcef4226ec17"}},{"cell_type":"markdown","source":"Now let's train the KMeans algorithm and check if we can get some meaningful clusters out.","metadata":{"_cell_guid":"b1666dd8-8f36-452c-8c3c-8d3653968879","_uuid":"5243c2c5b348f99b0b0c32de080353906697ee2f"}},{"cell_type":"code","outputs":[],"source":"from sklearn.cluster import KMeans\n\nclusters_count = 10\n\nkmc = KMeans(n_clusters=clusters_count, init='random', n_init=10, random_state=101)\nkmc.fit(user_reduced_scaled[outliers == 1,:])\nclusters = kmc.predict(user_reduced_scaled)\n\nunique, counts = np.unique(clusters, return_counts=True)\ndict(zip(unique, counts))","execution_count":null,"metadata":{"_cell_guid":"0fafe626-6f3f-4060-ab55-e4a1b9e19332","ExecuteTime":{"end_time":"2017-07-10T21:38:09.511632Z","start_time":"2017-07-10T21:37:50.153865Z"},"scrolled":true,"_uuid":"57e063952755ca104d9e2186a43a93550ffc3ed0"}},{"cell_type":"markdown","source":"Vast majority of users are in one single class, which might be not very good as we are not catching the actual differences between our users, but it might also be okay if in reality most of users are indeed super similar. There is really no way to know that for sure.","metadata":{"_cell_guid":"2202bfbc-512c-4d0f-a724-4cc26d1bcda6","_uuid":"7bdcb256ff40fb0542b129212dcf65d229773508"}},{"cell_type":"markdown","source":"Let's check how the clusters look like on the same 2D plane.","metadata":{"_cell_guid":"3ee4409c-e503-483c-846e-6864f1b64fab","_uuid":"c77f00694f50ece33b857fe274cb01d954d58e12"}},{"cell_type":"code","outputs":[],"source":"plt.scatter(user_reduced_scaled[:,0], user_reduced_scaled[:,1], c=clusters / (clusters_count-1), cmap='tab10', alpha=0.1)","execution_count":null,"metadata":{"_cell_guid":"91190172-c646-484f-bc2f-90d401cb9f35","ExecuteTime":{"end_time":"2017-07-10T21:38:18.740566Z","start_time":"2017-07-10T21:38:09.514635Z"},"_uuid":"2beffb9fb615cdfe1f78087eb148b0c58c21fffe"}},{"cell_type":"markdown","source":"It looks like a mess, but don't forget we are dealing with 10 dimensions in our clasterization problem. Still there is some clear structure visible.","metadata":{"_cell_guid":"cb02fce0-451a-4f4f-a535-93368fd27120","_uuid":"db3a54672a3384f98f5b6435ddf9558a4cd66f4b"}},{"cell_type":"markdown","source":"Now let's check what are the actual differences between clusters. We will check products that are the most popular in the clusters compared to all sample. It does not make a lot of sense to just look at the most popular products by cluster because they will likely have the same products across clusters, like bananas or avocadoes. We will compare product ranks (ranked by # of purchases) with ranks on the total population.","metadata":{"_cell_guid":"05aae211-17e3-4299-85ef-41e2055f3322","_uuid":"66f221594b7c0a897755c5caefecc3b448d2cbf1"}},{"cell_type":"code","outputs":[],"source":"# dataframe with overall product ranks\ntop_products_overall = user_product[['product_id','prior_order_count']].groupby('product_id').sum().reset_index().sort_values('prior_order_count', ascending=False)\ntop_products_overall['rank_overall'] = top_products_overall['prior_order_count'].rank(ascending=False)\n\n# packing clusters we found into dataframe\nusersdf = pd.DataFrame(clusters[1:], columns=['cluster'], index=np.arange(1, user_product['user_id'].max()+1))\n\n# dataframe with product ranks across clusters\ntop_products = user_product.merge(usersdf, left_on='user_id', right_index=True)[['product_id','cluster','prior_order_count']].groupby(['product_id','cluster']).sum().reset_index().sort_values(['cluster','prior_order_count'], ascending=False)\ntop_products['rank'] = top_products[['cluster','prior_order_count']].groupby('cluster').rank(ascending=False)\n\n# merging with overall top products\ntop_products = top_products.merge(top_products_overall[['product_id','rank_overall']], left_on='product_id', right_on='product_id')\n# calculating differences between ranks\ntop_products['rank_diff'] = top_products['rank'] - top_products['rank_overall']\n# leaving top products in each cluster: 2 with largest and 2 with smallest difference in ranks\ntop_products_asc_diff = top_products.sort_values(['cluster','rank_diff'], ascending=False).groupby('cluster').head(2).reset_index(drop=True)\ntop_products_desc_diff = top_products.sort_values(['cluster','rank_diff'], ascending=True).groupby('cluster').head(2).reset_index(drop=True)\ntop_products_diff = pd.concat([top_products_asc_diff,top_products_desc_diff], axis=0)\n\n# printing results\ntop_products_diff.merge(products[['product_name']], left_on='product_id', right_index=True)[['cluster','product_name','rank','rank_overall','rank_diff']].sort_values(['cluster','rank_diff'])","execution_count":null,"metadata":{"_cell_guid":"6f778fe7-2ab2-415d-82bc-d7c8be4f7952","ExecuteTime":{"end_time":"2017-07-10T21:39:22.395304Z","start_time":"2017-07-10T21:39:16.852171Z"},"_uuid":"d885584bd6cd619bb17e88e2fc76b6514cd49055"}},{"cell_type":"markdown","source":"It's not always very easy to describe each of the clusters with words and get a feeling of them. We can see that users from cluster 5 tend to buy *California Ripe Pitted Extra Large Olives* and *Vanilla Spiru-Tein High Protein Energy Shake* significantly more frequent than average but they don't like *Extra Fancy Unsalted Mixed Nuts* and *Baby Cucumbers* which are one of the most popular products overall.\n\nThis absense of a clear interpretation of clusters does not mean the clusters are meaningless though. The only possible proof if we are talking about data science is the actual usage of these clusters in some way that I mentioned in the beginning of the kernel. I will soon add this variable to my basic XGBoost classifier and post my results.","metadata":{"_cell_guid":"ad6d956c-d21d-4ce2-91ae-2da53e7c70e4","_uuid":"5ed9bc8f2816fde6afb1ef1a433cb1081fa5f62a"}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.1","name":"python"}}}