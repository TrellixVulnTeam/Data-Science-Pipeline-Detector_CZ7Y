{"metadata":{"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"851c5a0960ad8bcd9ee95a7681e08abf422efd41","_cell_guid":"99b96bae-2969-439e-95d7-e7f59504eeb2","_execution_state":"idle"},"source":"This script demonstrates basic use of catboost (CV 0.377750059602, LB 0.3802708)\n\nI believe higher scores are possible - I got better early results without using aisle, so there may be an issue with how I set it up - maybe you can make it work better? ;)\n\nThe core of the script is based from \"LB 0.3805009, Python Edition\" by üê≥È≤≤(China), the catboost settings are taken from Fred Navruzov's posts, and the CV code is reused from my earlier notebook.\n\n#### Original notes:\n\nThis script is translate from @Fabienvs's [R code](https://www.kaggle.com/fabienvs/instacart-xgboost-starter-lb-0-3791), I think it may help kagglers who do not use R.  \nI really appreciate @Fabienvs's great work, to be honest, I have no idea about how to handling this kind of problem(this is the first time I encounter recommendation problem- -)  \nhere we go!! \nbelow exist some very useful functions I write by my own, you can download from my [github repo](https://github.com/NickYi1990/Kaggle_Buddy.git)\nsorry for adding some Chinese in it, it would not affect the code\n## The dataset is too big, you should run it on your desktop!","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"39008a2aa2a73398cb80d8b683bb626b902893e3","_cell_guid":"e1d5f7cc-e69b-4b75-9e49-c42ab70e1f81","_execution_state":"idle"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport gc\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"cell_type":"code","execution_count":1},{"metadata":{"_uuid":"b839f7d60b63a40aa6ef2efbdf979468a837da7b"},"source":"import xgboost\nimport catboost","outputs":[],"cell_type":"code","execution_count":2},{"metadata":{"_uuid":"043ac17afde77d99e4e69565d3f4c090a6e797bd","collapsed":true,"_cell_guid":"4449943f-7006-4c6f-a7b4-4bfc24dc1f1a","_execution_state":"idle"},"source":"def load_data(path_data):\n    '''\n    --------------------------------order_product--------------------------------\n    * Unique in order_id + product_id\n    '''\n    priors = pd.read_csv(path_data + 'order_products__prior.csv', \n                     dtype={\n                            'order_id': np.int32,\n                            'product_id': np.uint16,\n                            'add_to_cart_order': np.int16,\n                            'reordered': np.int8})\n    train = pd.read_csv(path_data + 'order_products__train.csv', \n                    dtype={\n                            'order_id': np.int32,\n                            'product_id': np.uint16,\n                            'add_to_cart_order': np.int16,\n                            'reordered': np.int8})\n    '''\n    --------------------------------order--------------------------------\n    * This file tells us which set (prior, train, test) an order belongs\n    * Unique in order_id\n    * order_id in train, prior, test has no intersection\n    * this is the #order_number order of this user\n    '''\n    orders = pd.read_csv(path_data + 'orders.csv', \n                         dtype={\n                                'order_id': np.int32,\n                                'user_id': np.int64,\n                                'eval_set': 'category',\n                                'order_number': np.int16,\n                                'order_dow': np.int8,\n                                'order_hour_of_day': np.int8,\n                                'days_since_prior_order': np.float32})\n\n    #  order in prior, train, test has no duplicate\n    #  order_ids_pri = priors.order_id.unique()\n    #  order_ids_trn = train.order_id.unique()\n    #  order_ids_tst = orders[orders.eval_set == 'test']['order_id'].unique()\n    #  print(set(order_ids_pri).intersection(set(order_ids_trn)))\n    #  print(set(order_ids_pri).intersection(set(order_ids_tst)))\n    #  print(set(order_ids_trn).intersection(set(order_ids_tst)))\n\n    '''\n    --------------------------------product--------------------------------\n    * Unique in product_id\n    '''\n    products = pd.read_csv(path_data + 'products.csv')\n    aisles = pd.read_csv(path_data + \"aisles.csv\")\n    departments = pd.read_csv(path_data + \"departments.csv\")\n    sample_submission = pd.read_csv(path_data + \"sample_submission.csv\")\n    \n    return priors, train, orders, products, aisles, departments, sample_submission\n\nclass tick_tock:\n    def __init__(self, process_name, verbose=1):\n        self.process_name = process_name\n        self.verbose = verbose\n    def __enter__(self):\n        if self.verbose:\n            print(self.process_name + \" begin ......\")\n            self.begin_time = time.time()\n    def __exit__(self, type, value, traceback):\n        if self.verbose:\n            end_time = time.time()\n            print(self.process_name + \" end ......\")\n            print('time lapsing {0} s \\n'.format(end_time - self.begin_time))\n            \ndef ka_add_groupby_features_1_vs_n(df, group_columns_list, agg_dict, only_new_feature=True):\n    '''Create statistical columns, group by [N columns] and compute stats on [N column]\n\n       Parameters\n       ----------\n       df: pandas dataframe\n          Features matrix\n       group_columns_list: list_like\n          List of columns you want to group with, could be multiple columns\n       agg_dict: python dictionary\n\n       Return\n       ------\n       new pandas dataframe with original columns and new added columns\n\n       Example\n       -------\n       {real_column_name: {your_specified_new_column_name : method}}\n       agg_dict = {'user_id':{'prod_tot_cnts':'count'},\n                   'reordered':{'reorder_tot_cnts_of_this_prod':'sum'},\n                   'user_buy_product_times': {'prod_order_once':lambda x: sum(x==1),\n                                              'prod_order_more_than_once':lambda x: sum(x==2)}}\n       ka_add_stats_features_1_vs_n(train, ['product_id'], agg_dict)\n    '''\n    with tick_tock(\"add stats features\"):\n        try:\n            if type(group_columns_list) == list:\n                pass\n            else:\n                raise TypeError(k + \"should be a list\")\n        except TypeError as e:\n            print(e)\n            raise\n\n        df_new = df.copy()\n        grouped = df_new.groupby(group_columns_list)\n\n        the_stats = grouped.agg(agg_dict)\n        the_stats.columns = the_stats.columns.droplevel(0)\n        the_stats.reset_index(inplace=True)\n        if only_new_feature:\n            df_new = the_stats\n        else:\n            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n\n    return df_new\n\ndef ka_add_groupby_features_n_vs_1(df, group_columns_list, target_columns_list, methods_list, keep_only_stats=True, verbose=1):\n    '''Create statistical columns, group by [N columns] and compute stats on [1 column]\n\n       Parameters\n       ----------\n       df: pandas dataframe\n          Features matrix\n       group_columns_list: list_like\n          List of columns you want to group with, could be multiple columns\n       target_columns_list: list_like\n          column you want to compute stats, need to be a list with only one element\n       methods_list: list_like\n          methods that you want to use, all methods that supported by groupby in Pandas\n\n       Return\n       ------\n       new pandas dataframe with original columns and new added columns\n\n       Example\n       -------\n       ka_add_stats_features_n_vs_1(train, group_columns_list=['x0'], target_columns_list=['x10'])\n    '''\n    with tick_tock(\"add stats features\", verbose):\n        dicts = {\"group_columns_list\": group_columns_list , \"target_columns_list\": target_columns_list, \"methods_list\" :methods_list}\n\n        for k, v in dicts.items():\n            try:\n                if type(v) == list:\n                    pass\n                else:\n                    raise TypeError(k + \"should be a list\")\n            except TypeError as e:\n                print(e)\n                raise\n\n        grouped_name = ''.join(group_columns_list)\n        target_name = ''.join(target_columns_list)\n        combine_name = [[grouped_name] + [method_name] + [target_name] for method_name in methods_list]\n\n        df_new = df.copy()\n        grouped = df_new.groupby(group_columns_list)\n\n        the_stats = grouped[target_name].agg(methods_list).reset_index()\n        the_stats.columns = [grouped_name] + \\\n                            ['_%s_%s_by_%s' % (grouped_name, method_name, target_name) \\\n                             for (grouped_name, method_name, target_name) in combine_name]\n        if keep_only_stats:\n            return the_stats\n        else:\n            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n        return df_new","outputs":[],"cell_type":"code","execution_count":3},{"metadata":{"_uuid":"23a32c0dfd0d95bb48f640c5eb4b97e814c60d9f","collapsed":true},"source":"path_data = '../input/'\npriors, train, orders, products, aisles, departments, sample_submission = load_data(path_data)\n","outputs":[],"cell_type":"code","execution_count":4},{"metadata":{"_uuid":"28c353c1b3aca026bad48a3407d2637bbfd64a16"},"source":"### Time-saving measure for local work - cache a local copy of the final dataframe ","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"652ba1d76cc52f7bb8992bc002cd18ced9155ab5","collapsed":true,"_cell_guid":"13b01c67-a0f3-412a-ab62-b49390a494bf","_execution_state":"idle"},"source":"try:\n    data = pd.read_pickle('kernel38-data.pkl')\nexcept:\n\n    # Product part\n\n    # Products information ----------------------------------------------------------------\n    # add order information to priors set\n    priors_orders_detail = orders.merge(right=priors, how='inner', on='order_id')\n\n    # create new variables\n    # _user_buy_product_times: Áî®Êà∑ÊòØÁ¨¨Âá†Ê¨°Ë¥≠‰π∞ËØ•ÂïÜÂìÅ\n    priors_orders_detail.loc[:,'_user_buy_product_times'] = priors_orders_detail.groupby(['user_id', 'product_id']).cumcount() + 1\n    # _prod_tot_cnts: ËØ•ÂïÜÂìÅË¢´Ë¥≠‰π∞ÁöÑÊÄªÊ¨°Êï∞,Ë°®ÊòéË¢´ÂñúÊ¨¢ÁöÑÁ®ãÂ∫¶\n    # _reorder_tot_cnts_of_this_prod: Ëøô‰ª∂ÂïÜÂìÅË¢´ÂÜçÊ¨°Ë¥≠‰π∞ÁöÑÊÄªÊ¨°Êï∞\n    ### ÊàëËßâÂæó‰∏ãÈù¢‰∏§‰∏™Âæà‰∏çÂ•ΩÁêÜËß£ÔºåËÄÉËôëÊîπÂèò++++++++++++++++++++++++++\n    # _prod_order_once: ËØ•ÂïÜÂìÅË¢´Ë¥≠‰π∞‰∏ÄÊ¨°ÁöÑÊÄªÊ¨°Êï∞\n    # _prod_order_more_than_once: ËØ•ÂïÜÂìÅË¢´Ë¥≠‰π∞‰∏ÄÊ¨°‰ª•‰∏äÁöÑÊÄªÊ¨°Êï∞\n    agg_dict = {'user_id':{'_prod_tot_cnts':'count'}, \n                'reordered':{'_prod_reorder_tot_cnts':'sum'}, \n                '_user_buy_product_times': {'_prod_buy_first_time_total_cnt':lambda x: sum(x==1),\n                                            '_prod_buy_second_time_total_cnt':lambda x: sum(x==2)}}\n    prd = ka_add_groupby_features_1_vs_n(priors_orders_detail, ['product_id'], agg_dict)\n\n    # _prod_reorder_prob: Ëøô‰∏™ÊåáÊ†á‰∏çÂ•ΩÁêÜËß£\n    # _prod_reorder_ratio: ÂïÜÂìÅÂ§çË¥≠Áéá\n    prd['_prod_reorder_prob'] = prd._prod_buy_second_time_total_cnt / prd._prod_buy_first_time_total_cnt\n    prd['_prod_reorder_ratio'] = prd._prod_reorder_tot_cnts / prd._prod_tot_cnts\n    prd['_prod_reorder_times'] = 1 + prd._prod_reorder_tot_cnts / prd._prod_buy_first_time_total_cnt\n\n    prd.head()\n\n    # User Part\n\n    # _user_total_orders: Áî®Êà∑ÁöÑÊÄªËÆ¢ÂçïÊï∞\n    # ÂèØ‰ª•ËÄÉËôëÂä†ÂÖ•ÂÖ∂ÂÆÉÁªüËÆ°ÊåáÊ†á++++++++++++++++++++++++++\n    # _user_sum_days_since_prior_order: Ë∑ùÁ¶ª‰∏äÊ¨°Ë¥≠‰π∞Êó∂Èó¥(Âíå),Ëøô‰∏™Âè™ËÉΩÂú®ordersË°®ÈáåÈù¢ËÆ°ÁÆóÔºåpriors_orders_detail‰∏çÊòØÂú®order level‰∏äÈù¢unique\n    # _user_mean_days_since_prior_order: Ë∑ùÁ¶ª‰∏äÊ¨°Ë¥≠‰π∞Êó∂Èó¥(ÂùáÂÄº)\n    agg_dict_2 = {'order_number':{'_user_total_orders':'max'},\n                  'days_since_prior_order':{'_user_sum_days_since_prior_order':'sum', \n                                            '_user_mean_days_since_prior_order': 'mean'}}\n    users = ka_add_groupby_features_1_vs_n(orders[orders.eval_set == 'prior'], ['user_id'], agg_dict_2)\n\n    # _user_reorder_ratio: reorderÁöÑÊÄªÊ¨°Êï∞ / Á¨¨‰∏ÄÂçïÂêé‰π∞ÂêéÁöÑÊÄªÊ¨°Êï∞\n    # _user_total_products: Áî®Êà∑Ë¥≠‰π∞ÁöÑÊÄªÂïÜÂìÅÊï∞\n    # _user_distinct_products: Áî®Êà∑Ë¥≠‰π∞ÁöÑuniqueÂïÜÂìÅÊï∞\n    agg_dict_3 = {'reordered':\n                  {'_user_reorder_ratio': \n                   lambda x: sum(priors_orders_detail.ix[x.index,'reordered']==1)/\n                             sum(priors_orders_detail.ix[x.index,'order_number'] > 1)},\n                  'product_id':{'_user_total_products':'count', \n                                '_user_distinct_products': lambda x: x.nunique()}}\n    us = ka_add_groupby_features_1_vs_n(priors_orders_detail, ['user_id'], agg_dict_3)\n    users = users.merge(us, how='inner')\n\n    # Âπ≥ÂùáÊØèÂçïÁöÑÂïÜÂìÅÊï∞\n    # ÊØèÂçï‰∏≠ÊúÄÂ§öÁöÑÂïÜÂìÅÊï∞ÔºåÊúÄÂ∞ëÁöÑÂïÜÂìÅÊï∞++++++++++++++\n    users['_user_average_basket'] = users._user_total_products / users._user_total_orders\n\n    us = orders[orders.eval_set != \"prior\"][['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n    us.rename(index=str, columns={'days_since_prior_order': 'time_since_last_order'}, inplace=True)\n\n    users = users.merge(us, how='inner')\n\n    users.head()\n\n    # Database Part\n\n    # ËøôÈáåÂ∫îËØ•ËøòÊúâÂæàÂ§öÂèòÈáèÂèØ‰ª•Ë¢´Ê∑ªÂä†\n    # _up_order_count: Áî®Êà∑Ë¥≠‰π∞ËØ•ÂïÜÂìÅÁöÑÊ¨°Êï∞\n    # _up_first_order_number: Áî®Êà∑Á¨¨‰∏ÄÊ¨°Ë¥≠‰π∞ËØ•ÂïÜÂìÅÊâÄÂ§ÑÁöÑËÆ¢ÂçïÊï∞\n    # _up_last_order_number: Áî®Êà∑ÊúÄÂêé‰∏ÄÊ¨°Ë¥≠‰π∞ËØ•ÂïÜÂìÅÊâÄÂ§ÑÁöÑËÆ¢ÂçïÊï∞\n    # _up_average_cart_position: ËØ•ÂïÜÂìÅË¢´Ê∑ªÂä†Âà∞Ë¥≠Áâ©ÁØÆ‰∏≠ÁöÑÂπ≥Âùá‰ΩçÁΩÆ\n    agg_dict_4 = {'order_number':{'_up_order_count': 'count', \n                                  '_up_first_order_number': 'min', \n                                  '_up_last_order_number':'max'}, \n                  'add_to_cart_order':{'_up_average_cart_position': 'mean'}}\n\n    data = ka_add_groupby_features_1_vs_n(df=priors_orders_detail, \n                                                          group_columns_list=['user_id', 'product_id'], \n                                                          agg_dict=agg_dict_4)\n\n    data = data.merge(prd, how='inner', on='product_id').merge(users, how='inner', on='user_id')\n    # ËØ•ÂïÜÂìÅË¥≠‰π∞Ê¨°Êï∞ / ÊÄªÁöÑËÆ¢ÂçïÊï∞\n    # ÊúÄËøë‰∏ÄÊ¨°Ë¥≠‰π∞ÂïÜÂìÅ - ÊúÄÂêé‰∏ÄÊ¨°Ë¥≠‰π∞ËØ•ÂïÜÂìÅ\n    # ËØ•ÂïÜÂìÅË¥≠‰π∞Ê¨°Êï∞ / Á¨¨‰∏ÄÊ¨°Ë¥≠‰π∞ËØ•ÂïÜÂìÅÂà∞ÊúÄÂêé‰∏ÄÊ¨°Ë¥≠‰π∞ÂïÜÂìÅÁöÑÁöÑËÆ¢ÂçïÊï∞\n    data['_up_order_rate'] = data._up_order_count / data._user_total_orders\n    data['_up_order_since_last_order'] = data._user_total_orders - data._up_last_order_number\n    data['_up_order_rate_since_first_order'] = data._up_order_count / (data._user_total_orders - data._up_first_order_number + 1)\n\n    # add user_id to train set\n    train = train.merge(right=orders[['order_id', 'user_id']], how='left', on='order_id')\n    data = data.merge(train[['user_id', 'product_id', 'reordered']], on=['user_id', 'product_id'], how='left')\n\n    # release Memory\n    # del train, prd, users\n    # gc.collect()\n    # release Memory\n    del priors_orders_detail, orders\n    gc.collect()\n\n    data.head()\n    \n    data.to_pickle('kernel38-data.pkl')","outputs":[],"cell_type":"code","execution_count":5},{"metadata":{"_uuid":"c3955fb765c87c57da68424f172a2999d75e11b4","collapsed":true},"source":"## Validation code","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"92ab05205445b1325b0dd08f64cc51d67fa0906c","collapsed":true},"source":"try:\n    df_train_gt = pd.read_csv('train.csv', index_col='order_id')\nexcept:\n    train_gtl = []\n\n    for uid, subset in train_details.groupby('user_id'):\n        subset1 = subset[subset.reordered == 1]\n        oid = subset.order_id.values[0]\n\n        if len(subset1) == 0:\n            train_gtl.append((oid, 'None'))\n            continue\n\n        ostr = ' '.join([str(int(e)) for e in subset1.product_id.values])\n        # .strip is needed because join can have a padding space at the end\n        train_gtl.append((oid, ostr.strip()))\n\n    df_train_gt = pd.DataFrame(train_gtl)\n\n    df_train_gt.columns = ['order_id', 'products']\n    df_train_gt.set_index('order_id', inplace=True)\n    df_train_gt.sort_index(inplace=True)\n    \n    df_train_gt.to_csv('train.csv')","outputs":[],"cell_type":"code","execution_count":6},{"metadata":{"_uuid":"71ecb0473d5fcce561d7552855a2d32b2625f0ae","collapsed":true},"source":"def compare_results(df_gt, df_preds):\n    \n    df_gt_cut = df_gt.loc[df_preds.index]\n    \n    f1 = []\n    for gt, pred in zip(df_gt_cut.sort_index().products, df_preds.sort_index().products):\n        lgt = gt.replace(\"None\", \"-1\").split(' ')\n        lpred = pred.replace(\"None\", \"-1\").split(' ')\n\n        rr = (np.intersect1d(lgt, lpred))\n        precision = np.float(len(rr)) / len(lpred)\n        recall = np.float(len(rr)) / len(lgt)\n\n        denom = precision + recall\n        f1.append(((2 * precision * recall) / denom) if denom > 0 else 0)\n\n    #print(np.mean(f1))\n    return(np.mean(f1))","outputs":[],"cell_type":"code","execution_count":7},{"metadata":{"_uuid":"81180a81e251ab065267a13fc9c62cd8c021b823","_cell_guid":"0f016091-8029-4c63-a029-2829720e8890","_execution_state":"idle"},"source":"# Create Train / Test","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"7c5299490df990b5ed8527fc3bfa317aae036f0d"},"source":"### Add aisle and dept id for catboost","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"1201e1e46aa2a71cf57438bd00631e34124f5a4a","collapsed":true},"source":"data = pd.merge(data, products.drop('product_name', axis=1), on='product_id')","outputs":[],"cell_type":"code","execution_count":8},{"metadata":{"_uuid":"1c5771a2164150de1d3e08fd42cef6e2cbee8f3d","collapsed":true},"source":"X_test = data.loc[data.eval_set == \"test\",:].copy()\nX_test.drop(['eval_set'], axis=1, inplace=True)","outputs":[],"cell_type":"code","execution_count":16},{"metadata":{"_uuid":"31d1a0325cf5cf6363b54c60008651edce792498","collapsed":true},"source":"train = data.loc[data.eval_set == \"train\",:].copy()\ntrain.drop(['eval_set'], axis=1, inplace=True)\ntrain.loc[:, 'reordered'] = train.reordered.fillna(0)","outputs":[],"cell_type":"code","execution_count":9},{"metadata":{"_uuid":"868453f8cac0d87ea2fd70ecdcb6206d5ed85013"},"source":"### perform CV for catboost ","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"60524db0c3b38956f172f684e3b6c90556f9c6a0","collapsed":true},"source":"def catboost_cv(X_train, y_train, X_val, y_val, features_to_use):\n    cb = catboost.CatBoostClassifier(iterations=100, \n        thread_count=8, \n        verbose=True,\n        random_seed=42,\n        learning_rate=0.05,\n        use_best_model=True,\n        depth=8,\n        fold_permutation_block_size=64,\n        calc_feature_importance=True,\n        leaf_estimation_method='Gradient')\n\n    cb.fit(X=X_train[features_to_use].values, \n            y=y_train.values, \n            #cat_features=[c.get_loc('aisle_id'), c.get_loc('department_id')], \n            cat_features=[X_train[features_to_use].columns.get_loc('aisle_id')], \n            eval_set=[X_val[features_to_use].values, y_val.values],\n            verbose=True,\n            #plot=True\n           )\n    \n    return cb","outputs":[],"cell_type":"code","execution_count":10},{"metadata":{"_uuid":"ff10c22c13205282969265b4bd16749de8678d7b"},"source":"df_cvfolds = []\ncb = []\n\nfor fold in range(4):\n    train_subset = train[train.user_id % 4 != fold]\n    valid_subset = train[train.user_id % 4 == fold]\n\n    X_train = train_subset.drop('reordered', axis=1)\n    y_train = train_subset.reordered\n\n    X_val = valid_subset.drop('reordered', axis=1)\n    y_val = valid_subset.reordered\n\n    val_index = X_val[['user_id', 'product_id', 'order_id']]\n    \n    features_to_use = list(X_train.columns)\n    features_to_use.remove('user_id')\n    features_to_use.remove('product_id')\n    features_to_use.remove('order_id')\n\n    cb.append(catboost_cv(X_train, y_train, X_val, y_val, features_to_use))\n    rawpreds = cb[-1].predict_proba(X_val[features_to_use].values)\n\n    lim = .202\n    val_out = val_index.copy()\n\n    val_out.loc[:,'reordered'] = (rawpreds[:,1] > lim).astype(int)\n    val_out.loc[:, 'product_id'] = val_out.product_id.astype(str)\n    presubmit = ka_add_groupby_features_n_vs_1(val_out[val_out.reordered == 1], \n                                                   group_columns_list=['order_id'],\n                                                   target_columns_list= ['product_id'],\n                                                   methods_list=[lambda x: ' '.join(set(x))], keep_only_stats=True)\n\n    presubmit = presubmit.set_index('order_id')\n    presubmit.columns = ['products']\n\n    fullfold = pd.DataFrame(index = val_out.order_id.unique())\n\n    fullfold.index.name = 'order_id'\n    fullfold['products'] = ['None'] * len(fullfold)\n\n    fullfold.loc[presubmit.index, 'products'] = presubmit.products\n\n    print(fold, compare_results(df_train_gt, fullfold))\n    \n    df_cvfolds.append(fullfold)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"0eeda4582c0a25f16c263da426967deecf84c968"},"source":"df_cv = pd.concat(df_cvfolds)\nprint(compare_results(df_train_gt, df_cv))","outputs":[],"cell_type":"code","execution_count":15},{"metadata":{"_uuid":"bbf190af6f51fa073d6ef9216124384605c491d0","collapsed":true},"source":"### now run model with 100% of train set for submission","outputs":[],"cell_type":"code","execution_count":42},{"metadata":{"_uuid":"c6d6710b1eaa03f7e332df1309a7ca4c81d96655"},"source":"cb_full = catboost.CatBoostClassifier(iterations=100, \n    thread_count=8, \n    verbose=True,\n    random_seed=42,\n    learning_rate=0.05,\n    depth=8,\n    fold_permutation_block_size=64,\n    calc_feature_importance=True,\n    leaf_estimation_method='Gradient')\n\ncb_full.fit(X=train[features_to_use].values, \n        y=train.reordered.values, \n        cat_features=[train[features_to_use].columns.get_loc('aisle_id')], \n        verbose=True,\n        #plot=True\n       )","outputs":[],"cell_type":"code","execution_count":18},{"metadata":{"_uuid":"8469e9605cc17b95d629f7609fdc8e266b44e5c8"},"source":"### Prepare submission","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"cc56dcff4d1d9737562635e1d2ffab3fc772e2d5"},"source":"testpreds = X_test[['user_id', 'product_id', 'order_id']].copy()\ntestpreds['reordered'] = (cb_full.predict_proba(X_test[features_to_use].values)[:,1] > .202).astype(int)\ntestpreds.product_id = testpreds.product_id.astype(str)","outputs":[],"cell_type":"code","execution_count":20},{"metadata":{"_uuid":"c095b7327c95abb5467c0f37d51f17aac3090fd9"},"source":"g = testpreds[testpreds.reordered == 1].groupby('order_id', sort=False)\ndf_testpreds = g[['product_id']].agg(lambda x: ' '.join(set(x)))\n\ndf_testpreds.head()","outputs":[],"cell_type":"code","execution_count":30},{"metadata":{"_uuid":"d7eef14db8a49b4c52d628f47d781448ce60cf0c","collapsed":true},"source":"# complete (but empty) test df\ndf_test = pd.DataFrame(index=X_test.order_id.unique())\ndf_test.index.name = 'order_id'\ndf_test['products'] = ['None'] * len(df_test)","outputs":[],"cell_type":"code","execution_count":34},{"metadata":{"_uuid":"6f14388373d43667641c913229d1a1162847b2c7"},"source":"# yup, there are ~3345 order_id's with no orders\nlen(df_test), len(df_testpreds)","outputs":[],"cell_type":"code","execution_count":35},{"metadata":{"_uuid":"4618b914ec2d97fab17f8e6e04e82c8db2d6235b"},"source":"# combine empty output df with predictions\ndf_test.loc[df_testpreds.index, 'products'] = df_testpreds.product_id\ndf_test.sort_index(inplace=True)\ndf_test.to_csv('preds_catboost.csv')","outputs":[],"cell_type":"code","execution_count":37}]}