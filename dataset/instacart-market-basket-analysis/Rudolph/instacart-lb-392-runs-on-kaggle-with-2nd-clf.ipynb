{"metadata":{"language_info":{"mimetype":"text/x-python","file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1"},"anaconda-cloud":{},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"_uuid":"77c844eccae1919e94ed779f429b5ec05988fbec","_cell_guid":"2661ec78-7389-4fbc-89b4-9c498a996dbe"},"cell_type":"markdown","source":"This LightGBM runs on Kaggle as well as on my 3-year old i5/8GB Surface laptop in less than 15 minutes to generate a submission. It has 19 features and produced LB 0.392 (22% when submitted earlier today). Part of it is similar to the many public kernels (such as Fabien Vavrand, Khaled Elshamouty, Paul-Antoine Nguyen, China, etc. These kernels have all in common a global single threshold of around 0.21 and they lead to F1 around 0.375 - 0.38 (perhaps corresponding to the peak in the distribution of LB score just at this level). What is different here is that this kernel runs on Kaggle in the allowed time of 20 minutes using 100% of the data. With a single threshold it also gives 0.38 but when this is relaxed and the calculation is repeated for several thresholds (e.g. 0.17, 0.21, 0.25) and then a second classifier is applied to train again on the \"train\" data to select the best F1 of the three thresholds for each order we get meaningful improvement to 0.386 and a jump of 130 places on LB. The new features for the second clf are the mean,max,min of the reorder probabilities for each of the three thresholds (chosen just for simplicity - std and len work too).  Next, browsing the discussions I realized that F1 can be further improved by stupidly adding 'None' to orders of one or two products. I did that and the LB score jumped again to 0.39 and finally I made it a bit smarter by adding this the extra 'None' only if the probability was less than 0.5 - and score went to 0.392.\n\nI started this exercise before I had a chance to look at the theoretical papers on multi-label classification such as Nan Ye at al, Optimizing F-measures, 2012 and Z. Lipton et al, Thresholding classifiers, 2014. I entered this competition too late to dig into it deeper. I was initially puzzled by the fact that maximum F1 gives a much bigger cart size (about 8 products instead of 6) - see the chart below. Since the deadline is almost here I implemented at least this partial optimization of F1 with three separate thresholds and the extra 'None'. This F1 optimization added 0.011. Is more possible by further optimizing F1 or is the next jump due to more and better features or better parameters and more time training lgbm? I am not using here Faron's kernel nor any of Sh1ng's code.\n\nThis was my first real Kaggle competition and I have learned great deal doing it - thank you all.\n"},{"metadata":{"_uuid":"444c60d5d818b00f785962efbf86d58fa180e17b","_cell_guid":"f17a99d0-84fe-4d92-862f-80d4bdf792cf"},"cell_type":"markdown","source":"![alt text](F1_vs_mean_cart_size.jpg \"F1 vs Mean Cart Size\")"},{"metadata":{"_uuid":"51f1abaa042e1a3359c23c64b13cf1a056e4b23d","collapsed":true,"_execution_state":"idle","_cell_guid":"ccbed9e2-0123-43ce-a11a-a8b45f54f972"},"source":"import pandas as pd\nimport numpy as np\nimport gc\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nmyfolder = '../input/'\nprint('loading files ...')\n\nprior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n\ntrain_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n\norders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n\norders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\norders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n\nproducts = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n            'aisle_id': np.uint8, 'department_id': np.uint8},\n             usecols=['product_id', 'aisle_id', 'department_id'])\n\nprint('done loading')","outputs":[],"cell_type":"code","execution_count":1},{"metadata":{"_uuid":"992b4372da57ac2c1be6be36566e4674625fe8a2","collapsed":true,"_execution_state":"idle","_cell_guid":"31625301-4e90-40b7-9321-46248a8f537c"},"source":"print('merge prior and orders and keep train separate ...')\n\norders_products = orders.merge(prior, how = 'inner', on = 'order_id')\ntrain_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n\ndel prior\ngc.collect()","outputs":[],"cell_type":"code","execution_count":2},{"metadata":{"_uuid":"113199012e64fe02159a90d7c1b3775881e6e38e","collapsed":true,"_execution_state":"idle","_cell_guid":"41fdb373-4908-4dcf-b76e-2e73f62184da"},"source":"print('Creating features I ...')\n\n# sort orders and products to get the rank or the reorder frequency\nprdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\nprdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n\n# getting products ordered first and second times to calculate probability later\nsub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\nsub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\nsub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\nsub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\nsub2 = sub2.reset_index().merge(sub1.reset_index())\nsub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\nsub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\nprd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n\ndel sub1, sub2, prdss\ngc.collect()","outputs":[],"cell_type":"code","execution_count":3},{"metadata":{"_uuid":"c9fe258c45125985e420324606142814fc52dff0","collapsed":true,"_execution_state":"idle","_cell_guid":"3474e4ca-925c-46ae-bb16-a045c407d53f"},"source":"print('Creating features II ...')\n\n# extracting prior information (features) by user\nusers = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\nusers['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\nusers['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n\n# merging features about users and orders into one dataset\nus = orders_products.groupby('user_id').size().to_frame('user_total_products')\nus['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\nus['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\nus['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\nus.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\nus['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n\n# the average basket size of the user\nusers = users.reset_index().merge(us.reset_index())\nusers['user_average_basket'] = users['user_total_products'] / users['user_orders']\n\nus = orders[orders['eval_set'] != 0]\nus = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\nusers = users.merge(us)\n\ndel us\ngc.collect()","outputs":[],"cell_type":"code","execution_count":4},{"metadata":{"_uuid":"127cf9bc24fd1ce21947eb847062d9b7994cf8ca","collapsed":true,"_execution_state":"idle","_cell_guid":"7b3de257-4691-403f-9cb0-eabf71cacf1c"},"source":"print('Finalizing features and the main data file  ...')\n# merging orders and products and grouping by user and product and calculating features for the user/product combination\ndata = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\ndata['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\ndata['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\ndata['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\ndata = data.reset_index()\n\n#merging previous data with users\ndata = data.merge(prd, on = 'product_id')\ndata = data.merge(users, on = 'user_id')\n\n#user/product combination features about the particular order\ndata['up_order_rate'] = data['up_orders'] / data['user_orders']\ndata['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\ndata = data.merge(train_orders[['user_id', 'product_id', 'reordered']], \n                  how = 'left', on = ['user_id', 'product_id'])\ndata = data.merge(products, on = 'product_id')\n\ndel orders_products     #, orders, train_orders\ngc.collect()","outputs":[],"cell_type":"code","execution_count":5},{"metadata":{"_uuid":"a0dc69b813021979eccedb074fed681089416d67","collapsed":true,"_execution_state":"idle","_cell_guid":"01ed0a5d-5e19-4be1-8771-4816a8195814"},"source":"print(' Training and test data for later use in F1 optimization and training  ...')\n\n#save the actual reordered products of the train set in a list format and then delete the original frames\ntrain_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\norders.set_index('order_id', drop=False, inplace=True)\ntrain1=orders[['order_id','eval_set']].loc[orders['eval_set']==1]\ntrain1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\ntrain1['actual']=train1['actual'].fillna('')\nn_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n\ntest1=orders[['order_id','eval_set']].loc[orders['eval_set']==2]\ntest1['actual']=' '\ntraintest1=pd.concat([train1,test1])\ntraintest1.set_index('order_id', drop=False, inplace=True)\n\ndel orders, train_orders, train1, test1\ngc.collect()","outputs":[],"cell_type":"code","execution_count":6},{"metadata":{"_uuid":"ad0fe98d8bc66267562b63bfad8fef4aa79c21fc","collapsed":true,"_execution_state":"idle","_cell_guid":"c8f32368-559b-4b41-928f-ca55b5013861"},"source":"print('setting dtypes for data ...')\n\n#reduce the size by setting data types\ndata = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16, \n            'prod_reorder_probability' : np.float16,   \n            'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16, \n            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n            'order_id'  : np.uint32, 'eval_set' : np.uint8, \n            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16, \n            'up_orders_since_last_order':np.uint8,\n            'aisle_id': np.uint8, 'department_id': np.uint8})\n\ndata['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered) \ndata['reordered']=data['reordered'].astype(np.uint8)\n\ngc.collect()","outputs":[],"cell_type":"code","execution_count":7},{"metadata":{"_uuid":"8d21528ff6376efb8979abea1561484c576b8f80","collapsed":true,"_execution_state":"idle","_cell_guid":"251c7f5a-fdc1-4150-a0c8-adbecc26c79f"},"source":"print('Preparing Train and Test sets ...')\n\n# filter by eval_set (train=1, test=2) and dropp the id's columns (not part of training features) \n# but keep prod_id and user_id in test\n\ntrain = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\ntest =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n\ncheck =  data.drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n\ndel data\ngc.collect()","outputs":[],"cell_type":"code","execution_count":8},{"metadata":{"_uuid":"d44a3b43cc110b439f7414a87eb9fc1b7a7ce0a9","collapsed":true,"_execution_state":"idle","_cell_guid":"2a3e8cb7-2e6a-4a2b-9971-882a229a8a35"},"source":"print('preparing X,y for LightGBM ...')\n\nX_train, X_eval, y_train, y_eval = train_test_split(\n    train[train.columns.difference(['reordered'])], train['reordered'], test_size=0.1, random_state=2)\n\ndel train\ngc.collect()","outputs":[],"cell_type":"code","execution_count":9},{"metadata":{"_uuid":"fb5a4e19edde2c9992bf554e724bd69b79adbd2e","collapsed":true,"_execution_state":"busy","_cell_guid":"d845200a-423a-48fd-8af6-74105d00e31b"},"source":"print('formatting and training LightGBM ...')\n\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_eval = lgb.Dataset(X_eval, y_eval, reference = lgb_train)\n\n# there is some room to change the parameters and improve - I have not done it systematically\n\nparams = {'task': 'train', 'boosting_type': 'gbdt',   'objective': 'binary', 'metric': {'binary_logloss', 'auc'},\n    'num_iterations' : 1000, 'max_bin' : 100, 'num_leaves': 512, 'feature_fraction': 0.8,  'bagging_fraction': 0.95,\n    'bagging_freq': 5, 'min_data_in_leaf' : 200, 'learning_rate' : 0.05}\n\n# set lower num_boost_round (I used 300 instead of 50 at home) to avoid time-out on Kaggle\n\nlgb_model = lgb.train(params, lgb_train, num_boost_round = 50, valid_sets = lgb_eval, early_stopping_rounds=10)\n\ndel lgb_train, X_train, y_train\ngc.collect()\n","outputs":[],"cell_type":"code","execution_count":10},{"metadata":{"_uuid":"c9bad61c861b6d703f87ce12b6e1b1274f52b05e","collapsed":true,"_execution_state":"busy","_cell_guid":"ed5c66f2-f623-4111-a41e-cfa5544fe1c4"},"source":"# Define an auxiliary function to combine the product data into orders\n\ndef combi(z,df):\n    \n    prd_bag = dict()\n    z_bag = dict()\n    for row in df.itertuples():\n        if row.reordered > z:   \n            try:\n                prd_bag[row.order_id] += ' ' + str(row.product_id)\n                z_bag[row.order_id]+= ' ' + str(int(100*row.reordered))\n            except:\n                prd_bag[row.order_id] = str(row.product_id)\n                z_bag[row.order_id]= str(int(100*row.reordered))\n\n    for order in df.order_id:\n        if order not in prd_bag:\n            prd_bag[order] = ' '\n            z_bag[order] = ' '\n\n    return prd_bag,z_bag \n\n# F1 function uses the actual products as a list in the train set and the list of predicted products\n\ndef f1_score_single(x):                 #from LiLi but modified to get 1 for both empty\n\n    y_true = x.actual\n    y_pred = x.list_prod\n    if y_true == '' and y_pred ==[] : return 1.\n    y_true = set(y_true)\n    y_pred = set(y_pred)\n    cross_size = len(y_true & y_pred)\n    if cross_size == 0: return 0.\n    p = 1. * cross_size / len(y_pred)\n    r = 1. * cross_size / len(y_true)\n    return 2 * p * r / (p + r)","outputs":[],"cell_type":"code","execution_count":12},{"metadata":{"_uuid":"11afdc8b7091466843ded4cb7e153297c8f973c2","collapsed":true,"_execution_state":"busy","_cell_guid":"fd989c59-efae-4154-88b9-926bc1dfc497"},"source":"# check feature importance\n#lgb.plot_importance(lgb_model, figsize=(7,9))\n#plt.show()","outputs":[],"cell_type":"code","execution_count":13},{"metadata":{"_uuid":"0d774f84dae196179477cd4387a012ad89bcc050","collapsed":true,"_execution_state":"busy","_cell_guid":"136f4d2e-637a-42e1-8a8d-9cb1d79d302e"},"source":"print(' Applying model to all data - both train and test ')\n\n\ncheck['reordered'] = lgb_model.predict(check[check.columns.difference(\n    ['order_id', 'product_id'])], num_iteration = lgb_model.best_iteration)\n\ngc.collect()","outputs":[],"cell_type":"code","execution_count":14},{"metadata":{"_uuid":"4c33bccfd6fa21c17475047eb29947dbca72067c","_cell_guid":"032ae9fc-5081-4e0b-952a-1d813b49da38"},"cell_type":"markdown","source":"The next step is to chose a threshold to select the reordered products. We know that a single global threshold of 0.21 maximizes F1 (given that the max is about 0.4 - see also the chart at the begining or end). The method here is to use several (eg 3 such as 0.17,0.21,0.25) thresholds and recalculate F1 on the training set. In addition to the products for each order we will collect the probabilities and store them in list and calculate the mean, max and min as our new features. Then we set up a new classification problem with the target [0,1, or 2] corresponding to which threshold maximizes F1. Alltogether we will use 9 features (we could easily generalize to more thresholds and features) .  This is relatively easy problem and we use Gradient Boosting Classifier with default parameters. "},{"metadata":{"_uuid":"1f0ba053d504c0cd18c10314ced0e8c6885eed02","collapsed":true,"_execution_state":"busy","_cell_guid":"3fb1bf3a-a6f6-446d-9e4b-b4a4a7a837b9"},"source":"print(' summarizing products and probabilities ...')\n\n# get the prediction for a range of thresholds\n\ntt=traintest1.copy()\ni=0\n\nfor z in [0.17, 0.21, 0.25]:\n    \n    prd_bag,z_bag = combi(z,check)\n    ptemp = pd.DataFrame.from_dict(prd_bag, orient='index')\n    ptemp.reset_index(inplace=True)\n    ztemp = pd.DataFrame.from_dict(z_bag, orient='index')\n    ztemp.reset_index(inplace=True)\n    ptemp.columns = ['order_id', 'products']\n    ztemp.columns = ['order_id', 'zs']\n    ptemp['list_prod'] = ptemp['products'].apply(lambda x: list(map(int, x.split())))\n    ztemp['list_z'] = ztemp['zs'].apply(lambda x: list(map(int, x.split())))\n    n_cart = ptemp['products'].apply(lambda x: len(x.split())).mean()\n    tt = tt.merge(ptemp,on='order_id',how='inner')\n    tt = tt.merge(ztemp,on='order_id',how='inner')\n    tt.drop(['products','zs'],axis=1,inplace=True)\n    tt['zavg'] = tt['list_z'].apply(lambda x: 0.01*np.mean(x) if x!=[] else 0.).astype(np.float16)\n    tt['zmax'] = tt['list_z'].apply(lambda x: 0.01*np.max(x) if x!=[] else 0.).astype(np.float16)\n    tt['zmin'] = tt['list_z'].apply(lambda x: 0.01*np.min(x) if x!=[] else 0.).astype(np.float16)\n    tt['f1']=tt.apply(f1_score_single,axis=1).astype(np.float16)\n    F1 = tt['f1'].loc[tt['eval_set']==1].mean()\n    tt = tt.rename(columns={'list_prod': 'prod'+str(i), 'f1': 'f1'+str(i), 'list_z': 'z'+str(i),\n                'zavg': 'zavg'+str(i), 'zmax': 'zmax'+str(i),  'zmin': 'zmin'+str(i)})\n    print(' z,F1,n_actual,n_cart :  ', z,F1,n_actual,n_cart)\n    i=i+1\n\ntt['fm'] = tt[['f10', 'f11', 'f12']].idxmax(axis=1)\ntt['f1'] = tt[['f10', 'f11', 'f12']].max(axis=1)\ntt['fm'] = tt.fm.replace({'f10': 0,'f11': 1, 'f12':2}).astype(np.uint8)\nprint(' f1 maximized ', tt['f1'].loc[tt['eval_set']==1].mean())\n    \ndel prd_bag, z_bag, ptemp, ztemp\ngc.collect()","outputs":[],"cell_type":"code","execution_count":15},{"metadata":{"_uuid":"20eb1bd022084ad8bb47a804235bb6748c85ba77","collapsed":true,"_execution_state":"busy","_cell_guid":"7e7bc66d-cae7-48a9-8dbf-11261c97b314"},"source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\n\nprint('Fitting the second classifier for F1 ...')\n\nX=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==1]\ny=tt['fm'].loc[tt['eval_set']==1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nclf = GradientBoostingClassifier().fit(X_train, y_train)\nprint('GB Accuracy on training set: {:.2f}' .format(clf.score(X_train, y_train)))\nprint('Accuracy on test set: {:.2f}' .format(clf.score(X_test, y_test)))\n#pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=[\"Importance\"]).plot(kind='bar')\n#plt.show()\n\nfinal=tt[['order_id','prod0','prod1','prod2','zavg0']].loc[tt['eval_set']==2]\ndf_test=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==2]\nfinal['fit']= clf.predict(df_test)\nfinal['best'] = final.apply(lambda row: row['prod0'] if row['fit']==0 else \n                                 ( row['prod1'] if row['fit']==1 else  row['prod2'] )  , axis=1)\n\n#final['products']=final['best'].apply(lambda x: ' '.join(str(i) for i in x) if x!=[] else 'None')\n\n# I am adding 'None' to orders with one or two products because of the bias in F1\n\ndef mylist(x):\n    prodids = x.best\n    zavg = x.zavg0\n    if prodids == []: return 'None'            \n    if zavg < 0.5:\n        if len(prodids) == 1: return  str(prodids[0])+' None'\n        if len(prodids) == 2: return  str(prodids[0])+ ' '+ str(prodids[1]) +' None'\n    return ' '.join(str(i) for i in prodids)\n\nfinal['products']=final.apply(mylist,axis=1)\n\nfinal[['order_id','products']].to_csv('final_submission1.csv', index=False)  \n\ngc.collect()","outputs":[],"cell_type":"code","execution_count":17},{"metadata":{"_uuid":"f0bb42db622546a490110bec9a2fc6e84553a3ec","collapsed":true,"_execution_state":"busy","_cell_guid":"c4900de5-b35e-4d1b-8048-88c8c74bb617"},"source":"#I saved one of the previous runs so that it is not timed out on Kaggle\nX=np.arange(0.12,0.31,0.01)\nY2 = np.empty(19)\nY2.fill(6.31)\nY1=[ 0.3701,0.3757,0.38,0.3839,0.3867,0.3886,0.3897,0.3905,0.3906,0.3903,\n    0.3892,0.3877,0.3857,0.3834,0.3808,0.3779,0.3746,0.371,0.3669]\nY3=[ 15.45,14.29,13.26,12.34,11.51,10.76,10.09,9.47,8.91,8.39,7.92,7.49,\n    7.08,6.7,6.35,6.03,5.72,5.43,5.16]\n#replace X,Y1,Y2,Y3 with arrays from z,F1,n_actual,n_cart to update (running the above cell for the corresponding rane of z's)\n\nplt.clf()\nfig = plt.figure()\nax = fig.add_subplot(111)\nlns1 = ax.plot(X, Y2, '-', label = 'Actual')\nlns2 = ax.plot(X, Y3, '-', label = 'Predicted')\nax2 = ax.twinx()\nlns3 = ax2.plot(X, Y1, '-r', label = 'F1')\nlns = lns1+lns2+lns3\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc=0)\nax.set_xlabel('Threshold')\nax.set_ylabel('Mean Cart Size')\nax2.set_ylabel('F1')\nplt.suptitle('F1 vs Mean Cart Size', size=12)\nplt.savefig('F1_vs_mean_cart_size.jpg')\nplt.show()","outputs":[],"cell_type":"code","execution_count":18},{"metadata":{"_uuid":"50fcb107a13c25c0f4aadb978248772d4fca5c49","collapsed":true,"_execution_state":"busy","_cell_guid":"e6959399-22e8-468b-9815-42cccbf1c74c"},"source":"","outputs":[],"cell_type":"code","execution_count":null}],"nbformat_minor":1,"nbformat":4}