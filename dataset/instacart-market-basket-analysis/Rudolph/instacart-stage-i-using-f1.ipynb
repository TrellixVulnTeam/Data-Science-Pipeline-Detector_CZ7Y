{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"anaconda-cloud":{},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"metadata":{"_uuid":"678dfa423c0ff51e06a59676ae26908c95aa2fe4","_cell_guid":"94a8aa2c-1a68-4698-a9fb-ce25c6c46561"},"outputs":[],"cell_type":"markdown","source":"Goal: predict which previously purchased products will be in a userâ€™s next order. Specifically, for each order_id in the test set, predict a space-delimited list of product_ids for that order. \n\nOrders file tells to which set (prior, train, test) an order belongs to without giving the details of the products ordered. For any given user_id the last order is flagged as either train or test while all previous orders are marked as prior. The prior file has the prior orders for both test and train set while the train file has the details of the last order. All together we have 3,421,083 order_ids for 206,209 user_ids:\n\nprior (32,434,449 products) -> train user (131,209 user_ids) -> train (1,384,617 products)\n\nprior (32,434,449 products) -> test user  (75,000 user_ids)  -> to forecast (??? products)\n\nFor the 75,000 test user_ids there are also 75,000 order_ids that we need to include in the final answer. The \"prior\" pandas frame is 216 MB, orders is 88 MB, and train is 8MB. Fortunately one can reduce the size by merging and aggregating the data (I am working with 5 MB in all_data, 6 MB in train, and 2MB in test). \n\nWe will see that the train file (our target or y) has on average 6 reordered products per user (with standard dev 6 and full range of 0 to 71 - this is after we exclude the new products - otherwise the average would be 11 with a range of 1 to 80). The data in the prior set (our X) has on a average 8 products per user and order (on average 17 orders). Our task is to predict the 6 products in the basket from a set of 65 products (on average for each user) purchseded in the past (even though there are some 49,000 products total).\n\nI am not using the rest of the files (at first disregarding the train users as well). The prediction (at first) is to take (for each test user) all their past orders and select \"n\" of the most common products where \"n\" is the average number of products in past  orders.  The  program takes just a few  minutes to run  on Kaggle. The score it gives is 0.329 - clearly not good enough (about 1500 out of 2000 when first run) so we need to work on it harder and use this just as a start. \n\nBefore using any Machine Learning algorithm we need to understand what is the measure of sucess and so in the second part of the notebook we look at the train set and calculate F1 for the same prediction. I am concerned about the use of F1. For example if a the true order is [1,2,3,4,5,6] and we predict 2 correctly (1 in 3) , ie [1,2,7,8,9,10] then the F1 is 0.33 (and the same precision and recall). The leader board best score is 0.4 which is less than 3 correct (for the total of 6). The F1 score does not penalize for predicting incorrect number of products, for example if we add to the cart [11,12,13,14,3] , ie just 1 correct in 5 we improve the score to 0.35. To get to 0.4 one would need to add [11,12,3] or again 1 in 3 correct.  The precision will remain the same and only recall will increase (see the code at the end). The narrow task is to use F1 but is this really relevant to the business?\n\n","execution_count":null},{"metadata":{"_execution_state":"idle","trusted":true,"_uuid":"56fa2780e886c642c1e4f468b7aa2803f03c2123","collapsed":true,"_cell_guid":"5f57559a-eeb7-4b9e-bc62-d78a456e3f2a"},"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter\n\nmyfolder = '../input/'\nprior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16}).drop(['add_to_cart_order', 'reordered'], axis=1)\norders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n           'days_since_prior_order': np.float16}).drop(['order_dow','order_hour_of_day'], axis=1)\norders.set_index('order_id', drop=False, inplace=True)","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":true,"_uuid":"0edbc6be18bba072e5c9c65f0ffe18a33dbb0fce","collapsed":true,"_cell_guid":"8262487d-c536-4b70-bb03-670581838353"},"execution_count":null,"cell_type":"code","source":"#This might take a minute - adding the past products to the orders frame\n\norders['prod_list'] = prior.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\norders=orders.fillna('')\norders['num_items'] = orders['prod_list'].apply(len).astype(np.uint8)\n    ","outputs":[]},{"metadata":{"scrolled":true,"_execution_state":"idle","trusted":true,"_uuid":"46fa0b5904eb0b9ff5698c71d0bb3012fcb3ef7c","collapsed":true,"_cell_guid":"f8d8f7e0-af85-4afb-b1e6-856e9fc14291"},"execution_count":null,"cell_type":"code","source":"#aggregate again by creating a list of list of all products in all orders for each user\n\nall_products = orders.groupby('user_id').aggregate({'prod_list':lambda x: list(x)})\nall_products['mean_items']= orders.groupby('user_id').aggregate({'num_items':lambda x: np.mean(x)}).astype(np.uint8)\nall_products['max_items']= orders.groupby('user_id').aggregate({'num_items':lambda x: np.max(x)}).astype(np.uint8)\nall_products['user_id']=all_products.index\n","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":true,"_uuid":"8d5110a758a564bd7f68b45cad32b7c28b23122a","_cell_guid":"bd805faa-2784-4e2f-ac30-9ce4cfc26bff"},"execution_count":null,"cell_type":"code","source":"# This function flattens the list of list (of product_ids), then finds the most common elements in it\n# and joins them into the required format for the test set only\n\ndef myfrequent(x):\n    prodids = x.prod_list\n    n=x.mean_items\n    C=Counter( [elem for sublist in prodids for elem in sublist] ).most_common(n)\n    return ' '.join(str(C[i][0]) for i in range(0,n))  \n\ntest=orders[['order_id','user_id']].loc[orders['eval_set']=='test']\ntest=test.merge(all_products,on='user_id')\ntest['products']=test.apply(myfrequent,axis=1)\ntest[['order_id','products']].to_csv('mean_submission0.csv', index=False)  \ntest.head(3)","outputs":[]},{"metadata":{"_uuid":"815a062a31be18a6f12684bd89eddbdc52c922fc","_cell_guid":"a2083798-49c3-462a-ac56-fa22c9c6efdc"},"outputs":[],"cell_type":"markdown","source":"The score from LB is 0.329 and to understand it better we look at the train set:\n    ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"26ac2cb88d0c4da3eb3886957b1f2cf2bf7c4e93","collapsed":true,"_cell_guid":"418e0e83-9b40-4c7d-90e3-c84dd5651a9f"},"execution_count":null,"cell_type":"code","source":"train=orders[['order_id','user_id']].loc[orders['eval_set']=='train']\ntrain_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16, 'reordered': np.int8}).drop(['add_to_cart_order'], axis=1)\ntrain_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)  # predicting for reordered only\ntrain['true'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\ntrain['true']=train['true'].fillna('')\ntrain['true_n'] = train['true'].fillna('').apply(len).astype(np.uint8)\ntrain=train.merge(all_products,on='user_id')\ntrain['prod_list']=train['prod_list'].map(lambda x: [elem for sublist in x for elem in sublist])\n","outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad20a115160ccb82e3c6c28643e19c893479d87","_cell_guid":"561a51b5-39b0-4b78-b151-e98cf950457e"},"execution_count":null,"cell_type":"code","source":"def myfrequent2(x):     # select the n most common elements from the prod_list\n    prodids = x.prod_list\n    n=x.mean_items\n    C=Counter(prodids).most_common(n)\n    return list((C[i][0]) for i in range(0,n))  \n\ndef f1_score_single(x):    #copied from LiLi\n    y_true = set(x.true)\n    y_pred = set(x.prediction)\n    cross_size = len(y_true & y_pred)\n    if cross_size == 0: return 0.\n    p = 1. * cross_size / len(y_pred)\n    r = 1. * cross_size / len(y_true)\n    return 2 * p * r / (p + r)\n\ntrain['prediction']=train.apply(myfrequent2,axis=1)\ntrain['f1']=train.apply(f1_score_single,axis=1).astype(np.float16)\nprint('The F1 score on the traing set is  {0:.3f}.'.format(  train['f1'].mean()  ))\ntrain.head(3)","outputs":[]},{"metadata":{"_uuid":"2c18e77a84d65636243a55017ac1ee9adf7fb132"},"outputs":[],"cell_type":"markdown","source":"Look at the example below how one can get F1 from 0.33 to 0.4 without increasing precision just by increasing the number of products in the basket.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"00dd2596d993d26514dc2be9e74f529f50854e7e","_cell_guid":"c09e24fd-dbe4-40e0-aab1-8feee94f2a7e"},"execution_count":null,"cell_type":"code","source":"\ndef f1(y_true,y_pred):    \n    y_true = set(y_true)\n    y_pred = set(y_pred)\n    cross_size = len(y_true & y_pred)\n    if cross_size == 0: return 0.\n    p = 1. * cross_size / len(y_pred)\n    r = 1. * cross_size / len(y_true)\n    return 2 * p * r / (p + r)\n\ny_true=[1,2,3,4,5,6]\ny_pred=[1,2,7,8,9,10]\nprint (' True, Pred, F1:   ',y_true,y_pred,f1(y_true, y_pred))\ny_pred.extend([11,12,3])\nprint (' True, Pred, F1:   ',y_true,y_pred,f1(y_true, y_pred))\ny_pred=[1,2,3,8,9,10]\nprint (' True, Pred, F1:   ',y_true,y_pred,f1(y_true, y_pred))\n","outputs":[]},{"metadata":{"_uuid":"cb815f8b9741e2e221f47783b5068a26fe57dda3"},"outputs":[],"cell_type":"markdown","source":"Seeing this and the fact that highest LB score is about 0.4 I have to ask if that is due to the increase in recall only and not precision - by increasing the number of products in an order.  I checked a couple of other simple public kernels with similar approach and I noticed that they achieve higher LB score at the cost of having on average 12 products in an order instead of 6. ","execution_count":null},{"metadata":{"_uuid":"f2cc523c45a7a8b9a6e1aec48c9c964def01d17a","_cell_guid":"a1fe8005-7690-4e4f-b5d6-a25c322c526d"},"outputs":[],"cell_type":"markdown","source":"","execution_count":null}],"nbformat_minor":1,"nbformat":4}