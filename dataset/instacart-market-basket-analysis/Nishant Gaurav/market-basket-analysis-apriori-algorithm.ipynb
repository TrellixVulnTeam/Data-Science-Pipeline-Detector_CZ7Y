{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-07T12:25:44.334231Z","iopub.execute_input":"2022-04-07T12:25:44.334754Z","iopub.status.idle":"2022-04-07T12:25:44.373249Z","shell.execute_reply.started":"2022-04-07T12:25:44.334616Z","shell.execute_reply":"2022-04-07T12:25:44.372537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import required packages","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy\nimport os\nimport zipfile\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:25:44.394877Z","iopub.execute_input":"2022-04-07T12:25:44.395265Z","iopub.status.idle":"2022-04-07T12:25:45.648667Z","shell.execute_reply.started":"2022-04-07T12:25:44.395222Z","shell.execute_reply":"2022-04-07T12:25:45.647448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Stage 1:** Data Wrangling\n\nWe have five different files:\n\n`\n- orders.csv\n- order_products_train.csv\n- products.csv\n- aisles.csv\n- departments.csv\n`\n\nThese files contain the necessary data to solve the problem .Load all the files correctly, after observing the header level details, data records etc\n\n**Hint:** read_csv from pandas","metadata":{}},{"cell_type":"markdown","source":"### Load the data\nLoad all the given datasets","metadata":{}},{"cell_type":"code","source":"# extract and read files\nroot = \"/kaggle/input/instacart-market-basket-analysis/\"\nfolder = os.listdir(root)\nmissing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n/a\", \"na\", \"--\",\"-\"]\n\nfor file in folder:\n    if file.endswith('.zip'):\n            with zipfile.ZipFile(root+file, 'r') as zip_ref:\n                zip_ref.extractall(\"kaggle/working/\")\n\ndata = {}\nfor file in os.listdir(\"kaggle/working/\"):\n    if file.endswith('.csv'):\n        data[file[:file.find('.')]] = pd.read_csv(\"kaggle/working/\"+file, na_values = missing_value_formats)\n        \ndata.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:25:45.650753Z","iopub.execute_input":"2022-04-07T12:25:45.651541Z","iopub.status.idle":"2022-04-07T12:26:07.708018Z","shell.execute_reply.started":"2022-04-07T12:25:45.651489Z","shell.execute_reply":"2022-04-07T12:26:07.706797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/working/kaggle/working'):\n    for filename in filenames:\n        if filename.startswith('__'):\n            pass\n        else:\n            print(os.path.join(dirname, filename))\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:30:49.392544Z","iopub.execute_input":"2022-04-07T12:30:49.392915Z","iopub.status.idle":"2022-04-07T12:30:49.403371Z","shell.execute_reply.started":"2022-04-07T12:30:49.39288Z","shell.execute_reply":"2022-04-07T12:30:49.402349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#root = '/kaggle/input/instacart-market-basket-analysis/'\nfor file in os.listdir(\"/kaggle/working/kaggle/working/\"):\n    \n    root1 = \"/kaggle/working/kaggle/working/\"\n    if file == '__MACOSX':\n        pass\n    elif (file == 'order_products__prior.csv' or file == 'sample_submission.csv'):\n        pass\n    else:\n        orders = pd.read_csv(root1 + 'orders.csv')\n        order_products_train = pd.read_csv(root1 + 'order_products__train.csv')\n        products = pd.read_csv(root1 + 'products.csv')\n        aisles = pd.read_csv(root1 + 'aisles.csv')\n        departments = pd.read_csv(root1 + 'departments.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:35:11.837506Z","iopub.execute_input":"2022-04-07T12:35:11.838117Z","iopub.status.idle":"2022-04-07T12:35:24.909291Z","shell.execute_reply.started":"2022-04-07T12:35:11.838066Z","shell.execute_reply":"2022-04-07T12:35:24.908381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = {}\n\nfor i in os.listdir('/kaggle/working/kaggle/working/'):\n    if i == '__MACOSX':\n        pass\n    elif (i == 'order_products__prior.csv' or i == 'sample_submission.csv'):\n        pass\n    else:\n        print(i)\n        datasets[i] = pd.read_csv(\"/kaggle/working/kaggle/working/\"+i)\n    \ndatasets = dict(sorted(datasets.items()))\ndatasets.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:37:09.756144Z","iopub.execute_input":"2022-04-07T12:37:09.75683Z","iopub.status.idle":"2022-04-07T12:37:12.935459Z","shell.execute_reply.started":"2022-04-07T12:37:09.756784Z","shell.execute_reply":"2022-04-07T12:37:12.934381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = list(datasets.keys())\nnames","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:37:19.514463Z","iopub.execute_input":"2022-04-07T12:37:19.514786Z","iopub.status.idle":"2022-04-07T12:37:19.521195Z","shell.execute_reply.started":"2022-04-07T12:37:19.514751Z","shell.execute_reply":"2022-04-07T12:37:19.520243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Integration\n\nAs the required data is present in different files, we need to integrate all the five to make single dataframe/dataset. For that purpose, use the unique identifier provided in all the dataframes so that it can be used to map the data in different files correctly.\n\n**Example:** `product_id` is available in both `products` dataframe and `order_products_train` dataframe, we can merge these two into a single dataframe based on `product_id`\n\n**Hint:** [pd.merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)","metadata":{}},{"cell_type":"code","source":"df1 = datasets[names[2]]\ndf1.columns, df1.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:47.003415Z","iopub.execute_input":"2022-04-07T12:43:47.004665Z","iopub.status.idle":"2022-04-07T12:43:47.013897Z","shell.execute_reply.started":"2022-04-07T12:43:47.004611Z","shell.execute_reply":"2022-04-07T12:43:47.013285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df1.merge(datasets[names[-1]], on='product_id')\ndf2.columns, df2.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:47.182733Z","iopub.execute_input":"2022-04-07T12:43:47.183597Z","iopub.status.idle":"2022-04-07T12:43:47.49814Z","shell.execute_reply.started":"2022-04-07T12:43:47.18355Z","shell.execute_reply":"2022-04-07T12:43:47.497287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df2.merge(datasets[names[0]], on='aisle_id')\ndf3.columns, df3.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:47.499911Z","iopub.execute_input":"2022-04-07T12:43:47.500191Z","iopub.status.idle":"2022-04-07T12:43:47.710079Z","shell.execute_reply.started":"2022-04-07T12:43:47.500158Z","shell.execute_reply":"2022-04-07T12:43:47.709161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = df3.merge(datasets[names[3]], on='order_id')\ndf4.columns, df4.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:47.711602Z","iopub.execute_input":"2022-04-07T12:43:47.712361Z","iopub.status.idle":"2022-04-07T12:43:50.216075Z","shell.execute_reply.started":"2022-04-07T12:43:47.712298Z","shell.execute_reply":"2022-04-07T12:43:50.215219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5 = df4.merge(datasets[names[1]], on='department_id')\ndf5.columns, df5.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:50.2182Z","iopub.execute_input":"2022-04-07T12:43:50.218449Z","iopub.status.idle":"2022-04-07T12:43:50.700194Z","shell.execute_reply.started":"2022-04-07T12:43:50.218419Z","shell.execute_reply":"2022-04-07T12:43:50.699206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:50.703428Z","iopub.execute_input":"2022-04-07T12:43:50.703698Z","iopub.status.idle":"2022-04-07T12:43:50.722186Z","shell.execute_reply.started":"2022-04-07T12:43:50.703664Z","shell.execute_reply":"2022-04-07T12:43:50.721412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = df5","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:50.723701Z","iopub.execute_input":"2022-04-07T12:43:50.724568Z","iopub.status.idle":"2022-04-07T12:43:50.732961Z","shell.execute_reply.started":"2022-04-07T12:43:50.724518Z","shell.execute_reply":"2022-04-07T12:43:50.732043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Understanding relationship and new insights from the data\n1. How many times was each product ordered?\n    **Hint:** group orders by product\n2. Find the numbers of orders per department and visualize using an appropriate plot.\n\n3. On which day of the week do customers tend to buy more groceries? Which are the peak hours of shopping?\n\n    * Find the frequency of orders on week days  using an appropriate plot\n    * Find the frwquency of orders during hours of the day using and appropriate plot?\n    \n4. Find the ratio of Re-ordered and Not Re-ordered products and visualize it\n \n5. Plot the heatmap of Re-order ratio of the Day of week vs Hour of day","metadata":{}},{"cell_type":"markdown","source":"### Group orders by products and get how many times each product was ordered","metadata":{}},{"cell_type":"code","source":"g = final_df.product_id.value_counts()\ng = pd.DataFrame(g)\ng.reset_index(inplace=True)\ng.columns = [\"product_id\", \"count\"]\ng_products = g.merge(datasets['products.csv'], on=\"product_id\")\ng_products.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:50.734521Z","iopub.execute_input":"2022-04-07T12:43:50.73543Z","iopub.status.idle":"2022-04-07T12:43:50.806955Z","shell.execute_reply.started":"2022-04-07T12:43:50.735379Z","shell.execute_reply":"2022-04-07T12:43:50.806358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(50, 40))\ng_products.head(20).plot(kind=\"bar\", x=\"product_name\", y=\"count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:50.807866Z","iopub.execute_input":"2022-04-07T12:43:50.80863Z","iopub.status.idle":"2022-04-07T12:43:51.241464Z","shell.execute_reply.started":"2022-04-07T12:43:50.808583Z","shell.execute_reply":"2022-04-07T12:43:51.240552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(Banana is top ordered product)","metadata":{}},{"cell_type":"markdown","source":"### Find the number of orders per department\nHint: Groupby","metadata":{}},{"cell_type":"code","source":"g = final_df.department_id.value_counts()\ng = pd.DataFrame(g)\ng.reset_index(inplace=True)\ng.columns = [\"department_id\", \"count\"]\ng_dept = g.merge(datasets['departments.csv'], on=\"department_id\")\ng_dept.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:51.242791Z","iopub.execute_input":"2022-04-07T12:43:51.243036Z","iopub.status.idle":"2022-04-07T12:43:51.270162Z","shell.execute_reply.started":"2022-04-07T12:43:51.243004Z","shell.execute_reply":"2022-04-07T12:43:51.269103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_dept.plot(kind=\"bar\", x=\"department\", y=\"count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:51.272852Z","iopub.execute_input":"2022-04-07T12:43:51.27318Z","iopub.status.idle":"2022-04-07T12:43:51.607503Z","shell.execute_reply.started":"2022-04-07T12:43:51.273144Z","shell.execute_reply":"2022-04-07T12:43:51.606856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the frequency of orders on week days","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x='order_dow', data=final_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by week day\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:51.608901Z","iopub.execute_input":"2022-04-07T12:43:51.609157Z","iopub.status.idle":"2022-04-07T12:43:51.985342Z","shell.execute_reply.started":"2022-04-07T12:43:51.609114Z","shell.execute_reply":"2022-04-07T12:43:51.984338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the frequency of orders for hours of the day","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"order_hour_of_day\", data=final_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Hour of day', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by hour of day\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:51.988558Z","iopub.execute_input":"2022-04-07T12:43:51.988816Z","iopub.status.idle":"2022-04-07T12:43:52.43884Z","shell.execute_reply.started":"2022-04-07T12:43:51.988767Z","shell.execute_reply":"2022-04-07T12:43:52.437913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the ratio of Re-ordered and Not Re-ordered product and visualize","metadata":{}},{"cell_type":"code","source":"final_df[final_df['reordered'] == 1]['product_id'].max() #nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:52.440153Z","iopub.execute_input":"2022-04-07T12:43:52.440463Z","iopub.status.idle":"2022-04-07T12:43:52.827379Z","shell.execute_reply.started":"2022-04-07T12:43:52.44042Z","shell.execute_reply":"2022-04-07T12:43:52.826566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(final_df[final_df['reordered'] == 1]['product_id'], kde=False, label='Reordered')\nsns.distplot(final_df[final_df['reordered'] == 0]['product_id'], kde=False, label='Not reordered')\n\nplt.legend(prop={'size': 12})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:52.828706Z","iopub.execute_input":"2022-04-07T12:43:52.828981Z","iopub.status.idle":"2022-04-07T12:43:53.560141Z","shell.execute_reply.started":"2022-04-07T12:43:52.82895Z","shell.execute_reply":"2022-04-07T12:43:53.559258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the heatmap of Re-ordered ratio of week vs Hour of day?","metadata":{}},{"cell_type":"code","source":"grouped_df = final_df.groupby(['order_dow', 'order_hour_of_day'])['reordered'].aggregate(\"mean\").reset_index()\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(grouped_df, annot=True)\nplt.title(\"Reorder ratio of Day of week vs Hour of day\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:53.561535Z","iopub.execute_input":"2022-04-07T12:43:53.562451Z","iopub.status.idle":"2022-04-07T12:43:55.003088Z","shell.execute_reply.started":"2022-04-07T12:43:53.562406Z","shell.execute_reply":"2022-04-07T12:43:55.00245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stage 2: Create a basket\n\nAs the dataset contains huge amout of data, let us take a subset of the data to extract the association rules from it.\n\n**Assumptions:** Segment the data by considering the 100 most frequent ordered items. Please note it is just an assumption. You can consider 'n frequent order items as per your choice.\n\n**Hint:**\n\n* Drop the unwanted columns\n\n* Find the frequencies of orders based on the products and consider 100 most frequent order items\n\n    **Hint:** Count the frequencies of orders for each product_id using `groupby()` and `count()` respectively\n    \n * Extract the records of 100 most frequent items(which are extracted in previous step) from combined dataframe.\n \n * Create a Pivot table with `order_id` as index and `product_name` as columns and `reorder` as values.\n \n     * set the `order_id` as index using set_index()\n     * fill all the nan values with 0\n     \n  * After performing the above step, tehre are a lot of zeros in the data, make sure that any positive values are converted to a 1 and anything less than 0 is set to 0.","metadata":{}},{"cell_type":"code","source":"product_counts = final_df.groupby('product_id')['order_id'].count().reset_index().rename(columns = {'order_id':'frequency'})\nproduct_counts = product_counts.sort_values('frequency', ascending=False)[0:100].reset_index(drop=True)\nproduct_counts.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:55.004116Z","iopub.execute_input":"2022-04-07T12:43:55.004834Z","iopub.status.idle":"2022-04-07T12:43:55.067059Z","shell.execute_reply.started":"2022-04-07T12:43:55.004793Z","shell.execute_reply":"2022-04-07T12:43:55.066274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_products = list(product_counts.product_id)\nfreq_products[1:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:55.068452Z","iopub.execute_input":"2022-04-07T12:43:55.068721Z","iopub.status.idle":"2022-04-07T12:43:55.07457Z","shell.execute_reply.started":"2022-04-07T12:43:55.068691Z","shell.execute_reply":"2022-04-07T12:43:55.073881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_products = final_df[final_df.product_id.isin(freq_products)]\norder_products.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:55.07555Z","iopub.execute_input":"2022-04-07T12:43:55.075767Z","iopub.status.idle":"2022-04-07T12:43:55.154943Z","shell.execute_reply.started":"2022-04-07T12:43:55.075714Z","shell.execute_reply":"2022-04-07T12:43:55.154353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basket = order_products.pivot_table(columns='product_name', values='reordered', index='order_id').reset_index().fillna(0).set_index('order_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:55.156136Z","iopub.execute_input":"2022-04-07T12:43:55.15686Z","iopub.status.idle":"2022-04-07T12:43:55.740329Z","shell.execute_reply.started":"2022-04-07T12:43:55.156823Z","shell.execute_reply":"2022-04-07T12:43:55.739562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n    \nbasket = basket.applymap(encode_units)\nbasket.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:43:55.741767Z","iopub.execute_input":"2022-04-07T12:43:55.742192Z","iopub.status.idle":"2022-04-07T12:44:02.010497Z","shell.execute_reply.started":"2022-04-07T12:43:55.742144Z","shell.execute_reply":"2022-04-07T12:44:02.009605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Stage 3:** Apply Apriori algorithm\n\n* As the dataset contains huge amount of data, let us take a subset of the data to extract the association rules from it.\n\n    **Assumptions:** Segment the basket by considering 100000 record. Please note its just an assumption, you can consider 'n' records as per choice.\n    \n     **Hint:** [apriori](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/)\n\n- Find the association rules and make a dataframe","metadata":{}},{"cell_type":"code","source":"shortbasket = basket[:100000]","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:44:02.011987Z","iopub.execute_input":"2022-04-07T12:44:02.012222Z","iopub.status.idle":"2022-04-07T12:44:02.016076Z","shell.execute_reply.started":"2022-04-07T12:44:02.012193Z","shell.execute_reply":"2022-04-07T12:44:02.015175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequent_items = apriori(shortbasket, min_support=0.01, use_colnames=True)\nfrequent_items.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:44:02.017672Z","iopub.execute_input":"2022-04-07T12:44:02.017935Z","iopub.status.idle":"2022-04-07T12:44:07.656003Z","shell.execute_reply.started":"2022-04-07T12:44:02.017906Z","shell.execute_reply":"2022-04-07T12:44:07.65538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rules = association_rules(frequent_items, metric='lift', min_threshold=1)\nrules.sort_values('lift', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T12:44:07.657089Z","iopub.execute_input":"2022-04-07T12:44:07.657731Z","iopub.status.idle":"2022-04-07T12:44:07.710513Z","shell.execute_reply.started":"2022-04-07T12:44:07.657695Z","shell.execute_reply":"2022-04-07T12:44:07.709613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}