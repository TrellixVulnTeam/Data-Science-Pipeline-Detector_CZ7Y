{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/fugue-project/fugue/master/images/logo.svg\" align=\"left\" width=\"500\"/>","metadata":{}},{"cell_type":"markdown","source":"# About this notebook\n\nThis notebook is a demonstration of FugueSQL prepared for Thinkful Data Analyst Bootcamp students. **FugueSQL is a language that allows SQL Users to use in-memory data frameworks such Pandas, Spark, and Dask with a SQL interface**. It has some differences from standard SQL that will be shows here. \n\nFugueSQL aims to be more English-like, and provide a fun interface for Data Analysts to work with data in their tool of choice. The FugueSQL notebook extension allows users to use FugueSQL with syntax highlighting in Jupyter notebook cells\n\nFugue also has a programming interface that is not covered in this notebook. The programming interface is not covered here, but the link to the repo and Slack channels are listed below if anyone is interested.","metadata":{}},{"cell_type":"markdown","source":"## Links \n\nFugue is a pure abstraction layer that makes code portable across differing computing frameworks such as Pandas, Spark and Dask. It allows users to write code compatible across all 3 frameworks. It guarantees consistency regardless of scale and a unified framework for compute. All questions are welcome in the Slack channel.\n\n[Fugue Repo](https://github.com/fugue-project/fugue)\n\n[Fugue Slack](https://join.slack.com/t/fugue-project/shared_invite/zt-jl0pcahu-KdlSOgi~fP50TZWmNxdWYQ)","metadata":{}},{"cell_type":"markdown","source":"## Credits\n\nA lot of the plots and EDA here is based off this notebook: https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-instacart","metadata":{}},{"cell_type":"markdown","source":"## Installation\n\nNote that this installation is optimized for Kaggle notebooks. `fuggle` is a library for Fugue on Kaggle notebooks. Installating fugue for use outside Kaggle notebooks should just be `pip install fugue`. Join the Slack (listed above) if there are any questions.","metadata":{}},{"cell_type":"code","source":"%pip install fuggle","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-04T05:26:00.317879Z","iopub.execute_input":"2021-11-04T05:26:00.318617Z","iopub.status.idle":"2021-11-04T05:27:46.747848Z","shell.execute_reply.started":"2021-11-04T05:26:00.318506Z","shell.execute_reply":"2021-11-04T05:27:46.746255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This provides syntax highlighting for Fugue SQL cells and allows us to use the %%fsql magic.","metadata":{}},{"cell_type":"code","source":"from fuggle import setup\nsetup()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:27:46.751071Z","iopub.execute_input":"2021-11-04T05:27:46.751454Z","iopub.status.idle":"2021-11-04T05:27:49.481224Z","shell.execute_reply.started":"2021-11-04T05:27:46.751408Z","shell.execute_reply":"2021-11-04T05:27:49.480124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:27:49.482771Z","iopub.execute_input":"2021-11-04T05:27:49.483049Z","iopub.status.idle":"2021-11-04T05:27:49.571865Z","shell.execute_reply.started":"2021-11-04T05:27:49.483022Z","shell.execute_reply":"2021-11-04T05:27:49.57074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"This particular dataset comes in zipfiles so we'll preprocess and unzip to read with pandas/FugueSQL.","metadata":{}},{"cell_type":"code","source":"import zipfile\nfile_list = [\n    '/kaggle/input/instacart-market-basket-analysis/aisles.csv.zip',\n    '/kaggle/input/instacart-market-basket-analysis/orders.csv.zip',\n    '/kaggle/input/instacart-market-basket-analysis/sample_submission.csv.zip',\n    '/kaggle/input/instacart-market-basket-analysis/order_products__train.csv.zip',\n    '/kaggle/input/instacart-market-basket-analysis/products.csv.zip',  \n    '/kaggle/input/instacart-market-basket-analysis/order_products__prior.csv.zip',    \n    '/kaggle/input/instacart-market-basket-analysis/departments.csv.zip']\n\nfor file_name in file_list:\n    with zipfile.ZipFile(file=file_name) as target_zip:\n        target_zip.extractall()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:27:49.573067Z","iopub.execute_input":"2021-11-04T05:27:49.57339Z","iopub.status.idle":"2021-11-04T05:28:00.424112Z","shell.execute_reply.started":"2021-11-04T05:27:49.57336Z","shell.execute_reply":"2021-11-04T05:28:00.423153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick Experiments","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame({\"date\": ['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05'],\n                   \"val\": [1,2,3,4,5],\n                   \"val2\": [\"a\",\"b\",\"c\",\"a\",\"b\"]})\ndf['date'] = pd.to_datetime(df['date'])","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:00.428637Z","iopub.execute_input":"2021-11-04T05:28:00.428995Z","iopub.status.idle":"2021-11-04T05:28:00.462396Z","shell.execute_reply.started":"2021-11-04T05:28:00.428962Z","shell.execute_reply":"2021-11-04T05:28:00.461048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nSELECT *\n FROM df\nWHERE date < \"2020-01-03\"\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:00.466731Z","iopub.execute_input":"2021-11-04T05:28:00.467448Z","iopub.status.idle":"2021-11-04T05:28:01.495567Z","shell.execute_reply.started":"2021-11-04T05:28:00.467377Z","shell.execute_reply":"2021-11-04T05:28:01.494511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nSELECT date, val,\nCASE\n    WHEN val = 1 THEN 'The quantity is 1'\n    WHEN val = 2 THEN 'The quantity is 2'\n    ELSE 'The quantity is greater than 3'\nEND AS valText\nFROM df\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:01.497171Z","iopub.execute_input":"2021-11-04T05:28:01.497656Z","iopub.status.idle":"2021-11-04T05:28:02.384322Z","shell.execute_reply.started":"2021-11-04T05:28:01.49761Z","shell.execute_reply":"2021-11-04T05:28:02.383516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nSELECT date, val, val2\nFROM df\nWHERE val2 IN (\"a\",\"b\")\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:02.385712Z","iopub.execute_input":"2021-11-04T05:28:02.386304Z","iopub.status.idle":"2021-11-04T05:28:02.918005Z","shell.execute_reply.started":"2021-11-04T05:28:02.386258Z","shell.execute_reply":"2021-11-04T05:28:02.916771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mytuple = (\"a\", \"b\")","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:02.919246Z","iopub.execute_input":"2021-11-04T05:28:02.919586Z","iopub.status.idle":"2021-11-04T05:28:02.952805Z","shell.execute_reply.started":"2021-11-04T05:28:02.919558Z","shell.execute_reply":"2021-11-04T05:28:02.951721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nSELECT date, val, val2\nFROM df\nWHERE val2 IN {{mytuple}}\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:02.95439Z","iopub.execute_input":"2021-11-04T05:28:02.954827Z","iopub.status.idle":"2021-11-04T05:28:03.369984Z","shell.execute_reply.started":"2021-11-04T05:28:02.954785Z","shell.execute_reply":"2021-11-04T05:28:03.36914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FugueSQL Syntax\n\nBefore using FugueSQL for data analysis, we'll go over some quick examples on the syntax. These will be put together for more complciated operations later. These will show the enhancements over standard SQL","metadata":{}},{"cell_type":"markdown","source":"## Load and Save\n\nFugueSQL allows users to load from csv/json/parquet files using Pandas, Spark and Dask under the hood. This means we can load in data, perform transformations on it, and then write out the results. This allows data analysts to work with data not in a database.","metadata":{}},{"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE, infer_schema=TRUE)\n\nSELECT * FROM df\nWHERE aisle_id = 3\nPRINT\nSAVE OVERWRITE \"/kaggle/working/aisles-modified.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:03.371352Z","iopub.execute_input":"2021-11-04T05:28:03.371779Z","iopub.status.idle":"2021-11-04T05:28:03.980594Z","shell.execute_reply.started":"2021-11-04T05:28:03.371742Z","shell.execute_reply":"2021-11-04T05:28:03.979653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice the variable assignment during the `LOAD` statement. Variable assignment is not limited to `LOAD` opearations. it can also be used during the `SELECT` statement to create intermediate tables. All ANSI SQL keywords are available in FugueSQL","metadata":{}},{"cell_type":"markdown","source":"## Groupby and Filtering","metadata":{}},{"cell_type":"raw","source":"Here we perform a simple groupby and filter. The syntax is exactly like stadard SQL. There is just a quick `PRINT` statement to show the data we're working with.","metadata":{}},{"cell_type":"code","source":"%%fsql\nproducts = LOAD \"/kaggle/working/products.csv\" (header=TRUE, infer_schema=TRUE)\nPRINT 5 ROWS\n\n  SELECT department_id, COUNT(*) AS count\n    FROM products\n   WHERE department_id < 6\nGROUP BY department_id\n   PRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:03.982386Z","iopub.execute_input":"2021-11-04T05:28:03.982899Z","iopub.status.idle":"2021-11-04T05:28:04.870982Z","shell.execute_reply.started":"2021-11-04T05:28:03.982852Z","shell.execute_reply":"2021-11-04T05:28:04.870013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before moving on to other FugueSQL commands, this is a good place to show what the equivalent Panadas syntax would be for the same opearation. Note that `loc` is used to filter. We need to take care of resetting the index, and the renaming of the column is more verbose. In general, SQL is easier to read for some operations.","metadata":{}},{"cell_type":"code","source":"# Pandas implementation of previous\nproducts = pd.read_csv(\"/kaggle/working/products.csv\")\nproducts['department_id'] = products['department_id'].astype(int)\n\nproducts = products[['department_id']]\\\n    .loc[products['department_id'] < 6]\\\n    .value_counts()\\\n    .reset_index()\\\n    .rename(columns={0:'count'})\\\n\nproducts.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:04.872694Z","iopub.execute_input":"2021-11-04T05:28:04.873019Z","iopub.status.idle":"2021-11-04T05:28:04.969394Z","shell.execute_reply.started":"2021-11-04T05:28:04.872987Z","shell.execute_reply":"2021-11-04T05:28:04.968519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More important though, the Python code is very coupled with the Pandas framework. If the size of data becomes to big, we'd have to move to another framework like Spark or Dask. This Pandas-written code will no longer be applicable. In order to show this, we'll implement the same code in Spark to show how different the syntax is.","metadata":{}},{"cell_type":"code","source":"# Spark implementation of previous\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as sf\n\nspark = SparkSession.builder.getOrCreate()\nproducts = spark.read.format(\"csv\").load(\"/kaggle/working/products.csv\", header = True)\n\nproducts = products.where(\"department_id < 6\")\\\n    .groupBy(\"department_id\")\\\n    .agg(sf.count(sf.lit(1)).alias(\"count\"))\\\n\nproducts.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:04.97086Z","iopub.execute_input":"2021-11-04T05:28:04.971227Z","iopub.status.idle":"2021-11-04T05:28:18.94809Z","shell.execute_reply.started":"2021-11-04T05:28:04.971192Z","shell.execute_reply":"2021-11-04T05:28:18.946727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Case for SQL as Grammar for Logic**\n\nThis is the motivation for Fugue. Can we decouple the expression of our logic from the framework we are using? Fugue achieves this by letting users specity the execution during runtime. We can run the same SQL code on Pandas, Spark, or Dask by simple changing one line of code to define our execution engine. This provides more robust code that is agnostic to the volume of the data we're operating on.\n\nIt is important to note that Fugue also has a Python abstraction layer similar to this SQL abstraction layer. They can also work together. We'll see hints of this later.","metadata":{}},{"cell_type":"markdown","source":"## Defining Schema for a DataFrame","metadata":{}},{"cell_type":"markdown","source":"Note that if we don't infer the schema, Pandas loads most columns as strings. We can use `ALTER COLUMNS` to change the syntax. For DataFrames with a large number of columns, we recommend using infer_schema and then `ALTER COLUMNS` to ensure the correct types.","metadata":{}},{"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE)\nPRINT 1 ROW\ndf = ALTER COLUMNS aisle_id:int, aisle:str FROM df\nPRINT 1 ROW","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:18.949849Z","iopub.execute_input":"2021-11-04T05:28:18.95031Z","iopub.status.idle":"2021-11-04T05:28:19.46161Z","shell.execute_reply.started":"2021-11-04T05:28:18.950262Z","shell.execute_reply":"2021-11-04T05:28:19.460484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, schema can be explicitly defined while loading in the CSV.","metadata":{}},{"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE) COLUMNS aisle_id:int, aisle:str\nPRINT 1 ROWS","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:19.463291Z","iopub.execute_input":"2021-11-04T05:28:19.463708Z","iopub.status.idle":"2021-11-04T05:28:19.687394Z","shell.execute_reply.started":"2021-11-04T05:28:19.463674Z","shell.execute_reply":"2021-11-04T05:28:19.686537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passing DataFrames to fsql Cells\n\nFugueSQL allows for Python interoperatibility. DataFrames defined outside `%%fsql` cells can be used. In this example, we create a test DataFrame and use it inside a following FugueSQL code block.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/working/aisles.csv\")\ntest['new_col'] = 1\ntest.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:19.688583Z","iopub.execute_input":"2021-11-04T05:28:19.688892Z","iopub.status.idle":"2021-11-04T05:28:19.704398Z","shell.execute_reply.started":"2021-11-04T05:28:19.688829Z","shell.execute_reply":"2021-11-04T05:28:19.703543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nSELECT *\n  FROM test\n PRINT 5 ROWS","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:19.705652Z","iopub.execute_input":"2021-11-04T05:28:19.706126Z","iopub.status.idle":"2021-11-04T05:28:19.908439Z","shell.execute_reply.started":"2021-11-04T05:28:19.706075Z","shell.execute_reply":"2021-11-04T05:28:19.907317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passing DataFrames out of fsql Cells\n\nDataFrames defined in fsql cells can be in following cells or in native Python by using `YIELD DATAFRAME`. This holds the DataFrame in memory.  ","metadata":{}},{"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE)\nSELECT * \nFROM df\nWHERE aisle_id = '3'\nYIELD DATAFRAME AS result\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:19.909684Z","iopub.execute_input":"2021-11-04T05:28:19.909977Z","iopub.status.idle":"2021-11-04T05:28:20.322256Z","shell.execute_reply.started":"2021-11-04T05:28:19.90995Z","shell.execute_reply":"2021-11-04T05:28:20.321179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing dataframe from previous step\nprint(result.as_pandas().head())","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:20.323744Z","iopub.execute_input":"2021-11-04T05:28:20.324198Z","iopub.status.idle":"2021-11-04T05:28:20.331939Z","shell.execute_reply.started":"2021-11-04T05:28:20.324153Z","shell.execute_reply":"2021-11-04T05:28:20.331073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\n-- This is available because of the previous YIELD\nSELECT * FROM result\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:20.333321Z","iopub.execute_input":"2021-11-04T05:28:20.333755Z","iopub.status.idle":"2021-11-04T05:28:20.517127Z","shell.execute_reply.started":"2021-11-04T05:28:20.333717Z","shell.execute_reply":"2021-11-04T05:28:20.516022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Jinja Templating\n\nSometime a Python variable will be needed inside a SQL block. Think of dynamic lists used to filter values in a DataFrame. In this case, Jinja templating can be used to pass a variable inside a fsql code block.","metadata":{}},{"cell_type":"code","source":"# This is a Python code block\ncheese_aisle = 'specialty cheeses'","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:20.518951Z","iopub.execute_input":"2021-11-04T05:28:20.519735Z","iopub.status.idle":"2021-11-04T05:28:20.524818Z","shell.execute_reply.started":"2021-11-04T05:28:20.519686Z","shell.execute_reply":"2021-11-04T05:28:20.523566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE)\n\nSELECT *\nFROM df WHERE aisle = '{{cheese_aisle}}'\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:20.526353Z","iopub.execute_input":"2021-11-04T05:28:20.527075Z","iopub.status.idle":"2021-11-04T05:28:20.888932Z","shell.execute_reply.started":"2021-11-04T05:28:20.527017Z","shell.execute_reply":"2021-11-04T05:28:20.887856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Anonymity and Inline\n\nAnonymity is when the dataframe to perform the operation on is not specified. As a default, the output of the last operation will be used. This is a FugueSQL feature designed to simplify code. `PRINT` is an example of this. ","metadata":{}},{"cell_type":"code","source":"%%fsql\ndf = SELECT * FROM (LOAD \"/kaggle/working/products.csv\" (header=TRUE))\nALTER COLUMNS product_id:int, product_name:str, aisle_id:int, department_id:int\nPRINT 5 ROWS","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:20.890728Z","iopub.execute_input":"2021-11-04T05:28:20.891226Z","iopub.status.idle":"2021-11-04T05:28:21.542718Z","shell.execute_reply.started":"2021-11-04T05:28:20.891179Z","shell.execute_reply":"2021-11-04T05:28:21.54157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"# Some plotting utility functions. These will be used in conjunction with SQL later\ncolor = sns.color_palette()\n\ndef dow_countplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.countplot(df['order_dow'], color=color[0])\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('Day of week', fontsize=12)\n    plt.title(\"Frequency of order by week day\", fontsize=15)\n    plt.show()\n    \ndef hour_countplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.countplot(df['order_hour_of_day'], color=color[1])\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('Hour of Day', fontsize=12)\n    plt.title(\"Frequency of order by hour of day\", fontsize=15)\n    plt.show()\n    \ndef max_order_barplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.barplot(df['n_orders'], df['count'], alpha=0.8, color=color[2])\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel('Maximum order number', fontsize=12)\n    plt.title(\"Frequency of maximum order numbers\", fontsize=15)\n    plt.xticks(rotation='vertical')\n    plt.show()\n    \ndef days_since_prior_countplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.countplot(df['days_since_prior_order'], color=color[3])\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('Days since prior order', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.title(\"Frequency distribution by days since prior order\", fontsize=15)\n    plt.show()\n\ndef top_products_barplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.barplot(df['product_name'], df['count'], alpha=0.8, color=color[4])\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel('Product Name', fontsize=12)\n    plt.title(\"Frequency of product orders (top 20)\", fontsize=15)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.show()\n    \ndef top_aisles_barplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(12,8))\n    sns.barplot(df['aisle'], df['count'], alpha=0.8, color=color[5])\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel('Aisle Name', fontsize=12)\n    plt.title(\"Number of Occurances of each aisle\", fontsize=15)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.show()\n    \ndef department_pieplot(df:pd.DataFrame) -> None:\n    plt.figure(figsize=(10,10))\n    temp_series = df['department'].value_counts()\n    labels = (np.array(temp_series.index))\n    sizes = (np.array((temp_series / temp_series.sum())*100))\n    plt.pie(sizes, labels=labels, \n            autopct='%1.1f%%', startangle=200)\n    plt.title(\"Departments distribution\", fontsize=15)\n    plt.show()\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:21.544146Z","iopub.execute_input":"2021-11-04T05:28:21.544803Z","iopub.status.idle":"2021-11-04T05:28:21.567786Z","shell.execute_reply.started":"2021-11-04T05:28:21.544757Z","shell.execute_reply":"2021-11-04T05:28:21.566613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Orders and Order Products Table","metadata":{}},{"cell_type":"code","source":"%%fsql\nSELECT * FROM (LOAD \"/kaggle/working/orders.csv\" (header=TRUE)\n               COLUMNS order_id:int,user_id:int, eval_set:str, order_number:int, order_dow:int, order_hour_of_day:int, days_since_prior_order:double)\nYIELD FILE AS orders\nPRINT 10 ROWS\n\norder_products = SELECT order_id, product_id, reordered\n                 FROM (LOAD \"/kaggle/working/order_products__prior.csv\" (header=true, infer_schema=true))\norder_products = ALTER COLUMNS reordered:int\nYIELD FILE AS order_products\nPRINT 10 ROWS","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:21.569296Z","iopub.execute_input":"2021-11-04T05:28:21.56966Z","iopub.status.idle":"2021-11-04T05:28:55.582179Z","shell.execute_reply.started":"2021-11-04T05:28:21.569626Z","shell.execute_reply":"2021-11-04T05:28:55.581368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rowcount of order_products\n\nThis is the largest table and we get the row count here to understand the volume of data we are dealing with. For larger datasets, users should consider using Spark and Dask as the backend to FugueSQL. The Kaggle kernel is a 4-core machine also, but Pandas runs on 1 core by default.\n\nUsing Spark or Dask allows us to parallelize the operations performed on the data.","metadata":{}},{"cell_type":"markdown","source":"The order_products table is appromiximately 32 million rows. Operations on this stretches the limits of Pandas.","metadata":{}},{"cell_type":"code","source":"%%fsql\n-- This can be used because of YIELD FILE\nSELECT COUNT(*) AS count\n  FROM orders \n PRINT\n \nSELECT COUNT(*) AS count\n  FROM order_products\n PRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:55.583842Z","iopub.execute_input":"2021-11-04T05:28:55.584205Z","iopub.status.idle":"2021-11-04T05:28:58.098617Z","shell.execute_reply.started":"2021-11-04T05:28:55.584169Z","shell.execute_reply":"2021-11-04T05:28:58.097692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Value Count\n\nThis is an example of an opearation that is a lot more verbose to write in SQL. We can achieve the same thing by using a Python function and one line of Pandas code.","metadata":{}},{"cell_type":"code","source":"%%fsql\nSELECT COUNT(*) - COUNT(order_id) AS order_id,\n    COUNT(*) - COUNT(user_id) AS user_id,\n    COUNT(*) - COUNT(eval_set) AS eval_set,\n    COUNT(*) - COUNT(order_number) AS order_number,\n    COUNT(*) - COUNT(order_dow) AS order_dow,\n    COUNT(*) - COUNT(order_hour_of_day) AS order_hour_of_day,\n    COUNT(*) - COUNT(days_since_prior_order) AS days_since_prior_order\nFROM orders\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:28:58.100058Z","iopub.execute_input":"2021-11-04T05:28:58.100446Z","iopub.status.idle":"2021-11-04T05:29:00.993689Z","shell.execute_reply.started":"2021-11-04T05:28:58.100402Z","shell.execute_reply":"2021-11-04T05:29:00.992551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#schema: *\ndef null_count(df:pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame(df.isnull().sum(axis = 0)).T","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:00.994857Z","iopub.execute_input":"2021-11-04T05:29:00.995139Z","iopub.status.idle":"2021-11-04T05:29:01.000256Z","shell.execute_reply.started":"2021-11-04T05:29:00.995112Z","shell.execute_reply":"2021-11-04T05:29:00.999407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nTRANSFORM orders USING null_count\nPRINT\n\nTRANSFORM order_products USING null_count\nPRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:01.001698Z","iopub.execute_input":"2021-11-04T05:29:01.002007Z","iopub.status.idle":"2021-11-04T05:29:03.318053Z","shell.execute_reply.started":"2021-11-04T05:29:01.001976Z","shell.execute_reply":"2021-11-04T05:29:03.3169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction to Distributed Computing\n\n","metadata":{}},{"cell_type":"markdown","source":"Before we go into further analysis, we'll first give a brief introduction to distributed computing.\n\nThere is an image in the Dask repo [issues](https://github.com/dask/dask/issues/4471) that clearly illustrates the distributed computing paradigm. In general, there is a client or master that takes care of the orchestration and final data collection. The client is responsible for scheduling tasks among workers.\n\nBoth Spark and Dask have local modes also where they use the cores available on the local machine. This means we can still take advantage of the additional processing without having a cluster available.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://user-images.githubusercontent.com/11656932/62263986-bbba2f00-b3e3-11e9-9b5c-8446ba4efcf9.png\" align=\"left\" width=\"700\"/>","metadata":{}},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"code","source":"%%fsql\ntempdf = SELECT user_id, MAX(order_number) AS n_orders\nFROM orders\nGROUP BY user_id\nPRINT 2 ROWS\n\nSELECT n_orders, COUNT(n_orders) AS count\nFROM tempdf\nGROUP BY n_orders\nOUTPUT USING max_order_barplot","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:03.319459Z","iopub.execute_input":"2021-11-04T05:29:03.319949Z","iopub.status.idle":"2021-11-04T05:29:32.787888Z","shell.execute_reply.started":"2021-11-04T05:29:03.319905Z","shell.execute_reply":"2021-11-04T05:29:32.786928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\n-- Frequency of orders by day of week\ndf = SELECT * FROM orders\nOUTPUT USING dow_countplot\nOUTPUT USING hour_countplot\nOUTPUT USING days_since_prior_countplot","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:32.793608Z","iopub.execute_input":"2021-11-04T05:29:32.793939Z","iopub.status.idle":"2021-11-04T05:29:35.883461Z","shell.execute_reply.started":"2021-11-04T05:29:32.79391Z","shell.execute_reply":"2021-11-04T05:29:35.882526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Percentage of product orders that are reorders\n\nHere we check how many individual product orders are reorders. This tells us how many times users are buying new products that they have not ordered before.","metadata":{}},{"cell_type":"code","source":"%%fsql\ntempdf = SELECT COUNT(*) AS total,\n                SUM(CASE WHEN reordered = 1 THEN 1 ELSE 0 END) AS reordered\n           FROM order_products\n           \nSELECT reordered / total * 100 AS percent_reordered\n  FROM tempdf\n PRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:35.885793Z","iopub.execute_input":"2021-11-04T05:29:35.886267Z","iopub.status.idle":"2021-11-04T05:29:41.057963Z","shell.execute_reply.started":"2021-11-04T05:29:35.886215Z","shell.execute_reply":"2021-11-04T05:29:41.057026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Orders with no re-ordered products\n\nThese are the situations where either the customer is buying products for the first time or they are buying an entirely new set of products. We only have data for the second order of a user onwards so we don't need to filter by the order_number.\n\nWe can do this by aggregating on the order_id and getting the MAX of the reordered columns. The average of the resulting binary column will be the percent with reorders. 1 minus this value will be the percentage without reorders.","metadata":{}},{"cell_type":"code","source":"%%fsql\ntempdf = SELECT order_id, MAX(reordered) AS contains_reorder\n           FROM order_products\n       GROUP BY order_id\n\nSELECT 100 - AVG(contains_reorder) * 100 AS pct_w_no_reorder\n  FROM tempdf\n PRINT","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:29:41.059697Z","iopub.execute_input":"2021-11-04T05:29:41.060331Z","iopub.status.idle":"2021-11-04T05:36:21.392374Z","shell.execute_reply.started":"2021-11-04T05:29:41.060282Z","shell.execute_reply":"2021-11-04T05:36:21.391305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other Tables","metadata":{}},{"cell_type":"code","source":"%%fsql\nproducts = LOAD \"/kaggle/working/products.csv\" (header=TRUE, infer_schema=TRUE) YIELD FILE\nPRINT 5 ROWS\n\naisles = LOAD \"/kaggle/working/aisles.csv\" (header=TRUE, infer_schema=TRUE) YIELD DATAFRAME\nPRINT 5 ROWS\n\ndepartments = LOAD \"/kaggle/working/departments.csv\" (header=TRUE, infer_schema=TRUE) YIELD DATAFRAME\nPRINT 5 ROWS","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:36:21.393917Z","iopub.execute_input":"2021-11-04T05:36:21.394207Z","iopub.status.idle":"2021-11-04T05:36:22.117356Z","shell.execute_reply.started":"2021-11-04T05:36:21.39418Z","shell.execute_reply":"2021-11-04T05:36:22.116516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dask for Handle Memory Spillover","metadata":{}},{"cell_type":"markdown","source":"The code snippet below had a lot of memory issues because we are joining all of the tables to the order_products table, which has 32 million rows. This will need some clever optimization to pull off in Pandas (converting dtypes or filtering columns before join). With Dask though, we can perform the join, have the operation spill over to disk, and then get the smaller result set.\n\nPandas needs 3x more RAM than the size of the data to run effectively. This means Dask will probably help your workflows way earlier than you expect. Dask handles writing to disk when it hits around 60-70% of utilization by default. This keeps Pandas operating effectively.","metadata":{}},{"cell_type":"code","source":"%%fsql dask\n-- Memory issues but dask solves it\nresult = SELECT order_id, aisle, product_name, department, reordered\n           FROM order_products\n     INNER JOIN products\n             ON order_products.product_id = products.product_id\n     INNER JOIN aisles\n             ON products.aisle_id = aisles.aisle_id\n     INNER JOIN departments\n             ON departments.department_id = products.department_id\n     SAVE OVERWRITE \"/kaggle/working/result.parquet\"","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:36:22.118369Z","iopub.execute_input":"2021-11-04T05:36:22.118853Z","iopub.status.idle":"2021-11-04T05:38:38.32746Z","shell.execute_reply.started":"2021-11-04T05:36:22.118816Z","shell.execute_reply":"2021-11-04T05:38:38.326309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is also the first place we observe how to change the execution engine in FugueSQL. All we have to do is specify it after the `%%fsql` cell magic. The corresponding SQL code will then run on that engine. If there is a DataFrame that is available through `YIELD` , then it will have to be converted (under the hood).","metadata":{}},{"cell_type":"markdown","source":"## Parquet versus CSV\n\nThe previous operation was saved in a parquet file. Parquet is one of the most common file formats for distributed computing. There are a couple of advantages over CSVs. \n\n* Column based versus row based\n* Compression (70% reduction in size)\n* Optimization with Spark\n* Schema\n* Partition friendly","metadata":{}},{"cell_type":"markdown","source":"## Top 20 Products","metadata":{}},{"cell_type":"code","source":"%%fsql\nresult = LOAD \"/kaggle/working/result.parquet\"\n\n-- Top products\n  SELECT product_name, COUNT(*) AS count\n    FROM result\nGROUP BY product_name\nORDER BY count DESC\n   LIMIT 20\n  OUTPUT USING top_products_barplot\n\n-- Top aisles \n  SELECT aisle, COUNT(*) AS count\n    FROM result\nGROUP BY aisle\nORDER BY count DESC\n   LIMIT 20\n  OUTPUT USING top_aisles_barplot\n \n-- Department pieplot\nOUTPUT result USING department_pieplot\n ","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:38:38.329446Z","iopub.execute_input":"2021-11-04T05:38:38.329868Z","iopub.status.idle":"2021-11-04T05:39:35.412564Z","shell.execute_reply.started":"2021-11-04T05:38:38.32982Z","shell.execute_reply":"2021-11-04T05:39:35.411568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introductions to Partitions\n\nIn order to understand partitions, we can look at this image showing the way Dask scales Pandas. Each partition is a Pandas DataFrame. A Dask DataFrame is the collection of all of the Pandas DataFrames. Operations are done on each partition, and then aggregated back.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://docs.dask.org/en/latest/_images/dask-dataframe.svg\" align=\"left\" width=\"400\"/>","metadata":{}},{"cell_type":"markdown","source":"## [Reference on Partitions](https://blog.scottlogic.com/2018/03/22/apache-spark-performance.html) by Scott Logic\n\nThis reference has a lot of good images and explanations","metadata":{}},{"cell_type":"markdown","source":"### Ideal Partitioning Strategy\n![Partitioning](https://blog.scottlogic.com/mdebeneducci/assets/Ideal-Partitioning.png)\n### Skewed Partitions\n![Skewed Partitions](https://blog.scottlogic.com/mdebeneducci/assets/Skewed-Partitions.png)\n### Inefficient Scheduling\n![Inefficient Scheduling](https://blog.scottlogic.com/mdebeneducci/assets/Inefficient-Scheduling.png)\n### Data Shuffling\n![Shuffle](https://blog.scottlogic.com/mdebeneducci/assets/Shuffle-Diagram.png)","metadata":{}},{"cell_type":"markdown","source":"## Median Basket Size for Each Customer","metadata":{}},{"cell_type":"markdown","source":"Here we make a Python function to help us get the median `basket_size` for one specific user. The median `basket_size` of a `user_id` can be calculated without knowing the information of other `user_ids`. This is a good hint that we can do this on a per partition basis. The partition is a guide for parallelization. **Data that belong to the same partition will live inside the same executor**.","metadata":{}},{"cell_type":"code","source":"#schema: user_id:int, basket_size:int\ndef get_basket_size_median(df:pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame({'user_id': [df.iloc[0]['user_id']], 'basket_size' : [int(round(df[['basket_size']].median()))]})","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:39:35.414127Z","iopub.execute_input":"2021-11-04T05:39:35.414493Z","iopub.status.idle":"2021-11-04T05:39:35.420173Z","shell.execute_reply.started":"2021-11-04T05:39:35.41446Z","shell.execute_reply":"2021-11-04T05:39:35.418952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 206k users in the dataset. Let us downsample.\nn = 50000","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:39:35.421835Z","iopub.execute_input":"2021-11-04T05:39:35.422158Z","iopub.status.idle":"2021-11-04T05:39:35.434217Z","shell.execute_reply.started":"2021-11-04T05:39:35.422127Z","shell.execute_reply":"2021-11-04T05:39:35.433233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql\nuser_id = SELECT DISTINCT user_id FROM orders\nuser_id = SAMPLE {{n}} ROWS SEED 1 FROM user_id\nYIELD FILE AS user_ids","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:39:35.435739Z","iopub.execute_input":"2021-11-04T05:39:35.436352Z","iopub.status.idle":"2021-11-04T05:39:36.564061Z","shell.execute_reply.started":"2021-11-04T05:39:35.436317Z","shell.execute_reply":"2021-11-04T05:39:36.562688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Production Note**\n\nA side note, some people are curious about how to move Fugue from notebooks to production. There is an `fsql` function with Fugue that can be used for the programming interface. A user would wrap their SQL query in a string and then pass it to `fsql`. Of course, we lose syntax highlighting because it's a Python string.","metadata":{}},{"cell_type":"code","source":"from fugue_sql import fsql\n\nquery = \"\"\"\nresult = LOAD \"/kaggle/working/result.parquet\"\n\nbasket_size = SELECT order_id, COUNT(*) AS basket_size\n                FROM result\n            GROUP BY order_id \n  \norder_id = SELECT orders.user_id, order_id \n             FROM orders\n       INNER JOIN user_ids\n               ON user_ids.user_id = orders.user_id\n            \nbasket_size = SELECT user_id, order_id.order_id, basket_size\n                FROM basket_size\n          INNER JOIN order_id\n                  ON basket_size.order_id = order_id.order_id\n                  \nPRINT 2 ROWS\n\nTRANSFORM basket_size PREPARTITION BY user_id USING get_basket_size_median\nPRINT 2 ROWS\n\"\"\"\n\nstart = time.time()\nfsql(query).run()\nprint(f\"Operation took {time.time() - start} seconds\")","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:39:36.565277Z","iopub.execute_input":"2021-11-04T05:39:36.565624Z","iopub.status.idle":"2021-11-04T05:48:15.236746Z","shell.execute_reply.started":"2021-11-04T05:39:36.565592Z","shell.execute_reply":"2021-11-04T05:48:15.235518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we bring the same query into Spark by passing in the ending into the `run` method. We will see the benefits of parallelizing the opeartion. Some of the performance gains are due to the optimizations of SparkSQL. There is more than a 4x speed up just by changing the execution engine.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nfsql(query).run(\"spark\")\nprint(f\"Operation took {time.time() - start} seconds\")","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:48:15.238207Z","iopub.execute_input":"2021-11-04T05:48:15.238538Z","iopub.status.idle":"2021-11-04T05:49:08.256683Z","shell.execute_reply.started":"2021-11-04T05:48:15.238506Z","shell.execute_reply":"2021-11-04T05:49:08.255746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Top 5 Products for Each User","metadata":{}},{"cell_type":"markdown","source":"In this section we are interested in getting the top 5 products for each user. We also want to know how frequently they buy the products. Are they buying it every time they go into the store? Maybe this will tell us which customers are very predictable.","metadata":{}},{"cell_type":"code","source":"from fugue import FugueWorkflow\nfrom fugue_spark import SparkExecutionEngine\nfrom typing import List, Any, Iterable\n\n# schema: user_id:int, product_name:str, count:int\ndef product_count(df:pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame({'user_id': [df.iloc[0]['user_id']], \n                         'product_name': [df.iloc[0]['product_name']],\n                         'count': [df.shape[0]]})\n\n# schema: user_id:int, n_orders:int\ndef nth(df:Iterable[List[Any]], n) -> Iterable[List[Any]]:\n    for row in df:\n        if n==0:\n            yield [row[0], row[1]]\n            return\n        n-=1\n\n        \nwith FugueWorkflow(SparkExecutionEngine) as dag:\n    product_orders = dag.load(\"/kaggle/working/result.parquet\")\n    \n    order_id = dag.df(orders)\n    users = dag.df(user_ids)\n    \n    # Filtering to our sampled users. Note the persist\n    order_id = order_id.join(users, how=\"inner\", on=[\"user_id\"]).persist()\n    \n    # Count for each product by user_id\n    tempdf = dag.df(product_orders).join(order_id, how=\"inner\", on=[\"order_id\"])\n    tempdf = tempdf.partition(by=[\"user_id\", \"product_name\"]).transform(product_count)\n\n    # Join to tempdf2 which gets us the number of orders per user\n    tempdf2 = order_id.partition(by=['user_id'], presort=\"order_id desc\").transform(nth, params={\"n\":0})\n    result = tempdf.join(tempdf2, how=\"inner\", on=[\"user_id\"])\\\n                    .partition(by=[\"user_id\"], presort=\"count desc\")\\\n                    .take(5)\n    result.save(\"/kaggle/working/top_5_products.parquet\", mode=\"overwrite\")\n    result.show(15)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:49:08.257867Z","iopub.execute_input":"2021-11-04T05:49:08.258304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql spark\nproduct_orders = SELECT order_id, product_name FROM (LOAD \"/kaggle/working/result.parquet\")\n\n-- Filtering to our sampled users\norder_id = SELECT orders.user_id, order_id, order_number\n             FROM orders\n       INNER JOIN user_ids\n               ON user_ids.user_id = orders.user_id PERSIST\n\n-- Count for each product by user_id\ntempdf = SELECT user_id, product_name, COUNT(*) AS count\n           FROM product_orders\n     INNER JOIN order_id\n             ON product_orders.order_id = order_id.order_id\n       GROUP BY user_id, product_name\n    \n-- Join to an inner select which gets us the number of orders per user\n    SELECT tempdf.user_id, product_name, count, n_orders\n      FROM tempdf\nINNER JOIN (SELECT user_id, MAX(order_number) AS n_orders\n            FROM order_id\n            GROUP BY user_id) tempdf2\n        ON tempdf.user_id = tempdf2.user_id\n      TAKE 5 ROWS PREPARTITION BY user_id PRESORT count DESC  \n      SAVE OVERWRITE \"/kaggle/working/top_5_products.parquet\"\n     PRINT 15 ROWS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Persist and Lazy Evaluation","metadata":{}},{"cell_type":"markdown","source":"![DAG](https://www.edureka.co/community/?qa=blob&qa_blobid=12881994506202880144)","metadata":{}},{"cell_type":"markdown","source":"## Reorder Ratio Over Time","metadata":{}},{"cell_type":"markdown","source":"![Line of best fit](https://i.investopedia.com/content/video/line_of_best_fit_/lineofbestfit.png)","metadata":{}},{"cell_type":"code","source":"# schema: user_id:int, trend:double\ndef reorder_trend(df:pd.DataFrame) -> pd.DataFrame:\n    m, b = np.polyfit(list(range(df.shape[0])), df['reorder_rate'], 1)\n    if df.shape[0] > 5:\n        return pd.DataFrame({'user_id': [df.iloc[0]['user_id']], 'trend' : [m]})\n    else:\n        return pd.DataFrame({'user_id': [df.iloc[0]['user_id']], 'trend' : [0]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%fsql spark\nproduct_orders = SELECT order_id, reordered FROM (LOAD \"/kaggle/working/result.parquet\")\norder_id = SELECT orders.user_id, order_id, order_number\n             FROM orders\n       INNER JOIN user_ids\n               ON user_ids.user_id = orders.user_id\n               \n    SELECT user_id, order_number, AVG(reordered) AS reorder_rate\n      FROM product_orders\nINNER JOIN order_id\n        ON product_orders.order_id = order_id.order_id\n  GROUP BY user_id, order_number\n  TAKE 10 ROWS PREPARTITION BY user_id PRESORT order_number DESC\n  PRINT 10 ROWS\n  \ntrend = TRANSFORM PREPARTITION BY user_id PRESORT order_number ASC USING reorder_trend \n  PRINT 5 ROWS\n  \n  SELECT * FROM trend\n  YIELD FILE AS trend_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although not shown in this notebook, it is worth talking about PERSIST and lazy evaluation for those new to distributed computing. ","metadata":{}},{"cell_type":"code","source":"%%fsql\nSELECT * \n     FROM trend_stats\n ORDER BY trend DESC\n PRINT 10 ROWS\n \nSELECT * \n     FROM trend_stats\n ORDER BY trend ASC\n PRINT 10 ROWS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fugue import FugueWorkflow\nfrom fugue_spark import SparkExecutionEngine\nfrom typing import List, Any, Iterable\n\n# schema: user_id:int, product_name:str, count:int\ndef product_count(df:pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame({'user_id': [df.iloc[0]['user_id']], \n                         'product_name': [df.iloc[0]['product_name']],\n                         'count': [df.shape[0]]})\n\n# schema: user_id:int, n_orders:int\ndef nth(df:Iterable[List[Any]], n) -> Iterable[List[Any]]:\n    for row in df:\n        if n==0:\n            yield [row[0], row[1]]\n            return\n        n-=1\n\n        \nwith FugueWorkflow(SparkExecutionEngine) as dag:\n    product_orders = dag.load(\"/kaggle/working/result.parquet\")\n    \n    order_id = dag.df(orders)\n    users = dag.df(user_ids)\n    \n    # Filtering to our sampled users. Note the persist\n    order_id = order_id.join(users, how=\"inner\", on=[\"user_id\"]).persist()\n    \n    # Count for each product by user_id\n    tempdf = dag.df(product_orders).join(order_id, how=\"inner\", on=[\"order_id\"])\n    tempdf = tempdf.partition(by=[\"user_id\", \"product_name\"]).transform(product_count)\n\n    # Join to tempdf2 which gets us the number of orders per user\n    tempdf2 = order_id.partition(by=['user_id'], presort=\"order_id desc\").transform(nth, params={\"n\":0})\n    result = tempdf.join(tempdf2, how=\"inner\", on=[\"user_id\"])\\\n                    .partition(by=[\"user_id\"], presort=\"count desc\")\\\n                    .take(5)\n    result.save(\"/kaggle/working/top_5_products.parquet\", mode=\"overwrite\")\n    result.show(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}