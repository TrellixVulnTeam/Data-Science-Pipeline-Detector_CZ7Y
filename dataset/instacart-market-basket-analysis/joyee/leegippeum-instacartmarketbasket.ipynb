{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 데이터셋 분석\n\n* 목표는 사용자의 다음 주문에 어떤 제품이 있을지 예측하는 것.\n* 20 만 명 이상의 Instacart 사용자.\n* 3 백만 개 이상의 식료품 주문 샘플이 포함. \n* 각 사용자에 대해 주문마다 구매 한 제품 순서와 함께 4 ~ 100 개의 주문을 제공. \n* 주문한 주와 시간 및 주문 간의 상대적 시간 측정 값도 제공.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# PreProcessing","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/aisles.csv.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./\")\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/orders.csv.zip\",\"r\") as zip_ref:    \n    zip_ref.extractall(\"./\")\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/departments.csv.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./\")\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/products.csv.zip\",\"r\") as zip_ref:    \n    zip_ref.extractall(\"./\")\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/order_products__train.csv.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./\")\nwith zipfile.ZipFile(\"/kaggle/input/instacart-market-basket-analysis/order_products__prior.csv.zip\",\"r\") as zip_ref:    \n    zip_ref.extractall(\"./\")\n    \n# zip 압축풀기\n# 현재위치인 output_kaggle에 csv 파일 저장","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\naisles = pd.read_csv('aisles.csv')\norders = pd.read_csv('orders.csv')\nproducts = pd.read_csv('products.csv')\ndepartments = pd.read_csv('departments.csv')\norder_products__prior = pd.read_csv('order_products__prior.csv')\norder_products__train = pd.read_csv('order_products__train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aisles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"departments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"products\n# products에 aisle과 department가 연결되어 있음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_product1 = pd.merge(products, aisles, on=\"aisle_id\")\ndf_product1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_product2 = pd.merge(df_product1, departments, on=\"department_id\")\ndf_product2\n# product_id를 key로 사용하여 aisles,departments를 products에 merge.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['user_id'].value_counts()\n# 주문내역을 user_id로 count 해보면 206209명의 고객 정보가 있다는 것을 알 수 있음.\n# 각 고객 한명당 주문건수가 4번에서 부터 100번까지 있음.\n# 총 주문건수는 3421083개","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.eval_set.value_counts()\n# 총 206209명의 고객 중 train 할 고객은 131209명이고 test할 고객은 75000명이다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order_products__prior\n# 원본 데이터(모집단)\n# fit, predict 모델 검증 끝난후에","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order_products__train\n# 모델 검증을 위한 샘플 데이터(표본)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_product2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_order_product = pd.merge(order_products__train, df_product2, on=\"product_id\")\ndf_order_product","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = pd.merge(orders, df_order_product, on=\"order_id\")\ndf0\n# 모든 데이터 다 merge 한 것.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.keys()\n# order_number: 주문한 횟수\n# order_dow: 주문한 요일\n# order_hour_of_day: 하루 중 주문한 시각","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns 정리하기(순서변경, 삭제)\n\ndf1 = df0[['user_id', 'order_id', 'order_number', 'order_dow',\n       'order_hour_of_day',\n       'add_to_cart_order', 'reordered', 'product_id', 'product_name', 'aisle_id', 'aisle',\n       'department_id', 'department']]\n\n# days_since_prior_order: 목표가 다음에 주문할 것이 무엇일지 예측하는 것이기 때문에 필요없는 컬럼같음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['eval_set'].value_counts()\n# df는 orders에서 train 데이터만 모아놓은 것.\n# ['eval_set'] 컬럼 삭제해도 됨.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n# Sample data(order_products__train)\n# df1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1\n# 전처리 끝","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.keys()\n\n# 종속변수: 'reordered' \n# 0,1로 나누어지는 범주형 변수\n\n# 독립변수: 'user_id', 'order_id', 'order_number', 'order_dow', 'order_hour_of_day','product_id', 'product_name', 'aisle_id', 'aisle', 'department_id', 'department'\n# 연속형과 범주형이 섞여있음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df1[['user_id', 'order_number', 'order_dow', 'order_hour_of_day', 'product_name', 'aisle', 'department', 'reordered']]\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.to_csv('df2.csv')\n\n# jamovi로 확인해보기","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Elbow Method\nfrom sklearn.cluster import KMeans\ndistortions = []\nfor i in range(1,5):\n    kmeans = KMeans(n_clusters=i, n_init=10, max_iter=300)\n    kmeans.fit(df2)\n    distortions.append(kmeans.inertia_)\n\nimport matplotlib.pyplot as plt\nplt.plot(range(1,5), distortions, marker='o')\nplt.show()  # 꺽인 부분 찾기\n\n# # Silhouette\n# from sklearn.cluster import KMeans\n# kmeans = KMeans(n_clusters=2, max_iter=300)\n# labels = kmeans.fit_predict(squad)\n# target = pd.DataFrame(kmeans.labels_, columns=['reordered'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 종속변수, 독립변수 지정하기\ny = df2['reordered']\nx = df2[['user_id', 'order_number', 'order_dow', 'order_hour_of_day', 'product_name', 'aisle', 'department']]\nx_dummies = pd.get_dummies(x[['order_dow','order_hour_of_day','product_name','aisle','department']], drop_first=True)\nnew_x = pd.concat([df2['user_id'],x_dummies],axis=1)\n\nfrom sklearn.model_selection import train_test_split\nx_train0, x_test0, y_train0, y_test0 = train_test_split(new_x, y, test_size=0.5)\n\n# sample size로 데이터 양 줄이기(0.25 수준)\n# 모델링 속도 향상을 위한 쪼개기 작업\nx_train, x_test, y_train, y_test = train_test_split(x_train0, y, test_size=0.3)\n\n# StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pd.read_csv('')\nx_test\ny_train\ny_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble\n1. Voting\n2. Bagging","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. VotingClassifier()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# VotingClassifier() with No_params\n# voting(hard, soft)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nclflog = LogisticRegression()\nclfrf = RandomForestClassifier()\nclfgn = GaussianNB()\nclfsvc = SVC()\nclfknn = KNeighborsClassifier()\n\nfrom sklearn.ensemble import VotingClassifier\neclf_h = VotingClassifier(estimators = [('lr',clflog),('rf',clfrf),('gnb',clfgn),('svc',clfsvc),('knn',clfknn)], voting='hard')\neclf_s = VotingClassifier(estimators = [('lr',clflog),('rf',clfrf),('gnb',clfgn),('svc',clfsvc),('knn',clfknn)], voting='soft')\n\nfrom sklearn.metrics import classification_report\nmodels = [clflog, clfrf, clfgn, clfsvc, clfknn, eclf_h, eclf_s]\n\nfor model in models:\n    model.fit(x_train,y_train)\n    predictions = model.predict(x_test)\n    score = model.score(x_test,y_test)\n    print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VotingClassifier(hard)\n# GridSearchCV\n# best_params_\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nclflog = LogisticRegression()\nclfrf = RandomForestClassifier()\nclfgn = GaussianNB()\nclfsvc = SVC()\nclfknn = KNeighborsClassifier()\n\nfrom sklearn.ensemble import VotingClassifier\neclf_h = VotingClassifier(estimators = [('lr',clflog),('rf',clfrf),('gnb',clfgn),('svc',clfsvc),('knn',clfknn)],voting='hard')\nc_params = [0.001,0.01,0.1,1,5,10.50,100,300,500,1000]\nparams = {\n    'lr__solver':['liblinear','lbfgs','saga'], \n    'lr__penalty':['l1','l2','elasticnet'], \n    'lr__C':c_params,\n    'rf__criterion':['gini','entropy'],\n    'rf__min_samples_leaf':[1,2,3,4,5],\n    'rf__n_estimators':[100,150,200],\n    # 'gnb__':[], 파라미터 지정할 필요 없는듯.. 그냥 default로\n    'svc__C':c_params,\n    'svc__gamma':[0.001,0.01,0.1,1,10],\n    'svc__kernel':['rbf','sigmoid'],\n    'svc__decision_function_shape':['ovo'],\n    'knn__n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n    'knn__weights':['uniform','distance']\n}\n\n# In your example, the cv=5, so the data will be split into train and test folds 5 times. \n# The model will be fitted on train and scored on test. \n# These 5 test scores are averaged to get the score.\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator = eclf_h, param_grid=params, cv=5, n_jobs=-1)\ngrid = grid.fit(x_train,y_train)\ngrid.best_params_     # VotingClassifier의 best_params_의 의미는 lr, rf, gnb, svc, knn 다함께 사용 할때의 최적의 파라미터라는 뜻.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VotingClassifier(hard)의 score with best_params_\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nclflog = LogisticRegression()\nclfrf = RandomForestClassifier()\nclfgn = GaussianNB()\nclfsvc = SVC()\nclfknn = KNeighborsClassifier()\n# 아래처럼 best_params 넣어주어야 함\n# clflog = LogisticRegression(C=5.0, penalty='l2', solver='liblinear')\n# clfdt = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_leaf=5)\n\nfrom sklearn.ensemble import VotingClassifier\neclf_h = VotingClassifier(estimators = [('lr',clflog),('rf',clfrf),('gnb',clfgn),('svc',clfsvc),('knn',clfknn)], voting='hard')\neclf_h.fit(x_train,y_train)\ny_pred = eclf_h.predict(x_test)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n\nprint(eclf_h.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VotingClassifier(soft)\n# 위에꺼 따라하기\n\neclf_s = VotingClassifier(estimators = [('lr',clflog),('rf',clfrf),('gnb',clfgn),('svc',clfsvc),('knn',clfknn)], voting='soft')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}