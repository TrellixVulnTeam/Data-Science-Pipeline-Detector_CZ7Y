{"cells":[{"metadata":{"_cell_guid":"9a45f08b-c554-1b35-a50d-c64150d6f789"},"cell_type":"markdown","source":"In this notebook, we will try and explore the basic information about the dataset given. The dataset for this competition is a relational set of files describing customers' orders over time. \n\n**Objective:** \n\nThe goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users.\n\nFor each user, 4 and 100 of their orders are given, with the sequence of products purchased in each order\n\nLet us start by importing the necessary modules."},{"metadata":{"_cell_guid":"4c76ddb9-4f12-d6d2-56aa-82c3254de71a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn import metrics , model_selection\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom IPython.display import clear_output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\ntf.random.set_seed(123)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96355f94-3cd7-f536-0cf2-dfde8441a46a"},"cell_type":"markdown","source":"Let us list out the files that are present in this competition.!"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/aisle2/\"\norders_df = pd.read_csv(data_path + \"orders.csv\", usecols=[\"order_id\",\"user_id\",\"order_number\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the prior order file #\nprior_df = pd.read_csv(data_path + \"order_products__prior.csv\")\n# merge with the orders file to get the user_id #\nprior_df = pd.merge(prior_df, orders_df, how=\"inner\", on=\"order_id\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(prior_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# get the products and reorder status of the latest purchase of each user #\nprior_grouped_df = prior_df.groupby(\"user_id\")[\"order_number\"].aggregate(\"max\").reset_index()\n\n\nprint(prior_grouped_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_df_latest = pd.merge(prior_df, prior_grouped_df, how=\"inner\", on=[\"user_id\", \"order_number\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(prior_df_latest.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_df_latest = prior_df_latest[[\"user_id\", \"product_id\", \"reordered\"]]\nprior_df_latest.columns = [\"user_id\", \"product_id\", \"reordered_latest\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(prior_df_latest.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the count of each product and number of reorders by the customer #\nprior_df = prior_df.groupby([\"user_id\",\"product_id\"])[\"reordered\"].aggregate([\"count\", \"sum\"]).reset_index()\nprior_df.columns = [\"user_id\", \"product_id\", \"reordered_count\", \"reordered_sum\"]\n\n# merge the prior df with latest df #\nprior_df = pd.merge(prior_df, prior_df_latest, how=\"left\", on=[\"user_id\",\"product_id\"])\nprior_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## after first merge of tables we have prior_df as"},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45ad0ba2-2d31-e90d-4e10-83fd3247a859","trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a44f145-b654-7b37-9528-bcf1a62a6071"},"cell_type":"markdown","source":"Before we dive deep into the exploratory analysis, let us know a little more about the files given. To understand it better, let us first read all the files as dataframe objects and then look at the top few rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders_df.drop([\"order_number\"],axis=1,inplace=True)\n\ntrain_df = pd.read_csv(\"../input/aisle3/order_products__train.csv\", usecols=[\"order_id\"])\ntrain_df = train_df.groupby(\"order_id\").aggregate(\"count\").reset_index()\ntest_df = pd.read_csv(\"../input/aisle2/sample_submission.csv\", usecols=[\"order_id\"])\ntrain_df = pd.merge(train_df, orders_df, how=\"inner\", on=\"order_id\")\ntest_df = pd.merge(test_df, orders_df, how=\"inner\", on=\"order_id\")\n\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, prior_df, how=\"inner\", on=\"user_id\")\ntest_df = pd.merge(test_df, prior_df, how=\"inner\", on=\"user_id\")\ndel prior_df, prior_grouped_df, prior_df_latest\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"products_df = pd.read_csv(data_path + \"products.csv\", usecols=[\"product_id\", \"aisle_id\", \"department_id\"])\ntrain_df = pd.merge(train_df, products_df, how=\"inner\", on=\"product_id\")\ntest_df = pd.merge(test_df, products_df, how=\"inner\", on=\"product_id\")\ndel products_df\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.head(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y_df = pd.read_csv(\"../input/aisle3/order_products__train.csv\", usecols=[\"order_id\", \"product_id\", \"reordered\"])\ntrain_y_df = pd.merge(train_y_df, orders_df, how=\"inner\", on=\"order_id\")\ntrain_y_df = train_y_df[[\"user_id\", \"product_id\", \"reordered\"]]\nprint(train_y_df.reordered.sum())\ntrain_df = pd.merge(train_df, train_y_df, how=\"left\", on=[\"user_id\", \"product_id\"])\ntrain_df[\"reordered\"].fillna(0, inplace=True)\nprint(train_df.shape)\nprint(train_df.reordered.sum())\ndel train_y_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dataframe for test set predictions "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nout_df = test_df[[\"order_id\", \"product_id\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target variable for train set #\ntrain_y = train_df.reordered.values\n\n\n\n# drop the unnecessary columns #\ntrain_df = np.array(train_df.drop([\"order_id\", \"user_id\", \"reordered\"], axis=1))\ntest_df = np.array(test_df.drop([\"order_id\", \"user_id\"], axis=1))\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_y\ndftrain = train_df\ndfeval = test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For gradient boosted tree"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"## Now, building a pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"fc = tf.feature_column\nCATEGORICAL_COLUMNS = ['user_id', 'aisle_id', 'department_id']\nNUMERIC_COLUMNS = ['reordered_count','reordered_sum','reordered_latest']\n\ndef one_hot_cat_column(feature_name, vocab):\n  return tf.feature_column.indicator_column(\n      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n                                                 vocab))\nfeature_columns = []\n# for feature_name in CATEGORICAL_COLUMNS:\n#    Need to one-hot encode categorical features.\n#    vocabulary = dftrain[feature_name].unique()\n#   feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                           dtype=tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use entire batch since this is such a small dataset.\nNUM_EXAMPLES = len(train_y)\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n  def input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n    if shuffle:\n      dataset = dataset.shuffle(NUM_EXAMPLES)\n    # For training, cycle thru dataset as many times as need (n_epochs=None).\n    dataset = dataset.repeat(n_epochs)\n    # In memory training doesn't use batching.\n    dataset = dataset.batch(NUM_EXAMPLES)\n    return dataset\n  return input_fn\n\n# Training and evaluation input functions.\ntrain_input_fn = make_input_fn(out_df,dftrain)\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example = dict(dftrain.head(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.layers.DenseFeatures(feature_columns)(example).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'n_trees': 50,\n  'max_depth': 3,\n  'n_batches_per_layer': 1,\n  # You must enable center_bias = True to get DFCs. This will force the model to\n  # make an initial prediction before using any features (e.g. use the mean of\n  # the training labels for regression or log odds for classification when\n  # using cross entropy loss).\n  'center_bias': True\n}\n\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n# Train model.\nest.train(train_input_fn, max_steps=100)\n\n# Evaluation.\nresults = est.evaluate(eval_input_fn)\nclear_output()\npd.Series(results).to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using a linear model(logistic REgression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_est = tf.estimator.LinearClassifier(feature_columns)\n\n# Train model.\nlinear_est.train(train_input_fn, max_steps=100)\n\n# Evaluation.\nresult = linear_est.evaluate(eval_input_fn)\nclear_output()\nprint(pd.Series(result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now using boosted trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since data fits into memory, use entire dataset per layer. It will be faster.\n# Above one batch is defined as the entire dataset.\nn_batches = 1\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                          n_batches_per_layer=n_batches)\n\n# The model will stop training once the specified number of trees is built, not\n# based on the number of steps.\nest.train(train_input_fn, max_steps=100)\n\n# Eval.\nresult = est.evaluate(eval_input_fn)\nclear_output()\nprint(pd.Series(result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XG boost implimentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to run the xgboost model #\n# change the number of rounds to 100 in local to get 0.3450 score. getting time out here #\ndef runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n        params = {}\n        params[\"objective\"] = \"binary:logistic\"\n        params['eval_metric'] = 'logloss'\n        params[\"eta\"] = 0.05\n        params[\"subsample\"] = 0.7\n        params[\"min_child_weight\"] = 10\n        params[\"colsample_bytree\"] = 0.7\n        params[\"max_depth\"] = 8\n        params[\"silent\"] = 1\n        params[\"seed\"] = seed_val\n        num_rounds = 100\n        plst = list(params.items())\n        xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n        if test_y is not None:\n                xgtest = xgb.DMatrix(test_X, label=test_y)\n                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=10)\n        else:\n                xgtest = xgb.DMatrix(test_X)\n                model = xgb.train(plst, xgtrain, num_rounds)\n\n        pred_test_y = model.predict(xgtest)\n        return pred_test_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the xgboost model #\npred = runXGB(train_df, train_y, test_df)\ndel train_df, test_df\n\n# use a cut-off value to get the predictions #\ncutoff = 0.2\npred[pred>=cutoff] = 1\npred[pred<cutoff] = 0\nout_df[\"Pred\"] = pred\nout_df = out_df.ix[out_df[\"Pred\"].astype('int')==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# when there are more than 1 product, merge them to a single string #\ndef merge_products(x):\n    return \" \".join(list(x.astype('str')))\nout_df = out_df.groupby(\"order_id\")[\"product_id\"].aggregate(merge_products).reset_index()\nout_df.columns = [\"order_id\", \"products\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the sample csv file and populate the products from predictions #\nsub_df = pd.read_csv(data_path + \"sample_submission.csv\", usecols=[\"order_id\"])\nsub_df = pd.merge(sub_df, out_df, how=\"left\", on=\"order_id\")\n\n# when there are no predictions use \"None\" #\nsub_df[\"products\"].fillna(\"None\", inplace=True)\nsub_df.to_csv(\"xgb_starter_3450.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# From here the data exploration starts"},{"metadata":{"_cell_guid":"398b93eb-873e-728e-be4c-e101adb9d26d","trusted":true},"cell_type":"code","source":"order_products_train_df = pd.read_csv(\"../input/aisle3/order_products__train.csv\")\norder_products_prior_df = pd.read_csv(\"../input/aisle2/order_products__prior.csv\")\norders_df = pd.read_csv(\"../input/aisle2/orders.csv\")\nproducts_df = pd.read_csv(\"../input/aisle2/products.csv\")\naisles_df = pd.read_csv(\"../input/aisle1/aisles.csv\")\ndepartments_df = pd.read_csv(\"../input/aisle2/departments.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"baf0cd70-e2cd-5a2f-fec5-281c3e8efa28","trusted":true},"cell_type":"code","source":"orders_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07b7e152-8de3-3a18-d286-7158cba15e5d","trusted":true},"cell_type":"code","source":"order_products_prior_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0db75f9d-fa7d-65b6-4527-65830b84cb50","trusted":true},"cell_type":"code","source":"order_products_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ce2a903-52ba-d57b-401c-9ed9b3d5269a"},"cell_type":"markdown","source":"As we could see, orders.csv has all the information about the given order id like the user who has purchased the order, when was it purchased, days since prior order and so on.\n\nThe columns present in order_products_train and order_products_prior are same. Then what is the difference between these files.?\n\nAs mentioned earlier, in this dataset, 4 to 100 orders of a customer are given (we will look at this later) and we need to predict the products that will be re-ordered. So the last order of the user has been taken out and divided into train and test sets. All the prior order informations of the customer are present in order_products_prior file.  We can also note that there is a column in orders.csv file called eval_set which tells us as to which of the three datasets (prior, train or test) the given row goes to.\n\nOrder_products*csv file has more detailed information about the products that been bought in the given order along with the re-ordered status.\n\nLet us first get the count of rows in each of the three sets."},{"metadata":{"_cell_guid":"71019383-4ad9-62c3-9f9a-d0d5bb326f99","trusted":true},"cell_type":"code","source":"cnt_srs = orders_df.eval_set.value_counts()\n\nplt.figure(figsize=(16,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[1])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Eval set type', fontsize=12)\nplt.title('Count of rows in each dataset', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4631d5bd-54e4-5057-e3d6-de53f924c5e4","trusted":true},"cell_type":"code","source":"def get_unique_count(x):\n    return len(np.unique(x))\n\ncnt_srs = orders_df.groupby(\"eval_set\")[\"user_id\"].aggregate(get_unique_count)\ncnt_srs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a2dc58e-7df6-f42d-8b42-e94e2ece30a2"},"cell_type":"markdown","source":"So there are 206,209 customers in total. Out of which, the last purchase of 131,209 customers are given as train set and we need to predict for the rest 75,000 customers. \n\nNow let us validate the claim that 4 to 100 orders of a customer are given. "},{"metadata":{"_cell_guid":"ccddb4a4-1fb7-f1b1-fbaf-80a2d5814522","trusted":true},"cell_type":"code","source":"cnt_srs = orders_df.groupby(\"user_id\")[\"order_number\"].aggregate(np.max).reset_index()\ncnt_srs = cnt_srs.order_number.value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[2])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Maximum order number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2180926-1db1-aca7-1257-5906d92d24f0"},"cell_type":"markdown","source":"So there are no orders less than 4 and is max capped at 100 as given in the data page. \n\nNow let us see how the ordering habit changes with day of week."},{"metadata":{"_cell_guid":"fd013d90-47b5-aa52-fbe4-e86e1f568fb2","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"order_dow\", data=orders_df, color=color[0])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by week day\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c84085f7-f485-bc58-ffff-8794e5e0dc50"},"cell_type":"markdown","source":"Seems like 0 and 1 is Saturday and Sunday when the orders are high and low during Wednesday.\n\nNow we shall see how the distribution is with respect to time of the day."},{"metadata":{"_cell_guid":"d5a3cc83-fa06-5657-41c7-46bdbb7f8a72","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"order_hour_of_day\", data=orders_df, color=color[1])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Hour of day', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by hour of day\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed48981b-dba9-f695-28ca-416f61f7594e"},"cell_type":"markdown","source":"So majority of the orders are made during day time. Now let us combine the day of week and hour of day to see the distribution."},{"metadata":{"_cell_guid":"c8252648-05a3-959f-3e4e-03a3b01012fa","trusted":true},"cell_type":"code","source":"grouped_df = orders_df.groupby([\"order_dow\", \"order_hour_of_day\"])[\"order_number\"].aggregate(\"count\").reset_index()\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'order_number')\n\nplt.figure(figsize=(12,6))\nsns.heatmap(grouped_df)\nplt.title(\"Frequency of Day of week Vs Hour of day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c70b0de-247b-d415-8b75-a8578461ec91"},"cell_type":"markdown","source":"Seems Satuday evenings and Sunday mornings are the prime time for orders.\n\nNow let us check the time interval between the orders."},{"metadata":{"_cell_guid":"2845917a-0999-c9a1-94f2-c2c8e45fbf03","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"days_since_prior_order\", data=orders_df, color=color[3])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Days since prior order', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency distribution by days since prior order\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7edf3a56-1dd0-d0b3-3057-a0eafc57a8ac"},"cell_type":"markdown","source":"Looks like customers order once in every week (check the peak at 7 days) or once in a month (peak at 30 days). We could also see smaller peaks at 14, 21 and 28 days (weekly intervals).\n\nSince our objective is to figure out the re-orders, let us check out the re-order percentage in prior set and train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"order_products_prior_df.reordered.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order_products_prior_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"371aeeb5-ae2e-a917-a872-90ef56e063c9","trusted":true},"cell_type":"code","source":"# percentage of re-orders in prior set #\norder_products_prior_df.reordered.sum() / order_products_prior_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b2b567e-073a-d9e4-1b64-9ae66ba65188","trusted":true},"cell_type":"code","source":"# percentage of re-orders in train set #\norder_products_train_df.reordered.sum() / order_products_train_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bebb053d-7868-27b2-e57c-0997af22cec8"},"cell_type":"markdown","source":"On an average, about 59% of the products in an order are re-ordered products.\n\n**No re-ordered products:**\n\nNow that we have seen 59% of the products are re-ordered, there will also be situations when none of the products are re-ordered. Let us check that now."},{"metadata":{"_cell_guid":"f3182fcd-77bb-c1e0-a93f-437674bd282a","trusted":true},"cell_type":"code","source":"grouped_df = order_products_prior_df.groupby(\"order_id\")[\"reordered\"].aggregate(\"sum\").reset_index()\ngrouped_df[\"reordered\"].ix[grouped_df[\"reordered\"]>1] = 1\ngrouped_df.reordered.value_counts() / grouped_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d80b43f0-1139-5ed4-475d-c6bd9c144711","trusted":true},"cell_type":"code","source":"grouped_df = order_products_train_df.groupby(\"order_id\")[\"reordered\"].aggregate(\"sum\").reset_index()\ngrouped_df[\"reordered\"].ix[grouped_df[\"reordered\"]>1] = 1\ngrouped_df.reordered.value_counts() / grouped_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"577ca5cd-0222-9f42-d3be-8704d8085d96"},"cell_type":"markdown","source":"About 12% of the orders in prior set has no re-ordered items while in the train set it is 6.5%.\n\nNow let us see the number of products bought in each order."},{"metadata":{"_cell_guid":"5f008745-1fa5-96c9-3586-f333689ce032","trusted":true},"cell_type":"code","source":"grouped_df = order_products_train_df.groupby(\"order_id\")[\"add_to_cart_order\"].aggregate(\"max\").reset_index()\ncnt_srs = grouped_df.add_to_cart_order.value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of products in the given order', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e04654e3-c4ec-8711-140c-4fad41f421df"},"cell_type":"markdown","source":"A right tailed distribution with the maximum value at 5.!\n\nBefore we explore the product details, let us look at the other three files as well. "},{"metadata":{"_cell_guid":"c5a01d40-d6e8-efb6-6559-0fc5f674ddb6","trusted":true},"cell_type":"code","source":"products_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c48ba63b-117c-1900-2b4d-24d5a5442e71","trusted":true},"cell_type":"code","source":"aisles_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69cf5c8f-1c10-5731-6a68-3a8141442ae4","trusted":true},"cell_type":"code","source":"departments_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8898f07d-f6cb-227b-6a03-11cf857b035f"},"cell_type":"markdown","source":"Now let us merge these product details with the order_prior details."},{"metadata":{"_cell_guid":"a8973ae6-4f89-b4b7-18fe-a722fea7c470","trusted":true},"cell_type":"code","source":"order_products_prior_df = pd.merge(order_products_prior_df, products_df, on='product_id', how='left')\norder_products_prior_df = pd.merge(order_products_prior_df, aisles_df, on='aisle_id', how='left')\norder_products_prior_df = pd.merge(order_products_prior_df, departments_df, on='department_id', how='left')\norder_products_prior_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6508267f-4e84-d7fc-55e3-108f683dc751","trusted":true},"cell_type":"code","source":"cnt_srs = order_products_prior_df['product_name'].value_counts().reset_index().head(20)\ncnt_srs.columns = ['product_name', 'frequency_count']\ncnt_srs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b199c4ff-16ad-8e99-f8c4-2e11508b390b"},"cell_type":"markdown","source":"Wow. Most of them are organic products.! Also majority of them are fruits. \n\nNow let us look at the important aisles."},{"metadata":{"_cell_guid":"e5d7bdb0-3e66-f8e9-22c8-4cfa8b12b632","trusted":true},"cell_type":"code","source":"cnt_srs = order_products_prior_df['aisle'].value_counts().head(20)\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[5])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Aisle', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8464fc1-2e14-a879-e6b3-e30f6e59ca9a"},"cell_type":"markdown","source":"The top two aisles are fresh fruits and fresh vegetables.! \n\n**Department Distribution:**\n\nLet us now check the department wise distribution."},{"metadata":{"_cell_guid":"26c6b059-5b6e-ad21-588c-cc12fd59dade","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntemp_series = order_products_prior_df['department'].value_counts()\nlabels = (np.array(temp_series.index))\nsizes = (np.array((temp_series / temp_series.sum())*100))\nplt.pie(sizes, labels=labels, \n        autopct='%1.1f%%', startangle=200)\nplt.title(\"Departments distribution\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3ba7ecb-a613-95eb-369b-c8fd0e0b54e0"},"cell_type":"markdown","source":"Produce is the largest department. Now let us check the reordered percentage of each department. \n\n**Department wise reorder ratio:**"},{"metadata":{"_cell_guid":"c16f74ef-6649-3125-457f-e0cdbf6683d1","trusted":true},"cell_type":"code","source":"grouped_df = order_products_prior_df.groupby([\"department\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['department'].values, grouped_df['reordered'].values, alpha=0.8, color=color[2])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Department', fontsize=12)\nplt.title(\"Department wise reorder ratio\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f683942-23a8-cd48-f29e-1fa81329990d"},"cell_type":"markdown","source":"Personal care has lowest reorder ratio and dairy eggs have highest reorder ratio.\n\n**Aisle - Reorder ratio:**"},{"metadata":{"_cell_guid":"4b7e41e6-4363-b719-f61f-aa15659cb163","trusted":true},"cell_type":"code","source":"grouped_df = order_products_prior_df.groupby([\"department_id\", \"aisle\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nfig, ax = plt.subplots(figsize=(12,20))\nax.scatter(grouped_df.reordered.values, grouped_df.department_id.values)\nfor i, txt in enumerate(grouped_df.aisle.values):\n    ax.annotate(txt, (grouped_df.reordered.values[i], grouped_df.department_id.values[i]), rotation=45, ha='center', va='center', color='green')\nplt.xlabel('Reorder Ratio')\nplt.ylabel('department_id')\nplt.title(\"Reorder ratio of different aisles\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c529f57f-3bd0-6b42-b8b1-d306ba1671ee"},"cell_type":"markdown","source":"**Add to Cart - Reorder ratio:**\n\nLet us now explore the relationship between how order of adding the product to the cart affects the reorder ratio."},{"metadata":{"_cell_guid":"7ff1b0bb-1571-3a4d-ce2a-37ee8d2de0f4","trusted":true},"cell_type":"code","source":"order_products_prior_df[\"add_to_cart_order_mod\"] = order_products_prior_df[\"add_to_cart_order\"].copy()\norder_products_prior_df[\"add_to_cart_order_mod\"].ix[order_products_prior_df[\"add_to_cart_order_mod\"]>70] = 70\ngrouped_df = order_products_prior_df.groupby([\"add_to_cart_order_mod\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['add_to_cart_order_mod'].values, grouped_df['reordered'].values, alpha=0.8, color=color[2])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Add to cart order', fontsize=12)\nplt.title(\"Add to cart order - Reorder ratio\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7be627bf-2847-13c4-fbbe-f7f5f43f6068"},"cell_type":"markdown","source":"**Looks like the products that are added to the cart initially are more likely to be reordered again compared to the ones added later.** This makes sense to me as well since we tend to first order all the products we used to buy frequently and then look out for the new products available. \n\n**Reorder ratio by Time based variables:**"},{"metadata":{"_cell_guid":"f40cb0e9-7aad-e556-8c40-1ccf387c7a34","trusted":true},"cell_type":"code","source":"order_products_train_df = pd.merge(order_products_train_df, orders_df, on='order_id', how='left')\ngrouped_df = order_products_train_df.groupby([\"order_dow\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.barplot(grouped_df['order_dow'].values, grouped_df['reordered'].values, alpha=0.8, color=color[3])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title(\"Reorder ratio across day of week\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.ylim(0.5, 0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6dfdc0e5-3259-e83b-5015-33a8d35e9968","trusted":true},"cell_type":"code","source":"grouped_df = order_products_train_df.groupby([\"order_hour_of_day\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.barplot(grouped_df['order_hour_of_day'].values, grouped_df['reordered'].values, alpha=0.8, color=color[4])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Hour of day', fontsize=12)\nplt.title(\"Reorder ratio across hour of day\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.ylim(0.5, 0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"683ac606-fd17-fcd6-1b77-764c34b15fd0","trusted":true},"cell_type":"code","source":"\ngrouped_df = order_products_train_df.groupby([\"order_dow\", \"order_hour_of_day\"])[\"reordered\"].aggregate(\"mean\").reset_index()\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\n\nplt.figure(figsize=(12,6))\nsns.heatmap(grouped_df)\nplt.title(\"Reorder ratio of Day of week Vs Hour of day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"773fa291-3eb3-706a-2c75-25fd3d7906ea"},"cell_type":"markdown","source":"Looks like reorder ratios are quite high during the early mornings compared to later half of the day."},{"metadata":{"_cell_guid":"2e6b9c1b-86e1-1835-e257-c6483bb5ae12"},"cell_type":"markdown","source":"**Hope it helped. Please leave your comments / suggestions.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}