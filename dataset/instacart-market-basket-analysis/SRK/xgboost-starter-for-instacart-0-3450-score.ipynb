{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8a0f9d72-dcbc-8cde-4365-38446bc6c80e"},"source":"From the [Instacart blog post][1], it is seen that they are using XGBoost as one of their models in production.  \n\nIn this script, let us build a simple xgboost starter model which scores about 0.3450 on the LB ( currently #1 on 17th May, 2017 ;) )\n\nThis notebook just provides a framework for building xgboost models for this problem. There are lot of scope for improvement.\n\n  [1]: https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"762c42a8-fa75-d6dc-815d-f70729f8f898"},"outputs":[],"source":"### Import necessary modules ###\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn import metrics, model_selection"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a6d3cfc-1bb8-aba1-43cf-404a0365fcac"},"source":"**Objective:**\n\nTo predict which previously purchased products will be in a userâ€™s next order\n\nFor data exploration, please refer to this [notebook.][1]\n\nWe will start with reading the orders file.\n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-instacart"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bd76b15-214b-5edb-3a82-468cdbb9126b"},"outputs":[],"source":"data_path = \"../input/\"\norders_df = pd.read_csv(data_path + \"orders.csv\", usecols=[\"order_id\",\"user_id\",\"order_number\"])"},{"cell_type":"markdown","metadata":{"_cell_guid":"877bceaf-985d-49bc-6d3b-5c6ad7828a41"},"source":"Since the objective is to predict which previously purchased products will be in next order, let us first get the list of all products purchased by the customer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f5a2d50-e0d7-2c52-b26c-b147a62a770f"},"outputs":[],"source":"# read the prior order file #\nprior_df = pd.read_csv(data_path + \"order_products__prior.csv\")\n\n# merge with the orders file to get the user_id #\nprior_df = pd.merge(prior_df, orders_df, how=\"inner\", on=\"order_id\")\n\n# get the products and reorder status of the latest purchase of each user #\nprior_grouped_df = prior_df.groupby(\"user_id\")[\"order_number\"].aggregate(\"max\").reset_index()\nprior_df_latest = pd.merge(prior_df, prior_grouped_df, how=\"inner\", on=[\"user_id\", \"order_number\"])\nprior_df_latest = prior_df_latest[[\"user_id\", \"product_id\", \"reordered\"]]\nprior_df_latest.columns = [\"user_id\", \"product_id\", \"reordered_latest\"]\n\n# get the count of each product and number of reorders by the customer #\nprior_df = prior_df.groupby([\"user_id\",\"product_id\"])[\"reordered\"].aggregate([\"count\", \"sum\"]).reset_index()\nprior_df.columns = [\"user_id\", \"product_id\", \"reordered_count\", \"reordered_sum\"]\n\n# merge the prior df with latest df #\nprior_df = pd.merge(prior_df, prior_df_latest, how=\"left\", on=[\"user_id\",\"product_id\"])\nprior_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"41084b81-6fc7-9bab-0aa4-38525d37398a"},"source":"Read the train and test dataset and then merge with orders data to get the user_id for the corresponding order_id."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d590c6a-a573-fe56-7ba1-c233deb76555"},"outputs":[],"source":"orders_df.drop([\"order_number\"],axis=1,inplace=True)\n\ntrain_df = pd.read_csv(data_path + \"order_products__train.csv\", usecols=[\"order_id\"])\ntrain_df = train_df.groupby(\"order_id\").aggregate(\"count\").reset_index()\ntest_df = pd.read_csv(data_path + \"sample_submission.csv\", usecols=[\"order_id\"])\ntrain_df = pd.merge(train_df, orders_df, how=\"inner\", on=\"order_id\")\ntest_df = pd.merge(test_df, orders_df, how=\"inner\", on=\"order_id\")\nprint(train_df.shape, test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"572a6142-f36a-e368-e12e-6097de1f6e3c"},"source":"Merge the train and test data with prior_df to get the products purchased previously by the customer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"793028e8-90a5-a7f2-427d-046db698f838"},"outputs":[],"source":"train_df = pd.merge(train_df, prior_df, how=\"inner\", on=\"user_id\")\ntest_df = pd.merge(test_df, prior_df, how=\"inner\", on=\"user_id\")\ndel prior_df, prior_grouped_df, prior_df_latest\nprint(train_df.shape, test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6660abab-cd0e-6f65-a67c-118800b023e2"},"source":"products.csv file information about the products such as which department and aisle the given product belongs to. So merge the train and test data with product information."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"071ee61e-bd2d-fa6c-bb8e-3e1a4ecaeb78"},"outputs":[],"source":"products_df = pd.read_csv(data_path + \"products.csv\", usecols=[\"product_id\", \"aisle_id\", \"department_id\"])\ntrain_df = pd.merge(train_df, products_df, how=\"inner\", on=\"product_id\")\ntest_df = pd.merge(test_df, products_df, how=\"inner\", on=\"product_id\")\ndel products_df\nprint(train_df.shape, test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f10ba77-207b-d334-b48a-6b9c52962371"},"source":"Now we have all the products that has been purchased previously by the customer along with some characteristics / featrures. So we can use the train dataset to populate the target variable i.e., whether the product has been reordered in the next order. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45a63103-28bc-7e06-af7e-07773084c37e"},"outputs":[],"source":"train_y_df = pd.read_csv(data_path + \"order_products__train.csv\", usecols=[\"order_id\", \"product_id\", \"reordered\"])\ntrain_y_df = pd.merge(train_y_df, orders_df, how=\"inner\", on=\"order_id\")\ntrain_y_df = train_y_df[[\"user_id\", \"product_id\", \"reordered\"]]\n#print(train_y_df.reordered.sum())\ntrain_df = pd.merge(train_df, train_y_df, how=\"left\", on=[\"user_id\", \"product_id\"])\ntrain_df[\"reordered\"].fillna(0, inplace=True)\nprint(train_df.shape)\n#print(train_df.reordered.sum())\ndel train_y_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02b5535e-2f86-32d2-7b71-3d1f91154aee"},"outputs":[],"source":"# target variable for train set #\ntrain_y = train_df.reordered.values\n\n# dataframe for test set predictions #\nout_df = test_df[[\"order_id\", \"product_id\"]]\n\n# drop the unnecessary columns #\ntrain_df = np.array(train_df.drop([\"order_id\", \"user_id\", \"reordered\"], axis=1))\ntest_df = np.array(test_df.drop([\"order_id\", \"user_id\"], axis=1))\nprint(train_df.shape, test_df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82d3fef4-ba00-5b91-4a91-5bfc446210cc"},"outputs":[],"source":"# function to run the xgboost model #\ndef runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n        params = {}\n        params[\"objective\"] = \"binary:logistic\"\n        params['eval_metric'] = 'logloss'\n        params[\"eta\"] = 0.05\n        params[\"subsample\"] = 0.7\n        params[\"min_child_weight\"] = 10\n        params[\"colsample_bytree\"] = 0.7\n        params[\"max_depth\"] = 8\n        params[\"silent\"] = 1\n        params[\"seed\"] = seed_val\n        num_rounds = 100\n        plst = list(params.items())\n        xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n        if test_y is not None:\n                xgtest = xgb.DMatrix(test_X, label=test_y)\n                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=10)\n        else:\n                xgtest = xgb.DMatrix(test_X)\n                model = xgb.train(plst, xgtrain, num_rounds)\n\n        pred_test_y = model.predict(xgtest)\n        return pred_test_y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29cb3185-abd7-4f43-37d2-d58f1ce20639"},"outputs":[],"source":"# run the xgboost model #\npred = runXGB(train_df, train_y, test_df)\ndel train_df, test_df\n\n# use a cut-off value to get the predictions #\ncutoff = 0.2\npred[pred>=cutoff] = 1\npred[pred<cutoff] = 0\nout_df[\"Pred\"] = pred\nout_df = out_df.ix[out_df[\"Pred\"].astype('int')==1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64a8484b-6df0-9ead-0616-2016cffca6c4"},"outputs":[],"source":"# when there are more than 1 product, merge them to a single string #\ndef merge_products(x):\n    return \" \".join(list(x.astype('str')))\nout_df = out_df.groupby(\"order_id\")[\"product_id\"].aggregate(merge_products).reset_index()\nout_df.columns = [\"order_id\", \"products\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"257f81f7-060b-4fd2-9baa-8df2a89b08ce"},"outputs":[],"source":"# read the sample csv file and populate the products from predictions #\nsub_df = pd.read_csv(data_path + \"sample_submission.csv\", usecols=[\"order_id\"])\nsub_df = pd.merge(sub_df, out_df, how=\"left\", on=\"order_id\")\n\n# when there are no predictions use \"None\" #\nsub_df[\"products\"].fillna(\"None\", inplace=True)\nsub_df.to_csv(\"xgb_starter_3450.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}