{"cells":[{"cell_type":"markdown","outputs":[],"source":"### The objectives of this notebook are threefold:\n##### 1. To explore large data by storing it in disk (using sqlite) \n##### 2. To Visualizing large data using Seaborn \n##### 3. To attempt multiple techniques (market basket analysis, collaborative filtering, poisson regression etc.)\n\n### Notebook is organized in Four Sections\n##### Section 1: I will setup the environment. \n##### Section 2: I will import the data from  CSV to sqlite on disk.\n##### Section 3: I will start with our basic data exploration\n##### Section 4: I will attempt machine learning algorithms\n\nCredits:\nInspired from 3 Kernels and below website\nhttps://plot.ly/python/big-data-analytics-with-pandas-and-sqlite/","execution_count":null,"metadata":{"_uuid":"1f40d347b8c2e1f3e701eda82b6c485e57e5516d","_cell_guid":"8459f960-cfe1-2ee8-a481-71b75ff915d8"}},{"cell_type":"markdown","outputs":[],"source":"**Section 1**\n-------------","execution_count":null,"metadata":{"_uuid":"bbdb83fbc49ea2551654e69b8b7731d24cf35a04","_execution_state":"idle"}},{"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nfrom math import pi\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas.io.sql as pd_sql\nfrom sqlalchemy import create_engine\nimport sqlite3 as sql\nfrom datetime import datetime\nsns.set(color_codes=True)\nwarnings.filterwarnings('ignore') # silence annoying warningsn","metadata":{"_uuid":"27fff6efde5bc1cef78e4c19b30be75cf1e7b183","_execution_state":"idle","collapsed":true,"_cell_guid":"8b2a918d-b528-1a22-cb3c-ed47456ee7e6"},"execution_count":16},{"cell_type":"code","outputs":[],"source":"### setup the current working directory \nimport os\npath = \"/\"\napp_input=\"/kaggle/input\"\napp_output=\"../\"\n# Check current working directory.\nprint (os.getcwd())\nprint(\"\\n\")\n#os.remove('kaggle_instakart.db')\n# Now change the directory\nos.chdir( app_input )\n\n# Check current working directory.\nprint (\"Directory changed successfully \\n\")\nprint (os.getcwd())\n\n#print the list of csv files in the input folder\nfrom subprocess import check_output\nprint (\"\\n\")\nprint(check_output([\"ls\", \"/kaggle/input\"]).decode(\"utf8\"))\nos.chdir( \"/kaggle/working\" )\nconn = create_engine('sqlite:///kaggle_instakart.db')","metadata":{"_uuid":"2f8cb677cd872c821d24c76a8b8a0a0e8328111a","_execution_state":"idle","_cell_guid":"cf6bad90-951b-eb2d-edf7-c672b906a87b"},"execution_count":17},{"cell_type":"markdown","outputs":[],"source":"**Section 2**\n-------------","execution_count":null,"metadata":{"_uuid":"83bbfda0bd3e6dba03fbdab176679d5a265ab08b","_execution_state":"idle"}},{"cell_type":"markdown","outputs":[],"source":"# I have tried 3 different approaches to read large csv files to SQLite Database\n## Approach 1: pd.read_csv and pd.to_sql for complete files\n#### this approach failed with memory error on pd.to_sql().\n\naisles = pd.read_csv('../input/aisles.csv', engine='c')\n\nfull_sqllite_path = app_input + \"/kaggle_instakart.db\"\n\naisles.to_sql('aisles',conn,if_exists = 'replace',index=False )","execution_count":null,"metadata":{"_uuid":"4cfa30bd84e3f1317356efa67157fa7ed2b1db29"}},{"cell_type":"markdown","outputs":[],"source":"## Approach 2: pd.read_csv and pd.to_sql chunk by chunk\n#### Created a function read_large_csv_to_sqlite for this approach","execution_count":null,"metadata":{"_uuid":"aa6539286ff8b355b4c2a12cc1e60b970064111c"}},{"cell_type":"code","outputs":[],"source":"def read_large_csv_to_sqlite(filename,disk_engine, tablename):\n    start = datetime.now()\n    chunksize = 20000\n    j = 0\n    index_start = 1\n    drop_table=\"DROP TABLE IF EXISTS %s ;\" %(tablename)\n    conn.execute(drop_table)        \n    \n    for df in pd.read_csv(filename, chunksize=chunksize, iterator=True, encoding='utf-8'):\n        df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) # Remove spaces from columns\n        df.index += index_start\n        j+=1\n        if j<= 50:\n            df.to_sql(tablename, disk_engine, if_exists='append')\n            if j%10 == 0:\n                print ('{} Seconds: Completed rows {}'.format((datetime.now()-start).total_seconds(),j*chunksize))\n            index_start = df.index[-1] + 1\n        else:\n            print(\"Limiting Data for exploratory analysis\")\n            break\n    \n    # Created indexes on all id columns. this is a life saver\n    index_columns = [col for col in df.columns if col.find(\"_id\")>-1]\n    for col in index_columns:\n        create_indexes=\"CREATE INDEX index_%s on %s (%s);\" %(col+\"_\"+tablename,tablename,col)\n        conn.execute(create_indexes)        ","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"1c2dc23a6370c4fa25ba35efcbfd84dd2e335d3f"},"execution_count":18},{"cell_type":"code","outputs":[],"source":"# Read CSV into SQLite database\nlist_files1 = [file  for file in os.listdir(app_input) if file.find(\".csv\") != -1 ]\nprint (list_files1)\nfor file in list_files1:    \n    tablename = file.split(\".\")[0]\n    print('\\n Started Processing Table {}'.format(tablename))\n    filename = app_input + \"/\"+file\n    #print(filename)\n    read_large_csv_to_sqlite(filename,conn,tablename)\n    print (' Processing of Table {} ended'.format(tablename))","metadata":{"_execution_state":"idle","_uuid":"c5784fffe7ac74627f00df734f09ee7adf2f06e2"},"execution_count":19},{"cell_type":"markdown","outputs":[],"source":"## Approach 3: read csv using native csv reader and write to sqlite in batches \n#### pd.read_csv fails when the csv file size was 2 GB or greater\n#### code for this approach is beyond the scope of current project. Therefore, skipping it.","execution_count":null,"metadata":{"_uuid":"41d71b81698246108e880c9c3eb13fba7c7f958f"}},{"cell_type":"markdown","outputs":[],"source":"**Section 3**\n--","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"03a35a45702c4b696908b5a8b4f7ecba4d01bc3b"}},{"cell_type":"code","outputs":[],"source":"\"\"\"The table name is self-explanatory\"\"\"\n# Explore Aisles\naisles = pd.read_sql_query(\"SELECT * FROM aisles LIMIT 5 ;\", conn)\nprint('Total aisles: {}'.format(aisles.shape[0]))\nprint(\"Aisles Table\\n\",aisles.head(),\"\\n\\n\")\n\n\n# Explore Departments\ndepartments = pd.read_sql_query(\"SELECT * FROM departments LIMIT 5 ;\", conn)\nprint('Total departments: {}'.format(departments.shape[0]))\nprint('Departments Table\\n',departments.head(),\"\\n\\n\")\n\n# Explore Products csv\nproducts = pd.read_sql_query(\"SELECT * FROM products LIMIT 5 ;\", conn)\nprint('Total products: {}'.format(products.shape[0]))\nprint(\"Products Table\\n\",products.head(),\"\\n\\n\")\n\n# Explore orders csv\norders = pd.read_sql_query(\"SELECT * FROM orders LIMIT 5 ;\", conn)\nprint('Total orders: {}'.format(orders.shape[0]))\n\"\"\" This table contains the order and user linkage. Also, this table contains the day of week, \n  hour_of_day kind of fields\"\"\"\nprint(\"Orders Table\\n\",orders.head(5),\"\\n\\n\")\n\n# Explore orders_train csv\norders_products_train = pd.read_sql_query(\"SELECT * FROM order_products__train LIMIT 5 ;\", conn)\nprint('Total orders for training: {}'.format(orders_products_train.shape[0]))\n\"\"\" This table contains linkage between product table and the orders table. \n  Also, this table will be usefule for prediction for reorder\"\"\" \nprint(\"Orders_products_train\\n\",orders_products_train.head(5),\"\\n\\n\")\n\n# Explore order_product_prior.csv\norders_products_prior = pd.read_sql_query(\"SELECT * FROM order_products__prior LIMIT 5;\", conn)\n\"\"\" This table contains linkage between product table and the orders table. \n  Also, this table will be usefule for prediction for reorder\"\"\" \nprint('Total prior orders: {}'.format(orders_products_prior.shape[0]))\nprint(\"orders_products_prior\\n\",orders_products_prior.head(5),\"\\n\\n\")\n\ndel aisles,departments,products,orders,orders_products_train,orders_products_prior","metadata":{"_uuid":"e7997e06058ae8a486b87d648fb8f803fe775f36","_execution_state":"idle","_cell_guid":"7186da4a-9c41-bdd2-d88c-b899f7b29099"},"execution_count":20},{"cell_type":"code","outputs":[],"source":"# Combine aisles, departments and products to create goods table\n\"\"\"The purpose of creating goods table is to analyze combinations of aisle, department and products\"\"\"\nstart = datetime.now()\ndrop_goods_table=\"\"\" DROP TABLE IF EXISTS goods;\"\"\"\nconn.execute(drop_goods_table)\njoin_prod_dep_sql = \"\"\"\\\n    CREATE TABLE goods AS\n    SELECT p.*, d.department, a.aisle\n    FROM products p\n    INNER JOIN departments d ON p.department_id = d.department_id\n    INNER JOIN aisles a ON p.aisle_id = a.aisle_id;\n    \"\"\"\nconn.execute(join_prod_dep_sql)\ngoods = pd.read_sql_query(\"SELECT * FROM goods Limit 5;\", conn)\ngoods_step_time = datetime.now()\nprint('\\nTotal Time taken to delete and create goods table {}\\n'.format((goods_step_time-start).total_seconds()))\nprint(\"Goods Table \\n\", goods.head(),\"\\n\\n\")\n\n\n#Combine orders and the orders_prior dataframe\n#creating indexes earlier reduced time from 8 minutes to 3 minutes\ndrop_orders_combined_table=\"\"\" DROP TABLE IF EXISTS orders_combined;\"\"\"\nconn.execute(drop_orders_combined_table)\njoin_ordProdPrior_sql = \"\"\"\\\n    CREATE TABLE orders_combined AS\n    SELECT o.*, op.product_id, op.add_to_cart_order,op.reordered\n    FROM orders o\n    INNER JOIN order_products__prior op ON o.order_id = op.order_id;\n    \"\"\"\nconn.execute(join_ordProdPrior_sql)\norders_combined = pd.read_sql_query(\"SELECT * FROM orders_combined Limit 5;\", conn)\n\nindex_columns = [col for col in orders_combined.columns if col.find(\"_id\")>-1]\nfor col in index_columns:\n    create_indexes=\"CREATE INDEX index_%s on orders_combined (%s);\" %(col+\"_orders_combined\",col)\n    conn.execute(create_indexes)        \n        \norders_combined_time = datetime.now()\nprint('\\nTotal Time taken to delete and create orders_combined table {}\\n'.format((orders_combined_time-goods_step_time).total_seconds()))\nprint(\"Orders_Combined Table \\n\", orders_combined.head(),\"\\n\\n\")\n\n\n#create datamart with combined tables\ndrop_prior_datamart_table=\"\"\" DROP TABLE IF EXISTS prior_datamart;\"\"\"\nconn.execute(drop_prior_datamart_table)\njoin_prior_datamart_sql = \"\"\"\\\n    CREATE TABLE prior_datamart AS\n    SELECT o.*, gd.product_id, gd.product_name,gd.department,gd.aisle\n    FROM orders_combined o\n    INNER JOIN goods gd\n    ON o.product_id = gd.product_id;\n    \"\"\"\nconn.execute(join_prior_datamart_sql)\nprior_datamart = pd.read_sql_query(\"SELECT * FROM prior_datamart Limit 5;\", conn)\n\nindex_columns = [col for col in orders_combined.columns if col.find(\"_id\")>-1]\nfor col in index_columns:\n    create_indexes=\"CREATE INDEX index_%s on prior_datamart (%s);\" %(col+\"_prior_datamart\",col)\n    conn.execute(create_indexes)        \n        \nprior_datamart_time = datetime.now()\nprint('\\nTotal Time taken to delete and create prior_datamart table {}\\n'.format((prior_datamart_time-orders_combined_time).total_seconds()))\nprint(\"prior_datamart Table \\n\", prior_datamart.head(),\"\\n\\n\")\n\ndel goods, orders_combined,prior_datamart\n","metadata":{"_uuid":"dc77c236638380961e655a2433dc69dddc990e76","_execution_state":"idle","_cell_guid":"d0b40b64-4722-4fdb-de8e-07bfbbd3d80b"},"execution_count":21},{"cell_type":"code","outputs":[],"source":"\"\"\"\nUnivariate Analysis\n1. When do people order (Distribution of Time of Day) ?\n2. Day of Week (Distribution of day_of_week)?\n3. When do they order again (Distribution of Time Since Prior Order)?\n4. How many prior orders are there (Distribution of Reorders)?\n\"\"\"\n\n#read the data from sqlite database to dataframe\norders = pd.read_sql_query(\"SELECT order_id, order_dow,days_since_prior_order,order_hour_of_day\\\n                            FROM orders;\", conn)\ngoods = pd.read_sql_query(\"SELECT product_id,product_name,department,aisle FROM goods;\", conn)\nprior_datamart =  pd.read_sql_query(\"SELECT order_id, order_dow,days_since_prior_order,\\\n                                    order_hour_of_day,\\\n                                    product_id, product_name,department, aisle,reordered\\\n                                    FROM prior_datamart;\", conn)\n                                     \n\ntemp_df02 = pd.DataFrame(prior_datamart.groupby(['product_name']).agg({'order_id':pd.Series.nunique})\n                         .rename(columns={'order_id':'cnt_ord_by_prod'})).reset_index()\ntemp_df03 = pd.DataFrame(prior_datamart.groupby(['department']).agg({'order_id':pd.Series.nunique})\n                         .rename(columns={'order_id':'cnt_ord_by_dep'})).reset_index()\ntemp_df04 = pd.DataFrame(prior_datamart.groupby(['aisle']).agg({'order_id':pd.Series.nunique})\n                         .rename(columns={'order_id':'cnt_ord_by_aisle'})).reset_index()\n\ntop_10_products = temp_df02.nlargest(10,'cnt_ord_by_prod')['product_name']\ntop_10_departments = temp_df03.nlargest(10,'cnt_ord_by_dep')['department']\ntop_10_aisle = temp_df04.nlargest(10,'cnt_ord_by_aisle')['aisle']\n\ntemp_df05= prior_datamart[prior_datamart['product_name'].isin(top_10_products)]\ntemp_df06= prior_datamart[prior_datamart['department'].isin(top_10_departments)]\ntemp_df07= prior_datamart[prior_datamart['aisle'].isin(top_10_aisle)]\n\n\ndays = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\norders['day_of_week']=pd.Series([days[dow] for dow in orders['order_dow']]) \nprior_datamart['day_of_week']=pd.Series([days[dow] for dow in prior_datamart['order_dow']]) \n\nplt.close('all')\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax7)) = plt.subplots(nrows=4,ncols=2)\nfig.set_size_inches(30,36)\nax1=sns.countplot(x=\"day_of_week\", ax=ax1,  data=orders,palette=\"Blues_d\")        \nax2=sns.countplot(x=\"days_since_prior_order\",ax=ax2,   data=orders,palette=\"Reds_d\")\nax3=sns.countplot(x=\"order_hour_of_day\",ax=ax3,   data=orders,palette=\"Greens_d\")\nax4=sns.countplot(x=\"reordered\",ax=ax4, data=prior_datamart,palette=\"Blues_d\")\nax5=sns.countplot(x=\"product_name\",ax=ax5, data=temp_df05,palette=\"Blues_d\")\nax6=sns.countplot(x=\"department\",ax=ax6, data=temp_df06,palette=\"Blues_d\")\nax7=sns.countplot(x=\"aisle\",ax=ax7, data=temp_df07,palette=\"Blues_d\")\nfig.tight_layout(h_pad=4,w_pad=4,pad=4)\n\ndel temp_df02,temp_df03,temp_df04,temp_df05,temp_df06,temp_df07\n","metadata":{"_uuid":"6beb7c5f4797f4a13ba779c12fdcac4da42a4551","_execution_state":"idle"},"execution_count":26},{"cell_type":"code","outputs":[],"source":"\"\"\"\nBivariate Analysis\n1. When do people order (Distribution of Time of Day by orders) ?\n2. Day of Week (Distribution of day_of_week by orders)?\n\"\"\"\ntemp_df_01 = pd.DataFrame(orders.groupby(['day_of_week','order_hour_of_day'])\n                          .agg({'order_id':pd.Series.nunique})\n                          .rename(columns={'order_id':'count_of_orders'})).reset_index()\nplt.close('all')\nsns.factorplot(x=\"order_hour_of_day\", y=\"count_of_orders\",\n               col=\"day_of_week\", data=temp_df_01, kind=\"swarm\",col_wrap=3,size=5);\n\ndel temp_df_01","metadata":{"_uuid":"45b720f5047e0afda0f31eb58ed572a97f5002e0","_execution_state":"idle"},"execution_count":27},{"cell_type":"code","outputs":[],"source":"\"\"\"\nBivariate Analysis\nMondays and Tuesdays are busy days. It will be interesting to look for type of products ordered on Monday and Tuesdays\nduring the peak time\n\"\"\"\ntemp_groupby_01= prior_datamart.groupby(['day_of_week','order_hour_of_day','product_name',\n                                         'reordered']).agg({'order_id':\n                                                            pd.Series.nunique}).rename(columns={'order_id':\n                                                                                                'count_of_reorders'})\ntemp_groupby_02 = temp_groupby_01['count_of_reorders'].groupby(level=0, group_keys=False)\n\ntemp_df_01 = pd.DataFrame(temp_groupby_01).reset_index()\ntemp_df_02 = pd.DataFrame(temp_groupby_02.nlargest(50)).reset_index()\n\ntop_10_products_by_DayAndTime = temp_df_02[temp_df_02['reordered']==1]\nprint(\"\\n Top 10 products for Day and Time Combination\\n\",top_10_products_by_DayAndTime.head(5))\n\n#Limit data to only those products which are in top 10 category by any time and day combination\ntemp_df_01 = temp_df_01[temp_df_01['product_name'].isin(top_10_products_by_DayAndTime['product_name'])]\n\n#Monday and Tuesday are of interest\n#temp_df_01 = temp_df_01[temp_df_01['day_of_week'].isin(['Monday','Tuesday'])] \n\n#Most orders are between 6 and 20\n#temp_df_01 = temp_df_01[(temp_df_01['order_hour_of_day'] >= 6) & (temp_df_01['order_hour_of_day'] <= 20) ]\n\nplt.close('all')\ng = sns.factorplot(x='product_name', y='count_of_reorders',\n                   #col=\"day_of_week\", data=temp_df_01[temp_df_01['reordered']==1], kind=\"swarm\",col_wrap=3,size=5);\n                   col=\"day_of_week\", data=temp_df_01, kind=\"swarm\",col_wrap=3,size=5);\ng.set_xticklabels(rotation=90)","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"4efa85d3953af17372c011fadec06ab0ba98e345"}},{"cell_type":"code","outputs":[],"source":"\"\"\"\nBivariate Analysis\n1. When do people order (Distribution of Time of Day by orders) ?\n2. Day of Week (Distribution of day_of_week by orders)?\n\"\"\"\ntemp_df_01 = pd.DataFrame(prior_datamart.groupby(['day_of_week','order_hour_of_day','reordered'])\n                          .agg({'order_id':pd.Series.nunique})\n                          .rename(columns={'order_id':'count_of_reorders'})).reset_index()\nplt.close('all')\nsns.factorplot(x='order_hour_of_day', y='count_of_reorders',\n               col=\"day_of_week\", data=temp_df_01[temp_df_01['reordered']==1], kind=\"swarm\",col_wrap=3,size=5);\n\ndel temp_df_01","metadata":{"_uuid":"45b720f5047e0afda0f31eb58ed572a97f5002e0","_execution_state":"busy"},"execution_count":null},{"cell_type":"code","outputs":[],"source":"\"\"\"\nBivariate Analysis\nMondays and Tuesdays are busy days. It will be interesting to look for type of products ordered on Monday and Tuesdays\nduring the peak time\n\"\"\"\ntemp_groupby_01= prior_datamart.groupby(['day_of_week','order_hour_of_day','product_name',\n                                         'reordered']).agg({'order_id':\n                                                            pd.Series.nunique}).rename(columns={'order_id':\n                                                                                                'count_of_reorders'})\ntemp_groupby_02 = temp_groupby_01['count_of_reorders'].groupby(level=0, group_keys=False)\n\ntemp_df_01 = pd.DataFrame(temp_groupby_01).reset_index()\ntemp_df_02 = pd.DataFrame(temp_groupby_02.nlargest(50)).reset_index()\n\ntop_10_products_by_DayAndTime = temp_df_02[temp_df_02['reordered']==1]\nprint(\"\\n Top 10 products for Day and Time Combination\\n\",top_10_products_by_DayAndTime.head(5))\n\n#Limit data to only those products which are in top 10 category by any time and day combination\ntemp_df_01 = temp_df_01[temp_df_01['product_name'].isin(top_10_products_by_DayAndTime['product_name'])]\n\n#Monday and Tuesday are of interest\n#temp_df_01 = temp_df_01[temp_df_01['day_of_week'].isin(['Monday','Tuesday'])] \n\n#Most orders are between 6 and 20\n#temp_df_01 = temp_df_01[(temp_df_01['order_hour_of_day'] >= 6) & (temp_df_01['order_hour_of_day'] <= 20) ]\n\nplt.close('all')\ng = sns.factorplot(x='product_name', y='count_of_reorders',\n                   #col=\"day_of_week\", data=temp_df_01[temp_df_01['reordered']==1], kind=\"swarm\",col_wrap=3,size=5);\n                   col=\"day_of_week\", data=temp_df_01, kind=\"swarm\",col_wrap=3,size=5);\ng.set_xticklabels(rotation=90)","metadata":{"_uuid":"45b720f5047e0afda0f31eb58ed572a97f5002e0","_execution_state":"busy"},"execution_count":null}],"nbformat_minor":1,"metadata":{"_change_revision":0,"_is_fork":false,"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.0"},"anaconda-cloud":{},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}