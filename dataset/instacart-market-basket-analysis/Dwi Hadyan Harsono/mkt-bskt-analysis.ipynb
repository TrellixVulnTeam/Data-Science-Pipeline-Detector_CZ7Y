{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Market Basket Analysis\n-----\n### Goal : \n* Given all past orders, which dept that always come together ?\n* once known, which products to be specific on each od these dept are always bought together ?\n\n\n### Why?\n* So business can reorganize the store layout & run promotional campaign to bundle these item together. [Reference](https://www.kaggle.com/datatheque/association-rules-mining-market-basket-analysis)\n\n\n<br/>\n\n-----\n\n### Proposed Solution :\n1. We can answer part1 question using concept called \"Association Rules\"\n    * In short, calculate each pair of items relationships (lift). say we have item A & B.\n        * if lift > 1, A & B occur together more often than random\n        * if lift = 1, A & B occur together only by chance (random)\n        * if lift < 1, A & B occur together less often than random\n        \n    * calculate lift(A,B) = Pr(A & B bought together) / (Pr(A bought) * Pr(B bought))\n\n<br/>\n\n2. Formal answer : \n    * Pr : Probability of...\n\n```\nStep1 : \ncalculate support(A,B), support(A), support(B)\n\nsup(A,B) = Pr(A & B bought together) = Pr(A n B)\nsup(A)   = Pr(A is bought) = Pr(A)\nNote : sup(A,B) =/= sup(B,A)\n```\n\n\n```\nStep2 : \nuse support(A,B) to get Confidence(A,B) = Pr(B bought, given A alrd bought)\n\nCo(A,B) = Pr(B|A)\n        = Pr(B n A) / P(A).  https://images.app.goo.gl/Cb8Z6aQrtpBpeWTC8\n        = sup(A,B) / sup(A)\n        \nNote : Co(A,B) =/= Co(B,A)\n```\n\n```\nStep3 :\nuse Confidence(A,B) to get lift(A,B) = Likelihood of A & B bought together)\nLi(A,B) = Pr(A & B bought together) / ( Pr(A bought) * Pr(B bought) )\n        = Pr(A & B bought together) / Pr(A bought) / Pr(B bought)\n        = Pr(A n B) / Pr(A) / Pr(B)\n        = Pr(A n B) / Pr(B) / Pr(A)\n        = Pr(A|B) / Pr(A)\n        = Co(B,A) / sup(A)\n```\n\n```\n0 < sup(A,B) < 1\n0 < Co(A,B) < 1\n0 < Li(A,B) < inf\n```\n\n<br/>\n\n3. 0.01 min threshold is fine, meaning, out of 100 unique orders, 1 order has that pair \n* see [Apriori Algorithm](https://www.kaggle.com/datatheque/association-rules-mining-market-basket-analysis#Apriori-Algorithm) small example. we have 5 orders, and we measure by pair, hence the least pair (next to 0) is that \"it happen in onc order, out of 5\", hence 1/5\n\n<br/>\n\n4. How \"Association Rules\" works in order to find pair items that happen to be come together more frequent than random ? (simplified terms)\n* get each orders (where 1 order has many products inside)\n* compare 1 order with another order\n* does a particular pair of products always exist in each of the orders ?\n    * if yes, record. the more frequent this happens, the higher the 'lift' value \n    * if no, if its very less frequent than min_threshold, we throw that pair entirely\n* repeat to all order_id pairs.\n    * hence \"0 < max number of rows in output result < len(order_id) C 2\" . (if len(order_id)=10, -> 10C2)\n\n<br/>\n\n-----\n\n### Useful reads :\n* [Association Rules Explained](https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)","metadata":{}},{"cell_type":"code","source":"# First timer read here on [loading zip data issue](https://www.kaggle.com/dansbecker/finding-your-files-in-kaggle-kernels) \n# DO NOT RUN THIS CELL TOO OFTEN, WILL TAKE 2 MINS+ TO RELOAD DATA\n\n# basic packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# garbage collector to free up memory\nimport gc\ngc.enable()\n\n\n# ori, as in this original variable is to be preserved \n# ori_aisles = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/aisles.csv')\nori_dept = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/departments.csv')\nori_order_prod_prior = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/order_products__prior.csv')\n# ori_order_prod_train = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/order_products__train.csv')\n# ori_orders = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/orders.csv')\nori_products = pd.read_csv('../input/d/psparks/instacart-market-basket-analysis/products.csv')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:27:44.182517Z","iopub.execute_input":"2021-09-14T03:27:44.183119Z","iopub.status.idle":"2021-09-14T03:28:04.755784Z","shell.execute_reply.started":"2021-09-14T03:27:44.183081Z","shell.execute_reply":"2021-09-14T03:28:04.754713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtain\n* load dataset & see their size","metadata":{}},{"cell_type":"code","source":"# additional packages needed\nimport sys\nfrom itertools import combinations, groupby\nfrom collections import Counter\nfrom IPython.display import display\nimport pdb\n\n# load data needed\norders_prod_prior = ori_order_prod_prior\nproducts = ori_products\ndept = ori_dept\n\n# aisles = ori_aisles\n# orders_prod_train = ori_order_prod_train\n# orders = ori_orders\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:04.758027Z","iopub.execute_input":"2021-09-14T03:28:04.758914Z","iopub.status.idle":"2021-09-14T03:28:04.76615Z","shell.execute_reply.started":"2021-09-14T03:28:04.75885Z","shell.execute_reply":"2021-09-14T03:28:04.764753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def size(obj):\n    '''Input a variable object, Output size of the variable in MB '''\n    \n    return \"{0:2f} MB\".format(sys.getsizeof(obj) / (1000 * 1000))\n\n\n# sneak peek of order data\nprint(f\"orders_prod_prior dimensions : {orders_prod_prior.shape};   size : {size(orders_prod_prior)}\")\nprint(orders_prod_prior.head(3))\n\nprint(\"------------------\")\n\n# sneak peek of product data\nprint(f\"products dimensions : {products.shape};   size : {size(products)}\")\nprint(products.head(3))\n\nprint(\"------------------\")\n\n# sneak peek of product data\nprint(f\"departments dimensions : {dept.shape};   size : {size(dept)}\")\nprint(dept.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:04.768586Z","iopub.execute_input":"2021-09-14T03:28:04.769165Z","iopub.status.idle":"2021-09-14T03:28:04.829005Z","shell.execute_reply.started":"2021-09-14T03:28:04.769102Z","shell.execute_reply":"2021-09-14T03:28:04.828024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scrub\n* merge orders & products df first\n* Convert dept dataframe to format suitable for association rules (df to series), which is order_id as index & dept_id as value\n* Convert products dataframe to format suitable for association rules (df to series), which is order_id as index & product_id as value","metadata":{}},{"cell_type":"code","source":"# since every order have UNIQUE product_id (since add_to_cart_order represent qty of that product_id being ordered), \n# set every order have UNIQUE dept_id too (with add_to_cart_order as qty of that particular dept_id being ordered)\n\n\n# merge orders_prod_prior & products df, to get dept_id\norders_products = orders_prod_prior.merge(products, left_on='product_id', right_on='product_id')\n\nprint(orders_products.head(3))\nprint(f\"dimensions : {orders_products.shape};   size : {size(orders_products)}\")   ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:04.830636Z","iopub.execute_input":"2021-09-14T03:28:04.831193Z","iopub.status.idle":"2021-09-14T03:28:19.803745Z","shell.execute_reply.started":"2021-09-14T03:28:04.831151Z","shell.execute_reply":"2021-09-14T03:28:19.80242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get order_id & dept_id only\norders_products = orders_products[['order_id', 'department_id']]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:19.80523Z","iopub.execute_input":"2021-09-14T03:28:19.8056Z","iopub.status.idle":"2021-09-14T03:28:22.668815Z","shell.execute_reply.started":"2021-09-14T03:28:19.805562Z","shell.execute_reply":"2021-09-14T03:28:22.66751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing duplicates idea\nprint(orders_products[orders_products['order_id'] == 4]) # what we have now\nprint(orders_products[orders_products['order_id'] == 4].drop_duplicates(subset=['department_id'])) # what we hope to have as final result","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:22.670691Z","iopub.execute_input":"2021-09-14T03:28:22.671086Z","iopub.status.idle":"2021-09-14T03:28:22.780344Z","shell.execute_reply.started":"2021-09-14T03:28:22.671045Z","shell.execute_reply":"2021-09-14T03:28:22.779369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop dept_id duplicates using numpy vectorization\n\n# # sort df by order_id\n# orders_products = orders_products.sort_values(by=['order_id'])\n\n# set order_id as index\norders_products_array = orders_products.set_index('order_id')\n\n# convert the whole dataframe to numpy array pairs\norders_products_array = orders_products_array.reset_index().to_numpy()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:22.781656Z","iopub.execute_input":"2021-09-14T03:28:22.782228Z","iopub.status.idle":"2021-09-14T03:28:24.293248Z","shell.execute_reply.started":"2021-09-14T03:28:22.782184Z","shell.execute_reply":"2021-09-14T03:28:24.292105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%timeit   result : 41 s ± 13.5 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n# drop duplicates\norders_products_array_unique = np.unique(orders_products_array, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:28:24.295598Z","iopub.execute_input":"2021-09-14T03:28:24.296224Z","iopub.status.idle":"2021-09-14T03:29:30.835552Z","shell.execute_reply.started":"2021-09-14T03:28:24.296178Z","shell.execute_reply":"2021-09-14T03:29:30.83459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orders_products_array_unique","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:29:30.836982Z","iopub.execute_input":"2021-09-14T03:29:30.837501Z","iopub.status.idle":"2021-09-14T03:29:30.846874Z","shell.execute_reply.started":"2021-09-14T03:29:30.837462Z","shell.execute_reply":"2021-09-14T03:29:30.845329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert numpy array back to pandas df\norders_departments_id = pd.DataFrame(orders_products_array_unique, \n             columns=['order_id', \n                      'department_id'])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:29:30.848468Z","iopub.execute_input":"2021-09-14T03:29:30.848802Z","iopub.status.idle":"2021-09-14T03:29:30.866532Z","shell.execute_reply.started":"2021-09-14T03:29:30.848768Z","shell.execute_reply":"2021-09-14T03:29:30.865175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set orders_departments_id that is suitable for 'associate rules' analysis\norders_departments_id = orders_departments_id.set_index('order_id')['department_id'].rename('item_id')\nprint(orders_departments_id.head(5))\nprint(f\"dimensions: {orders_departments_id.shape};  \\nsize: {size(orders_departments_id)};   \\nunique_orders: {len(orders_departments_id.index.unique())};   \\nunique_items: {len(orders_departments_id.value_counts())}; \\ntype: {type(orders_departments_id)};\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:29:30.868382Z","iopub.execute_input":"2021-09-14T03:29:30.86883Z","iopub.status.idle":"2021-09-14T03:29:32.018605Z","shell.execute_reply.started":"2021-09-14T03:29:30.868756Z","shell.execute_reply":"2021-09-14T03:29:32.01705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert order df to series. order_id as index & product_id (renamed to item_id) as value\norders_products_id = ori_order_prod_prior.set_index('order_id')['product_id'].rename('item_id')   # take data from orders_products will error in get_item_pairs(). must take from orders.\nprint(orders_products_id.head(5))\nprint(f\"dimensions: {orders_products_id.shape};  \\nsize: {size(orders_products_id)};   \\nunique_orders: {len(orders_products_id.index.unique())};   \\nunique_items: {len(orders_products_id.value_counts())}; \\ntype: {type(orders_products_id)};\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:30:07.152869Z","iopub.execute_input":"2021-09-14T03:30:07.153305Z","iopub.status.idle":"2021-09-14T03:30:10.943774Z","shell.execute_reply.started":"2021-09-14T03:30:07.153267Z","shell.execute_reply":"2021-09-14T03:30:10.942541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore\n\n* Skipped entirely, as this analysis itself almost eats up 15GB ram\n* Will be covered [here](https://www.kaggle.com/dwihdyn/mkt-bskt-prediction/)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n* Functions to calculate Association Rule & its helpers\n* Input df that inside is order_id & product_id, Output every pair of product_id that has high chance it buys together (lift), same procedure for dept\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T07:54:45.869644Z","iopub.execute_input":"2021-08-23T07:54:45.870162Z","iopub.status.idle":"2021-08-23T07:54:45.8813Z","shell.execute_reply.started":"2021-08-23T07:54:45.870128Z","shell.execute_reply":"2021-08-23T07:54:45.880497Z"}}},{"cell_type":"code","source":"# helper function ensure we can calculate lift with no buffer due to dealing large dataset (data mgmt)\n\n\ndef freq(iterable):\n    '''Returns frequency counts for items & item pairs'''\n    \n    if type(iterable) == pd.core.series.Series:\n        return iterable.value_counts().rename(\"freq\")\n    else: \n        return pd.Series(Counter(iterable)).rename(\"freq\")\n\n\n    \ndef order_count(order_item):\n    '''Return no of unique orders'''\n    return len(set(order_item.index))\n\n\n\ndef get_item_pairs(order_item):\n    '''Return generator that yields item pairs, one at a time. (helps to facilitate large dataset)'''\n    \n    # input\n    # order_id     product_id\n    # 1             222\n    # 1             223\n    # 2             222\n    # 2             192\n    # output : array([1, 222], [1, 223], [2,222], [2,192])\n    order_item = order_item.reset_index().to_numpy()\n    \n    \n    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n        item_list = [item[1] for item in order_object]\n\n        for item_pair in combinations(item_list, 2):\n            yield item_pair\n        \n\n        \ndef merge_item_stats(item_pairs, item_stats):\n    '''Returns frequency & support associated with items'''\n    \n    return (item_pairs\n                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True)\n           )\n\n\n\ndef merge_targeted_category(rules, targeted_category):\n    '''Returns name associated with item'''\n    \n    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n               'confidenceAtoB','confidenceBtoA','lift']\n    rules = (rules\n                .merge(targeted_category.rename(columns={'targeted_category': 'itemA'}), left_on='item_A', right_on='item_id')\n                .merge(targeted_category.rename(columns={'targeted_category': 'itemB'}), left_on='item_B', right_on='item_id')\n            )\n    return rules[columns]               ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-14T02:59:06.005531Z","iopub.execute_input":"2021-09-14T02:59:06.005854Z","iopub.status.idle":"2021-09-14T02:59:06.017563Z","shell.execute_reply.started":"2021-09-14T02:59:06.005823Z","shell.execute_reply":"2021-09-14T02:59:06.016751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Association Rules function, to get lift() of all pairs\n\ndef association_rules(order_item, min_support):\n\n    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n\n    # 1) Calculate item frequency and support of each item (support(A))\n    item_stats             = freq(order_item).to_frame(\"freq\") \n    item_stats['support']  = item_stats['freq'] / order_count(order_item) # * 100  \n    \n\n\n\n    \n    # 2) Remove items from order_item df that items is below min support \n    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n    order_item             = order_item[order_item.isin(qualifying_items)]\n     \n \n    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n    \n    \n    \n    # 3) Since we want to know what PAIRS happen the most from EVERY ORDERS, there is no point including any orders data that has only 1 item inside it.\n    # Hence, Remove orders from order_item df that order with less than 2 items (remove any single-orders)\n    order_size             = freq(order_item.index)\n    qualifying_orders      = order_size[order_size >= 2].index\n    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n\n    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n\n    \n    \n    # 4) Since all single orders has been removed, Recalculate item frequency and support (repeat of step1, optimisation to remove any useless data & make code run faster)\n    item_stats             = freq(order_item).to_frame(\"freq\")\n    item_stats['support']  = item_stats['freq'] / order_count(order_item) # * 100\n\n    \n    \n    \n    \n    # 5) Get all unique items from all orders, pair them up, and count the frequency of each pair happen from all orders\n    # run \"Counter(item_pair_gen)\"\n    item_pair_gen          = get_item_pairs(order_item)  \n\n\n    \n    \n    \n    # 6) Calculate item pair frequency and support of each pair (support(A,B))\n    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) # * 100\n\n    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n\n\n    \n    \n    \n    # 7) Remove pairs from item_pairs that the pair supportAB  below min support (repeat of step2, optimisation to remove any useless data & make code run faster)\n    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n\n    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n            \n\n\n        \n        \n    # 8) Create table of association rules and compute relevant metrics (confidence & lift)\n    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n    item_pairs = merge_item_stats(item_pairs, item_stats)\n    \n    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n    \n    \n    # Return association rules sorted by lift in descending order\n    return item_pairs.sort_values('lift', ascending=False)\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-14T02:59:06.041013Z","iopub.execute_input":"2021-09-14T02:59:06.041451Z","iopub.status.idle":"2021-09-14T02:59:06.058791Z","shell.execute_reply.started":"2021-09-14T02:59:06.041402Z","shell.execute_reply":"2021-09-14T02:59:06.057482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.01 min threshold is fine, meaning, out of 100 unique orders, 1 order has that pair \n\n# associate rules : products\nproducts_rules = association_rules(orders_products_id, 0.01)  \n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T02:59:06.060761Z","iopub.execute_input":"2021-09-14T02:59:06.061089Z","iopub.status.idle":"2021-09-14T03:21:49.590685Z","shell.execute_reply.started":"2021-09-14T02:59:06.06106Z","shell.execute_reply":"2021-09-14T03:21:49.588891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1) Which products that's always bought together ?\n\n# Replace item ID with item name and display association rules\ntargeted_category   = products.rename(columns={'product_id':'item_id', 'product_name':'targeted_category'})\nproducts_rules_final = merge_targeted_category(products_rules, targeted_category).sort_values('lift', ascending=False)\n\n# display result on item pair, ascending order by \"lift\", only show lift > 1\nprint(products_rules_final[products_rules_final['lift'] > 1][['itemA', 'itemB', 'lift']].head(10))\nprint(f\"dimensions: {products_rules_final.shape};  \\nsize: {size(products_rules_final)};   \\nunique_orders: {len(products_rules_final.index.unique())};   \\nunique_items: {len(products_rules_final.value_counts())};\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:34:45.848764Z","iopub.execute_input":"2021-09-01T04:34:45.849094Z","iopub.status.idle":"2021-09-01T04:34:45.892211Z","shell.execute_reply.started":"2021-09-01T04:34:45.849065Z","shell.execute_reply":"2021-09-01T04:34:45.891178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# associate rules : departments\ndept_rules = association_rules(orders_departments_id, 0.01)  ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:34:45.893424Z","iopub.execute_input":"2021-09-01T04:34:45.893689Z","iopub.status.idle":"2021-09-01T04:35:35.822638Z","shell.execute_reply.started":"2021-09-01T04:34:45.893665Z","shell.execute_reply":"2021-09-01T04:35:35.821771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2) Which dept that's always bought together ?\n\n# Replace item ID with item name and display association rules\ntargeted_category   = dept.rename(columns={'department_id':'item_id', 'department':'targeted_category'})\ndept_rules_final = merge_targeted_category(dept_rules, targeted_category).sort_values('lift', ascending=False)\n\n\n# display result on item pair, ascending order by \"lift\"\nprint(dept_rules_final[dept_rules_final['lift'] > 1][['itemA', 'itemB', 'lift']].head(10))\nprint(f\"dimensions: {dept_rules_final.shape};  \\nsize: {size(dept_rules_final)};   \\nunique_orders: {len(dept_rules_final.index.unique())};   \\nunique_items: {len(dept_rules_final.value_counts())};\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:35:35.823835Z","iopub.execute_input":"2021-09-01T04:35:35.824156Z","iopub.status.idle":"2021-09-01T04:35:35.857788Z","shell.execute_reply.started":"2021-09-01T04:35:35.824125Z","shell.execute_reply":"2021-09-01T04:35:35.856692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## iNterpret\n* Jump to \"Call-To-Action\" section [here](https://dwihdyn.github.io/journals/6-mba-retail.html)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T07:54:45.882854Z","iopub.execute_input":"2021-08-23T07:54:45.88327Z","iopub.status.idle":"2021-08-23T07:54:45.89298Z","shell.execute_reply.started":"2021-08-23T07:54:45.883238Z","shell.execute_reply":"2021-08-23T07:54:45.891958Z"}}},{"cell_type":"code","source":"# # Scrub process \n# # naive way. DO NOT RUN THIS! here to just explain concept on how to tackele this problem the most reckless way\n\n# # empty df to store non-duplicate dept\n# unique_dept_tes = pd.DataFrame(columns = ['order_id', 'department_id'])\n\n\n# # function to remove duplicate\n# def remove_duplicate_depts_given_orderid(df, order_id):\n#     '''input dataframe & order_id, output that given order_id with UNIQUE dept_id'''\n    \n#     return df[df['order_id'] == order_id].drop_duplicates(subset=['department_id'])\n\n\n\n# # small version for order 2,3,4 only\n# i = 2\n# # while i <= tes['order_id'].max():\n# while i <= 4:    \n    \n#     # remove duplicate in that selected order_id\n#     temp_df = remove_duplicate_depts_given_orderid(tes, i)\n    \n#     # append temp_df to new df \n#     unique_dept_tes = unique_dept_tes.append(temp_df, ignore_index=True)\n\n    \n#     i = i + 1\n    \n#     if i == 10 or i == 100 or i == 1000 or i == 10000 or i == 100000 or i == 1000000 or i == 3000000 or i == 3421000 or i == 3421080:\n#         print(f\" order_id up to {i} out of {tes['order_id'].max()} is done, continuing............\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-01T04:35:35.864289Z","iopub.execute_input":"2021-09-01T04:35:35.86457Z","iopub.status.idle":"2021-09-01T04:35:35.875099Z","shell.execute_reply.started":"2021-09-01T04:35:35.864538Z","shell.execute_reply":"2021-09-01T04:35:35.874089Z"},"trusted":true},"execution_count":null,"outputs":[]}]}