{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello everyone! This is my solution for this month tabular playground. I learned a lot during my work on this dataset and from notebooks of other participants. Those two kernels were most helpfull and informative. Don't forget to check them too :D <br>\n[TPS-May Categorical EDA](https://www.kaggle.com/subinium/tps-may-categorical-eda) <br>\n[TPS May: RAPIDS](https://www.kaggle.com/ruchi798/tps-may-rapids)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport umap.umap_ as umap\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.metrics import log_loss\n\n# ML\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv').drop('id', axis=1)\ndata_test = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv').drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.concat([data_train, data_test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>EDA</h2>","metadata":{}},{"cell_type":"markdown","source":"<h3>Missing values</h3>","metadata":{}},{"cell_type":"code","source":"all_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there aren't any missing values in this dataset","metadata":{}},{"cell_type":"markdown","source":"<h3>Feature Description</h3>","metadata":{}},{"cell_type":"code","source":"data_test.describe().T.style.bar(subset=['mean', 'std'], color='#d65f5f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.describe().T.style.bar(subset=['mean', 'std'], color='#d65f5f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Target Distribution</h3>","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\n\nto_plot = data_train.value_counts('target')\n\nfig.add_trace(go.Pie(\n    labels = to_plot.index,\n    values = to_plot.values,\n    textinfo='label+percent'\n))\n\nfig.update_layout(\n    template='plotly_dark',\n    title_text = 'Target Distribution'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately we have huge disbalance in our target variable","metadata":{}},{"cell_type":"markdown","source":"<h3>Features Distribution</h3>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(\n    rows=10,\n    cols=5,\n    subplot_titles=data_train.columns,\n)\n\n# Add traces\ncolumns = data_train.drop('target', axis=1).columns.tolist()\n\nfor row in range(10):\n    for col in range(5):\n        column = columns.pop(0)\n        to_plot = data_train[column].value_counts()\n\n        fig.add_trace(go.Scatter(\n            x = to_plot.index,\n            y = to_plot.values,\n            name = column,\n            mode='lines'\n        ), col=col+1, row=row+1)\n\n        fig.update_yaxes(title='y', visible=False, showticklabels=False)\n\n        if(col+1 == 5):\n            break\n\nfig.update_layout(\n    height=1000,\n    width=700,\n    showlegend=False,\n    template='plotly_dark',\n)\nfig.update_annotations(font_size=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of zero values in every feature. I'm curious how much of dataset is filled with them.","metadata":{}},{"cell_type":"code","source":"to_plot = data_train.drop('target', axis=1).isin([0]).sum(axis=0)\npercent = np.array(to_plot)/100000 * 100\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x = to_plot.values,\n    y = to_plot.index,\n    orientation='h',\n    text = np.round(percent, 2),\n    textposition='outside',\n    marker={\n        'color': to_plot.values,\n        'colorscale': 'Purples',\n\n    }\n))\n\nfig.update_layout(\n    height=1000,\n    width=700,\n    template='plotly_dark',\n    title_text='Percent of zeros in every column'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Correlation</h3>","metadata":{}},{"cell_type":"code","source":"data_train_target_num = data_train.replace({'target': {'Class_1': 1, 'Class_2': 2, 'Class_3': 3, 'Class_4': 4}})\n\nplt.figure(figsize=(8, 12))\n\nheatmap = sns.heatmap(data_train_target_num.corr()[['target']].sort_values(by='target', ascending=False),\n                     vmin=-1, vmax=1, annot=True, cmap='Purples')\n\nheatmap.set_title('Linear correlation of features with target variable', fontdict={'fontsize': 18}, pad=16);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Conclusion</h3>","metadata":{}},{"cell_type":"markdown","source":"After some visualization and discussion couple of things come up to the light\n<ul>\n    <li>There aren't any missing values</li>\n    <li>Mean and standard deviation is fairly the same for train and test datasets</li>\n    <li>Target variable is disbalanced which can be a problem</li>\n    <li>Features are left skewed and nearly 60% of every feature is filled with zeros</li>\n    <li>Features show weak linear correlation with target variable</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<h2>Dimensionality Reduction</h2>\nThere is 50 features in our dataset. It's good opportunity to perform dimensionality reduction but first we gonna check if it's necessary to do so.","metadata":{}},{"cell_type":"markdown","source":"<h3>Dimensionality reduction using PCA </h3>","metadata":{}},{"cell_type":"code","source":"pca = PCA().fit(data_train.drop('target', axis=1))\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x = list(range(50)),\n    y = np.cumsum(pca.explained_variance_ratio_)\n))\n\nfig.update_layout(\n    template = 'plotly_dark',\n    title_text = 'PCA Performence',\n    xaxis_title = 'Number of components',\n    yaxis_title = 'Cumulative explained variance'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from scatter plot above variance decreasing quite fast. By the time PCA reduce number of features to the 30 we had lost almost 10% of the variance. It's definitely not worth it to reduce dimensionality of this dataset in order to create prediction model but still we can use dimensionality reduction to visualize our dataset. ","metadata":{}},{"cell_type":"code","source":"pca_vis = PCA(3)\nprojected = pca_vis.fit_transform(data_train.drop('target', axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_vis = pd.DataFrame(projected, columns=['x', 'y', 'z'])\ndf_vis['target'] = data_train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d(df_vis, x='x', y='y', z='z', color='target')\n\n# tight layout\nfig.update_layout(\n    template='plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA doesn't work very well but it doesn't mean that visualization is impossible. We gonna use other method to do so","metadata":{}},{"cell_type":"markdown","source":"<h3> Dimensionality reduction using umap </h3>","metadata":{}},{"cell_type":"code","source":"sample_data_train = data_train.sample(1000, random_state=42)\nscaled_sample_train = pd.DataFrame(StandardScaler().fit_transform(sample_data_train.drop('target', axis=1)))\nscaled_sample_target = sample_data_train.replace({'target': {'Class_1': 1, 'Class_2': 2, 'Class_3': 3, 'Class_4': 4}})['target'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reducer_2d = umap.UMAP(random_state=1)\nembedding_2d = reducer_2d.fit_transform(scaled_sample_train, scaled_sample_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_2d = pd.DataFrame(embedding_2d, columns=['x', 'y'])\ndf_test_2d['target'] = scaled_sample_target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 1]['x'],\n    y = df_test_2d[df_test_2d['target'] == 1]['y'],\n    mode='markers',\n    name='Class_1'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 2]['x'],\n    y = df_test_2d[df_test_2d['target'] == 2]['y'],\n    mode='markers',\n    name='Class_2'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 3]['x'],\n    y = df_test_2d[df_test_2d['target'] == 3]['y'],\n    mode='markers',\n    name='Class_3'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 4]['x'],\n    y = df_test_2d[df_test_2d['target'] == 4]['y'],\n    mode='markers',\n    name='Class_4'\n))\n\nfig.update_layout(\n    title_text = '2d dataset visualization using UMAP',\n    template = 'plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reducer_3d = umap.UMAP(random_state=42, n_components=3)\nembedding_3d = reducer_3d.fit_transform(scaled_sample_train, scaled_sample_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_3d = pd.DataFrame(embedding_3d, columns=['x', 'y', 'z'])\ndf_test_3d['target'] = scaled_sample_target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 1]['x'],\n    y = df_test_3d[df_test_3d['target'] == 1]['y'],\n    z = df_test_3d[df_test_3d['target'] == 1]['z'],\n    mode = 'markers',\n    name = 'Class_1',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 2]['x'],\n    y = df_test_3d[df_test_3d['target'] == 2]['y'],\n    z = df_test_3d[df_test_3d['target'] == 2]['z'],\n    mode = 'markers',\n    name = 'Class_2',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 3]['x'],\n    y = df_test_3d[df_test_3d['target'] == 3]['y'],\n    z = df_test_3d[df_test_3d['target'] == 3]['z'],\n    mode = 'markers',\n    name = 'Class_3',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 4]['x'],\n    y = df_test_3d[df_test_3d['target'] == 4]['y'],\n    z = df_test_3d[df_test_3d['target'] == 4]['z'],\n    mode = 'markers',\n    name = 'Class_4',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.update_layout(\n    title_text = '3d dataset visualization using UMAP',\n    template = 'plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our visualization looks much better, we can clearly see clouds of different classes. ","metadata":{}},{"cell_type":"markdown","source":"<h2> Prediction model creation </h2>","metadata":{}},{"cell_type":"code","source":"data_train_num = data_train\n\nX = data_train_num.drop('target', axis=1)\ny = data_train_num['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> XGBoost </h3>","metadata":{}},{"cell_type":"code","source":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = XGBClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = xgb_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        eval_metric = 'mlogloss',\n        early_stopping_rounds = 50, \n        verbose = 0\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred1 = test_pred/5\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Light Gradient Boost </h3>","metadata":{}},{"cell_type":"code","source":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_model = LGBMClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = lg_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        eval_metric = 'multi_logloss',\n        early_stopping_rounds = 50,\n        verbose = 0\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred2 = test_pred/5\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Catboost </h3>","metadata":{}},{"cell_type":"code","source":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_model = CatBoostClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = cat_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        early_stopping_rounds = 50,\n        verbose=0\n\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred3 = test_pred/10\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Submission </h2>","metadata":{}},{"cell_type":"code","source":"df_pred1 = pd.DataFrame(test_pred1)\ndf_pred2 = pd.DataFrame(test_pred2)\ndf_pred3 = pd.DataFrame(test_pred3)\ndata_sub = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv').drop(['Class_1', 'Class_2', 'Class_3', 'Class_4'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub1 = data_sub.copy()\n\ndata_sub1['Class_1'] = df_pred1[0]\ndata_sub1['Class_2'] = df_pred1[1]\ndata_sub1['Class_3'] = df_pred1[2]\ndata_sub1['Class_4'] = df_pred1[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub2 = data_sub.copy()\n\ndata_sub2['Class_1'] = df_pred2[0]\ndata_sub2['Class_2'] = df_pred2[1]\ndata_sub2['Class_3'] = df_pred2[2]\ndata_sub2['Class_4'] = df_pred2[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub3 = data_sub.copy()\n\ndata_sub3['Class_1'] = df_pred3[0]\ndata_sub3['Class_2'] = df_pred3[1]\ndata_sub3['Class_3'] = df_pred3[2]\ndata_sub3['Class_4'] = df_pred3[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sub3.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}