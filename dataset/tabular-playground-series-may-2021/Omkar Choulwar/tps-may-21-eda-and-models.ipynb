{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2> Tabular Playground Series-May 2021</h2>\n\n- For this competition, we are being provided with E-commerce data, which is of synthetic in nature and generated by CTGAN, a deep learning technique used to generate data, when the available one is not sufficient to be worked with. \n\n- In this kernel, I have tried to perform Exploratory Data Analysis followed by Some feature Enginnering, further worked with Modelling. \n\n- Models are being selected based on there accuracy and Selection of features is done based on SHAP values and those obtained from the Feature Importances. \n\n<h3><i>If you like or this kernel enhances your knowledges or provides some better insights then do upvote the kernel‚úî‚úî</i></h3>","metadata":{}},{"cell_type":"markdown","source":"<h3>\n<i>The kernel follows a set of steps which are listed as follows:\n    \n    \n1. Data Extraction and Exploration for data insights\n    \n2. Exploratory Data Analysis\n    \n3. Feature Selection\n    \n4. Feature Engineering\n    \n5. Model Development and Tuning\n    \n6. Final Model and Result Generation and Submission\n    \n</i>\n    \n</h3>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:Violet;\" align = 'center'><i>1. Data Extraction and Exploration</i></h1>","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Getting the data in dataframe. \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/test.csv')\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/train.csv')\nsample = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Id attribute is irrelevant and hence is decided to be dropped from both the sets of training and testing. Also the id in testing set is stored since it is required in result generation</h4>","metadata":{}},{"cell_type":"code","source":"##Since the target is categorical, we try encoding them to numerical format.\ntrain.drop('id',axis = 1,inplace = True)\nclass_dict = {'Class_0':0,'Class_1':1,'Class_2':2,'Class_3':3,'Class_4':4}\ntrain['target_encode'] = train['target'].map(class_dict)\n\ntrain.drop('target',axis = 1,inplace  =True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Since id is required for result generation, we store it and drop from the test table.\nid = test['id']\ntest.drop('id',axis = 1,inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;color:white;\" align = 'center'><i>NULL Value Checkingüìëüìç</i></h1>","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train.isnull())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> üìåAs from the above plot it can be clearly seen that there are no NULL/NaN values in the dataset and hence we are good to procced further to work on the dataset and modelling part</h3>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:SlateBlue;\" align = 'center'><i>2. Exploratory Data Analysisüéá‚ú®üìä</i></h1>\n\n- Here we try to explore to given training data to get the most of the information from it.\n- This is achieved by using various plots using Matplot and seaborn. Most time should be spent on this step since this step is crucial in determining the model accuracy since this is the data being fed to the model for its learning and hence should be wisely done","metadata":{}},{"cell_type":"code","source":"X = train.drop('target_encode',axis = 1)\ny = train['target_encode']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in X.columns:\n    sns.distplot(X[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><b>üìåüìå Some points that can be observed from above plots are: </b></h3>\n\n\n- **From the above plots it can be clearly seen that for almost all the features being selected, we have conclusion that most features have maximum value to be 0 as compared to others i.e the data provided is much of sparse nature**\n\n- Another observation could be done from the plots is that the data is discrete in nature and is of **right skewed**  nature. i.e the data distribution is more aligned to one side of the data instead of being evenly distributed. \n\n- This might affect our models being trained and hence the data needs to be preprocessed before proceeding for modelling.\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Some more Data Exploration</h2>\n\n- Here we try to visualize the features with negative values these features are being selected from then insights from the above distribution plots","metadata":{}},{"cell_type":"code","source":"l_neg = [19,30,31,32,35,38,39,42]\nfor i in l_neg:\n    name = 'feature_'+str(i)\n    sns.countplot(X[name])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>From above it is clear that, the target Class_2 is most frequent followed by class_3 and class_4 and then is class_1</h3>","metadata":{}},{"cell_type":"markdown","source":"<h1 style = 'background-color:Cornsilk' align = 'center'><i>3. Feature Selection üìãüìù </i></h1>\n\n\n1. Feature Selection is a necessary concept in Machine Learning since selection of proper and appropriate features leads to improving the models performance and its overall accuracy too\n2. Feature Selection is a must done thing when features are more in numbers in any given dataset.Here as can be seen, we have **51 features** and training any model on such large number of features lead to a complex model which is not desirable. \n3. Here we have performed two feature selection techniques, one is by **SHAP values** and other is by using the **feature_importances** attribute in the ExtraTreesClassifier","metadata":{}},{"cell_type":"code","source":" \nimport xgboost\nimport shap\n# load JS visualization code to notebook\nshap.initjs()\n# train XGBoost model\nX,y = X,y\nmodel = xgboost.train({\"learning_rate\": 0.01,'max_depth':4}, xgboost.DMatrix(X, label=y), 100)\n# explain the model's predictions using SHAP values\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.ensemble import ExtraTreesClassifier\nmdl = ExtraTreesClassifier().fit(X,y)\nl1 = []\nfor i,j in zip(X.columns,mdl.feature_importances_):\n    \n    if(j>0.02):\n        print(i,j)\n        l1.append(i)\n        \nl2 = [0,10,12,37,45,25,29]\nfor i in l2:\n    name = 'feature_'+str(i)\n    l1.append(name)\n    \nl1 = sorted(list(set(l1)))\n\nX1 = X[l1]\n\ntest1 = test[l1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1.shape,y.shape,test1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style = 'background-color: PaleGoldenRod' align = 'center'><i>Handling The Right Skewness in the Data</i></h1>\n\n- As mentioned earlier in the kernel, from the graphs it can be clearly seen that we are provided with right skewed data. So inorder to work with the given data, we need to apply some transformations to make the data uniform and useful for the model to be trained with.\n\n- This is done by using Three main transformations as follows:\n  - Log Transformation\n  - Squareroot Transformation\n  - Box-cox Transformation\n\n\n- All of them require positive data in input and of all, box-cox is efficient and provides similar accuarcy in transformation as in log transformation but is usually preferred to remove skewness from the data.\n \n- Here, I have used square root transformation to treat the skewness since we have lot of sparse data and hence other transformations aren't suitable to be applied here. ","metadata":{}},{"cell_type":"code","source":"\nXtemp = X.copy()\nl_neg = [19,30,31,32,35,38,39,42]\nlneg = []\nfor i in l_neg:\n    name = 'feature_'+str(i)\n    lneg.append(name)\n    \ncols = [x for x in Xtemp.columns if x not in lneg]\nfor i in cols:\n    Xtemp[i] = np.sqrt(Xtemp[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtemp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testt = test.copy()\nfor i in cols:\n    testt[i] = np.sqrt(testt[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>\nüìùüìùüìùSome important things to be considered here is that: \n\n- We should not always go with Feature Selection directly at times as can be seen here. But we should also look at the attributes nature and accordingly perform it. Here earlier an attempt was performed to select features but after certain trials, it is being recognised that all the features given are relevant and should be considered fo model building and no feature should be eliminated\n    \n</h3>","metadata":{}},{"cell_type":"markdown","source":"\n<h1 style=\"background-color:DodgerBlue;\" align = 'center'><i>4. Feature Transformation/ Engineering üõ†üîß</i></h1>\n\n- This Step in the Development of Efficient ML Model is performed in order to increase or improve model performance. This includes Scaling the data so that the data adapts to a single common range\n\n- Performing this step is more benefical if performed in case the data is being trained on linear models or the ones where distance is a metric considered for measuring in the model, since there exists a single range of values in the data, the accuracy is significantly affected.\n\n- Some Scalers used here are QuantileTransformation and RobustScaling. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer,RobustScaler\nqt = QuantileTransformer()\nrs = RobustScaler()\nX11 = qt.fit_transform(X)\nX2 = rs.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1 = qt.transform(test)\ntest2 = rs.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\nX_train1,X_test1,y_train1,y_test1 = train_test_split(X11,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2,X_test2,y_train2,y_test2 = train_test_split(Xtemp,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LightGreen;\"align = 'center'><i>5. Model Development and Parameter Tuning</i></h1>\n\n- Considering Different models starting with simple linear ones and then to complex ensemble methods.\n- Applying appropriate hyperparameter tuning so that the model does not overfits the data","metadata":{}},{"cell_type":"code","source":"#1. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nmdl = LogisticRegression().fit(X11,y)\nmdl.score(X_test1,y_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#. 2. XGBOOST\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,StackingClassifier\n\n\nm2 = XGBClassifier().fit(X11,y)\nm2.score(X_test1,y_test1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. LightGBM\nfrom lightgbm import LGBMClassifier\nmodel1 = LGBMClassifier(n_estimators = 10000,reg_alpha = 13.156371523046678, reg_lambda = 0.21096899232756502, colsample_bytree = 0.3, subsample = 0.23259307103110016, learning_rate = 0.013455439600227555, max_depth = 39, num_leaves = 305, min_child_samples = 211, min_child_weight = 9.748412613201383e-05,random_state= 13, objective= 'multiclass', metric = 'multi_logloss')\nmodel1.fit(X, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. LightGBM\nfrom lightgbm import LGBMClassifier\nmodel11 = LGBMClassifier(n_estimators = 10000,reg_alpha = 13.156371523046678, reg_lambda = 0.21096899232756502, colsample_bytree = 0.3, subsample = 0.23259307103110016, learning_rate = 0.013455439600227555, max_depth = 39, num_leaves = 305, min_child_samples = 211, min_child_weight = 9.748412613201383e-05,random_state= 13, objective= 'multiclass', metric = 'multi_logloss')\nmodel11.fit(X11, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\ny_pred2 = model1.predict_proba(X_test)\nlog_loss(y_test,y_pred2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\ny_pred21 = model11.predict_proba(X_test1)\nlog_loss(y_test1,y_pred21)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model1.predict_proba(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = model11.predict_proba(test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style = 'background-color:Lavender; 'align = 'center'><i>6. Result Generation</i></h1>","metadata":{}},{"cell_type":"code","source":"res = pd.DataFrame(pred1,columns = ['Class_1','Class_2','Class_3','Class_4'])\nres = pd.concat([id,res],axis = 1)\nres.to_csv('result.csv',index = False)\nres.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = 'background-color: BurlyWood;'>\n<b>Further Updates to the notebooks will be made soon!</b>\n<p> \n    \n<i>Any improvement Suggestions are always welcome !üòä</i>\n    </p>\n</h2>\n","metadata":{}}]}