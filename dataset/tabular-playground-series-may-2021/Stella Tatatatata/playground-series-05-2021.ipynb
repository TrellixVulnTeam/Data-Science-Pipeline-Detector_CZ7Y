{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T13:29:24.849159Z","iopub.execute_input":"2021-05-31T13:29:24.84961Z","iopub.status.idle":"2021-05-31T13:29:24.859833Z","shell.execute_reply.started":"2021-05-31T13:29:24.84957Z","shell.execute_reply":"2021-05-31T13:29:24.858849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. See Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\", index_col='id')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:24.861411Z","iopub.execute_input":"2021-05-31T13:29:24.861955Z","iopub.status.idle":"2021-05-31T13:29:25.241179Z","shell.execute_reply.started":"2021-05-31T13:29:24.861906Z","shell.execute_reply":"2021-05-31T13:29:25.240073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_num_cols = train.shape[0]\ntrain_num_idx = train.shape[1]\nprint(train.shape)\nprint(train_num_cols)\nprint(train_num_idx)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.243366Z","iopub.execute_input":"2021-05-31T13:29:25.243687Z","iopub.status.idle":"2021-05-31T13:29:25.249812Z","shell.execute_reply.started":"2021-05-31T13:29:25.243654Z","shell.execute_reply":"2021-05-31T13:29:25.248628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.2516Z","iopub.execute_input":"2021-05-31T13:29:25.251919Z","iopub.status.idle":"2021-05-31T13:29:25.525037Z","shell.execute_reply.started":"2021-05-31T13:29:25.251885Z","shell.execute_reply":"2021-05-31T13:29:25.523869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().all()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.526825Z","iopub.execute_input":"2021-05-31T13:29:25.527285Z","iopub.status.idle":"2021-05-31T13:29:25.550764Z","shell.execute_reply.started":"2021-05-31T13:29:25.527223Z","shell.execute_reply":"2021-05-31T13:29:25.549581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no NA cells in \"train.csv\".","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\", index_col='id')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.5521Z","iopub.execute_input":"2021-05-31T13:29:25.552409Z","iopub.status.idle":"2021-05-31T13:29:25.734496Z","shell.execute_reply.started":"2021-05-31T13:29:25.552377Z","shell.execute_reply":"2021-05-31T13:29:25.733499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_num_cols = test.shape[0]\ntest_num_idx = test.shape[1]\nprint(test.shape)\nprint(test_num_cols)\nprint(test_num_idx)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.735982Z","iopub.execute_input":"2021-05-31T13:29:25.736554Z","iopub.status.idle":"2021-05-31T13:29:25.74392Z","shell.execute_reply.started":"2021-05-31T13:29:25.736506Z","shell.execute_reply":"2021-05-31T13:29:25.742839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.746474Z","iopub.execute_input":"2021-05-31T13:29:25.74678Z","iopub.status.idle":"2021-05-31T13:29:25.946068Z","shell.execute_reply.started":"2021-05-31T13:29:25.74675Z","shell.execute_reply":"2021-05-31T13:29:25.945025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().all()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.947668Z","iopub.execute_input":"2021-05-31T13:29:25.947962Z","iopub.status.idle":"2021-05-31T13:29:25.957603Z","shell.execute_reply.started":"2021-05-31T13:29:25.947933Z","shell.execute_reply":"2021-05-31T13:29:25.956442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no NA cells in \"test.csv\".","metadata":{}},{"cell_type":"markdown","source":"# 2. EDA (Explanatory Data Analysis) ","metadata":{}},{"cell_type":"markdown","source":"First of all, visualize class distribution in \"train.csv\".","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# create new DataFrme from Numpy\nname = [\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]\ndf = pd.DataFrame(name, columns=[\"Name\"])\n\n# count every Class\ncnt = []\nclass_1 = train[train[\"target\"] == \"Class_1\"].shape[0]\ncnt.append(class_1)\nclass_2 = train[train[\"target\"] == \"Class_2\"].shape[0]\ncnt.append(class_2)\nclass_3 = train[train[\"target\"] == \"Class_3\"].shape[0]\ncnt.append(class_3)\nclass_4 = train[train[\"target\"] == \"Class_4\"].shape[0]\ncnt.append(class_4)\ndf[\"Class\"] = cnt\n\nsns.barplot(x=\"Name\", y=\"Class\", data=df)\nax1 = plt.subplot(111)\nax1 = sns.barplot(x=\"Name\", y=\"Class\", data=df)\nax1.set(title=\"train target Distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:25.959072Z","iopub.execute_input":"2021-05-31T13:29:25.959385Z","iopub.status.idle":"2021-05-31T13:29:26.220902Z","shell.execute_reply.started":"2021-05-31T13:29:25.959355Z","shell.execute_reply":"2021-05-31T13:29:26.219894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform data Standardization","metadata":{}},{"cell_type":"code","source":"max_value = 0\nmin_value = 0\nmax_list = train.max()\nmin_list = train.min()\n\nfor i in range(0,50):\n    if max_list[i] > max_value:\n        max_value = max_list[i]\n    if min_value > min_list[i]:\n        min_value = min_list[i]\n    \nprint(f\"Max value in train data: {max_value}\")\nprint(f\"Min value in train data: {min_value}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:26.222014Z","iopub.execute_input":"2021-05-31T13:29:26.222312Z","iopub.status.idle":"2021-05-31T13:29:26.270124Z","shell.execute_reply.started":"2021-05-31T13:29:26.222283Z","shell.execute_reply":"2021-05-31T13:29:26.269012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hist_plt(x):\n    plt.figure()\n    sns.displot(x, kde=False, bins=4)\n    plt.xlim(-10,70)\n    plt.ylim(0, 10000)\n    plt.show()\n\nfor i in range(0,50):\n    hist_plt(train.iloc[:,i])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:26.271504Z","iopub.execute_input":"2021-05-31T13:29:26.271887Z","iopub.status.idle":"2021-05-31T13:29:43.152223Z","shell.execute_reply.started":"2021-05-31T13:29:26.271847Z","shell.execute_reply":"2021-05-31T13:29:43.151255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add new features\ntrain_sum = train.sum(axis=1)\ntrain_sum = train_sum.rename(\"sum\")\ntrain_mean = train.mean(axis=1)\ntrain_mean = train_mean.rename(\"mean\")\ntrain_median = train.median(axis=1)\ntrain_median = train_median.rename(\"median\")\ntrain_std = train.std(axis=1)\ntrain_std = train_std.rename(\"std\")\n\ntrain_add = pd.concat([train, train_sum, train_mean, train_median, train_std], axis=1)\ntrain_add.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:43.153515Z","iopub.execute_input":"2021-05-31T13:29:43.153837Z","iopub.status.idle":"2021-05-31T13:29:46.745214Z","shell.execute_reply.started":"2021-05-31T13:29:43.153801Z","shell.execute_reply":"2021-05-31T13:29:46.744234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train_add[\"target\"])\ntrain_add[\"target\"] = le.transform(train[\"target\"])\ntrain_add.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:46.746667Z","iopub.execute_input":"2021-05-31T13:29:46.74727Z","iopub.status.idle":"2021-05-31T13:29:46.808571Z","shell.execute_reply.started":"2021-05-31T13:29:46.74719Z","shell.execute_reply":"2021-05-31T13:29:46.8075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nsns.heatmap(train_add.corr(), cmap='Reds', square=True, linewidths=0.5) #.corr is pandas function which can be caluculated correlation, annot=True shows value, fmt is format\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:46.80971Z","iopub.execute_input":"2021-05-31T13:29:46.809981Z","iopub.status.idle":"2021-05-31T13:29:49.675298Z","shell.execute_reply.started":"2021-05-31T13:29:46.809955Z","shell.execute_reply":"2021-05-31T13:29:49.674263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_mod = train_add.drop(\"target\", axis=1)\n\nscaler.fit(train_mod)\n# train_scaler and test_std are ndarray, need to transform to DataFrame\ntrain_scaler = scaler.transform(train_mod)\ntrain_scaler = pd.DataFrame(train_scaler)\ntarget = train_add[\"target\"].to_numpy()\ntrain_scaler[\"target\"] = target # add target column in train_scaler\nprint(train_scaler.describe())\n\n# Add new features in test data\ntest_sum = test.sum(axis=1)\ntest_sum = test_sum.rename(\"sum\")\ntest_mean = test.mean(axis=1)\ntest_mean = test_mean.rename(\"mean\")\ntest_median = test.median(axis=1)\ntest_median = test_median.rename(\"median\")\ntest_std = test.std(axis=1)\ntest_std = test_std.rename(\"std\")\n\ntest_add = pd.concat([test, test_sum, test_mean, test_median, test_std], axis=1)\ntest_add.describe()\ntest_scaler = scaler.transform(test_add) # scaler should be based on train data, not test data\ntest_scaler = pd.DataFrame(test_scaler)\nprint()\nprint(test_scaler.describe()) # std is not equal to 1.0 but no problem","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:49.676551Z","iopub.execute_input":"2021-05-31T13:29:49.676842Z","iopub.status.idle":"2021-05-31T13:29:50.730653Z","shell.execute_reply.started":"2021-05-31T13:29:49.676811Z","shell.execute_reply":"2021-05-31T13:29:50.729696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(train_scaler)\nfeatures = pca.transform(train_scaler)\npca_list = pca.explained_variance_ratio_\npca_df = pd.DataFrame(pca_list)\nprint(np.cumsum(pca_list)+[5])\nprint(list(np.cumsum(pca_list)+[5]))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:50.731858Z","iopub.execute_input":"2021-05-31T13:29:50.732159Z","iopub.status.idle":"2021-05-31T13:29:51.669654Z","shell.execute_reply.started":"2021-05-31T13:29:50.73212Z","shell.execute_reply":"2021-05-31T13:29:51.668441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\n\ndef PCA_plt(train_scaler, i):\n    pca = PCA(n_components=i)\n    pca.fit(train_scaler)\n    features = pca.transform(train_scaler)\n    pca_list = pca.explained_variance_ratio_\n    pca_list = np.insert(pca_list, 0, 0)\n    \n    fig = plt.figure(figsize=(12, 8))\n\n    title_ax1 = \"PCA Analysis for train data \" + \"num \" + str(i)\n    ax1 = fig.add_subplot(121)\n    ax1.scatter(features[:,0], features[:,1], alpha=0.5, c=list(train_add.loc[:,\"target\"]))\n    ax1.grid()\n    ax1.set_xlabel(\"PC1\")\n    ax1.set_ylabel(\"PC2\")\n    ax1.set(title=title_ax1)\n    \n    title_ax2 = \"Contribution Rate \" + \"num \" + str(i)\n    ax2 = fig.add_subplot(122)\n    ax2.plot(0, marker=\"o\")\n    ax2.plot(np.cumsum(pca_list), marker=\"o\")\n    ax2.grid()\n    ax2.set_xlabel(\"Number of Components\")\n    ax2.set_ylabel(\"Cumulative Contribution\")\n    ax2.set(title=title_ax2)\n    \n    plt.show()\n    \nfor i in range(2,54):\n    PCA_plt(train_scaler, i)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:29:51.671921Z","iopub.execute_input":"2021-05-31T13:29:51.672882Z","iopub.status.idle":"2021-05-31T13:32:40.440991Z","shell.execute_reply.started":"2021-05-31T13:29:51.672818Z","shell.execute_reply":"2021-05-31T13:32:40.44025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use CatBoost\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import LinearSVC, SVC\n\n#train_x = train_scaler.drop([\"target\"], axis=1)\ntrain_x = train_scaler.iloc[:,51:54]\ntrain_y = train_scaler[\"target\"]\n\nscore_logloss = []\n\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\nfor tr_idx, va_idx in kf.split(train_x):\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    \n    model = CatBoostClassifier(random_state=0)\n    model.fit(tr_x, tr_y)\n    \n    va_pred = model.predict_proba(va_x)\n    logloss = log_loss(va_y, va_pred)\n    \n    score_logloss.append(logloss)\n\nlogloss_mean = np.mean(score_logloss)\nprint(f'log loss score(mean): {logloss_mean:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:32:40.450231Z","iopub.execute_input":"2021-05-31T13:32:40.450506Z","iopub.status.idle":"2021-05-31T13:34:13.885333Z","shell.execute_reply.started":"2021-05-31T13:32:40.450479Z","shell.execute_reply":"2021-05-31T13:34:13.884252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_scaler.iloc[:,51:54]\nprediction = model.predict_proba(test_data)\nsubmission = pd.DataFrame(prediction, columns=['Class_1','Class_2','Class_3','Class_4'])\n\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\")\nsubmission[\"id\"] = sample_submission[\"id\"]\nsubmission = submission.reindex(columns=[\"id\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:46:33.598931Z","iopub.execute_input":"2021-05-31T13:46:33.5993Z","iopub.status.idle":"2021-05-31T13:46:34.300912Z","shell.execute_reply.started":"2021-05-31T13:46:33.599266Z","shell.execute_reply":"2021-05-31T13:46:34.299984Z"},"trusted":true},"execution_count":null,"outputs":[]}]}