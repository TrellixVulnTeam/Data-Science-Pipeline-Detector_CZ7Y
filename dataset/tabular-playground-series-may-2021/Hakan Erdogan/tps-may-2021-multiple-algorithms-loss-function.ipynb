{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#enc = preprocessing.OrdinalEncoder()\n#train.target = enc.fit_transform(train[['target']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['target']\nX_train = train.drop(['id','target'], axis=1)\nX_test = test.drop(['id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(10,20))\nsns.boxplot(data=X_train, orient=\"h\");","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\n\nscaler = preprocessing.StandardScaler().fit(X_test)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\n\nX_train_trn, X_train_tst, y_train_trn, y_train_tst = train_test_split( X_train, y_train, test_size=0.1)","metadata":{}},{"cell_type":"code","source":"\n# Precision: \n    # tp / (tp + fp)\n    # fp = typeI error\n    # True positive / Predicted condition positive\n    # Among the ones predicted as certain label, actually correct ones.\n    # The precision will be \"how many are correctly classified among that class\"\n    # “for all instances classified positive, what percent was correct?”\n    # Precision is the ability of a classifier not to label an instance positive that is actually negative. \n\n# Recall/Sensitivity/Power of a Test:\n    # tp / (tp + fn)\n    # fn = actual label does not belongs predicted label.\n    # True Predicted Positive / All Real Positives \n    # Correctly identifying (true pos) a label when sample belongs to that (all pos). \n    # Proportion of positives that are correctly identified\n    # TPR = true positive rate\n    # The recall means \"how many of this class you find over the whole number of element of this class\n    # What percent of the positive cases did you catch?\n    \n# Specificity =(True Negative rate) \n\n# F1 = 2 * (precision * recall) / (precision + recall)\n# F1 score reaches its best value at 1 and worst score at 0. \n# What percent of positive predictions were correct?\n\n# support in classification report in reality true instances.  The support is the number of occurrences of each class in y_true.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nclf_HistGradientBoostingClassifier = HistGradientBoostingClassifier()\n\nclf_HistGradientBoostingClassifier.fit(X_train, y_train)\n#predictions_b = clf_HistGradientBoostingClassifier.predict(X_test)\npredictions_p = clf_HistGradientBoostingClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_HistGradientBoostingClassifier.csv',index=False)\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf_HistGradientBoostingClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)\n#from sklearn.metrics import classification_report, log_loss\n\n#print('Simple clf.score method of the estimator:', clf_HistGradientBoostingClassifier.score(X_train, y_train), '\\n\\n')\n#print('Detailed Classification Report: \\n\\n',classification_report(y_train, clf_HistGradientBoostingClassifier.predict(X_train), zero_division=0))\n#print('LogLoss:',log_loss(y_train, clf_HistGradientBoostingClassifier.predict_proba(X_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Understanding Multi-class logarithmic loss with Pandas\n\ndf1 = pd.DataFrame(clf_HistGradientBoostingClassifier.predict_proba(X_train), columns=['Class_1','Class_2','Class_3','Class_4'])\ndf1['actuals'] = y_train\ndf1['predictions'] = clf_HistGradientBoostingClassifier.predict(X_train)\ndf1['nl1']=-np.log(df1['Class_1'])\ndf1['nl2']=-np.log(df1['Class_2'])\ndf1['nl3']=-np.log(df1['Class_3'])\ndf1['nl4']=-np.log(df1['Class_4'])\ndf1['Prediction Correct ?'] = (df1['actuals'] == df1['predictions'])*1\ndf1['nl_final'] = df1['nl1']*(df1['actuals']=='Class_1')+df1['nl2']*(df1['actuals']=='Class_2')+df1['nl3']*(df1['actuals']=='Class_3')+df1['nl4']*(df1['actuals']=='Class_4')\nprint('Correct Prediction #:',df1['Prediction Correct ?'].sum())\nprint('Correct Prediction %:',df1['Prediction Correct ?'].sum()/len(df1))\nprint('nl_final_average:',df1['nl_final'].sum()/len(df1))\n\ndf1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from lightgbm import LGBMClassifier\nclf_LGBMClassifier = LGBMClassifier()\nclf_LGBMClassifier.fit(X_train, y_train)\npredictions_p = clf_LGBMClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_LGBMClassifier.csv',index=False)\n\nscores = cross_val_score(clf_LGBMClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nclf_CatBoostClassifier = CatBoostClassifier(verbose=0)\nclf_CatBoostClassifier.fit(X_train, y_train)\npredictions_p = clf_CatBoostClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_CatBoostClassifier.csv',index=False)\n\nscores = cross_val_score(clf_CatBoostClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from xgboost import XGBClassifier\nclf_XGBClassifier = XGBClassifier() \nclf_XGBClassifier.fit(X_train, y_train)\npredictions_p = clf_XGBClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_XGBClassifier.csv',index=False)\n\nscores = cross_val_score(clf_XGBClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)","metadata":{}},{"cell_type":"markdown","source":"from sklearn.neural_network import MLPClassifier\nclf_MLPClassifier = MLPClassifier(random_state=1, max_iter=1000)\nclf_MLPClassifier.fit(X_train, y_train)\npredictions_p = clf_MLPClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_MLPClassifier.csv',index=False)\n\nscores = cross_val_score(clf_MLPClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)","metadata":{}}]}