{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Utils\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, make_scorer, accuracy_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom kmodes.kmodes import KModes\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_uniform\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import pairwise_kernels\nfrom sklearn.metrics.pairwise  import cosine_similarity\nfrom sklearn.metrics.pairwise import chi2_kernel\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A basic methodology for approacing TPG May competition","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/train.csv', sep=',')\ntest= pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/test.csv', sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.set_index('id')\ntest = test.set_index('id')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/sample_submission.csv', sep=',')\nsub_sample = sub_sample.set_index('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(train, x=\"target\",\n                   width=600, \n                   height=400,\n                   histnorm='percent',\n                   template=\"simple_white\"\n                   )\nfig.update_layout(title=\"Target Description\", \n                  font_family=\"San Serif\",\n                  titlefont={'size': 20},\n                  showlegend=True,\n                  legend=dict(\n                      orientation=\"v\", \n                      y=1, \n                      yanchor=\"top\", \n                      x=1.0, \n                      xanchor=\"right\"\n                  )                \n                 ).update_xaxes(categoryorder='total descending')#\nfig.update_traces( \n                  marker_line_width=1.5, opacity=0.99)\nfig.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Estimation model Log-loss vs Random Guessing","metadata":{}},{"cell_type":"markdown","source":"rewrite a function I've read in an article some time ago..","metadata":{}},{"cell_type":"code","source":"def calc_loss(class_perc, num):\n    \n    lst=[]\n    \n    for i,z in enumerate(class_perc):\n        lst = lst+[i for x in range(int(z*(num+1)))]\n        \n    preds=[]\n    \n    for i in range(num):\n        preds+=[class_perc]\n    \n    return (log_loss(lst,preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_loss([0.57497, 0.21420, 0.12593, 0.08490], 200000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example of how could be the distribution in classes according to the LB\ncalc_loss([0.60, 0.20, 0.10, 0.10], 200000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#target Map\ndict1 = dict(zip(list(train.target.unique()),range(4)))\ndict1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target']= train['target'].replace(dict1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = train.corr()\nfig = go.Figure(data= go.Heatmap(z=corr,\n                                 x=corr.index.values,\n                                 y=corr.columns.values,\n                                 zmin=-0.05,\n                                 zmax=0.05\n                                 )\n                )\nfig.update_layout(title_text='<b>Correlation Matrix<b>',\n                  title_x=0.5,\n                  titlefont={'size': 24},\n                  width=900, height=800,\n                  xaxis_showgrid=False,\n                  yaxis_showgrid=False,\n                  yaxis_autorange='reversed', \n                  paper_bgcolor=None,\n                  )\nfig.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train.columns[:-1]\ntarget = train['target']\ntrain = train[cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[cols], test[cols]], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_df = pd.DataFrame(df.nunique()).reset_index()\nunique_df.columns=['features','count']\n\nfig1 = px.bar(unique_df, y='count', x=cols)\n\nfig1.update_layout(title='Feature cardinality in train+test set',\n                  xaxis_title='features',\n                  yaxis_title='# unique values',\n                  titlefont={'size': 28, 'family':'Serif'},\n                  template='simple_white',\n                  showlegend=True,\n                  width=900, height=500)\nfig1.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train==0).mean().plot(kind='bar', figsize=(15, 5), title='# zeros on total')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rule of thumb seems to be: If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and â€“ 0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed.","metadata":{}},{"cell_type":"code","source":"df.skew().plot(kind='bar', figsize=(15, 5), title='# Skew')\nplt.ylim(0,15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=df\ndf2[df2<0]=0\ndf2 = np.log(df2+1)\ndf2.skew().plot(kind='bar', figsize=(15, 5), title='# skew after log+1 trasformation')\nplt.ylim(0,15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_skew = pd.DataFrame(df2.skew()).reset_index()\ndf_skew.columns=['features','skew']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_skew[df_skew['skew']<1.1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first attempt is to consider as numeric features the one with skewness <1.5 and as categorical all the other\nn=1.1\nlow =list(df_skew[df_skew['skew']<=n]['features'])\nhigh = list(df_skew[df_skew['skew']>n]['features'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=200, max_features='sqrt')\nclf = clf.fit(train, target)\nfeatures = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training set log+1 trasformation\ntrain[train<0]=0\ntest[test<0]=0\nfor l in low:\n    train[l] = np.log(train[l]+1)\n    test[l] = np.log(test[l]+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[high] = train[high].astype('category')\ntest[high] = test[high].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Optuna with Catboost","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial , data = train , target = target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n            test_size = 0.028059109276941666 , random_state = 2)\n\n    params = {'iterations':20000,\n              'depth': trial.suggest_int(\"depth\", 3, 80),\n              'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 0.0001, 25, log=True),\n              'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0, 100),\n              'auto_class_weights':trial.suggest_categorical('auto_class_weights', [None,'Balanced','SqrtBalanced']),\n              'grow_policy': 'Lossguide',\n              'loss_function':trial.suggest_categorical(\"loss_function\", ['MultiClassOneVsAll', 'MultiClass']),\n              'bootstrap_type':trial.suggest_categorical(\"bootstrap_type\", ['Poisson']),\n              'use_best_model':True,\n              'task_type':'GPU', \n              'cat_features':high,\n              'learning_rate': trial.suggest_uniform('learning_rate' , 1e-5 , 1.0),\n              'max_bin': trial.suggest_int('max_bin', 5, 700),\n              'verbose':False,\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 600),\n              'subsample': trial.suggest_uniform('subsample' , 1e-5 , 1.0),\n              'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 2, 15),\n             }\n    model = CatBoostClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , \\\n             verbose = False)\n    preds =  model.predict_proba(test_x)\n    lgl = log_loss(test_y , preds)\n    return lgl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study = optuna.create_study(direction = 'minimize' , study_name = 'cb')\n#study.optimize(objective , n_trials = 100)\n#print('numbers of the finished trials:' , len(study.trials))\n#print('the best params:' , study.best_trial.params)\n#print('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use isolation Forest to clean the training set","metadata":{}},{"cell_type":"code","source":"iso = IsolationForest(n_estimators=500,contamination=0.003, random_state=1)\nyhat = iso.fit_predict(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['anomaly']=yhat\ntrain_clean=train.loc[train['anomaly']!=-1]\ntrain_clean = train_clean.drop(columns=['anomaly'])\ntrain_clean.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_index=list(train_clean.index)\ntarget_clean = target.iloc[train_index]\ntarget_clean.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_cb ={\n    'depth': 3, \n    'l2_leaf_reg': 4.287566030099442, \n    'bagging_temperature': 27.174417642203863, \n    'auto_class_weights': None, \n    'loss_function': 'MultiClassOneVsAll',\n    'eval_metric': 'MultiClassOneVsAll',\n    'grow_policy': 'Lossguide',\n    'bootstrap_type': 'Poisson',\n    'cat_features': high,\n    'iterations':10000,\n    'max_bin': 484, \n    'min_data_in_leaf': 414,\n    'task_type':'GPU',\n    'subsample': 0.13534551086578891,\n    'max_ctr_complexity':10\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"markdown","source":"using np.clip function to keep probabilities between 0.025â€“0.975. In this case,there won't be a big growth of the log loss funcion in case of wrong predictions.","metadata":{}},{"cell_type":"code","source":"preds = pd.DataFrame(index=test.index)\noof_preds = np.zeros(train_clean.shape[0])\nkf = StratifiedKFold(n_splits = 7, random_state = 22 , shuffle = True)\nroc = []\n\nn = 0\nfor trn_idx , val_idx in kf.split(train_clean , target_clean):\n    train_x = train_clean.iloc[trn_idx]\n    train_y = target_clean.iloc[trn_idx]\n    val_x = train_clean.iloc[val_idx]\n    val_y = target_clean.iloc[val_idx]\n    \n    \n    model = CatBoostClassifier(**param_cb, random_seed=1)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] , verbose = False)\n    preds = pd.concat([preds, pd.DataFrame(model.predict_proba(test),index=test.index)], axis=1)\n    oof_preds = model.predict_proba(train_clean)\n    roc.append(log_loss(val_y ,np.clip(model.predict_proba(val_x),0.025,0.975)))\n    \n    print(n+1 , roc[n])\n    \n    n+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample['Class_1']=np.clip(preds[1].mean(axis=1), 0.025,0.975)\nsub_sample['Class_2']=np.clip(preds[0].mean(axis=1),0.025,0.975)\nsub_sample['Class_3']=np.clip(preds[3].mean(axis=1), 0.025,0.975)\nsub_sample['Class_4']=np.clip(preds[2].mean(axis=1), 0.025,0.975)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample = sub_sample.reset_index()\nsub_sample.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_sample.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}