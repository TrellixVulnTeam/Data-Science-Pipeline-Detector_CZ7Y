{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimension reduction for EDA","metadata":{}},{"cell_type":"markdown","source":"A use of dimensionality reduction is visualization of datasets with a high numbers od features.\nThis dataset deals with predicting the category on an eCommerce product given various attributes about the listing. There are four different classes to predict and 50 different features.\nDimensionality reduction can be useful to reduce the number of features while preserving the variance of the original dataset. By reducing the numbers of features you can also make plot easier and find the components that enable to differentiate the different classes.\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First let's count the number of times each class is present in the training data.","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots()\nplt.bar(train['target'].unique(), train['target'].value_counts(), color = ['red','blue','green','yellow'])\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.title('Counting target class for training data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training dataset has 100000 rows and 50 features.\nClass_2 is the most represented class in the dataset while Class_3 it is the least represented class.","metadata":{}},{"cell_type":"markdown","source":"Let's now drop 'id' column.","metadata":{}},{"cell_type":"code","source":"data = [train,test]\n\nfor d in data:\n    d.drop('id', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features have a lot of zeros values in it. Let's count them.","metadata":{}},{"cell_type":"code","source":"train_null_perc = pd.DataFrame(np.round(train[train == 0].count()/len(train),2)*100, columns = ['Train_null_perc'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_null_perc.sort_values(by = 'Train_null_perc',ascending = False, inplace = True)\ntrain_null_perc.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"95% of values in features_13 are 0, followed by 93% of feature_2.\nLet's plot all percentages.","metadata":{}},{"cell_type":"code","source":"fig , ax = plt.subplots()\nsns.barplot(x = train_null_perc.index, y= train_null_perc['Train_null_perc'], ax= ax, dodge = False)\nplt.xticks(rotation=90)\nplt.title('Zero values for training data')\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LinearDiscriminantAnalysis for data visualization","metadata":{}},{"cell_type":"code","source":"X_cols = [col for col in train.columns if col not in ('target','id')]\ny_col = 'target'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will apply Linear Discriminant Analysis to the first 3000 rows of the training dataset, so it will be faster.","metadata":{}},{"cell_type":"code","source":"X = train[X_cols].iloc[:3000,:]\ny = np.array(train[y_col])[:3000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before applying LDA we need to HotEncode variables. We can create a pipe to apply OneHotEncoding and LinearDiscriminantAnalysis sequantially to training data.","metadata":{}},{"cell_type":"markdown","source":"LDA works with no sparse data so we need to set sparse = False in OneHotEncoding.","metadata":{}},{"cell_type":"code","source":"pipe = Pipeline([('ohe',OneHotEncoder(sparse = False)),('LDA', LinearDiscriminantAnalysis())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xt = pipe.fit_transform(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xt.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying LDA we get an array with 3 components that we can convert in a dataframe for data visualization.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(Xt, columns = ['Component_1','Component_2','Component_3'])\ndf['target'] = train['target'][:3000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now every class is associated to a different combination of the 3 components.","metadata":{}},{"cell_type":"markdown","source":"Let's now visualize components and classes!","metadata":{}},{"cell_type":"code","source":"g = sns.PairGrid(df , hue = 'target', palette = ['red','blue','green','yellow'])\ng.map(sns.scatterplot)\ng.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see different components are useful to distinguish the classes.","metadata":{}},{"cell_type":"markdown","source":"Component_1 and Component_2 seem useful to distinguish between Class_2 and Class_3 while Component_3 and Component_1 seem useful distinguish Class_1 and Component_3 and Component_2 to distinguish Class_4.","metadata":{}},{"cell_type":"markdown","source":"LDA can be applied not only to visualiza data but also to reduce numbers of features before developing a model.","metadata":{}}]}