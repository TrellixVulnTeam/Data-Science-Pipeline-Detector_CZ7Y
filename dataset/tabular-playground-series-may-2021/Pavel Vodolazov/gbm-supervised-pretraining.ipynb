{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Pretraining makes a huge difference in many fields envolving deep learning. TabNet use very clever unsupervised pretraining, which is manages to improve the score.\nIt still not as good as GBMs but they are synergy well.\nI thought if there is a way to pretrain with GBM, may be there is a way to leverage other models.\n\nLet me introduce you a way to pretrain the data with LightGBM. Technically it is a transformation. What I actually do is:\n* Train a lightgbm model. I suggest to engineer your features if possible and optimize the parameters. \n* Extract shap values for unseen fold\n* Repeat for all folds and combine\n\nAs a result, you end up with a new dataset, which is:\n* Normalized\n* Linearized - kind of. Features transformed into their importances\n* Categorical features encoded smarter! Encoding is not linear and depends on other features of the sample.\n* Missing values a handled smarter!\n\nI suggest you to read about shap values before you try.\n\nI choosed lightgbm because it fast, good, and super-lazy: no need to worry about categories, missing values etc.\nYou may use other tree-based models.","metadata":{}},{"cell_type":"code","source":"pip install pytorch_tabnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport lightgbm as lgb\nfrom tqdm.autonotebook import tqdm\nfrom sklearn import metrics\nimport shap\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\nimport random\nimport os\nshap.initjs()","metadata":{"_uuid":"8422c101-b93a-4f1e-b917-272554859ddb","_cell_guid":"e084ed06-ea73-4241-8faa-7be314013adb","ExecuteTime":{"end_time":"2021-04-05T07:36:19.629205Z","start_time":"2021-04-05T07:36:19.625401Z"},"jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv').set_index('id')\ntest_df = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv').set_index('id')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{"_uuid":"7685b8a3-c0c9-48de-a607-5798e38e29cc","_cell_guid":"af349323-3a02-4979-9345-74a25709144a","ExecuteTime":{"end_time":"2021-04-05T07:36:20.37188Z","start_time":"2021-04-05T07:36:20.084718Z"},"jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My params are optunized. You do not have to, but I believe it will improve the result.","metadata":{}},{"cell_type":"code","source":"params = {}\n\nN_SPLITS = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used feature extraction from another notebooks, just removed missing value imputation. Based on my expirience, lightgbm produce better scores for NaNs, rather than imputed values.","metadata":{}},{"cell_type":"code","source":"all_df = pd.concat([train_df, test_df])\n\nfor col in all_df.select_dtypes(['object']).columns:\n    all_df[col] = all_df[col].astype('category')\n\nX = all_df[all_df.index.isin(train_df.index)]\ny = X.pop('target')\n\nx_tst = all_df[~all_df.index.isin(train_df.index)].drop(columns='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = KFold(n_splits = N_SPLITS)\noof = np.zeros(X.shape[0])\npredictions = np.zeros(x_tst.shape[0])\nshap_list = []\nshap_tst_list = []\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X, y)), total=folds.n_splits):\n    print(\"Fold {}\".format(fold_))\n    x_trn = X.iloc[trn_idx]\n    y_trn = y[trn_idx]\n    x_val = X.iloc[val_idx]\n    y_val = y[val_idx]\n    model = lgb.LGBMClassifier(**params, random_state=42, n_estimators=9999999)\n    model.fit(x_trn, y_trn, \n            eval_set=[(x_trn, y_trn),(x_val, y_val)],\n#             eval_metric='auc', \n            early_stopping_rounds=500, \n            verbose=500\n           )\n    oof[val_idx] = model.predict_proba(x_val, num_iteration=model.best_iteration_)[:,1]\n    predictions += model.predict_proba(x_tst, num_iteration=model.best_iteration_)[:,1] / folds.n_splits\n    shap_explainer = shap.TreeExplainer(model)\n    shap_val = pd.DataFrame(shap_explainer.shap_values(x_val)[1], index=x_val.index, columns=x_val.columns)\n    shap_list.append(shap_val)\n    shap_tst = pd.DataFrame(shap_explainer.shap_values(x_tst)[1], index=x_tst.index, columns=x_tst.columns)\n    shap_tst_list.append(shap_tst)\nsubmission1 = pd.Series(predictions, index=x_tst.index).to_frame('target').reset_index()\nlgb.plot_importance(model)\nmodel1_score = metrics.log_loss(y, oof)","metadata":{"_uuid":"ff8bd9fb-1682-414e-a4ca-5594aa47969c","_cell_guid":"415c57cb-daae-4b1a-8962-957e8ffe30f1","ExecuteTime":{"start_time":"2021-04-05T07:42:15.363Z"},"jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then I create the transformed dataset and apply 3 model on it:\n* Lightgbm. I use the same params. Optimized should perform better.\n* Tabnet w/o unsupervised pretraining\n* Tabnet with unsupervised pretraining\n","metadata":{}},{"cell_type":"code","source":"X = pd.concat(shap_list).join(y)\ny = X.pop('target')\nx_tst = pd.concat(shap_tst_list).groupby(level=0).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = np.zeros(X.shape[0])\npredictions = np.zeros(x_tst.shape[0])\n\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X, y)), total=folds.n_splits):\n    print(\"Fold {}\".format(fold_))\n    x_trn = X.iloc[trn_idx]\n    y_trn = y[trn_idx]\n    x_val = X.iloc[val_idx]\n    y_val = y[val_idx]\n    model = lgb.LGBMClassifier(**params, random_state=42, n_estimators=9999999)\n    model.fit(x_trn, y_trn, \n            eval_set=[(x_trn, y_trn),(x_val, y_val)],\n            eval_metric='auc', \n            early_stopping_rounds=500, \n            verbose=500\n           )\n    oof[val_idx] = model.predict_proba(x_val, num_iteration=model.best_iteration_)[:,1]\n    predictions += model.predict_proba(x_tst, num_iteration=model.best_iteration_)[:,1] / folds.n_splits\nsubmission2 = pd.Series(predictions, index=x_tst.index).to_frame('target').reset_index()\nlgb.plot_importance(model)\nmodel2_score = metrics.log_loss(y, oof)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = np.zeros(X.shape[0])\npredictions = np.zeros(x_tst.shape[0])\n\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X, y)), total=folds.n_splits):\n    print(\"Fold {}\".format(fold_))\n    x_trn = X.iloc[trn_idx]\n    y_trn = y[trn_idx]\n    x_val = X.iloc[val_idx]\n    y_val = y[val_idx]\n    model = TabNetClassifier()\n    model.fit(\n        x_trn.values, y_trn, \n#         eval_metric=['accuracy'],\n        eval_set=[(x_val.values, y_val)]\n    )\n    oof[val_idx] = model.predict_proba(x_val.values)[:,1]\n    predictions += model.predict_proba(x_tst.values)[:,1] / folds.n_splits\nsubmission3 = pd.Series(predictions, index=x_tst.index).to_frame('target').reset_index()\nmodel3_score = metrics.log_loss(y, oof)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = np.zeros(X.shape[0])\npredictions = np.zeros(x_tst.shape[0])\n\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X, y)), total=folds.n_splits):\n    print(\"Fold {}\".format(fold_))\n    x_trn = X.iloc[trn_idx]\n    y_trn = y[trn_idx]\n    x_val = X.iloc[val_idx]\n    y_val = y[val_idx]\n    unsupervised_model = TabNetPretrainer(optimizer_fn=torch.optim.Adam,\n                                          optimizer_params=dict(lr=2e-2),\n                                          mask_type='entmax' # \"sparsemax\"\n                                         )\n    \n    unsupervised_model.fit(X_train=x_trn.values,\n                           eval_set=[x_val.values],\n                           pretraining_ratio=0.8,\n                          )\n    \n    model = TabNetClassifier()\n    model.fit(x_trn.values, y_trn, \n#               eval_metric=['accuracy'],\n              eval_set=[(x_val.values, y_val)],\n              from_unsupervised=unsupervised_model\n             )\n    oof[val_idx] = model.predict_proba(x_val.values)[:,1]\n    predictions += model.predict_proba(x_tst.values)[:,1] / folds.n_splits\nsubmission4 = pd.Series(predictions, index=x_tst.index).to_frame('target').reset_index()\nmodel4_score = metrics.log_loss(y, oof)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('lgbm model:', model1_score)\nprint('lgbm model(shap-pretrained):', model2_score)\nprint('tabnet model(shap-pretrained):', model3_score)\nprint('tabnet model(shap and tabnet pretrained):', model4_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission1.to_csv('submission1.csv', index=False)\nsubmission2.to_csv('submission2.csv', index=False)\nsubmission3.to_csv('submission3.csv', index=False)\nsubmission4.to_csv('submission4.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading\n\nNow working on unsupervised GBM-pretraining...","metadata":{}}]}