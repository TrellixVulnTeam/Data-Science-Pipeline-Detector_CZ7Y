{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a super basic starter on XGBoost. If you need something to get you started this hopefully will help. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the model split test size. Usually it is .20 but for fun made it .30.\n# and set our seed\ntest_size = .3\nseed = 69","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import our training file\ntrain_df = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what does our data look like? \ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is a category. Well, that is interesting. We might want to encode those labels!","metadata":{}},{"cell_type":"code","source":"# we need to drop our target from X so we can predict\nX = train_df.drop(['target'], axis=1)\n# and add the target to y which is what we are trying to predict!\ny = train_df['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can encode our labels for the target since they are cats. meow.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoded_y = label_encoder.transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is very standard fair. We split our data into train and test sets. \n# train is to train... so we have the x part of train to learn on and the y as the target we want to predict\n# ditto for the y portion of the test set. Which as you remember above is .30 of our dataset. \nX_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y,\n    test_size=test_size, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are making this a basic, out of the box XGBoost. \nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\nprint(model)\n# we print the model so you can see that we used the out of the box features of XGBoost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will set our prediction on the test data\npred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and finally, the payoff. How accurate is our model?\naccuracy = accuracy_score(y_test, pred)\nprint('Accuracy: %.2f%%' % (accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eh. We could almost flip a coin and predict this. Not so great. But we get ~58% without doing a single thing to the data. Time to engineer some features! I'll create another notebook with that and link here. ","metadata":{}}]}