{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport optuna\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score, average_precision_score, log_loss\nfrom sklearn.model_selection import train_test_split, KFold , StratifiedKFold, cross_val_score\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rm(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))    \n    for col in df.columns:\n        col_type = df[col].dtype        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = rm(pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\"))\nncol = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49' ]\nsample = rm(pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\"))\ntest = rm(pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\"))\nto_test = test[ncol]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train]).reset_index(drop = True)  # for future use\ndf = df.drop('id', axis=1)\nonehot_cols = ['target']\nonehot_encoded_df = pd.get_dummies(df[onehot_cols])\ndf = pd.concat([df[ncol], onehot_encoded_df], axis=1)\nX = df[ncol]\ny_cols = ['target_Class_1', 'target_Class_2', 'target_Class_3', 'target_Class_4']\ny = np.stack([df[c] for c in y_cols]).T\ny = pd.DataFrame(y)\ndel train\ndel test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class 1 \ndef objective(trial , X = X , y = y[0]):\n    train_x , test_x , train_y , test_y = train_test_split(X , y , \\\n            test_size = 0.2 , random_state = 42, stratify = y)    \n    params = {\n        'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n        'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n       # 'fit_intercept' : trial.suggest_categorical('fit_intercept' , [True, False]),\n       #  'random_state' : trial.suggest_categorical('random_state' , [0, 42, 2021, 555]),\n       # 'solver' : trial.suggest_categorical('solver' , ['lbfgs','liblinear']),\n        \"n_jobs\" : -1\n    }\n    model1 = LogisticRegression(**params, random_state = 2020)\n    model1.fit(train_x , train_y)\n    y_predlr1 = model1.predict_proba(test_x)[:,1]\n    model2 = LogisticRegression(**params, random_state = 2021)\n    model2.fit(train_x , train_y)\n    y_predlr2 = model2.predict_proba(test_x)[:,1]\n    y_predlr = (y_predlr1 + y_predlr2) / 2\n    ll = log_loss(test_y , y_predlr)\n    return ll\noptuna.logging.set_verbosity(optuna.logging.WARNING) # i do not want to see trail information\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lr'\n                            , pruner = optuna.pruners.HyperbandPruner() \n                           )\nstudy.optimize(objective, n_trials = 500)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_class1 = {'tol': 8.48578916594731e-05, 'C': 0.010068250605161195, \"n_jobs\" : -1}\n#the best value: 0.29062582857124236\n    \nlr_class11 = LogisticRegression(**p_class1, random_state = 2020)\nlr_class11.fit(X, y[0])\npred_class11 = lr_class11.predict_proba(to_test)[:,1]\nlr_class12 = LogisticRegression(**p_class1, random_state = 2021)\nlr_class12.fit(X, y[0])\npred_class12 = lr_class12.predict_proba(to_test)[:,1]\npred_class1 = (pred_class11 + pred_class12) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class 2\ndef objective(trial , X = X , y = y[1]):\n    train_x , test_x , train_y , test_y = train_test_split(X , y , \\\n            test_size = 0.2 , random_state = 42, stratify = y)    \n    params = {\n        'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n        'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n       # 'fit_intercept' : trial.suggest_categorical('fit_intercept' , [True, False]),\n       #  'random_state' : trial.suggest_categorical('random_state' , [0, 42, 2021, 555]),\n       # 'solver' : trial.suggest_categorical('solver' , ['lbfgs','liblinear']),\n        \"n_jobs\" : -1\n    }\n    model1 = LogisticRegression(**params, random_state = 2020)\n    model1.fit(train_x , train_y)\n    y_predlr1 = model1.predict_proba(test_x)[:,1]\n    model2 = LogisticRegression(**params, random_state = 2021)\n    model2.fit(train_x , train_y)\n    y_predlr2 = model2.predict_proba(test_x)[:,1]\n    y_predlr = (y_predlr1 + y_predlr2) / 2\n    ll = log_loss(test_y , y_predlr)\n    return ll\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lr'\n                            , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective, n_trials = 500)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_class2 = {'tol': 0.0005028269911647464, 'C': 0.010143399742532085, \"n_jobs\" : -1}\n# the best value: 0.6775994526616997\n\nlr_class21 = LogisticRegression(**p_class2, random_state = 2020)\nlr_class21.fit(X, y[1])\npred_class21 = lr_class21.predict_proba(to_test)[:,1]\nlr_class22 = LogisticRegression(**p_class2, random_state = 2021)\nlr_class22.fit(X, y[1])\npred_class22 = lr_class22.predict_proba(to_test)[:,1]\npred_class2 = (pred_class21 + pred_class22) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class 3\ndef objective(trial , X = X , y = y[2]):\n    train_x , test_x , train_y , test_y = train_test_split(X , y , \\\n            test_size = 0.2 , random_state = 42, stratify = y)    \n    params = {\n        'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n        'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n       # 'fit_intercept' : trial.suggest_categorical('fit_intercept' , [True, False]),\n       #  'random_state' : trial.suggest_categorical('random_state' , [0, 42, 2021, 555]),\n       # 'solver' : trial.suggest_categorical('solver' , ['lbfgs','liblinear']),\n        \"n_jobs\" : -1\n    }\n    model1 = LogisticRegression(**params, random_state = 2020)\n    model1.fit(train_x , train_y)\n    y_predlr1 = model1.predict_proba(test_x)[:,1]\n    model2 = LogisticRegression(**params, random_state = 2021)\n    model2.fit(train_x , train_y)\n    y_predlr2 = model2.predict_proba(test_x)[:,1]\n    y_predlr = (y_predlr1 + y_predlr2) / 2\n    ll = log_loss(test_y , y_predlr)\n    return ll\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lr'\n                            , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective, n_trials = 500)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_class3 =  {'tol': 0.0005335002526146307, 'C': 0.5980679171662785, \"n_jobs\" : -1}\n# the best value: 0.5131335576493116\n\nlr_class31 = LogisticRegression(**p_class3, random_state = 2020)\nlr_class31.fit(X, y[2])\npred_class31 = lr_class31.predict_proba(to_test)[:,1]\nlr_class32 = LogisticRegression(**p_class3, random_state = 2021)\nlr_class32.fit(X, y[2])\npred_class32 = lr_class32.predict_proba(to_test)[:,1]\npred_class3 = (pred_class31 + pred_class32) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class 4\ndef objective(trial , X = X , y = y[3]):\n    train_x , test_x , train_y , test_y = train_test_split(X , y , \\\n            test_size = 0.2 , random_state = 42, stratify = y)    \n    params = {\n        'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n        'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n       # 'fit_intercept' : trial.suggest_categorical('fit_intercept' , [True, False]),\n       #  'random_state' : trial.suggest_categorical('random_state' , [0, 42, 2021, 555]),\n       # 'solver' : trial.suggest_categorical('solver' , ['lbfgs','liblinear']),\n        \"n_jobs\" : -1\n    }\n    model1 = LogisticRegression(**params, random_state = 2020)\n    model1.fit(train_x , train_y)\n    y_predlr1 = model1.predict_proba(test_x)[:,1]\n    model2 = LogisticRegression(**params, random_state = 2021)\n    model2.fit(train_x , train_y)\n    y_predlr2 = model2.predict_proba(test_x)[:,1]\n    y_predlr = (y_predlr1 + y_predlr2) / 2\n    ll = log_loss(test_y , y_predlr)\n    return ll\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lr'\n                            , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective, n_trials = 500)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_class4 = {'tol': 0.00012534268581491389, 'C': 0.010057756278091795, \"n_jobs\" : -1}\n# the best value: 0.374101486607313\n\nlr_class41 = LogisticRegression(**p_class4, random_state = 2020)\nlr_class41.fit(X, y[3])\npred_class41 = lr_class41.predict_proba(to_test)[:,1]\nlr_class42 = LogisticRegression(**p_class4, random_state = 2021)\nlr_class42.fit(X, y[3])\npred_class42 = lr_class42.predict_proba(to_test)[:,1]\npred_class4 = (pred_class41 + pred_class42) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_all = pd.concat([pd.DataFrame(pred_class1), pd.DataFrame(pred_class2), \n           pd.DataFrame(pred_class3), pd.DataFrame(pred_class4)], axis=1)\nsample[['Class_1','Class_2', 'Class_3', 'Class_4']] = pred_all.to_numpy()\nsample.to_csv('output.csv',index=False)\nsample","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}