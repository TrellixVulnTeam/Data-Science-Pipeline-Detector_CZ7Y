{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nimport plotly.graph_objects as go\nfrom sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\nfrom sklearn.utils import class_weight\nfrom sklearn.decomposition import NMF\nfrom sklearn.model_selection import train_test_split,KFold, GroupKFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> NMF  Non-Negative Matrix Factorization </h1>\n\n# This notebook is kind of spinnof of my [PCA from scratch](https://www.kaggle.com/marcinstasko/pca-analysis-tutorial-from-scratch). \n\nAs we have tested many suprvised method in this competition without spectacular sucess I will test NMF with is an example of unsupervised feature selection. We will does not analyse the y labels. <br>\n# If you like this notboook let me know. I will concider to make it as tutorial\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2. Initial Setup-Data Preparaton </h1>\n","metadata":{}},{"cell_type":"code","source":"# Initialize local variables\n\nSEED = 1992\nTARGET = ['target']\nSAMPLE = 2000 #For Visualization purposees","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the data and prepare for the analysis. I have used sampling for speed up purposes. You can remove it :)\n\ntrain = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')\n\n#Prepare train and test dataset\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n#Checking the null Data\nnull_data = (train.isna().sum().sort_values(ascending=False) / len(train) * 100) \nfig, ax = plt.subplots(1,1,figsize=(35, 7)) \nax.bar(null_data.index, 100, color='#dadada', width=0.6) \nbar = ax.bar(null_data.index,null_data, width=0.6) \nax.bar_label(bar, fmt='%.01f %%') \nax.spines.left.set_visible(False) \nax.set_yticks([]) \nax.set_title('Null Data Ratio', fontweight='bold') \nplt.show()\n\ntrain.describe().drop('count').T\\\n        .style.bar(subset=['mean'])\\\n        .background_gradient(subset=['std'])\\\n        .background_gradient(subset=['50%'])\\\n        .background_gradient(subset=['max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Dataset Preparation </h1>\n","metadata":{}},{"cell_type":"code","source":"def label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nall_df = pd.concat([train, test]).reset_index(drop=True)\nall_df[TARGET]=label_encoder(all_df[TARGET])\ntrain_last_id = train.shape[0]\n\nX = all_df.drop(TARGET,axis=1)\ny = all_df[TARGET]\nprint('Feature dataset format:\\t{}\\nTarget dataset format:\\t{}\\nTrain dataset size:\\t{}'.format(X.shape,y.shape,train_last_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the classes weights\nclass_ratio = class_weight.compute_class_weight('balanced', y.target.unique(), y.target)\nprint(class_ratio)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Variable standarization and preparation\nX_scaled = MinMaxScaler().fit_transform(X)\ntrain_X_to_select = X_scaled[:train_last_id]\ntest_X_to_select = X_scaled[train_last_id:]\n\ntrain_y_to_select = y[:train_last_id]\ntest_y_to_select = y[train_last_id:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How scaled matrix looks like\nX_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">NMF Applicaton</h1>\n","metadata":{}},{"cell_type":"code","source":"num_classes = len(class_ratio)\n\n#Initialize NMF instance\nnmf = NMF(n_components=num_classes, init='random', random_state=SEED)\n\n#Weights calculation\nW_train = nmf.fit_transform(train_X_to_select)\nW_test = nmf.fit_transform(test_X_to_select)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create column labels accoring to nmf components number\n\ncolumn_labels = []\nfor i in range(num_classes):\n        column_labels.append('NMFClass_{}'.format(i+1))\n\nNMF_DF = pd.DataFrame(W_train , columns = column_labels)\nNMF_DF = pd.concat([NMF_DF , y] , axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Prediction </h1>\nSample prediction \n","metadata":{}},{"cell_type":"code","source":"lgbm_params = {\n    'n_estimators': 45000,\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'random_state': SEED,\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 750,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 3,#16\n    'colsample_bytree': 0.15,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(NMF_DF.columns)\ncols.remove(\"target\")\n\nNMF_train = NMF_DF[:train_last_id]\nNMF_test = NMF_DF[train_last_id:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KFoldTraining(classifier):\n    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 5\n\n    classifier = classifier\n    \n\n    kf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\n    for fold, (tr_index , val_index) in enumerate(kf.split(NMF_train[cols].values , NMF_train['target'].values)):\n\n        print(\"-\" * 50)\n        print(f\"Fold {fold + 1}\")\n\n        x_train,x_val = NMF_DF[cols].values[tr_index] , NMF_train[cols].values[val_index]\n        y_train,y_val = NMF_DF['target'].values[tr_index] , NMF_train['target'].values[val_index]\n\n        eval_set = [(x_val, y_val)]\n\n        model = classifier\n        model.fit(x_train, y_train, eval_set = eval_set, verbose = 1000)\n\n        train_preds = model.predict(x_train)\n        train_rmse += mean_squared_error(y_train ,train_preds , squared = False)\n        print(\"Training RMSE : \" , mean_squared_error(y_train ,train_preds , squared = False))\n\n        val_preds = model.predict(x_val)\n        val_rmse += mean_squared_error(y_val , val_preds , squared = False)\n        print(\"Validation RMSE : \" , mean_squared_error(y_val , val_preds , squared = False))\n\n        if test_preds is None:\n            test_preds = model.predict_proba(NMF_test[cols].values)\n        else:\n            test_preds += model.predict_proba(NMF_test[cols].values)\n\n    print(\"-\" * 200)\n    print(\"Average Training RMSE : \" , train_rmse / n_splits)\n    print(\"Average Validation RMSE : \" , val_rmse / n_splits)\n\n    test_preds /= n_splits\n    return test_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBM_classifier = LGBMClassifier(**lgbm_params)\nLGBM_pred = KFoldTraining(LGBM_classifier)\n\n\ntest_preds = LGBM_pred.copy() # Next I will make model ensembing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Class_1']=test_preds[:,0]\nsample_submission['Class_2']=test_preds[:,1]\nsample_submission['Class_3']=test_preds[:,2]\nsample_submission['Class_4']=test_preds[:,3]\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"NMF_1.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Notebook development plan</h1>\n\n# If you like the notebook I will be happy. This motivate me to develop it :) <br>\n# Plans\n- Making notebook as NMF tutorial - if you will be interested in\n- The ideas in comments<br>\n\nLets connect on [LinkedIn](https://www.linkedin.com/in/marcinstasko/?locale=en_US) :)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}