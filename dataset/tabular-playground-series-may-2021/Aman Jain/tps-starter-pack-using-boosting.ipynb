{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Packages","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt    \nimport seaborn as sns\nimport os\nfrom scipy.stats import norm\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n\n%matplotlib inline\npd.set_option('display.max_columns', None)   # for showing all columns of dataset\n#pd.set_option('display.max_rows', None)     # for showing all rows of dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of dataset(rows,columns)\nprint(train.shape)              \nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check our train dataset\ntrain.head()     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()    # Information about train dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there are 51 columns and 100000 rows in train dataset and most of them having \"int64\" datatype only target variable's data type is 'object' .\n\n\n","metadata":{}},{"cell_type":"code","source":"# id column is not useful so we drop it\ntrain.drop('id',axis=1,inplace=True)   \ntest.drop('id',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# describe the Statistics of dataset \ntrain.describe().T.style.bar(subset=['mean'])\\\n                            .background_gradient(subset=['std'])\\\n                            .background_gradient(subset=['50%'])\\\n                            .background_gradient(subset=['max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Count** : Number of rows in data set.\n- **mean**  : mean is the average value of particular feature.\n- **std.**  : std stands for Standard Deviation.It measures the spread of a data distribution. The more spread out a data distribution is, the greater its standard deviation.\n- **min**   : Minimum value of Feature.\n- **25%**   : It shows the 25% value of that feature.\n- **50%**   : It shows the 50% value of that feature.\n- **75%**   : It shows the 75% value of that feature.\n- **max**   : Maximum value of that feature\n\nSome of you think why we need them.Above all gives us the basic statistical information which will be very helpful in our EDA.\n","metadata":{}},{"cell_type":"code","source":"test.describe().T.style.bar(subset=['mean'])\\\n                            .background_gradient(subset=['std'])\\\n                            .background_gradient(subset=['50%'])\\\n                            .background_gradient(subset=['max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare between train and test dataset\n\ndef diff_color(x):\n    color = 'red' if x<0 else ('green' if x > 0 else 'black')\n    return f'color: {color}'\n\n(train.describe() - test.describe())[test.columns].T.iloc[:,1:].style\\\n        .bar(subset=['mean', 'std'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n        .applymap(diff_color, subset=['min', 'max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Null values\ntrain.isnull().sum()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there no null value\n\n\n\n\n**Lets See Distribution of Target.**","metadata":{}},{"cell_type":"code","source":"# check target variable\nsns.countplot(train['target'])\ntrain.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of features in respect of target variable\nfeature_columns = train.columns.drop('target')\nnum_rows, num_cols = 10,5\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 30))\n\n\nfor index, column in enumerate(feature_columns):\n    i,j = (index // num_cols, index % num_cols)\n\n    sns.kdeplot(train.loc[train['target'] == 'Class_1', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_2', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_3', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_4', column], shade=True, ax=axes[i,j])\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation matrix**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10 , 10))\n\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\nsns.heatmap(train.corr(),\n        square=True, center=0, linewidth=0.2,\n        cmap='Reds',\n        mask=mask, ax=ax) \n\nax.set_title('Feature Correlation', loc='center', fontweight='bold')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the data is not internally correlated thus all variables can be used in features selecton. Internal correlation may leads to strong correlation and covariation signals making other relations underated","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nle =LabelEncoder()\nle.fit(train['target'])\ntrain['target'] = le.transform(train['target'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop('target',axis=1)\ny = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n- XGBOOST\n- Catboost\n- LGBM\n","metadata":{}},{"cell_type":"code","source":"folds = 5\nSEED = 24\nbasic_model =[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBOOST**","metadata":{}},{"cell_type":"code","source":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_xgb = np.zeros((test.shape[0], 4))\nsf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=SEED)\nfor fold,(train_idx,val_idx) in enumerate(sf.split(X,y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    xgb =XGBClassifier(objective ='multi:softprob',random_state=SEED)\n    xgb.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 50)\n    y_val_pred = xgb.predict_proba(X_val)\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_xgb += xgb.predict_proba(test)\n\n\ny_test_pred_xgb = y_test_pred_xgb / folds\n\nprint(f\"Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\n\nbasic_model.append({'model': 'xgboost', 'logloss': log_loss(y, y_oof_pred)})\n    \n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CatBoost**","metadata":{}},{"cell_type":"code","source":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_catb = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=SEED)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx] \n        \n    catb = CatBoostClassifier(random_state=SEED)\n    catb.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 200)\n\n    y_val_pred = catb.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_catb += catb.predict_proba(test)\n\n\ny_test_pred_catb = y_test_pred_catb / folds\n\nprint(f\"Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\nbasic_model.append({'model': 'catboost', 'logloss': log_loss(y, y_oof_pred)})","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LGBM**","metadata":{}},{"cell_type":"code","source":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_lgbm = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle= True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n    lgbm = LGBMClassifier(random_state=SEED)\n\n    lgbm.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 200, early_stopping_rounds=150)\n\n    y_val_pred = lgbm.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_lgbm += lgbm.predict_proba(test)\n\n\ny_test_pred_lgbm= y_test_pred_lgbm/ folds\n\nprint(f\"-- Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\nbasic_model.append({'model': 'lgbm', 'logloss': log_loss(y, y_oof_pred)})","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(basic_model, index=None)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsns.catplot(y=\"model\", x=\"logloss\", data=df,kind='violin')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As plot shows us CatBoost is better than lgbm and xgboost","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-may-2021/sample_submission.csv')\n\n\nsubmission = pd.DataFrame(y_test_pred_catb)\nsubmission.columns = ['Class_1', 'Class_2','Class_3','Class_4']\n\n\n\n\nsubmission['id'] = sample_submission['id']\nsubm = submission[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission.csv\", index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Future vision Update for this notebook: \n1. Hyperparameter optimization for the above models\n2. Use other ML Algorithms also\n3. create ANN and then hypertune the Ann model","metadata":{}}]}