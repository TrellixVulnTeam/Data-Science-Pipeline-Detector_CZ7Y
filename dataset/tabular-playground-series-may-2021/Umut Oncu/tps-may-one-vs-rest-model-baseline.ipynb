{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\nfrom warnings import simplefilter\n\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nsimplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## I have wondered what if I use four binary classification models instead of one multiclassification model. So, I've decided to build a baseline depends on this idea. \n\n\n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/test.csv\")\n\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df[\"id\"]\ntest.index = test[\"id\"]\nid = test[\"id\"]\ntest.drop(\"id\",axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"class1\"] = np.where(df[\"target\"]==\"Class_1\", 1, 0)\n\ndf[\"class2\"] = np.where(df[\"target\"]==\"Class_2\", 1, 0)\n\ndf[\"class3\"] = np.where(df[\"target\"]==\"Class_3\", 1, 0)\n\ndf[\"class4\"] = np.where(df[\"target\"]==\"Class_4\", 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df.drop([\"class2\", \"class3\",\"class4\",\"target\"], axis=1)\ndf2 = df.drop([\"class1\", \"class3\",\"class4\",\"target\"], axis=1)\ndf3 = df.drop([\"class1\", \"class2\",\"class4\",\"target\"], axis=1)\ndf4 = df.drop([\"class1\", \"class2\",\"class3\",\"target\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df1.drop(\"class1\", axis=1)\ny = df1[\"class1\"]\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df2.drop(\"class2\", axis=1)\ny = df2[\"class2\"]\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df3.drop(\"class3\", axis=1)\ny = df3[\"class3\"]\n\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df4.drop(\"class4\", axis=1)\ny = df4[\"class4\"]\n\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1 = test.copy()\ntest2 = test.copy()\ntest3 = test.copy()\ntest4 = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set1 = [(X_train1, y_train1), (X_test1, y_test1)]\neval_set2 = [(X_train2, y_train2), (X_test2, y_test2)]\neval_set3 = [(X_train3, y_train3), (X_test3, y_test3)]\neval_set4 = [(X_train4, y_train4), (X_test4, y_test4)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=4, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.6)\n\n\nxgb.fit(X_train1, y_train1, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set1, verbose=False)\ny_pred_train = xgb.predict_proba(X_train1)\ny_pred_test = xgb.predict_proba(X_test1)\nprint(\"Train: \", log_loss(y_train1, y_pred_train))\nprint(\"Test: \", log_loss(y_test1, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class1 = xgb.predict_proba(test)[:, 1]\nclass1[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.8)\n\n\nxgb.fit(X_train2, y_train2, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set2, verbose=False)\ny_pred_train = xgb.predict_proba(X_train2)\ny_pred_test = xgb.predict_proba(X_test2)\nprint(\"Train: \", log_loss(y_train2, y_pred_train))\nprint(\"Test: \", log_loss(y_test2, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class2 = xgb.predict_proba(test2)[:,1]\nclass2[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.6)\n\n\nxgb.fit(X_train3, y_train3, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set3, verbose=False)\n\ny_pred_train = xgb.predict_proba(X_train3)\ny_pred_test = xgb.predict_proba(X_test3)\nprint(\"Train: \", log_loss(y_train3, y_pred_train))\nprint(\"Test: \", log_loss(y_test3, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class3 = xgb.predict_proba(test3)[:,1]\nclass3[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.7)\n\n\nxgb.fit(X_train4, y_train4, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set4, verbose=False)\ny_pred_train = xgb.predict_proba(X_train4)\ny_pred_test = xgb.predict_proba(X_test4)\nprint(\"Train: \", log_loss(y_train4, y_pred_train))\nprint(\"Test: \", log_loss(y_test4, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class4 = xgb.predict_proba(test4)[:,1]\nclass4[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(class1, columns=[\"Class_1\"])\nsub.index = test.index\nsub[\"Class_2\"] = class2\nsub[\"Class_3\"] = class3\nsub[\"Class_4\"] = class4\nsub.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('./sub.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Even though I haven't used feature engineering, the submission result is not bad. Moreover, I haven't applied anything for an imbalance of datasets in which 2 of 4 datasets are severely imbalanced. Also, the hyperparameters of models are almost arbitrary because I haven't used Grid or Random search. \n\n","metadata":{}}]}