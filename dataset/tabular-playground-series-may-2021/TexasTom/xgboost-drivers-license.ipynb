{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# Author ......... Tom Bresee\n",
    "# Company ........ T-Mobile\n",
    "# Grad School .... University of Michigan\n",
    "# Program ........ Applied Data Science \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "#   output of above code would be:\n",
    "#    /kaggle/input/tabular-playground-series-may-2021/sample_submission.csv\n",
    "#    /kaggle/input/tabular-playground-series-may-2021/train.csv\n",
    "#    /kaggle/input/tabular-playground-series-may-2021/test.csv\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- latest copies ---\n",
    "!pip install xgboost --upgrade\n",
    "!pip install optuna --upgrade\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "from IPython.core.display import display, HTML\n",
    "import pickle\n",
    "import json \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- common standard libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "# high rez images \n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import altair as alt\n",
    "import plotly.express as px\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- scientific stuff ---\n",
    "import scipy.sparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy.sparse as sparse\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- main ML algorithm ---\n",
    "import xgboost as xgb\n",
    "# i actually like to call it out as xgb... \n",
    "# use xgb.<something>, in case someone thinks for\n",
    "# instance plot_tree is something from sklearn\n",
    "# or matplotlib or altair or whatever...\n",
    "# Will Use:\n",
    "#   - xgb.XGBClassifier \n",
    "#   - xgb.plot_importance, xgb.plot_tree\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- i may use gridsearch or do more complicated optuna ---\n",
    "import lightgbm as lgb\n",
    "# probably won't use lgb, focus on xgb \n",
    "# import optuna.integration.lightgbm as lgb\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import optuna \n",
    "from optuna import Trial, visualization\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- read in files and process --- \n",
    "# a. I will specify the files to read in \n",
    "#    I also can say i don't need that 'id' column, its superficial, \n",
    "#    i.e. it brings no real 'information' to the table...\n",
    "sample_sub = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/sample_submission.csv\")\n",
    "train_df   = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/train.csv\")\n",
    "#    later i will find i need to do this\n",
    "# train_df['target'] = train_df['target'].map({'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3})\n",
    "#    I actually think this looks better:\n",
    "train_df.rename(columns = lambda x: x.replace('_', ' '), inplace=True) \n",
    "test_df    = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/test.csv\")\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# b.  clean and make ready input\n",
    "# X_train = train_df.copy()\n",
    "# remove the id and target col obviously\n",
    "# X_train.drop(columns=[\"id\", \"target\"], inplace=True)\n",
    "# y_train = train_df.copy()\n",
    "# y_train = y_train['target']\n",
    "#     consolidate and remove the id and target at the same time\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df.drop(columns=[\"id\",\"target\"]), \n",
    "    train_df.target, \n",
    "    test_size=0.2,\n",
    "    random_state = 42,\n",
    "    stratify=train_df.target)\n",
    "\n",
    "X_test=test_df.drop(columns=\"id\")\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# APPENDIX:\n",
    "# --- bst ---\n",
    "# type(bst)  ->  xgboost.sklearn.XGBClassifier  \n",
    "# --- plot tree ---\n",
    "#  xgb.plot_tree(bst, num_trees=2)\n",
    "#  fig = matplotlib.pyplot.gcf()\n",
    "#  fig.set_size_inches(150, 100)\n",
    "#  fig.set_size_inches(180.5, 16.5)\n",
    "#  plt.show()\n",
    "#  fig.savefig('tree.svg')\n",
    "# --- creating example table for clarity --- \n",
    "# print = pd.DataFrame({'Data': ['100,000',  '50', 'int64', '0','4']})\n",
    "# print.index=['Total Rows of Data', \n",
    "#              'Number of Features', \n",
    "#              'Features Dtype', \n",
    "#              'Num Missing Values',\n",
    "#              'Number of target classes']\n",
    "# print.to_html()\n",
    "# --- utility for download hi-rez images ---\n",
    "# this way i can download the file from kaggle, \n",
    "# upload to github so you see closer image zoom\n",
    "def download_my_file(my_file):\n",
    "    import os \n",
    "    os.chdir(r'/kaggle/working')\n",
    "    from IPython.display import FileLink \n",
    "    FileLink(my_file)\n",
    "# --- replacing primitive ---\n",
    "#    y.replace({1:0, 2:1, 3:2, 4:3, 5:4}, inplace = True)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# --- download latest xgboost ---\n",
    "# !pip install xgboost --upgrade \n",
    "# print(\"xgb version: {}\". format(xgb.__version__))\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# interesting links:\n",
    "# https://www.avato-consulting.com/?p=28903&lang=en\n",
    "# $ python json_parser_python_function.py --model try_to_parse_this.json\n",
    "# https://www.kaggle.com/c/higgs-boson/discussion/10286\n",
    "# https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "# https://github.com/dmlc/xgboost/blob/master/demo/json-model/json_parser.py\n",
    "# https://dzone.com/articles/inspecting-decision-trees-in-h2o\n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# https://stats.stackexchange.com/questions/395697/what-is-an-intuitive-interpretation-of-the-leaf-values-in-xgboost-base-learners\n",
    "# https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated\n",
    "#"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:03.593506Z",
     "iopub.execute_input": "2022-04-21T17:39:03.593898Z",
     "iopub.status.idle": "2022-04-21T17:39:16.464765Z",
     "shell.execute_reply.started": "2022-04-21T17:39:03.59385Z",
     "shell.execute_reply": "2022-04-21T17:39:16.463798Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "%%HTML\n",
    "\n",
    "<style type=\"text/css\">  \n",
    "\n",
    "div.h2 {\n",
    " color: white;\n",
    " background-image: linear-gradient(120deg, #155799, #159957);\n",
    " text-align: center;  \n",
    " /* original:  text-align: center; */ \n",
    " padding:9px;\n",
    " /* removing this: padding-right: 100px; */\n",
    " font-size: 20px;  \n",
    " max-width: 1500px;  \n",
    " margin: auto; \n",
    " margin-top: 40px;}                                   \n",
    "                                                                              \n",
    "div.h3 {\n",
    "    color: #159957; \n",
    "    font-size: 18px; \n",
    "    margin-top: 20px; \n",
    "    margin-bottom:4px;\n",
    "       }\n",
    "                               \n",
    "div.h4 {\n",
    "    color: #159957;\n",
    "    font-size: 16px; \n",
    "    margin-top: 20px; \n",
    "    margin-bottom: 8px;\n",
    "}\n",
    "     \n",
    "body {font-size: 10px;}   \n",
    "                                       \n",
    "span.note {\n",
    "    font-size: 5; \n",
    "    color: gray; \n",
    "    font-style: italic;\n",
    "}\n",
    "                                       \n",
    "hr {display: block; \n",
    "    color: gray\n",
    "    height: 1px; \n",
    "    border: 0; \n",
    "    border-top: 1px solid;}\n",
    "                                     \n",
    "hr.light {display: block; \n",
    "          color: lightgray\n",
    "          height: 1px; \n",
    "          border: 0; \n",
    "          border-top: 1px solid;}   \n",
    "                                   \n",
    "table.dataframe th \n",
    "{\n",
    "    border: 1px darkgray solid;\n",
    "    color: black;\n",
    "      <table align=\"left\">\n",
    "    ...\n",
    "  </table>\n",
    "    background-color: white;\n",
    "}\n",
    "                                   \n",
    "table.dataframe td \n",
    "{\n",
    "    border: 1px darkgray solid;\n",
    "    color: black;\n",
    "    background-color: white;\n",
    "    font-size: 10px;\n",
    "    text-align: center;\n",
    "} \n",
    "   \n",
    "                                   \n",
    "table.rules th \n",
    "{\n",
    "    border: 1px darkgray solid;\n",
    "    color: black;\n",
    "    background-color: white;\n",
    "    font-size: 10px;\n",
    "    align: left;\n",
    "}\n",
    "                                            \n",
    "table.rules td \n",
    "{\n",
    "    border: 1px darkgray solid;\n",
    "    color: black;\n",
    "    background-color: white;\n",
    "    font-size: 13px;\n",
    "    text-align: center;\n",
    "} \n",
    "                                                                           \n",
    "table.rules tr.best\n",
    "{\n",
    "    color: green;\n",
    "}    \n",
    "    \n",
    "                                      \n",
    ".output { \n",
    "    align-items: center; \n",
    "    /* changed from align-items: left; */\n",
    "}\n",
    "        \n",
    "                                      \n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: left;\n",
    "    margin:auto;\n",
    "}                                          \n",
    "                                                                                                                               \n",
    "</style>  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have the above starter code also hosted [here](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/base_starter_section_of_code.txt) as well for viewing ease, I just think its a touch easier to see the actual python txt above on github in one long flow.  "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h2\"><i><center>Kaggle Tabular Data Series</center></i></div>\n",
    "<div class=\"h3\"><i><center>Dataset</center></i></div>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>1. &ensp; Introduction</i></div> \n",
    "\n",
    "* **Author:** &nbsp; Tom Bresee\n",
    "* **Contact 1:** &nbsp; tbresee@umich.edu\n",
    "* **Contact 2:** &nbsp; tom.bresee@t-mobile.com\n",
    "* **Location:** &nbsp; Frisco, Texas\n",
    "* **LinkedIn:** &nbsp; [Tom Bresee](https://www.linkedin.com/in/tombresee/)\n",
    "* **Notebook Template Format:** &nbsp; Streamlined \n",
    "* **Why?** &nbsp; Understand algorithm for research towards Capstone\n",
    "* **Background:** &nbsp; First time using XGBoost, as well as Optuna"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>2.&ensp; Overall Approach</i></div>  \n",
    "\n",
    "* I'm going to take a bit of a different approach here.  I am going to try to streamline this notebook with external links here and there, as well as minimize code and get straight to the problem and attempt to solve it. I also will attempt to create the notebook in a way where it is more optimal for **smartphone** viewing. 5G will change our lives.\n",
    "\n",
    "* **Drivers License Concept** - One could argue that today more and more data scientists are using machine learning algorithms that they may not understand fully, but do know the basics of how to get answers from, without also knowing how to measure how accurate/successful those algorithms actually are. Some of these machine learning algorithms are immensely powerful, and yet are wielded in unhealthy or unnatural ways. One could argue that one shouldn't necessarily use a specific machine learning algorithm without **truly understanding it**.  Just as young adults today are not allowed to drive a vehicle until after they pass a written and practical test, one could argue that one shouldn't be allowed to use certain powerful machine learning algorithms until one has obtained the data science equivalent of a `Drivers License`, demonstrating beginner level proficiency, on the road to intermediate-level understanding, and then eventually advanced-level understanding. I attempt to demonstrate the basics of the XGBoost algorithm, and try to dive deeper into how it actually works.  I will in fact update this notebook considerably over time, come back once in a while, my goal is to completely explain XGBoost end-to-end. \n",
    "* **Note** - In some cases I was running this kaggle notebook on GPU, to enable on the kaggle platform just click Accelerator and then set to enable GPU. Set certain parameters (i list below); highly recommended when training XGBoost classifier, speeds things up considerably...\n",
    "* **Update -  Memorial Day** - I will continue to document more and more about XGBoost in this notebook, even though the competition part of this ends in 10 mins.  I feel like I have a pretty good start here, but as I dive in deeper I find more and more to write about.  Goal will be to add a fair amount of verbage and analysis here for the next month, by end of June I believe I will have this 'done' and no more additions will be included...check back for more later"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h4\"><i>Read Everything</i></div>  \n",
    "\n",
    "\n",
    "* Read xgboost user guide downloaded from [here](https://xgboost.readthedocs.io/_/downloads/en/release_1.4.0/pdf/), the release 1.4.2 version and analyze:\n",
    "  * xgboost allows training with multiple GPUs only on linux platforms (not windows, which means the million years it took me to get all my multiple gpus on my windows platform to function correctly won't help me).  no big deal.  why ?  because xgboost is very very very fast out of the box. nice.  also, make sure if you do run on windows that you download a up-to-date version of the cuda toolkit.  done. \n",
    "  * user docs refer to the classifier or models as 'bst' (they don't use names like `clf` like scikit-learn does.  ok fine.  we will call our classifer model `bst` for clarity)\n",
    "  * wow julia support, nice. something i worked with a while back, kinda nice. the mighty scala as well.\n",
    "  * the term 'boosting' itself is not very descriptive and relatable (boost usually has the connotation of helping or increasing, which um doesn't help me undestand),  i'll just start calling it boosting sequential ensemble instead. \n",
    "  * interesting:  couldn't actually get this to command to take:  bst.save_config()\n",
    "  * overfitting:  the key to accurate models is very much to control this, via any intelligent way we can. we are fundamentally working with **decision tree ensembles**\n",
    " "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>3. &ensp; Training Data Summary</i></div>\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>Data</th>    </tr>  </thead>  <tbody>    <tr>      <th>Total Rows of Data</th>      <td>100,000</td>    </tr>    <tr>      <th>Number of Features</th>      <td>50</td>    </tr>    <tr>      <th>Features Dtype</th>      <td>int64</td>    </tr>    <tr>      <th>Num Missing Values</th>      <td>0</td>    </tr>    <tr>      <th>Number of target classes</th>      <td>4</td>    </tr>  </tbody></table>\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Features are labelled `feature_0` thru `feature_49` (which makes it easy to view), and are all int64 dtypes\n",
    "* This is important:  None of the features are actually encoded as text/strings/objects, they are **all** base integers (symbolizing feature categories is assumed).  Effectively, the dataset has been converted to numeric for us for easy of understanding (as if original dataset had true categories, and then was label encoded into intergers symbolizing categories)\n",
    "* Fairly large dataset of 100,000 rows (or what I would call `observations` to be technical, resulting in 5M individual cells of data\n",
    "* No missing values as verified with _.isnull().sum()_ command. \n",
    "* Approach:  Dive into the EDA, come up with a reasonable ML algorithm and method\n",
    "* Remember, later we will see that we can delete the 'id' column, since its not actually pertinent nor a feature, and thus our dataframes are all going to either have the feature_0 - feature_49, or just have the target class, but for now its safe to delete the 'id' col as it brings nothing substantial in any form to the model\n",
    "* Note:  Test data follows similar trend as the training data, but there are only 50k observations (rows) of data\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**XGBoost:** Default library version initially was 1.4.0, but i forced an upgrade to 1.4.2 (very latest version as of May 2021) !  "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>4. &ensp; Balance Check</i></div> "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# are we balanced or imbalanced ?  you decide\n",
    "#\n",
    "# I actually like to use Altair, lots of advantages I believe compared\n",
    "# to some other visualization approaches\n",
    "\n",
    "source1 = pd.DataFrame({\n",
    "    'a': list(train_df['target'].value_counts(sort=True).values), \n",
    "    'b': list(train_df['target'].value_counts(sort=True).index.values)})\n",
    "\n",
    "source2 = pd.DataFrame({\n",
    "    'a': list(train_df['target'].value_counts(sort=True, normalize=True).values), \n",
    "    'b': list(train_df['target'].value_counts(sort=True, normalize=True).index.values)})\n",
    "\n",
    "chart1 = alt.Chart(source1).mark_bar(color='red', opacity=.55).encode(\n",
    "    x=alt.X('a', title='class instances counts'),\n",
    "    y=alt.Y('b', title=None)).properties(width=410, height=120)\n",
    "\n",
    "chart2 = alt.Chart(source2).mark_bar(color='blue', opacity=.55).encode(\n",
    "    x=alt.X('a', title='class instances (in % format)', axis=alt.Axis(format='%')),\n",
    "    y=alt.Y('b', title=None)).properties(width=410, height=120)\n",
    "\n",
    "# i like how with altair you can render in svg form, looks\n",
    "# pretty sharp and clear... \n",
    "(chart1  &  chart2).properties(title='Target Class Distribution - Training Data').display(renderer='svg')\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:16.476985Z",
     "iopub.execute_input": "2022-04-21T17:39:16.477353Z",
     "iopub.status.idle": "2022-04-21T17:39:16.591216Z",
     "shell.execute_reply.started": "2022-04-21T17:39:16.477318Z",
     "shell.execute_reply": "2022-04-21T17:39:16.590425Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Numeric Summary of Above Charts:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>Instance Counts</th>      <th>% of Total Obs</th>    </tr>  </thead>  <tbody>    <tr>      <th>Class 2</th>      <td>57,497</td>      <td>57.50</td>    </tr>    <tr>      <th>Class 3</th>      <td>21,420</td>      <td>21.42</td>    </tr>    <tr>      <th>Class 4</th>      <td>12,593</td>      <td>12.60</td>    </tr>    <tr>      <th>Class 1</th>      <td>8,490</td>      <td>8.49</td>    </tr>  </tbody></table>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Closep of above target class distribution (in red and blue) image can be found [here](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/counts.svg) \n",
    "* So you know how the iris dataset has three classes/targets (0 = setosa, 1 = versicolor, 2 = virginica)? This tabular dataset has 4 classes/targets, initially labelled (as string/object) Class_1, Class_2, Class_3, and Class_4.  I will relabel these obviously as numeric, and have chosen to re-assign them the integer values:  1, 2, 3, and 4.  Check that, it wouldn't actually let me do that, which I find interesting.  I was forced to have to use labels as 0,1,2, and 3. \n",
    "* Just like the iris dataset has 4 features (sepal length, sepal width, petal length, petal width), this tabular dataset we are given has 50 features, **none** of which you can conclude too much about since they are given generic names of `feature_0` to `feature_49` \n",
    "* The real question is how severe is the imbalance below ? We will dive into this, its possible that using the default value scale_pos_weight of 1 is adequate ? \n",
    "* What we observe is a <u>*somewhat*</u> imbalanced classes, with Class 2 dominating with 57.50% of the total observation counts, almost 7x the Class 1 count and over 4x the Class 4 count..."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>5. &ensp; Training Data Feature Value Distribution</i></div> \n",
    "\n",
    "* I am going to re-use some medal awarded visualization [notebook code](https://www.kaggle.com/tombresee/nfl-eval) I did from the past [2021 NFL Big Data Bowl](https://www.kaggle.com/c/nfl-big-data-bowl-2021), but if you want to just see the below code, click [here](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/initial_histogram.txt)\n",
    "* When you have a large number of features, plotting things horizontally doesn't work very well, I choose to plot this vertically and I dont facet it into rows and columns, i like to be able to quickly compare every feature to the others..."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# this line is very important\n",
    "train_df['target'] = train_df['target'].map({'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3})\n",
    "temp = train_df.copy()\n",
    "temp.drop(['id','target'], inplace=True, axis=1)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set_style(\"ticks\", {'grid.linestyle': '--'})\n",
    "\n",
    "flierprops = dict(markerfacecolor='0.2', \n",
    "                  markersize=10,\n",
    "                  linestyle='none')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 13))\n",
    "\n",
    "ax.set_ylim(-7, 22)\n",
    "\n",
    "ax.set_title('\\nAll 50 Features:  Training Value Distribution\\n', fontsize=11, loc='left')\n",
    "\n",
    "# sns.boxplot(y='DefensePersonnel',\n",
    "#             x='Yards',\n",
    "#             data=dff,\n",
    "#             ax=ax,\n",
    "#             showfliers=False , \n",
    "#             #color='blue'\n",
    "#             )\n",
    "\n",
    "# sns.boxplot(y=dff['personnelD'].sort_values(ascending=False),\n",
    "#             x=dff['offensePlayResult'],\n",
    "#             ax=ax,\n",
    "#             showfliers=False ,\n",
    "#             linewidth=.8\n",
    "#             #color='blue'\n",
    "#             )\n",
    "\n",
    "sns.boxplot(data=temp, \n",
    "            orient=\"h\",\n",
    "            ax=ax,\n",
    "            showfliers=True,\n",
    "            fliersize=.5,\n",
    "            linewidth=.8\n",
    "            #color='blue'\n",
    "            )\n",
    "\n",
    "# ax.yaxis.grid(False)   # Show the horizontal gridlines\n",
    "# ax.xaxis.grid(True)  # Hide x-axis gridlines \n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "\n",
    "# Add transparency to colors\n",
    "for patch in ax.artists:\n",
    "  r, g, b, a = patch.get_facecolor()\n",
    "  patch.set_facecolor((r, g, b, .3))\n",
    "    \n",
    "# ax.set(xlabel=''common xlabel', ylabel='common ylabel', title='some title')\n",
    "ax.set(xlabel=\"\\nValue Distribution\\n\")\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "\n",
    "ax.xaxis.set_ticks_position('none') \n",
    "\n",
    "ax.set_ylabel('')\n",
    "\n",
    "#-----more control-----#\n",
    "ax.grid(linestyle='--', \n",
    "        linewidth='0.3', \n",
    "        color='lightgray', \n",
    "        alpha=0.8,\n",
    "        axis='x'\n",
    "       )\n",
    "\n",
    "plt.xlim(-10, 70)\n",
    "plt.axvline(0, 0,1, linewidth=.5, color=\"black\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# import os\n",
    "# os.chdir(r'/kaggle/working')\n",
    "# from IPython.display import FileLink \n",
    "# FileLink(r'closeup_histogram.svg')\n",
    "\n",
    "plt.show();"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:16.592578Z",
     "iopub.execute_input": "2022-04-21T17:39:16.59288Z",
     "iopub.status.idle": "2022-04-21T17:39:18.773804Z",
     "shell.execute_reply.started": "2022-04-21T17:39:16.592848Z",
     "shell.execute_reply": "2022-04-21T17:39:18.772996Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this line is very important\n",
    "train_df['target'] = train_df['target'].map({'Class_1':1, 'Class_2':2, 'Class_3':3, 'Class_4':4})\n",
    "temp = train_df.copy()\n",
    "temp.drop(['id','target'], inplace=True, axis=1)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set_style(\"ticks\", {'grid.linestyle': '--'})\n",
    "\n",
    "flierprops = dict(markerfacecolor='0.2', \n",
    "                  markersize=10,\n",
    "                  linestyle='none')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_xlim(-1, 1)\n",
    "\n",
    "ax.set_title('\\nAll 50 Features:  Training Value Distribution (showing chopped -2 thru +5 x-Range)\\n', fontsize=11, loc='left')\n",
    "\n",
    "# sns.boxplot(y='DefensePersonnel',\n",
    "#             x='Yards',\n",
    "#             data=dff,\n",
    "#             ax=ax,\n",
    "#             showfliers=False , \n",
    "#             #color='blue'\n",
    "#             )\n",
    "\n",
    "# sns.boxplot(y=dff['personnelD'].sort_values(ascending=False),\n",
    "#             x=dff['offensePlayResult'],\n",
    "#             ax=ax,\n",
    "#             showfliers=False ,\n",
    "#             linewidth=.8\n",
    "#             #color='blue'\n",
    "#             )\n",
    "\n",
    "sns.boxplot(data=temp, \n",
    "            orient=\"h\",\n",
    "            ax=ax,\n",
    "            showfliers=True,\n",
    "            fliersize=.5,\n",
    "            linewidth=.8\n",
    "            #color='blue'\n",
    "            )\n",
    "\n",
    "# ax.yaxis.grid(False)   # Show the horizontal gridlines\n",
    "# ax.xaxis.grid(True)  # Hide x-axis gridlines \n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "# Add transparency to colors\n",
    "for patch in ax.artists:\n",
    "  r, g, b, a = patch.get_facecolor()\n",
    "  patch.set_facecolor((r, g, b, .3))\n",
    "    \n",
    "# ax.set(xlabel=''common xlabel', ylabel='common ylabel', title='some title')\n",
    "ax.set(xlabel=\"\\nValue Distribution\\n\")\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "\n",
    "ax.xaxis.set_ticks_position('none') \n",
    "\n",
    "ax.set_ylabel('')\n",
    "\n",
    "#-----more control-----#\n",
    "ax.grid(linestyle='--', \n",
    "        linewidth='0.3', \n",
    "        color='lightgray', \n",
    "        alpha=0.8,\n",
    "        axis='x'\n",
    "       )\n",
    "\n",
    "plt.xlim(-2, 5)\n",
    "plt.axvline(0, 0,1, linewidth=.5, color=\"black\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:18.775133Z",
     "iopub.execute_input": "2022-04-21T17:39:18.775659Z",
     "iopub.status.idle": "2022-04-21T17:39:21.014534Z",
     "shell.execute_reply.started": "2022-04-21T17:39:18.775621Z",
     "shell.execute_reply": "2022-04-21T17:39:21.013624Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Somewhat unusual feature values distribution, many of the features have values centered closely around 0...\n",
    "* Let's zoom in **ONLY** on the features that appear to be mainly non-zero:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "temp2 = temp.copy()\n",
    "\n",
    "temp2 = temp2[ ['feature 7', \n",
    "                'feature 9', \n",
    "                'feature 14', \n",
    "                'feature 15', \n",
    "                'feature 19', \n",
    "                'feature 24', \n",
    "                'feature 28', \n",
    "                'feature 31', \n",
    "                'feature 34', \n",
    "                'feature 38', \n",
    "                'feature 40', \n",
    "                'feature 48']]\n",
    "dff = temp2.copy()\n",
    "\n",
    "flierprops = dict(markerfacecolor='0.75', \n",
    "                  markersize=.1,\n",
    "                  linestyle='none')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9.2,10))\n",
    "ax.set_ylim(-6, 11)\n",
    "ax.set_title('Training Data:  Closeup of Specific Features\\n', fontsize=12, loc='left')\n",
    "sns.boxplot(data=dff,\n",
    "            ax=ax,\n",
    "            showfliers=False,\n",
    "            width=.8,\n",
    "            #color='blue'\n",
    "            )\n",
    "            #flierprops=flierprops)\n",
    "#Completely hide tick markers...\n",
    "# ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "# ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.yaxis.grid(True)   # Show the horizontal gridlines\n",
    "ax.xaxis.grid(False)  # Hide x-axis gridlines \n",
    "\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_minor_locator(plt.MultipleLocator(5))\n",
    "\n",
    "# Add transparency to colors\n",
    "for patch in ax.artists:\n",
    "  r, g, b, a = patch.get_facecolor()\n",
    "  patch.set_facecolor((r, g, b, .3))\n",
    "    \n",
    "# ax.set(xlabel=''common xlabel', ylabel='common ylabel', title='some title')\n",
    "ax.set(xlabel=\"\\nCore Non-zero Features (f7 = Feature 7 for instance)\\n\")\n",
    "ax.set(ylabel=\"\")\n",
    "\n",
    "ax.set_xticklabels(['f 7', 'f 9','f 14', 'f 15', 'f 19', 'f 24', 'f 28', 'f 31', 'f 34', 'f 38', 'f 40', 'f 48'])\n",
    "\n",
    "# plt.title('My subtitle',fontsize=16)\n",
    "# plt.suptitle('My title',fontsize=24, x=0, y=1,ha=\"left\")\n",
    "plt.text(x=6.8, y=8.2, s='Feature 38 is the\\nwidest in range', fontsize=10)\n",
    "# plt.text(x=4.7, y=4.6, s='The size of each point corresponds to sepal width', fontsize=8, alpha=0.75)\n",
    "# ax.set(xlabel='common xlabel', ylabel='common ylabel', title='some title'\n",
    "\n",
    "plt.tick_params(left=False)\n",
    "plt.tick_params(bottom=False)\n",
    "\n",
    "# plt.savefig('closeup_histogram.svg', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:21.016042Z",
     "iopub.execute_input": "2022-04-21T17:39:21.01638Z",
     "iopub.status.idle": "2022-04-21T17:39:21.739723Z",
     "shell.execute_reply.started": "2022-04-21T17:39:21.016345Z",
     "shell.execute_reply": "2022-04-21T17:39:21.738941Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Feature 38 appears to have the widest distribution, followed by Feature 14...\n",
    "* Visualization of the above chart available [here](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/closeup_histogram.svg)\n",
    "* Below we will count the number of unique category values per individual feature.  We see that Feature_38 has 71 unique values, and Feature_14 has 52. \n",
    "* Overall, this is somewhat unusual data, appears to be a massive count of zero values in features...\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import seaborn\n",
    "# seaborn.set(style=\"whitegrid\")\n",
    "# seaborn.swarmplot(x=temp['feature 24'])"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:21.741002Z",
     "iopub.execute_input": "2022-04-21T17:39:21.741536Z",
     "iopub.status.idle": "2022-04-21T17:39:21.745022Z",
     "shell.execute_reply.started": "2022-04-21T17:39:21.741496Z",
     "shell.execute_reply": "2022-04-21T17:39:21.744052Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tempd = train_df.copy()          \n",
    "tempd.drop('id', axis=1, inplace=True)\n",
    "tempd.drop('target', axis=1, inplace=True)\n",
    "tempd = tempd.apply(lambda x: len(x.unique()))\n",
    "#tempd = tempd.sort_values(ascending=False, inplace=True)\n",
    "tempd = tempd.sort_values(ascending=False)\n",
    "tempd = pd.DataFrame(tempd)\n",
    "tempd.columns=['unique value count']\n",
    "tempd.index.name = 'feature_id'\n",
    "cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "tempd.style.set_caption('Num of unique values per Feature').background_gradient(cmap=cm)\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:21.747975Z",
     "iopub.execute_input": "2022-04-21T17:39:21.748562Z",
     "iopub.status.idle": "2022-04-21T17:39:21.838738Z",
     "shell.execute_reply.started": "2022-04-21T17:39:21.748512Z",
     "shell.execute_reply": "2022-04-21T17:39:21.83772Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Why did I plot the above ?  Because look at the bottom five features (Features 44, 2, 22, 36, 13).  They have extremely few unique values, out of 100,000 entries.  That has got to be some previous true categorical value encoded... but who knows.  Looking at the top, one has to think either those values are actually categories or possibly actually continuous variables of some form...\n",
    "* So we need to look at the percentages (per feature) of the time the values are zero (below)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tempd = train_df.copy()          \n",
    "tempd.drop('id', axis=1, inplace=True)\n",
    "tempd.drop('target', axis=1, inplace=True)\n",
    "tempd = tempd[tempd == 0].count(axis=0)/len(tempd.index)\n",
    "tempd = tempd.sort_values(ascending=False)\n",
    "tempd = pd.DataFrame(tempd)\n",
    "tempd.columns=['% of values that are 0']\n",
    "tempd.index.name = 'feature_id'\n",
    "cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "#tempd.style.format(\"{:.2%}\")\n",
    "tempd.style.format(\"{:.1%}\").set_caption('Percent of feature values equaling zero').background_gradient(cmap=cm)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:21.840847Z",
     "iopub.execute_input": "2022-04-21T17:39:21.841249Z",
     "iopub.status.idle": "2022-04-21T17:39:22.016782Z",
     "shell.execute_reply.started": "2022-04-21T17:39:21.84121Z",
     "shell.execute_reply": "2022-04-21T17:39:22.015755Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* So what do we see here ?  I would consider this is a somewhat unusual dataset, with a large percentage of zero valeus in some features.  Which really also means those features that have less percent of zero will be more 'impactful' in the model calculation one would suspect. We will keep our eye on feature_38 and feature_14 when we plot things like xgboost importance.  But what bothers me here is that overall there aren't that many actual values that are nonzero, so going to be somewhat hard to actually create an accurate model.  We need variation.  We have not that much. "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tempd = train_df.copy()          \n",
    "tempd.drop('id', axis=1, inplace=True)\n",
    "tempd.drop('target', axis=1, inplace=True)\n",
    "# np.count_nonzero(tempd)\n",
    "print (\"Number of entries that are nonzero:    \", (tempd != 0).values.sum())\n",
    "print (\"Number of entries that are zero:       \", (tempd == 0).values.sum())\n",
    "print (\"Number of entries that are below zero: \", np.sum(tempd.values < 0))\n",
    "print (\"Total Number of entries in training df:\", (tempd == 0).values.sum() + (tempd != 0).values.sum())"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:22.018743Z",
     "iopub.execute_input": "2022-04-21T17:39:22.01947Z",
     "iopub.status.idle": "2022-04-21T17:39:22.114147Z",
     "shell.execute_reply.started": "2022-04-21T17:39:22.019405Z",
     "shell.execute_reply": "2022-04-21T17:39:22.113193Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tempd = train_df.copy()          \n",
    "# tempd.drop('id', axis=1, inplace=True)\n",
    "# tempd.drop('target', axis=1, inplace=True)\n",
    "# a = tempd.to_numpy()\n",
    "# b = sparse.csr_matrix(a)\n",
    "# from matplotlib.pyplot import figure\n",
    "# figure(figsize=(13, 5), dpi=180)\n",
    "\n",
    "# plt.spy(b)\n",
    "# # plt.spy(x, precision = 0.1, markersize = 5)\n",
    "\n",
    "# fig = matplotlib.pyplot.gcf()\n",
    "# fig.set_size_inches(18.5, 10.5)\n",
    "# # fig.savefig('test2png.png', dpi=100)\n",
    "# plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "# fig.set_dpi(100)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:22.115663Z",
     "iopub.execute_input": "2022-04-21T17:39:22.116061Z",
     "iopub.status.idle": "2022-04-21T17:39:22.120308Z",
     "shell.execute_reply.started": "2022-04-21T17:39:22.116018Z",
     "shell.execute_reply": "2022-04-21T17:39:22.119304Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>6. &ensp; XGBoost Model Creation</i></div> \n",
    "\n",
    "* In my case, we obviously want to create an XGBoost classifer (and not regressor)\n",
    "* As a follow up, will look into when to use the objective function 'multi:softmax' vs 'multi:softprob' for multi-class problems like this, i'm still not convinced the documentation makes sense\n",
    "* We have technically four classes, so i will specifically code the num_class (its not num_classes fyi) variable to 4 (remembering that that means the actual encoded targets MUST be 0,1,... num_class-1, i.e. [0,1,2,3]\n",
    "* Need to dive in more to base_score, it is the initial prediction score of all instances, i.e. global bias but I want to undertand this mathematically, will draw it out...\n",
    "* XGBoost (as well as other gradient boosting machine algs) have a number of parameters that can be tuned to avoid overfitting. For instance, max_depth can be set to avoid overfitting (in fact, the more, the more likely to overfit)\n",
    "* I don't think most people know you can either use `eta` (formally documented in the xgboost docs) or what is technically an alias for eta, i.e. `learning_rate`. I know what eta is, but really how many people if you didn't tell them would be able to give you the greek symbol for it, much less know how to say $\\eta$ correctly, so I will actually never use parameter eta, i will specifically call it out as learning_rate.  \n",
    "\n",
    "* Upgraded xgboost library to very latest (**xgb version: 1.4.2**)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from io import StringIO\n",
    "\n",
    "data = (\"\"\"\n",
    "\n",
    "id,param,def_value,descr\n",
    "\n",
    "1,base_score,0.5,initial prediction score of all instances\n",
    "\n",
    "2,booster,gbtree, can be gbtree gblinear or dart. gbtree is tree-based model\n",
    "\n",
    "3,num_estimates,100,number of estimates\n",
    "\n",
    "4,max_depth, 6,maximum depth of a tree (too high a value leads to overfitting)\n",
    "\n",
    "5,eta, 0.3, step size shrinkage (learning rate) used in update to prevent overfitting\n",
    "\n",
    "6,gamma, 0, alias of min_split_loss with min loss reduction required to make a further partition on a leaf node of the tree\n",
    "\n",
    "7,min_child_weight,1,min sum of instance weight (hessian) needed in a child\n",
    "\n",
    "8,max_delta_step, 0, max delta step allowed each output to be\n",
    "\n",
    "9,subsample,1,subsample ratio of the training instances\n",
    "\n",
    "10, sampling_method, uniform, method used to sample the training instances\n",
    "\n",
    "11,colsample_bytree, 1, subsampling\n",
    "\n",
    "12,colsample_bylevel, 1, subsampling\n",
    "\n",
    "13, colsample_bynode,1,subsampling\n",
    "\n",
    "14,lambda,1, alias of reg_lambda the L2 regularization term on weights\n",
    "\n",
    "15, alpha, 0, alias of reg_alpha the L1 regularization term on weights\n",
    "\n",
    "16, tree_method, auto, tree construction alg used in xgboost\n",
    "\n",
    "17,sketch_eps, 0.03,-\n",
    "\n",
    "18, scale_pos_weight, 1, controls balance of positive and negative weights\n",
    "\n",
    "19, updater, grow_colmaker prun, -\n",
    "\n",
    "20, refresh_leaf, 1, refresh updater\n",
    "\n",
    "21,process_type, default, choices of default and update\n",
    "\n",
    "22, grow_policy, depthwise, controls way new nodes are added to the tree\n",
    "\n",
    "23, max_leaves,0, max number of nodes to be added\n",
    "\n",
    "24,max_bin, 256, only used if tree_method is set to hist or gpu_hist\n",
    "\n",
    "25, verbosity, 1, verbosity of printing messages\n",
    "\n",
    "26, num_feature, automatically discovered, feature dimension used in boosting set to max dim of the feature\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "mapper = pd.read_csv(StringIO(data))\n",
    "\n",
    "d = dict(selector=\"th\",\n",
    "    props=[('text-align', 'center')])\n",
    "\n",
    "mapper = mapper.set_index('id')\n",
    "\n",
    "mapper.style.set_properties(**{'width':'10em', 'text-align':'center'})\\\n",
    "        .set_table_styles([d])"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:22.122081Z",
     "iopub.execute_input": "2022-04-21T17:39:22.122491Z",
     "iopub.status.idle": "2022-04-21T17:39:22.151721Z",
     "shell.execute_reply.started": "2022-04-21T17:39:22.12245Z",
     "shell.execute_reply": "2022-04-21T17:39:22.150816Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* There are actually more than listed above. So now you see my point about how powerful XGBoost is, as well as why it needs a drivers license, there are a LOT of parameters you can vary...\n",
    "* A default that you cannot change is `validate_parameters`, which is 'on' by default.  XGBoost will perform validation of input parameters to check whether any parameter you entered above is used or **not**.  This is a somewhat experimental feature right now, but i think its pretty cool because it actually tells you when you make a MISTAKE. \n",
    "  * I created a fake parameter called 'tom_is_awesome', and set it equal to 'True'.  What the algorithm spits out after you run it then would be `Parameters: { \"tom_is_awesome\" } might not be used`, but technically it's NOT saying it isn't True.  This is helpful when you accidentally are adding parameters that aren't recognized by XGBoost.    \n",
    "* **GPU support**:  Nice feature, where tree construction (training) and prediction can be accelerated with CUDA-capable GPUs. Set the tree_method to gpu_hist, and predictor to gpu_predictor. \n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# basically redoing things to be safe:\n",
    "train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n",
    "test = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\n",
    "sample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "le = LabelEncoder() # practice, i could do it manually if i wanted... \n",
    "train.target = le.fit_transform(train.target)\n",
    "features = train.columns[1:51] # only fifty \n",
    "X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=[\"id\",\"target\"]), train.target, stratify=train.target)\n",
    "# y_train = y_train.map({'Class_1':0, \n",
    "#                        'Class_2':1, \n",
    "#                        'Class_3':2, \n",
    "#                        'Class_4':3})"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:22.152849Z",
     "iopub.execute_input": "2022-04-21T17:39:22.153153Z",
     "iopub.status.idle": "2022-04-21T17:39:22.710636Z",
     "shell.execute_reply.started": "2022-04-21T17:39:22.153122Z",
     "shell.execute_reply": "2022-04-21T17:39:22.709756Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* I will use bst versus clf naming notation, per the XGBoost documentation to maintain consistency"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# at one point i actually wanted to \n",
    "# encode the target classes to be \n",
    "# 1,2,3,4 to match their names (and \n",
    "# not 0,1,2,3) but don't do that :) \n",
    "\n",
    "# i will create a base xgboost classifier, \n",
    "# but will specifically call out all of the \n",
    "# default parameters to explain what they\n",
    "# actually do. \n",
    "#\n",
    "# later, i will tweak...\n",
    "\n",
    "bst = xgb.XGBClassifier(\\\n",
    "predictor='gpu_predictor',\n",
    "tree_method='gpu_hist',\n",
    "# tree_method='exact',\n",
    "base_score=0.5, \n",
    "booster='gbtree', \n",
    "colsample_bylevel=1,\n",
    "colsample_bynode=1, \n",
    "colsample_bytree=1,\n",
    "# set during training ?  \n",
    "# eval_metric=\"mlogloss\",\n",
    "gamma=0,\n",
    "gpu_id=-1,\n",
    "importance_type='gain', \n",
    "interaction_constraints='',\n",
    "learning_rate=0.2,\n",
    "max_delta_step=0, \n",
    "#max_depth=6 (def)\n",
    "max_depth=3, \n",
    "min_child_weight=1, \n",
    "monotone_constraints='()',\n",
    "# n_estimators=100 (def)\n",
    "n_estimators=500, # 5 x def\n",
    "n_jobs=4, \n",
    "num_class=4, # < - - \n",
    "num_parallel_tree=1,\n",
    "# objective='multi:softmax', \n",
    "objective='multi:softprob',                \n",
    "random_state=42, \n",
    "reg_alpha=0,\n",
    "reg_lambda=1, \n",
    "scale_pos_weight=None,\n",
    "subsample=1,\n",
    "use_label_encoder=False,\n",
    "# validate_parameters=1,\n",
    "verbosity=3, \n",
    "eval_metric = 'mlogloss', \n",
    "tom_is_awesome=True)\n",
    "\n",
    "\n",
    "bst.fit(X_train,\n",
    "y_train,\n",
    "verbose=True,\n",
    "#  should i restore this ? eval_metric=['mlogloss'],\n",
    "# multiclass logloss...\n",
    "# already default with latest release\n",
    "# early_stopping_rounds=10,\n",
    "# initial attempt at early stopping, \n",
    "# hold for now\n",
    ")"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:22.712Z",
     "iopub.execute_input": "2022-04-21T17:39:22.71234Z",
     "iopub.status.idle": "2022-04-21T17:39:25.351064Z",
     "shell.execute_reply.started": "2022-04-21T17:39:22.712304Z",
     "shell.execute_reply": "2022-04-21T17:39:25.350208Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* I will use bst versus clf naming notation, per the XGBoost documentation to maintain consistency"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre style=\"font-family:Consolas; font-size:.7em\">\n",
    "--- OUTPUT FROM MODEL CREATION: ---\n",
    "\n",
    "XGBClassifier(\n",
    "base_score=0.5, \n",
    "booster='gbtree', \n",
    "colsample_bylevel=1,\n",
    "colsample_bynode=1, \n",
    "colsample_bytree=1, \n",
    "gamma=0,\n",
    "gpu_id=-1,\n",
    "importance_type='gain', \n",
    "interaction_constraints='',\n",
    "learning_rate=0.2, \n",
    "max_delta_step=0, \n",
    "max_depth=3,\n",
    "min_child_weight=1, \n",
    "missing=nan, \n",
    "monotone_constraints='()',\n",
    "n_estimators=400, \n",
    "n_jobs=4, \n",
    "num_class=4, \n",
    "num_parallel_tree=1,\n",
    "objective='multi:softprob', \n",
    "random_state=42, \n",
    "reg_alpha=0,\n",
    "reg_lambda=1, \n",
    "scale_pos_weight=None, \n",
    "subsample=1,\n",
    "tree_method='exact', \n",
    "use_label_encoder=False,\n",
    "validate_parameters=1, \n",
    "verbosity=0)\n",
    "</pre>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[click here to see verbose run output, easier to view](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/proto_run.txt)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"font-size:11x;\">Let's say you are a relatively motivated person and wanted to review every iteration of the model and all of its parameters, you could do something like this below.  Save the model as a json file.  Open that json file and convert the contents to a python dictionary, and you are on your way...</span>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#  I will actually comment this out for now, \n",
    "#  but if you want it saved, uncomment out:\n",
    "bst.save_model('mark_I.json')\n",
    "\n",
    "# --- if you want to download file from kaggle ---\n",
    "# import os \n",
    "# os.chdir(r'/kaggle/working')\n",
    "# from IPython.display import FileLink \n",
    "# FileLink(r'mark_I.json')\n",
    "# to download it and look at it on github\n",
    "\n",
    "with open('mark_I.json') as json_file:\n",
    "    blueprint = json.load(json_file)\n",
    "    # now we have the model in python dictionary form"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.353675Z",
     "iopub.execute_input": "2022-04-21T17:39:25.353928Z",
     "iopub.status.idle": "2022-04-21T17:39:25.490328Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.353901Z",
     "shell.execute_reply": "2022-04-21T17:39:25.489509Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0].keys()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.491625Z",
     "iopub.execute_input": "2022-04-21T17:39:25.491949Z",
     "iopub.status.idle": "2022-04-21T17:39:25.497874Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.491911Z",
     "shell.execute_reply": "2022-04-21T17:39:25.497018Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **blueprint['learner']['feature_names']** &ensp;  feature_0 thru feature_49\n",
    "* **blueprint['learner']['feature_types']** &ensp;  feature dtypes, i.e. int\n",
    "* **blueprint['learner']['gradient_booster']['model'].keys()** &ensp; 'gbtree_model_param', 'tree_info', 'trees'\n",
    "* **len(blueprint['learner']['gradient_booster']['model']['trees'])** &ensp; 1600\n",
    "* **blueprint['learner']['gradient_booster']['model']['trees'][0]** &ensp; first tree metadata (shown below)\n",
    "* **blueprint['learner']['gradient_booster']['model']['trees'][0].keys()** &ensp; ['base_weights', 'categories', 'categories_nodes', 'categories_segments', 'categories_sizes', 'default_left', 'id', 'left_children', 'loss_changes', 'parents', 'right_children', 'split_conditions', 'split_indices', 'split_type', 'sum_hessian', 'tree_param']  < - - - now we are talkin' ! ! ! \n",
    "* now we have molecular level tree data...\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* show me the **first tree's** molecular data (you also see how you can open the dict to various keys to get precisely what you want):"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.499041Z",
     "iopub.execute_input": "2022-04-21T17:39:25.499543Z",
     "iopub.status.idle": "2022-04-21T17:39:25.511265Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.4995Z",
     "shell.execute_reply": "2022-04-21T17:39:25.510318Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will look more into this shortly..."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>7. &ensp; Feature Importance</i></div>  \n",
    "\n",
    "* XGBoost actually has a built in `plot_importance` function that is relatively useful, but its default visualization is somewhat unappealing.  The actual values are contained within ```your_model.feature_importances_``` (after you train the model obviously)\n",
    "* Just printing out the feature importances, you see abc `print(mymodel.feature_importances_)`"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bst.get_params(deep=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.512706Z",
     "iopub.execute_input": "2022-04-21T17:39:25.513153Z",
     "iopub.status.idle": "2022-04-21T17:39:25.52349Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.513039Z",
     "shell.execute_reply": "2022-04-21T17:39:25.522555Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bst.get_xgb_params()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.524987Z",
     "iopub.execute_input": "2022-04-21T17:39:25.52545Z",
     "iopub.status.idle": "2022-04-21T17:39:25.533996Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.525416Z",
     "shell.execute_reply": "2022-04-21T17:39:25.532819Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bst.feature_importances_"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.535435Z",
     "iopub.execute_input": "2022-04-21T17:39:25.53588Z",
     "iopub.status.idle": "2022-04-21T17:39:25.544918Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.535846Z",
     "shell.execute_reply": "2022-04-21T17:39:25.544046Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* bst.classes_:  *array([0, 1, 2, 3])*\n",
    "* Actual type of the bst instance is:  *xgboost.sklearn.XGBClassifier*\n",
    "* bst.n_features_in_:  *50*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "feature_names = ['feature 0', 'feature 1', 'feature 2', 'feature 3', 'feature 4',\n",
    "       'feature 5', 'feature 6', 'feature 7', 'feature 8', 'feature 9',\n",
    "       'feature 10', 'feature 11', 'feature 12', 'feature 13', 'feature 14',\n",
    "       'feature 15', 'feature 16', 'feature 17', 'feature 18', 'feature 19',\n",
    "       'feature 20', 'feature 21', 'feature 22', 'feature 23', 'feature 24',\n",
    "       'feature 25', 'feature 26', 'feature 27', 'feature 28', 'feature 29',\n",
    "       'feature 30', 'feature 31', 'feature 32', 'feature 33', 'feature 34',\n",
    "       'feature 35', 'feature 36', 'feature 37', 'feature 38', 'feature 39',\n",
    "       'feature 40', 'feature 41', 'feature 42', 'feature 43', 'feature 44',\n",
    "       'feature 45', 'feature 46', 'feature 47', 'feature 48', 'feature 49']\n",
    "    \n",
    "plt.barh(feature_names, bst.feature_importances_); \n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:25.54587Z",
     "iopub.execute_input": "2022-04-21T17:39:25.546123Z",
     "iopub.status.idle": "2022-04-21T17:39:26.073217Z",
     "shell.execute_reply.started": "2022-04-21T17:39:25.546081Z",
     "shell.execute_reply": "2022-04-21T17:39:26.072371Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds = bst.predict(X_val) \n",
    "preds_sub = bst.predict_proba(X_test)\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "xgb.plot_importance(bst, ax=ax); \n",
    "# fix, this is not pretty (use altair)\n",
    "\n",
    "# plt.style.use('seaborn-white')\n",
    "# preds = bst.predict(X_val) \n",
    "# preds_sub = bst.predict_proba(X_test)\n",
    "# fig, ax = plt.subplots(figsize=(12, 14))\n",
    "# xgb.plot_importance(bst, ax=ax); \n",
    "# # fix, this is not pretty (use altair)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:26.074629Z",
     "iopub.execute_input": "2022-04-21T17:39:26.075203Z",
     "iopub.status.idle": "2022-04-21T17:39:27.014261Z",
     "shell.execute_reply.started": "2022-04-21T17:39:26.075165Z",
     "shell.execute_reply": "2022-04-21T17:39:27.013471Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plt.style.use('seaborn-white')\n",
    "\n",
    "feature_names = ['feature 0', 'feature 1', 'feature 2', 'feature 3', 'feature 4',\n",
    "       'feature 5', 'feature 6', 'feature 7', 'feature 8', 'feature 9',\n",
    "       'feature 10', 'feature 11', 'feature 12', 'feature 13', 'feature 14',\n",
    "       'feature 15', 'feature 16', 'feature 17', 'feature 18', 'feature 19',\n",
    "       'feature 20', 'feature 21', 'feature 22', 'feature 23', 'feature 24',\n",
    "       'feature 25', 'feature 26', 'feature 27', 'feature 28', 'feature 29',\n",
    "       'feature 30', 'feature 31', 'feature 32', 'feature 33', 'feature 34',\n",
    "       'feature 35', 'feature 36', 'feature 37', 'feature 38', 'feature 39',\n",
    "       'feature 40', 'feature 41', 'feature 42', 'feature 43', 'feature 44',\n",
    "       'feature 45', 'feature 46', 'feature 47', 'feature 48', 'feature 49']\n",
    "    \n",
    "# plt.barh(feature_names, bst.feature_importances_); \n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# df04 = tf.groupby('PossessionTeam')['Yards'].agg(sum).sort_values(ascending=True)\n",
    "# df04 = pd.DataFrame(df04)\n",
    "# df04['group'] = df04.index\n",
    "\n",
    "my_range=range(0,50)\n",
    "my_size=range(0,50)\n",
    "# my_color = 'skyblue'\n",
    "orb_size = [.8]*50\n",
    "\n",
    "# my_size = [1] *50 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,11))\n",
    "\n",
    "# Create a color if the group is \"B\"\n",
    "my_color=np.where(bst.feature_importances_ > 0.022, 'orange', 'skyblue')\n",
    "\n",
    "# my_color=np.where(df04[  ('group'=='NE') | ('group'=='NO')  ], 'orange', 'skyblue')\n",
    "\n",
    "# original code:  my_color=np.where( (df04.group == 'NE') | (df04.group == 'NO') | (df04.group == 'LA') , 'orange', 'skyblue')\n",
    "\n",
    "##movies[(movies.duration >= 200) | (movies.genre == 'Drama')]\n",
    "##df04[(df04.group == 'NE') | (df04.group == 'NO') ]\n",
    "##(movies.duration >= 200) & (movies.genre == 'Drama')\n",
    "\n",
    "# original code:  my_size=np.where(df04['group']=='B', 70, 30)\n",
    " \n",
    "# original code:  plt.hlines(y=my_range, xmin=0, xmax=df04['Yards'], color=my_color, alpha=0.4)\n",
    "# backup:         plt.hlines(y=feature_names , xmin=0, xmax=df04['Yards'], color=my_color, alpha=0.4)\n",
    "plt.hlines(y=feature_names , xmin=0, xmax= bst.feature_importances_, color=my_color, alpha=0.4)\n",
    "\n",
    "# original code:  plt.scatter(df04.Yards, my_range, color=my_color, s=my_size, alpha=1)\n",
    "# plt.scatter(bst.feature_importances_, my_range, color=\"orange\", s=my_size, alpha=1)\n",
    "# plt.scatter(bst.feature_importances_, my_range, color=my_color, s=my_size, alpha=1)\n",
    "\n",
    "plt.scatter(bst.feature_importances_, my_range, color=my_color, alpha=.9)\n",
    "\n",
    "# plt.yticks(my_range, df04.group)\n",
    "\n",
    "plt.title(\"\\nCalculated Feature Importance (Raw Value) per individual feature\", loc='left', fontsize=11)\n",
    "plt.xlabel(\"\\n Feature Importance 'Score'\\n\", fontsize=10)\n",
    "plt.ylabel('')\n",
    "ax.spines['top'].set_linewidth(.3)  \n",
    "ax.spines['left'].set_linewidth(.3)  \n",
    "ax.spines['right'].set_linewidth(.3)  \n",
    "ax.spines['bottom'].set_linewidth(.3)  \n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "plt.axvline(x=0.01, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\n",
    "plt.axvline(x=0.02, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\n",
    "plt.axvline(x=0.03, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\n",
    "plt.axvline(x=0.04, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\n",
    "plt.tight_layout()\n",
    "plt.show();\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:27.018349Z",
     "iopub.execute_input": "2022-04-21T17:39:27.018779Z",
     "iopub.status.idle": "2022-04-21T17:39:27.83689Z",
     "shell.execute_reply.started": "2022-04-21T17:39:27.018743Z",
     "shell.execute_reply": "2022-04-21T17:39:27.836053Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **Note:** &ensp; There is a difference between 'feature importance' and XGBoost's feature importance plot (which is actually based on F-Score)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>8. &ensp; Plotting Decision Trees</i></div> \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Let's plot the first decision tree here (top down view):"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# xgb.plot_tree(\\\n",
    "# bst, \n",
    "# num_trees=0,\n",
    "# plot_width = 50,\n",
    "# plot_height = 50,\n",
    "# render = True,\n",
    "# show_node_id=True, \n",
    "# )\n",
    "# fig = matplotlib.pyplot.gcf()\n",
    "# fig.set_size_inches(100, 100)\n",
    "# # fig.savefig('tree.svg')\n",
    "\n",
    "\n",
    "# xgb.plot_tree(bst)\n",
    "# fig = matplotlib.pyplot.gcf()\n",
    "# fig.set_size_inches(22,11)\n",
    "\n",
    "# xgb.plot_tree(bst, num_trees=4); plt.show()\n",
    "# xgb.plot_tree(bst, num_trees=0, rankdir='LR'); plt.show()\n",
    "    \n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:27.838748Z",
     "iopub.execute_input": "2022-04-21T17:39:27.839225Z",
     "iopub.status.idle": "2022-04-21T17:39:27.844576Z",
     "shell.execute_reply.started": "2022-04-21T17:39:27.839178Z",
     "shell.execute_reply": "2022-04-21T17:39:27.84357Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xgb.plot_tree(bst, num_trees=0)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(20,20)\n",
    "# fig.savefig('tree.svg') < - saving if you want\n",
    "\n",
    "# later can use:\n",
    "# import os\n",
    "# os.chdir(r'/kaggle/working')\n",
    "# from IPython.display import FileLink \n",
    "# FileLink(r'tree.svg')"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:27.846318Z",
     "iopub.execute_input": "2022-04-21T17:39:27.847048Z",
     "iopub.status.idle": "2022-04-21T17:39:28.742184Z",
     "shell.execute_reply.started": "2022-04-21T17:39:27.84701Z",
     "shell.execute_reply": "2022-04-21T17:39:28.741214Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "STOP FOR A SECOND: let's see if we can get this data another way...and really understand XGBOOST ! \n",
    "\n",
    "We will use our json model data we pulled earlier, and see now if we can reverse-engineer the parameters in that blueprint-type json file ...\n",
    "\n",
    "If we enter:  `blueprint['learner']['gradient_booster']['model']['trees'][0]['split_indices']`, we get output: `[23, 29, 7, 26, 14, 37, 0, 0, 0, 0, 0, 0, 0]`\n",
    "\n",
    "Now we look at our plot above, and bam we get it now:  first split is at `23` i.e. feature_23 (and you see in the plot), and second split is at `29`, i.e. feature_29 (upper left of plot), and to the right of that we see `7` i.e. feature_7 (upper right in plot), and on down to feature_`26`, feature_`14`, feature_`37`, and then we stop (three levels stopped). \n",
    "\n",
    "If we enter:  `blueprint['learner']['gradient_booster']['model']['trees'][0]['split_conditions']`, we get output:  `[0.5, 10.5, 23.5, 16.5, 0.5, 11.5, 0.12173913, -0.08919667, -0.018791948, -0.0, 0.16000001, -0.08308835, 0.06315789]`.  See if you can see those numbers in the plot. And then it hits you, they are the spliting conditions (shown physically inside the circles), i.e the first feature (feature_23) is split on the first split point (0.5), and the second feature (feature_29) is split on the second split_condition (10.5), etc etc. \n",
    "\n",
    "If we enter:  `blueprint['learner']['gradient_booster']['model']['trees'][0]['tree_param']['num_nodes']`, we see output `13`.  Count the total number of nodes, you see 13 right ?\n",
    "\n",
    "Seven 'left' nodes, seven 'right' nodes.  node index for even:  2, 4, 6, 8, 10, 12, 14. node index for odd: 1, 3, 5, 7, 9, 11, 13. "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0].keys()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.743607Z",
     "iopub.execute_input": "2022-04-21T17:39:28.744061Z",
     "iopub.status.idle": "2022-04-21T17:39:28.754088Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.744013Z",
     "shell.execute_reply": "2022-04-21T17:39:28.753063Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]['split_indices']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.755459Z",
     "iopub.execute_input": "2022-04-21T17:39:28.755904Z",
     "iopub.status.idle": "2022-04-21T17:39:28.763075Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.755867Z",
     "shell.execute_reply": "2022-04-21T17:39:28.762189Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]['split_conditions']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.764399Z",
     "iopub.execute_input": "2022-04-21T17:39:28.764747Z",
     "iopub.status.idle": "2022-04-21T17:39:28.773042Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.764711Z",
     "shell.execute_reply": "2022-04-21T17:39:28.772166Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]['tree_param']['num_nodes']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.77449Z",
     "iopub.execute_input": "2022-04-21T17:39:28.774859Z",
     "iopub.status.idle": "2022-04-21T17:39:28.787405Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.774823Z",
     "shell.execute_reply": "2022-04-21T17:39:28.786254Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]['base_weights']\n",
    "# base weight, before learning rate"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.788993Z",
     "iopub.execute_input": "2022-04-21T17:39:28.789539Z",
     "iopub.status.idle": "2022-04-21T17:39:28.798751Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.789499Z",
     "shell.execute_reply": "2022-04-21T17:39:28.797516Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "blueprint['learner']['gradient_booster']['model']['trees'][0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.800233Z",
     "iopub.execute_input": "2022-04-21T17:39:28.800671Z",
     "iopub.status.idle": "2022-04-21T17:39:28.813745Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.800631Z",
     "shell.execute_reply": "2022-04-21T17:39:28.812774Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now lets get fancy:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nFor the 191st decision tree, we see\", blueprint['learner']['gradient_booster']['model']['trees'][190]['tree_param']['num_nodes'], \"nodes\")\n",
    "print(\"We also see that the first split condition value was\", blueprint['learner']['gradient_booster']['model']['trees'][190]['split_conditions'][0])\n",
    "print(\"We also see that the third split condition value was\", blueprint['learner']['gradient_booster']['model']['trees'][190]['split_conditions'][2])\n",
    "print(\"We also see that the fourth split condition value was\", blueprint['learner']['gradient_booster']['model']['trees'][190]['split_conditions'][3], \"\\n\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.815249Z",
     "iopub.execute_input": "2022-04-21T17:39:28.815995Z",
     "iopub.status.idle": "2022-04-21T17:39:28.827144Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.815953Z",
     "shell.execute_reply": "2022-04-21T17:39:28.82582Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h4\"><i>Switching over to the decision tree visualizations</i></div> \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* It's not very clear in the name, but reading the documentation  you see that `num_trees` (which obviously grammatically is plural) is actually the exact tree (singular) you want to see (0-based).  Effectively this is specifying the ordinal number of the target tree.  "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "xgb.to_graphviz(bst, num_trees=1, rankdir='LR') \n",
    "# second tree, left right format... "
   ],
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:28.82887Z",
     "iopub.execute_input": "2022-04-21T17:39:28.829636Z",
     "iopub.status.idle": "2022-04-21T17:39:29.081337Z",
     "shell.execute_reply.started": "2022-04-21T17:39:28.829592Z",
     "shell.execute_reply": "2022-04-21T17:39:29.080374Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xgb.to_graphviz(bst, num_trees=2, rankdir='LR') \n",
    "# third tree "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:29.083182Z",
     "iopub.execute_input": "2022-04-21T17:39:29.083837Z",
     "iopub.status.idle": "2022-04-21T17:39:29.325431Z",
     "shell.execute_reply.started": "2022-04-21T17:39:29.083793Z",
     "shell.execute_reply": "2022-04-21T17:39:29.324474Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xgb.to_graphviz(\\\n",
    "bst, \n",
    "num_trees=299, \n",
    "rankdir='LR') \n",
    "# LAST tree (300-1) i.e. 299"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:29.327319Z",
     "iopub.execute_input": "2022-04-21T17:39:29.327941Z",
     "iopub.status.idle": "2022-04-21T17:39:29.561168Z",
     "shell.execute_reply.started": "2022-04-21T17:39:29.327898Z",
     "shell.execute_reply": "2022-04-21T17:39:29.560135Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We will get fancy and loop thru the first chunks of trees for understanding\n",
    "* An interesting thing and it actually took me a while to figure this out is how many trees it plots in multi-class problems.  I was looking thru the verbose debug output from the training and I noticed when I set number of runs to 400 that I actually was generating 1,600 trees.  Then it hit me that XGBoost is plotting a tree for EACH class per run, and I obviously have four classes.  Which  means if you want to look at a chunk, do it is increments of four :)   "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18,15\n",
    "for i in range(4):\n",
    "    xgb.plot_tree(bst, num_trees=i)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    # fig.set_size_inches(22,22)\n",
    "    \n",
    "# plot tree 0, 1, 2, and 3, which are for the first four class run "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:29.563239Z",
     "iopub.execute_input": "2022-04-21T17:39:29.563783Z",
     "iopub.status.idle": "2022-04-21T17:39:31.962868Z",
     "shell.execute_reply.started": "2022-04-21T17:39:29.563739Z",
     "shell.execute_reply": "2022-04-21T17:39:31.961834Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(4,8):\n",
    "    xgb.plot_tree(bst, num_trees=i)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "# plot tree 4,5,6, and 7 which are the next four class run "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:31.964813Z",
     "iopub.execute_input": "2022-04-21T17:39:31.965082Z",
     "iopub.status.idle": "2022-04-21T17:39:34.850081Z",
     "shell.execute_reply.started": "2022-04-21T17:39:31.965045Z",
     "shell.execute_reply": "2022-04-21T17:39:34.848848Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(8,13):\n",
    "    xgb.plot_tree(bst, num_trees=i)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "# plot tree 8, 9, 10, 11 which are the next four class run "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:34.851853Z",
     "iopub.execute_input": "2022-04-21T17:39:34.852212Z",
     "iopub.status.idle": "2022-04-21T17:39:37.867272Z",
     "shell.execute_reply.started": "2022-04-21T17:39:34.852174Z",
     "shell.execute_reply": "2022-04-21T17:39:37.866196Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Click here for high res decision tree visual output](https://raw.githubusercontent.com/tombresee/XGBOOST/main/ENTER/image_of_tree.svg)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>8. &ensp; We Need To Tune Hyperparameters !</i></div> \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# original cod:\n",
    "\n",
    "# mytrain = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n",
    "# mytest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\n",
    "# mysamplesubmission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "\n",
    "# big_X = mytrain.drop(['id', 'target'], axis=1)\n",
    "# small_y = mytrain.target\n",
    "# le = LabelEncoder() # practice, i could do it manually if i wanted... \n",
    "# small_y = le.fit_transform(small_y)\n",
    "\n",
    "# X_test = mytest.drop(['id'], axis=1) \n",
    "\n",
    "# # X_train, X_val, y_train, y_val = train_test_split(big_X, \n",
    "# # small_y, test_size=0.22, random_state=42)\n",
    "\n",
    "# import sklearn\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb\n",
    "\n",
    "# def objective(trial):\n",
    "    \n",
    "#     X_train, X_val, y_train, y_val = train_test_split(big_X, \n",
    "#                     small_y, test_size=0.2,random_state=42)\n",
    "    \n",
    "#     # old but i think i might use: \n",
    "#     # train_x, valid_x, train_y, valid_y = train_test_split(big_X, \n",
    "#     # small_y, test_size=0.125, random_state=42)\n",
    "#     # dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "#     # dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    \n",
    "#     params = {\"verbosity\":0, \n",
    "#     'tree_method':'gpu_hist', #not exact for now  \n",
    "#     \"objective\": \"multi:softmax\",  # or softprob \n",
    "#     #  'objective': 'multiclass'  \n",
    "#     \"num_class\":4, \n",
    "#     'eval_metric': 'mlogloss',\n",
    "#     # keep ? 'metric': 'multi_logloss', \n",
    "                 \n",
    "#     'booster' : 'gbtree',\n",
    "#     # not this:  \"boosting\": 'gbtree', \n",
    "            \n",
    "#     # \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#     'lambda' : trial.suggest_loguniform('lambda' , 1e-8 , 1.0), \n",
    "             \n",
    "#     \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "             \n",
    "#     # 'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
    "#     # 'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
    "             \n",
    "#     # \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, step=.1),\n",
    "#     # updated:\n",
    "#     'subsample' : 0.8,\n",
    "             \n",
    "#     \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\",0.5,1,0.1),\n",
    "             \n",
    "#     \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20, step=1),\n",
    "             \n",
    "#     \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 50), \n",
    "#     # \"eta\":  trial.suggest_float(\"eta\", 1e-8, 1.0, log=True), \n",
    "#     # 'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n",
    "#     'learning_rate' : trial.suggest_uniform('learning_rate' , 0.005 , 0.3),\n",
    "#     'gamma' : trial.suggest_loguniform('gamma' , 1e-8 , 1.0),           \n",
    "#     \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "#     'n_estimators' : trial.suggest_int('n_estimators' , 100, 2500),\n",
    "#             } \n",
    "      \n",
    "#     earlyStop=100\n",
    "    \n",
    "#     # bst = xgb.train(param, dtrain)\n",
    "#     #     # --- used demo from optuna's github, but wrong...\n",
    "#     #     preds = bst.predict(dvalid)\n",
    "#     #     pred_labels = np.rint(preds)\n",
    "#     #     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "#     #     return accuracy\n",
    "\n",
    "#     bst = xgb.XGBClassifier(**params)\n",
    "    \n",
    "#     bst.fit(X_train, y_train, eval_set = [(X_val,y_val)], \n",
    "#             early_stopping_rounds = 100, verbose = False)\n",
    "    \n",
    "#     y_pred = bst.predict_proba(X_val)\n",
    "        \n",
    "#     return sklearn.metrics.log_loss(y_val, y_pred)\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:37.868897Z",
     "iopub.execute_input": "2022-04-21T17:39:37.869462Z",
     "iopub.status.idle": "2022-04-21T17:39:37.875624Z",
     "shell.execute_reply.started": "2022-04-21T17:39:37.869415Z",
     "shell.execute_reply": "2022-04-21T17:39:37.874829Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mytrain = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n",
    "mytest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\n",
    "mysamplesubmission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "big_X = mytrain.drop(['id', 'target'], axis=1)\n",
    "small_y = mytrain.target\n",
    "le = LabelEncoder() # practice, i could do it manually if i wanted... \n",
    "small_y = le.fit_transform(small_y)\n",
    "X_test = mytest.drop(['id'], axis=1) \n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(big_X, \n",
    "# small_y, test_size=0.22, random_state=42)\n",
    "\n",
    "import sklearn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(big_X, \n",
    "                    small_y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    # old but i think i might use: \n",
    "    # train_x, valid_x, train_y, valid_y = train_test_split(big_X, \n",
    "    # small_y, test_size=0.125, random_state=42)\n",
    "    # dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    # dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    \n",
    "    params = {\"verbosity\":0,\n",
    "              \n",
    "    'tree_method':'gpu_hist', #not exact for now \n",
    "    \n",
    "    # i dont know if i need:\n",
    "    'predictor':'gpu_predictor', \n",
    "              \n",
    "    \"objective\": \"multi:softprob\",  \n",
    "    # at one point i actually used softmax !  \n",
    "              \n",
    "    \"num_class\":4, \n",
    "\n",
    "    'eval_metric': 'mlogloss',\n",
    "    # keep ? 'metric': 'multi_logloss', \n",
    "                 \n",
    "    'booster' : 'gbtree',\n",
    "    # not this: \"boosting\": 'gbtree', \n",
    "    \n",
    "    'random_state':42, \n",
    "              \n",
    "    # \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "    'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-8 , 30), \n",
    "    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "    # 'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
    "    # 'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
    "             \n",
    "    \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, step=.1),\n",
    "    # 'subsample' : 0.8,\n",
    "             \n",
    "    \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\",0.5,1,0.1),\n",
    "    # no: 'colsample_bytree' : trial.suggest_uniform('colsample_bytree',0,1),\n",
    "             \n",
    "    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 25, step=1),\n",
    "             \n",
    "    \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100), \n",
    "              \n",
    "    # \"eta\":  trial.suggest_float(\"eta\", 1e-8, 1.0, log=True), \n",
    "    # 'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n",
    "    'learning_rate' : trial.suggest_uniform('learning_rate' , 0.005 , 0.3),\n",
    "    'gamma' : trial.suggest_loguniform('gamma' , 1e-8 , 1.0),           \n",
    "    \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "    'n_estimators' : trial.suggest_int('n_estimators' , 100, 2500),\n",
    "    'max_leaves': trial.suggest_int('max_leaves', 10, 40)\n",
    "            } \n",
    "      \n",
    "    earlyStop=10\n",
    "    \n",
    "    # bst = xgb.train(param, dtrain)\n",
    "    #     # --- used demo from optuna's github, but wrong...\n",
    "    #     preds = bst.predict(dvalid)\n",
    "    #     pred_labels = np.rint(preds)\n",
    "    #     accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "    #     return accuracy\n",
    "\n",
    "    bst = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    bst.fit(X_train, y_train, eval_set = [(X_val,y_val)], \n",
    "            early_stopping_rounds = 10, verbose = True)\n",
    "    \n",
    "    y_pred = bst.predict_proba(X_val)\n",
    "        \n",
    "    return sklearn.metrics.log_loss(y_val, y_pred)\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:37.876869Z",
     "iopub.execute_input": "2022-04-21T17:39:37.877369Z",
     "iopub.status.idle": "2022-04-21T17:39:38.296294Z",
     "shell.execute_reply.started": "2022-04-21T17:39:37.87733Z",
     "shell.execute_reply": "2022-04-21T17:39:38.295396Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**A Trick:** enter the `objective` parameter in your param dictionary incorrectly, and it will spit out what your options are:  \n",
    "\n",
    "Objective candidates: binary:hinge, multi:softmax, multi:softprob, rank:pairwise, rank:ndcg, rank:map, reg:squarederror, reg:squaredlogerror, reg:logistic, reg:pseudohubererror, binary:logistic, binary:logitraw, reg:linear, count:poisson, survival:cox,     reg:gamma, reg:tweedie\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre style=\"font-family:Consolas; font-size:.7em\">\n",
    "\n",
    "--- MODEL 1 CREATION ---\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100) \n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "\n",
    "--- MODEL 1 - OPTUNA OUTPUT ---\n",
    "\n",
    "Number of finished trials:  100\n",
    "Best trial:\n",
    "  Value: 1.08671334613692\n",
    "  Params: \n",
    "    lambda: 0.6838907631827036\n",
    "    alpha: 0.004407099400692902\n",
    "    colsample_bytree: 0.5\n",
    "    max_depth: 3\n",
    "    min_child_weight: 49\n",
    "    learning_rate: 0.038142105877918005\n",
    "    gamma: 0.0021897234762554415\n",
    "    grow_policy: depthwise\n",
    "    n_estimators: 1377\n",
    "\n",
    "\n",
    "--- MODEL 2 - OPTUNA OUTPUT ---\n",
    "\n",
    "Number of finished trials:  200\n",
    "Best trial:\n",
    "  Value: 1.0855198438808322\n",
    "  Params: \n",
    "    reg_lambda: 0.001161451712738042\n",
    "    reg_alpha: 8.35639898103767e-06\n",
    "    subsample: 0.7\n",
    "    colsample_bytree: 0.5\n",
    "    max_depth: 11\n",
    "    min_child_weight: 87\n",
    "    learning_rate: 0.0632151193138034\n",
    "    gamma: 0.0025864112778741883\n",
    "    grow_policy: lossguide\n",
    "    n_estimators: 1409\n",
    "    max_leaves: 12\n",
    "    \n",
    "</pre>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# COMMENTING \n",
    "# OUT \n",
    "# SO \n",
    "# I \n",
    "# CAN \n",
    "# SUBMIT \n",
    "# WITH \n",
    "# FINAL \n",
    "# CALCULATED, \n",
    "# OPTIMIZED \n",
    "# VALUES...\n",
    "# --- \n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=300) \n",
    "\n",
    "# print(\"Number of finished trials: \", len(study.trials))\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: {}\".format(trial.value))\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:38.297548Z",
     "iopub.execute_input": "2022-04-21T17:39:38.297888Z",
     "iopub.status.idle": "2022-04-21T17:39:38.302558Z",
     "shell.execute_reply.started": "2022-04-21T17:39:38.297851Z",
     "shell.execute_reply": "2022-04-21T17:39:38.301459Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"h3\"><i>9. &ensp; Score Tracker</i></div> \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Pushing in my tuned hyperparameters (that were calculated above) in the new model below:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# basically redoing things to be safe:\n",
    "# original run:\n",
    "\n",
    "# train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n",
    "# test = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\n",
    "# sample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "# le = LabelEncoder() # practice, i could do it manually if i wanted... \n",
    "# train.target = le.fit_transform(train.target)\n",
    "# features = train.columns[1:51] # only fifty \n",
    "# X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=[\"id\",\"target\"]), train.target, stratify=train.target)\n",
    "# # y_train = y_train.map({'Class_1':0, \n",
    "# #                        'Class_2':1, \n",
    "# #                        'Class_3':2, \n",
    "# #                        'Class_4':3})\n",
    "\n",
    "# X_test = test.drop(['id'], axis=1) \n",
    "    \n",
    "# bst_2 = xgb.XGBClassifier(seed=42, \n",
    "# base_score=0.5,\n",
    "# booster='gbtree',\n",
    "# reg_lambda = 0.6838907631827036, \n",
    "# reg_alpha = 0.004407099400692902, \n",
    "# colsample_bytree = 0.5, \n",
    "# max_depth = 3, \n",
    "# min_child_weight = 49, \n",
    "# learning_rate = 0.038142105877918005, \n",
    "# gamma = 0.0021897234762554415, \n",
    "# grow_policy = \"depthwise\", \n",
    "# n_estimators = 1377, \n",
    "# num_class=4,                \n",
    "# subsample=0.8,\n",
    "# importance_type='gain', \n",
    "# interaction_constraints='', \n",
    "# monotone_constraints='()',\n",
    "# num_parallel_tree=1,\n",
    "# verbosity=0, \n",
    "# scale_pos_weight=None,\n",
    "# tree_method = 'gpu_hist',  \n",
    "# objective = \"multi:softmax\")\n",
    "# # eval_metric = 'mlogloss'\n",
    "# # should be down below, not here\n",
    "                                           \n",
    "# bst_2.fit(\\\n",
    "# X_train, \n",
    "# y_train,\n",
    "# verbose=False,\n",
    "# early_stopping_rounds=30,\n",
    "# eval_metric=['mlogloss'],\n",
    "# eval_set=[(X_train, y_train),(X_val, y_val)])"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:38.30404Z",
     "iopub.execute_input": "2022-04-21T17:39:38.304397Z",
     "iopub.status.idle": "2022-04-21T17:39:38.31275Z",
     "shell.execute_reply.started": "2022-04-21T17:39:38.304362Z",
     "shell.execute_reply": "2022-04-21T17:39:38.311826Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# history = study.trials_dataframe()\n",
    "# history.sort_values(by=\"value\", ascending=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:38.314133Z",
     "iopub.execute_input": "2022-04-21T17:39:38.314781Z",
     "iopub.status.idle": "2022-04-21T17:39:38.32306Z",
     "shell.execute_reply.started": "2022-04-21T17:39:38.314743Z",
     "shell.execute_reply": "2022-04-21T17:39:38.322291Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n",
    "test = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\n",
    "sample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "le = LabelEncoder() # practice, i could do it manually if i wanted... \n",
    "train.target = le.fit_transform(train.target)\n",
    "features = train.columns[1:51] # only fifty \n",
    "X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=[\"id\",\"target\"]), train.target, stratify=train.target)\n",
    "# y_train = y_train.map({'Class_1':0, \n",
    "#                        'Class_2':1, \n",
    "#                        'Class_3':2, \n",
    "#                        'Class_4':3})\n",
    "\n",
    "X_test = test.drop(['id'], axis=1) \n",
    "\n",
    "bst_2 = xgb.XGBClassifier(seed=42, \n",
    "base_score=0.5,\n",
    "booster='gbtree',\n",
    "reg_lambda = 10,  \n",
    "reg_alpha =  0.000403384473190648,  \n",
    "max_depth = 4, \n",
    "min_child_weight = 58, \n",
    "learning_rate = 0.09, \n",
    "gamma = 0.28, \n",
    "# grow_policy = \"lossguide\", \n",
    "# not sure i agree with optuna here... \n",
    "n_estimators = 1000,  \n",
    "num_class=4,                \n",
    "subsample=0.87,\n",
    "# NOT 0.5 ! \n",
    "colsample_bytree=0.5, \n",
    "importance_type='gain', \n",
    "interaction_constraints='', \n",
    "monotone_constraints='()',\n",
    "num_parallel_tree=1,\n",
    "use_label_encoder=False,\n",
    "verbosity=0, \n",
    "scale_pos_weight=None,\n",
    "tree_method = 'gpu_hist',  \n",
    "objective = \"multi:softprob\")\n",
    "                                           \n",
    "bst_2.fit(\\\n",
    "X_train, \n",
    "y_train,\n",
    "verbose=False,\n",
    "early_stopping_rounds=45,\n",
    "# early_stopping_rounds=42,\n",
    "eval_metric=['mlogloss'],\n",
    "eval_set=[(X_train, y_train),(X_val, y_val)])"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:38.324838Z",
     "iopub.execute_input": "2022-04-21T17:39:38.32514Z",
     "iopub.status.idle": "2022-04-21T17:39:41.335164Z",
     "shell.execute_reply.started": "2022-04-21T17:39:38.325108Z",
     "shell.execute_reply": "2022-04-21T17:39:41.334046Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre style=\"font-family:Consolas; font-size:.7em\">\n",
    "\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.5,\n",
    "              gamma=0.0025864112778741883, gpu_id=0, grow_policy='lossguide',\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.0632151193138034, max_delta_step=0, max_depth=11,\n",
    "              max_leaves=12, min_child_weight=87, missing=nan,\n",
    "              monotone_constraints='()', n_estimators=1409, n_jobs=2,\n",
    "              num_class=4, num_parallel_tree=1, objective='multi:softprob',\n",
    "              random_state=42, reg_alpha=8.35639898103767e-06,\n",
    "              reg_lambda=0.001161451712738042, scale_pos_weight=None, seed=42,\n",
    "              subsample=0.7, tree_method='gpu_hist', validate_parameters=1, ...)\n",
    "              \n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.28, gpu_id=0,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.09, max_delta_step=0, max_depth=4,\n",
    "              min_child_weight=58, missing=nan, monotone_constraints='()',\n",
    "              n_estimators=1000, n_jobs=2, num_class=4, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=42,\n",
    "              reg_alpha=0.000403384473190648, reg_lambda=10,\n",
    "              scale_pos_weight=None, seed=42, subsample=0.87,\n",
    "              tree_method='gpu_hist', use_label_encoder=False,\n",
    "              validate_parameters=1, verbosity=0)\n",
    "</pre>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preds_sub = bst_2.predict_proba(X_test)\n",
    "classes = [\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\"]\n",
    "sample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') \n",
    "sample_submission.drop(columns=classes,inplace=True)\n",
    "submission = (sample_submission.join(pd.DataFrame(data=preds_sub, \n",
    "                                    columns=classes)))\n",
    "submission.to_csv(\"my_submission.csv\", index=False)"
   ],
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:41.336634Z",
     "iopub.execute_input": "2022-04-21T17:39:41.337Z",
     "iopub.status.idle": "2022-04-21T17:39:42.481451Z",
     "shell.execute_reply.started": "2022-04-21T17:39:41.336959Z",
     "shell.execute_reply": "2022-04-21T17:39:42.480595Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "display(submission.head(10))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:39:42.482774Z",
     "iopub.execute_input": "2022-04-21T17:39:42.483126Z",
     "iopub.status.idle": "2022-04-21T17:39:42.498197Z",
     "shell.execute_reply.started": "2022-04-21T17:39:42.483076Z",
     "shell.execute_reply": "2022-04-21T17:39:42.497346Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}