{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Hey Dear Kagglers,\n##### Wanted to put my 2 cents in;\n##### While I see most of the public kernels on this competition focusing on parameter tunning/automl/cutting edge ML - I decided to show a slightly different approach -  and also a gentle reminder that the above is NOT a substitution for the good old Feature Engineering in any way.\n##### So - what's happening here:\n* Some FE:\n     - One Hot Encoding + subsequent feature selection (see below)\n     - Target Encoding (generally very useful in case the test set is similar enough to your train (confirmed it is/adversarial validation))\n* SUPER SIMPLE Generalized Linear Model (Multinomial)  \n\n##### No fancy stuff here ... still performing better than most of the fancy stuff out there...\n##### Stay tuned - more is to come (fancy stuff as well) ...\n##### ... and Happy Kaggling :)","metadata":{}},{"cell_type":"markdown","source":"#### Imports ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedKFold\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Functions","metadata":{}},{"cell_type":"code","source":"def target_encoder(class_='', smoothing=0.2, X_train=None, X_test=None):\n    # Inspired by this great kernel - please upvote: https://www.kaggle.com/caesarlupum/2020-20-lines-target-encoding\n    train = X_train.copy()\n    train['target'] = np.where(tr['target']==class_, 1, 0)\n    test = X_test.copy()\n    train_y = train['target']\n    train_id = train['id']\n    test_id = test['id']\n    train.drop(['target', 'id'], axis=1, inplace=True)\n    test.drop('id', axis=1, inplace=True)\n    \n    cat_feat_to_encode = train.columns.tolist()\n    \n    oof = pd.DataFrame([])\n    \n    for tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(train, train_y):\n        ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n        ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n        oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n    \n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train, train_y)\n    train = oof.reindex(train.index) #.sort_index()\n    test = ce_target_encoder.transform(test)\n    train.columns = [class_ + '_' + str(col) for col in train.columns]\n    test.columns = [class_ + '_' + str(col) for col in test.columns]\n    \n    return train, test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Read data","metadata":{}},{"cell_type":"code","source":"tr = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/train.csv\")\nte = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/test.csv\")\ntr_te = pd.concat([tr, te], axis=0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### One Hot Encoding","metadata":{}},{"cell_type":"code","source":"enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\ntr_te_ohe = pd.DataFrame(enc.fit_transform(tr_te.drop(['id','target'], axis=1).copy())).astype(int)\nsel = VarianceThreshold(threshold=0.01) # remove sparse features\ntr_te_ohe = pd.DataFrame(sel.fit_transform(tr_te_ohe))\ntr_te_ohe.columns = ['ohe_' + str(col) for col in tr_te_ohe.columns]\ntr_te_ohe = pd.concat([tr_te[['id','target']],tr_te_ohe], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Target Encoding","metadata":{}},{"cell_type":"code","source":"tr_class_1, te_class_1 = target_encoder(class_='Class_1', X_train=tr, X_test=te)\ntr_class_2, te_class_2 = target_encoder(class_='Class_2', X_train=tr, X_test=te)\ntr_class_3, te_class_3 = target_encoder(class_='Class_3', X_train=tr, X_test=te)\ntr_class_4, te_class_4 = target_encoder(class_='Class_4', X_train=tr, X_test=te)\ntr_tgt = pd.concat([tr_class_1,tr_class_2,tr_class_3,tr_class_4], axis=1)\nte_tgt = pd.concat([te_class_1,te_class_2,te_class_3,te_class_4], axis=1)\ntr_te_tgt = pd.concat([tr_tgt, te_tgt], axis=0).reset_index(drop=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Format out and cleanup","metadata":{}},{"cell_type":"code","source":"tr_te_fin = pd.concat([tr_te_ohe, tr_te_tgt], axis=1)\ntr_fin = tr_te_fin[tr_te_fin['target'].notnull()].drop(['id'], axis=1).copy()\nte_fin = tr_te_fin[tr_te_fin['target'].isnull()].drop(['id','target'], axis=1).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tr_class_1, te_class_1, tr_class_2, te_class_2, tr_class_3, te_class_3, tr_class_4, te_class_4, tr_te_ohe, tr_tgt, te_tgt, tr_te_tgt, tr_te_fin\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model","metadata":{}},{"cell_type":"code","source":"# H2O ML MODEL ======================================================================================================================\n# preproc ===========================\nimport h2o\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n# init ==============================\nh2o.init(max_mem_size='8G')\n\n# import data =======================\ntrain = h2o.H2OFrame(tr_fin)\ntrain[\"target\"] = train[\"target\"].asfactor()\ntest = h2o.H2OFrame(te_fin)\n\ny = \"target\"\nx = test.columns\n\n# fit model =========================\nglm_model = H2OGeneralizedLinearEstimator(\n    family=\"multinomial\", \n    solver='AUTO', \n    alpha=0.5,\n    #lambda_=0.6,\n    link='Family_Default',\n    intercept=True,\n    lambda_search=True, \n    nlambdas=100,\n    max_iterations = 1000,\n    #missing_values_handling='MeanImputation',\n    standardize=True,\n    nfolds = 5, \n    seed = 1333\n)\nglm_model.train(x=x, y=y, training_frame=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eval mod ==========================\nglm_model.model_performance(xval=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model pred ========================\npreds = glm_model.predict(test).as_data_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h2o.cluster().shutdown()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Submission","metadata":{}},{"cell_type":"code","source":"subm = pd.read_csv(\"/kaggle/input/tabular-playground-series-may-2021/sample_submission.csv\")\nsubm = pd.concat([subm['id'], preds[['Class_1','Class_2','Class_3','Class_4']]],axis=1)\nsubm.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hopefully you liked it (please upvote) - and saw something new today. Let me know if you have any questions in the comments :)","metadata":{}}]}