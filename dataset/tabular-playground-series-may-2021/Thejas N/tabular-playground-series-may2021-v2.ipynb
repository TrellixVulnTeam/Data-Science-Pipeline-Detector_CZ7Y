{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Competition Overview:** [Tabular_Playground_Series](https://www.kaggle.com/c/tabular-playground-series-may-2021/overview)\n\n## **Objective:** To predict the probability, the id of test dataset belongs to each class\n\n## **Dataset:** [Synthetic Dataset](https://www.kaggle.com/c/tabular-playground-series-may-2021/data)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\n\n# import the library to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the train dataset\ntrain_original = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the train dataset\ntrain_original.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify the shape of the train dataset\ntrain_original.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> There are 100000 rows and 52 columns in train dataset including 'target' variable. ","metadata":{}},{"cell_type":"code","source":"# Check if there are any missing values\ntrain_original.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> There are no null values and all the independent features are of integer datatype and 'target' is of object datatype","metadata":{}},{"cell_type":"code","source":"# Describe the dataset\ntrain_original.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import the test dataset\ntest_original = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the test dataset\ntest_original.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the null values\ntest_original.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> 1. There are 50000 rows and 51 columns in test dataset which excludes 'target' variable.\n> 2. There are no null values in the test dataset and all the independent features are of integer datatype","metadata":{}},{"cell_type":"code","source":"# Check the distribution of the features in the train dataset using histogram\n# Since there are more features, randomly chose the features out from 50 features \n\n# Histogram for 'feature_0'\nplt.figure(figsize=(20, 10))\nplt.subplot(2,5,1)\nplt.title('Histogram for feature_0')\nplt.xlabel('feature_0')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_0, kde=False, color=['red'])\n\n# Histogram for 'feature_10'\nplt.subplot(2,5,2)\nplt.title('Histogram for feature_10')\nplt.xlabel('feature_10')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_10, kde=False, color=['green'])\n\n# Histogram for 'feature_20'\nplt.subplot(2,5,3)\nplt.title('Histogram for feature_20')\nplt.xlabel('feature_20')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_20, kde=False, color=['blue'])\n\n# Histogram for 'feature_35'\nplt.subplot(2,5,4)\nplt.title('Histogram for feature_35')\nplt.xlabel('feature_35')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_35, kde=False, color=['black'])\n\n\n# Histogram for 'feature_49'\nplt.subplot(2,5,5)\nplt.title('Histogram for feature_49')\nplt.xlabel('feature_49')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_49, kde=False, color=['magenta'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> It is evident that most of the features distribution are not normal (Right skewed). So, normalization of the data is required before training the model. ","metadata":{}},{"cell_type":"code","source":"# Check the balancing of the classes in the 'target'variable\ntrain_original['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the balance of the data by plotting count of the target by their values\nplt.figure(figsize=(5,2))\nplt.title('Count plot for target')\nsns.countplot(train_original['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> 1. Count of Class2 category is more compared to other class categories. Hence, the problem is imbalanced muticlass classification problem.\n\n> 2. Resampling technique should be used to deal with imbalanced multiclass classification problem. It consists of removing samples from the majority class 'Class2' (under-sampling) and / or adding more samples from the minority classes 'Class_1,Class_4,Class_3' (over-sampling).\n\n> 3. Removing samples from the majority class will lead to data loss and adding more samples from the minority class will lead to overfitting.\n\n> 4. Hence, we will use SMOTE (Synthetic Minority Oversampling Technique). Its an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem.","metadata":{}},{"cell_type":"code","source":"# store the 'ID' values into the new object train_ID and test_ID and verify the shape\n\n# Train dataset\ntrain_ID = train_original.id\nprint('The shape of the ID feature in train dataset is:',train_ID.shape)\n\n# Test dataset\ntest_ID = test_original.id\nprint('\\nThe shape of the ID feature in test dataset is:',test_ID.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the remaining values into the new object train_X and test_X and verify the shape\n\n# Train dataset\ntrain_X = train_original.drop(columns = ['id','target'])\nprint('The shape of the final train dataset is:',train_X.shape)\n\n# Test dataset\ntest_X = test_original.drop(columns = ['id'])\nprint('\\nThe shape of the final test dataset is:',test_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the 'target' values into the new object train_y and verify the shape\ntrain_y=train_original.target\nprint('The shape of the target feature of train dataset is:',train_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save these datasets to csv for future use if required\ntrain_ID.to_csv('train_ID.csv',index=False)\ntest_ID.to_csv('test_ID.csv',index=False)\ntrain_X.to_csv('train_X.csv',index=False)\ntest_X.to_csv('test_X.csv',index=False)\ntrain_y.to_csv('train_y.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform correlation analysis between the features of train dataset and visualize using a heat map.\nplt.figure(figsize=(20,10))\nsns.heatmap(train_X.corr(),annot=True,cmap='RdYlGn');\n##\nplt.xticks(rotation=90, color='indigo', size=10)\nplt.yticks(rotation=0, color='indigo', size=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments** \n> It is observed that correlation between the features are not crossing 0.25. So, it is likely the features are independent of each other","metadata":{}},{"cell_type":"code","source":"# import the required library\nfrom sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a PCA object (instantiate)\npca=PCA()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X = pca.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X = pca_fit_train_X.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X = pca.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of variation of each principal components\n# Assuming we have to principal components PC1 and PC2, then  \n# explained_variance_ratio for PC1 = (Variation for PC1/ (Total variation ie (PC1+PC2)))*100\n##\npca_train_X_variation = np.round(pca_fit_train_X.explained_variance_ratio_.cumsum()*100,decimals=1)\n##\n# .cumsum() is used to display PC's variation with respect to cumulative percentage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the cumulative percentage of expalined variance\npca_train_X_variation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assign labels for each PC's as PC1,2,etc., for visulaization in scree plot \n##\nlabels = ['PC' + str(x) for x in range(1, len(pca_train_X_variation)+1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the scree plot\n## \nplt.figure(figsize=(25,5))\n##\nplt.bar(x=range(1, len(pca_train_X_variation)+1), height=pca_train_X_variation,tick_label=labels)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\n##################\nplt.title('Scree Plot',color='tab:orange', fontsize=25)\n###################\n##\nplt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\nplt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n##","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> By looking into the array of elements and scree plot, It is observed that among 50 Principal components, ~90% of variation of the data in train_X dataset is explained by only first 28 principal components.","metadata":{}},{"cell_type":"code","source":"# create a PCA object again (instantiate) by considering first 28 principal components\npca_28=PCA(n_components=28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X_28 = pca_28.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X_28 = pca_fit_train_X_28.transform(train_X)\n\n#transform the test dataset\npca_transform_test_X_28 = pca_28.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From PCA, the final train and test datasets are as follows\n\n#train data\npca_fit_transform_train_X_28.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test data\npca_transform_test_X_28.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train label\ntrain_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the library SMOTE to deal with Imbalanced Multiclass Classification\nfrom imblearn.over_sampling import SMOTE\n\n#instantiate SMOTE\nsmote = SMOTE(random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample and fit the dataset to balance the classes,thereby increasing the observations\nX_smote, y_smote = smote.fit_resample(pca_fit_transform_train_X_28,train_y)\n\nprint('Shape of independent features(X) before SMOTE :', pca_fit_transform_train_X_28.shape)\nprint('Shape of independent features(X) after SMOTE :' , X_smote.shape)\n\nprint('Shape of dependent feature(y) before SMOTE :', train_y.shape)\nprint('Shape of dependent feature(y) after SMOTE :' , y_smote.shape)\n\nprint('Count of the classes before SMOTE:\\n',train_y.value_counts())\nprint('Count of the classes before SMOTE:\\n',y_smote.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> 1. After SMOTE, the number of observation got increased to 229988 rows from 100000 rows.\n> 2. After SMOTE, the number of majority class_2 remains unchanged (57497), whereas number of minority classes 1,3 and 4 are increased to 57497. This ensures, balancing of the classes is done perfectly.","metadata":{}},{"cell_type":"code","source":"# I will use only XGBoost Classifier for the second submission. \n# Later, will check how i can improve the score by trying with different algorithms\n\n# import the required libraries\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters\nparams={ 'learning_rate' : [0.01, 0.05, 0.1] ,\n'max_depth' : [3,4,5],\n'min_child_weight': [ 0, 1, 3],\n'gamma' : [ 0, 0.25,1],\n'colsample_bytree': [ 0.2, 0.4, 0.6],\n'reg_lambda' : [0,1,10]\n}\n## Explainations of the parameters\n# 'max_depth' - Maximum depth of trees (default = 6, range: [0,∞])\n# 'Learning rate'(eta) - scaling the tree by learning rate predicts the output in smaller steps close to the\n# 'reg_lambda' - L2 regularization parameter on weights which estimate the mean of the data to avoid overfit\n# 'gamma' - Minimum loss reduction required to make a further partition on a leaf node of the tree (pruning)\n# 'min_child_weight' - default =1. If the weights of each leaf is less than the min_child weight, then the t\n# So weights of the each leaf is > min_child_weight\n# 'colsample_bytree': It is the subsample ratio of columns when constructing each tree.(default=1, range 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimize the Hyperparameter using RandomizedSearchCV\n\n#import the required libraries\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instantiate the classifier\nXGB=xgb.XGBClassifier(missing=1)\n# Objective will be automatically set to 'multi:softprob' # Alternate is 'multi:softmax'\n# 'multi:softmax' returns predicted class\n# 'multi:softprob' returns predicted probabilities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Random search of parameters with 5 fold cross validation\n# Improve the predictions using cross validation to optimize the parameters\nRandom_Search=RandomizedSearchCV (XGB,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=-1)\n# cv=5 - Number of folds in a `(Stratified)KFold`","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the train dataset to the Random_Search to obtain the best estimators and parameters.\nRandom_Search.fit(X_smote,y_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the best estimator\nRandom_Search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the best parameters\nRandom_Search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the XGBoost classifier with the best estimators and parameters\nXGB=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=4,\n              min_child_weight=1, missing=1, monotone_constraints='()',\n              n_estimators=100, n_jobs=2, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n# leave the other parameters to default values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the accuracy of the model using Number of folds in a `(Stratified)KFold` cv=5\n# import the library\nfrom sklearn.model_selection import cross_val_score\n\nAccuracy=cross_val_score(XGB,X_smote,y_smote,cv=5)\nAccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the mean accuracy of each k-fold\nprint(\"Accuracy of XGBoost Model with Cross Validation is:\",Accuracy.mean() * 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the training data\nXGB.fit(X_smote,y_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict the test dataset\ny_predict_xgb = XGB.predict(pca_transform_test_X_28)\ny_predict_xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the probabality,the id of test dataset belongs to each class \nprobability_xgb=XGB.predict_proba(pca_transform_test_X_28)\nprobability_xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the sample submission file\nsample_submit=pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dataframe similar to sample submission file\npred_prob_xgb = pd.DataFrame(probability_xgb, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\npred_prob_xgb['id'] = sample_submit['id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set the index to 'id'\npred_prob_xgb=pred_prob_xgb.set_index('id')\npred_prob_xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export the predicted probability, the id of test dataset belongs to each class \npred_prob_xgb.to_csv('submission_20210510_V2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}