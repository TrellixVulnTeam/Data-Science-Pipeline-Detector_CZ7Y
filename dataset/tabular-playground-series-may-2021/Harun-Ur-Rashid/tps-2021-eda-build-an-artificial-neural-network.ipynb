{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://imgur.com/asJQHOg.png\">","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Table of Contents:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Import libraries](#Libraries)** <br>\n**3. [Knowning the data](#Data)** <br>\n**4. [EDA](#Explorations)** <br>\n**5. [Preprocessing](#Prepocess)** <br>\n**6. [Model Training](#Model)** <br>\n**7. [Validation](#Validation)** <br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Introduction\"></a> <br> \n# **1. Introduction:** \nAlthough the data used for this competition is synthetic, it is based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. \n\n> üéØ Goal: To predict the probability the id belongs to each class\n\n> üìñ Data:\n> - ```train.csv``` - *training data*, one product (id) per row, with the associated features (feature_*) and class label (target)\n> - ```test.csv``` - *test data*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Libraries\"></a> <br> \n# **2. Import libraries üìö** ","metadata":{}},{"cell_type":"code","source":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n#This library use for building ANN model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input\n#This library use for data preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n# This function makes the plot directly on browser\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Data\"></a> <br> \n# **3. First look at the data:** ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking data format and types\nprint(train.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing test info()\nprint(test.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ``id`` value is meaningless, so I will leave it out in advance.","metadata":{}},{"cell_type":"code","source":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Explorations\"></a> <br> \n# **4. EDA üìä** ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(x='target', data=train, order=sorted(train['target'].unique()), ax=ax)\nax.set_ylim(0, 63000)\nax.set_title('Target Distribution', weight='bold')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, let's look at the statistical values for each feature.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary**\n* No missing data\n\n* The representative statistics of train and test are almost similar.\n\n* There is no significant difference between the mean and the deviation.\n\n* There are cases where the minimum value and the maximum value are different, which means that the feature range of the test may be different in the train.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Preprocess\"></a> <br> \n# **5. Preprocessing :** ","metadata":{}},{"cell_type":"markdown","source":"**Train-Test Split & Normalization üè∑Ô∏è**\n\nThe data needs to be normalized and split train-test to fit into the ANN","metadata":{}},{"cell_type":"code","source":"for i in range(50):\n    mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n    train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)/std)\n    test[f'feature_{i}'] = test[f'feature_{i}'].apply(lambda x : (x-mean)/std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = {var:index for index, var in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label)\n\ntarget = train['target']\ntrain.drop(['target'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.values\ntarget = target.values\ntarget =  to_categorical(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.1, random_state = 2, stratify=target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Model\"></a> <br> \n# **6. Model Build and Training ‚öôÔ∏è** ","metadata":{}},{"cell_type":"markdown","source":"**Theory of ANN**\n\nAn artificial neural network is a supervised learning algorithm which means that we provide it the input data containing the independent variables and the output data that contains the dependent variable. For instance, in our example our independent variables are X1, X2 and X3. The dependent variable is Y.\n\nIn the beginning, the ANN makes some random predictions, these predictions are compared with the correct output and the error(the difference between the predicted values and the actual values) is calculated. The function that finds the difference between the actual value and the propagated values is called the cost function. The cost here refers to the error. Our objective is to minimize the cost function. Training a neural network basically refers to minimizing the cost function. We will see how we can perform this task.\n\nA neural network executes in two phases: **Feed Forward phase** and **Back Propagation phase**. Let us discuss both these steps in detail.","metadata":{}},{"cell_type":"markdown","source":"**Feed Forward**\n\nIn the feed-forward phase of ANN, predictions are made based on the values in the input nodes and the weights. If you look at the neural network in the above figure, you will see that we have three features in the dataset: X1, X2, and X3, therefore we have three nodes in the first layer, also known as the input layer.\n\nThe weights of a neural network are basically the strings that we have to adjust in order to be able to correctly predict our output. For now, just remember that for each input feature, we have one weight.\n\n<img src =\"https://imgur.com/mninr5z.png\">","metadata":{}},{"cell_type":"markdown","source":"**Back Propagation**\n\nIn the beginning, before you do any training, the neural network makes random predictions which are of course incorrect.\nWe start by letting the network make random output predictions. We then compare the predicted output of the neural network with the actual output. Next, we update the weights and the bias in such a manner that our predicted output comes closer to the actual output. In this phase, we train our algorithm.\n\n<img src = \"https://imgur.com/FI0R6lf.png\" width=\"1080\" height=\"1080\">","metadata":{}},{"cell_type":"markdown","source":"**How do Neural networks learn?**\n\nLooking at an analogy may be useful in understanding the mechanisms of a neural network. Learning in a neural network is closely related to how we learn in our regular lives and activities ‚Äî we perform an action and are either accepted or corrected by a trainer or coach to understand how to get better at a certain task. Similarly, neural networks require a trainer in order to describe what should have been produced as a response to the input. Based on the difference between the actual value and the predicted value, an error value also called **Cost Function** is computed and sent back through the system.\n\n<img src = \"https://imgur.com/FQ2Tabx.gif\" width=\"1080\" height=\"1080\">","metadata":{}},{"cell_type":"code","source":"# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(32,activation='relu',input_dim=50,kernel_initializer='uniform'))\n#The argument being passed to each Dense layer (32) is the number of hidden units of the layer. \n# A hidden unit is a dimension in the representation space of the layer.\n\n\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.3))\n\n#adding second hidden layer \nmodel.add(Dense(64,kernel_initializer='uniform',activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.2))\n\n#adding third hidden layer \nmodel.add(Dense(128,kernel_initializer='uniform',activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.2))\n\n\n# adding the output layer that is categorical\nmodel.add(Dense(4,kernel_initializer='uniform',activation='softmax'))\n#With such a scalar sigmoid output on a categorical classification problem, the loss\n#function you should use is categorical_crossentropy\n\n#Visualizing the model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we need to choose a loss function and an optimizer.","metadata":{}},{"cell_type":"code","source":"# Compiling our model\nmodel.compile(optimizer = 'sgd', \n                   loss = 'categorical_crossentropy', \n                   metrics = ['accuracy'])\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, epochs = 30, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Validation\"></a> <br> \n# **7. Validation** ","metadata":{}},{"cell_type":"code","source":"# Fit the model\nhistory = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n                    epochs=100, verbose=0)\n\n# list all data in history\nprint(history.history.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluating the model**","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarizing historical accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Submission\"></a> <br> \n# **8. Make Submission file üìù** ","metadata":{}},{"cell_type":"code","source":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model.predict(test)\nsample_submission.to_csv(f'tps_ann.csv',index=False)\nsample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the content is helpful, please upvote. üôÇ Work in progress üöß","metadata":{}},{"cell_type":"markdown","source":"## References:\n1. [TPS May: RAPIDS üèÉ‚Äç](https://www.kaggle.com/ruchi798/tps-may-rapids)\n2. [Build an Artificial Neural Network From Scratch: Part 1](https://www.kdnuggets.com/2019/11/build-artificial-neural-network-scratch-part-1.html)\n3. [Titanic [EDA] + Model Pipeline + Keras NN](https://www.kaggle.com/kabure/titanic-eda-model-pipeline-keras-nn)\n4. [[TPS-May] Categorical EDA](https://www.kaggle.com/subinium/tps-may-categorical-eda)","metadata":{}}]}