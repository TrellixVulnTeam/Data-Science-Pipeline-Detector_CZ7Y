{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello.\n\nI am a data science professional, and spend my weekends on Kaggle.\n\nSharing an amazing technique I use often in business scenario, to validate if the train and test sets are similar or not. Since most beginners come here with the intent of landing a job, hope this helps you.\n\n**This is an artificially generated data, so the check is not needed, but tis always good to check before you begin modelling**\n\n** Please upvote this Kernel if you find it useful **","metadata":{}},{"cell_type":"markdown","source":"<h1> Adversarial Validation </h1>\nDrawing a Parallel from Adversarial networks, We make parts of dataset compete against each other to analyse for differences and similarities.\nSimply put, these are the steps for Adversarial Validation:\n\n\n1. Combine the training and testing dataset.Create a flag that indicates the source dataset.\n2. Train a base classifier on the created flag\n3. Analyze AUC, accuracy etc to see how similar training and testing datasets are. \n\n**Generally, The closer AUC is to .5, the better it is.**\n\n<h1> Implications </h1>","metadata":{}},{"cell_type":"markdown","source":"Adversarial Validation can help us analyze whether a data science problem can be formulated or not. It also checks for data stability.\nIn business context, AV is a great analysis to apply at the EDA phase. Any situation where the characteristics of Train and Test data itself are very different, ML might not be the best choice to solve a problem.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading Data\ntrain_data = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\n\n\ntest_data = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Creating Flags </h1>","metadata":{}},{"cell_type":"code","source":"#Creating flags\ntrain_data['istrain'] = 1\n\ntest_data['istrain'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Combining Data </h1>","metadata":{}},{"cell_type":"code","source":"#Combining Data\ncombined_data = pd.concat([train_data, test_data], axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data['istrain'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numeric = combined_data.select_dtypes(exclude=['object'])\n\ndf_obj = combined_data.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \ncombined_data = pd.concat([df_numeric, df_obj], axis=1)\n\ny = combined_data['istrain']\n\n#dropping identifiers and target. \n\ncombined_data.drop(['id','istrain','target'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"combined_data.head()","metadata":{}},{"cell_type":"markdown","source":"<h1> Model training </h1>","metadata":{"trusted":true}},{"cell_type":"code","source":"#I'll use an XGB classifier here with default parameters, you may use anything\n\nskf = StratifiedShuffleSplit()\nxgb_params = {'objective': 'binary:logistic','use_label_encoder':False,'n_estimators':100}   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\n\navg_loss = []\n\nfor train_index, test_index in skf.split(combined_data, y):\n       \n        x0, x1 = combined_data.iloc[train_index], combined_data.iloc[test_index]\n        \n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        \n        print(x0.shape)\n        \n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric=['logloss','auc'], verbose=True,early_stopping_rounds=50)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        \n        roc = roc_auc_score(y1,prval)\n\n        scores.append(roc)\n        \n        avg_loss.append(clf.best_score)\n\n        print ('XGB Val OOF AUC=',roc)\n    \nprint(\"Log Loss Stats {0:.8f},{1:.8f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 Moment of truth :AUC </h1","metadata":{}},{"cell_type":"code","source":"print('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Interpretation </h1>","metadata":{}},{"cell_type":"markdown","source":"AUC score is close to .5 here, implying the train and test datasets are not distinguishable.\n\nIn case of a high AUC, This means normal validation techniques might not work well here.\n\nIt can also indicate the case of a target leakage for train dataset.\n\nIt can be used to identify anomalies and refine training set if needed.","metadata":{}},{"cell_type":"markdown","source":"<h1> Feature importance </h1>","metadata":{}},{"cell_type":"markdown","source":"Looking at what features are helping the model distinguish the best,so we may drop it.","metadata":{}},{"cell_type":"code","source":"# print(clf.feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,10\nplot_importance(clf,max_num_features=20) #top 20 features only\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> References </h1>\n\n1. https://www.kaggle.com/gcspkmdr/tabular-playground-mar-adversarial-validation\n\n2. https://www.kaggle.com/c/widsdatathon2021/discussion/222991\n\n3. https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n\n4. https://www.kaggle.com/kritidoneria/wids-2021-adversarial-validation-on-train-test\n","metadata":{}}]}