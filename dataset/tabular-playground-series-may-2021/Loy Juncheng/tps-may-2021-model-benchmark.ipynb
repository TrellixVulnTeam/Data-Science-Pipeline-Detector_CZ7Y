{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# welcome to model benchmarking\n#### this notebooks focus is on benchmarking baselines for every single model possible for tps may 2021,if you have any models to suggest comment below and i can add it into this notebook for everyone to see üòÅ\n![testing_gif](https://cdn2.hubspot.net/hubfs/2621212/job-benchmarking-basics.gif)\n### TODO for future versions\n1. add and test feature engieering \n2. add more models!!!! ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score,log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data preprocessing ","metadata":{}},{"cell_type":"markdown","source":"### import data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### label encoding ","metadata":{}},{"cell_type":"markdown","source":"To keep this at baseline i am not going to preprocess the data as of now \n<br>\nwill add feature engineering steps in future versions of this notebook","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(['id', 'target'], axis=1)\nle =LabelEncoder()\ntrain_df['target'] = le.fit_transform(train_df['target'])\ny = train_df['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# benchmarking code","metadata":{}},{"cell_type":"markdown","source":"A simple script to do cross validaion with StratifiedKFold using for loop","metadata":{}},{"cell_type":"code","source":"results_dict = {}\ndef model_benchmarking(model,model_name,N_SPLITS,X,y):\n    eval_list = []\n    kf = StratifiedKFold(n_splits = N_SPLITS)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):    \n        # Prepare training and validation data\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n        trained_model = model.fit(X_train,y_train)\n        pred = trained_model.predict_proba(X_val)\n        evaluation = log_loss(y_val,pred)\n        eval_list.append(evaluation)\n    results_dict[model_name] = eval_list\n    return eval_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model testing ","metadata":{}},{"cell_type":"markdown","source":"## xgboost","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier()\nxgb_score = model_benchmarking(xgb,\"xgboost\",5,X,y)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"xgb logloss: \",np.mean(xgb_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## lgbm","metadata":{}},{"cell_type":"code","source":"lgbm = LGBMClassifier()\nlgbm_score = model_benchmarking(lgbm,\"lgbm\",5,X,y)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"lgbm logloss: \",np.mean(lgbm_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## catboost","metadata":{}},{"cell_type":"code","source":"catb = CatBoostClassifier()\ncatb_score = model_benchmarking(catb,\"catboost\",5,X,y)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"catboost logloss: \",np.mean(catb_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## knn","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn_score = model_benchmarking(knn,\"knn\",5,X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"knn logloss: \",np.mean(knn_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"decitree = DecisionTreeClassifier()\ndecitree_score = model_benchmarking(decitree,\"Decision_Tree\",5,X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Decision Tree logloss: \",np.mean(decitree_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"randomforest = RandomForestClassifier()\nrandomforest_score = model_benchmarking(randomforest,\"random_forest\",5,X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"random forest  logloss: \",np.mean(randomforest_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg_score = model_benchmarking(logreg,\"Logistic_Regression\",5,X,y)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Logistic Regression logloss: \",np.mean(logreg_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# results of benchmark test","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame.from_dict(results_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = df.mean().sort_values(ascending=True).to_frame(\"avg logloss\")\nfig = go.Figure(data=[go.Table(header=dict(values=[\"Model\",\"Avg LogLoss\"]),\n                 cells=dict(values=[result_df.index,result_df[\"avg logloss\"]]))\n                     ])\nfig.update_layout(\n    title=\"Benchmark Results\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig = px.box(df)\n# fig.update_layout(\n#     title=\"Benchmark Results\",\n#     xaxis_title=\"Models\",\n#     yaxis_title=\"Log Loss\"\n# )\n# fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### the best model at baseline is catboost followed by lgbm  \n#### to be continued with more models üòä............","metadata":{}}]}