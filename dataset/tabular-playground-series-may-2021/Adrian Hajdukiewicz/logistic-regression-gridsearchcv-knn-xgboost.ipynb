{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook was prepared by Adrian Hajdukiewicz.","metadata":{}},{"cell_type":"markdown","source":"# Kaggle ML Competition: Tabular Playground Series - May 2021<hr>","metadata":{}},{"cell_type":"markdown","source":"* Setup Imports and Variables\n* Exploratory Data Analysis (EDA)\n* Data cleaning\n * Second dataset - balanced sample\n* Models\n * Linear regression\n * Overfitting / underfitting - KNN\n * KNN\n * KNN with auto tuning\n * XGBoost\n * Logistic Regression - automation\n* Submissions/Final models\n * Logistic Regression - 4 models - unbalanced samples\n * Logistic Regression, balanced sample\n * Logistic Regression - whole dataset + logs of variables + normalized data\n * Logistic Regression - dummy variables for each feature \n* Next steps","metadata":{}},{"cell_type":"markdown","source":"<hr>","metadata":{}},{"cell_type":"markdown","source":"Models were compared using F1 (harmonic mean of recall and precision) and kaggle score (log-loss, calculated after submission by kaggle)","metadata":{}},{"cell_type":"markdown","source":"Types of models used in this notebook:<br>\n1. Linear regression <font color=\"red\">(bad model)</font><br>\n2. Logistic regression<br>\n3. KNN<br>\n4. XGBoost<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"# Setup Imports and Variables<hr>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn import preprocessing\nfrom copy import deepcopy\nimport statistics as stat\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# own utility script\nimport roc_conf_matrix2 as conf_m\n\n# set charts size\nsns.set(rc={'figure.figsize':(9.36, 8.27)}) \n\n# iterations in Logistic regression - max\n# this should be at least around 200-300\nmax_iterations = 800\n\n# Paths\ntest_path = '../input/tabular-playground-series-may-2021/test.csv'\ntrain_path = '../input/tabular-playground-series-may-2021/train.csv'\nsample_submission_path = '../input/tabular-playground-series-may-2021/sample_submission.csv'\n\n# Read data\ntest_data = pd.read_csv(test_path)\ntrain_data = pd.read_csv(train_path)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:19.386384Z","iopub.execute_input":"2021-05-25T10:57:19.387032Z","iopub.status.idle":"2021-05-25T10:57:22.387573Z","shell.execute_reply.started":"2021-05-25T10:57:19.38688Z","shell.execute_reply":"2021-05-25T10:57:22.386631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)<hr>","metadata":{}},{"cell_type":"code","source":"train_data.drop(columns=['id']).describe().T.style.background_gradient(cmap='Greens', axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:22.389034Z","iopub.execute_input":"2021-05-25T10:57:22.389547Z","iopub.status.idle":"2021-05-25T10:57:22.755022Z","shell.execute_reply.started":"2021-05-25T10:57:22.389514Z","shell.execute_reply":"2021-05-25T10:57:22.753805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the table above, I have to **normalize the data if** I assume they are **continuous variables.** (due to different means, min and max values)","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:22.757166Z","iopub.execute_input":"2021-05-25T10:57:22.757497Z","iopub.status.idle":"2021-05-25T10:57:22.796128Z","shell.execute_reply.started":"2021-05-25T10:57:22.757465Z","shell.execute_reply":"2021-05-25T10:57:22.795016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of unique values for each column\nfor column in train_data:\n    print(column, train_data[column].unique())","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:22.79805Z","iopub.execute_input":"2021-05-25T10:57:22.798468Z","iopub.status.idle":"2021-05-25T10:57:22.865718Z","shell.execute_reply.started":"2021-05-25T10:57:22.798422Z","shell.execute_reply":"2021-05-25T10:57:22.864589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No NaN values (or other problematic cases) so **no imputation required**.","metadata":{}},{"cell_type":"code","source":"train_data.skew()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:22.867066Z","iopub.execute_input":"2021-05-25T10:57:22.867356Z","iopub.status.idle":"2021-05-25T10:57:22.964554Z","shell.execute_reply.started":"2021-05-25T10:57:22.867326Z","shell.execute_reply":"2021-05-25T10:57:22.963449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Due to positive skewness**, I will **create an additional dataset with logs of variables** (assuming they are continuous variables).","metadata":{}},{"cell_type":"code","source":"sns.countplot(train_data['target'], palette='winter')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:22.965743Z","iopub.execute_input":"2021-05-25T10:57:22.966034Z","iopub.status.idle":"2021-05-25T10:57:23.265217Z","shell.execute_reply.started":"2021-05-25T10:57:22.966007Z","shell.execute_reply":"2021-05-25T10:57:23.263967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby('target').id.count()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:23.266842Z","iopub.execute_input":"2021-05-25T10:57:23.267275Z","iopub.status.idle":"2021-05-25T10:57:23.286004Z","shell.execute_reply.started":"2021-05-25T10:57:23.267228Z","shell.execute_reply":"2021-05-25T10:57:23.284945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imbalanced sample. That is why predictions are in favour of class 2 and class 3.","metadata":{}},{"cell_type":"markdown","source":"# Data cleaning<hr>","metadata":{}},{"cell_type":"markdown","source":"Assumptions of my basic plan:<br><br>\n1. First I will treat **every variable as continuous**.<br>\n2. Alternatively I will try to create **dummy variables for features with 5 or less unique values** (to treat them as categorical ones). <br>\n3. Eventually I will create **a dataset with dummy variables for each feature**.\n\nIt turned out that the best solution was to treat the variables as categorical or continuous (getting to this point is not included in the notebook) and create dummy variables for each category.","metadata":{}},{"cell_type":"code","source":"x_temp = train_data.drop(columns=['target', 'id'])\n\n# for dummy variables\nx_temp_dum = pd.get_dummies(deepcopy(x_temp), columns=x_temp.columns, prefix=x_temp.columns)\n\nscaler = preprocessing.StandardScaler().fit(x_temp)\nx_temp = pd.DataFrame(scaler.transform(x_temp), columns = x_temp.columns)\n\n#y = train_data['target'].replace(to_replace = 'Class_2', value = '')\ny_temp = pd.to_numeric(train_data['target'].str.replace('Class_', ''))\n\nx_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, train_size=0.8,\ntest_size=0.2)\n\n# create dummy variables\nx_train_dum, x_test_dum, y_train_dum, y_test_dum = train_test_split(x_temp_dum, y_temp, \n                                                                    train_size=0.8, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:23.288264Z","iopub.execute_input":"2021-05-25T10:57:23.288593Z","iopub.status.idle":"2021-05-25T10:57:25.087448Z","shell.execute_reply.started":"2021-05-25T10:57:23.288561Z","shell.execute_reply":"2021-05-25T10:57:25.086331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"># Second dataset - balanced sample","metadata":{}},{"cell_type":"code","source":"# provide function with no of observations for each class\ndef get_unbalanced_data(no_observations):\n    class1_data = train_data[train_data['target']=='Class_1']\n    class2_data = train_data[train_data['target']=='Class_2']\n    class3_data = train_data[train_data['target']=='Class_3']\n    class4_data = train_data[train_data['target']=='Class_4']\n\n    datasets_by_classes = [class1_data, class2_data, class3_data, class4_data]\n    \n    list_balanced_data = []\n    for count, dataset in enumerate(datasets_by_classes):\n        list_balanced_data.append(dataset.tail(no_observations[count]))\n\n    balanced_dataset = pd.concat(list_balanced_data)\n    x_balanced_temp = balanced_dataset.drop(columns=['id', 'target'])\n    y_balanced_temp = pd.to_numeric(balanced_dataset['target'].str.replace('Class_', ''))\n    return x_balanced_temp, y_balanced_temp\n\nx_balanced, y_balanced = get_unbalanced_data([8000, 8000, 8000, 8000])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:25.088937Z","iopub.execute_input":"2021-05-25T10:57:25.089256Z","iopub.status.idle":"2021-05-25T10:57:25.258259Z","shell.execute_reply.started":"2021-05-25T10:57:25.089226Z","shell.execute_reply":"2021-05-25T10:57:25.257222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the colormap\ncolors = sns.diverging_palette(150, 275, s=80, l=55, n=9, as_cmap=True)\n \n# Create heatmap of correlation between features\nsns.heatmap(train_data.corr(), center=0, cmap=colors, robust=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:25.259641Z","iopub.execute_input":"2021-05-25T10:57:25.260072Z","iopub.status.idle":"2021-05-25T10:57:26.890671Z","shell.execute_reply.started":"2021-05-25T10:57:25.260031Z","shell.execute_reply":"2021-05-25T10:57:26.88935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There should not be any problems with collinearity.","metadata":{}},{"cell_type":"code","source":"# Show correlation with target\ny_classes = pd.get_dummies(y_temp, prefix=\"class\")                                            \nplot_corr_data = train_data.join(y_classes)\n\nsns.heatmap(plot_corr_data.corr(), center=0, cmap=colors, robust=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:57:26.892333Z","iopub.execute_input":"2021-05-25T10:57:26.892685Z","iopub.status.idle":"2021-05-25T10:57:28.59662Z","shell.execute_reply.started":"2021-05-25T10:57:26.892652Z","shell.execute_reply":"2021-05-25T10:57:28.59548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The linear correlation between classes and features is low. Which is why I will try to get dummies for each feature to build a better model.","metadata":{}},{"cell_type":"markdown","source":"# Models <hr>","metadata":{}},{"cell_type":"markdown","source":"># Linear regression","metadata":{}},{"cell_type":"code","source":"lin_reg = LinearRegression()\n\nlin_reg.fit(x_train_dum, y_train_dum)\npredictions_lin_reg = lin_reg.predict(x_test_dum)\n\nconf_m.conf_m_report_w_labels(y_test_dum, predictions_lin_reg, 'Linear Regression 1')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T23:28:04.070911Z","iopub.execute_input":"2021-05-24T23:28:04.071459Z","iopub.status.idle":"2021-05-24T23:28:17.024808Z","shell.execute_reply.started":"2021-05-24T23:28:04.071426Z","shell.execute_reply":"2021-05-24T23:28:17.023569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the model results, Linear regression is not a good model for classification because predicted probabilities may go beyond the limits of the set. A way better solution is to use probit/logit model if we want to have linear endogenous variables (but estimation is not linear in the latter case of course).<br><br>\n\nI will not conduct tests for heteroskedasticity (White test), functional form (RESET test), ... because the model is bad at the beginning. \n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T23:02:48.140899Z","iopub.execute_input":"2021-05-23T23:02:48.141241Z","iopub.status.idle":"2021-05-23T23:02:48.147385Z","shell.execute_reply.started":"2021-05-23T23:02:48.141211Z","shell.execute_reply":"2021-05-23T23:02:48.146336Z"}}},{"cell_type":"markdown","source":"># Overfitting / underfitting - KNN","metadata":{}},{"cell_type":"markdown","source":"In general, models have similar performance within test and train data (tests excluded from the notebook) so there is no need of tuning the models (based on test not included in the notebook).<br><br>\n**But in case of KNN we need to select proper number of neighbors.**","metadata":{}},{"cell_type":"code","source":"KNN_test_F1 = {}\nKNN_train_F1 = {}\n\nfor k in range(1, 14):\n    KNN = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    KNN.fit(x_train_dum, y_train_dum)\n    knn_y_train_pred = KNN.predict(x_train_dum)\n    knn_y_test_pred = KNN.predict(x_test_dum)\n\n    # training avg macro F1\n    KNN_train_F1[k] = f1_score(y_train_dum, knn_y_train_pred, average='macro')\n    \n    # testing accuracy\n    KNN_test_F1[k]= f1_score(y_test_dum, knn_y_test_pred, average='macro')\n\nplt.plot(KNN_test_F1.keys(),KNN_test_F1.values(), label = 'Testing avg macro F1')\nplt.plot(KNN_train_F1.keys(),KNN_train_F1.values(), label = 'Training avg macro F1')\nplt.legend()\nplt.title('F1 (avg macro) vs. K Value')\nplt.xlabel('K')\nplt.ylabel('F1 (avg macro)')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T23:28:17.026577Z","iopub.execute_input":"2021-05-24T23:28:17.027032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen above, K equal to 4 is the best option (minimizing the overfitting effect and maximizing F1 (I am trying to tune the model for better results for each class))","metadata":{}},{"cell_type":"markdown","source":"># KNN","metadata":{}},{"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors=4, algorithm='brute')\nKNN.fit(x_train_dum, y_train_dum)\npredictionsKNN1 = KNN.predict(x_test_dum)\n\nconf_m.conf_m_report_w_labels(y_test_dum, predictionsKNN1, 'KNN; model 1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"># KNN with auto tuning","metadata":{}},{"cell_type":"markdown","source":"Using original dataset (without changes)","metadata":{}},{"cell_type":"markdown","source":"**Decrease train dataset size (especially since execution time of KNN does not grow in linear pace along with the size of the dataset ; )**","metadata":{}},{"cell_type":"code","source":"x_train_CV, x_test_CV, y_train_CV, y_test_CV = train_test_split(x_temp, y_temp, \n                                                                train_size=0.4, test_size=0.6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kn = KNeighborsClassifier()\nparams = {\n    'n_neighbors' : list(range(1, 30, 6)), # max val among features is 66\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['ball_tree', 'kd_tree', 'brute']\n}\ngrid_kn = GridSearchCV(estimator = kn,\n                        param_grid = params,\n                        scoring = 'f1_macro', \n                        cv = 3, \n                        verbose = 1,\n                        n_jobs = -1)\ngrid_kn.fit(x_train_CV, y_train_CV)\n\n# best estimator\nprint(grid_kn.best_estimator_)\npreds_knn_grid = grid_kn.predict(x_train_CV)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train dataset - confusion matrix:","metadata":{}},{"cell_type":"code","source":"conf_m.conf_m_report_w_labels(y_train_CV, preds_knn_grid, 'KNN, auto tune, train data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test dataset - confusion matrix:","metadata":{}},{"cell_type":"code","source":"preds_knn_grid_test = grid_kn.predict(x_test_CV)\nconf_m.conf_m_report_w_labels(y_test_CV, preds_knn_grid_test, 'KNN, auto tune, test data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing results within train and test dataset, we may suspect noticeable **overfitting**. Test conducted before proved that k=4 may minimize that effect. <br>So the model needs improvement but KNN did not perform well during submissions so I will not be developing models using KNN anymore.","metadata":{}},{"cell_type":"markdown","source":"># XGBoost","metadata":{}},{"cell_type":"code","source":"XGB1 = XGBRegressor()\nXGB1.fit(x_train_dum, y_train_dum)\npredictionsXGB1 = XGB1.predict(x_test_dum)\n\nconf_m.conf_m_report_no_labels(y_test_dum, predictionsXGB1, 'XGB; model 1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"># Logistic Regression - automation","metadata":{}},{"cell_type":"markdown","source":"Code to create models faster:","metadata":{}},{"cell_type":"code","source":"def get_model_lr(x,y, set_max_iter=max_iterations, set_class_weight='None', set_warm_start=False):\n    lr_balanced1 = LogisticRegression(solver=\"newton-cg\", C=1, multi_class=\"ovr\", max_iter=set_max_iter, n_jobs=-1,\n                                     warm_start=set_warm_start, class_weight=set_class_weight)\n    lr_balanced1.fit(x,y)\n    return lr_balanced1","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:02:50.509348Z","iopub.execute_input":"2021-05-25T13:02:50.509792Z","iopub.status.idle":"2021-05-25T13:02:50.515985Z","shell.execute_reply.started":"2021-05-25T13:02:50.509756Z","shell.execute_reply":"2021-05-25T13:02:50.514658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submissions/Final models<hr>","metadata":{}},{"cell_type":"markdown","source":"KNN and XGB models were excluded because they provided me with not accurate enough probabilities.","metadata":{}},{"cell_type":"markdown","source":"># Logistic Regression - 4 models - unbalanced samples","metadata":{}},{"cell_type":"markdown","source":"4 models (for each class)<br>\nEach one fitted based on sample where the specific class prevailed<br>\nEach model used to predict the probability of one class (where samples from it dominated)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:32:52.631753Z","iopub.execute_input":"2021-05-24T22:32:52.632589Z","iopub.status.idle":"2021-05-24T22:32:52.639387Z","shell.execute_reply.started":"2021-05-24T22:32:52.632517Z","shell.execute_reply":"2021-05-24T22:32:52.638082Z"}}},{"cell_type":"code","source":"# create LR model where class 1 prevails:    (sample: [class1: 8000, class2: 5000, class3: 5000, class4: 5000] )\nx,y = get_unbalanced_data([8000, 5000, 5000, 5000])\nlr_bal_1 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 8000, 5000, 5000])\nlr_bal_2 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 5000, 8000, 5000])\nlr_bal_3 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 5000, 5000, 8000])\nlr_bal_4 = get_model_lr(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [lr_bal_1, lr_bal_2, lr_bal_3, lr_bal_4]\nfinal_submission_lr = test_data['id'].to_frame()\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\ntest_data_to_pred = test_data.drop(columns=['id'])\n\nfor count, model in enumerate(models):\n    final_submission_lr[cols_submission[count]] = model.predict_proba(test_data_to_pred)[0:,count]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_submission_lr.to_csv('submission_lr_4.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle public score: 1.37537","metadata":{}},{"cell_type":"markdown","source":"># Logistic Regression, balanced sample","metadata":{}},{"cell_type":"markdown","source":"Model based on balanced sample (8490 samples of each class - this is the max possible number of samples to have them balanced).","metadata":{}},{"cell_type":"code","source":"# Get model\nx,y = get_unbalanced_data([8490, 8490, 8490, 8490])\n\nlr_balanced_8490 = get_model_lr(x, y)\ntest_data_to_pred = test_data.drop(columns=['id'])\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\n\npreds_lr_8490 = lr_balanced_8490.predict_proba(test_data_to_pred)\n\npreds_lr_8490_df = pd.DataFrame(preds_lr_8490, columns=cols_submission)\nsubmission_lr_balanced_8490 = test_data['id'].to_frame().join(preds_lr_8490_df)\nsubmission_lr_balanced_8490.to_csv('submission_lr_balanced_8490.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:58:13.854736Z","iopub.execute_input":"2021-05-25T10:58:13.855182Z","iopub.status.idle":"2021-05-25T10:58:19.264125Z","shell.execute_reply.started":"2021-05-25T10:58:13.855144Z","shell.execute_reply":"2021-05-25T10:58:19.26267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle public score: 1.37538\n<br>So balancing the sample was not the thing.","metadata":{}},{"cell_type":"markdown","source":"># Logistic Regression - whole dataset + logs of variables + normalized data","metadata":{}},{"cell_type":"code","source":"lr_whole_dataset_log = get_model_lr(np.log(x_temp+10), y_temp)\n\ntest_data_to_pred = test_data.drop(columns=['id'])\ntest_data_to_pred_normal = pd.DataFrame(scaler.transform(test_data_to_pred), \n                                        columns = test_data_to_pred.columns)\ntest_data_to_pred_log_norm = np.log(\n    test_data_to_pred_normal +10 )\n\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npreds_lr_log = lr_whole_dataset_log.predict_proba(test_data_to_pred_log_norm)\npreds_lr_log_df = pd.DataFrame(preds_lr_log, columns=cols_submission)\nsubmission_lr_log_df = test_data['id'].to_frame().join(preds_lr_log_df)\n\nsubmission_lr_log_df.to_csv('submission_lr_log_df_normal.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle public score: 1.10090","metadata":{}},{"cell_type":"markdown","source":"># Logistic Regression - dummy variables for each feature","metadata":{}},{"cell_type":"markdown","source":"Create dummy variables for each feature in whole training dataset and train a logistic regression model using it.","metadata":{}},{"cell_type":"markdown","source":"1. Prepare data for final prediction (to make a submission)<br>\n(generate dummies, same as in the training set):","metadata":{}},{"cell_type":"code","source":"x_test_data_temp = test_data.drop(columns=['id'])\n\n# dummy variables\nx_test_data_dum = pd.get_dummies(deepcopy(x_test_data_temp), \n                                 columns=x_test_data_temp.columns, \n                                 prefix=x_test_data_temp.columns)\n\nnew_submission_data = test_data['id'].to_frame()\n\n# for each column in data used to train models (I want to have same coulmns in the dataset for predictions)\nfor col_submission_data in x_temp_dum.columns:\n    #if the columns appears in submission data:\n    if col_submission_data in list(x_test_data_dum.columns):\n        new_submission_data[col_submission_data] = x_test_data_dum[col_submission_data].values\n    else:\n        new_submission_data[col_submission_data] = 0\n\nnew_submission_data_no_id = new_submission_data.drop(columns=['id'])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T11:35:29.506274Z","iopub.execute_input":"2021-05-25T11:35:29.507018Z","iopub.status.idle":"2021-05-25T11:35:31.607002Z","shell.execute_reply.started":"2021-05-25T11:35:29.506956Z","shell.execute_reply":"2021-05-25T11:35:31.605849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Train model, make predictions, upload results: <br>\n(be aware that logistic regression with so many features takes a lot of time to train; luckily making predictions is only a simple math)","metadata":{}},{"cell_type":"code","source":"lr_dummies = get_model_lr(x_temp_dum, y_temp, 400)\npreds_lr_dummies = lr_dummies.predict_proba(new_submission_data_no_id)\n\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npreds_lr_dummies_df = pd.DataFrame(preds_lr_dummies, columns=cols_submission)\nsubmission_lr_dummies_df = test_data['id'].to_frame().join(preds_lr_dummies_df)\n\nsubmission_lr_dummies_df.to_csv('submission_lr_dummies_df.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T11:36:17.414313Z","iopub.execute_input":"2021-05-25T11:36:17.414689Z","iopub.status.idle":"2021-05-25T12:04:07.547667Z","shell.execute_reply.started":"2021-05-25T11:36:17.414657Z","shell.execute_reply":"2021-05-25T12:04:07.545275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Kaggle public score: 1.09789 <font color=\"green\">(BEST MODEL)</font>**","metadata":{}},{"cell_type":"markdown","source":"# Next steps<hr>","metadata":{}},{"cell_type":"markdown","source":"I would expect better performance for predicting class 1 and class 4 but using balanced sample did not help. So other ways may be considered.\n<br><br>\n\nIt is also an option to delete outliers (e.g. using Least Trimmed Squares or quantile regression, though these models may not be fitted well) and verify if it helped.<br><br>","metadata":{}}]}