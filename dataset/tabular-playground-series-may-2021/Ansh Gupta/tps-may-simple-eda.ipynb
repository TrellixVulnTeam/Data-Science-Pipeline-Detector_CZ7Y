{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\nFILE_PATH='/kaggle/input/tabular-playground-series-may-2021/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The tabular playground series are hosted by kaggle that are always more approachable compared to the their normal featured competitions. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. \n","metadata":{}},{"cell_type":"markdown","source":"## Let's talk about data!","metadata":{}},{"cell_type":"markdown","source":"The dataset is used for this competition is synthetic, but based on a real dataset and generated using a **CTGAN**. The original dataset deals with **predicting the category on an eCommerce product** given various attributes about the listing. Let's check them!","metadata":{}},{"cell_type":"code","source":"# reading the training and test data\ntrain_data=pd.read_csv(FILE_PATH+'train.csv')\ntest_data=pd.read_csv(FILE_PATH+'test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns of training data\ntrain_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features are **anonymized**, so it's difficult to directly get insights for each features from their names. The feature 'id' is not normally used in EDA so for now, let's remove it for now.","metadata":{}},{"cell_type":"code","source":"# dropping the 'id' column from both train and test data\ntrain_data.drop(['id'],inplace=True,axis=1)\ntest_data.drop(['id'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking unique elements of target feature of train data\ntrain_data['target'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the 50 features seems to have **discrete numbers (starting from 0)**, and the target has 4 unique values but they have data type 'object'. ***It's a dataset with aim to classify the target among the 4 classes(namely Class_1, Class_2,Class_3,Class_4)***","metadata":{}},{"cell_type":"code","source":"# converting the data type from 'object' to 'int'\ntrain_data['target']=train_data['target'].str[6:].astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features columns\nfeatures=[]\nfor i in range(0,50):\n    features.append('feature_'+str(i))\n#features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display features and their minimum value (other than zero).\nprint('For training dataset--')\nfor fea in features:\n    if train_data[fea].min()!=0:\n        print(fea,\" \",train_data[fea].min())\nprint('For test dataset--')\nfor fea in features:\n    if test_data[fea].min()!=0:\n        print(fea,\" \",test_data[fea].min())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that very few features are present (given above) which have their minimum value less than zero. ","metadata":{}},{"cell_type":"code","source":"# Maximum value present in the dataset\nprint('Maximum value in training data',train_data.max().values.max())\nprint('Maximum value in test data',test_data.max().values.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value is **not very high(compared to shape of data)**. Let's check in which feature the are maximum unique values.","metadata":{}},{"cell_type":"code","source":"# Finding feature that has maximum number of unique values both in training and test data\nprint('For training data--')\nmax_value=-1\nfeat=''\nfor fea in features:\n    if train_data[fea].nunique()>max_value:\n        feat=fea\n        max_value=train_data[fea].nunique()\nprint(feat,\" \",max_value)\nprint('For test data--')\nmax_value=-1\nfeat=''\nfor fea in features:\n    if test_data[fea].nunique()>max_value:\n        feat=fea\n        max_value=test_data[fea].nunique()\nprint(feat,\" \",max_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, the maximum number of unique values is **71**, in training set and **65** in test set (not a large number compared to number of samples in the dataset), both present on the same feature **feature_38** . Thus, we can work thinking that all the features present are to be considered as categorical type, due to presence of discrete and finite values.","metadata":{}},{"cell_type":"markdown","source":"One more thing to note if one is working considering that the features are categorical is the values of features that is present of training data but not on test and vice-versa. ","metadata":{}},{"cell_type":"code","source":"print('Value of features that is present on training data but not on test data')\nprint('-'*100)\nfor f in features:\n    train_set=set(train_data[f])\n    test_set=set(test_data[f])\n    # values present in train but not in test\n    dif_set=train_set.difference(test_set)\n    if dif_set != set():\n        print(f,'-'*10,dif_set)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Value of features that is present on test set but not on training set')\nprint('-'*100)\nfor f in features:\n    train_set=set(train_data[f])\n    test_set=set(test_data[f])\n    # values present in test but not in train\n    dif_set=test_set.difference(train_set)\n    if dif_set != set():\n        print(f,'-'*10,dif_set)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time for Visuals!\n","metadata":{}},{"cell_type":"markdown","source":"The first thing that needs to be checked is obviously how the **target is distributed.**\n","metadata":{}},{"cell_type":"code","source":"#count plot\nsns.countplot(x='target',data=train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentile of each class\nfor i in range(1,5):\n    print('Class_',i,end=' is ')\n    print(((train_data['target']==i).sum()/train_data.shape[0])*100,'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**57.497%** of training data has target 'Class_2', whereas only **8.49%** of training data has target 'Class_1'. Thus, our target is bit **imbalanced.**\n","metadata":{}},{"cell_type":"markdown","source":"Let's check the distribution for our features now,","metadata":{}},{"cell_type":"code","source":"def display(feature):\n    ''' Function to display plot of a feature present both in training and test side by side'''\n    ax=[]\n    fig=plt.figure(figsize=(15,5))\n    ax.append(fig.add_subplot(1,2,1))\n    ax[-1].set_title(\"Training Data:\")\n    sns.histplot(x=feature,data=train_data,stat='density',kde =True)\n    ax.append(fig.add_subplot(1,2,2))\n    ax[-1].set_title(\"Test Data:\")\n    sns.histplot(x=feature,data=test_data,stat='density', kde=True)\n    return plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fea in features:\n    print('\\033[1m',fea.upper(),'\\033[0m') #\\033[1m and \\033[0m can be used to make python text bold\n    display(fea)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visuals, we can see that that majority of features are skewed. To check if a feature is left skewed or right skewed, we can just check the condition that for a feature to be **left skewed**, it's mean should be less than it's median, (from the figure below).\n![Skewness](https://www.statisticshowto.com/wp-content/uploads/2014/02/pearson-mode-skewness.jpg)","metadata":{}},{"cell_type":"code","source":"for fea in features:\n    if train_data[fea].mean() < train_data[fea].median():\n        print(fea,' is left skewed')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, we can clearly infer that majority of features are **right skewed**. ","metadata":{}},{"cell_type":"markdown","source":"Let's see how much percentage of our data has our value that is present the most at each feature","metadata":{}},{"cell_type":"code","source":"# making a table showing the maximum occuring value and what percentage of data does it occupy on the training data\nfreq_table=pd.DataFrame()\nfreq=[]\nper=[]\nfor fea in features:\n    freq.append(train_data[fea].mode()[0])\n    per.append(((train_data[fea]==train_data[fea].mode()[0]).sum()/train_data.shape[0])*100)\nfreq_table['Features']=features\nfreq_table['Max Occuring Value']=freq\nfreq_table['Percentage Occupied']=per\nfreq_table=freq_table.sort_values('Percentage Occupied')\nfreq_table.reset_index()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value that occurs most frequently at training data is **zero**, many of which features are occupying **more than 50% of our data**. Let's visualize it to get a better idea.","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(15,15))\nbarh=plt.barh(freq_table['Features'],freq_table['Percentage Occupied'])\nplt.bar_label(barh, fmt='%.01f%%')\nplt.xlabel('Percentage Occupied')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At last, let's use **heatmap** to visualize the **correlation** between the features or between our features and target.","metadata":{}},{"cell_type":"code","source":"corr=train_data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(20, 20))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True,cmap=\"YlGnBu\",linewidth=0.5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above visual, we can easily confirm that there is **no high correlation present between any features.**","metadata":{}},{"cell_type":"markdown","source":"Any suggestions what can I also add are most welcome and **Kindly Upvote the notebook if it is of any help!**","metadata":{}}]}