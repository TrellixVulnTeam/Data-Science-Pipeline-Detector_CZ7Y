{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://github.com/SauravMaheshkar/Tabular-Playground-Series-May-2021/blob/main/assets/Banner.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"# Table of Content\n\n1. [Packages üì¶ and Basic Setup](#basic)\n2. [Pre-Processing üëéüèª -> üëç](#preprocess)\n3. [The Model üë∑‚Äç‚ôÄÔ∏è](#model)\n4. [Training üí™üèª](#train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Disclaimer\n\nThis kernel builds on top of [@subinium](https://www.kaggle.com/subinium)'s kernel [TPS-May:Deeplearning Pipeline for Beginner](https://www.kaggle.com/subinium/tps-may-deeplearning-pipeline-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"basic\"> </a>\n\n# Packages üì¶ and Basic Setup","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install wandb\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Weights and Biases\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=api_key);\nwandb.init(project='TPS May 2021', entity='sauravmaheshkar')","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the dataset consists of 50 feature columnns with 4 classes. ","metadata":{}},{"cell_type":"code","source":"# Basic Paths\ntrain = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\ntrain.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 'preprocess'> </a>\n# Pre-Processing üëéüèª -> üëç\n\nIn this kernel I'll highlight the relatively new `tensorflow.feature_column` submodule which contains a ton of useful methods to deal with structured data. For more details kindly visit the [documentation](https://www.tensorflow.org/api_docs/python/tf/feature_column). Tensorflow offers a ton of feature columns for us to experiment with, viz :\n\n* Numeric columns\n* Bucketized columns\n* Categorical columns\n* Embedding columns\n* Hashed feature columns\n* Crossed feature columns\n\nHave a look at [this tutorial](https://www.tensorflow.org/tutorials/structured_data/feature_columns) for more details.","metadata":{}},{"cell_type":"markdown","source":"Upon a closer look, we realize that most features are left skewed in this dataset. Thus, Normalization seems ideal.","metadata":{}},{"cell_type":"code","source":"for i in range(50):\n    mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n    train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)/std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also convert the `target` columns into binary class matrices using the `tf.keras.utils.to_categorical()` function.","metadata":{}},{"cell_type":"code","source":"# transform target column into 0,1,2,3 values\nlabel_dict = {val:idx for idx, val in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label_dict)\n\ntrain['target'] = tf.keras.utils.to_categorical(train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, we split out dataset into train, validation and test splits. Most deep learning models will overfit, so having a nice split ratio is key üîë","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(train, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.4)\n\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rather than using dataframes we'll create a `tf.data.Dataset` from our original dataframe. This also allows us to efficiently shuffle, prefetch and create batches.","metadata":{}},{"cell_type":"code","source":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('target')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A numeric column is the simplest type of feature column. It is used to represent real valued features. When using this column, our model will receive the column value from the dataframe **unchanged**. The output of a feature column will become the input to the model.","metadata":{}},{"cell_type":"code","source":"from tensorflow import feature_column\n\nfeature_columns = []\n\nfor i in range(50):\n    feature_columns.append(feature_column.numeric_column(f'feature_{i}'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='model'></a>\n# The Model üë∑‚Äç‚ôÄÔ∏è","metadata":{}},{"cell_type":"markdown","source":"We create a `feature_layer`, which will act as the first layer in our model. A batch size of 64 was arbitrarily chosen.","metadata":{}},{"cell_type":"code","source":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n\nbatch_size = 64\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We just add a Dense layer in the end to act as a classification head for the model and compile using the `sgd` optimizer and the `sparse_categorical_crossentropy` loss function.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n        feature_layer,\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(4, activation='softmax')])\n\nmodel.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='train'></a>\n# Training üí™üèª","metadata":{}},{"cell_type":"markdown","source":"We train for a mere 10 epochs and the overfitting is quite obvious here. Better Regularization strategies + better feature selection can be extremely helpful moving forward. We'll also log our metrics to [Weights and Biases](https://wandb.ai/site) for efficient experiment tracking.","metadata":{}},{"cell_type":"code","source":"model.fit(train_ds,epochs = 10, verbose = 2,\n          validation_data=val_ds,\n          validation_steps = 100,\n          callbacks = [WandbCallback()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can evaluate on our test set and see the loss and accuracy. As, we can see the model overfits to a large extent.","metadata":{}},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy\", accuracy)\nprint(\"Loss\", loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\ntrain_df = train_df.drop('id', axis = 1)\ntest_df = test_df.drop('id', axis=1)\n\nfor i in range(50):\n    mean, std = train_df[f'feature_{i}'].mean(), train_df[f'feature_{i}'].std()\n    test_df[f'feature_{i}'] = test_df[f'feature_{i}'].apply(lambda x : (x-mean)/std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_to_dataset_test(dataframe, batch_size=32):\n    dataframe = dataframe.copy()\n    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds\n\ntest_df = df_to_dataset_test(test_df, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model.predict(test_df)\nsample_submission.to_csv('densefeatures_submission.csv',index = False)\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}