{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras Model with DAE/TE Features (TPU)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I will show how to train a NN model with DAE and target encoded features in Keras (TPU).\n\nThe contents of the notebooks are organized as follows:\n1. Installing and loading libraries: installs Kaggler and load data and libraries\n2. Feature engineering: shows how to transform features with target encoding with Kaggler\n3. Model definition and training: shows how to setup TPU and define a NN model with skip connection in Keras\n4. Submission\n\nEnjoy~!","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries and Data","metadata":{}},{"cell_type":"code","source":"!pip install -U kaggler","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom warnings import simplefilter\nprint(kaggler.__version__, tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name = 'dae_te'\nalgo_name = 'nn'\nversion = 2\nmodel_name = f'{algo_name}_{feature_name}_v{version}'\n\ndata_dir = Path('../input/tabular-playground-series-may-2021')\ntrain_file = data_dir / 'train.csv'\ntest_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\n\ndae_feature_file = '../input/tps5-dae-features/dae.h5'\nfeature_file = f'{feature_name}.h5'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\nid_col = 'id'\ntarget_col = 'target'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\nn_fold = 5\nn_class = 4\nn_stop = 5\nn_epoch = 100\nn_emb = 16\nn_hidden_unit = 128 \ndropout = .3\nbatch_size = 1024","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = trn[target_col].str.split('_').str[1].astype(int) - 1\nn_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nfeature_cols = df.columns.tolist()\nprint(y.shape, df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering: DAE + Target Encoding + Label Encoding","metadata":{}},{"cell_type":"code","source":"df_dae = pd.read_hdf(dae_feature_file, key='data')\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\nte.fit(trn[feature_cols], y)\ndf_te = te.transform(df[feature_cols])\ndf_te.columns = [f'te_{x}' for x in df.columns]\n\nall_df = pd.concat([df, df_te, df_dae], axis=1)\nall_df.to_hdf(feature_file, key='data')\nprint(all_df.shape)\nall_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = all_df.columns.tolist()\nn_feature = len(feature_cols)\ncat_cols = df.columns.tolist()\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nn_cat_col = len(cat_cols)\nn_num_col = len(num_cols)\nprint(n_feature, n_cat_col, n_num_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3. Keras NN Model Training","metadata":{}},{"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(n_emb=16, n_hidden_unit=128, dropout=.3):\n    cat_inputs = []\n    embs = []\n    for i, col in enumerate(cat_cols):\n        inp = keras.layers.Input((1,), name=f'{col}')\n        emb = keras.layers.Embedding(input_dim=all_df[col].nunique(), output_dim=n_emb)(inp)\n        cat_inputs.append(inp)\n        embs.append(emb)\n\n    num_inputs = keras.layers.Input((len(num_cols),))\n    \n    inputs = cat_inputs + [num_inputs]\n    merged_inputs = keras.layers.Concatenate()(inputs)\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(merged_inputs)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n\n    outputs = keras.layers.Dense(n_class, 'softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    model = build_model(n_emb, n_hidden_unit, dropout)\n    model.summary()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch, lr, warmup=5):\n    if epoch < warmup:\n        return lr * 1.5\n    else:\n        return lr * tf.math.exp(-.1)\n\nes = keras.callbacks.EarlyStopping(patience=n_stop, restore_best_weights=True)\nlr = keras.callbacks.LearningRateScheduler(scheduler)\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\nX = all_df.iloc[:n_trn].values\nX_tst = all_df.iloc[n_trn:].values\n\nP = np.zeros((n_trn, n_class), dtype=float)\nP_tst = np.zeros((X_tst.shape[0], n_class), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n    model = build_model()\n    X_trn, X_val = X[i_trn], X[i_val]\n    y_trn, y_val = y[i_trn], y[i_val]\n    history = model.fit([[X_trn[:, i] for i in range(n_cat_col)] + [X_trn[:, n_cat_col:]]], y_trn, \n                        validation_data=([[X_val[:, i] for i in range(n_cat_col)] + [X_val[:, n_cat_col:]]], y_val), \n                        epochs=n_epoch, batch_size=batch_size, callbacks=[es, lr], verbose=0)\n    P[i_val] = model.predict([[X_val[:, i] for i in range(n_cat_col)] + [X_val[:, n_cat_col:]]])\n    P_tst += model.predict([[X_tst[:, i] for i in range(n_cat_col)] + [X_tst[:, n_cat_col:]]]) / n_fold\n    \n    print(f'CV #{i} Loss: {log_loss(y[i_val], P[i_val]):.6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['lr'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'CV Log Loss: {log_loss(y, P):.6f}')\nnp.savetxt(predict_val_file, P, fmt='%.6f')\nnp.savetxt(predict_tst_file, P_tst, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4. Submission","metadata":{}},{"cell_type":"code","source":"sub[sub.columns] = P_tst\nsub.to_csv(submission_file)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you find this notebook helpful, please upvote it and share your feedback in comments. I really appreciate it.\n\nYou can find my other notebooks in both the current and previous TPS competitions below:\n* [Kaggler DAE + AutoLGB Baseline](https://www.kaggle.com/jeongyoonlee/kaggler-dae-autolgb-baseline): trains the LightGBM model with Kaggler's DAE features and AutoLGB\n* [Adversarial Validation with LightGBM](https://www.kaggle.com/jeongyoonlee/adversarial-validation-with-lightgbm): shows how close/different the feature distributions between the training and test data. It's a good exercise to perform it at the begining of the competition to understand the risk of overfitting to the training data.\n* [DAE with 2 Lines of Code with Kaggler](https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler): shows how to extract DAE features and train the AutoLGB model with TPS4 data.\n* [AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb): shows how to build a basic AutoEncoder using Keras, and perform automated feature selection and hyperparameter optimization using Kaggler's AutoLGB.\n* [Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder): shows how to build a more sophiscated version of * AutoEncoder, called supervised emphasized Denoising AutoEncoder (DAE), which trains DAE and a classifier simultaneously.\n* [Stacking Ensemble](https://www.kaggle.com/jeongyoonlee/stacking-ensemble): shows how to perform stacking ensemble.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}