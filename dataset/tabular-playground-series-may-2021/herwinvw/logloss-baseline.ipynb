{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multiclass logarithmic loss\n\nA quick look into the behavior and definition of the logloss as used in the Kaggle Tabular Data competition of May 2021. The logloss uses natural logs rather than log2 (as in https://en.wikipedia.org/wiki/Cross_entropy). In addition to that an epsilon value is used to prevent returning infinite loss values. The definition used in the competition is exactly the one from sklearn. The Tensorflows implementation is similar (with a higher epsilon), as is the one in LightGMB (https://github.com/microsoft/LightGBM/blob/master/src/metric/multiclass_metric.hpp).","metadata":{}},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\ntrain['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['target'].unique())\nle.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = le.transform(train['target'])\ntarget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def submit(test_pred, filename):\n    submission = pd.DataFrame(test_pred, columns=le.classes_)\n    submission.insert(0, 'id', test['id'])\n    submission.to_csv(filename, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline model: majority class","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nlabels, frequencies = np.unique(target, return_counts=True)\n(labels, frequencies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequencies/len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculated value:\n* y = class 1 (57.5% of cases): logloss = 0\n* y != class 1 (42.5% of cases): logloss = log(10^-15) = -34.5","metadata":{}},{"cell_type":"code","source":"pred = [[0.0,1.0,0.0,0.0]]*len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nprint(f'logloss:{-math.log(10**-15)*(1-frequencies[1]/len(train))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate with sklearn","metadata":{}},{"cell_type":"code","source":"log_loss(target, pred, labels=[0,1,2,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate with tensorflow, uses a non-configurable(?) epsilon of 10^-7.","metadata":{}},{"cell_type":"code","source":"import math\nprint(f'logloss:{-math.log(10**-7)*(1-frequencies[1]/len(train))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nscc = SparseCategoricalCrossentropy()\nscc(target, pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check with leaderboard","metadata":{}},{"cell_type":"code","source":"test_pred = [[0.0,1.0,0.0,0.0]]*len(test)\nsubmit(test_pred, 'baseline_majority.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Score is 14.62209, seems to match","metadata":{}},{"cell_type":"markdown","source":"# Baseline model: a priori probabilities","metadata":{}},{"cell_type":"markdown","source":"Calculated value:\n* y = class 0 (8.49% of cases): logloss=log(0.0849)=-2.46\n* y = class 1 (57.5% of cases): logloss = log(0.57497)=-0.55\n* etc\n\nThis is the best single-point estimator.","metadata":{}},{"cell_type":"code","source":"pred = frequencies/len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'logloss:{-np.sum(pred * np.log(pred))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.tile(frequencies/len(train), (len(train),1))\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_loss(target, pred, labels=[0,1,2,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.losses import Reduction\nscc = SparseCategoricalCrossentropy()\nscc(target, pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check with leaderboard","metadata":{}},{"cell_type":"code","source":"test_pred = pred = np.tile(frequencies/len(test), (len(test),1))\nsubmit(test_pred, 'baseline_apriori.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Leaderboard score is 1.11634, seems to match","metadata":{}}]}