{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_squared_log_error, classification_report, balanced_accuracy_score\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom imblearn.over_sampling import SMOTE\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n# counts each type of Class\nsorted(train['target'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lencoder = LabelEncoder()\ntarget = pd.DataFrame(lencoder.fit_transform(train['target']),columns=['target'])\n\ntrain.drop(['target'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'target', data = target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#oversample = SMOTE()\n#train, target = oversample.fit_resample(train, target)\n#np.sum(target, axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.20, \n                                                  stratify = target, random_state = 2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking from https://www.kaggle.com/remekkinas/tps-5-weighted-training-xgb-rf-lr-smote\n\ndef training(model, X_train_oof, y_train_oof, weighted = False, b_type = True):\n    test_preds = None\n    test_oof_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 2\n    \n    skf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = 4042)\n    for fold, (tr_index, val_index) in enumerate(skf.split(X_train_oof.values, y_train_oof.values)):\n        \n        print(f\"\\nFold {fold + 1}\")\n        x_train_o, x_val_o = X_train_oof.iloc[tr_index], X_train_oof.iloc[val_index]\n        y_train_o, y_val_o = y_train_oof.iloc[tr_index], y_train_oof.iloc[val_index]\n        \n        if weighted:\n            weights_y = weights_df.iloc[tr_index]\n\n        eval_set = [(x_val_o, y_val_o)]\n        \n        if b_type:\n            if weighted:\n                model.fit(x_train_o, y_train_o, eval_set = eval_set, verbose = 500, sample_weight = weights_y)\n            else:\n                model.fit(x_train_o, y_train_o, eval_set = eval_set, verbose = 500)\n        \n        else:\n            model.fit(x_train_o, y_train_o)\n\n        train_preds = model.predict(x_train_o)\n        train_rmse += mean_squared_error(y_train_o,train_preds, squared = False)\n        print(\"\\n- Training RMSE : \", mean_squared_error(y_train_o,train_preds, squared = False))\n\n        val_preds = model.predict(x_val_o)\n        val_rmse += mean_squared_error(y_val_o, val_preds, squared = False)\n        print(\"- Validation RMSE : \", mean_squared_error(y_val_o, val_preds, squared = False))\n        print('---------------')\n\n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n            test_oof_preds = model.predict_proba(X_val.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n            test_oof_preds += model.predict_proba(X_val.values)\n\n    print(\"\\nAverage Training RMSE : \" , train_rmse / n_splits)\n    print(\"Average Validation RMSE : \" , val_rmse / n_splits)\n\n    test_preds /= n_splits\n    test_oof_preds /= n_splits\n    \n    return test_preds, test_oof_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_sample_weight\nweights_df = pd.DataFrame(compute_sample_weight(\"balanced\", y_train.target), columns = ['weight'])\n\nxgb_model_weighted = xgb.XGBClassifier(eval_metric='mlogloss')\ntest_preds, y_pred = training(xgb_model_weighted, X_train, y_train, weighted = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = np.argmax(y_pred, axis=1)\nprint(f'MSE Score: {mean_squared_error(y_val,y_preds)}\\n')\nprint(classification_report(y_val, y_preds))\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_val, y_preds)), annot=True, linewidths=.5, fmt=\"d\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'target', data= pd.DataFrame(y_preds, columns=['target']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'objective': 'multiclass', 'num_class' : 4,  'metric': 'multi_logloss', \n              'verbosity' : -1, 'boosting_type' : 'gbdt', 'bagging_freq' : 1}\n\n# boosting = ['gbdt', 'goss','dart'] ´rf´ ?\n\n#, 'class_weight' : 'balanced'\n# , 'is_unbalance':False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    num_iterations = trial.suggest_int('num_iterations',50,500)\n    max_depth = trial.suggest_int('max_depth',3,10)\n    num_leaves = trial.suggest_int('num_leaves',10,30)\n    learning_rate = trial.suggest_uniform('learning_rate',0.01,0.2)\n    subsample = trial.suggest_uniform('subsample',0.5, 0.9)\n    feature_fraction = trial.suggest_uniform('feature fraction',0.5, 0.9)\n    #min_child_samples = trial.suggest_int('min_child_samples', 1, 110),\n    #min_child_weight = trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n    lambda_l2 = trial.suggest_uniform('lambda_l2',1e-5,20)   \n    \n    model = LGBMClassifier(**params,\n            num_iterations = num_iterations,\n            max_depth = max_depth,\n            num_leaves = num_leaves,\n            learning_rate = learning_rate,\n            subsample = subsample,\n            feature_fraction = feature_fraction,\n            #min_child_samples = min_child_samples,\n            #min_child_weight = min_child_weight,\n            lambda_l2 = lambda_l2\n            )\n    \n    nll = cross_val_score(model,X_train,y_train,scoring = 'neg_log_loss', cv = 5).mean()\n    return -1*nll","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampler = TPESampler(seed=1111)\nstudy = optuna.create_study(direction = 'minimize', sampler = sampler)\nstudy.optimize(objective,n_trials = 1)\nprint('numbers of the finished trials:' , len(study.trials))\nprint(study.best_value)\nprint(study.best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier(**params, \n                    num_iterations = 490,\n                    max_depth = 5,\n                    num_leaves = 22,\n                    learning_rate = 0.026798877915977834,\n                    subsample = 0.6615232298649514,\n                    feature_fraction = 0.5881079099486431,\n                    #min_child_samples = 27,\n                    #min_child_weight = 0.04781667419116532,\n                    lambda_l2 = 10.543869110101163)\n\nlgbm.fit(X_train,y_train,verbose = False)\npreds = lgbm.predict(X_val)\n\nprint('Classification report:\\n')\nprint(classification_report(y_val,preds))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_val, preds)), annot=True, linewidths=.5, fmt=\"d\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'target', data= pd.DataFrame(preds, columns=['target']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')\n\nsample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = lgbm.predict_proba(test.values)\n\nsample_submission.to_csv(\"my_submissionOPT.csv\",index = False)\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}