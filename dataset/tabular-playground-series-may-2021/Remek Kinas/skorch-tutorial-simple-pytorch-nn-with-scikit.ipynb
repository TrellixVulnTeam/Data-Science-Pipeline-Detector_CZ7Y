{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SKORCH EXAMPLE ON TPS-05 - SIMPLE NN START WITH FEW LINES OF CODE\n\n<div class=\"alert alert-warning\">\nThe goal of skorch is to make it possible to use PyTorch with sklearn. This is achieved by providing a wrapper around PyTorch that has an sklearn interface. In that sense, skorch is the spiritual successor to nolearn, but instead of using Lasagne and Theano, it uses PyTorch. Skorch does not re-invent the wheel, instead getting as much out of your way as possible. If you are familiar with sklearn and PyTorch, you don’t have to learn any new concepts, and the syntax should be well known. (If you’re not familiar with those libraries, it is worth getting familiarized.)\n</div>\n<div><br></div>\n<div aligh=\"center\"><img src=\"https://skorch.readthedocs.io/en/stable/_static/logo.svg\"></div>\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:10:27.687445Z","iopub.execute_input":"2021-05-27T14:10:27.687815Z","iopub.status.idle":"2021-05-27T14:10:35.447107Z","shell.execute_reply.started":"2021-05-27T14:10:27.687772Z","shell.execute_reply":"2021-05-27T14:10:35.445966Z"}}},{"cell_type":"markdown","source":"The goal of this notebook is to show how to create very simple NN using Pytorch model with Scikit-Learn Wrapper. Content:\n\n<ul>\n    <li>Install skorch</li>\n    <li>Prepare data</li>\n    <li>Define Pytorch simple Sequential model</li>\n    <li>Define skorch wrapper</li>\n    <li>Create simple scikit-learn Pipeline</li>\n    <li>Search for NN hyperparameters using GridSearchCV</li>\n    <li>Callback implemented</li>\n    <ul>\n        <li>EarlyStopping</li>\n        <li>Lerning Scheduler</li>\n    </ul>\n    <li>Gridsearch - searching for best Network Architecture</li>\n    <ul>\n        <li>The best NonLinear module search using GridSearch</li>\n        <li>Module configuration (eg. BatchNormalization on/off)</li>\n    </ul>\n</ul>\n\n<div class=\"alert alert-info\">\n    <strong>Important links:</strong>\n<ul>\n    <li><a href=\"https://skorch.readthedocs.io/en/stable/\">Skorch documentation</a></li>\n    <li><a href=\"https://github.com/skorch-dev/skorch\">Skorch repo</a></li>\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install skorch -q ","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:30.492519Z","iopub.execute_input":"2021-05-30T10:36:30.49286Z","iopub.status.idle":"2021-05-30T10:36:36.101375Z","shell.execute_reply.started":"2021-05-30T10:36:30.49283Z","shell.execute_reply":"2021-05-30T10:36:36.100251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import EpochScoring\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.105053Z","iopub.execute_input":"2021-05-30T10:36:36.105363Z","iopub.status.idle":"2021-05-30T10:36:36.114984Z","shell.execute_reply.started":"2021-05-30T10:36:36.105334Z","shell.execute_reply":"2021-05-30T10:36:36.114074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MINIMAL DATASET PREPARATION","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\", index_col = 'id').values.astype('float32')\n\nX = train.drop('target', axis = 1).values.astype('float32')\n\nlencoder = LabelEncoder()\ny = lencoder.fit_transform(train['target']).astype('int64')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.117211Z","iopub.execute_input":"2021-05-30T10:36:36.117906Z","iopub.status.idle":"2021-05-30T10:36:36.550201Z","shell.execute_reply.started":"2021-05-30T10:36:36.117866Z","shell.execute_reply":"2021-05-30T10:36:36.549331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LET'S DEFINE NN MODEL (WE USE PYTORCH WAY OF DEFINING MODEL)","metadata":{}},{"cell_type":"code","source":"num_features = 50\n\nblock_config = {\n        'fc_block1':{\n            'values':[num_features, 128]},\n        'fc_block2':{\n            'values':[num_features, 128, 64]},\n        'fc_block3':{\n            'values':[num_features, 256, 128, 64]},\n        'fc_block4':{  \n            'values':[num_features, 512, 256, 128, 64]} \n    }\n\n\ndef linear_block(in_features, out_features, p_drop, nonlinear, batch_norm, *args, **kwargs):\n    \n    layers = []\n    layers.append(nn.Linear(in_features, out_features))\n    if batch_norm:\n        layers.append(nn.BatchNorm1d(out_features))\n    layers.append(nonlinear)\n    layers.append(nn.Dropout(p = p_drop))    \n    \n    return nn.Sequential(*layers)\n\nclass TPS05Classification(nn.Module):\n    def __init__(self, num_class = 4, dropout = 0.3, nonlinear = nn.ReLU(), block = 1, batch_norm = True):\n        super(TPS05Classification, self).__init__()\n        \n        self.non_linear = nonlinear\n        \n        self.lin_sizes = block_config['fc_block'+str(block)]['values']\n        \n        lin_blocks = [linear_block(in_f, out_f, dropout, self.non_linear, batch_norm) \n                      for in_f, out_f in zip(self.lin_sizes, self.lin_sizes[1:])]\n        \n        self.linear = nn.Sequential(*lin_blocks)\n        \n        self.out = nn.Sequential(\n            nn.Linear(block_config['fc_block'+str(block)]['values'][-1], num_class))\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return  F.softmax(self.out(x), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.552022Z","iopub.execute_input":"2021-05-30T10:36:36.552387Z","iopub.status.idle":"2021-05-30T10:36:36.563805Z","shell.execute_reply.started":"2021-05-30T10:36:36.552348Z","shell.execute_reply":"2021-05-30T10:36:36.562816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SKORCH SCIKIT-LEARN WRAPPER ","metadata":{}},{"cell_type":"markdown","source":"### CALLBACKS\n\nInstead of searching LR and max_epochs params using GridSearch I decided to use better strategy - use Callbacks. Skorch supports wide range of callbacks (you can even write custom one).\n\n#### BUILD-IN","metadata":{}},{"cell_type":"code","source":"from skorch.callbacks import LRScheduler, EarlyStopping\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nlr_scheduler = LRScheduler(policy = ReduceLROnPlateau, monitor = 'valid_loss', mode = 'min', patience = 3, factor = 0.1, verbose = True)\nearly_stopping = EarlyStopping(monitor='valid_loss', patience = 10, threshold = 0.0001, threshold_mode='rel', lower_is_better=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.56515Z","iopub.execute_input":"2021-05-30T10:36:36.565565Z","iopub.status.idle":"2021-05-30T10:36:36.578Z","shell.execute_reply.started":"2021-05-30T10:36:36.565521Z","shell.execute_reply":"2021-05-30T10:36:36.577078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### CUSTOM\n\nThis is very easy sample to show you how to jump into two stages: epoch_end and train_end but you can define more callbacks: https://skorch.readthedocs.io/en/stable/callbacks.html# ","metadata":{}},{"cell_type":"code","source":"from skorch.callbacks import Callback\n\n\nclass TPS05CustomCallback(Callback):\n    def __init__(self, ):\n        self.best_epoch_ = 0\n\n    def initialize(self):\n        self.best_log_loss_ = 999\n\n    def on_epoch_end(self, net, **kwargs):\n        if net.history[-1, 'valid_loss'] < self.best_log_loss_:\n            self.best_log_loss_ = net.history[-1, 'valid_loss']\n            self.best_epoch_ = len(net.history)\n\n    def on_train_end(self, net, **kwargs):\n        print(f\">>>> Training end. The best log_loss: {self.best_log_loss_} on epoch: {self.best_epoch_} <<<< \\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.579307Z","iopub.execute_input":"2021-05-30T10:36:36.579692Z","iopub.status.idle":"2021-05-30T10:36:36.588528Z","shell.execute_reply.started":"2021-05-30T10:36:36.579655Z","shell.execute_reply":"2021-05-30T10:36:36.587616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SKORCH - SCIKIT PYTORCH WRAPPER","metadata":{}},{"cell_type":"code","source":"# No additional parameters - we will find them using GridSearchCV\n\nnet = NeuralNetClassifier(TPS05Classification, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping, TPS05CustomCallback])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.589878Z","iopub.execute_input":"2021-05-30T10:36:36.59036Z","iopub.status.idle":"2021-05-30T10:36:36.602896Z","shell.execute_reply.started":"2021-05-30T10:36:36.590319Z","shell.execute_reply":"2021-05-30T10:36:36.601942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SCIKIT-LEARN PIPELINE","metadata":{}},{"cell_type":"code","source":"steps = [('scaler', StandardScaler()), ('net', net)]\npipeline = Pipeline(steps)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.605707Z","iopub.execute_input":"2021-05-30T10:36:36.606172Z","iopub.status.idle":"2021-05-30T10:36:36.612111Z","shell.execute_reply.started":"2021-05-30T10:36:36.606134Z","shell.execute_reply":"2021-05-30T10:36:36.611196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LET'S DEFINE GRIDSEARCH PARAMETERS","metadata":{}},{"cell_type":"code","source":"grid_params = {\n    # For the first two params we used Callbacks \n    #'net__max_epochs':[20, 40], \n    #'net__lr': [0.001, 0.0001], \n    'net__module__dropout': [0.2, 0.3],\n    'net__optimizer': [optim.Adam, optim.RMSprop], \n    'net__module__block': [2, 3],\n    'net__module__nonlinear': [nn.ReLU(), nn.Softmax(dim = 1)], # we can play with network architecture as well \n    'net__module__batch_norm': [True, False] # BatchNormalization test\n} \n\ngrid_net = GridSearchCV(pipeline, grid_params, refit = True, cv = 3, scoring = 'neg_log_loss', verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:36:36.613832Z","iopub.execute_input":"2021-05-30T10:36:36.614203Z","iopub.status.idle":"2021-05-30T10:36:36.622344Z","shell.execute_reply.started":"2021-05-30T10:36:36.614167Z","shell.execute_reply":"2021-05-30T10:36:36.621543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LET'S TRAIN THE NETWORK AND FIND THE BEST HYPERPARAMETERS","metadata":{}},{"cell_type":"code","source":"result = grid_net.fit(X,y)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-30T10:36:36.623688Z","iopub.execute_input":"2021-05-30T10:36:36.624087Z","iopub.status.idle":"2021-05-30T10:47:28.883411Z","shell.execute_reply.started":"2021-05-30T10:36:36.624051Z","shell.execute_reply":"2021-05-30T10:47:28.879824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOW BEST PARAMETERS FOR CURRENT NN ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"print(grid_net.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.884627Z","iopub.status.idle":"2021-05-30T10:47:28.885168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOW MODEL RANKING (TOP 5)","metadata":{}},{"cell_type":"code","source":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\nreport(grid_net.cv_results_,5)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.886429Z","iopub.status.idle":"2021-05-30T10:47:28.887172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOW THE BEST ESTIMATOR CONFIGURATION","metadata":{}},{"cell_type":"code","source":"grid_net.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.888227Z","iopub.status.idle":"2021-05-30T10:47:28.888936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PLOT BEST ESTIMATOR LEARNING CURVES","metadata":{}},{"cell_type":"code","source":"epochs = [i for i in range(len(grid_net.best_estimator_[1].history))]\ntrain_loss = grid_net.best_estimator_[1].history[:,'train_loss']\nvalid_loss = grid_net.best_estimator_[1].history[:,'valid_loss']","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.88999Z","iopub.status.idle":"2021-05-30T10:47:28.890557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs,train_loss,'g-');\nplt.plot(epochs,valid_loss,'r-');\nplt.title('Training Loss Curves');\nplt.xlabel('Epochs');\nplt.ylabel('Mean Squared Error');\nplt.legend(['Train','Validation']);","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.89185Z","iopub.status.idle":"2021-05-30T10:47:28.892593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREDICT ","metadata":{}},{"cell_type":"code","source":"y_pred = grid_net.predict_proba(test)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.893776Z","iopub.status.idle":"2021-05-30T10:47:28.894515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ... AND SUBMIT","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(y_pred, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.895655Z","iopub.status.idle":"2021-05-30T10:47:28.896242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.897324Z","iopub.status.idle":"2021-05-30T10:47:28.898046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\npalette = itertools.cycle(sns.color_palette())\n\nplt.figure(figsize=(16, 8))\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    c = next(palette)\n    sns.histplot(predictions_df, x = f'Class_{i+1}', color=c)\nplt.suptitle(\"Class prediction distribution\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.899261Z","iopub.status.idle":"2021-05-30T10:47:28.8999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv(\"skorch_nn_tutorial_submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:47:28.901026Z","iopub.status.idle":"2021-05-30T10:47:28.901662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BONUS (for LB score lovers) - What if we blend NN score with my blender database .... ? :)","metadata":{}},{"cell_type":"code","source":"# GREAT kernel: https://www.kaggle.com/lazaro97/tps-may-stacking-blending-pseudolabelling \n\nblend_l1 = pd.read_csv(\"../input/tps-may-stacking-blending-pseudolabelling/sub_lb_0.09080355083449096_0.7.csv\")\n\n\noutput = predictions_df.copy()\noutput[\"Class_1\"] = (predictions_df.Class_1 * 0.3 + blend_l1.Class_1 * 0.7)\noutput[\"Class_2\"] = (predictions_df.Class_2 * 0.3 + blend_l1.Class_2 * 0.7)\noutput[\"Class_3\"] = (predictions_df.Class_3 * 0.3 + blend_l1.Class_3 * 0.7) \noutput[\"Class_4\"] = (predictions_df.Class_4 * 0.3 + blend_l1.Class_4 * 0.7) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(output, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']\npredictions_df.to_csv(\"TPS-05-skorch_blended_submission.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]}]}