{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# My path to finding for best features ....\n\n**This is attempt for finding TPS-05 solution and best feature for model building. I decided to create simple model using LGBM and then analize what features drive model for each particular class.**\n\nInteresting in my TPS-05 notebooks?\n- [Pytorch NN for tabular - step by step](https://www.kaggle.com/remekkinas/tps-5-pytorch-nn-for-tabular-step-by-step)\n- [CNN (2D Convolution) for solving TPS-05](https://www.kaggle.com/remekkinas/cnn-2d-convolution-for-solving-tps-05)\n- [Weighted training - XGB, RF, LR, ... SMOTE](https://www.kaggle.com/remekkinas/tps-5-weighted-training-xgb-rf-lr-smote)\n- [HydraNet!! ... Keras Stacked Ensemble ..](https://www.kaggle.com/remekkinas/tps-5-hydranet-keras-stacked-ensemble)\n","metadata":{}},{"cell_type":"markdown","source":"# Reference - [\"Interpretable Machine Learning, A Guide for Making Black Box Models Explainable.\"](https://christophm.github.io/interpretable-ml-book/) Christoph Molnar (updated 09.05.2021)\n","metadata":{}},{"cell_type":"markdown","source":"#### Shap in my opinion is absolutely one of the best tool you can use for model understanding and hacking (improving). In this competition I made many analysis using Shap which lead me to interesting solutions.  ","metadata":{}},{"cell_type":"code","source":"import shap\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom tqdm import tqdm\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LOAD TPS-04 DATA AND PREPROCESS (QUICK WAY)","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\", index_col = 'id')\ntrain = train[~train.drop('target', axis = 1).duplicated()]\n\nX = pd.DataFrame(train.drop(\"target\", axis = 1))\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM CROSS VALIDATED TRAINING LOOP\nLGBM is one of the most popular algorith used in TPS-05. Let's look how it predict. Let's look how it sees the TPS-05 data.","metadata":{}},{"cell_type":"code","source":" params = { \n        'objective': 'multiclass', \n        'num_class' : 4, \n        'metric': 'multi_logloss' \n    } ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 10\n    \n    model =  LGBMClassifier(**params)\n    \n    skf = StratifiedKFold(n_splits = n_splits, shuffle = True,  random_state = 0)\n    \n    for tr_index , val_index in tqdm(skf.split(X.values , y.values), total=skf.get_n_splits(), desc=\"k-fold\"):\n\n        x_train_o, x_val_o = X.iloc[tr_index] , X.iloc[val_index]\n        y_train_o, y_val_o = y.iloc[tr_index] , y.iloc[val_index]\n        \n        eval_set = [(x_val_o, y_val_o)]\n        \n        model.fit(x_train_o, y_train_o, eval_set = eval_set, early_stopping_rounds=100, verbose=False)\n\n        train_preds = model.predict(x_train_o)\n        train_rmse += mean_squared_error(y_train_o ,train_preds , squared = False)\n\n        val_preds = model.predict(x_val_o)\n        val_rmse += mean_squared_error(y_val_o , val_preds , squared = False)\n\n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n\n    print(f\"\\nAverage Training RMSE : {train_rmse / n_splits}\")\n    print(f\"Average Validation RMSE : {val_rmse / n_splits}\\n\")\n\n    test_preds /= n_splits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SHAP model explainer","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HOW FEATURES IMPACT MODEL","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see:\n- Feature_14 and feature_15 affect the model the most, but you can also see the balance between classes (feature_15 plays big role in class_2 prediction) - they are definitely important variables\n- The most important feature for class_0 is feature_25, class_1 -> feature_14, class_2 -> feature_15, class_3 -> feature_31 \n","metadata":{}},{"cell_type":"code","source":"selected_features = [\"feature_25\", \"feature_14\", \"feature_15\", \"feature_31\"]\n\nplt.figure(figsize=(20,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Looks like most of values is ZERO(!) This is why you can observe some patterns later during class prediction analysis.\n- This could be a potential problem with the model. Which will divide classes with respect to 0. And if there is a majority of them then ...\n- In my opinion they in this competition is feature modeling - not dataset imbalance, not super model building ...","metadata":{}},{"cell_type":"code","source":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets look inside TOP6 features .... ","metadata":{}},{"cell_type":"code","source":"selected_features = [\"feature_14\", \"feature_15\", \"feature_6\", \"feature_16\", \"feature_31\", \"feature_37\"]\n\nplt.figure(figsize=(15,10))\nc = 1\nfor feat in selected_features:\n    plt.subplot(2, 3, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The same situation we can see here - ZERO rulez! ZERO vs ........ REST!!!","metadata":{}},{"cell_type":"code","source":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CLASS 0 - FEATURES IMPORTANCE","metadata":{}},{"cell_type":"code","source":" shap.summary_plot(shap_values[0], test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = [\"feature_25\", \"feature_37\", \"feature_23\", \"feature_38\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n- there is no obvious patters here, this is why model has problem with class 0 prediction - it works almost randomly (I think but I could be wrong)\n- as you can see reds are on the left and right - but we can see that most \"high\" values are on right side of chart \n- almost all features are 0 balanced ... so 0 can drive model to Class 0 and .... rest of Class -> ZERO plays main role in this competition. \n- Feature 25 - we can see that \"medium\" feature value (purple) drives model to predict class 0 \n- Feature 37 - the highest value (red) then class 0 is predicted,\n- Feature 23 - high values drive model to predict class 0\n- there is almost no uniform distribution","metadata":{}},{"cell_type":"markdown","source":"## CLASS 1 - FEATURES IMPORTANCE","metadata":{}},{"cell_type":"code","source":" shap.summary_plot(shap_values[1], test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n- We see some interesting patters here - blue blobs\n- if most of features (till 20) are \"low\" they drives model to other class (not class 1, probably for class 3)\n- feature 14 and \"low\" values says that probaly there is no Class_1\n- feature 14->37 are uniform - \"higher\" values drive model to predict Class_1\n- feature_34 - \"higher\" values drive model to Class_0","metadata":{}},{"cell_type":"code","source":"selected_features = [\"feature_14\", \"feature_6\", \"feature_28\", \"feature_37\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CLASS 2 - FEATURES IMPORTANCE","metadata":{}},{"cell_type":"code","source":" shap.summary_plot(shap_values[2], test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n- We see some interesting patters here - blue blobs\n- The most important feature is 15 and 14. Here we can see obvious pattens. In feature_15 small values (probalby 0) drives model to Class 2. Feature_14 - high values drive to Class 2.\n- Feature_2 there is seen good separation - all positive values (blue blob is 0) highly influence model in Class_2 direction.\n- Almost in all features from list we can see that \"high\" values drives model to Class_2","metadata":{}},{"cell_type":"code","source":"selected_features = [\"feature_15\", \"feature_14\", \"feature_2\", \"feature_11\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CLASS 3 - FEATURES IMPORTANCE","metadata":{}},{"cell_type":"code","source":" shap.summary_plot(shap_values[3], test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n- We see some interesting patters here - blue blobs\n- From feature 31 to 23 ... we can see that small values drive model to Class_3. This is probably 0 (we shold see data in these feature).\n- The bigger number in features the less chance that Class 3 will be predicted.","metadata":{}},{"cell_type":"code","source":"selected_features = [\"feature_31\", \"feature_24\", \"feature_14\", \"feature_16\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCLUTIONS\n\nIn my opinion ....  focus points:\n- not model .... (low priority) -  same results you can achive using XGBoost, LightGBM, CatBoost (slight differences) ... and even poorly designed NN :) \n- not imbalance ..... (low priority) - weighted training is not a solution (see this notebook: https://www.kaggle.com/remekkinas/tps-5-weighted-training-xgb-rf-lr-smote ), sampling (uder/over) is not a solution \n\nFeature \n- feature engineering (HIGH priority) - dealing with 0 which drives models to biased prediction\n- finding better ML model to deal with sparsity ...\n\nOf course, you may have a different opinion on this. Conversations below are welcome.","metadata":{}}]}