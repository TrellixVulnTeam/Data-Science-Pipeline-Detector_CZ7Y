{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pytorch NN for tabular - step by step (day by day)\nI am not a Pytorch Grand Master. I just use it for solving problems. This notebook called Pytorch NN for tabular step by step (day by day) is my contribution to TPS-05 helping people to start building simple NN models using Pytorch. If you have any improvements to my code please let me know - I will add all improvements to final notebook.\n\nThis notebook intentionaly will be updated every day - step by step ... we have time ... lets eat an elephant slice by slice ....\n\n<div class=\"alert alert-info\">\n  <strong>People often ask a question - Keras/Tensorflow vs Pytorch?</strong>\n <div>Answer is simple - both. Many recent publications use both Keras and Pytorch. If you want to be flexible and understand how solutions work you should know both. This is why I encourage you start today ... and implement your first NN in Pytorch.</div>\n</div>\n\n\n\nHere you can find my funny (out of the box) tps-05 implementation in Keras: [CNN (2D Convolution) for solving TPS-05](https://www.kaggle.com/remekkinas/cnn-2d-convolution-for-solving-tps-05) So as you can see I use framewors interchangeably when it is more convenient for me.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n  <strong>Notebook scope and implementation schedule</strong>\n    <ul>\n        <li>Preparation - import modules, find device for torch, load TPS-05 data, create train and test dataset, create dataloader classes</li>\n        <li>Define feed forward NN using mModule, plot model</li>\n        <li>Define feed forward NN using Sequential, criterion and optimization</li>\n        <li>Build train and validation loop, metric functions</li>\n        <li>Plot training metrics (integrate with Neptune.ai)</li>\n        <li>Implementing callbacks</li>\n        <li>Hyperparameter tuning - learning rate using Scheduler</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Great tutorials I recommend:\n- [Deep Learning 60 min blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n- [Pytorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n- [Deep Learning (with PyTorch)](https://github.com/Atcold/pytorch-Deep-Learning)\n- [Deep learning for Pytorch](https://github.com/sgrvinod/Deep-Tutorials-for-PyTorch)\n- [Awesome Pytorch](https://github.com/bharathgs/Awesome-pytorch-list#tutorials-books--examples)\n- [Dive into Deep Learning](https://d2l.ai/index.html)\n- [Deep Learning with PyTorch Step-by-Step](https://github.com/dvgodoy/PyTorchStepByStep)\n- [Deep Learning with PyTorch](https://www.tomasbeuzen.com/deep-learning-with-pytorch/README.html)\n\nYoutube:\n- [PyTorch Tutorials - Complete Beginner Course](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)\n- [Pytorch tutorials](https://www.youtube.com/playlist?list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz)","metadata":{}},{"cell_type":"markdown","source":"Updated:\n- New book came out - \"Deep Learning with PyTorch Step-by-Step. A Beginner's Guide\" by Daniel Voigt Godoy\n\n![](https://d2sofvawe08yqg.cloudfront.net/pytorch/hero?1620637439)","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"**Day 1 (5.5.2021) - PREPARATION - import modules, find device for torch, load TPS-05 data, create train and test dataset, create dataloader classes**","metadata":{}},{"cell_type":"markdown","source":"# PREPARATION","metadata":{}},{"cell_type":"code","source":"!pip install torchviz -q","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:11.891898Z","iopub.execute_input":"2021-05-19T20:29:11.892256Z","iopub.status.idle":"2021-05-19T20:29:17.62246Z","shell.execute_reply.started":"2021-05-19T20:29:11.892221Z","shell.execute_reply":"2021-05-19T20:29:17.621373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have to prepare for this yourney .... import modules is e great idea .... :)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler  \n\n# Pytorch modules\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n\nfrom torchviz import make_dot, make_dot_from_trace\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:17.624282Z","iopub.execute_input":"2021-05-19T20:29:17.624627Z","iopub.status.idle":"2021-05-19T20:29:17.633296Z","shell.execute_reply.started":"2021-05-19T20:29:17.624587Z","shell.execute_reply":"2021-05-19T20:29:17.632476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\", index_col = 'id')\n\nTARGET = 'target'\nRANDOM_STATE = 2021","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:17.635408Z","iopub.execute_input":"2021-05-19T20:29:17.635833Z","iopub.status.idle":"2021-05-19T20:29:18.01802Z","shell.execute_reply.started":"2021-05-19T20:29:17.635795Z","shell.execute_reply":"2021-05-19T20:29:18.017011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Duplicates in dataset? This is noise ... kill them ....\n# I find it thanks @omarvivas: https://www.kaggle.com/c/tabular-playground-series-may-2021/discussion/236561\n\ntrain = train[~train.drop('target', axis = 1).duplicated()]\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.019575Z","iopub.execute_input":"2021-05-19T20:29:18.019927Z","iopub.status.idle":"2021-05-19T20:29:18.118698Z","shell.execute_reply.started":"2021-05-19T20:29:18.019887Z","shell.execute_reply":"2021-05-19T20:29:18.117688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.DataFrame(train.drop(\"target\", axis = 1))\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.120081Z","iopub.execute_input":"2021-05-19T20:29:18.120417Z","iopub.status.idle":"2021-05-19T20:29:18.166752Z","shell.execute_reply.started":"2021-05-19T20:29:18.120381Z","shell.execute_reply":"2021-05-19T20:29:18.165993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use stratify ... to ensure that we have the same class representation in each dataset\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state= RANDOM_STATE)\n\nsns.countplot(x = TARGET, data= y)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.168131Z","iopub.execute_input":"2021-05-19T20:29:18.168473Z","iopub.status.idle":"2021-05-19T20:29:18.876612Z","shell.execute_reply.started":"2021-05-19T20:29:18.168438Z","shell.execute_reply":"2021-05-19T20:29:18.875868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NN likes numbers from 0-1 .... so we scale our dataset \nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.879356Z","iopub.execute_input":"2021-05-19T20:29:18.879598Z","iopub.status.idle":"2021-05-19T20:29:18.925018Z","shell.execute_reply.started":"2021-05-19T20:29:18.879571Z","shell.execute_reply":"2021-05-19T20:29:18.924155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then we convert our dataset temporary to numpy .... we will use then torch data structure\nX_train, y_train = np.array(X_train, dtype= np.float32), y_train['target'].values \nX_valid, y_valid = np.array(X_valid, dtype= np.float32), y_valid['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.927708Z","iopub.execute_input":"2021-05-19T20:29:18.928074Z","iopub.status.idle":"2021-05-19T20:29:18.937652Z","shell.execute_reply.started":"2021-05-19T20:29:18.928023Z","shell.execute_reply":"2021-05-19T20:29:18.936689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PYTORCH FOR TABULAR (MUTLICLASS) - STEP BY STEP","metadata":{}},{"cell_type":"code","source":"# Here we will define all params for rest of notebook\n\nBATCH_SIZE = 64\nNUM_FEATURES = len(train.columns)-1\nNUM_CLASSES = 4\nNUM_EPOCHS = 100","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.939873Z","iopub.execute_input":"2021-05-19T20:29:18.940359Z","iopub.status.idle":"2021-05-19T20:29:18.946582Z","shell.execute_reply.started":"2021-05-19T20:29:18.94032Z","shell.execute_reply":"2021-05-19T20:29:18.945768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEVICE (CPU/GPU)","metadata":{}},{"cell_type":"code","source":"# Torch is like numpy (kind of data structure) but it is designed to work on GPU ... so we will catch device (mostly GPU) and load all data to speed up learning process\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# if GPU enabled then device = gpu (recommended)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.947741Z","iopub.execute_input":"2021-05-19T20:29:18.948096Z","iopub.status.idle":"2021-05-19T20:29:18.956039Z","shell.execute_reply.started":"2021-05-19T20:29:18.948041Z","shell.execute_reply":"2021-05-19T20:29:18.955235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATASET CLASSES","metadata":{}},{"cell_type":"code","source":"# We define very simple Datast class for TPS-05 (three methods are required)\n\nclass TPS05Dataset(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n# As you can see we define torch arrays - then we will put them into device\ntrain_dataset = TPS05Dataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\nvalid_dataset = TPS05Dataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid).long())","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.957385Z","iopub.execute_input":"2021-05-19T20:29:18.957824Z","iopub.status.idle":"2021-05-19T20:29:18.969876Z","shell.execute_reply.started":"2021-05-19T20:29:18.957695Z","shell.execute_reply":"2021-05-19T20:29:18.969083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA LOADERS","metadata":{}},{"cell_type":"code","source":"# Data loaders \ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE)\n\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.971015Z","iopub.execute_input":"2021-05-19T20:29:18.971588Z","iopub.status.idle":"2021-05-19T20:29:18.980401Z","shell.execute_reply.started":"2021-05-19T20:29:18.971551Z","shell.execute_reply":"2021-05-19T20:29:18.979576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can test it if you want - take first batch (size = 16) and print size\n\ndataiter = iter(train_loader)\ntrain_features, train_labels = dataiter.next()\nprint('Batch #1')\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nprint(\"First row from batch #1\")\nprint(train_features[1])\n\n# take next batch (another way)\ntrain_features, train_labels = dataiter.next()\nprint('\\nBatch #2')\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nprint(\"First row from batch #2\")\nprint(train_features[1])","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.982992Z","iopub.execute_input":"2021-05-19T20:29:18.98328Z","iopub.status.idle":"2021-05-19T20:29:18.996855Z","shell.execute_reply.started":"2021-05-19T20:29:18.98325Z","shell.execute_reply":"2021-05-19T20:29:18.995583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"**Day 2 (6.5.2021) - DEFINE FEED FORWARD NN MODEL (using Module)**\n\n\n## DEFINE FEED FORWARD NN MODEL\n","metadata":{}},{"cell_type":"code","source":"# Lets define model from: https://www.kaggle.com/subinium/tps-may-deeplearning-pipeline-for-beginner as a benchamrk model.\n\n#model = Sequential([\n#        Dense(512, input_dim=num_features, activation='relu'),\n#        BatchNormalization(),\n#        Dropout(0.3),\n    \n#        Dense(256, activation='relu'),\n#        BatchNormalization(),\n#        Dropout(0.2),\n    \n#        Dense(128, activation='relu'),\n#        BatchNormalization(),\n#        Dropout(0.2),\n    \n#        Dense(num_classes, activation='softmax')\n#    ]) \n\n\n# There is many way you can create models in Pytorch using:\n# - Module,\n# - Sequential, \n# - ModuleList,\n# - ModuleDict\n\n# This is starter so we use first way using Module\n\nclass TPS05ClassificationModule(nn.Module):\n    def __init__(self, num_feature, num_class):\n        super(TPS05ClassificationModule, self).__init__()\n        \n        self.layer_1 = nn.Linear(num_feature, 512)\n        self.layer_2 = nn.Linear(512, 256)\n        self.layer_3 = nn.Linear(256, 128)\n        self.layer_out = nn.Linear(128, num_class)\n        \n        torch.nn.init.xavier_normal_(self.layer_1.weight)\n        torch.nn.init.xavier_normal_(self.layer_2.weight)\n        torch.nn.init.xavier_normal_(self.layer_3.weight)\n        torch.nn.init.xavier_normal_(self.layer_out.weight)\n        \n        self.dropout_1 = nn.Dropout(p=0.3)\n        self.dropout_2 = nn.Dropout(p=0.2)\n        \n        self.batchnorm_1 = nn.BatchNorm1d(512)\n        self.batchnorm_2 = nn.BatchNorm1d(256)\n        self.batchnorm_3 = nn.BatchNorm1d(128)\n        \n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim = 1)\n    \n    def forward(self, x):\n        x = self.layer_1(x)\n       #x = self.batchnorm_1(x)\n        x = F.relu(x)   # Second one using torch.nn.functional\n        x = self.dropout_1(x)\n        \n        x = self.layer_2(x)\n       #x = self.batchnorm_2(x)\n        x = self.softmax(x)\n        x = self.dropout_2(x)\n        \n        x = self.layer_3(x)\n        #x = self.batchnorm_3(x)\n        x = self.softmax(x)\n        x = self.dropout_2(x)\n        \n        x = self.layer_out(x)\n        return x\n    \n    \n# Day 3 (7.5.2021)- using Sequential\n# And ... what do you thing ... is it better? :)\n\ndef linear_block(in_features, out_features, p_drop, *args, **kwargs):\n    return nn.Sequential(\n        nn.Linear(in_features, out_features),\n        #nn.BatchNorm1d(out_features),\n        nn.ReLU(),\n        nn.Dropout(p = p_drop)\n    )\n\nclass TPS05ClassificationSeq(nn.Module):\n    def __init__(self, num_feature, num_class):\n        super(TPS05ClassificationSeq, self).__init__()\n        \n        self.linear = nn.Sequential(\n            linear_block(num_feature, 100, 0.3),\n            linear_block(100, 250, 0.3),\n            linear_block(250, 128, 0.3),\n        )\n        \n        self.out = nn.Sequential(\n            nn.Linear(128, num_class)\n        )\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return self.out(x)\n\n# Day 4 (8.5.2021) - using Dynamic Sequential \nclass TPS05ClassificationDynSeq(nn.Module):\n    def __init__(self, num_feature, num_class):\n        super(TPS05ClassificationDynSeq, self).__init__()\n        \n        self.lin_sizes = [num_feature, 64, 32, 128]\n        self.b_norm = [0.3, 0.2, 0.2]\n        \n        lin_blocks = [linear_block(in_f, out_f, b_in) \n                      for in_f, out_f , b_in in zip(self.lin_sizes, self.lin_sizes[1:], self.b_norm)]\n        \n        self.linear = nn.Sequential(*lin_blocks)\n        \n        self.out = nn.Sequential(\n            nn.Linear(128, num_class)\n        )\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return self.out(x)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:18.998401Z","iopub.execute_input":"2021-05-19T20:29:18.998734Z","iopub.status.idle":"2021-05-19T20:29:19.015084Z","shell.execute_reply.started":"2021-05-19T20:29:18.9987Z","shell.execute_reply":"2021-05-19T20:29:19.014257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model using Module \nmodelMod = TPS05ClassificationModule(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n# Then pushi it to device (CPU/GPU)\nmodelMod.to(device)\n\n# model.eval() is switch off for some specific layers/parts of the model (Dropouts Layers, BatchNorm Layers etc.) \nmodelMod.eval()\n\n# Whenever you want you can print model \nprint(modelMod)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.017517Z","iopub.execute_input":"2021-05-19T20:29:19.017834Z","iopub.status.idle":"2021-05-19T20:29:19.035677Z","shell.execute_reply.started":"2021-05-19T20:29:19.017795Z","shell.execute_reply":"2021-05-19T20:29:19.034746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model using Module \nmodelSeq = TPS05ClassificationSeq(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n# Then pushi it to device (CPU/GPU)\nmodelSeq.to(device)\n\n# model.eval() is switch off for some specific layers/parts of the model (Dropouts Layers, BatchNorm Layers etc.) \nmodelSeq.eval()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.03698Z","iopub.execute_input":"2021-05-19T20:29:19.037338Z","iopub.status.idle":"2021-05-19T20:29:19.048734Z","shell.execute_reply.started":"2021-05-19T20:29:19.037311Z","shell.execute_reply":"2021-05-19T20:29:19.047516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model using Module \nmodelDynSeq = TPS05ClassificationDynSeq(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n# Then pushi it to device (CPU/GPU)\nmodelDynSeq.to(device)\n\n# model.eval() is switch off for some specific layers/parts of the model (Dropouts Layers, BatchNorm Layers etc.) \nmodelDynSeq.eval()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.049882Z","iopub.execute_input":"2021-05-19T20:29:19.050479Z","iopub.status.idle":"2021-05-19T20:29:19.059236Z","shell.execute_reply.started":"2021-05-19T20:29:19.050436Z","shell.execute_reply":"2021-05-19T20:29:19.058252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VISUALIZE MODELS","metadata":{}},{"cell_type":"markdown","source":"### CREATING BY MODULE","metadata":{}},{"cell_type":"code","source":"# Generate random array (torch)\nx = torch.randn(1, NUM_FEATURES).to(device)\n\n# Pass through model\ny = modelMod(x)\n\n# Visualize\nmake_dot(y.mean(), params=dict(modelMod.named_parameters()))","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.061025Z","iopub.execute_input":"2021-05-19T20:29:19.061324Z","iopub.status.idle":"2021-05-19T20:29:19.142731Z","shell.execute_reply.started":"2021-05-19T20:29:19.061295Z","shell.execute_reply":"2021-05-19T20:29:19.141854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CREATING BY SEQUENTIAL","metadata":{}},{"cell_type":"code","source":"y = modelSeq(x)\nmake_dot(y.mean(), params=dict(modelSeq.named_parameters()))","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.144242Z","iopub.execute_input":"2021-05-19T20:29:19.144589Z","iopub.status.idle":"2021-05-19T20:29:19.221908Z","shell.execute_reply.started":"2021-05-19T20:29:19.14455Z","shell.execute_reply":"2021-05-19T20:29:19.221086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CREATING BY DYNAMIC SEQUENTIAL","metadata":{}},{"cell_type":"code","source":"y = modelDynSeq(x)\nmake_dot(y.mean(), params=dict(modelDynSeq.named_parameters()))","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.22346Z","iopub.execute_input":"2021-05-19T20:29:19.22383Z","iopub.status.idle":"2021-05-19T20:29:19.300084Z","shell.execute_reply.started":"2021-05-19T20:29:19.223791Z","shell.execute_reply":"2021-05-19T20:29:19.299051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Day 3 (7.5.2021) - DEFINE CRITERION AND OPTIMIZER**\n## DEFINE CRITERION","metadata":{}},{"cell_type":"code","source":"# Loss function -> CrossEntropy \n# This criterion combines LogSoftmax and NLLLoss in one single class.\n# It is useful when training a classification problem with C classes. \n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.301683Z","iopub.execute_input":"2021-05-19T20:29:19.302047Z","iopub.status.idle":"2021-05-19T20:29:19.306678Z","shell.execute_reply.started":"2021-05-19T20:29:19.302007Z","shell.execute_reply":"2021-05-19T20:29:19.30575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEFINE OPTIMIZER","metadata":{}},{"cell_type":"code","source":"# Lets choose one model (as you remember we created three models using different ways)\nmodel = modelSeq","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.308029Z","iopub.execute_input":"2021-05-19T20:29:19.308621Z","iopub.status.idle":"2021-05-19T20:29:19.316584Z","shell.execute_reply.started":"2021-05-19T20:29:19.308583Z","shell.execute_reply":"2021-05-19T20:29:19.315725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# During neural network training, its weights are randomly initialized initially.\n# Then they are updated in each epoch in a manner such that they increase the overall accuracy of the network.\n# This is actually a problem of optimization where the goal is to optimize the loss function and get the ideal weights. \n# And the method used for optimization is called Optimizer.\n\n# Define learning rate -> then (in day 8 we will find it during hyperparameter optimization)\nLEARNING_RATE = 0.001\noptimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n\n# 2021.05.17\n# This is a part of NN optimization \nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience = 3)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.321579Z","iopub.execute_input":"2021-05-19T20:29:19.321878Z","iopub.status.idle":"2021-05-19T20:29:19.327297Z","shell.execute_reply.started":"2021-05-19T20:29:19.321852Z","shell.execute_reply":"2021-05-19T20:29:19.326472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Day 4 (8.5.2021) - DEFINE TRAIN AND VALIDATION LOOP, METRIC FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"## DEFINE TRAIN AND VALIDATION LOOP, METRIC FUNCTIONS","metadata":{}},{"cell_type":"code","source":"accuracy_stat = {'train': [],\"validation\": []}\nloss_stat = {'train': [], \"validation\": [] }\n\ndef acc_calc(y_pred, y_test):\n    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n    \n    correct_pred = (y_pred_tags == y_test).float()\n    acc = correct_pred.sum() / len(correct_pred)\n    \n    acc = torch.round(acc * 100)\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.329086Z","iopub.execute_input":"2021-05-19T20:29:19.329782Z","iopub.status.idle":"2021-05-19T20:29:19.338282Z","shell.execute_reply.started":"2021-05-19T20:29:19.329745Z","shell.execute_reply":"2021-05-19T20:29:19.337433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEFINE SIMPLE CALLBACK ","metadata":{}},{"cell_type":"code","source":"# Let's create simple Pytorch Callback \n\nclass EarlyStoppingCallback:   \n    def __init__(self, min_delta = 0.1, patience = 5):\n        \n        self.min_delta = min_delta\n        self.patience = patience\n        self.best_epoch_score = 0\n        \n        self.attempt = 0\n        self.best_score = None\n        self.stop_training = False\n        \n        \n    def __call__(self, validation_loss):\n\n        self.epoch_score = validation_loss\n\n        if self.best_epoch_score == 0:\n            self.best_epoch_score = self.epoch_score\n        elif self.epoch_score > self.best_epoch_score - self.min_delta:\n            self.attempt += 1\n            print(f'Message from callback (Early Stopping) counter: {self.attempt}/{self.patience}')\n            if self.attempt >= self.patience:\n                self.stop_training = True\n        else:\n            self.best_epoch_score = self.epoch_score\n            self.attempt = 0","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.33968Z","iopub.execute_input":"2021-05-19T20:29:19.34009Z","iopub.status.idle":"2021-05-19T20:29:19.347867Z","shell.execute_reply.started":"2021-05-19T20:29:19.340042Z","shell.execute_reply":"2021-05-19T20:29:19.347087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# This is training and validation loop\n# for each epoch\ndef train_nn():\n    for progress in tqdm(range(1, NUM_EPOCHS+1)):\n\n        train_epoch_loss = 0\n        train_epoch_acc = 0\n\n        model.train()\n\n        # We loop over training dataset using batches (we use DataLoader to load data with batches)\n        for X_train_batch, y_train_batch in train_loader:\n            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n\n            # Clear gradients\n            optimizer.zero_grad()\n\n            # Forward pass ->>>>\n            y_train_pred = model(X_train_batch)\n\n            # Find Loss and backpropagation of gradients\n            train_loss = criterion(y_train_pred, y_train_batch)\n            train_acc = acc_calc(y_train_pred, y_train_batch)\n\n            # backward <------    \n            train_loss.backward()\n\n            # Update the parameters (weights and biases)\n            optimizer.step()\n\n            train_epoch_loss += train_loss.item()\n            train_epoch_acc += train_acc.item()\n\n\n        #  Then we validate our model - concept is the same\n        with torch.no_grad():\n\n            val_epoch_loss = 0\n            val_epoch_acc = 0\n\n            model.eval()\n            for X_val_batch, y_val_batch in valid_loader:\n                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n\n                y_val_pred = model(X_val_batch)\n\n                val_loss = criterion(y_val_pred, y_val_batch)\n                val_acc = acc_calc(y_val_pred, y_val_batch)\n\n                val_epoch_loss += val_loss.item()\n                val_epoch_acc += val_acc.item()\n\n        # end of validation loop\n        early_stopping_callback(val_epoch_loss/len(valid_loader))\n        if early_stopping_callback.stop_training:\n            print(f'Training stopped -> Early Stopping Callback : validation_loss: {val_epoch_loss/len(valid_loader)}')\n            break\n\n        loss_stat['train'].append(train_epoch_loss/len(train_loader))\n        loss_stat['validation'].append(val_epoch_loss/len(valid_loader))\n        accuracy_stat['train'].append(train_epoch_acc/len(train_loader))\n        accuracy_stat['validation'].append(val_epoch_acc/len(valid_loader))                           \n        \n        \n        # 2021.05.17 \n        # This is a part of NN optimization\n        clr = optimizer.param_groups[0]['lr']        \n        scheduler.step(val_epoch_acc/len(valid_loader))\n\n        print(f'Epoch { progress + 0:03}: Loss: [Train: {train_epoch_loss/len(train_loader):.5f} | Validation: {val_epoch_loss/len(valid_loader):.5f} ] Accuracy: [Train: {train_epoch_acc/len(train_loader):.3f} | Validation: {val_epoch_acc/len(valid_loader):.3f}] LR: {clr}')","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.349282Z","iopub.execute_input":"2021-05-19T20:29:19.349681Z","iopub.status.idle":"2021-05-19T20:29:19.36283Z","shell.execute_reply.started":"2021-05-19T20:29:19.349638Z","shell.execute_reply":"2021-05-19T20:29:19.361697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I created function for training - it will be more flexible during NN tuning \nearly_stopping_callback = EarlyStoppingCallback(0.001, 5)\n\ntrain_nn()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:29:19.364119Z","iopub.execute_input":"2021-05-19T20:29:19.364442Z","iopub.status.idle":"2021-05-19T20:32:28.466252Z","shell.execute_reply.started":"2021-05-19T20:29:19.364409Z","shell.execute_reply":"2021-05-19T20:32:28.46526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Day 5 (9.5.2021) - PLOT TRAINING METRICS (integrate with Neptune.ai)**","metadata":{}},{"cell_type":"markdown","source":"## PLOT TRAINING METRICS","metadata":{}},{"cell_type":"code","source":"# First define DataFrames with our data from training\n\ndf_train_va = pd.DataFrame.from_dict(accuracy_stat).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\ndf_train_vl = pd.DataFrame.from_dict(loss_stat).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.467571Z","iopub.execute_input":"2021-05-19T20:32:28.467921Z","iopub.status.idle":"2021-05-19T20:32:28.484092Z","shell.execute_reply.started":"2021-05-19T20:32:28.467882Z","shell.execute_reply":"2021-05-19T20:32:28.483301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then plot two charts for Train/Val \n#   - Accuracy per epoch\n#   - Loss\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n\nsns.lineplot(data = df_train_va, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train - Validation Accuracy/Epoch')\nsns.lineplot(data = df_train_vl, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train - Validation Loss/Epoch')","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.485359Z","iopub.execute_input":"2021-05-19T20:32:28.485757Z","iopub.status.idle":"2021-05-19T20:32:28.84034Z","shell.execute_reply.started":"2021-05-19T20:32:28.485718Z","shell.execute_reply":"2021-05-19T20:32:28.839375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREDICT AND SUBMIT\nTime to predict and submit our first NN TPS-05 predictions (!)","metadata":{}},{"cell_type":"code","source":"# First we have to forward pass our test data. To do this we have to transform it and send to device (CPU/GPU) \ntensor_preds = model(torch.from_numpy(scaler.transform(test)).float().to(device))\n\n# As we can see we got Tensor (!). This is output from our last layer. This is not probabilities .....\n# In next days I will show you how to do it in a much more elegant way :) \n# We change Network class in two ways (change in architecture definition and one more method for predicion)\n\n# We look into first prediction\ntensor_preds[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.841722Z","iopub.execute_input":"2021-05-19T20:32:28.842046Z","iopub.status.idle":"2021-05-19T20:32:28.873208Z","shell.execute_reply.started":"2021-05-19T20:32:28.84201Z","shell.execute_reply":"2021-05-19T20:32:28.872246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now it is time to convert our las layer tensor output to probablilities, send in to CPU and convert to Numpy\n\nnn_preds = torch.nn.functional.softmax(tensor_preds, dim=1).cpu().detach().numpy() \nnn_preds[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.874479Z","iopub.execute_input":"2021-05-19T20:32:28.874819Z","iopub.status.idle":"2021-05-19T20:32:28.882474Z","shell.execute_reply.started":"2021-05-19T20:32:28.874784Z","shell.execute_reply":"2021-05-19T20:32:28.88137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(nn_preds, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.883975Z","iopub.execute_input":"2021-05-19T20:32:28.884388Z","iopub.status.idle":"2021-05-19T20:32:28.916946Z","shell.execute_reply.started":"2021-05-19T20:32:28.884332Z","shell.execute_reply":"2021-05-19T20:32:28.916235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets look on first predictions\npredictions_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.918118Z","iopub.execute_input":"2021-05-19T20:32:28.91844Z","iopub.status.idle":"2021-05-19T20:32:28.928298Z","shell.execute_reply.started":"2021-05-19T20:32:28.918404Z","shell.execute_reply":"2021-05-19T20:32:28.927453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets look on submission predition and distribution\n\npredictions_df.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.929497Z","iopub.execute_input":"2021-05-19T20:32:28.930079Z","iopub.status.idle":"2021-05-19T20:32:28.9768Z","shell.execute_reply.started":"2021-05-19T20:32:28.930029Z","shell.execute_reply":"2021-05-19T20:32:28.976001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submit\n\npredictions_df.to_csv(\"pytorch_nn_tutorial_submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T20:32:28.978071Z","iopub.execute_input":"2021-05-19T20:32:28.978472Z","iopub.status.idle":"2021-05-19T20:32:29.337157Z","shell.execute_reply.started":"2021-05-19T20:32:28.978436Z","shell.execute_reply":"2021-05-19T20:32:29.33625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 08.05.2021 - First submission is 1.10542. It is ok but we will make it better in next days. I am sure this will be the best NN submission for TPS-05. ","metadata":{}},{"cell_type":"markdown","source":"# NN OPIMIZATION\n\nWhat we can do to optimize NN? This is a very broad topic for several notebooks. Before we start optimizing the network (here I will show only 2-3 selected elements) let's think what we can do:\n\n- Configure Nodes and Layers\n- Optimize Gradient algorithm \n- Optimize Batch Size\n- Optimize Loss Function\n- Configure Speed of Learning\n- Data preparation\n- Vanishing Gradient / Gradient Clipping / Batch Normalization / Dropout\n- Transfer Learning\n- Regularization\n- ...\n","metadata":{}},{"cell_type":"markdown","source":"In this tutotial I will show:\n1. Configure Speed of Learning \n2. Configure Nodes and Layers","metadata":{}},{"cell_type":"markdown","source":"17.05.2021\n## Configure Speed of Learning\n\nIn order to implement Learning Rate we can use various scheduler in optim library in PyTorch (https://pytorch.org/docs/stable/optim.html):\n\n* LambdaLR()\n* MultiplicativeLR()\n* StepLR()\n* MultiStepLR()\n* ExponentialLR()\n* CosineAnnealingLR()\n* ReduceLROnPlateau()\n* CyclicLR()\n* OneCycleLR()\n\nIn our tutorial we use ReduceLROnPlateau as example. So out notebook requires two changes:\n\n**1. Define scheduler**\n\n<code>from torch.optim.lr_scheduler import ReduceLROnPlateau\noptimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience = 5) </code>\n\n**2. Change training loop**\n\n<code>clr = optimizer.param_groups[0]['lr']        \nscheduler.step(val_epoch_acc/len(valid_loader))</code>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n  <strong>All changes were introduced in notebook - just play with parameters and check how learning looks like</strong> \n</div>","metadata":{}},{"cell_type":"markdown","source":"TO BE CONTINUED ... ","metadata":{}}]}