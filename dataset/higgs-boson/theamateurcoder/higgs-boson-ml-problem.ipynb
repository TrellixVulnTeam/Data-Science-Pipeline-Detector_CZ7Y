{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path of the input files - ../input/higgs-boson/training.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Libraries\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = pd.read_csv('/kaggle/input/higgs-boson/training.zip', delimiter=',')\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/higgs-boson/test.zip')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = test.iloc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the size of the data\n\nprint(training.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the domain knowledge we can reduce the no. of columns. We can use coorelation matrix or heatmaps to see wqhich are the most relevant columns\nthis is a classification problem. test data is short of 2 columns - weight and label\ncalculating weight might be a regression problem and calculating the class is classification problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the data and see if it is Classification problem or Regression.\n0. Statistics of data, size of rows, columns, head, tail, describe, plotting, checking column.values.sum()\n1. Dealing with Outliers\n2. Dealing with Missing Data\n3. Handling Categorical Data\n4. Scaling Data\n5. Handling Imbalance\n6. Feature Selection:\n   1. Univariate Selection\n7. Dimensionality Reduction\n   PCA\n8. Model Selection\n9. Evaluation Techniques\n10. Ensemble Learning\n     Hyper parameter optimization\n     Grid CV\n11. Cross Fold validation\n\n12. Bagging\n    Boosting - AdaBoost, XGBoost\n13. Learning Curves - Check overfitting and underfitting\n14. Check deep learning models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA\n\nprint(type(training))\nprint(training.head())\nprint(training.tail())\nprint(training.shape)\nprint(training.size)\nprint(training.columns)\nprint(training.describe())\nprint(training.info)\nprint(training.dtypes)\nprint(training.index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Explore the target\ndflabel = training['Label'].rank\nprint(training['Label'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Check for missing values\nprint(training.isnull().sum())\n\n# There are no missing values, thus no need of Imputing the missing values\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the target, which is categorical in nature\nle = preprocessing.LabelEncoder()\ntraining['Label'] = le.fit_transform(training['Label'])\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normal Plot between 2 axis\n\nplt.figure()\nplt.title(\"Plot between event id and label\")\nplt.xlabel('EventId')\nplt.ylabel('Label')\n#plt.ylim() -- here you define the range\nplt.plot(training['EventId'], training['Label'])\n\n# Scatter Plot to see the labels scattered over all the EventIds\n\n# Heatmap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntraining = training.drop('Weight', axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = training.loc[:,training.columns != 'Label']\nY = training.loc[:,'Label']\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the training data into training and validation \ntrainX, valX, trainY, valY = model_selection.train_test_split(X, Y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection\n# bestfeatures = SelectKBest(score_func=chi2, k=10)\n# fit = bestfeatures.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Selection\nprint(\"Using Naive Bayes\")\n\nnBayes = GaussianNB()\nnBayes = nBayes.fit(trainX, trainY)\naccuracy = nBayes.score(valX, valY)\nprint(accuracy)\nresults = nBayes.predict(test)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate Decision Tree model\ndecTree = DecisionTreeClassifier()\ndecTree = decTree.fit(trainX, trainY)\naccuracy = decTree.score(valX, valY)\nprint (\"Decision Tree Accuracy \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \n# Evaluate SVM model\nsvc = SVC(gamma='auto')\nsvc = svc.fit(trainX, trainY)\naccuracy = svc.score(valX, valY)\nprint (\"SVM Accuracy \", accuracy)    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allResults = []\n    \nfor pVal in range(1, 4):\n    for kValue in range(1, 100):\n\n        knn = neighbors.KNeighborsClassifier(n_neighbors=kValue, metric=\"minkowski\", p=pVal)\n        knn = knn.fit(trainX, trainY)\n        accuracy = knn.score(valX, valY)\n        allResults.append(accuracy)\n\n    plt.plot(allResults)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultSeries = pd.Series(data = results, name = 'Label', dtype='int64')\n    \ndf = pd.DataFrame({\"EventId\":eventIdSeries, \"Survived\":resultSeries})\n\ndf.to_csv(\"kaggle_submission.csv\", index=False, header=True)\n    \n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}