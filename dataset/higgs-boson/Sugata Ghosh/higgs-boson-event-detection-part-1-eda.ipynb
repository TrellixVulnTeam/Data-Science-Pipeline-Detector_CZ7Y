{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Higgs Boson Event Detection </center></h1>\n<h2><center> Part 1. Exploratory Data Analysis </center></h2>\n<h2><center> Sugata Ghosh </center></h2>","metadata":{}},{"cell_type":"markdown","source":"### Contents\n\n- [Introduction](#1.-Introduction)\n- [Basic Data Exploration](#2.-Basic-Data-Exploration)\n- [Univariate Analysis](#3.-Univariate-Analysis)\n- [Multivariate Analysis](#4.-Multivariate-Analysis)\n- [Acknowledgements](#Acknowledgements)\n- [References](#References)","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n# File system manangement\nimport time, psutil, os\n\n# Progress bar for loops\n!pip install tqdm\nfrom tqdm import tqdm\n\n# Mathematical functions\nimport math\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as mpatches\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d.axes3d import get_test_data\n\nimport seaborn as sns\nsns.set_theme()\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:33.94418Z","iopub.execute_input":"2022-05-29T16:27:33.944976Z","iopub.status.idle":"2022-05-29T16:27:47.283215Z","shell.execute_reply.started":"2022-05-29T16:27:33.944853Z","shell.execute_reply":"2022-05-29T16:27:47.282158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runtime and memory usage\nstart = time.time()\nprocess = psutil.Process(os.getpid())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:47.285072Z","iopub.execute_input":"2022-05-29T16:27:47.285699Z","iopub.status.idle":"2022-05-29T16:27:47.291206Z","shell.execute_reply.started":"2022-05-29T16:27:47.285652Z","shell.execute_reply":"2022-05-29T16:27:47.290344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Introduction\n\n- [Backstory](#1.1.-Backstory)\n- [LHC at Work](#1.2.-LHC-at-Work)\n- [Enter ML](#1.3.-Enter-ML)\n- [Data](#1.4.-Data)\n- [Project Objective](#1.5.-Project-Objective)\n- [Evaluation Metric](#1.6.-Evaluation-Metric)","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Backstory","metadata":{}},{"cell_type":"markdown","source":"**Particle accelerators.** To probe into the basic questions on how matter, space and time work and how they are structured, physicists focus on the simplest interactions (for example, collision of [subatomic particles](https://en.wikipedia.org/wiki/Subatomic_particle)) at very high energy. [Particle accelerators](https://en.wikipedia.org/wiki/Particle_accelerator) enable physicists to explore the fundamental nature of matter by observing subatomic particles produced by high-energy collisions of [particle beams](https://en.wikipedia.org/wiki/Particle_beam). The experimental measurements from these collisions inevitably lack precision, which is where [machine learning](https://en.wikipedia.org/wiki/Machine_learning) (ML) comes into picture. The research community typically relies on standardized machine learning software packages for the analysis of the data obtained from such experiments and spends a huge amount of effort towards improving statistical power by extracting features of significance, derived from the raw measurements.\n\n**Higgs boson.** The [Higgs boson](https://en.wikipedia.org/wiki/Higgs_boson) [particle](https://en.wikipedia.org/wiki/Elementary_particle), also called the *God particle* in mainstream media, is the final ingredient of the [standard model](https://en.wikipedia.org/wiki/Standard_Model) of [particle physics](https://en.wikipedia.org/wiki/Particle_physics), which sets the rules for the subatomic particles and forces. The [elementary particles](https://en.wikipedia.org/wiki/Elementary_particle) are supposed to be massless at very high energies, but some of them can acquire mass at low-energies. The mechanism of this acquiring remained an enigma in theoretical physics for a long time. In $1964$, [Peter Higgs](https://en.wikipedia.org/wiki/Peter_Higgs) and others proposed a [mechanism](https://en.wikipedia.org/wiki/Higgs_mechanism) that theoretically explains the [origin of mass of elementary particles](https://en.wikipedia.org/wiki/Mass_generation). The mechanism involves a *field*, commonly known as [Higgs field](https://en.wikipedia.org/wiki/Higgs_mechanism#Structure_of_the_Higgs_field), that the paricles can interact with to gain mass. The more a particle interacts with it, the heavier it is. Some particles, like [photon](https://en.wikipedia.org/wiki/Photon), do not interact with this field at all and remain massless. The Higgs boson particle is the associated particle of the Higgs field (all fundamental fields have one). It is essentially the physical manifestation of the Higgs field, which gives mass to other particles. The detection of this elusive particle waited almost half a century since its theorization!\n\n**The discovery.** On 4th July 2012, the [ATLAS](https://home.cern/science/experiments/atlas) and [CMS](https://home.cern/science/experiments/cms) experiments at [CERN](https://en.wikipedia.org/wiki/CERN)'s [Large Hadron Collider](https://en.wikipedia.org/wiki/Large_Hadron_Collider) (LHC) announced that both of them had observed a new particle in the mass region around 125 GeV. This particle is consistent with the theorized Higgs boson. This experimental confirmation earned [François Englert](https://en.wikipedia.org/wiki/Fran%C3%A7ois_Englert) and Peter Higgs [The Nobel Prize in Physics 2013](https://www.nobelprize.org/prizes/physics/2013/summary/)\n> \"for the theoretical discovery of a mechanism that contributes to our understanding of the origin of mass of subatomic particles, and which recently was confirmed through the discovery of the predicted fundamental particle, by the ATLAS and CMS experiments at CERN's Large Hadron Collider.\"\n\n**Giving mass to fermions.** There are many different processes through which the Higgs boson can decay and produce other particles. In physics, the possible transformations a particle can undergo as it decays are referred to as [channels](https://atlas.cern/glossary/decay-channel). The Higgs boson has been observed first to decay in three distinct decay channels, all of which are [boson](https://en.wikipedia.org/wiki/Boson) pairs. To establish that the Higgs field provides the interaction which gives mass to the fundamental [fermions](https://en.wikipedia.org/wiki/Fermion) (particles which follow the [Fermi-Dirac statistics](https://en.wikipedia.org/wiki/Fermi%E2%80%93Dirac_statistics), contrary to the bosons which follow the [Bose-Einstein statistics](https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_statistics)) as well, it has to be demonstrated that the Higgs boson can decay into fermion pairs through direct [decay](https://en.wikipedia.org/wiki/Particle_decay) modes. Subsequently, to seek evidence on the decay of Higgs boson into fermion pairs (such as [tau leptons](https://simple.wikipedia.org/wiki/Tau_lepton) $(\\tau)$ or [b-quarks](https://en.wikipedia.org/wiki/Bottom_quark)) and to precisely measure their characteristics became one of the important lines of enquiry. Among the available modes, the most promising is the decay to a pair of tau leptons, which balances a modest branching ratio with manageable backgrounds. The first evidence of $h \\to \\tau^+\\tau^-$ decays [was recently reported](https://cds.cern.ch/record/1632191), based on the full set of proton–proton collision data recorded by the ATLAS experiment at the LHC during $2011$-$2012$. Despite the consistency of the data with $h \\to \\tau^+\\tau^-$ decays, it could not be ensured that the statistical power exceeds the $5\\sigma$ threshold, which is the required standard for claims of discovery in high-energy physics community.","metadata":{}},{"cell_type":"markdown","source":"<figure>\n    <img src = \"https://raw.githubusercontent.com/sugatagh/Higgs-Boson-Event-Detection/main/Image/atlas_experiment.png\" alt = \"Higgs into fermions: Evidence of the Higgs boson decaying to fermions\" width = \"600\">\n    <figcaption> Fig 1. Higgs into fermions: Evidence of the Higgs boson decaying to fermions (image credit: CERN) </figcaption>\n</figure>","metadata":{}},{"cell_type":"markdown","source":"## 1.2. LHC at Work","metadata":{}},{"cell_type":"markdown","source":"**Proton-proton collisions.** In particle physics, an *event* refers to the results just after a [fundamental interaction](https://en.wikipedia.org/wiki/Fundamental_interaction) took place between subatomic particles, occurring in a very short time span, at a well-localized region of space. In the LHC, swarms of protons are accelerated on a circular trajectory in both directions, at an extremely high speed. These swarms are made to cross in the ATLAS detector, causing hundreds of millions of proton-proton collisions per second. The resulting *events* are detected by sensors, producing a sparse vector of about a hundred thousand dimensions (roughly corresponding to an image or speech signal in classical machine learning applications). The feature construction phase involves extracting type, energy, as well as $3$-D direction of each particle from the raw data. Also, the variable-length list of four-tuples is digested into a fixed-length vector of features containing up to tens of real-valued variables.\n\n**Background events, signal events and selection region.** Some of these variables are first used in a real-time multi-stage cascade classifier (called the trigger) to discard most of the uninteresting events (called *background events*). The selected events (roughly four hundred per second) are then written on disks by a large CPU farm, producing petabytes of data per year. The saved events still, in large majority, represent known processes (these are also *background events*). The background events are mostly produced by the decay of particles which, though exotic in nature, are known beforehand from previous generations of experiments. The goal of the offline analysis is to find a region (called *selection region*) in the feature space that produces significantly excess of events (called *signal events*) compared to what known background processes can explain. Once the region has been fixed, a statistical test is applied to determine the significance of the excess. If the probability that the excess has been produced by background processes falls below a certain limit, it indicates the discovery of a new particle.\n\n**The classification problem.** To optimize the selection region, multivariate classification techniques are routinely utilized. The formal objective function is unique and somewhat different from the classification error or other objectives that are used regularly in machine learning. Nevertheless, finding a *pure* signal region corresponds roughly to separating background events and signal events, which is a standard classification problem. Consequently, established classification methods are useful, as they provide better discovery sensitivity than traditional, manual techniques.\n\n**Weighting and normalization.** The classifier is trained on simulated background events and signal events. Simulators produce weights for each event to correct for the mismatch between the prior probability of the event and the instrumental probability applied by the simulator. The weights are normalized such that in any region, the sum of the weights of events falling in the region gives an unbiased estimate of the expected number of events found there for a fixed integrated luminosity, which corresponds to a fixed data taking time for a given beam intensity. In this case, it corresponds to the data collected by the ATLAS experiment in 2012. Since the probability of a signal event is usually several orders of magnitudes lower than the probability of a background event, the signal samples and the background samples are usually renormalized to produce a balanced classification problem. A real-valued discriminant function is then trained on this reweighted sample to minimize the weighted classification error. The signal region is then defined by cutting the discriminant value at a certain threshold, which is optimized on a held-out set to maximize the sensitivity of the statistical test.\n\n**The goal:** To broad goal is to improve the procedure that produces the selection region, i.e. the region (not necessarily connected) in the feature space which produces signal events.","metadata":{}},{"cell_type":"markdown","source":"## 1.3. Enter ML","metadata":{}},{"cell_type":"markdown","source":"**Shallow neural network.** Machine learning plays a major role in processing data resulting from experiments at particle colliders. The ML classifiers learn to distinguish between different types of collision events by training on simulated data from sophisticated Monte-Carlo programs. Shallow [neural networks](https://en.wikipedia.org/wiki/Neural_network) with single hidden layer are one of the primary techniques used for this analysis and standardized implementations are included in the prevalent multivariate analysis software tools used by physicists. Efforts to increase statistical power tend to focus on developing new features for use with the existing machine learning classifiers. These high-level features are non-linear functions of the low-level measurements, derived using knowledge of the underlying physical processes.\n\n**Deep neural network.** The abundance of labeled simulation training data and the complex underlying structure make this an ideal application for [deep learning](https://en.wikipedia.org/wiki/Deep_learning), in particular for large, [deep neural networks](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks). Deep neural networks can simplify and improve the analysis of high-energy physics data by automatically learning high-level features from the data. In particular, they increase the statistical power of the analysis even without the help of manually derived high-level features.","metadata":{}},{"cell_type":"markdown","source":"## 1.4. Data","metadata":{}},{"cell_type":"markdown","source":"**Source:** https://www.kaggle.com/competitions/higgs-boson/data\n\n**The simulator.** The dataset has been built from official ATLAS full-detector simulation. The simulator has two parts. In the first, random proton-proton collisions are simulated based on the knowledge that we have accumulated on particle physics. It reproduces the random microscopic explosions resulting from the proton-proton collisions. In the second part, the resulting particles are tracked through a virtual model of the detector. The process yields simulated events with properties that mimic the statistical properties of the real events with additional information on what has happened during the collision, before particles are measured in the detector.\n\n**Signal sample and background sample.** The signal sample contains events in which Higgs bosons (with a fixed mass of 125 GeV) were produced. The background sample was generated by other known processes that can produce events with at least one electron or muon and a hadronic tau, mimicking the signal. Only three background processes were retained for the dataset. The first comes from the decay of the $Z$ boson (with a mass of 91.2 GeV) into two taus. This decay produces events with a topology very similar to that produced by the decay of a Higgs. The second set contains events with a pair of top quarks, which can have a lepton and a hadronic tau among their decay. The third set involves the decay of the $W$ boson, where one electron or muon and a hadronic tau can appear simultaneously only through imperfections of the particle identification procedure.\n\n**Training set and test set.** The training set and the test set respectively contains $250000$ and $550000$ observations. The two sets share $31$ common features between them. Additionally, the training set contains *labels* (signal or background) and *weights*.","metadata":{}},{"cell_type":"code","source":"# Loading the training data\ndata_train = pd.read_csv('../input/higgs-boson/training.zip')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_train.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_train.shape)}).to_string())\nprint(\" \")\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:47.292411Z","iopub.execute_input":"2022-05-29T16:27:47.293313Z","iopub.status.idle":"2022-05-29T16:27:49.41015Z","shell.execute_reply.started":"2022-05-29T16:27:47.293238Z","shell.execute_reply":"2022-05-29T16:27:49.409197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the test data\ndata_test = pd.read_csv('../input/higgs-boson/test.zip')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_test.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_test.shape)}).to_string())\nprint(\" \")\ndata_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:49.412013Z","iopub.execute_input":"2022-05-29T16:27:49.41234Z","iopub.status.idle":"2022-05-29T16:27:54.378378Z","shell.execute_reply.started":"2022-05-29T16:27:49.412311Z","shell.execute_reply":"2022-05-29T16:27:54.377372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5. Project Objective","metadata":{}},{"cell_type":"markdown","source":"The objective of the project is to classify an event produced in the particle accelerator as *background* or *signal*. As described earlier, a background event is explained by the existing theories and previous observations. A signal event, however, indicates a process that cannot be described by previous observations and leads to the potential discovery of a new particle.","metadata":{}},{"cell_type":"markdown","source":"## 1.6. Evaluation Metric","metadata":{}},{"cell_type":"markdown","source":"The [evaluation metric](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers), used in this project, is the approximate median significance (AMS), given by\n\n$$ AMS := \\sqrt{2\\left(\\left(s+b+b_r\\right)\\log{\\left(1+\\frac{s}{b+b_r}\\right)}-s\\right)},$$\n\nwhere\n- $s:$ unnormalized true positive rate,\n- $b:$ unnormalized [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate),\n- $b_r = 10:$ constant regularization term,\n- $\\log:$ [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm).\n\nPrecisely, let $(y_1, \\ldots, y_n) \\in \\{\\text{b},\\text{s}\\}^n$ be the vector of true test labels ($\\text{b}$ indicating background event and $\\text{s}$ indicating signal event) and let $(\\hat{y}_1, \\ldots, \\hat{y}_n) \\in \\{\\text{b},\\text{s}\\}^n$ be the vector of predicted test labels. Also let $(w_1, \\ldots, w_n) \\in {\\mathbb{R}^+}^n$ be the vector of weights ($\\mathbb{R}^+$ denoting the set of positive real numbers). Then\n\n$$ s = \\sum_{i=1}^n w_i \\mathbb{1}\\left\\{y_i = s\\right\\} \\mathbb{1}\\left\\{\\hat{y_i} = s\\right\\} $$\n\nand\n\n$$ b = \\sum_{i=1}^n w_i \\mathbb{1}\\left\\{y_i = b\\right\\} \\mathbb{1}\\left\\{\\hat{y_i} = s\\right\\}, $$\n\nwhere the [indicator function](https://en.wikipedia.org/wiki/Indicator_function) $\\mathbb{1}\\left\\{S\\right\\}$ is $1$ if $S$ is true and $0$ otherwise.","metadata":{}},{"cell_type":"markdown","source":"# 2. Basic Data Exploration","metadata":{}},{"cell_type":"code","source":"# Shape of the data\nprint(pd.Series({\"Shape of the training set\": data_train.shape,\n                 \"Shape of the test set\": data_test.shape}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.379831Z","iopub.execute_input":"2022-05-29T16:27:54.380381Z","iopub.status.idle":"2022-05-29T16:27:54.387228Z","shell.execute_reply.started":"2022-05-29T16:27:54.380323Z","shell.execute_reply":"2022-05-29T16:27:54.386244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of observations\ndf_obs = pd.DataFrame(index = ['Number of observations'], columns = ['Training set', 'Test set'])\ndf_obs['Training set'] = len(data_train)\ndf_obs['Test set'] = len(data_test)\ndf_obs","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.38844Z","iopub.execute_input":"2022-05-29T16:27:54.388884Z","iopub.status.idle":"2022-05-29T16:27:54.409149Z","shell.execute_reply.started":"2022-05-29T16:27:54.388841Z","shell.execute_reply":"2022-05-29T16:27:54.408421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of columns\ndf_cols_count = pd.DataFrame(index = ['Number of columns'], columns = ['Training set', 'Test set'])\ndf_cols_count['Training set'] = len(data_train.columns)\ndf_cols_count['Test set'] = len(data_test.columns)\ndf_cols_count","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.410749Z","iopub.execute_input":"2022-05-29T16:27:54.411144Z","iopub.status.idle":"2022-05-29T16:27:54.427646Z","shell.execute_reply.started":"2022-05-29T16:27:54.41111Z","shell.execute_reply":"2022-05-29T16:27:54.426494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for the training dataset\ndata_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.429106Z","iopub.execute_input":"2022-05-29T16:27:54.431153Z","iopub.status.idle":"2022-05-29T16:27:54.438417Z","shell.execute_reply.started":"2022-05-29T16:27:54.431105Z","shell.execute_reply":"2022-05-29T16:27:54.437404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for the test dataset\ndata_test.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.440059Z","iopub.execute_input":"2022-05-29T16:27:54.440617Z","iopub.status.idle":"2022-05-29T16:27:54.451293Z","shell.execute_reply.started":"2022-05-29T16:27:54.440571Z","shell.execute_reply":"2022-05-29T16:27:54.450271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns in the training dataset which are not in the test dataset\n[col for col in data_train.columns if col not in data_test.columns]","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.455118Z","iopub.execute_input":"2022-05-29T16:27:54.455594Z","iopub.status.idle":"2022-05-29T16:27:54.469474Z","shell.execute_reply.started":"2022-05-29T16:27:54.455561Z","shell.execute_reply":"2022-05-29T16:27:54.468695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column datatypes for the training dataset\ndata_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.470739Z","iopub.execute_input":"2022-05-29T16:27:54.471271Z","iopub.status.idle":"2022-05-29T16:27:54.483027Z","shell.execute_reply.started":"2022-05-29T16:27:54.471239Z","shell.execute_reply":"2022-05-29T16:27:54.482071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns of the test set are exactly the first $31$ columns of the training set, i.e. all columns except *Weight* and *Label*. The datatypes of these columns are already reported above, for the training set. So we are not repeating it again for the test set.","metadata":{}},{"cell_type":"code","source":"# Count of column datatypes for the training dataset\ndf_cols_train = pd.DataFrame(index = ['Number of columns for the training set'], columns = ['Integer', 'Float', 'Object'])\ndf_cols_train['Integer'] = len(data_train.columns[data_train.dtypes == 'int64'])\ndf_cols_train['Float'] = len(data_train.columns[data_train.dtypes == 'float64'])\ndf_cols_train['Object'] = len(data_train.columns[data_train.dtypes == 'object'])\ndf_cols_train","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.484404Z","iopub.execute_input":"2022-05-29T16:27:54.485039Z","iopub.status.idle":"2022-05-29T16:27:54.506595Z","shell.execute_reply.started":"2022-05-29T16:27:54.485005Z","shell.execute_reply":"2022-05-29T16:27:54.505943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Integer columns in the training dataset\ndata_train.columns[data_train.dtypes == 'int64']","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.508124Z","iopub.execute_input":"2022-05-29T16:27:54.508558Z","iopub.status.idle":"2022-05-29T16:27:54.515889Z","shell.execute_reply.started":"2022-05-29T16:27:54.508516Z","shell.execute_reply":"2022-05-29T16:27:54.514888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Object columns in the training dataset\ndata_train.columns[data_train.dtypes == 'object']","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.517242Z","iopub.execute_input":"2022-05-29T16:27:54.518321Z","iopub.status.idle":"2022-05-29T16:27:54.529455Z","shell.execute_reply.started":"2022-05-29T16:27:54.518277Z","shell.execute_reply":"2022-05-29T16:27:54.528509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of column datatypes for the test dataset\ndf_cols_test = pd.DataFrame(index = ['Number of columns for the test set'], columns = ['Integer', 'Float', 'Object'])\ndf_cols_test['Integer'] = len(data_test.columns[data_test.dtypes == 'int64'])\ndf_cols_test['Float'] = len(data_test.columns[data_test.dtypes == 'float64'])\ndf_cols_test['Object'] = len(data_test.columns[data_test.dtypes == 'object'])\ndf_cols_test","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.530625Z","iopub.execute_input":"2022-05-29T16:27:54.530983Z","iopub.status.idle":"2022-05-29T16:27:54.548712Z","shell.execute_reply.started":"2022-05-29T16:27:54.530953Z","shell.execute_reply":"2022-05-29T16:27:54.548015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of duplicate rows\ndf_duplicate_rows = pd.DataFrame(index = ['Number of duplicate rows'], columns = ['Training set', 'Test set'])\ndf_duplicate_rows['Training set'] = data_train.duplicated().sum()\ndf_duplicate_rows['Test set'] = data_test.duplicated().sum()\ndf_duplicate_rows","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:54.549815Z","iopub.execute_input":"2022-05-29T16:27:54.55025Z","iopub.status.idle":"2022-05-29T16:27:56.190364Z","shell.execute_reply.started":"2022-05-29T16:27:54.550221Z","shell.execute_reply":"2022-05-29T16:27:56.187116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant columns in the training set\ncols_constant_train = data_train.columns[data_train.nunique() == 1].tolist()\nif len(cols_constant_train) == 0:\n    cols_constant_train = \"None\"\nprint(pd.Series({\"Constant columns in the training set\": cols_constant_train}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:56.19237Z","iopub.execute_input":"2022-05-29T16:27:56.192711Z","iopub.status.idle":"2022-05-29T16:27:56.418726Z","shell.execute_reply.started":"2022-05-29T16:27:56.192681Z","shell.execute_reply":"2022-05-29T16:27:56.417731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant columns in the test set\ncols_constant_test = data_test.columns[data_test.nunique() == 1].tolist()\nif len(cols_constant_test) == 0:\n    cols_constant_test = \"None\"\nprint(pd.Series({\"Constant columns in the test set\": cols_constant_test}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:56.420133Z","iopub.execute_input":"2022-05-29T16:27:56.420476Z","iopub.status.idle":"2022-05-29T16:27:56.862848Z","shell.execute_reply.started":"2022-05-29T16:27:56.420435Z","shell.execute_reply":"2022-05-29T16:27:56.861509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of columns with missing values\ndf_missing = pd.DataFrame(index = ['Number of columns with missing values'], columns = ['Training set', 'Test set'])\ndf_missing['Training set'] = len(data_train.isna().sum()[data_train.isna().sum() != 0])\ndf_missing['Test set'] = len(data_test.isna().sum()[data_test.isna().sum() != 0])\ndf_missing","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:56.864483Z","iopub.execute_input":"2022-05-29T16:27:56.865022Z","iopub.status.idle":"2022-05-29T16:27:57.097712Z","shell.execute_reply.started":"2022-05-29T16:27:56.86498Z","shell.execute_reply":"2022-05-29T16:27:57.09697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of numerical variables in the training set\ndata_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:57.098651Z","iopub.execute_input":"2022-05-29T16:27:57.099377Z","iopub.status.idle":"2022-05-29T16:27:57.536551Z","shell.execute_reply.started":"2022-05-29T16:27:57.099345Z","shell.execute_reply":"2022-05-29T16:27:57.535844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of categorical variables in the training set\ndata_train.describe(include = ['O'])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:57.537641Z","iopub.execute_input":"2022-05-29T16:27:57.538093Z","iopub.status.idle":"2022-05-29T16:27:57.610845Z","shell.execute_reply.started":"2022-05-29T16:27:57.538053Z","shell.execute_reply":"2022-05-29T16:27:57.609386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of numerical variables in the test set\ndata_test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:57.612411Z","iopub.execute_input":"2022-05-29T16:27:57.615153Z","iopub.status.idle":"2022-05-29T16:27:58.43147Z","shell.execute_reply.started":"2022-05-29T16:27:57.615106Z","shell.execute_reply":"2022-05-29T16:27:58.430342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training set synopsis:**\n\n- Number of observations: $250000$\n- Number of columns: $33$\n- Number of integer columns: $2$\n- Number of float columns: $30$\n- Number of object columns: $1$\n- Number of duplicate observations: $0$\n- Constant columns: None\n- Number of columns with missing values: $0$\n- Memory Usage: $62.94$ MB\n\n**Test set synopsis:**\n\n- Number of observations: $550000$\n- Number of columns: $31$\n- Number of integer columns: $2$\n- Number of float columns: $29$\n- Number of object columns: $0$\n- Number of duplicate observations: $0$\n- Constant columns: None\n- Number of columns with missing values: $0$\n- Memory Usage: $130.08$ MB","metadata":{}},{"cell_type":"markdown","source":"# 3. Univariate Analysis\n\n- [Target variable](#3.1.-Target-variable)\n- [Predictor variables](#3.2.-Predictor-variables)","metadata":{}},{"cell_type":"code","source":"# Matplotlib patches for adding manual legends\ngrey_patch = mpatches.Patch(color = 'grey', label = \"Train\")\nred_patch = mpatches.Patch(color = 'red', label = \"Test\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:58.432924Z","iopub.execute_input":"2022-05-29T16:27:58.433498Z","iopub.status.idle":"2022-05-29T16:27:58.439228Z","shell.execute_reply.started":"2022-05-29T16:27:58.433452Z","shell.execute_reply":"2022-05-29T16:27:58.438232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, throughout the notebook, we choose the number of bins of a histogram by the [Freedman-Diaconis rule](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule), which suggests the optimal number of bins to grow as $k \\sim n^{1/3},$ where $n$ is the total number of observations.","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Target variable","metadata":{}},{"cell_type":"markdown","source":"The target `Label` is a binary variable, taking values `b` and `s`, indicating the status of an event.\n\\begin{align*}\n&\\text{b} \\,\\,\\mapsto \\text{background event}\\\\\n&\\text{s} \\,\\,\\mapsto \\text{signal event}\n\\end{align*}","metadata":{}},{"cell_type":"code","source":"# Function to construct barplot and donutplot of a dataframe column\ndef bar_donut(df, col, h = 500, w = 800):\n    fig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'xy'}, {'type': 'domain'}]])\n    x_val, y_val = df[col].value_counts(sort = False).index.tolist(), df[col].value_counts(sort = False).tolist()\n    fig.add_trace(go.Bar(x = x_val, y = y_val, text = y_val, textposition = 'auto'), row = 1, col = 1)\n    fig.add_trace(go.Pie(values = y_val, labels = x_val, hole = 0.5, textinfo = 'label+percent', title = f\"{col}\"), row = 1, col = 2)\n    fig.update_layout(height = h, width = w, showlegend = False, xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 1), title = dict(text = f\"Frequency distribution of {col}\", x = 0.5, y = 0.95)) \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:58.440659Z","iopub.execute_input":"2022-05-29T16:27:58.44112Z","iopub.status.idle":"2022-05-29T16:27:58.454587Z","shell.execute_reply.started":"2022-05-29T16:27:58.441075Z","shell.execute_reply":"2022-05-29T16:27:58.453601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target variable\nbar_donut(data_train, 'Label')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:58.456398Z","iopub.execute_input":"2022-05-29T16:27:58.456923Z","iopub.status.idle":"2022-05-29T16:27:58.823841Z","shell.execute_reply.started":"2022-05-29T16:27:58.456876Z","shell.execute_reply":"2022-05-29T16:27:58.823123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the training by target class\ndata_train_b = data_train[data_train['Label'] == 'b'] # Background events in the training set\ndata_train_s = data_train[data_train['Label'] == 's'] # System events in the training set","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:58.824916Z","iopub.execute_input":"2022-05-29T16:27:58.825679Z","iopub.status.idle":"2022-05-29T16:27:58.937679Z","shell.execute_reply.started":"2022-05-29T16:27:58.825645Z","shell.execute_reply":"2022-05-29T16:27:58.936855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Predictor variables","metadata":{}},{"cell_type":"code","source":"# Number of unique values for the predictor variables\ndf_unique = pd.DataFrame()\ndf_unique['Training set (background events)'] = [data_train_b[col].nunique() for col in data_test.columns]\ndf_unique['Training set (signal events)'] = [data_train_s[col].nunique() for col in data_test.columns]\ndf_unique['Training set (all events)'] = [data_train[col].nunique() for col in data_test.columns]\ndf_unique['Test set (all events)'] = [data_test[col].nunique() for col in data_test.columns]\ndf_unique.set_index(data_test.columns, inplace = True)\ndf_unique.style.set_caption(\"Number of unique values for the predictor variables\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:58.940128Z","iopub.execute_input":"2022-05-29T16:27:58.940634Z","iopub.status.idle":"2022-05-29T16:27:59.862319Z","shell.execute_reply.started":"2022-05-29T16:27:58.940587Z","shell.execute_reply":"2022-05-29T16:27:59.861269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Proportion of the value -999 in the dataset columns\ndef minus_999(df):\n    index = [col for col in df.columns if -999 in df[col].value_counts().index]\n    data = [df[col].value_counts()[-999]/len(df) for col in index]\n    return pd.Series(index = index, data = data).sort_values(ascending = False)\n\ndf_minus_999 = pd.DataFrame()\ndf_minus_999['Training set (background events)'] = minus_999(data_train_b).values\ndf_minus_999['Training set (signal events)'] = minus_999(data_train_s).values\ndf_minus_999['Training set (all events)'] = minus_999(data_train).values\ndf_minus_999['Test set (all events)'] = minus_999(data_test).values\ndf_minus_999.set_index(minus_999(data_train).index, inplace = True)\ndf_minus_999.style.set_caption(\"Proportion of the value -999 in the dataset columns which contain -999\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:27:59.868337Z","iopub.execute_input":"2022-05-29T16:27:59.868702Z","iopub.status.idle":"2022-05-29T16:28:02.542919Z","shell.execute_reply.started":"2022-05-29T16:27:59.868671Z","shell.execute_reply":"2022-05-29T16:28:02.541944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining the data\ndata_train_temp = data_train.copy(deep = True)\ndata_train_temp['dataset'] = \"Train\"\ndata_test_temp = data_test.copy(deep = True)\ndata_test_temp['dataset'] = \"Test\"\ndata = pd.concat([data_train_temp, data_test_temp], axis = 0, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:28:02.544471Z","iopub.execute_input":"2022-05-29T16:28:02.544829Z","iopub.status.idle":"2022-05-29T16:28:03.976358Z","shell.execute_reply.started":"2022-05-29T16:28:02.544776Z","shell.execute_reply":"2022-05-29T16:28:03.975248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Float features","metadata":{}},{"cell_type":"markdown","source":"#### Comparison of feature distributions for the training set and the test set\n\nFirst we check the distributions of the features for the training set and the test set. If we are to train our model on one set (the training set) and use it to make predictions on another (the test set), then it is desirable that the distributions corresponding to the two sets have similar structure.","metadata":{}},{"cell_type":"code","source":"# Function to plot distributions of the float features\ndef hist(df, cols, bins, ncols = 3):\n    nrows = math.ceil(len(cols) / ncols)\n    fig, ax = plt.subplots(nrows, ncols, figsize = (5 * ncols, 4.2 * nrows), sharey = False)\n    for i in range(len(cols)):\n        sns.histplot(data = df, x = cols[i], bins = bins, hue = 'dataset', palette = ['red', 'grey'], ax = ax[i // ncols, i % ncols])\n        ax[i // ncols, i % ncols].set_xlabel(cols[i])\n        if i % ncols != 0:\n            ax[i // ncols, i % ncols].set_ylabel(\" \")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:28:03.97761Z","iopub.execute_input":"2022-05-29T16:28:03.977932Z","iopub.status.idle":"2022-05-29T16:28:03.987307Z","shell.execute_reply.started":"2022-05-29T16:28:03.977903Z","shell.execute_reply":"2022-05-29T16:28:03.986608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distributions of the float features\nhist(data.replace(-999, np.nan),\n     list(data_test.columns[data_test.dtypes == 'float64']),\n     bins = max(math.floor(len(data_train)**(1/3)), math.floor(len(data_test)**(1/3))),\n     ncols = 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:28:03.988341Z","iopub.execute_input":"2022-05-29T16:28:03.989173Z","iopub.status.idle":"2022-05-29T16:29:08.785825Z","shell.execute_reply.started":"2022-05-29T16:28:03.989128Z","shell.execute_reply":"2022-05-29T16:29:08.785072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comparison of feature distributions by target class in the training set\n\nNext we compare the univariate distributions of the features for the background events and the signal events in the training set. If a feature has reasonably different distributions corresponding to the background events and the signal events, then it is a clear indication that the feature is important in the task of classifying the events when the label is unknown. Similarly, if a feature has very similar distributions for the two target classes, then it is unlikely to help in the classification problem based on the feature alone. This, however, does not take into account the possible dependence the feature may have with other features which may turn out to be useful in the task of classification. To check that will require a multivariate analysis.","metadata":{}},{"cell_type":"code","source":"# Function to plot distributions of the float features in the training set by target class\ndef hist_target(df, cols, target, bins, ncols = 3):\n    nrows = math.ceil(len(cols) / ncols)\n    fig, ax = plt.subplots(nrows, ncols, figsize = (5 * ncols, 4.2 * nrows), sharey = False)\n    for i in range(len(cols)):\n        sns.histplot(data = df, x = cols[i], bins = bins, hue = target, palette = ['red', 'grey'], ax = ax[i // ncols, i % ncols])\n        ax[i // ncols, i % ncols].set_xlabel(cols[i])\n        if i % ncols != 0:\n            ax[i // ncols, i % ncols].set_ylabel(\" \")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:08.786902Z","iopub.execute_input":"2022-05-29T16:29:08.787542Z","iopub.status.idle":"2022-05-29T16:29:08.795325Z","shell.execute_reply.started":"2022-05-29T16:29:08.787511Z","shell.execute_reply":"2022-05-29T16:29:08.794681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distributions of the float features in the training set by target class\nhist_target(data_train.replace(-999, np.nan),\n     list(data_test.columns[data_test.dtypes == 'float64']),\n     target = 'Label',\n     bins = max(math.floor(len(data_train_b)**(1/3)), math.floor(len(data_train_s)**(1/3))),\n     ncols = 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:08.796362Z","iopub.execute_input":"2022-05-29T16:29:08.796903Z","iopub.status.idle":"2022-05-29T16:29:38.229826Z","shell.execute_reply.started":"2022-05-29T16:29:08.796865Z","shell.execute_reply":"2022-05-29T16:29:38.228849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Skewness\n\n[Skewness](https://en.wikipedia.org/wiki/Skewness) quantifies the asymmetry of a distribution about its mean. It is given by\n\n$$ g_1 := \\frac{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^3}{\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2\\right]^{3/2}}, $$\n\nwhere $\\bar{x}$ is the mean of the observations, given by $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. The measure $g_1$ can be negative, zero, positive. A value close to $0$ suggests that the distribution is more or less symmetric. However, as it deviates from $0$, it becomes more and more skewed (either positively or negatively). A positive skewness indicates that the distribution is concentrated towards the left side, with the longer tail being on the right side. A negative skewness indicates that the distribution is concentrated towards the right side, with the longer tail being on the left side.","metadata":{}},{"cell_type":"code","source":"# Skewness of the float features\ndf_skew = pd.DataFrame()\ncols_float_test = data_test.columns[data_test.dtypes == 'float64']\nskew_train_b = data_train_b.replace(-999, np.nan)[cols_float_test].skew().values\nskew_train_s = data_train_s.replace(-999, np.nan)[cols_float_test].skew().values\nskew_train = data_train.replace(-999, np.nan)[cols_float_test].skew().values\nskew_test = data_test.replace(-999, np.nan)[cols_float_test].skew().values\ndf_skew['Training set (background events)'], df_skew['Training set (signal events)'] = skew_train_b, skew_train_s\ndf_skew['Training set (all events)'], df_skew['Test set (all events)'] = skew_train, skew_test\ndf_skew.set_index(np.array(cols_float_test), inplace = True)\ndf_skew.style.set_caption(\"Skewness of the float features\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:38.231327Z","iopub.execute_input":"2022-05-29T16:29:38.231662Z","iopub.status.idle":"2022-05-29T16:29:39.25691Z","shell.execute_reply.started":"2022-05-29T16:29:38.231632Z","shell.execute_reply":"2022-05-29T16:29:39.255967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Columns with extreme positive skewness (absolute value greater than $3$ in the training set): `DER_mass_MMC`, `DER_mass_vis`, `DER_pt_tot`, `PRI_tau_pt`, `PRI_lep_pt`, `PRI_met`, `PRI_jet_subleading_pt` \n- Columns with high positive skewness (absolute value between $1$ and $3$ in the training set): `DER_mass_transverse_met_lep`, `DER_pt_h`, `DER_mass_jet_jet`, `DER_sum_pt`, `DER_pt_ratio_lep_tau`, `PRI_met_sumet`, `PRI_jet_leading_pt`, `PRI_jet_all_pt`\n- Columns with moderate positive skewness (absolute value between $0.5$ and $1$ in the training set): `DER_deltaeta_jet_jet`","metadata":{}},{"cell_type":"code","source":"# Distribution of skewness of the float features in the training set and the test set\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_skew, x = 'Training set (all events)', ax = ax[0])\nsns.kdeplot(data = df_skew, x = 'Test set (all events)', ax = ax[1])\nax[0].set_title(\"Training set\", fontsize = 14)\nax[0].set_xlabel(\"Skewness\", fontsize = 14)\nax[1].set_title(\"Test set\", fontsize = 14)\nax[1].set_xlabel(\"Skewness\", fontsize = 14)\nplt.suptitle(\"Skewness of float features\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:39.258161Z","iopub.execute_input":"2022-05-29T16:29:39.258486Z","iopub.status.idle":"2022-05-29T16:29:39.808182Z","shell.execute_reply.started":"2022-05-29T16:29:39.258458Z","shell.execute_reply":"2022-05-29T16:29:39.807226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The skewness distribution for both the training set and the test set have global peak near $0$\n- There is a mild peak far towards the right (around $11$) in the skewness distribution for the training set, indicating that some of the float features in the training set exhibit extremely high positive skewness\n- The skewness distribution for the test set has a clear bimodal structure, with a local peak near $2.5$","metadata":{}},{"cell_type":"code","source":"# Distribution of skewness of the float features in the training set by target class\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_skew, x = 'Training set (background events)', ax = ax[0])\nsns.kdeplot(data = df_skew, x = 'Training set (signal events)', ax = ax[1])\nax[0].set_title(\"Background events\", fontsize = 14)\nax[0].set_xlabel(\"Skewness\", fontsize = 14)\nax[1].set_title(\"Signal events\", fontsize = 14)\nax[1].set_xlabel(\"Skewness\", fontsize = 14)\nplt.suptitle(\"Skewness of float features in the training set by target class\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:39.809948Z","iopub.execute_input":"2022-05-29T16:29:39.810428Z","iopub.status.idle":"2022-05-29T16:29:40.350482Z","shell.execute_reply.started":"2022-05-29T16:29:39.810385Z","shell.execute_reply":"2022-05-29T16:29:40.349216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The skewness distribution for both the background events and the signal events in the training set have global peak near $0$\n- There is a mild peak far towards the right (around $22$) in the skewness distribution for the signal events, indicating presence of float features with extreme positive skewness for the signal events\n- The skewness distribution for the background events has a clear bimodal structure, with a local peak near $2.5$","metadata":{}},{"cell_type":"markdown","source":"#### Kurtosis\n\n[Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) generally quantifies the peakedness and tailedness of a distribution. It is given by\n\n$$ b_2 = \\frac{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^4}{\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2\\right]^2}, $$\n\nwhere $\\bar{x}$ is the mean of the observations, given by $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. A relocated version of the kurtosis, taking into account the fact that $b_2 = 3$ for the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) is defined as the [Excess kurtosis](https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis), given by $g_2 := b_2 - 3$. The measure $g_2$ can be negative, zero, positive. A value close to $0$ suggests that the distribution has similar kurtosis as that of the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). A positive excess kurtosis ([leptokurtic distribution](https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic)) indicates that the distribution has more concentration towards the center or the tails, giving it a rather slender shape (*lepto-* literally means slender). On the other hand a negative excess kurtosis ([Platykurtic distribution](https://en.wikipedia.org/wiki/Kurtosis#Platykurtic)) indicates that the distribution has more concentration in the regions between the center and the tails, giving it a broad shape (*platy-* literally means *broad*).\n\n**Note:** The ability of the measure $b_2$ or $g_2$ to quantify peakedness of a distribution has been a topic of debate. See [this paper](https://www.jstor.org/stable/24591697?seq=1) for more details.","metadata":{}},{"cell_type":"code","source":"# Kurtosis of the float features\ndf_kurt = pd.DataFrame()\nkurt_train_b = data_train_b.replace(-999, np.nan)[cols_float_test].kurt().values\nkurt_train_s = data_train_s.replace(-999, np.nan)[cols_float_test].kurt().values\nkurt_train = data_train.replace(-999, np.nan)[cols_float_test].kurt().values\nkurt_test = data_test.replace(-999, np.nan)[cols_float_test].kurt().values\ndf_kurt['Training set (background events)'], df_kurt['Training set (signal events)'] = kurt_train_b, kurt_train_s\ndf_kurt['Training set (all events)'], df_kurt['Test set (all events)'] = kurt_train, kurt_test\ndf_kurt.set_index(np.array(cols_float_test), inplace = True)\ndf_kurt.style.set_caption(\"Kurtosis of the float features\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:40.351705Z","iopub.execute_input":"2022-05-29T16:29:40.352067Z","iopub.status.idle":"2022-05-29T16:29:41.291903Z","shell.execute_reply.started":"2022-05-29T16:29:40.352037Z","shell.execute_reply":"2022-05-29T16:29:41.290949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Columns with extreme leptokurtosis (excess kurtosis greater than $3$ in the training set): `DER_mass_MMC`, `DER_mass_transverse_met_lep`, `DER_mass_vis`, `DER_pt_h`, `DER_mass_jet_jet`, `DER_pt_tot`, `DER_sum_pt`, `DER_pt_ratio_lep_tau`, `PRI_tau_pt`, `PRI_lep_pt`, `PRI_met`, `PRI_met_sumet`, `PRI_jet_leading_pt`, `PRI_jet_subleading_pt`, `PRI_jet_all_pt`\n- Columns with high leptokurtosis (excess kurtosis between $1$ and $3$ in the training set): `DER_prodeta_jet_jet`\n- Columns with high platykurtosis (excess kurtosis between $-3$ and $-1$ in the training set): `DER_met_phi_centrality`, `DER_lep_eta_centrality`, `PRI_tau_phi`, `PRI_lep_phi`, `PRI_met_phi`, `PRI_jet_leading_phi`, `PRI_jet_subleading_phi`\n- Columns with moderate platykurtosis (excess kurtosis between $-1$ and $-0.5$ in the training set): `DER_deltaeta_jet_jet`, `PRI_tau_eta`, `PRI_lep_eta`, `PRI_jet_leading_eta`, `PRI_jet_subleading_eta`","metadata":{}},{"cell_type":"code","source":"# Distribution of kurtosis of the float features\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_kurt, x = 'Training set (all events)', ax = ax[0])\nsns.kdeplot(data = df_kurt, x = 'Test set (all events)', ax = ax[1])\nax[0].set_title(\"Training set\", fontsize = 14)\nax[0].set_xlabel(\"Kurtosis\", fontsize = 14)\nax[1].set_title(\"Test set\", fontsize = 14)\nax[1].set_xlabel(\"Kurtosis\", fontsize = 14)\nplt.suptitle(\"Kurtosis of float features\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:41.293127Z","iopub.execute_input":"2022-05-29T16:29:41.293461Z","iopub.status.idle":"2022-05-29T16:29:41.861172Z","shell.execute_reply.started":"2022-05-29T16:29:41.293431Z","shell.execute_reply":"2022-05-29T16:29:41.860157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The kurtosis distribution for both the training set and the test set have global peak near $0$\n- The kurtosis distribution for the training set is more or less bell-shaped and relatively dispersed, whereas the same for the test set is far more concentrated about $0$ with high peakedness","metadata":{}},{"cell_type":"code","source":"# Distribution of kurtosis of the float features in the training set by target class\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_kurt, x = 'Training set (background events)', ax = ax[0])\nsns.kdeplot(data = df_kurt, x = 'Training set (signal events)', ax = ax[1])\nax[0].set_title(\"Background events\", fontsize = 14)\nax[0].set_xlabel(\"Kurtosis\", fontsize = 14)\nax[1].set_title(\"Signal events\", fontsize = 14)\nax[1].set_xlabel(\"Kurtosis\", fontsize = 14)\nplt.suptitle(\"Kurtosis of float features in the training set by target class\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:41.862408Z","iopub.execute_input":"2022-05-29T16:29:41.862735Z","iopub.status.idle":"2022-05-29T16:29:42.409471Z","shell.execute_reply.started":"2022-05-29T16:29:41.862705Z","shell.execute_reply":"2022-05-29T16:29:42.408498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The kurtosis distribution for both the background events and the signal events in the training set have global peak near $0$\n- The kurtosis distribution for the signal events is more or less bell-shaped and relatively dispersed, whereas the same for the background events is far more concentrated about $0$ with high peakedness","metadata":{}},{"cell_type":"markdown","source":"**Observation:** For both skewness and kurtosis, we see uncanny resemblance between the respective distribution plots for background events in the training set and all events in the test set. This indicates that majority of the observations in the test set may be background events.","metadata":{}},{"cell_type":"markdown","source":"### Integer feature","metadata":{}},{"cell_type":"code","source":"# Donutplots\ndef donut(df1, df2, col,text1, text2, title_text = \"Title\"):\n    fig = make_subplots(rows = 1, cols = 2, column_widths = [0.5, 0.5], row_heights = [0.5], specs = [[ {\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n    fig.add_trace(go.Pie(labels = df1[col].value_counts().index, values = df1[col].value_counts(), legendgroup = \"group\", textinfo = 'percent', hole = 0.3, title = dict(text = text1)), row = 1, col = 1)\n    fig.add_trace(go.Pie(labels = df2[col].value_counts().index, values = df2[col].value_counts(), legendgroup = \"group\", textinfo = 'percent', hole = 0.3, title = dict(text = text2)), row = 1, col = 2)\n    fig.update_layout(title = dict(text = title_text, y = 0.9, x = 0.5, xanchor = 'center', yanchor = 'top'))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:42.410885Z","iopub.execute_input":"2022-05-29T16:29:42.411474Z","iopub.status.idle":"2022-05-29T16:29:42.422581Z","shell.execute_reply.started":"2022-05-29T16:29:42.411431Z","shell.execute_reply":"2022-05-29T16:29:42.421641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Frequency comparison of PRI_jet_num for the training set and the test set\ndonut(data_train, data_test, col = 'PRI_jet_num', text1 = 'Train', text2 = 'Test', title_text = \"Frequency comparison of PRI_jet_num\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:42.425182Z","iopub.execute_input":"2022-05-29T16:29:42.425552Z","iopub.status.idle":"2022-05-29T16:29:42.477314Z","shell.execute_reply.started":"2022-05-29T16:29:42.425522Z","shell.execute_reply":"2022-05-29T16:29:42.476528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The proportions of values of `PRI_jet_num` are more or less same for both the training set and the test set","metadata":{}},{"cell_type":"code","source":"# Frequency comparison of PRI_jet_num for the background events and the signal events in the training set\ndonut(data_train_b, data_train_s, col = 'PRI_jet_num', text1 = 'Background', text2 = 'Signal', title_text = \"Frequency comparison of PRI_jet_num in the training set by target class\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:42.478961Z","iopub.execute_input":"2022-05-29T16:29:42.479431Z","iopub.status.idle":"2022-05-29T16:29:42.508744Z","shell.execute_reply.started":"2022-05-29T16:29:42.479387Z","shell.execute_reply":"2022-05-29T16:29:42.507796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The proportions of values of `PRI_jet_num`, especially $0$ and $2$, differ for the background events and the signal events in the training set","metadata":{}},{"cell_type":"markdown","source":"# 4. Multivariate Analysis\n\n- [Correlation structure of float features](#4.1.-Correlation-structure-of-float-features)\n- [Bivariate scatterplots](#4.2.-Bivariate-scatterplots)\n- [Trivariate scatterplots](#4.3.-Trivariate-scatterplots)","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Correlation structure of float features","metadata":{}},{"cell_type":"markdown","source":"[Correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a statistical measure of linear dependence between two variables. Extreme correlation gives an indication that the two variables are linearly related, however this does not prove any causal relationship between the said variables. The measure is defined as the covariance of the two variables, scaled by the product of respective standard deviations. Let $\\left\\{\\left(x_1, y_1\\right), \\left(x_2, y_2\\right), \\cdots, \\left(x_n, y_n\\right)\\right\\}$ be paired data on the variables $\\left(x, y\\right)$. Then the correlation coefficient of the two variables is given by\n$$ r_{xy} := \\frac{\\text{cov}\\left(x, y\\right)}{s_x s_y} = \\frac{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2} \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2}},$$\n\nwhere $\\bar{x}$ and $\\bar{y}$ denote the respective sample means of the two variables, given by $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$.","metadata":{}},{"cell_type":"code","source":"# Correlation coefficients of pairs of float features\ndf_corr = pd.DataFrame(columns = ['feature_1', 'feature_2', 'Training set (background events)', 'Training set (signal events)', 'Training set (all events)', 'Test set (all events)'])\ncols = cols_float_test\nfor i in range(len(cols)):\n    for j in range(len(cols)):\n        if i<j:\n            df_corr.loc[len(df_corr.index)] = [cols[i], cols[j], data_train_b[cols[i]].corr(data_train_b[cols[j]]), data_train_s[cols[i]].corr(data_train_s[cols[j]]), data_train[cols[i]].corr(data_train[cols[j]]), data_test[cols[i]].corr(data_test[cols[j]])]\ndf_corr.sort_values(by = 'Training set (all events)', ascending = False, inplace = True)\ndf_corr # df_corr.style.set_caption(\"Correlation coefficient of pairs of float features\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:42.510002Z","iopub.execute_input":"2022-05-29T16:29:42.51032Z","iopub.status.idle":"2022-05-29T16:29:49.997569Z","shell.execute_reply.started":"2022-05-29T16:29:42.510291Z","shell.execute_reply":"2022-05-29T16:29:49.996462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of correlation coefficient of pairs of float features\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_corr, x = 'Training set (all events)', clip = (-1.0, 1.0), ax = ax[0])\nsns.kdeplot(data = df_corr, x = 'Test set (all events)', clip = (-1.0, 1.0), ax = ax[1])\nax[0].set_title(\"Training set\", fontsize = 14)\nax[0].set_xlabel(\"Correlation coefficient\", fontsize = 14)\nax[1].set_title(\"Test set\", fontsize = 14)\nax[1].set_xlabel(\"Correlation coefficient\", fontsize = 14)\nplt.suptitle(\"Correlation coefficient of float features\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:49.999212Z","iopub.execute_input":"2022-05-29T16:29:49.999646Z","iopub.status.idle":"2022-05-29T16:29:50.557924Z","shell.execute_reply.started":"2022-05-29T16:29:49.999605Z","shell.execute_reply":"2022-05-29T16:29:50.557039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The correlation distribution is almost identical for the training set and the test set\n- The density has its global peak near $0$, however there is a local peak between $0.5$ and $0.6$, indicating a fair number of moderately to highly correlated pairs of features\n- There is even an increase in the density after $0.8$, with a small peak very close to $1$, indicating the presence of a few extremely correlated pairs of features","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap of float features for the training set\nplt.figure(figsize = (26, 19.5))\nsns.heatmap(data_train[cols_float_test].corr(), vmin = -1, vmax = 1, annot = True, cmap = plt.cm.CMRmap_r)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:50.559361Z","iopub.execute_input":"2022-05-29T16:29:50.559659Z","iopub.status.idle":"2022-05-29T16:29:55.249864Z","shell.execute_reply.started":"2022-05-29T16:29:50.559632Z","shell.execute_reply":"2022-05-29T16:29:55.248855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation heatmap of float features for the test set\nplt.figure(figsize = (26, 19.5))\nsns.heatmap(data_test[cols_float_test].corr(), vmin = -1, vmax = 1, annot = True, cmap = plt.cm.CMRmap_r)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:29:55.251257Z","iopub.execute_input":"2022-05-29T16:29:55.251605Z","iopub.status.idle":"2022-05-29T16:30:00.672898Z","shell.execute_reply.started":"2022-05-29T16:29:55.251573Z","shell.execute_reply":"2022-05-29T16:30:00.672022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The following groups have shown extremely high positive correlation structure within themselves, i.e. any two features from a single group has extremely high correlation coefficient.\n\n- `DER_deltaeta_jet_jet`, `DER_mass_jet_jet`, `DER_prodeta_jet_jet`, `DER_lep_eta_centrality`, `PRI_jet_subleading_pt`, `PRI_jet_subleading_eta`, `PRI_jet_subleading_phi`\n- `DER_sum_pt`, `PRI_met_sumet`, `PRI_jet_all_pt`\n- `PRI_jet_leading_pt`, `PRI_jet_leading_eta`, `PRI_jet_leading_phi`","metadata":{}},{"cell_type":"code","source":"# Distribution of correlation coefficient of pairs of float features by target class in the training set\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\nsns.kdeplot(data = df_corr, x = 'Training set (background events)', clip = (-1.0, 1.0), ax = ax[0])\nsns.kdeplot(data = df_corr, x = 'Training set (signal events)', clip = (-1.0, 1.0), ax = ax[1])\nax[0].set_title(\"Background events\", fontsize = 14)\nax[0].set_xlabel(\"Correlation coefficient\", fontsize = 14)\nax[1].set_title(\"Signal events\", fontsize = 14)\nax[1].set_xlabel(\"Correlation coefficient\", fontsize = 14)\nplt.suptitle(\"Correlation coefficient of pairs of float features by target class in the training set\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:30:00.674322Z","iopub.execute_input":"2022-05-29T16:30:00.674935Z","iopub.status.idle":"2022-05-29T16:30:01.255851Z","shell.execute_reply.started":"2022-05-29T16:30:00.674873Z","shell.execute_reply":"2022-05-29T16:30:01.254874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The left tail of the correlation distributions corresponding to background events and signal events in the training set are different\n- The densities have their respective global peaks near $0$, however there is a local peak between $0.5$ and $0.6$ for both of them, indicating a fair number of pairs of float features with moderate to high positive correlation\n- The left tail of the correlation distribution for the signal events is heavier compared to the same for the background events, indicating that there are more pairs of float features with moderate to high negative correlation for the signal events than the background events in the training set\n- For both target classes, there is an increase in the density after $0.8$, with a small peak very close to $1$, indicating the presence of a few pairs of float features with extreme positive correlation","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap of float features for background events and signal events in the training set\nfig, ax = plt.subplots(1, 2, figsize = (15, 6.5), sharex = True, sharey = True)\nsns.heatmap(data_train_b[cols_float_test].corr(), vmin = -1, vmax = 1, annot = False, xticklabels = False, yticklabels = False, cmap = plt.cm.CMRmap_r, ax = ax[0])\nsns.heatmap(data_train_s[cols_float_test].corr(), vmin = -1, vmax = 1, annot = False, xticklabels = False, yticklabels = False, cmap = plt.cm.CMRmap_r, ax = ax[1])\nax[0].set_title(\"Background events\", fontsize = 14)\nax[1].set_title(\"Signal events\", fontsize = 14)\nplt.suptitle(\"Correlation heatmap of float features for background events and signal events in the training set\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:30:01.25761Z","iopub.execute_input":"2022-05-29T16:30:01.258283Z","iopub.status.idle":"2022-05-29T16:30:02.347859Z","shell.execute_reply.started":"2022-05-29T16:30:01.258238Z","shell.execute_reply":"2022-05-29T16:30:02.346746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The two heatmaps are more or less similar, i.e. the overall correlation structure of the float features corresponding to the background events and the signal events in the training set are very much alike.","metadata":{}},{"cell_type":"markdown","source":"In the next two subsections, we present bivariate (trivariate) scatterplots for some selected pairs (triples) of features. These plots are given separately for the background events and signal events appearing in the training set. So they can be used not only to understand the relationship between (among) a pair (triple) of features, but also the ability of the pair (triple) to classify an event as *background* (b) or *signal* (s).","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Bivariate scatterplots","metadata":{}},{"cell_type":"code","source":"# Selected bivariate scatterplots\npairs_selected = [\n    ('DER_mass_MMC', 'DER_mass_jet_jet'),\n    ('DER_mass_MMC', 'DER_prodeta_jet_jet'),\n    ('DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet'),\n    ('DER_mass_jet_jet', 'DER_deltar_tau_lep'),\n    ('DER_mass_jet_jet', 'PRI_jet_leading_eta'),\n    ('DER_prodeta_jet_jet', 'PRI_jet_leading_eta'),\n    ('PRI_tau_eta', 'PRI_lep_eta'),\n    ('PRI_jet_num', 'PRI_jet_subleading_pt')\n]\nfor z in pairs_selected:\n    fig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\n    sns.scatterplot(data = data_train_b.replace(-999, np.nan), x = z[0], y = z[1], ax = ax[0])\n    sns.scatterplot(data = data_train_s.replace(-999, np.nan), x = z[0], y = z[1], ax = ax[1])\n    ax[0].set_title(\"Background events\", fontsize = 14)\n    ax[1].set_title(\"Signal events\", fontsize = 14)\n    plt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:30:02.349451Z","iopub.execute_input":"2022-05-29T16:30:02.349956Z","iopub.status.idle":"2022-05-29T16:30:08.457579Z","shell.execute_reply.started":"2022-05-29T16:30:02.349894Z","shell.execute_reply":"2022-05-29T16:30:08.456593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Trivariate scatterplots","metadata":{}},{"cell_type":"markdown","source":"**Note:** The coloring of the observations in the plots corresponds to the second component of the triple.","metadata":{}},{"cell_type":"code","source":"# Selected trivariate scatterplots\ntriples_selected = [\n    ('DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_pt_tot'),\n    ('DER_mass_MMC', 'DER_mass_vis', 'DER_prodeta_jet_jet'),\n    ('DER_mass_MMC', 'DER_mass_vis', 'DER_pt_tot'),\n    ('DER_mass_MMC', 'DER_mass_vis', 'PRI_lep_pt'),\n    ('DER_mass_MMC', 'DER_pt_h', 'DER_pt_tot'),\n    ('DER_mass_MMC', 'DER_pt_h', 'PRI_jet_subleading_pt'),\n    ('DER_mass_MMC', 'DER_deltaeta_jet_jet', 'PRI_jet_num'),\n    ('DER_mass_MMC', 'DER_mass_jet_jet', 'PRI_jet_subleading_eta'),\n    ('DER_mass_MMC', 'DER_prodeta_jet_jet', 'DER_pt_tot'),\n    ('DER_mass_MMC', 'DER_lep_eta_centrality', 'PRI_met'),\n    ('DER_mass_MMC', 'DER_lep_eta_centrality', 'PRI_jet_num'),\n    ('DER_mass_MMC', 'PRI_met_phi', 'PRI_jet_subleading_pt'),\n    ('DER_mass_MMC', 'PRI_jet_num', 'PRI_jet_leading_pt'),\n    ('DER_mass_vis', 'DER_pt_h', 'PRI_jet_leading_pt'),\n    ('DER_pt_h', 'PRI_jet_num', 'PRI_jet_leading_eta'),\n    ('DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet', 'PRI_jet_subleading_eta'),\n    ('DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet', 'PRI_jet_subleading_phi'),\n    ('DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet', 'PRI_jet_all_pt'),\n    ('DER_deltaeta_jet_jet', 'DER_met_phi_centrality', 'PRI_jet_num'),\n    ('DER_mass_jet_jet', 'DER_deltar_tau_lep', 'PRI_lep_pt'),\n    ('DER_mass_jet_jet', 'PRI_tau_pt', 'PRI_jet_subleading_eta'),\n    ('DER_prodeta_jet_jet', 'DER_sum_pt', 'PRI_jet_all_pt'),\n    ('DER_mass_jet_jet', 'PRI_jet_leading_eta', 'PRI_jet_subleading_eta'),\n    ('DER_deltar_tau_lep', 'PRI_lep_eta', 'PRI_jet_subleading_pt'),\n    ('DER_pt_ratio_lep_tau', 'PRI_jet_num', 'PRI_jet_leading_pt'),\n    ('DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_jet_num'),\n    ('DER_met_phi_centrality', 'PRI_lep_eta', 'PRI_jet_num')\n]\nfor z in triples_selected:\n    fig = plt.figure(figsize = (15, 9))\n    ax = fig.add_subplot(1, 2, 1, projection = '3d')\n    x_b = data_train_b.replace(-999, np.nan)[z[0]]\n    y_b = data_train_b.replace(-999, np.nan)[z[1]]\n    z_b = data_train_b.replace(-999, np.nan)[z[2]]\n    s1 = ax.scatter(x_b, y_b, z_b, s = 40, marker = 'o', c = y_b, alpha = 1)\n    ax.set_title(\"Background events\", fontsize = 14)\n    ax.set_xlabel(z[0])\n    ax.set_ylabel(z[1]) # ax.set_zlabel(z[2])\n    ax = fig.add_subplot(1, 2, 2, projection = '3d')\n    x_s = data_train_s.replace(-999, np.nan)[z[0]]\n    y_s = data_train_s.replace(-999, np.nan)[z[1]]\n    z_s = data_train_s.replace(-999, np.nan)[z[2]]\n    s2 = ax.scatter(x_s, y_s, z_s, s = 40, marker = 'o', c = y_s, alpha = 1)\n    ax.set_title(\"Signal events\", fontsize = 14)\n    ax.set_xlabel(z[0])\n    ax.set_ylabel(z[1])\n    ax.set_zlabel(z[2])\n    plt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:30:08.459001Z","iopub.execute_input":"2022-05-29T16:30:08.459331Z","iopub.status.idle":"2022-05-29T16:31:39.469859Z","shell.execute_reply.started":"2022-05-29T16:30:08.459302Z","shell.execute_reply":"2022-05-29T16:31:39.468748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledgements\n\n- [Dataset from the ATLAS Higgs Boson Machine Learning Challenge 2014](http://opendata.cern.ch/record/328)\n- [The Higgs boson machine learning challenge documentation](https://higgsml.ijclab.in2p3.fr/documentation/)","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- [ATLAS](https://home.cern/science/experiments/atlas)\n- [Bose-Einstein statistics](https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_statistics)\n- [Boson](https://en.wikipedia.org/wiki/Boson)\n- [Bottom quark](https://en.wikipedia.org/wiki/Bottom_quark)\n- [CERN](https://en.wikipedia.org/wiki/CERN)\n- [CMS](https://home.cern/science/experiments/cms)\n- [Correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n- [Decay channel](https://atlas.cern/glossary/decay-channel)\n- [Deep learning](https://en.wikipedia.org/wiki/Deep_learning)\n- [Deep neural networks](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks)\n- [Elementary particle](https://en.wikipedia.org/wiki/Elementary_particle)\n- [Evaluation of binary classifiers](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)\n- [Evidence for Higgs Boson Decays to the $\\tau^+\\tau^-$ Final State with the ATLAS Detector](https://cds.cern.ch/record/1632191)\n- [Excess kurtosis](https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis)\n- [False positive rate](https://en.wikipedia.org/wiki/False_positive_rate)\n- [Fermi-Dirac statistics](https://en.wikipedia.org/wiki/Fermi%E2%80%93Dirac_statistics)\n- [Fermion](https://en.wikipedia.org/wiki/Fermion)\n- [François Englert](https://en.wikipedia.org/wiki/Fran%C3%A7ois_Englert)\n- [Freedman-Diaconis rule](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule)\n- [Fundamental interaction](https://en.wikipedia.org/wiki/Fundamental_interaction)\n- [Higgs boson](https://en.wikipedia.org/wiki/Higgs_boson)\n- [Higgs field](https://en.wikipedia.org/wiki/Higgs_mechanism#Structure_of_the_Higgs_field)\n- [Higgs mechanism](https://en.wikipedia.org/wiki/Higgs_mechanism)\n- [Indicator function](https://en.wikipedia.org/wiki/Indicator_function)\n- [Kurtosis](https://en.wikipedia.org/wiki/Kurtosis)\n- [Kurtosis as Peakedness, 1905–2014. R.I.P.](https://www.jstor.org/stable/24591697?seq=1)\n- [Large Hadron Collider](https://en.wikipedia.org/wiki/Large_Hadron_Collider)\n- [Leptokurtic distribution](https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic)\n- [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)\n- [Mass generation](https://en.wikipedia.org/wiki/Mass_generation)\n- [Mesokurtic distribution](https://en.wikipedia.org/wiki/Kurtosis#Mesokurtic)\n- [Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n- [Natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm)\n- [Neural network](https://en.wikipedia.org/wiki/Neural_network)\n- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)\n- [Particle accelerator](https://en.wikipedia.org/wiki/Particle_accelerator)\n- [Particle beam](https://en.wikipedia.org/wiki/Particle_beam)\n- [Particle decay](https://en.wikipedia.org/wiki/Particle_decay)\n- [Particle physics](https://en.wikipedia.org/wiki/Particle_physics)\n- [Photon](https://en.wikipedia.org/wiki/Photon)\n- [Platykurtic distribution](https://en.wikipedia.org/wiki/Kurtosis#Platykurtic)\n- [Root mean square deviation](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n- [Skewness](https://en.wikipedia.org/wiki/Skewness)\n- [Standard Model](https://en.wikipedia.org/wiki/Standard_Model)\n- [Subatomic particle](https://en.wikipedia.org/wiki/Subatomic_particle)\n- [Tau lepton](https://simple.wikipedia.org/wiki/Tau_lepton)\n- [The HiggsML challenge](https://higgsml.ijclab.in2p3.fr/)\n- [The Nobel Prize in Physics 2013](https://www.nobelprize.org/prizes/physics/2013/summary/)","metadata":{}},{"cell_type":"code","source":"# Runtime and memory usage\nstop = time.time()\nprint(pd.Series({\"Process runtime\": \"{:.2f} seconds\".format(float(stop - start)),\n                 \"Process memory usage\": \"{:.2f} MB\".format(float(process.memory_info()[0]/(1024*1024)))}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T16:31:39.471487Z","iopub.execute_input":"2022-05-29T16:31:39.472346Z","iopub.status.idle":"2022-05-29T16:31:39.480404Z","shell.execute_reply.started":"2022-05-29T16:31:39.472312Z","shell.execute_reply":"2022-05-29T16:31:39.479465Z"},"trusted":true},"execution_count":null,"outputs":[]}]}