{"cells":[{"metadata":{},"cell_type":"markdown","source":"## ИИ в игровой индустрии: каково будущее ... и почему это сложнее для деплоймента, чем кажется","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*От автора: Сначала ноутбук я начала писать на английском в виду своего комфорта и терминологии, его можно найти <a href='https://www.kaggle.com/erelin6613/ai-in-game-industry'>здесь</a>*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Машинное обучение и искуственный интелект всё больше наполняют нашу жизнь. Реклама в Google? Рекомендации Amazon или Netflix? В конце концов, улучщенные опыт пользователей сайтов и игр? Думаю, ответ уже понятен. Кстати, мы уже видели, как ИИ победил грандмастера шахмат Гарри Каспарова. Следуюющее видео даст больше объяснений.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import YouTubeVideo\nYouTubeVideo('6vYJyOGKCHE', width=800, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ИИ не идеален, но он учиться\n\nБлагодаря платформе kaggle мы можем легко экспериментировать с парой идей касательно тренировки агентов. Я не буду нудить, но пару слов об игре Halite:\n> Halite by Two Sigma (\"Halite\") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.\n\n(Мой перевод)\n> Halite от Two Sigma (\"Halite\") игра с управлением ресурсами, в которой ты строишь небольшую армаду кораблей. Твои алгоритмы определяют их движение для сбора халитов, блестящие ресурсы энергии. Побеждает матч тот, кто собрал больше халитов, но ты решаешь какие ходы эффективные. Ты управляешь кораблём, строишь новые, создаёшь станции и собираешь возобновляющиеся халиты на игровой доске.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для начала стоит взглянуть на обычную игру, сыгранную случаным образом для понимания.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\nenv = make(\"halite\", debug=True)\nenv.run([\"random\", \"random\", \"random\", \"random\"])\nenv.render(mode=\"ipython\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nfrom kaggle_environments.envs.halite.helpers import *\n\nboard_size = 5\nenvironment = make(\"halite\", \n                   configuration={\n                       \"size\": board_size, \n                       \"startingHalite\": 1000})\nagent_count = 2\nenvironment.reset(agent_count)\nstate = environment.state[0]\n\nboard = Board(state.observation, \n              environment.configuration)\n\ndef move_ships_north_agent(\n    observation, configuration):\n    board = Board(\n        observation, configuration)\n    current_player = board.current_player\n    for ship in current_player.ships:\n        ship.next_action = ShipAction.NORTH\n    return current_player.next_actions\n\nenvironment.reset(agent_count)\nenvironment.run([move_ships_north_agent, \"random\"])\nenvironment.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь мы будем знать как контролировать агента.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile submission.py\nimport time\nimport copy\nimport sys\nimport math\nimport collections\nimport pprint\nimport numpy as np\nimport scipy.optimize\nimport scipy.ndimage\nfrom kaggle_environments.envs.halite.helpers import *\nimport kaggle_environments\nimport random\n\nCONFIG_MAX_SHIPS=20\nall_actions=[ShipAction.NORTH, ShipAction.EAST,ShipAction.SOUTH,ShipAction.WEST]\nall_dirs=[Point(0,1), Point(1,0), Point(0,-1), Point(-1,0)]\nstart=None\nnum_shipyard_targets=4\nsize=None\nship_target={}\nme=None\ndid_init=False\nquiet=False\nC=None\nclass Obj:\n  pass\nturn=Obj()\nturns_optimal=np.array(\n  [[0, 2, 3, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8],\n   [0, 1, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7],\n   [0, 0, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7],\n   [0, 0, 1, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6],\n   [0, 0, 0, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6],\n   [0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5],\n   [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n#### Functions\ndef print_enemy_ships(board):\n  print('\\nEnemy Ships')\n  for ship in board.ships.values():\n    if ship.player_id != me.id:\n      print('{:6}  {} halite {}'.format(ship.id,ship.position,ship.halite))\n      \ndef print_actions(board):\n  print('\\nShip Actions')\n  for ship in me.ships:\n    print('{:6}  {}  {} halite {}'.format(ship.id,ship.position,ship.next_action,ship.halite))\n  print('Shipyard Actions')\n  for sy in me.shipyards:\n    print('{:6}  {}  {}'.format(sy.id,sy.position,sy.next_action))\n\ndef print_none(*args):\n  pass\n\ndef compute_max_ships(step):\n  if step < 200:\n    return CONFIG_MAX_SHIPS\n  elif step < 300:\n    return CONFIG_MAX_SHIPS-2\n  elif step < 350:\n    return CONFIG_MAX_SHIPS-4\n  else:\n    return CONFIG_MAX_SHIPS-5\n\ndef set_turn_data(board):\n  turn.num_ships=len(me.ships)\n  turn.max_ships=compute_max_ships(board.step)\n  turn.total_halite=me.halite\n  turn.halite_matrix=np.reshape(board.observation['halite'], (board.configuration.size,board.configuration.size))\n  turn.num_shipyards=len(me.shipyards)\n  turn.EP,turn.EH,turn.ES=gen_enemy_halite_matrix(board)\n  turn.taken={}\n  turn.last_episode = (board.step == (board.configuration.episode_steps-2))\n  \ndef init(obs,config):\n  global size\n  global print\n  if hasattr(config,'myval') and config.myval==9 and not quiet:\n    pass\n  else:\n    print=print_none\n    pprint.pprint=print_none\n  size = config.size\n\ndef limit(x,a,b):\n  if x<a:\n    return a\n  if x>b:\n    return b\n  return x\n  \ndef num_turns_to_mine(C,H,rt_travel):\n  if C==0:\n    ch=0\n  elif H==0:\n    ch=turns_optimal.shape[0]\n  else:\n    ch=int(math.log(C/H)*2.5+5.5)\n    ch=limit(ch,0,turns_optimal.shape[0]-1)\n  rt_travel=int(limit(rt_travel,0,turns_optimal.shape[1]-1))\n  return turns_optimal[ch,rt_travel]\n\ndef halite_per_turn(carrying, halite,travel,min_mine=1):\n  turns=num_turns_to_mine(carrying,halite,travel)\n  if turns<min_mine:\n    turns=min_mine\n  mined=carrying+(1-.75**turns)*halite\n  return mined/(travel+turns), turns\n  \ndef move(pos, action):\n  ret=None\n  if action==ShipAction.NORTH:\n    ret=pos+Point(0,1)\n  if action==ShipAction.SOUTH:\n    ret=pos+Point(0,-1)\n  if action==ShipAction.EAST:\n    ret=pos+Point(1,0)\n  if action==ShipAction.WEST:\n    ret=pos+Point(-1,0)\n  if ret is None:\n    ret=pos\n  return ret % size\n\ndef dirs_to(p1, p2, size=21):\n  deltaX, deltaY=p2 - p1\n  if abs(deltaX)>size/2:\n    #we wrap around\n    if deltaX<0:\n      deltaX+=size\n    elif deltaX>0:\n      deltaX-=size\n  if abs(deltaY)>size/2:\n    #we wrap around\n    if deltaY<0:\n      deltaY+=size\n    elif deltaY>0:\n      deltaY-=size\n  ret=[]\n  if deltaX>0:\n    ret.append(ShipAction.EAST)\n  if deltaX<0:\n    ret.append(ShipAction.WEST)\n  if deltaY>0:\n    ret.append(ShipAction.NORTH)\n  if deltaY<0:\n    ret.append(ShipAction.SOUTH)\n  if len(ret)==0:\n    ret=[None]\n  return ret, (deltaX,deltaY)\n\ndef shipyard_actions():\n  for sy in me.shipyards:\n    if turn.num_ships < turn.max_ships:\n      if turn.total_halite >= 500 and sy.position not in turn.taken:\n        sy.next_action = ShipyardAction.SPAWN\n        turn.taken[sy.position]=1\n        turn.num_ships+=1\n        turn.total_halite-=500\n\ndef gen_enemy_halite_matrix(board):\n  EP=np.zeros((size,size))\n  EH=np.zeros((size,size))\n  ES=np.zeros((size,size))\n  for id,ship in board.ships.items():\n    if ship.player_id != me.id:\n      EH[ship.position.y,ship.position.x]=ship.halite\n      EP[ship.position.y,ship.position.x]=1\n  for id, sy in board.shipyards.items():\n    if sy.player_id != me.id:\n      ES[sy.position.y,sy.position.x]=1\n  return EP,EH,ES\n\ndef dist(a,b):\n  action,step=dirs_to(a, b, size=21) \n  return abs(step[0]) + abs(step[1])\n\ndef nearest_shipyard(pos):\n  mn=100\n  best_pos=None\n  for sy in me.shipyards:\n    d=dist(pos, sy.position)\n    if d<mn:\n      mn=d\n      best_pos=sy.position\n  return mn,best_pos\n  \ndef assign_targets(board,ships):\n  old_target=copy.copy(ship_target)\n  ship_target.clear()\n  if len(ships)==0:\n    return\n  halite_min=50\n  pts1=[]\n  pts2=[]\n  for pt,c in board.cells.items():\n    assert isinstance(pt,Point)\n    if c.halite > halite_min:\n      pts1.append(pt)\n  for sy in me.shipyards:\n    for i in range(num_shipyard_targets):\n      pts2.append(sy.position)\n  C=np.zeros((len(ships),len(pts1)+len(pts2)))\n  for i,ship in enumerate(ships):\n    for j,pt in enumerate(pts1+pts2):\n      d1=dist(ship.position,pt)\n      d2,shipyard_position=nearest_shipyard(pt)\n      if shipyard_position is None:\n        d2=1\n      my_halite=ship.halite\n      if j < len(pts1):\n        v, mining=halite_per_turn(my_halite,board.cells[pt].halite, d1+d2)\n      else:\n        if d1>0:\n          v=my_halite/d1\n        else:\n          v=0\n      if board.cells[pt].ship and board.cells[pt].ship.player_id != me.id:\n        enemy_halite=board.cells[pt].ship.halite\n        if enemy_halite <= my_halite:\n          v = -1000\n        else:\n          if d1<5:\n            v+= enemy_halite/(d1+1)\n      C[i,j]=v\n  print('C is {}'.format(C.shape))\n  row,col=scipy.optimize.linear_sum_assignment(C, maximize=True)\n  pts=pts1+pts2\n  for r,c in zip(row,col):\n    ship_target[ships[r].id]=pts[c]\n  print('\\nShip Targets')\n  print('Ship      position          target')\n  for id,t in ship_target.items():\n    st=''\n    ta=''\n    if board.ships[id].position==t:\n      st='MINE'\n    elif len(me.shipyards)>0 and t==me.shipyards[0].position:\n      st='SHIPYARD'\n    if id not in old_target or old_target[id] != ship_target[id]:\n      ta=' NEWTARGET'\n    print('{0:6}  at ({1[0]:2},{1[1]:2})  assigned ({2[0]:2},{2[1]:2}) h {3:3} {4:10} {5:10}'.format(\n      id, board.ships[id].position, t, board.cells[t].halite,st, ta))\n\n  return\n\ndef make_avoidance_matrix(myship_halite):\n  filter=np.array([[0,1,0],[1,1,1],[0,1,0]])\n  bad_ship=np.logical_and(turn.EH <= myship_halite,turn.EP)\n  avoid=scipy.ndimage.convolve(bad_ship, filter, mode='wrap',cval=0.0)\n  avoid=np.logical_or(avoid,turn.ES)\n  return avoid\n\ndef make_attack_matrix(myship_halite):\n  attack=np.logical_and(turn.EH > myship_halite,turn.EP)\n  return attack\n\ndef get_max_halite_ship(board, avoid_danger=True):\n  mx=-1\n  the_ship=None\n  for ship in me.ships:\n    x=ship.position.x\n    y=ship.position.y\n    avoid=make_avoidance_matrix(ship.halite)\n    if ship.halite>mx and (not avoid_danger or not avoid[y,x]):\n      mx=ship.halite\n      the_ship=ship\n  return the_ship\n\ndef remove_dups(p):\n  ret=[]\n  for x in p:\n    if x not in ret:\n      ret.append(x)\n  return ret\n\ndef matrix_lookup(matrix,pos):\n  return matrix[pos.y,pos.x]\n\ndef ship_converts(board):\n  if turn.num_shipyards==0 and not turn.last_episode:\n    mx=get_max_halite_ship(board)\n    if mx is not None:\n      if mx.halite + turn.total_halite > 500:\n        mx.next_action=ShipAction.CONVERT\n        turn.taken[mx.position]=1\n        turn.num_shipyards+=1\n        turn.total_halite-=500\n  for ship in me.ships:\n    if ship.next_action:\n      continue\n    avoid=make_avoidance_matrix(ship.halite)\n    z=[matrix_lookup(avoid,move(ship.position,a)) for a in all_actions]\n    if np.all(z) and ship.halite > 500:\n      ship.next_action=ShipAction.CONVERT\n      turn.taken[ship.position]=1\n      turn.num_shipyards+=1\n      turn.total_halite-=500\n      print('ship id {} no escape converting'.format(ship.id))\n    if turn.last_episode and ship.halite > 500:\n      ship.next_action=ShipAction.CONVERT\n      turn.taken[ship.position]=1\n      turn.num_shipyards+=1\n      turn.total_halite-=500\n      \ndef ship_moves(board):\n  ships=[ship for ship in me.ships if ship.next_action is None]\n  assign_targets(board,ships)\n  actions={}\n  for ship in ships:\n    if ship.id in ship_target:\n      a,delta = dirs_to(ship.position, ship_target[ship.id],size=size)\n      actions[ship.id]=a\n    else:\n      actions[ship.id]=[random.choice(all_actions)]\n      \n  for ship in ships:\n    action=None\n    x=ship.position\n    avoid=make_avoidance_matrix(ship.halite)\n    attack=make_attack_matrix(ship.halite)\n    action_list=actions[ship.id]+[None]+all_actions\n    for a in all_actions:\n      m=move(x,a)\n      if attack[m.y,m.x]:\n        print('ship id {} attacking {}'.format(ship.id,a))\n        action_list.insert(0,a)\n        break\n    action_list=remove_dups(action_list)\n    for a in action_list:\n      m=move(x,a)\n      if avoid[m.y,m.x]:\n        print('ship id {} avoiding {}'.format(ship.id,a))\n      if m not in turn.taken and not avoid[m.y,m.x]:\n        action=a\n        break\n    ship.next_action=action\n    turn.taken[m]=1\n    \ndef agent(obs, config):\n  global size\n  global start\n  global prev_board\n  global me\n  global did_init\n  #Do initialization 1 time\n  start_step=time.time()\n  if start is None:\n    start=time.time()\n  if not did_init:\n    init(obs,config)\n    did_init=True\n  board = Board(obs, config)\n  me=board.current_player\n  set_turn_data(board)\n  print('==== step {} sim {}'.format(board.step,board.step+1))\n  print('ships {} shipyards {}'.format(turn.num_ships,turn.num_shipyards))\n  print_enemy_ships(board)\n  ship_converts(board)\n  ship_moves(board)\n  shipyard_actions()\n  print_actions(board)\n  print('time this turn: {:8.3f} total elapsed {:8.3f}'.format(time.time()-start_step,time.time()-start))\n  return me.next_actions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([\"/kaggle/working/submission.py\", \"/kaggle/working/submission.py\"])\n\nenv.run([\"/kaggle/working/submission.py\", \"random\"])\nenv.render(mode=\"ipython\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Так мы собирём много ресурсов... Но нам и правда нужно писать столько кода? А если игра намного сложнее?\n\nДля нашего проекта мы хотим создать положительный опыт пользователя. Базисная идея (хотя бы для начала) - создание гибких агентов, которые будут поддерживать игру достаточно сложной для поддержания интереса пользователя, и в то же время не на столько сложной, чтобы отталкнуть этой же сложностью.\n\nСначала мы можем написать простые правила, допустим, если пользователь получает урон очень часто, мы ослабляем агентов. Однако, с ростом проекта нужно будет учитывать технические разности в мобильных устройствах, как часто пользователь останавливает игру, следить за его прогрессом. Это уже сложнее обработать, и наши агенты тоже будут расти и становиться сложнее.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Наш любимый проект: TimeHack","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://ibb.co/sRC0PBY\"><img src=\"https://i.ibb.co/dMtdchY/imgonline-com-ua-twotoone-2-UPOwp-B0-K2v.png\" alt=\"imgonline-com-ua-twotoone-2-UPOwp-B0-K2v\" border=\"0\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Наша игра развилась быстро и сложность её также выросла. По мере разработки колличество параметров, метрик и характеристик будет только расти. Потому уже сейчас стоит задуматься как контролировать компьютерных агентов.\n\nГлавная идея игры заключается в сражении с компьютерными агентами с некоторыми особенностями: игрок имеет шанс воскреснуть как раз за 5 секунд до его смерти и изменить ход событий. Это только добавляет сложности, однако.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Жестокая реальность: Разработчики с Марса, Дата Саинтисты с Венеры\n\nТрудности возникают на стыке двух разных сфер с багажем любимых инструментов: дата саинтисты чаще всего пользуются языками програмирования Python, R или Scala. Разработчики мобильных игр часто выбирают Unity.\n\nНу что ж, мы можем построить несколько базисных моделей, но замечаю: <br>1) сложности с интеграцией никуда не уходят <br>2) данные мы генерируем случайно, потому в них может напрочь отсутствовать какие-либо закономерности.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### В зависимости от подхода (классическое машинное или глубокое обучение) поддержка CUDA обязательно для Tensorflow <a href='https://developer.nvidia.com/cuda-toolkit-archive'> здесь можно найти CUDA toolkit</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Сценарий 1. Используем anaconda и классические подходы машинного обучения","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_curve\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assuming we have times damaged, time alive and speed of clicking as Nx3 matrix\n# and we are given some simple feedback 0 to degrade difficulty, 1 to upgrade\n# we will skip scaling step for now\nseed = 13\nN = 1000\ngame_stats = np.random.randint(0, 10, N*3).reshape(N, 3)\ntarget = np.random.choice([0, 1], size=N).reshape(N, 1)\nplt.hist(target);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    game_stats, target, test_size=0.25, random_state=seed)\n\nmodel = LogisticRegression()\nmodel = model.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy_score(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(roc_curve(y_test, preds)[0], label='false positive',);\nplt.plot(roc_curve(y_test, preds)[1], label='false negative');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Преимущества:** Быстрый алгоритм, не требует GPU<br>\n**Недостатки:** Алгоритм контролируемого обучения (необходимы данные с известными классами/значениями желаемого выхода), алгоритм не сможет выучить очень сложные зависимости.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Сценарий 2. Используем tensorflow многослойную модель глубокого обучения и (возможно) mlagents библиотека поможет разработчикам","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = K.Input(shape=(3,))\nx = K.layers.Dense(32, activation=tf.nn.relu)(inputs)\nx = K.layers.Dense(64, activation=tf.nn.relu)(x)\noutputs = K.layers.Dense(1, activation=tf.nn.softmax)(x)\nmodel = K.Model(inputs=inputs, outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nepochs = 5\nhistory = model.fit(X_train, y_train, epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_test)\naccuracy_score(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Преимущества:** Простой API, обычно алгоритмы глубокого обучения способны \"словить\" более сложные зависимости и связи в данных<br>\n**Недостатки:** Алгоритм контролируемого обучения (необходимы данные с известными классами), дольше разработка, с точки зрения дата саинтистов ограничивает набор инструментов (или же делает их разработку сложнее)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Сценарий 3. Колаборативная фильтрация\nМы можем смотреть на проблему как на проблему рекомендационной системы (по мнению автора, это самый оптимальный подход из всех). Такой подход потребует сбора данных игр других пользовалей (в алгоритме юзер-юзер), а вот отстутствие некоторых данных проблемой абсолютно не будет. Два слова о самом алгоритме: факторизация матриц.\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/4M6prjD/latentfactor.png\" alt=\"latentfactor\" border=\"0\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom surprise import SVD, accuracy, Reader, dataset\nfrom surprise.model_selection import cross_validate\n\nratings_dict = {'userID': np.random.randint(0, 1000, 100),\n                'itemID': np.random.randint(0, 100, 100), # say it is going to be some metric, hp, time_alive etc\n                'rating': np.random.randint(0, 5, 100)}\ndf = pd.DataFrame.from_dict(ratings_dict)\n\nreader = Reader(line_format='user item rating', rating_scale=(1, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UserSet(dataset.DatasetAutoFolds):\n    def __init__(self, df, reader):\n        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n                            zip(df['userID'], df['itemID'], df['rating'])]\n        self.reader=reader\n\ndata = UserSet(df, reader)\nalgo = SVD()\nperf = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)\nprint(perf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"На заметку: \"из коробки\" библиотека scikit-surprise предоставляет некий функционал для быстрого создания базовых алгоритмов. В случае невозможности интеграции алгоритм нужно будет писать с нуля (допустим, в том же самом tensorflow, который пока что лидирует по простоте деплоймента). В очень больших масштабах Spark фреймфорк позволяет упростить этот процесс, однако пути интеграции пока ещё не исследованы.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Преимущества:** Простой API, алгоритм позволяет работать даже с отсутствующими значениями матриц<br>\n**Недостатки:** Много програмных зависимостей, необходимость наличия уже собранных данных (и скорее всего нужно будет давать знать пользователям мы эти данные собираем)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"На данном этапе времени не так уж и много объяснять ещё одну идею, а уж тем более исплиментировать её. Потому ограничусь несколькими словами.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Сценарий 4. Копаем глубже в обучение с подкреплением\nЛучщим инструментом будет библиотека pytorch. В кратце, агенты будут стремиться увеличивать функцию вознаграждения в соответствии с правилами, которые мы определим изначально\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/M1myPzd/agent-diagram.png\" alt=\"agent-diagram\" border=\"0\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Куда дальше?\nА дальше стоит пределить внимание деплойменту: узнать как можно больше об интеграции МО алгоритмов на мобильные устройства (не смотря на мои исследования, пути интеграции оказались сложнее, чем я ожидала) и учиться самим как нам обучать агентов и улучшать себя и свои проекты.\n\nНа текущий момент могу выделить два вероятных кандидата для деплоймента: \n* <a href='https://ironpython.net/'>IronPython</a> (Позволит интегрировать только Python 2.7)\n* Связка библиотеки <a href='https://github.com/Unity-Technologies/ml-agents'>ml-agents</a> для Unity и tensorflow","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}