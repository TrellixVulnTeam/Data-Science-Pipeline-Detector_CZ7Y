{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"大本はこちら。\nhttps://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book/tree/master/program\n\n強化学習のコード、だいたい、Brain/Agent/AnvironmentとClassを作っています。\nここのコードは別れてませんが、別れたものを調べて載せます。\n\n\n![強化学習](https://i1.wp.com/www.tcom242242.net/wp-content/uploads/2019/10/ql.gif?resize=1089%2C514&ssl=1)"},{"metadata":{},"cell_type":"markdown","source":"## 変数の説明\n\n方策反復法<br>\n方策に従って行動してゴールにたどり着いたとき、早くゴールできたケースで実行した行動(action)は重要だと考え、<br>\nその行動を多く取り入れるように方策を更新する方法。<>br\n\n価値反復法<br>\nゴールから逆算して、ゴールの一つ手前、2つ手前の位置（状態）へとエージェントを順々に誘導してく方法。<br>\nここのコードは価値反復法。\n    \n\ns:<br>\n状態のこと。迷路だと場所。カートポールだと、棒の傾きや横の位置など。<br>\n\na:<br>\nアクションのこと。\n\npi:<br>\n方策のこと。Policyのギリシャ文字のπθ(s,a)。<br>\n状態sのときに行動aを採用する確率はθで決まる方策πに従うということ。<br>\n\nθ：<br>\n確率へと変換される値。なんのこっちゃいだが、割合にする前の値や、softmaxする前の値のこと。<br>\nニューラルネットの場合は、素子間の結合パラメータに対応。<br>\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Q-Learning\n\n参考URL<br>\nhttps://qiita.com/youichiro/items/416e0dd95881ed9f17ac\n\nQ = 行動価値関数\n\n次のcodeにあるように、ゴールは1、つまり確率100%を与えている。\n\n            ##===============================================================\n            ##===================      ここが重要      ========================\n            r = 1  # ゴールにたどり着いたなら報酬を与える\n            ##===============================================================\n\nゴールの1つ手前の状態を考えると、\nゴールへ向かう行動価値は100%、自然に他の行動は排除されるという事。\nゴール２つ手前の状態を考えると、1つ手前の状態に移れば１００％になるから、他の行動は排除される。\nということを繰り返していく。\n行動価値関数は、goalから逆算するときに、一定の割合で価値を減らす(未来の報酬が1なので、今は0.98といった具合)ので、実際は100%より少しずつ小さくなります。\nこのあたりは、\n\n    if s_next == 8:  # ゴールした場合\n        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n    else:\n        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\nで計算されています。\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\nimport matplotlib.cm as cm  # color map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 迷路の設定　:: 共通"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 初期位置での迷路の様子\n\n# 図を描く大きさと、図の変数名を宣言\nfig = plt.figure(figsize=(5, 5))\nax = plt.gca()\n\n# 赤い壁を描く\nplt.plot([1, 1], [0, 1], color='red', linewidth=2)\nplt.plot([1, 2], [2, 2], color='red', linewidth=2)\nplt.plot([2, 2], [2, 1], color='red', linewidth=2)\nplt.plot([2, 3], [1, 1], color='red', linewidth=2)\n\n# 状態を示す文字S0～S8を描く\nplt.text(0.5, 2.5, 'S0', size=14, ha='center')\nplt.text(1.5, 2.5, 'S1', size=14, ha='center')\nplt.text(2.5, 2.5, 'S2', size=14, ha='center')\nplt.text(0.5, 1.5, 'S3', size=14, ha='center')\nplt.text(1.5, 1.5, 'S4', size=14, ha='center')\nplt.text(2.5, 1.5, 'S5', size=14, ha='center')\nplt.text(0.5, 0.5, 'S6', size=14, ha='center')\nplt.text(1.5, 0.5, 'S7', size=14, ha='center')\nplt.text(2.5, 0.5, 'S8', size=14, ha='center')\nplt.text(0.5, 2.3, 'START', ha='center')\nplt.text(2.5, 0.3, 'GOAL', ha='center')\n\n# 描画範囲の設定と目盛りを消す設定\nax.set_xlim(0, 3)\nax.set_ylim(0, 3)\nplt.tick_params(axis='both', which='both', bottom='off', top='off',\n                labelbottom='off', right='off', left='off', labelleft='off')\n\n# 現在地S0に緑丸を描画する\nline, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 初期状態の設定 :: 共通"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 初期の方策を決定するパラメータtheta_0を設定\n\n# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\ntheta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n                    [np.nan, 1, np.nan, 1],  # s1\n                    [np.nan, np.nan, 1, 1],  # s2\n                    [1, 1, 1, np.nan],  # s3\n                    [np.nan, np.nan, 1, 1],  # s4\n                    [1, np.nan, np.nan, np.nan],  # s5\n                    [1, np.nan, np.nan, np.nan],  # s6\n                    [1, 1, np.nan, np.nan],  # s7、※s8はゴールなので、方策はなし\n                    ])\n\n# 方策パラメータtheta_0をランダム方策piに変換する関数の定義\ndef simple_convert_into_pi_from_theta(theta):\n    '''単純に割合を計算する'''\n\n    [m, n] = theta.shape  # thetaの行列サイズを取得\n    pi = np.zeros((m, n))\n    for i in range(0, m):\n        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 割合の計算\n\n    pi = np.nan_to_num(pi)  # nanを0に変換\n\n    return pi\n\n# ランダム行動方策pi_0を求める\npi_0 = simple_convert_into_pi_from_theta(theta_0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 初期設定"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 初期の行動価値関数Qを設定\n[a, b] = theta_0.shape  # 行と列の数をa, bに格納\nQ = np.random.rand(a, b) * theta_0 * 0.1\n# *theta0をすることで要素ごとに掛け算をし、Qの壁方向の値がnanになる","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 関数の実装\n\n[ε-greedy法](https://qiita.com/youichiro/items/416e0dd95881ed9f17ac#%CE%B5-%E3%82%B0%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E6%B3%95)<br>\n\n与えられた確立の中から最大のものを常に選ぶのではなく、たまにランダムに動くよ、ってことです。\nNNにも、この要素が入っていたと思います。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ε-greedy法を実装\ndef get_action(s, Q, epsilon, pi_0):\n    direction = [\"up\", \"right\", \"down\", \"left\"]\n\n    # 行動を決める\n    if np.random.rand() < epsilon:\n        # εの確率でランダムに動く\n        next_direction = np.random.choice(direction, p=pi_0[s, :])\n    else:\n        # Qの最大値の行動を採用する\n        next_direction = direction[np.nanargmax(Q[s, :])]\n\n    # 行動をindexに\n    if next_direction == \"up\":\n        action = 0\n    elif next_direction == \"right\":\n        action = 1\n    elif next_direction == \"down\":\n        action = 2\n    elif next_direction == \"left\":\n        action = 3\n\n    return action\n\n\ndef get_s_next(s, a, Q, epsilon, pi_0):\n    direction = [\"up\", \"right\", \"down\", \"left\"]\n    next_direction = direction[a]  # 行動aの方向\n\n    # 行動から次の状態を決める\n    if next_direction == \"up\":\n        s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n    elif next_direction == \"right\":\n        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n    elif next_direction == \"down\":\n        s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n    elif next_direction == \"left\":\n        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n\n    return s_next\n\n\n# Q学習による行動価値関数Qの更新\ndef Q_learning(s, a, r, s_next, Q, eta, gamma):\n\n    if s_next == 8:  # ゴールした場合\n        ## ゴールしているので、r=1と考えると、ゴール1手前のQ[s,a]を1に近づけようとしている。\n        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n\n    else:\n        ## ゴールしていないので、r=0と考えると、Q[s_next,:]とQ[s,a]を近づけようとしている。\n        ## ゴール2手前を考えると、ゴール1手前のQに近づけようとしている。\n        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n\n    return Q\n\n\n# Q学習で迷路を解く関数の定義、状態と行動の履歴および更新したQを出力\ndef goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n    s = 0  # スタート地点\n    a = a_next = get_action(s, Q, epsilon, pi)  # 初期の行動\n    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n\n    while (1):  # ゴールするまでループ\n        a = a_next  # 行動更新\n\n        s_a_history[-1][1] = a\n        # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n\n        s_next = get_s_next(s, a, Q, epsilon, pi)\n        # 次の状態を格納\n\n        s_a_history.append([s_next, np.nan])\n        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n\n        # 報酬を与え,　次の行動を求めます\n        if s_next == 8:\n            \n            ##===============================================================\n            ##===================      ここが重要      ========================\n            r = 1  # ゴールにたどり着いたなら報酬を与える\n            ##===============================================================\n            \n            a_next = np.nan\n        else:\n            r = 0\n            a_next = get_action(s_next, Q, epsilon, pi)\n            # 次の行動a_nextを求めます。\n\n        # 価値関数を更新\n        Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n\n        # 終了判定\n        if s_next == 8:  # ゴール地点なら終了\n            break\n        else:\n            s = s_next\n\n    return [s_a_history, Q]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q-Learning実行"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q学習で迷路を解く\n\neta = 0.1  # 学習率\ngamma = 0.9  # 時間割引率\nepsilon = 0.5  # ε-greedy法の初期値\nv = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\nis_continue = True\nepisode = 1\n\nV = []  # エピソードごとの状態価値を格納する\nV.append(np.nanmax(Q, axis=1))  # 状態ごとに行動価値の最大値を求める\n\nwhile is_continue:  # is_continueがFalseになるまで繰り返す\n    print(\"エピソード:\" + str(episode))\n\n    # ε-greedyの値を少しずつ小さくする\n    epsilon = epsilon / 2\n\n    # Q学習で迷路を解き、移動した履歴と更新したQを求める\n    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n\n    # 状態価値の変化\n    new_v = np.nanmax(Q, axis=1)  # 状態ごとに行動価値の最大値を求める\n    print(np.sum(np.abs(new_v - v)))  # 状態価値関数の変化を出力\n    v = new_v\n    V.append(v)  # このエピソード終了時の状態価値関数を追加\n\n    print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n\n    # 100エピソード繰り返す\n    episode = episode + 1\n    if episode > 100:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##　可視化"},{"metadata":{"trusted":true},"cell_type":"code","source":"def init():\n    # 背景画像の初期化\n    line.set_data([], [])\n    return (line,)\n\n\ndef animate(i):\n    # フレームごとの描画内容\n    # 各マスに状態価値の大きさに基づく色付きの四角を描画\n    line, = ax.plot([0.5], [2.5], marker=\"s\",\n                    color=cm.jet(V[i][0]), markersize=85)  # S0\n    line, = ax.plot([1.5], [2.5], marker=\"s\",\n                    color=cm.jet(V[i][1]), markersize=85)  # S1\n    line, = ax.plot([2.5], [2.5], marker=\"s\",\n                    color=cm.jet(V[i][2]), markersize=85)  # S2\n    line, = ax.plot([0.5], [1.5], marker=\"s\",\n                    color=cm.jet(V[i][3]), markersize=85)  # S3\n    line, = ax.plot([1.5], [1.5], marker=\"s\",\n                    color=cm.jet(V[i][4]), markersize=85)  # S4\n    line, = ax.plot([2.5], [1.5], marker=\"s\",\n                    color=cm.jet(V[i][5]), markersize=85)  # S5\n    line, = ax.plot([0.5], [0.5], marker=\"s\",\n                    color=cm.jet(V[i][6]), markersize=85)  # S6\n    line, = ax.plot([1.5], [0.5], marker=\"s\",\n                    color=cm.jet(V[i][7]), markersize=85)  # S7\n    line, = ax.plot([2.5], [0.5], marker=\"s\",\n                    color=cm.jet(1.0), markersize=85)  # S8\n    return (line,)\n\n\n#　初期化関数とフレームごとの描画関数を用いて動画を作成\nanim = animation.FuncAnimation(\n    fig, animate, init_func=init, frames=len(V), interval=200, repeat=False)\n\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}