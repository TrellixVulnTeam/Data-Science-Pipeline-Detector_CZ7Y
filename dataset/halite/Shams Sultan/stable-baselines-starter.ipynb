{"cells":[{"metadata":{"_uuid":"562609dc-afa8-4f21-8cbf-5b73568a266b","_cell_guid":"c5a40836-a978-433e-8eb4-e6ee34b993c7","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"!apt-get update -qq\n!apt-get install -qq -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!python -m pip install -q --upgrade pip\n!pip install -q --upgrade kaggle-environments\n!pip install -q 'tensorflow==1.15.0'\n!pip install -q 'stable-baselines[mpi]==2.10.0'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%writefile subproc_vec_env.py\n# Original Source: https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/vec_env/subproc_vec_env.py\n\n# Changes Made: Modified to preserve terminal observation\n\n# Original License:\n# The MIT License\n\n# Copyright (c) 2017 OpenAI (http://openai.com)\n# Copyright (c) 2018-2019 Stable-Baselines Team\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\n\nimport multiprocessing\nfrom collections import OrderedDict\nfrom typing import Sequence\n\nimport gym\nimport numpy as np\n\nfrom stable_baselines.common.vec_env.base_vec_env import VecEnv, CloudpickleWrapper\n\n\ndef _worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.var()\n    env_done = False\n    while True:\n        try:\n            cmd, data = remote.recv()\n            if cmd == 'step':\n                if env_done:\n                    observation = env.reset()\n                    reward, done, info = 0, False, {}\n                    env_done = False\n                else:\n                    observation, reward, done, info = env.step(data)\n                    env_done = done\n                # save final observation where user can get it, then reset\n                # info['terminal_observation'] = observation\n                # observation = env.reset()\n                remote.send((observation, reward, done, info))\n            elif cmd == 'seed':\n                remote.send(env.seed(data))\n            elif cmd == 'reset':\n                observation = env.reset()\n                env_done = False\n                remote.send(observation)\n            elif cmd == 'render':\n                remote.send(env.render(data))\n            elif cmd == 'close':\n                env.close()\n                remote.close()\n                break\n            elif cmd == 'get_spaces':\n                remote.send((env.observation_space, env.action_space))\n            elif cmd == 'env_method':\n                method = getattr(env, data[0])\n                remote.send(method(*data[1], **data[2]))\n            elif cmd == 'get_attr':\n                remote.send(getattr(env, data))\n            elif cmd == 'set_attr':\n                remote.send(setattr(env, data[0], data[1]))\n            else:\n                raise NotImplementedError(\"`{}` is not implemented in the worker\".format(cmd))\n        except EOFError:\n            break\n\n\nclass SubprocVecEnv(VecEnv):\n    \"\"\"\n    Creates a multiprocess vectorized wrapper for multiple environments, distributing each environment to its own\n    process, allowing significant speed up when the environment is computationally complex.\n    For performance reasons, if your environment is not IO bound, the number of environments should not exceed the\n    number of logical cores on your CPU.\n    .. warning::\n        Only 'forkserver' and 'spawn' start methods are thread-safe,\n        which is important when TensorFlow sessions or other non thread-safe\n        libraries are used in the parent (see issue #217). However, compared to\n        'fork' they incur a small start-up cost and have restrictions on\n        global variables. With those methods, users must wrap the code in an\n        ``if __name__ == \"__main__\":`` block.\n        For more information, see the multiprocessing documentation.\n    :param env_fns: ([callable]) A list of functions that will create the environments\n        (each callable returns a `Gym.Env` instance when called).\n    :param start_method: (str) method used to start the subprocesses.\n           Must be one of the methods returned by multiprocessing.get_all_start_methods().\n           Defaults to 'forkserver' on available platforms, and 'spawn' otherwise.\n    \"\"\"\n\n    def __init__(self, env_fns, start_method=None):\n        self.waiting = False\n        self.closed = False\n        n_envs = len(env_fns)\n\n        if start_method is None:\n            # Fork is not a thread safe method (see issue #217)\n            # but is more user friendly (does not require to wrap the code in\n            # a `if __name__ == \"__main__\":`)\n            forkserver_available = 'forkserver' in multiprocessing.get_all_start_methods()\n            start_method = 'forkserver' if forkserver_available else 'spawn'\n        ctx = multiprocessing.get_context(start_method)\n\n        self.remotes, self.work_remotes = zip(*[ctx.Pipe(duplex=True) for _ in range(n_envs)])\n        self.processes = []\n        for work_remote, remote, env_fn in zip(self.work_remotes, self.remotes, env_fns):\n            args = (work_remote, remote, CloudpickleWrapper(env_fn))\n            # daemon=True: if the main process crashes, we should not cause things to hang\n            process = ctx.Process(target=_worker, args=args, daemon=True)  # pytype:disable=attribute-error\n            process.start()\n            self.processes.append(process)\n            work_remote.close()\n\n        self.remotes[0].send(('get_spaces', None))\n        observation_space, action_space = self.remotes[0].recv()\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send(('step', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return _flatten_obs(obs, self.observation_space), np.stack(rews), np.stack(dones), infos\n\n    def seed(self, seed=None):\n        for idx, remote in enumerate(self.remotes):\n            remote.send(('seed', seed + idx))\n        return [remote.recv() for remote in self.remotes]\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send(('reset', None))\n        obs = [remote.recv() for remote in self.remotes]\n        return _flatten_obs(obs, self.observation_space)\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send(('close', None))\n        for process in self.processes:\n            process.join()\n        self.closed = True\n\n    def get_images(self) -> Sequence[np.ndarray]:\n        for pipe in self.remotes:\n            # gather images from subprocesses\n            # `mode` will be taken into account later\n            pipe.send(('render', 'rgb_array'))\n        imgs = [pipe.recv() for pipe in self.remotes]\n        return imgs\n\n    def get_attr(self, attr_name, indices=None):\n        \"\"\"Return attribute from vectorized environment (see base class).\"\"\"\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send(('get_attr', attr_name))\n        return [remote.recv() for remote in target_remotes]\n\n    def set_attr(self, attr_name, value, indices=None):\n        \"\"\"Set attribute inside vectorized environments (see base class).\"\"\"\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send(('set_attr', (attr_name, value)))\n        for remote in target_remotes:\n            remote.recv()\n\n    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n        \"\"\"Call instance methods of vectorized environments.\"\"\"\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send(('env_method', (method_name, method_args, method_kwargs)))\n        return [remote.recv() for remote in target_remotes]\n\n    def _get_target_remotes(self, indices):\n        \"\"\"\n        Get the connection object needed to communicate with the wanted\n        envs that are in subprocesses.\n        :param indices: (None,int,Iterable) refers to indices of envs.\n        :return: ([multiprocessing.Connection]) Connection object to communicate between processes.\n        \"\"\"\n        indices = self._get_indices(indices)\n        return [self.remotes[i] for i in indices]\n\n\ndef _flatten_obs(obs, space):\n    \"\"\"\n    Flatten observations, depending on the observation space.\n    :param obs: (list<X> or tuple<X> where X is dict<ndarray>, tuple<ndarray> or ndarray) observations.\n                A list or tuple of observations, one per environment.\n                Each environment observation may be a NumPy array, or a dict or tuple of NumPy arrays.\n    :return (OrderedDict<ndarray>, tuple<ndarray> or ndarray) flattened observations.\n            A flattened NumPy array or an OrderedDict or tuple of flattened numpy arrays.\n            Each NumPy array has the environment index as its first axis.\n    \"\"\"\n    assert isinstance(obs, (list, tuple)), \"expected list or tuple of observations per environment\"\n    assert len(obs) > 0, \"need observations from at least one environment\"\n\n    if isinstance(space, gym.spaces.Dict):\n        assert isinstance(space.spaces, OrderedDict), \"Dict space must have ordered subspaces\"\n        assert isinstance(obs[0], dict), \"non-dict observation for environment with Dict observation space\"\n        return OrderedDict([(k, np.stack([o[k] for o in obs])) for k in space.spaces.keys()])\n    elif isinstance(space, gym.spaces.Tuple):\n        assert isinstance(obs[0], tuple), \"non-tuple observation for environment with Tuple observation space\"\n        obs_len = len(space.spaces)\n        return tuple((np.stack([o[i] for o in obs]) for i in range(obs_len)))\n    else:\n        return np.stack(obs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d202a3-4a06-48c2-a96b-e02bd7f26fcd","_cell_guid":"411fe350-c7e4-410b-9e65-2d3c6a594370","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom collections import OrderedDict\n\nfrom matplotlib import pyplot as plt\n\nfrom stable_baselines import PPO2, results_plotter\nfrom stable_baselines.bench import Monitor\nfrom stable_baselines.common.callbacks import BaseCallback\nfrom stable_baselines.common.env_checker import check_env\nfrom stable_baselines.common.policies import MlpPolicy\n# from stable_baselines.common.vec_env import SubprocVecEnv  # issue with terminal observation\nfrom subproc_vec_env import SubprocVecEnv\n\nfrom gym import Env, spaces\nfrom kaggle_environments import make, evaluate\nfrom kaggle_environments.envs.halite.helpers import Board, Point\n\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30ab56ee-0486-4a93-aa0a-a217f61f527a","_cell_guid":"71861e5d-f498-4f15-bec0-9bac7d30d4b7","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"N_CPU = os.cpu_count()\nprint('CPU Cores =', N_CPU)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ad4323c-ca34-40e3-926a-e242dc9be3b1","_cell_guid":"a2e217b0-e23c-42ed-838e-58300a5826da","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"LOG_DIR = './log/'\nAGENT_DIR = '../input/swarm-intelligence-with-sdk/'\nMODEL_DIR = '../input/stable-baselines-starter'\nMODEL_FILE = 'halite.pkl'\n\nos.makedirs(LOG_DIR, exist_ok=True)\n\nagents = {'idle_bot.py': 'idle.py',\n          'beetle_bot.py': 'beetle.py', \n          'duo_bot.py': 'duo.py', \n          'submission.py': 'swarm.py', \n          'attack_bot.py': 'attack.py'}\n\nfor infile, outfile in agents.items():\n    with open(os.path.join(AGENT_DIR, infile), 'rt') as f:\n        agent_src = f.read()\n\n    with open(outfile, 'wt') as f:\n        f.write(agent_src)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83412b94-eb7f-4037-bd73-88826a030d6a","_cell_guid":"2c5012d0-eeac-45ee-a356-64e6cde23113","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# GAME_CONFIG = {'episodeSteps': 400, 'size': 21, 'num_agents': 4}\nGAME_CONFIG = {'episodeSteps': 100, 'size': 11, 'num_agents': 4}\n\n# GAME_AGENTS = ['random', 'idle.py', 'beetle.py', 'duo.py', 'swarm.py', 'attack.py']\nGAME_AGENTS = ['idle.py'] * 4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65f6d66b-8fa3-4859-bdac-fd2c5c89b629","_cell_guid":"44f36883-822b-41c0-84fb-0922430a656b","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def sort_cells(cells):\n    ordered_cells = OrderedDict()\n    size = int(len(cells) ** 0.5)\n        \n    for x in range(size):\n        for y in range(size):\n            point = Point(x, y)\n            ordered_cells[point] = cells[point]\n    return ordered_cells","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b17c795-2bd6-4e4f-a747-f233195896df","_cell_guid":"c5586ecf-916c-416d-9823-d6757a17ccbc","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"SHIP_ACTIONS = [None, 'CONVERT', 'NORTH', 'EAST', 'SOUTH', 'WEST']\nYARD_ACTIONS = [None, 'SPAWN']\n\nN_SHIP_ACTIONS = len(SHIP_ACTIONS)\nN_YARD_ACTIONS = len(YARD_ACTIONS)\n\nMAX_SHIPS = 5\nMAX_YARDS = 5\n\ndef transform_actions(actions, obs, config):\n    next_actions = dict()\n    \n    board = Board(obs, config)\n    me = board.current_player\n    \n    board_cells = sort_cells(board.cells)\n    \n    si = 0\n    yi = MAX_SHIPS\n    \n    for _, c in board_cells.items():\n        if c.ship in me.ships and si < MAX_SHIPS:\n            i = actions[si]\n            ship_action = SHIP_ACTIONS[i]\n            si += 1\n\n            if ship_action is not None:\n                next_actions[c.ship.id] = ship_action\n                    \n        if c.shipyard in me.shipyards and yi < MAX_SHIPS + MAX_YARDS:\n            i = actions[yi]\n            yard_action = YARD_ACTIONS[i]\n            yi += 1\n\n            if yard_action is not None:\n                next_actions[c.shipyard.id] = yard_action\n        \n    return next_actions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"992a5676-1b0d-4926-8fea-6df7a875bcbb","_cell_guid":"acf40c34-c676-42fd-a37b-0f30b2f1ce42","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"N_FEATURES = 14\nMAX_SHIP_HALITE = 500\nMAX_PLAYER_HALITE = 1000\nMAX_DIFF_HALITE = 500\n\ndef transform_observation(done, obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    board_cells = sort_cells(board.cells)\n\n    step = []\n    final_step = []\n    halite = []\n    cargo = []\n    halite_diff = []\n    cargo_diff = []\n    cell_yield = []\n    me_yard = []\n    me_ship = []\n    me_ship_cargo = []\n    opp_yard = []\n    opp_ship = []\n    opp_ship_cargo = []\n    directions = []\n        \n    halite_val = me.halite / MAX_PLAYER_HALITE\n    cargo_val = sum(s.halite for s in me.ships) / MAX_PLAYER_HALITE\n    \n    halite_diff_val = me.halite - max(p.halite for p in board.opponents)\n    halite_diff_val = (halite_diff_val + MAX_DIFF_HALITE) / (2 * MAX_DIFF_HALITE)\n\n    cargo_diff_val = (sum(s.halite for s in me.ships) -\n                      max(sum(s.halite for s in p.ships) for p in board.opponents))\n    cargo_diff_val = (cargo_diff_val + MAX_DIFF_HALITE) / (2 * MAX_DIFF_HALITE)\n    \n    for _, c in board_cells.items():\n        step.append(obs['step'] / config.episodeSteps)\n        final_step.append(int(done))\n        \n        halite.append(halite_val)\n        cargo.append(cargo_val)\n        halite_diff.append(halite_diff_val)\n        cargo_diff.append(cargo_diff_val)\n        \n        cell_yield.append(0 if c.halite < 4 else c.halite / config.maxCellHalite)\n        \n        if c.ship is None:\n            me_ship.append(0)\n            me_ship_cargo.append(0)\n            opp_ship.append(0)\n            opp_ship_cargo.append(0)\n            \n        elif c.ship in me.ships:\n            me_ship.append(1)\n            me_ship_cargo.append(c.ship.halite / MAX_SHIP_HALITE)\n            opp_ship.append(0)\n            opp_ship_cargo.append(0)\n            \n        else:\n            me_ship.append(0)\n            me_ship_cargo.append(0)\n            opp_ship.append(1)\n            opp_ship_cargo.append(c.ship.halite / MAX_SHIP_HALITE)\n\n        if c.shipyard is None:\n            me_yard.append(0)\n            opp_yard.append(0)\n        \n        elif c.shipyard in me.shipyards:\n            me_yard.append(1)\n            opp_yard.append(0)\n            \n        else:\n            me_yard.append(0)\n            opp_yard.append(1)\n        \n        if (c.ship in me.ships or \n            c.north.ship in me.ships or\n            c.east.ship in me.ships or\n            c.south.ship in me.ships or\n            c.west.ship in me.ships):\n            directions.append(1)\n        else:\n            directions.append(0)\n            \n    x_obs = np.vstack((step,\n                       final_step,\n                       halite,\n                       cargo,\n                       halite_diff,\n                       cargo_diff,\n                       cell_yield, \n                       me_yard, \n                       me_ship, \n                       me_ship_cargo, \n                       opp_yard, \n                       opp_ship, \n                       opp_ship_cargo,\n                       directions))\n    \n    x_obs = x_obs.reshape(config.size, config.size, N_FEATURES)\n    x_obs = x_obs.astype(np.float32).clip(0, 1)\n    \n    return x_obs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d330f4b-d907-4444-99e0-95bd7fa1f897","_cell_guid":"d871f7b6-3316-4daa-926d-d81d97a90174","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"REWARD_WON = GAME_CONFIG['episodeSteps']\nREWARD_LOST = -REWARD_WON\n\nMAX_DELTA = 500\n\ndef transform_reward(done, last_obs, obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    nships = len(me.ships)\n    nyards = len(me.shipyards)\n    halite = me.halite\n    cargo = sum(s.halite for s in me.ships)\n    \n    if nships == 0:\n        if nyards == 0:\n            return REWARD_LOST\n        \n        if halite < config.spawnCost:\n            return REWARD_LOST\n    \n    if done:\n        scores = [p.halite for p in board.players.values() if \n                  len(p.ships) > 0 or\n                  (len(p.shipyards) > 0 and p.halite >= config.spawnCost)]\n        \n        if halite == max(scores):\n            if scores.count(halite) == 1:\n                return REWARD_WON\n        return REWARD_LOST\n        \n    delta = 0\n    \n    if last_obs is not None:\n        last_board = Board(last_obs, config)\n        last_me = last_board.current_player\n        \n        last_nships = len(last_me.ships)\n        last_nyards = len(last_me.shipyards)\n        last_halite = last_me.halite\n        last_cargo = sum(s.halite for s in last_me.ships)\n        \n        delta_ships = (nships - last_nships) * config.spawnCost\n        delta_yards = (nyards - last_nyards) * (config.convertCost + config.spawnCost)\n        delta_halite = halite - last_halite\n        delta_cargo = cargo - last_cargo\n    \n        delta = delta_ships + delta_yards + delta_halite + delta_cargo        \n        \n        if delta_halite > 0:\n            delta += MAX_DELTA\n            \n        if delta_cargo > 0:\n            delta += MAX_DELTA // 2\n            \n        if nyards == 0:\n            delta -= MAX_DELTA\n            \n        if nships == 0:\n            delta -= MAX_DELTA\n            \n        delta = float(np.clip(delta / MAX_DELTA, -1, 1))\n    \n    reward = delta + 1 / MAX_DELTA\n    return reward","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"303727ee-4982-4390-8b55-b178c3276445","_cell_guid":"ee1bda15-623c-4a8a-b6ed-4c5676d5c5bd","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def get_actions(model, obs, config, deterministic=False):\n    x_obs = transform_observation(False, obs, config)\n    actions, state = model.predict(x_obs, deterministic=deterministic)\n    next_actions = transform_actions(actions, obs, config)\n    return next_actions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HaliteGym(Env):\n    def __init__(self, config={}):\n        self.agents = GAME_AGENTS\n\n        size = config.get('size', 21)\n        num_agents = config.get('num_agents', 4)\n        episodeSteps = config.get('episodeSteps', 400)\n\n        cfg = {'size': size, 'num_agents': num_agents, 'episodeSteps': episodeSteps}\n\n        self.halite_env = make('halite', configuration=cfg, debug=True)\n        self.config = self.halite_env.configuration\n\n        self.env = None\n        self.obs = self.reset_env()\n        self.last_obs = None\n\n        self.action_space = spaces.MultiDiscrete([N_SHIP_ACTIONS] * MAX_SHIPS +\n                                                 [N_YARD_ACTIONS] * MAX_YARDS)\n\n        self.observation_space = spaces.Box(low=0, high=1, \n                                            shape=(self.config.size, \n                                                   self.config.size, \n                                                   N_FEATURES), \n                                            dtype=np.float32)\n\n        self.reward_range = (REWARD_LOST, REWARD_WON)\n\n        self.spec = None\n        self.metadata = None\n\n    def reset_env(self):       \n        game_agents = random.sample(self.agents, self.config.num_agents-1)\n        position = random.randint(0, self.config.num_agents-1) \n        game_agents.insert(position, None)\n        self.env = self.halite_env.train(game_agents)\n        return self.env.reset()\n\n    def reset(self):\n        self.obs = self.reset_env()\n        self.last_obs = None\n        x_obs = transform_observation(False, self.obs, self.config)\n        return x_obs\n\n    def step(self, actions):\n        next_actions = transform_actions(actions, self.obs, self.config)\n\n        self.last_obs = self.obs\n        self.obs, reward, done, info = self.env.step(next_actions)\n\n        x_obs = transform_observation(done, self.obs, self.config)\n        x_reward = transform_reward(done, self.last_obs, self.obs, self.config)\n\n        if x_reward <= REWARD_LOST:\n            done, info = True, {}\n\n        return x_obs, x_reward, done, info\n\n# Multi-Agent Environment: \n# https://github.com/openai/multiagent-particle-envs/blob/master/multiagent/environment.py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20b5f31a-21ed-41de-8013-1db0a8d9ca7d","_cell_guid":"10708585-b614-4db5-9309-0e85a3cee965","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"test_env = HaliteGym()\ncheck_env(test_env)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87fc2f0d-9751-4a9e-b57d-02b1ad8f0fef","_cell_guid":"8152ddfe-6cda-4c74-93ed-9e522eb83082","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def make_env(config, rank=0):\n    def _init():\n        env = HaliteGym(config)\n        log_file = os.path.join(LOG_DIR, str(rank))\n        env = Monitor(env, log_file, allow_early_resets=True)\n        return env\n    return _init","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = SubprocVecEnv([make_env(GAME_CONFIG, i) for i in range(N_CPU)])\n# env = Monitor(HaliteGym(GAME_CONFIG), LOG_DIR, allow_early_resets=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85427563-bbaa-44f8-a4f4-b1d0690dafb1","_cell_guid":"d399e0cd-fb14-4bb5-8d56-dc47ee0a6b17","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"model_path = os.path.join(MODEL_DIR, MODEL_FILE)\n\nif os.path.isfile(model_path):\n    print('Loading model')\n    model = PPO2.load(model_path)\n    model.set_env(env)\n    \nelse:\n    print('Making model')\n    model = PPO2(policy=MlpPolicy, \n                 env=env, \n                 n_cpu_tf_sess=N_CPU, \n                 verbose=0, \n                 n_steps=GAME_CONFIG['episodeSteps'], \n                 nminibatches=N_CPU, \n                 noptepochs=4, \n                 seed=None,\n                 _init_setup_model=True, \n                 learning_rate=0.00025,\n                 gamma=0.99, \n                 ent_coef=0.01,  \n                 vf_coef=0.5, \n                 max_grad_norm=0.5, \n                 lam=0.95, \n                 cliprange=0.2, \n                 cliprange_vf=None,\n                 policy_kwargs=None, \n                 tensorboard_log=None, \n                 full_tensorboard_log=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd850478-aa1e-4a60-9fe3-112a1df07804","_cell_guid":"80e39ea5-5248-49e9-82de-01a84055b4df","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class ProgressBar(BaseCallback):\n    def __init__(self, verbose=0):\n        super(ProgressBar, self).__init__(verbose)\n        self.pbar = None\n\n    def _on_training_start(self):\n        self.pbar = tqdm(total=self.locals['n_updates'])\n\n    def _on_rollout_start(self):\n        self.pbar.refresh()\n\n    def _on_step(self):\n        return True\n\n    def _on_rollout_end(self):\n        self.pbar.update()\n\n    def _on_training_end(self):\n        self.pbar.close()\n        self.pbar = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bed6dbfc-70f3-4fcc-b36e-302954e428e2","_cell_guid":"8e243a99-c8a4-4b1c-a4e1-48025ba44a92","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"TIMESTEPS = 400000\nprogressbar = ProgressBar()\nmodel = model.learn(total_timesteps=TIMESTEPS, callback=progressbar)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model.save(MODEL_FILE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63401b77-d2b3-4e1b-a306-0a10bc190d77","_cell_guid":"112eb18e-1b19-4625-9ebf-cadb8f921208","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.style.use(['seaborn-whitegrid'])\n\nresults_plotter.plot_results([LOG_DIR], TIMESTEPS, \n                             results_plotter.X_TIMESTEPS, 'Halite Timesteps')\n\nresults_plotter.plot_results([LOG_DIR], TIMESTEPS, \n                             results_plotter.X_EPISODES, 'Halite Episodes')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a66905e5-0ce9-4bed-8d78-b7d0798cb81f","_cell_guid":"780ef826-91b7-4339-b0cb-97e46e9851e6","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"log_files = [os.path.join(LOG_DIR, f'{i}.monitor.csv') for i in range(N_CPU)]\n# log_files = [os.path.join(LOG_DIR, 'monitor.csv')]\n\nfor i, log_file in enumerate(log_files):\n  if os.path.isfile(log_file):\n    df = pd.read_csv(log_file, skiprows=1)\n\n    fig = plt.figure(figsize=(8, 2))\n    plt.subplot(1, 2, 1, label=log_file)\n    df['r'].rolling(window=TIMESTEPS//1000).mean().plot(title=f'Rewards {i}')\n\n    plt.subplot(1, 2, 2, label=log_file)\n    df['l'].rolling(window=TIMESTEPS//1000).mean().plot(title=f'Lengths {i}')\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def run_test(model, deterministic=False):\n    env = make('halite', configuration=GAME_CONFIG, debug=True)\n    config = env.configuration\n\n    game_agents = random.sample(GAME_AGENTS, config.num_agents-1)\n    position = random.randint(0, config.num_agents-1) \n    game_agents.insert(position, None)\n    print('Agents:', game_agents)\n    \n    trainer = env.train(game_agents)\n    obs = trainer.reset()\n\n    while not env.done:\n        actions = get_actions(model, obs, config, deterministic=deterministic)\n        obs, reward, done, info = trainer.step(actions)\n    \n    env.render(mode='ipython', width=640, height=480)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ccf0b0b-d267-4d2e-a6ec-bbe23fffcf5a","_cell_guid":"bb4f6a71-f59e-468e-9841-dfe3c1f0e10f","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"run_test(model, deterministic=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test(model, deterministic=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}