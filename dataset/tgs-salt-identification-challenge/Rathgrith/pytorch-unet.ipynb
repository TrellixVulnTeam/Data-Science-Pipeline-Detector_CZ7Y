{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q imutils","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:57:49.206861Z","iopub.execute_input":"2022-04-19T13:57:49.207657Z","iopub.status.idle":"2022-04-19T13:58:00.307301Z","shell.execute_reply.started":"2022-04-19T13:57:49.207569Z","shell.execute_reply":"2022-04-19T13:58:00.306475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch U-net implementation report","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:00.309582Z","iopub.execute_input":"2022-04-19T13:58:00.309847Z","iopub.status.idle":"2022-04-19T13:58:00.313475Z","shell.execute_reply.started":"2022-04-19T13:58:00.309809Z","shell.execute_reply":"2022-04-19T13:58:00.3128Z"}}},{"cell_type":"markdown","source":"* This is the implementation for the challenge https://www.kaggle.com/competitions/tgs-salt-identification-challenge\n* You may access this notebook online via: https://www.kaggle.com/code/rathgrith/pytorch-unet","metadata":{}},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"# import the necessary packages\nimport os\nimport time\n\nimport cv2\nimport zipfile\n\nimport torch\nfrom torch.nn import ReLU\nfrom torch.nn import Conv2d\nfrom torch.nn import Module\nfrom torch.optim import Adam\nfrom torch.nn import MaxPool2d\nfrom torch.nn import ModuleList\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.nn import ConvTranspose2d\nfrom torch.nn import functional as F\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import CenterCrop\n\nfrom sklearn.model_selection import train_test_split\n\n\nfrom imutils import paths\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-19T13:58:00.314768Z","iopub.execute_input":"2022-04-19T13:58:00.315214Z","iopub.status.idle":"2022-04-19T13:58:02.959303Z","shell.execute_reply.started":"2022-04-19T13:58:00.315177Z","shell.execute_reply":"2022-04-19T13:58:02.958439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Files are then loaded after the packages imported.","metadata":{}},{"cell_type":"code","source":"z= zipfile.ZipFile('../input/tgs-salt-identification-challenge/competition_data.zip')\nz.extractall()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:02.964567Z","iopub.execute_input":"2022-04-19T13:58:02.96681Z","iopub.status.idle":"2022-04-19T13:58:13.402038Z","shell.execute_reply.started":"2022-04-19T13:58:02.966768Z","shell.execute_reply":"2022-04-19T13:58:13.401253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base path of the dataset\nDATASET_PATH = \"./competition_data/train/\"\n\n# define the path to the images and masks dataset\nIMAGE_DATASET_PATH = os.path.join(DATASET_PATH, \"images\")\nMASK_DATASET_PATH = os.path.join(DATASET_PATH, \"masks\")\n\n# define the test split\nTEST_SPLIT = 0.15\n\n# determine the device to be used for training and evaluation\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# determine if we will be pinning memory during data loading\nPIN_MEMORY = True if DEVICE == \"cuda\" else False","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:13.403246Z","iopub.execute_input":"2022-04-19T13:58:13.403504Z","iopub.status.idle":"2022-04-19T13:58:13.467104Z","shell.execute_reply.started":"2022-04-19T13:58:13.40347Z","shell.execute_reply":"2022-04-19T13:58:13.466225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nim = Image.open(IMAGE_DATASET_PATH + \"/7cc53fe88b.png\")\nplt.subplot(2,2,1)\nplt.imshow(im)\nim = Image.open(MASK_DATASET_PATH + \"/7cc53fe88b.png\")\nplt.subplot(2,2,2)\nplt.imshow(im)\nim = Image.open(IMAGE_DATASET_PATH + \"/9ca520f895.png\")\nplt.subplot(2,2,3)\nplt.imshow(im)\nim = Image.open(MASK_DATASET_PATH + \"/9ca520f895.png\")\nplt.subplot(2,2,4)\nplt.imshow(im)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:13.468335Z","iopub.execute_input":"2022-04-19T13:58:13.468808Z","iopub.status.idle":"2022-04-19T13:58:14.071215Z","shell.execute_reply.started":"2022-04-19T13:58:13.468769Z","shell.execute_reply":"2022-04-19T13:58:14.07025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls ./competition_data/train/images\n# !ls ./competition_data/train/masks","metadata":{"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T13:58:14.074172Z","iopub.execute_input":"2022-04-19T13:58:14.074597Z","iopub.status.idle":"2022-04-19T13:58:14.078158Z","shell.execute_reply.started":"2022-04-19T13:58:14.074555Z","shell.execute_reply":"2022-04-19T13:58:14.077252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# define the number of channels in the input, number of classes,\n# and number of levels in the U-Net model\nNUM_CHANNELS = 1\nNUM_CLASSES = 1\nNUM_LEVELS = 3\n\n# initialize learning rate, number of epochs to train for, and the batch size\nINIT_LR = 0.001\nNUM_EPOCHS = 40\nBATCH_SIZE = 64\n\n# define the input image dimensions\n# Crop\nINPUT_IMAGE_WIDTH = 128\nINPUT_IMAGE_HEIGHT = 128\n\n# define threshold to filter weak predictions\nTHRESHOLD = 0.5\n\n# define the path to the base output directory\nBASE_OUTPUT = \"output\"\n\n# define the path to the output serialized model, model training plot, and testing image paths\nMODEL_PATH = \"unet_tgs_salt.pth\"\nPLOT_PATH = \"plot.png\"\nTEST_PATHS = \"test_paths.txt\"","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.079485Z","iopub.execute_input":"2022-04-19T13:58:14.079955Z","iopub.status.idle":"2022-04-19T13:58:14.089146Z","shell.execute_reply.started":"2022-04-19T13:58:14.079899Z","shell.execute_reply":"2022-04-19T13:58:14.088405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n\tdef __init__(self, imagePaths, maskPaths, transforms):\n\t\t# store the image and mask filepaths, and augmentation transforms\n\t\tself.imagePaths = imagePaths\n\t\tself.maskPaths = maskPaths\n\t\tself.transforms = transforms\n        \n\tdef __len__(self):\n\t\t# return the number of total samples contained in the dataset\n\t\treturn len(self.imagePaths)\n    \n\tdef __getitem__(self, idx):\n\t\t# grab the image path from the current index\n\t\timagePath = self.imagePaths[idx]\n        \n\t\t# load the image from disk, swap its channels from BGR to RGB,\n\t\t# and read the associated mask from disk in grayscale mode\n\t\timage = cv2.imread(imagePath)\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\tmask = cv2.imread(self.maskPaths[idx], 0)\n        \n\t\t# check to see if we are applying any transformations\n\t\tif self.transforms is not None:\n\t\t\t# apply the transformations to both image and its mask\n\t\t\timage = self.transforms(image)\n\t\t\tmask = self.transforms(mask)\n            \n\t\t# return a tuple of the image and its mask\n\t\treturn (image, mask)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.090521Z","iopub.execute_input":"2022-04-19T13:58:14.09099Z","iopub.status.idle":"2022-04-19T13:58:14.100718Z","shell.execute_reply.started":"2022-04-19T13:58:14.090955Z","shell.execute_reply":"2022-04-19T13:58:14.099913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Structure","metadata":{}},{"cell_type":"markdown","source":"Similar to FCN, the overall structure of U-Net is also an Encoder-Decoder Structure, in which 3x3 convolution layers and 2x2 max-polling are used alternately for down sampling. In the end of the encoding the network will output a feature map which is then be used for decoding.","metadata":{}},{"cell_type":"markdown","source":"![](https://pic.imgdb.cn/item/625d8915239250f7c502d886.jpg)","metadata":{}},{"cell_type":"markdown","source":"As for the decoder, the decoder will concatenate cropped outputs of each encoder layer as new feature map, which is different to FCN (adding pixel values). Which can produce \"thicker\" feature. Noted that in the original paper, the input and output sizes are different. While by controlling convolution parameters, it is also possible to rebuild a output image of the same size to inputs.","metadata":{}},{"cell_type":"code","source":"class Block(Module):\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper().__init__()\n\t\t# store the convolution and RELU layers\n\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n\t\tself.relu = ReLU()\n\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n        \n\tdef forward(self, x):\n\t\t# apply CONV => RELU => CONV block to the inputs and return it\n\t\treturn self.conv2(self.relu(self.conv1(x)))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.104235Z","iopub.execute_input":"2022-04-19T13:58:14.104684Z","iopub.status.idle":"2022-04-19T13:58:14.111309Z","shell.execute_reply.started":"2022-04-19T13:58:14.104648Z","shell.execute_reply":"2022-04-19T13:58:14.110574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(Module):\n\tdef __init__(self, channels=(3, 16, 32, 64)):\n\t\tsuper().__init__()\n\t\t# store the encoder blocks and maxpooling layer\n\t\tself.encBlocks = ModuleList([Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)])\n\t\tself.pool = MaxPool2d(2)\n        \n\tdef forward(self, x):\n\t\t# initialize an empty list to store the intermediate outputs\n\t\tblockOutputs = []\n        \n\t\t# loop through the encoder blocks\n\t\tfor block in self.encBlocks:\n\t\t\t# pass the inputs through the current encoder block, store the outputs, and then apply maxpooling on the output\n\t\t\tx = block(x)\n            # add blocked x to the end\n\t\t\tblockOutputs.append(x)\n            # pooling\n\t\t\tx = self.pool(x)\n            \n        # return the list containing the intermediate outputs\n\t\treturn blockOutputs","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.112744Z","iopub.execute_input":"2022-04-19T13:58:14.11311Z","iopub.status.idle":"2022-04-19T13:58:14.126258Z","shell.execute_reply.started":"2022-04-19T13:58:14.113077Z","shell.execute_reply":"2022-04-19T13:58:14.125403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the encoder layer get deeper and deeper, the scope of feature map get larger and larger. \n\ni.e. Shallow layers cares about textural details while deeper layers consider the general features of an area, which are both contributing to the segmentaions.\n\nAlso, by concatenating, some lost marginal information are retrieved by reviewing encoding layers which are not recoverable by up-sampling. ","metadata":{}},{"cell_type":"code","source":"class Decoder(Module):\n\tdef __init__(self, channels=(64, 32, 16)):\n\t\tsuper().__init__()\n        \n\t\t# initialize the number of channels, upsampler blocks, and decoder blocks\n\t\tself.channels = channels\n        # upconvolution 64 to 32, 32 to 16 respectively\n\t\tself.upconvs = ModuleList([ConvTranspose2d(channels[i], channels[i + 1], 2, 2) for i in range(len(channels) - 1)])\n        # define 3*3 conv and RELU block\n\t\tself.dec_blocks = ModuleList([Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)])\n        \n\tdef forward(self, x, encFeatures):\n\t\t# loop through the number of channels\n\t\tfor i in range(len(self.channels) - 1):\n\t\t\t# pass the inputs through the upsampler blocks\n\t\t\tx = self.upconvs[i](x)\n            \n\t\t\t# crop the current features from the encoder blocks, concatenate them with the current upsampled features,\n\t\t\t# and pass the concatenated output through the current decoder block\n\t\t\tencFeat = self.crop(encFeatures[i], x)\n            # Concatenate!\n\t\t\tx = torch.cat([x, encFeat], dim=1)\n            # 3*3 conv and RELU for each upscending layer.\n\t\t\tx = self.dec_blocks[i](x)\n            \n\t\t# return the final decoder output\n\t\treturn x\n    \n\tdef crop(self, encFeatures, x):\n\t\t# grab the dimensions of the inputs, and crop the encoder features to match the dimensions\n\t\t(_, _, H, W) = x.shape\n\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n        \n\t\t# return the cropped features\n\t\treturn encFeatures","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.127953Z","iopub.execute_input":"2022-04-19T13:58:14.12826Z","iopub.status.idle":"2022-04-19T13:58:14.143636Z","shell.execute_reply.started":"2022-04-19T13:58:14.128224Z","shell.execute_reply":"2022-04-19T13:58:14.142885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You may find that the feature maps are croped before the concatenation that is because of that there may be mismatch between the encoder dim and decoder dim. Thus, crop is essential to homogenize their dimensions.","metadata":{}},{"cell_type":"code","source":"class UNet(Module):\n\tdef __init__(self, encChannels=(3, 16, 32, 64), decChannels=(64, 32, 16), nbClasses=1, retainDim=True, outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n\t\tsuper().__init__()\n\t\t\n\t\t# initialize the encoder and decoder\n\t\tself.encoder = Encoder(encChannels)\n\t\tself.decoder = Decoder(decChannels)\n\t\t\n\t\t# initialize the regression head and store the class variables\n\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n\t\tself.retainDim = retainDim\n\t\tself.outSize = outSize\n\t\t\n\tdef forward(self, x):\n\t\t# grab the features from the encoder\n\t\tencFeatures = self.encoder(x)\n\t\t\n\t\t# pass the encoder features through decoder making sure that their dimensions are suited for concatenation\n        # last output as 1st decfeature and every\n\t\tdecFeatures = self.decoder(encFeatures[::-1][0],encFeatures[::-1][1:])\n\t\t\n\t\t# pass the decoder features through the regression head to obtain the segmentation mask\n\t\tmap_ = self.head(decFeatures)\n\t\t\n\t\t# check to see if we are retaining the original output dimensions and if so, then resize the output to match them\n\t\tif self.retainDim:\n\t\t\tmap_ = F.interpolate(map_, self.outSize)\n\t\t\t\n\t\t# return the segmentation map\n\t\treturn map_","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.145044Z","iopub.execute_input":"2022-04-19T13:58:14.145387Z","iopub.status.idle":"2022-04-19T13:58:14.157464Z","shell.execute_reply.started":"2022-04-19T13:58:14.145354Z","shell.execute_reply":"2022-04-19T13:58:14.156756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training & Testing","metadata":{}},{"cell_type":"code","source":"# load the image and mask filepaths in a sorted manner\nimagePaths = sorted(list(paths.list_images(IMAGE_DATASET_PATH)))\nmaskPaths = sorted(list(paths.list_images(MASK_DATASET_PATH)))\n\n# partition the data into training and testing splits using 85% of the data for training and the remaining 15% for testing\nsplit = train_test_split(imagePaths, maskPaths, test_size=TEST_SPLIT, random_state=42)\n\n# unpack the data split\n(trainImages, testImages) = split[:2]\n(trainMasks, testMasks) = split[2:]\n\n# write the testing image paths to disk so that we can use then when evaluating/testing our model\nprint(\"[INFO] saving testing image paths...\")\nf = open(\"test_paths.txt\", \"w\")\nf.write(\"\\n\".join(testImages))\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.158603Z","iopub.execute_input":"2022-04-19T13:58:14.159257Z","iopub.status.idle":"2022-04-19T13:58:14.259546Z","shell.execute_reply.started":"2022-04-19T13:58:14.159197Z","shell.execute_reply":"2022-04-19T13:58:14.258799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define transformations\ntransforms = transforms.Compose([transforms.ToPILImage(), transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)), transforms.ToTensor()])\n\n# create the train and test datasets\ntrainDS = SegmentationDataset(imagePaths=trainImages, maskPaths=trainMasks,\ttransforms=transforms)\ntestDS = SegmentationDataset(imagePaths=testImages, maskPaths=testMasks, transforms=transforms)\nprint(f\"[INFO] found {len(trainDS)} examples in the training set...\")\nprint(f\"[INFO] found {len(testDS)} examples in the test set...\")\n\n# create the training and test data loaders\ntrainLoader = DataLoader(trainDS, shuffle=True,\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\ntestLoader = DataLoader(testDS, shuffle=False, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.260764Z","iopub.execute_input":"2022-04-19T13:58:14.261221Z","iopub.status.idle":"2022-04-19T13:58:14.278703Z","shell.execute_reply.started":"2022-04-19T13:58:14.261181Z","shell.execute_reply":"2022-04-19T13:58:14.277989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize our UNet model\nunet = UNet().to(DEVICE)\n\n# initialize loss function and optimizer\nlossFunc = BCEWithLogitsLoss()\nopt = Adam(unet.parameters(), lr=INIT_LR)\n\n# calculate steps per epoch for training and test set\ntrainSteps = len(trainDS) // BATCH_SIZE\ntestSteps = len(testDS) // BATCH_SIZE\n\n# initialize a dictionary to store training history\nH = {\"train_loss\": [], \"test_loss\": []}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:14.279598Z","iopub.execute_input":"2022-04-19T13:58:14.279821Z","iopub.status.idle":"2022-04-19T13:58:17.172463Z","shell.execute_reply.started":"2022-04-19T13:58:14.27979Z","shell.execute_reply":"2022-04-19T13:58:17.171646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loop over epochs\nprint(\"[INFO] training the network...\")\nstartTime = time.time()\nfor e in tqdm(range(NUM_EPOCHS)):\n\t# set the model in training mode\n\tunet.train()\n    \n\t# initialize the total training and validation loss\n\ttotalTrainLoss = 0\n\ttotalTestLoss = 0\n    \n\t# loop over the training set\n\tfor (i, (x, y)) in enumerate(trainLoader):\n\t\t# send the input to the device\n\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n        \n\t\t# perform a forward pass and calculate the training loss\n\t\tpred = unet(x)\n\t\tloss = lossFunc(pred, y)\n        \n\t\t# first, zero out any previously accumulated gradients, then perform backpropagation, and then update model parameters\n\t\topt.zero_grad()\n\t\tloss.backward()\n\t\topt.step()\n        \n\t\t# add the loss to the total training loss so far\n\t\ttotalTrainLoss += loss\n        \n\t# switch off autograd\n\twith torch.no_grad():\n\t\t# set the model in evaluation mode\n\t\tunet.eval()\n        \n\t\t# loop over the validation set\n\t\tfor (x, y) in testLoader:\n\t\t\t# send the input to the device\n\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n            \n\t\t\t# make the predictions and calculate the validation loss\n\t\t\tpred = unet(x)\n\t\t\ttotalTestLoss += lossFunc(pred, y)\n            \n\t# calculate the average training and validation loss\n\tavgTrainLoss = totalTrainLoss / trainSteps\n\tavgTestLoss = totalTestLoss / testSteps\n    \n\t# update our training history\n\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n    \n\t# print the model training and validation information\n\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n    \n# display the total time needed to perform the training\nendTime = time.time()\nprint(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:58:17.176038Z","iopub.execute_input":"2022-04-19T13:58:17.176251Z","iopub.status.idle":"2022-04-19T14:02:27.557287Z","shell.execute_reply.started":"2022-04-19T13:58:17.176225Z","shell.execute_reply":"2022-04-19T14:02:27.555623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the training loss\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(H[\"train_loss\"], label=\"train_loss\")\nplt.plot(H[\"test_loss\"], label=\"test_loss\")\nplt.title(\"Training Loss on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\nplt.savefig(PLOT_PATH)\n\n# serialize the model to disk\ntorch.save(unet, MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:02:27.560615Z","iopub.execute_input":"2022-04-19T14:02:27.561111Z","iopub.status.idle":"2022-04-19T14:02:27.853448Z","shell.execute_reply.started":"2022-04-19T14:02:27.561068Z","shell.execute_reply":"2022-04-19T14:02:27.852793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Our Trained U-Net Model for Prediction","metadata":{}},{"cell_type":"code","source":"# import the necessary packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:02:27.854658Z","iopub.execute_input":"2022-04-19T14:02:27.85611Z","iopub.status.idle":"2022-04-19T14:02:27.860579Z","shell.execute_reply.started":"2022-04-19T14:02:27.85607Z","shell.execute_reply":"2022-04-19T14:02:27.859727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_plot(origImage, origMask, predMask):\n\t# initialize our figure\n\tfigure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n\n\t# plot the original image, its mask, and the predicted mask\n\tax[0].imshow(origImage)\n\tax[1].imshow(origMask)\n\tax[2].imshow(predMask)\n\n\t# set the titles of the subplots\n\tax[0].set_title(\"Image\")\n\tax[1].set_title(\"Original Mask\")\n\tax[2].set_title(\"Predicted Mask\")\n\n\t# set the layout of the figure and display it\n\tfigure.tight_layout()\n\tfigure.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:02:27.862715Z","iopub.execute_input":"2022-04-19T14:02:27.863022Z","iopub.status.idle":"2022-04-19T14:02:27.87134Z","shell.execute_reply.started":"2022-04-19T14:02:27.862978Z","shell.execute_reply":"2022-04-19T14:02:27.870675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, imagePath):\n\t# set model to evaluation mode\n\tmodel.eval()\n\n\t# turn off gradient tracking\n\twith torch.no_grad():\n\t\t# load the image from disk, swap its color channels, cast it to float data type, and scale its pixel values\n\t\timage = cv2.imread(imagePath)\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image.astype(\"float32\") / 255.0\n\n\t\t# resize the image and make a copy of it for visualization\n\t\timage = cv2.resize(image, (128, 128))\n\t\torig = image.copy()\n\n\t\t# find the filename and generate the path to ground truth mask\n\t\tfilename = imagePath.split(os.path.sep)[-1]\n\t\tgroundTruthPath = os.path.join(MASK_DATASET_PATH, filename)\n\n\t\t# load the ground-truth segmentation mask in grayscale mode and resize it\n\t\tgtMask = cv2.imread(groundTruthPath, 0)\n\t\tgtMask = cv2.resize(gtMask, (INPUT_IMAGE_HEIGHT, INPUT_IMAGE_HEIGHT))\n        \n        # make the channel axis to be the leading one, add a batch dimension, create a PyTorch tensor, and flash it to the\n\t\t# current device\n\t\timage = np.transpose(image, (2, 0, 1))\n\t\timage = np.expand_dims(image, 0)\n\t\timage = torch.from_numpy(image).to(DEVICE)\n\n\t\t# make the prediction, pass the results through the sigmoid\n\t\t# function, and convert the result to a NumPy array\n\t\tpredMask = model(image).squeeze()\n\t\tpredMask = torch.sigmoid(predMask)\n\t\tpredMask = predMask.cpu().numpy()\n\n\t\t# filter out the weak predictions and convert them to integers\n\t\tpredMask = (predMask > THRESHOLD) * 255\n\t\tpredMask = predMask.astype(np.uint8)\n\n\t\t# prepare a plot for visualization\n\t\tprepare_plot(orig, gtMask, predMask)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:02:27.872785Z","iopub.execute_input":"2022-04-19T14:02:27.873514Z","iopub.status.idle":"2022-04-19T14:02:27.884431Z","shell.execute_reply.started":"2022-04-19T14:02:27.87337Z","shell.execute_reply":"2022-04-19T14:02:27.883723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the image paths in our testing file and randomly select 10 image paths\nprint(\"[INFO] loading up test image paths...\")\nimagePaths = open(TEST_PATHS).read().strip().split(\"\\n\")\nimagePaths = np.random.choice(imagePaths, size=10)\n\n# load our model from disk and flash it to the current device\nprint(\"[INFO] load up model...\")\nunet = torch.load(MODEL_PATH).to(DEVICE)\n\n# iterate over the randomly selected test image paths\nfor path in imagePaths:\n\t# make predictions and visualize the results\n\tmake_predictions(unet, path)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:03:21.89302Z","iopub.execute_input":"2022-04-19T14:03:21.893332Z","iopub.status.idle":"2022-04-19T14:03:27.519355Z","shell.execute_reply.started":"2022-04-19T14:03:21.893296Z","shell.execute_reply":"2022-04-19T14:03:27.518499Z"},"trusted":true},"execution_count":null,"outputs":[]}]}