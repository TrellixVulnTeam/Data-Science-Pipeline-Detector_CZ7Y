{"cells":[{"metadata":{"id":"Bi2hnapjmUbw","colab_type":"text","_uuid":"55789921142828745929ba9285b3e430241639ff"},"cell_type":"markdown","source":"I've made this repository in research.colab so there might be some sort of errors.\nGithub Repository Link : https://github.com/harshitahluwalia7895/TGS-Salt-Identification-Challenge-Kaggle \nLinkedin Link: https://www.linkedin.com/in/harshit-ahluwalia-4b1153141/\n![alt text](https://pbs.twimg.com/media/DilKBjUVsAA74j1.jpg)\n\n## Background Data\n\n- Where there is salt, there is oil.\n- But where is the salt?\n- Classifying seismic imaging currently requires human (salt/not salt)\n- Can we use an algorithm to do this instead? (yes)\n- Seismic data is like an ultra-sound of the subsurface\n- It uses wavelengths around 1m to 100m\n- The Society of Exploration Geophysicists has 10K publications using the keyword 'salt'\n- We can think of Earth as layered.\n- Sand gets deposited on top of existing sand. And in comes the salt.\n- There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface.\n\n## Our steps\n\n- Install Dependencies\n- Upload our Dataset\n- View the labeled seismic imaging data\n- Plot the depth distribution in the training data \n- Plot the proportion of salt vs depth in the training data\n- Build a U-Net algorithm to learn the mapping between seismic images and the salt filter mask\n"},{"metadata":{"id":"vMhn8b3sqmHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"fe5b663d-fa63-40dd-96cd-c0c0d25f3806","trusted":true,"_uuid":"902c44cc9766080e9e0c7475ee72b9db5d476fd3"},"cell_type":"code","source":"#read/write image data\n!pip install imageio\n#deep learning library\n!pip install torch\n#access kaggle datasets from colab\n!pip install kaggle\n#model loading\n!pip install ipywidgets","execution_count":null,"outputs":[]},{"metadata":{"id":"Klc_bTWoqZXe","colab_type":"code","colab":{},"trusted":true,"_uuid":"1decd7b3a42d9d6406a9cd4b5f22f0e91917bfa4"},"cell_type":"code","source":"#File input output\nimport os\n#matrix math\nimport numpy as np\n#read/write image data\nimport imageio\n#visualize data\nimport matplotlib.pyplot as plt\n#data preprocessing \nimport pandas as pd\n#deep learning\nimport torch\n#just in case we need a backup datasets\nfrom torch.utils import data\n#will output the plot right below the cell that produces it\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"QL3hJ6VkwCnN","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":89},"outputId":"82592e04-28e8-4fc4-e946-17a4ab132bd2","trusted":true,"_uuid":"afac21973fd2db00fb7389f4c315b4de9495633f"},"cell_type":"code","source":"#allows us to upload files into colab\n#we'll need to upload the kaggle.json file\n#in kaggle, under accounts, click 'create new API token'\n#upload the kaggle.json file that is automatically downloaded\n\n# only Run in Reasearch.colab\n\n'''\nfrom google.colab import files\nfiles.upload()\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"1cJuzk8kw5TN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"98b9d87d-e573-458e-8016-17b024006911","trusted":true,"_uuid":"a89d762d0c01d3958c0bbd955cd2ce9f040a8d65"},"cell_type":"code","source":"#ensure its there\n!ls -lha kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"id":"Hrzlqyr_xHcd","colab_type":"code","colab":{},"trusted":true,"_uuid":"27d511d612f4c6af935e50474c6bdb972786fb8e"},"cell_type":"code","source":"# The Kaggle API client expects this file to be in ~/.kaggle,\n# so lets move it there.\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n\n# This permissions change avoids a warning on Kaggle tool startup.\n!chmod 600 ~/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"id":"WavozskmxMZe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"813c7ade-2102-4593-fa95-df39ba11d1b4","trusted":true,"_uuid":"e0a61015232f69680db01825317a3b460252aae8"},"cell_type":"code","source":"#lets now download our dataset\n!kaggle competitions download -c tgs-salt-identification-challenge ","execution_count":null,"outputs":[]},{"metadata":{"id":"Fur6k1fnxiUt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136102},"outputId":"ee400fdb-7c4d-4c2a-a30e-72137f58106b","trusted":true,"_uuid":"bbd918f371624869a440ef33c328c35e4129ee6e"},"cell_type":"code","source":"#and we'll need those training images unzipped\n!ls\n!unzip train.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"uMHZUr4rvpA2","colab_type":"code","colab":{},"trusted":true,"_uuid":"4474a86b20305d1975836bc877458729f4b98b6e"},"cell_type":"code","source":"#lets create a class to represent this data, to make it easier to access\n\nclass TGSSaltDataset(data.Dataset):\n    #init with the location of the dataset, and the list of file \n    def __init__(self, root_path, file_list):\n        self.root_path = root_path\n        self.file_list = file_list\n    #get method - how long is the list\n    def __len__(self):\n        return len(self.file_list)\n    #get method - return the seismic image + label for a given index\n    def __getitem__(self, index):\n        #if the index is out of bounds, get a random image\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        #define a file ID using the index parameter\n        file_id = self.file_list[index]\n        #image folder + path\n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        #label folder + path\n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        #read it, store it in memory as a byte array\n        image = np.array(imageio.imread(image_path), dtype=np.uint8)\n        mask = np.array(imageio.imread(mask_path), dtype=np.uint8)\n        #return image + label\n        return image, mask","execution_count":null,"outputs":[]},{"metadata":{"id":"pNnfAi_7zehe","colab_type":"code","colab":{},"trusted":true,"_uuid":"4523d175c068c269cb14318ef389b30b907b8062"},"cell_type":"code","source":"#only run in Google.colab.research\n'''\n#train image + mask data\ntrain_mask = pd.read_csv('train.csv')\n#depth data\ndepth = pd.read_csv('depths.csv')\n#training path\ntrain_path = \"./\"\n\n#list of files\nfile_list = list(train_mask['id'].values)\n#define our dataset using our class\ndataset = TGSSaltDataset(train_path, file_list)\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"AfD9rM1-zvlW","colab_type":"code","colab":{},"trusted":true,"_uuid":"44301b8135a1de645d6666bbedb61548d9f44108"},"cell_type":"code","source":"#function to visualize these images\ndef plot2x2Array(image, mask):\n    #invoke matplotlib!\n    f, axarr = plt.subplots(1,2)\n    axarr[0].imshow(image)\n    axarr[1].imshow(mask)\n    axarr[0].grid()\n    axarr[1].grid()\n    axarr[0].set_title('Image')\n    axarr[1].set_title('Mask')","execution_count":null,"outputs":[]},{"metadata":{"id":"NKiZ4Vetzxxm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1007},"outputId":"6f24ea53-5114-4b49-d902-b0f295931a7f","trusted":true,"_uuid":"9a509e5b733bf31c1dd5eee74a18e54f275fd1ee"},"cell_type":"code","source":"#only run in Google.colab.research\n'''\nfor i in range(5):\n    image, mask = dataset[np.random.randint(0, len(dataset))]\n    plot2x2Array(image, mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"zrvKS0sB2WpW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"8714e1a1-b743-4ba8-ba17-5fe765244da5","trusted":true,"_uuid":"2f1eae9872822d2cad50a1906b8b030ecc20c4ff"},"cell_type":"code","source":"'''plt.figure(figsize = (6, 6))\nplt.hist(depth['z'], bins = 50)\nplt.title('Depth distribution')'''","execution_count":null,"outputs":[]},{"metadata":{"id":"rLkdbt9Z0Y-B","colab_type":"text","_uuid":"178e304e09b268f3d4b71b24aa2fa76233c94160"},"cell_type":"markdown","source":"Run-length encoding (RLE) is a very simple form of lossless data compression in which runs of data (that is, sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run. \n\n![alt text](https://sites.google.com/a/ruknuddin.com/inqilab-patel-magazine/_/rsrc/1525765922681/term-of-the-day/runlengthencoding/RunLengthEncoding.PNG)\n\n"},{"metadata":{"id":"J9piDHb420c-","colab_type":"code","colab":{},"trusted":true,"_uuid":"c36f7624adaa232ffbfe6129d3ed0eb31007460d"},"cell_type":"code","source":"#convert to image\ndef rleToMask(rleString,height,width):\n    #width heigh\n    rows,cols = height,width\n    try:\n        #get numbers\n        rleNumbers = [int(numstring) for numstring in rleString.split(' ')]\n        #get pairs\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        #create an image\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        #for each pair\n        for index,length in rlePairs:\n            #get the pixel value \n            index -= 1\n            img[index:index+length] = 255\n        \n        \n        #reshape\n        img = img.reshape(cols,rows)\n        img = img.T\n    \n    #else return empty image\n    except:\n        img = np.zeros((cols,rows))\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"id":"LtpKsYeN23ie","colab_type":"code","colab":{},"trusted":true,"_uuid":"221fe1b9e884c0c98f59913b22f65ffe2b70b475"},"cell_type":"code","source":"#only run in Google.research.colab\n#for measuring how salty an image is\ndef salt_proportion(imgArray):\n    try: \n        unique, counts = np.unique(imgArray, return_counts=True)\n        ## The total number of pixels is 101*101 = 10,201\n        return counts[1]/10201.\n    \n    except: \n        return 0.0","execution_count":null,"outputs":[]},{"metadata":{"id":"x0LPqHZF26vm","colab_type":"code","colab":{},"trusted":true,"_uuid":"af05e39bc7545e039c7aa08bff3a1a8dd9b4926f"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\ntrain_mask['mask'] = train_mask['rle_mask'].apply(lambda x: rleToMask(x, 101,101))\ntrain_mask['salt_proportion'] = train_mask['mask'].apply(lambda x: salt_proportion(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"aNGJ0Cwf3ET-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"2ca49c32-f957-4be3-8c27-8edf2196b9b6","trusted":true,"_uuid":"c41d84aac07df4423de009cee33335d0307f5d69"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\nmerged = train_mask.merge(depth, how = 'left')\nmerged.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"dXDXssow3Jh2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"a1b64e1b-9208-43c5-a1e7-98e0b28a8a3b","trusted":true,"_uuid":"1edcd79808c5344c306201923aa91d2f43ec356c"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\nplt.figure(figsize = (12, 6))\nplt.scatter(merged['salt_proportion'], merged['z'])\nplt.title('Proportion of salt v. depth')","execution_count":null,"outputs":[]},{"metadata":{"id":"-jZ8PBlF3L_u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"14192487-54e2-4cd5-bfdf-eb87c8fd82ac","trusted":true,"_uuid":"cb0f50b6dda9ba3139c1fdd020834d662c94d47f"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\nprint(\"Correlation: \", np.corrcoef(merged['salt_proportion'], merged['z'])[0, 1])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"P-kAY13X73uE","colab_type":"text","_uuid":"96e428dda7cb3f61aabeccad644de277ec60b43d"},"cell_type":"markdown","source":"#### U-Nets\n\n- This looks like a computer vision problem!\n- What outperforms everything else when it comes to computer vision most of the time? (hint deep learning)\n- Specifically, deep convolutional neural networks.\n- AlexNet\n- VGG-16, VGG-19;\n- Inception Nets;\n- ResNet;\n- Squeeze Net \n\n![alt text](https://pics.spark-in.me/upload/388373c74ab710cc4c429538d4779a93.png)\n![alt text](https://pics.spark-in.me/upload/8b468c6dd60a499e6cdde0af803cb96f.png)\n\n#### Why not use one of these, why use a U-Net?\n\n- U-NET is considered one of standard architectures for image classification tasks, when we need not only to segment the whole image by its class, but also to segment areas of image by class, i.e. produce a mask that will separate image into several classes.\n-its architecture is input image size agnostic since it does not contain fully connected layers \n- Because of many layers takes significant amount of time to train;\n- U-Net is designed like an auto-encoder. \n- It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape.  \n- However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. \n- For each pixel in the original image, it asks the question: “To which class does this pixel belong?\n- This flexibility allows U-Net to predict different parts of the seismic image (salt, not salt) simultaneously.\n- U-Net passes the feature maps from each level of the contracting path over to the analogous level in the expanding path.  \n- These are similar to residual connections in a ResNet type model, and allow the classifier to consider features at various scales and complexities to make its decision.\n\n#### What does a U-Net look like?\n\n![alt text](https://cdn-images-1.medium.com/max/800/1*dKPBgCdJx6zj3MpED3lcNA.png)\n![alt text](https://pics.spark-in.me/upload/cb8197bb7e57317eda88f921dfc1c413.png)\n\nits high to low res, then low res to high res\n\n- The first part (encoder) is where we apply convolutional blocks followed by a maxpool downsampling to encode the input image into feature representations at multiple different levels.\n\nconvolutions→ downsampling.\n\nExample\n\n```\n# a sample down block\ndef make_conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n    return [\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    ]\nself.down1 = nn.Sequential(\n    *make_conv_bn_relu(in_channels, 64, kernel_size=3, stride=1, padding=1 ),\n    *make_conv_bn_relu(64, 64, kernel_size=3, stride=1, padding=1 ),\n)\n\n# convolutions followed by a maxpool\ndown1 = self.down1(x)\nout1   = F.max_pool2d(down1, kernel_size=2, stride=2)\n\n```\n\n- The  second part (decoder) of the network consists of upsample and concatenation followed by regular convolution operations\n\nupsampling → concatenation →convolutions.\n\n```\n# a sample up block\ndef make_conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n    return [\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    ]\nself.up4 = nn.Sequential(\n    *make_conv_bn_relu(128,64, kernel_size=3, stride=1, padding=1 ),\n    *make_conv_bn_relu(64,64, kernel_size=3, stride=1, padding=1 )\n)\nself.final_conv = nn.Conv2d(32, num_classes, kernel_size=1, stride=1, padding=0 )\n\n# upsample out_last, concatenate with down1 and apply conv operations\nout   = F.upsample(out_last, scale_factor=2, mode='bilinear')  \nout   = torch.cat([down1, out], 1)\nout   = self.up4(out)\n\n# final 1x1 conv for predictions\nfinal_out = self.final_conv(out)\n```\n\n\n"},{"metadata":{"id":"YC3C7MPmDWoH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bc04ef92-8c24-4781-9879-f851600fb6d0","trusted":true,"_uuid":"11db3c1df99aed9c74a43161b12b6a9cf1bf28d6"},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"id":"i5HPnK47DdaP","colab_type":"code","colab":{},"trusted":true,"_uuid":"6769932a5fde6ab167ff8552c8f1f9a30e290f47"},"cell_type":"code","source":"im_width = 128\nim_height = 128\nborder = 5\nim_chan = 2 # Number of channels: first is original and second cumsum(axis=0)\nn_features = 1 # Number of extra features, like depth\n#path_train = '../input/train/'\n#path_test = '../input/test/'","execution_count":null,"outputs":[]},{"metadata":{"id":"kusRnaQF6Sj3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1445},"outputId":"4b7f389a-ea7b-466d-9482-49e7030b9cb6","trusted":true,"_uuid":"a9afb17d256c014aeacf3f63aa325f0c296d4eba"},"cell_type":"code","source":"# Build U-Net model\ninput_img = Input((im_height, im_width, im_chan), name='img')\ninput_features = Input((n_features, ), name='feat')\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (input_img)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\n# Join features information in the depthest layer\nf_repeat = RepeatVector(8*8)(input_features)\nf_conv = Reshape((8, 8, n_features))(f_repeat)\np4_feat = concatenate([p4, f_conv], -1)\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4_feat)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n#check out this skip connection thooooo\nu6 = concatenate([u6, c4])\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[input_img, input_features], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy') #, metrics=[mean_iou]) # The mean_iou metrics seens to leak train and test values...\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"WxuKipDiEbZP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":632},"outputId":"220d733d-e4a1-445e-d937-9eace26f3d6c","trusted":true,"_uuid":"af5d273bde721668c2ece16ca25f357ac2662e59"},"cell_type":"code","source":"!pip install ipywidgets\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Co8wbV9wDpEX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"4662dfbf-4c64-4eaa-aea2-e0f9c28d4290","trusted":true,"_uuid":"9adebc63c740f8ec35fbcdf312a47c49be6c7e8a"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\nimport sys\nfrom tqdm import tqdm\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.transform import resize\n\n\ntrain_ids = next(os.walk(train_path+\"images\"))[2]\n\n\n# Get and resize train images and masks\nX = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.float32)\ny = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.float32)\nX_feat = np.zeros((len(train_ids), n_features), dtype=np.float32)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = train_path\n    \n    # Depth\n    #X_feat[n] = depth.loc[id_.replace('.png', ''), 'z']\n    \n    # Load X\n    img = load_img(path + '/images/' + id_, grayscale=True)\n    x_img = img_to_array(img)\n    x_img = resize(x_img, (128, 128, 1), mode='constant', preserve_range=True)\n    \n    # Create cumsum x\n    x_center_mean = x_img[border:-border, border:-border].mean()\n    x_csum = (np.float32(x_img)-x_center_mean).cumsum(axis=0)\n    x_csum -= x_csum[border:-border, border:-border].mean()\n    x_csum /= max(1e-3, x_csum[border:-border, border:-border].std())\n\n    # Load Y\n    mask = img_to_array(load_img(path + '/masks/' + id_, grayscale=True))\n    mask = resize(mask, (128, 128, 1), mode='constant', preserve_range=True)\n\n    # Save images\n    X[n, ..., 0] = x_img.squeeze() / 255\n    X[n, ..., 1] = x_csum.squeeze()\n    y[n] = mask / 255\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"id":"K6Kj_YzKKIsn","colab_type":"code","colab":{},"trusted":true,"_uuid":"3907809a55ea7671faf613760dee1fd2aa3d6a05"},"cell_type":"code","source":"#only run in Google.research.colab\n'''from sklearn.model_selection import train_test_split\n\nX_train, X_valid, X_feat_train, X_feat_valid, y_train, y_valid = train_test_split(X, X_feat, y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"aH2z9T-KKOsn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1407},"outputId":"96e182b7-a887-4868-bb35-b70ec0af1246","trusted":true,"_uuid":"64c0c3e4fcdab9e9125d0e1dd28758fc4d76794e"},"cell_type":"code","source":"#only run in Google.research.colab\n'''\ncallbacks = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(patience=3, verbose=1),\n    ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]\n\nresults = model.fit({'img': X_train, 'feat': X_feat_train}, y_train, batch_size=16, epochs=50, callbacks=callbacks,\n                    validation_data=({'img': X_valid, 'feat': X_feat_valid}, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a103943461af8a0bb54abe4a36460e1b2f711b1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2f8d717198a051ec9bcc0058392a2cbb9e33b7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbaa09c23a1beed2e086449152a99c406701550"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a9459c95e4184d2dd557229f6d57c52e631f98f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8b9551e99660fd63bf310030bb01ae4ec3e4a83"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"salt_identification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}