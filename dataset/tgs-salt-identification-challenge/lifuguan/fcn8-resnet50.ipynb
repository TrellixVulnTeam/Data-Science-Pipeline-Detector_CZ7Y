{"cells":[{"metadata":{"id":"LVDWaJda-Y8R"},"cell_type":"markdown","source":"# 基于VGG主干的FCN网络的图像分割\n\n**TGS Salt Identification Challenge**\n\nSegment salt deposits beneath the Earth's surface\n\n![image](https://storage.googleapis.com/kaggle-competitions/kaggle/10151/logos/header.png)\n\n数据集网址：https://www.kaggle.com/c/tgs-salt-identification-challenge"},{"metadata":{"id":"-P2N4vq1Yaod","outputId":"e02837fa-7916-40f0-beda-d80544a6fabd","trusted":true},"cell_type":"code","source":"!cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c\n!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"id":"0sgM51Uy4xOd","outputId":"f205416b-83d7-408a-df77-217175faab92","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set your own project id here\nPROJECT_ID = 'fcn8_resnet50'\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)\ndef upload_files(bucket_name, source_folder):\n bucket = storage_client.get_bucket(bucket_name)\n for filename in os.listdir(source_folder):\n \n  blob = bucket.blob(filename)\n  blob.upload_from_filename(source_folder + filename)","execution_count":null,"outputs":[]},{"metadata":{"id":"PaVbcoh_aGwq","outputId":"8de4257d-2d5a-4fa6-86c3-9bb54adf471c","trusted":true},"cell_type":"code","source":"!mkdir -p '../content/kgs_data'\n!cp '/kaggle/input/tgs-salt-identification-challenge/train.zip' -d '../content/kgs_data_train.zip'\n!unzip '../content/kgs_data_train.zip' -d '../content/kgs_data'","execution_count":null,"outputs":[]},{"metadata":{"id":"Ase3ld-CStlQ"},"cell_type":"markdown","source":"## 数据预处理\n#### 调用必要库"},{"metadata":{"id":"o06BX-8_T03a","outputId":"1182a33f-1a08-4777-ee69-25cf1cd030e6","trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport time \nimport math\nimport glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as T\nfrom torchsummary import summary\n\ndef timeSince(since):\n  now = time.time()\n  s = now - since\n  m = math.floor(s / 60)\n  s -= m * 60\n  return '%dm %ds' % (m, s)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"current device is : \", device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"id":"ENqTyM1RBsEU"},"cell_type":"markdown","source":"#### 确认数据集位置（查看train）\n*@note*：一定要解压在非云盘位置，避免后期训练集因为图片读取速率而导致的速度问题"},{"metadata":{"id":"WzKPKQz-VXTc","trusted":true},"cell_type":"code","source":"image_path = \"../content/kgs_data/images\"\nmask_path = \"../content/kgs_data/masks\"","execution_count":null,"outputs":[]},{"metadata":{"id":"0Ve_E_HzBwzo"},"cell_type":"markdown","source":"#### 样例输出"},{"metadata":{"id":"iOsktiaXBpKX","outputId":"4db3acde-9908-4f45-9c1f-56b6fe559a75","trusted":true},"cell_type":"code","source":"# 从训练集中随意挑选几张图片和蒙版，输出来看看\nnames = ['000e218f21','41cfd4b320','3c2f5ba174']\nimages = [Image.open(os.path.join(image_path, name+'.png')) for name in names]\nmasks = [Image.open(os.path.join(mask_path, name+'.png')) for name in names]\n\n'''Transform 用法\ntransform = transforms.Compose([\n    transforms.Grayscale(),  # 将图像转化为灰度图\n    transforms.RandomCrop(32, padding=4),  #先四周填充0，在吧图像随机裁剪成32*32\n    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n    transforms.RandomRotation((-45,45)), #随机旋转\n    transforms.ToTensor(),   # 图像转tensor\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差\n])\n'''\ntransforms1 = T.Resize((56,56),interpolation=Image.NEAREST)\ntransforms2 = T.Compose([T.Grayscale(), T.ToTensor()]) # 转换模板\nx = torch.stack([transforms2(transforms1(image)) for image in images])\ny = torch.stack([transforms2(transforms1(mask)) for mask in masks])\nprint(x.size())\nfig = plt.figure( figsize=(9, 9))\n\nax = fig.add_subplot(331)\nplt.imshow(images[0])\nax = fig.add_subplot(332)\nplt.imshow(masks[0])\nax = fig.add_subplot(333)\nax.imshow(x[0].squeeze(), cmap=\"Greys\")\nax.imshow(y[0].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nax = fig.add_subplot(334)\nplt.imshow(images[1])\nax = fig.add_subplot(335)\nplt.imshow(masks[1])\nax = fig.add_subplot(336)\nax.imshow(x[1].squeeze(), cmap=\"Greys\")\nax.imshow(y[1].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nax = fig.add_subplot(337)\nplt.imshow(images[2])\nax = fig.add_subplot(338)\nplt.imshow(masks[2])\nax = fig.add_subplot(339)\nax.imshow(x[2].squeeze(), cmap=\"Greys\")\nax.imshow(y[2].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5BTmO5qyzXJm"},"cell_type":"markdown","source":"#### 训练集获取"},{"metadata":{"id":"ektGdTl6W22t","trusted":true},"cell_type":"code","source":"class segmentDataset(Dataset):\n    def __init__(self, image_path, mask_path):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        \n        # 根据所规定的pattern，返回图片目录组成的list\n        image_list= glob.glob(image_path +'/*.png')\n        # 利用for循环的语句获取图像文件的文件名\n        sample_names = []\n        for file in image_list:\n            sample_names.append(file.split('/')[-1].split('.')[0])\n        self.sample_names = sample_names\n        # 图像缩放，获得(224, 224)\n        # 获取灰度图 + 将其转化为tensor\n        self.transforms = T.Compose([T.Grayscale(), T.ToTensor(), T.Resize((112,112),interpolation=Image.NEAREST)])\n            \n    def __getitem__(self, idx):\n        image = Image.open(os.path.join(self.image_path, self.sample_names[idx]+'.png') )\n        mask = Image.open(os.path.join(self.mask_path, self.sample_names[idx]+'.png') )\n        return self.transforms(image), self.transforms(mask)\n\n    def __len__(self):\n        return len(self.sample_names)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pNxocCCe0YqK","trusted":true},"cell_type":"code","source":"train_dataset = segmentDataset(image_path = image_path, mask_path = mask_path)","execution_count":null,"outputs":[]},{"metadata":{"id":"6_fjFLwUITuh"},"cell_type":"markdown","source":"## FCN(fully Convolutional Networks)全卷积神经网络\n提出论文：[Fully Convolutional Networks\nfor Semantic Segmentation](https://arxiv.org/pdf/1605.06211.pdf)\n\n### 与VGG-Net的不同之处\n\n对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。\n```python\n# 三层线性全连接层\nself.classifier = nn.Sequential(\n    # first layer\n    nn.Linear(512 * 7 * 7, 4096, bias = True),\n    nn.ReLU(True),\n    nn.Dropout(),\n    # second layer\n    nn.Linear(4096, 4096, bias = True),\n    nn.ReLU(True),\n    nn.Dropout(),\n    # third layer\n    nn.Linear(4096, num_classes, bias = True),\n)\n```\n但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。\n\n强行训练会在评估时与评估函数发生冲突，如下：\n```bash\nValueError: Target size (torch.Size([64, 1, 224, 224])) must be the same as input size (torch.Size([64, 1000]))\n```\n\n为此，FCN提出可以把后面几个全连接都换成卷积，这样就可以获得一张2维的feature map，后接softmax获得每个像素点的分类信息，从而解决了分割问题，如下图所示：\n\n![iamge](https://pic1.zhimg.com/80/v2-7bfe6e1792c2fb8bcfab6eea632d5e2c_720w.jpg)\n\n![image](https://pic1.zhimg.com/80/v2-721ef7417b32a5aa4973f1e8dd16d90c_720w.jpg)\n\n1. 对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即分割图）。\n2. 对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature逐点相加，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。\n3. 对于FCN-8s，首先进行pool4+2x upsampled feature逐点相加，然后又进行pool3+2x upsampled逐点相加，即进行更多次特征融合。具体过程与16s类似，不再赘述。\n\n### 总结\nFCN网络是以VGG为主干，将分类器用反卷积进行替换，有点类似于这Encode-Decode过程。\n\n### 参考\n1. https://zhuanlan.zhihu.com/p/31428783\n2. https://zhuanlan.zhihu.com/p/32506912\n"},{"metadata":{"id":"1Bwo6BzpITWV","trusted":true},"cell_type":"code","source":"import torchvision.models as models\n       \n\n \nclass FCNx8_ResNet(nn.Module):\n    debug_info = True\n    def __init__(self,num_classes = 1):\n        super(FCNx8_ResNet, self).__init__()\n        pretrained_net = models.resnet50(pretrained=True)\n        pretrained_net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        conv_sequential= list(pretrained_net.children())[:-1]\n        summary(pretrained_net.to(device), (1, 56, 56))\n        modules_list = []\n        for i in range(4):\n            modules_list.append(conv_sequential[i])\n        self.head = nn.Sequential(*modules_list)\n \n        modules_list = []\n        for i in range(4,6):\n            temp = list(conv_sequential[i])\n            for j in range(len(temp)):\n                modules_list.append(temp[j])\n        self.stage1 = nn.Sequential(*modules_list)\n \n        modules_list = []\n        temp = list(conv_sequential[6])\n        for j in range(len(temp)):\n            modules_list.append(temp[j])\n        self.stage2 = nn.Sequential(*modules_list)\n \n        modules_list = []\n        temp = list(conv_sequential[7])\n        for j in range(len(temp)):\n            modules_list.append(temp[j])\n        modules_list.append(conv_sequential[8])\n        modules_list.append(nn.Conv2d(in_channels=2048,out_channels=1024,kernel_size=1,stride=1,padding=0))\n        modules_list.append(nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=1,stride=1,padding=0))\n        self.stage3 = nn.Sequential(*modules_list)\n \n        self.scores3 = nn.Conv2d(in_channels=512,out_channels=num_classes,kernel_size=1)\n        self.scores2 = nn.Conv2d(in_channels=1024,out_channels=num_classes,kernel_size=1)\n        self.scores1 = nn.Conv2d(in_channels=512,out_channels=num_classes,kernel_size=1)\n        #\n        # # N=(w-1)xs+k-2p\n        self.upsamplex8 = nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=16,stride=8,padding=4,bias= False)\n        self.upsamplex8.weight.data = self.bilinear_kernel(num_classes,num_classes,16)\n        self.upsamplex16 = nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=4,stride=2,padding=1,bias=False)\n        self.upsamplex16.weight.data = self.bilinear_kernel(num_classes,num_classes,4)\n        self.upsamplex32= nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=5,stride=3,padding=0,bias=False)\n        self.upsamplex32.weight.data = self.bilinear_kernel(num_classes,num_classes,7)\n \n    def forward(self, x):\n        x = self.head(x)\n        x = self.stage1(x)\n        s1 = x\n \n        x = self.stage2(x)\n        s2 = x\n \n        x = self.stage3(x)\n        s3 = x\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n \n        s3 = self.scores3(s3)\n        s3 = self.upsamplex32(s3)\n \n        s2 = self.scores2(s2)\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n        s2 = s2 + s3\n        s2 = self.upsamplex16(s2)\n \n        s1 = self.scores1(s1)\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n        s = s1 + s2\n        s = self.upsamplex8(s)\n         \n        self.debug_info = False\n        return s\n \n    def bilinear_kernel(self,in_channels, out_channels, kernel_size):\n        '''\n        return a bilinear filter tensor\n        '''\n        factor = (kernel_size + 1) // 2\n        if kernel_size % 2 == 1:\n            center = factor - 1\n        else:\n            center = factor - 0.5\n        og = np.ogrid[:kernel_size, :kernel_size]\n        filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n        weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype='float32')\n        weight[range(in_channels), range(out_channels), :, :] = filt\n        return torch.from_numpy(weight)\n# Example\nfcn_model = FCNx8_ResNet(num_classes = 1).to(device)\n# print(fcn_model)\nsummary(fcn_model, (1, 112, 112))","execution_count":null,"outputs":[]},{"metadata":{"id":"-MkKBNq5UGx-"},"cell_type":"markdown","source":"## 模型训练"},{"metadata":{"id":"mGbkH_BfxP9F","trusted":true},"cell_type":"code","source":"def get_iou_score(outputs, labels):\n    A = labels.squeeze().bool()\n    pred = torch.where(outputs<0., torch.zeros_like(outputs), torch.ones_like(outputs))\n    B = pred.squeeze().bool()\n    intersection = (A & B).float().sum((1,2))\n    union = (A | B).float().sum((1, 2)) \n    iou = (intersection + 1e-6) / (union + 1e-6)  \n    return iou\n  \ndef train_one_batch(model, x, y):\n    # print(\"input x: \", x.size(),\", input y = \", y.size())\n    x, y = x.to(device), y.to(device)\n    outputs = model(x)\n    # print(\"outputs:\", outputs.size())\n    loss = loss_fn(outputs, y)\n    iou = get_iou_score(outputs, y).mean()\n    \n    optimizer.zero_grad() # 将模型中的梯度设置为0\n    loss.backward()\n    optimizer.step()\n    return loss.item(), iou.item()","execution_count":null,"outputs":[]},{"metadata":{"id":"gsbeYXR80gIz"},"cell_type":"markdown","source":"### 保存模型和参数"},{"metadata":{"id":"cMCFHDlK0fIr","trusted":true},"cell_type":"code","source":"def save_model_args(epoch):\n    # 模型地址\n    model_path = 'fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,epoch,BATCH_SIZE)  \n    # 三个参数：网络参数；优化器参数；epoch\n    state = {'net':fcn_model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n    torch.save(state, model_path)\n\n    # 保存训练损失数据和IoU得分数据\n    train_losses_save = np.array(train_losses)\n    train_ious_save = np.array(train_ious)\n    plt.plot(train_losses_save, label = 'loss')\n    plt.plot(train_ious_save, label = 'IoU')\n    plt.xlabel('Epoch')\n    plt.ylabel('Metric')\n    plt.legend()\n    # 保存曲线\n    plt.savefig('fcn{}_resnet{}_loss_{}_batch_{}.png'.format(8,50,200,64), bbox_inches='tight')\n    plt.show()\n    np.save('fcn{}_resnet{}_loss_{}_batch_{}'.format(8,50,epoch,BATCH_SIZE),train_losses_save)\n    np.save('fcn{}_resnet{}_iou_{}_batch_{}'.format(8,50,epoch,BATCH_SIZE),train_ious_save)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p '../content/drive/MyDrive/实验数据&模型/fcn_resnet'","execution_count":null,"outputs":[]},{"metadata":{"id":"3sZQ5JzQoaDw","trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 200\nBATCH_SIZE = 64\n\nfcn_model.train() # 一定要表明是训练模式!!!\noptimizer = torch.optim.Adam(fcn_model.parameters())\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n\nloss_fn = nn.BCEWithLogitsLoss()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\nsteps  = train_dataset.__len__() // BATCH_SIZE\nprint(steps,\"steps per epoch\")\n\nstart = time.time()\ntrain_losses = []\ntrain_ious = []\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print('-' * 10)\n    print('Epoch {}/{}'.format(epoch, NUM_EPOCHS))\n    running_iou = []\n    running_loss = []\n    for step, (x, y) in enumerate(train_dataloader):\n        loss, iou = train_one_batch(fcn_model, x, y)\n        running_iou.append(iou)\n        running_loss.append(loss)\n        print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}'.format(100*(step+1)/steps, loss,iou), end = \"\") \n        \n    print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}\\t{}'.format(100*(step+1)/steps,np.mean(running_loss),np.mean(running_iou), timeSince(start)))\n    scheduler.step(np.mean(running_iou))\n    \n    train_losses.append(loss)\n    train_ious.append(iou)\n    if epoch % 50 is 0:\n        save_model_args(epoch)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TiwXYc0OLen7"},"cell_type":"markdown","source":"epoch 200/200 253m46s"},{"metadata":{"id":"cTQ5wnYa3nLK"},"cell_type":"markdown","source":"### 恢复模型"},{"metadata":{"id":"xaKC0EwN3lta","trusted":true},"cell_type":"code","source":"'''\nimport torchvision.models as models\n# 加载训练参数（损失+iou）\ntrain_losses_save = np.load(\"../content/drive/MyDrive/实验数据&模型/fcn_resnet/fcn{}_resnet{}_loss_{}_batch_{}.npy\".format(8,50,200,64))\ntrain_ious_save = np.load(\"../content/drive/MyDrive/实验数据&模型/fcn_resnet/fcn{}_resnet{}_iou_{}_batch_{}.npy\".format(8,50,200,64))\n\n\n# 加载模型参数\nparams = torch.load('../content/drive/MyDrive/实验数据&模型/fcn_resnet/fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,50,64))\nprint(params)\n\n# 加载模型\nnet = fcn_model()\npthfile = r'../content/drive/MyDrive/实验数据&模型/fcn_resnet/fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,50,64)\nnet.load_state_dict(torch.load(pthfile)['net'])\nprint(net)\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}