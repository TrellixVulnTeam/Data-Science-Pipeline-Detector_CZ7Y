{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import Required Libraries\n\n\nimport os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm_notebook, tnrange\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nfrom keras import optimizers\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#,save_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"version = 1\nbasic_name = f'Unet_resnet_v{version}'\nsave_model_name = basic_name + '.model'\nsubmission_file = basic_name + '.csv'\n\nprint(save_model_name)\nprint(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 101\n\ndef upsample(img):# not used\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    \ndef downsample(img):# not used\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tgs-salt-identification-challenge/train.csv',index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/tgs-salt-identification-challenge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\n\nfilepath = \"../input/tgs-salt-identification-challenge/train.zip\"\n\nwith ZipFile(filepath, 'r') as f:\n  f.extractall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Just for EDA\nimg = load_img(\"masks/{}.png\".format(train_df.index[12]), grayscale=True)\narr = img_to_array(img)\nload_img(\"masks/{}.png\".format(train_df.index[12]), grayscale=True)\nunique, counts = np.unique(arr, return_counts=True)\ndict(zip(unique, counts))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculates the area covered by 1s in the image\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.coverage_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the depth distributionsÂ¶\n\nsns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train/validation split stratified by salt coverage\n\nids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the Model\n\ndef BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Build model\ndef build_model(input_layer, start_neurons, DropoutRatio = 0.5):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n    output_layer =  Activation('sigmoid')(output_layer_noActi)\n    \n    return output_layer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    pred = pred > 0.5\n    pred = tf.dtypes.cast(pred, tf.int32)\n    return tf.py_function(get_iou_vector, [label, pred ], tf.float64)\n\n\ndef my_iou_metric_2(label, pred):\n    pred = pred > 0\n    pred = tf.dtypes.cast(pred, tf.int32)\n    return tf.py_function(get_iou_vector, [label, pred], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data augmentation\n\n#Data augmentation\nx_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\nprint(x_train.shape)\nprint(y_valid.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.5)\n\nmodel1 = Model(input_layer, output_layer)\n\nc = optimizers.adam(lr = 0.01)\nmodel1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint(save_model_name,monitor='my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='my_iou_metric', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n\nepochs = 50\nbatch_size = 32\nhistory = model1.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[ model_checkpoint,reduce_lr], \n                    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch[1:], history.history[\"loss\"][1:], label=\"Train loss\")\nax_loss.plot(history.epoch[1:], history.history[\"val_loss\"][1:], label=\"Validation loss\")\nax_loss.legend()\nax_score.plot(history.epoch, history.history[\"my_iou_metric\"], label=\"Train score\")\nax_score.plot(history.epoch, history.history[\"val_my_iou_metric\"], label=\"Validation score\")\nax_score.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### INFERENCE ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, iou_score = model1.evaluate(x_valid, y_valid, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_val = model1.predict(x_valid, verbose=1)\n\npreds_val_binary = preds_val > 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample(X, y, preds, binary_preds, ix=None):\n    \"\"\"Function to plot the results\"\"\"\n    if ix is None:\n        ix = random.randint(0, len(X))\n\n    has_mask = y[ix].max() > 0\n\n    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n    ax[0].imshow(X[ix, ..., 0], cmap='seismic')\n    if has_mask:\n        ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n    ax[0].set_title('Seismic')\n\n    ax[1].imshow(y[ix].squeeze())\n    ax[1].set_title('Salt')\n\n    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n    if has_mask:\n        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n    ax[2].set_title('Salt Predicted')\n    \n    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n    if has_mask:\n        ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n    ax[3].set_title('Salt Predicted binary');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_sample(x_valid, y_valid, preds_val, preds_val_binary, ix = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_val_binary[15].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#implementing dice loss funtion\n\ndef soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n\n  '''\n    # Arguments\n        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n        epsilon: Used for numerical stability to avoid divide by zero errors\n    \n    '''\n# skip the batch and class axis for calculating Dice score\n  axes = tuple(range(1, len(y_pred.shape)-1))\n  numerator = 2. * K.sum(y_pred * y_true, axes)\n  denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n  \n  return 1 - K.mean(numerator / (denominator + epsilon)) # average over classes and batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def diceLoss(y_true, y_pred):\n    #y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n    #logits = K.log(y_pred / (1. - y_pred))\n    loss = soft_dice_loss(y_true,y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.5)\n\nmodel2 = Model(input_layer, output_layer)\n\n\nc = optimizers.adam(lr = 0.01)\nmodel2.compile(loss=diceLoss, optimizer=c, metrics=[my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_my_iou_metric', mode = 'max',patience=20, verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\nepochs = 50\nbatch_size = 32\n\nhistory2 = model2.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[ model_checkpoint,reduce_lr,early_stopping], \n                    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history2.epoch, history2.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history2.epoch, history2.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nax_score.plot(history2.epoch, history.history[\"my_iou_metric\"], label=\"Train score\")\nax_score.plot(history2.epoch, history2.history[\"val_my_iou_metric\"], label=\"Validation score\")\nax_score.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}