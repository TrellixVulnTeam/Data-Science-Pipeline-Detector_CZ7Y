{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls /kaggle/input/tgs-salt-identification-challenge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder = '/kaggle/input/tgs-salt-identification-challenge'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('{}/{}'.format(data_folder, 'train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['rle_mask'].fillna(-1, inplace=True)\ntrain_df['binary'] = train_df.apply(lambda row: row.rle_mask != -1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[2,'rle_mask']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[2,'id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/tgs-salt-identification-challenge/train.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='./images/a266a2a9df.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename='./masks/a266a2a9df.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_zeros(array, MAX_WIDTH=128, MAX_HEIGHT=128):\n    #assert array.size == (101, 101)\n    padded_image = np.zeros(shape=(128,128))\n    padded_image[13:114, 13:114] = array\n    #assert sum(padded_image) > 0\n    return padded_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import PIL.Image\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_image =  np.asarray(PIL.Image.open('./images/a266a2a9df.png').convert('L'),dtype =np.uint8)\nprint(original_image.shape)\npadded_image = pad_zeros(original_image)\nplt.subplot(\"221\")\nplt.imshow(original_image,cmap = plt.cm.gray)\nplt.subplot(\"222\")\nplt.imshow(padded_image, cmap =plt.cm.gray)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass ImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.train_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.train_frame)\n        \n    def __getitem__(self, idx):\n        #if torch.is_tensor(idx):\n            #idx = idx.tolist()\n            \n        Id = self.train_frame.loc[idx, 'id']\n        feature = pad_zeros(np.asarray(Image.open('{}/images/{}.png'.format(self.root_dir, Id)).convert('L'), dtype =np.uint8))\n        label   = pad_zeros(np.asarray(Image.open('{}/masks/{}.png'.format(self.root_dir, Id)).convert('1'),dtype =np.uint8))\n        sample = {'feature': feature, 'label': label}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    \nclass BinaryDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.train_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.train_frame['rle_mask'].fillna(-1, inplace=True)\n        self.train_frame['binary'] = train_df.apply(lambda row: row.rle_mask != -1, axis=1)\n        \n    def __len__(self):\n        return len(self.train_frame)\n        \n    def __getitem__(self, idx):\n        #if torch.is_tensor(idx):\n            #idx = idx.tolist()\n            \n        Id = self.train_frame.loc[idx, 'id']\n        feature = pad_zeros(np.asarray(Image.open('{}/images/{}.png'.format(self.root_dir, Id)).convert('L'), dtype =np.uint8))\n        feature = np.expand_dims(feature, axis=0)\n        label   = int(self.train_frame.loc[idx,'binary'])\n        sample = {'feature': feature, 'label': label}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_item(idx):\n    \n    Id = train_df.loc[idx, 'id']\n    root_dir = \"./\"\n    feature = np.asarray(PIL.Image.open('{}/images/{}.png'.format(root_dir, Id)).convert('L'), dtype =np.uint8)\n    label   = np.asarray(PIL.Image.open('{}/masks/{}.png'.format(root_dir, Id)).convert('1'),dtype =np.uint8)\n    return {'feature': feature, 'label': label}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgDataset = ImageDataset('/kaggle/input/tgs-salt-identification-challenge/train.csv','./')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_sample(id):\n    import matplotlib.pyplot as plt\n    sample = imgDataset[id]\n    plt.subplot(\"121\")\n    plt.imshow(sample['feature'],cmap = plt.cm.gray)\n    plt.subplot(\"122\")\n    plt.imshow(sample['label'], cmap =plt.cm.gray)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsample = imgDataset[1]\nplt.imshow(sample['feature'], cmap=plt.cm.gray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_sample(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndataloader = DataLoader(imgDataset, batch_size=4,\n                        shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms, utils\n\ndef show_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, labels_batch = \\\n            sample_batched['feature'], sample_batched['label']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n    \n    fig, ax = plt.subplots(nrows=batch_size, ncols =2)\n    for i, row in enumerate(ax):\n        plt.subplot(batch_size, 2, i*2+1)\n        plt.imshow(images_batch[i], cmap = plt.cm.gray)\n        plt.subplot(batch_size, 2, i*2+2)\n        plt.imshow(labels_batch[i], plt.cm.gray)\n    #plt.title('Batch from dataloader')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['feature'].size(),\n          sample_batched['label'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bDataset = BinaryDataset('/kaggle/input/tgs-salt-identification-challenge/train.csv','./')\nbDataset[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binaryDataLoader = DataLoader(bDataset, batch_size=16,\n                        shuffle=True, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i_batch, sample_batched in enumerate(binaryDataLoader):\n    #print(sample_batched)\n    #print(i_batch, sample_batched['feature'],sample_batched['label'])\n    print(sample_batched['feature'].size(), sample_batched['label'])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n#assert '.'.join(torch.__version__.split('.')[:2]) == '1.6'\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_GPU = True\n\ndtype = torch.float32 # we will be using float throughout this tutorial\n\nif USE_GPU and torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\n# Constant to control how frequently we print train loss\nprint_every = 100\n\nprint('using device:', device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchsummary import summary\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, stride=1, kernel_size=3, padding=1,bias=False):\n        super(ResidualBlock,self).__init__()\n\n        self.cnn1 =nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, 1, stride, padding=0, bias=False),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(True)\n        )\n\n        self.cnn2 = nn.Sequential(\n            nn.Conv2d(middle_channels, middle_channels, 3, 1, padding, bias=False),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(True)\n        )\n\n        self.cnn3 = nn.Sequential(\n            nn.Conv2d(middle_channels, out_channels, 1, 1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n            \n    def forward(self,x):\n        residual = x\n        x = self.cnn1(x)\n        x = self.cnn2(x)\n        x = self.cnn3(x)\n        x += self.shortcut(residual)\n        x = nn.ReLU(True)(x)\n        return x\n    \n    \nclass ResNet50(nn.Module):\n    def __init__(self):\n        super(ResNet50,self).__init__()\n        \n#           (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#   (relu): ReLU(inplace=True)\n#   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n        self.block1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.MaxPool2d((3,3), stride=2, dilation=1, padding=1)\n        )\n        \n        self.block2 = nn.Sequential(\n            ResidualBlock(64, 64, 256),\n            ResidualBlock(256, 64, 256),\n            ResidualBlock(256, 64, 256)\n        )\n        \n        self.block3 = nn.Sequential(\n            ResidualBlock(256, 128, 512, stride=2),\n            ResidualBlock(512, 128, 512),\n            ResidualBlock(512, 128, 512),\n            ResidualBlock(512, 128, 512)\n        )\n        \n        self.block4 = nn.Sequential(\n            ResidualBlock(512, 256, 1024, stride=2),\n            ResidualBlock(1024, 256, 1024),\n            ResidualBlock(1024, 256, 1024),\n            ResidualBlock(1024, 256, 1024)\n        )\n\n        self.block5 = nn.Sequential(\n            ResidualBlock(1024, 512, 2048, stride=2),\n            ResidualBlock(2048, 512, 2048),\n            ResidualBlock(2048, 512, 2048),\n            ResidualBlock(2048, 512, 2048)\n        )\n        \n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n        self.fc1 = nn.Linear(2048, 1000)\n\n        self.fc2 = nn.Linear(1000, 2)\n    \n        \n    def forward(self,x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0),-1)\n        x1 = self.fc2(F.relu(self.fc1(x)))\n\n        return x1\n\nresnet = ResNet50()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = resnet.to(device)\n\n#summary(resnet, (1, 128, 128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check binary or not\ndef flatten(x):\n    N = x.shape[0] # read in N, C, H, W\n    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n\ndef check_accuracy_part34(loader, model):\n#     if loader.dataset.train:\n#         print('Checking accuracy on validation set')\n#     else:\n#         print('Checking accuracy on test set')   \n    num_correct = 0\n    num_samples = 0\n    model.eval()  # set model to evaluation mode\n    with torch.no_grad():\n        for sampled_batch in loader:\n            x = sampled_batch['feature']\n            y = sampled_batch['label']\n            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n            y = y.to(device=device, dtype=torch.long)\n            scores = model(x)\n            _, preds = scores.max(1)\n            num_correct += (preds == y).sum()\n            num_samples += preds.size(0)\n        acc = float(num_correct) / num_samples\n        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n        \n\ndef train_part34(model, optimizer, epochs=1):\n    \"\"\"\n    Train a model on CIFAR-10 using the PyTorch Module API.\n    \n    Inputs:\n    - model: A PyTorch Module giving the model to train.\n    - optimizer: An Optimizer object we will use to train the model\n    - epochs: (Optional) A Python integer giving the number of epochs to train for\n    \n    Returns: Nothing, but prints model accuracies during training.\n    \"\"\"\n    model = model.to(device=device)  # move the model parameters to CPU/GPU\n    for e in range(epochs):\n        print(\"***********\", e, \"*************\")\n        for t, sampled_batch in enumerate(binaryDataLoader):\n            model.train()  # put model to training mode\n            x = sampled_batch['feature']\n            y = sampled_batch['label']\n            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n            y = y.to(device=device, dtype=torch.long)\n\n            scores = model(x)\n            loss = F.cross_entropy(scores, y)\n\n            # Zero out all of the gradients for the variables which the optimizer\n            # will update.\n            optimizer.zero_grad()\n\n            # This is the backwards pass: compute the gradient of the loss with\n            # respect to each  parameter of the model.\n            loss.backward()\n\n            # Actually update the parameters of the model using the gradients\n            # computed by the backwards pass.\n            optimizer.step()\n\n            if t % print_every == 0:\n                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n                check_accuracy_part34(binaryDataLoader, model)\n                print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################\n# TODO:                                                                        #         \n# Experiment with any architectures, optimizers, and hyperparameters.          #\n# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n#                                                                              #\n# Note that you can use the check_accuracy function to evaluate on either      #\n# the test set or the validation set, by passing either loader_test or         #\n# loader_val as the second argument to check_accuracy. You should not touch    #\n# the test set until you have finished your architecture and  hyperparameter   #\n# tuning, and only run the test set once at the end to report a final value.   #\n################################################################################\nlearning_rate = 1e-2\nmodel = None\noptimizer = None\n\n# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\nmodel = resnet\noptimizer = optim.SGD(model.parameters(), lr=learning_rate,\n                     momentum=0.9, nesterov=True)\n\n# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n################################################################################\n#                                 END OF YOUR CODE                             \n################################################################################\n\n# You should get at least 70% accuracy\ntrain_part34(model, optimizer, epochs=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}