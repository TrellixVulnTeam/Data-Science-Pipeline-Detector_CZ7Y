{"cells":[{"metadata":{"_uuid":"56e76a641efc73d7e2044fa760fa178ddbb815c7"},"cell_type":"markdown","source":"# Changelog\n* Fixed argument order\n* Implemented another RLE encoding method, suggested by  Tadeusz Hupało\n* Implemented an counter in the tasks, to properly empty queues."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# For testing, multiprocessing and chaining dictionaries\nimport numpy as np\nimport multiprocessing\nfrom collections import ChainMap\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Introduction\nI don't really like waiting, so I wanted to speed up the submission process a little bit. So here is a class you can use to utilize multiprocessing in Python for creating the submission file."},{"metadata":{"trusted":true,"_uuid":"1de99af5fbdcd2178233b9c423c673327696ef26"},"cell_type":"code","source":"# Default RLenc\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  # list of run lengths\n    r = 0  # the current run length\n    pos = 1  # count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs += [pos, r]\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs += [pos, r]\n        pos += r\n        r = 0\n\n    return runs\n\n# RLE encoding, as suggested by Tadeusz Hupało\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"254f956a2e52679a1a42dfd18a5d1ed4ca9fa2c6"},"cell_type":"code","source":"class Consumer(multiprocessing.Process):\n    \"\"\"Consumer for performing a specific task.\"\"\"\n\n    def __init__(self, task_queue, result_queue):\n        \"\"\"Initialize consumer, it has a task and result queues.\"\"\"\n        multiprocessing.Process.__init__(self)\n        self.task_queue = task_queue\n        self.result_queue = result_queue\n\n    def run(self):\n        \"\"\"Actual run of the consumer.\"\"\"\n        while True:\n            next_task = self.task_queue.get()\n            if next_task is None:\n                # Poison pill means shutdown\n                self.task_queue.task_done()\n                break\n            # Fetch answer from task\n            answer = next_task()\n            self.task_queue.task_done()\n            # Put into result queue\n            self.result_queue.put(answer)\n        return\n\n\nclass RleTask_Suggested(object):\n    \"\"\"Wrap the RLE Encoder into a Task.\"\"\"\n\n    def __init__(self, idx, img):\n        \"\"\"Save image to self.\"\"\"\n        self.idx = idx\n        self.img = img\n\n    def __call__(self):\n        \"\"\"When object is called, encode.\"\"\"\n        return {self.idx: rle_encoding(self.img)}\n\nclass RleTask(object):\n    \"\"\"Wrap the RLE Encoder into a Task.\"\"\"\n\n    def __init__(self, idx, img):\n        \"\"\"Save image to self.\"\"\"\n        self.idx = idx\n        self.img = img\n\n    def __call__(self):\n        \"\"\"When object is called, encode.\"\"\"\n        return {self.idx: RLenc(self.img)}\n\nclass MultiOriginal(object):\n    \"\"\"Perform RLE in paralell.\"\"\"\n\n    def __init__(self, num_consumers=2):\n        \"\"\"Initialize class.\"\"\"\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n        self._add_count = 0\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n\n    def add(self, idx, img):\n        \"\"\"Add a task to perform.\"\"\"\n        self._add_count += 1\n        self._tasks.put(RleTask(idx, img))\n\n    def get_results(self):\n        \"\"\"Close all tasks.\"\"\"\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        for _ in range(self._add_count):\n            singles.append(self._results.get())\n        return dict(ChainMap({}, *singles))\n\nclass MultiSuggested(object):\n    \"\"\"Perform RLE in paralell.\"\"\"\n\n    def __init__(self, num_consumers=2):\n        \"\"\"Initialize class.\"\"\"\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n        self._add_count = 0\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n\n    def add(self, idx, img):\n        \"\"\"Add a task to perform.\"\"\"\n        self._add_count += 1\n        self._tasks.put(RleTask_Suggested(idx, img))\n\n    def get_results(self):\n        \"\"\"Close all tasks.\"\"\"\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        for _ in range(self._add_count):\n            singles.append(self._results.get())\n        return dict(ChainMap({}, *singles))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d12f398b5c6f6750382f6bfa41864fd4dd1fb01"},"cell_type":"markdown","source":"# Comparisons\nJust to given an idea of the speed increase."},{"metadata":{"trusted":true,"_uuid":"b0f0c90d03c088bd990ecba4d0656f2a637f2628"},"cell_type":"code","source":"example_batch = np.random.uniform(0, 1, size=(100, 101, 101)) > 0.5\n\n# Wrap the FastRle class into a method so we measure the time\ndef original(array):\n    results = {}\n    for i, arr in enumerate(array):\n        results['%d' % i] = RLenc(arr)\n    return results\n\ndef multi_original(array):\n    rle = MultiOriginal(4)\n    for i, arr in enumerate(array):\n        rle.add('%d' % i, arr)\n    return rle.get_results()\n\ndef suggested(array):\n    results = {}\n    for i, arr in enumerate(array):\n        results['%d' % i] = rle_encoding(arr)\n    return results\n\ndef multi_suggested(array):\n    rle = MultiSuggested(4)\n    for i, arr in enumerate(array):\n        rle.add('%d' % i, arr)\n    return rle.get_results()\n    \n# Measure the time\n%timeit -n1 original(example_batch)\n%timeit -n1 multi_original(example_batch)\n%timeit -n1 suggested(example_batch)\n%timeit -n1 multi_suggested(example_batch)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da34e6e59e534a56a4a1f460da4a7c11ff313d32"},"cell_type":"markdown","source":"The different solutions are, unsurprisingly, heavily dependent on sample size."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f19631033560638cda81126894009a4a5d2ec585"},"cell_type":"code","source":"# Create a loop, collect time info for different methods\nsample_sizes = [100, 250, 500, 1000, 2000, 5000, 10000]\norg, morg, sug, msug = ([], [], [], [])\nfor n_samples in sample_sizes:\n    example_batch = np.random.uniform(0, 1, size=(n_samples, 101, 101)) > 0.5\n    result_org = %timeit -n1 -o original(example_batch)\n    result_multi_org = %timeit -n1 -o multi_original(example_batch)\n    result_sug = %timeit -n1 -o suggested(example_batch)\n    result_multi_sug = %timeit -n1 -o multi_suggested(example_batch)\n    \n    org.append(result_org.average)\n    morg.append(result_multi_org.average)\n    sug.append(result_sug.average)\n    msug.append(result_multi_sug.average)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"082d8d6efb6ba491acc4bb28ba63d3ec0606a393"},"cell_type":"code","source":"# Plot the results\nplt.figure(dpi=150)\nax = plt.axes()\nax.plot(sample_sizes, org, label='original');\nax.plot(sample_sizes, morg, label='multi-original');\nax.plot(sample_sizes, sug, label='suggested');\nax.plot(sample_sizes, msug, label='multi-suggested');\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73e4f8c4221746f6bec09c967e1dc7a86d98db84"},"cell_type":"markdown","source":"# Sanity check\nJust to make sure, that all the new solutions are returning the same thing!"},{"metadata":{"trusted":true,"_uuid":"36320ea9a833cda2e423a412354784c6e853319f"},"cell_type":"code","source":"example_batch = np.random.uniform(0, 1, size=(10, 101, 101)) > 0.5\na = original(example_batch)\nb = multi_original(example_batch)\nc = suggested(example_batch)\nd = multi_suggested(example_batch)\n# Make sure they are the same\nfor key in a:\n    if a[key] != b[key]:\n        print(\"Multi processed original differs from original!\")\n    if a[key] != c[key]:\n        print(\"Suggested differs from original!\")\n    if a[key] != d[key]:\n        print(\"Multi processed suggested differs from original!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfc1292a383a7ce4230332cabe0a1e838992e8f8"},"cell_type":"markdown","source":"# Conclusion\nUsing the method provided by Tadeusz Hupało is blazingly fast!  No performance gains are seen when using multiple processes to perform the task.\n\nMight be possible to make it faster, but I'm quite happy with this new method! \n\n\nHappy Kaggling!"},{"metadata":{"trusted":true,"_uuid":"44d080a35774ef265ffd98159fc56fd4564640be"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b81bb6266861f5c3880ecb0977e67c59a9fd54b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}