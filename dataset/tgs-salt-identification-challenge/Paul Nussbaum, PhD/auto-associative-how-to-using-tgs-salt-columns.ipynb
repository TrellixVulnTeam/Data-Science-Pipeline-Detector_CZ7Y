{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Someone asked me how create an Auto-Associative Neural Network in a different discussion group, so I thought I'd use the TGS Salt contest data to explain. \n\nAn auto-associative network has the same number of input dimensions as it does output dimensions, because it is being trained to precisely recreate the input at the output. This is a boring math exercise, except in the case when some middle layer of the feed forward network has much fewer dimensions than the input. \n\n[many dimensions n] --> [much fewer dimensions m] --> [many dimensions n]\n\nThis has the effect of compressing the original data from n dimensions down to m dimensions. The auto-associative network is therefore a CODEC, with the encoder being the front half, and the decoder being the back half. \n\nIt also provides the researcher a qualitative way of picking a lower bound on the number of dimensions for internal layers of their feed-forward networks. \n\nHow? Just try different values for m, and then look at the output. If you can no longer recognize (classify) the output, then probably neither can a deep learning algorithm, and so m is probably too small. \n\nMake sure that the smallest layer in your network (through which all data must pass) is larger than that value of m you found to be \"too small.\"\n\n* References - originally forked from AshishPatel and collaborator: Have you check this approach... ðŸŽµ ðŸŽµ ðŸŽµ? - forked from seasalt or not??? by AshishPatel (+0/â€“0)\n\n* P.S. The code also shows a method of taking the seismic 2d image data and turning it into a series of 1d columns. I was working on this as part of my contest entry. I was experimenting with doing some first-pass signal analysis transformations on the seismic data as the one-dimensional time varying data that it is (the time varying signal of the bounced back sound) like an ultrasound. I wanted to sanity check not only internal network dimensions, but also any extractable features that would make the classification run smoother.\n\nStart with some included components:"},{"metadata":{"trusted":true,"_uuid":"00833d394e3069216af171fd979c814e7e1e430d","_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nfrom tqdm import tqdm_notebook, tnrange\nfrom itertools import chain\n\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, SpatialDropout2D\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"173fd4ce849dc06e56b866839e27c27f882aeb6d"},"cell_type":"markdown","source":"Limit the training set to only 500 images for speed. Feel free to use the whole set (buy commenting out the last line below), etc..."},{"metadata":{"trusted":true,"_uuid":"0e26e21ff39e8b2afc0003fec4e4f5269f61aa4c","_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"im_width = 101\nim_height = 101\nim_chan = 1\npath_train = '../input/train/'\n\n# create a \"list\" of filenames of all the image files\ntrain_ids = next(os.walk(path_train+\"images\"))[2]\n# LIMIT THE SIZE OF THE TRAINING AND TESTING SETS FOR SPEED - Comment out the below to use the whole set\ntrain_ids=train_ids[:500]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f15b03d887b1f9eef6f612e8a0c084ca399ce3b"},"cell_type":"markdown","source":"Instead of using the 101x101 image graphics from the contest, will only use individual columns of the data (101x1), thereby creating one-hundred and one times more examples.\n\nApologies for not using TensorFlow below and saving a few pennies of electricity (would have been much quicker, but for this small data set it's OK)"},{"metadata":{"trusted":true,"_uuid":"a8f02165966489c8a21bb7127bb88e7cf607599d","collapsed":true},"cell_type":"code","source":"X_train = np.zeros((len(train_ids) * im_width, im_height,1), dtype=np.uint8)\nprint('Loading train images and converting them into columns... ')\nsys.stdout.flush()\n\n# for loop counts n, and also pulls from train_ids list using \"progress bar\" tqdm_notebook()\nfor n, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n    path = path_train\n    img = load_img(path + '/images/' + id_)\n    x = img_to_array(img)[:,:,1]\n    for y in range(0,im_width):\n        for z in range(0,im_height):\n            X_train[n*im_width + y,z] = x[z,y]\n\nprint(X_train.shape)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f925dae282a6669f9c475528d08b0932f47c1db4"},"cell_type":"markdown","source":"Pick random image and make sure the columns reconstruct to look the same (sanity check)."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"faf6ea42655fb0f5ee8994a65a7c3bef888ef1ae","collapsed":true},"cell_type":"code","source":"print(\"SANITY CHECK\")\n# Check if training data looks all right\n# by reconstructing an image from the columns\nix = random.randint(0, len(train_ids))\nprint(\"original image\")\noriginal_image = load_img(path_train + '/images/' + train_ids[ix])\nplt.imshow(original_image)\nplt.show()\n\nprint(\"reconstructed columns image\")\nix = int(ix*im_width)\nimage = np.zeros((im_width,im_height), dtype=np.uint8)\nfor x in range(0,im_width):\n    for y in range(0,im_height):\n        image[y,x]=X_train[ix+x,y]\nplt.imshow(np.dstack((image, image, image)))\nplt.show()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c983d9ff7c94d40f8f0fb9a1be820830ca3a4d8"},"cell_type":"markdown","source":"**Auto-Associatve Network Example**\n* Input (columns of data 101 x 1)\n* Middle layers 101 or fewer \"relu\" activation function perceptrons\n* Output layer - 101 \"linear\" activated perceptrons"},{"metadata":{"trusted":true,"_uuid":"58e87797db5bb02b8f0ad6a0af6592e94f9f8b3f","collapsed":true},"cell_type":"code","source":"# use a simple sequential model\nmodel = Sequential()\n# this first layer (connecting the input to the output layer) will have fewer than 101 neurons.\n\n#######################################################################################\n#     CHANGE THE FIRST PARAMETER (20) BELOW                                           #\n#     THIS IS THE VALUE OF m                                                          #\n# TRY A VERY SMALL VALUE (DOWN to 1)                                                  #\n#     SEE THAT YOU CAN NO LONGER RECOGNIZE THE IMAGE, MUCH LESS FIND THE SALT         #\n# TRY A VERY LARGE VALUE (UP TO 101)                                                  #\n#     SEE THAT THE ORIGINAL IMAGE IS NEARLY PERFECTLY RECREATED                       #\n#######################################################################################\nmodel.add(Dense(20, activation='relu', input_shape=(im_height,)))\n\n# this output layer has to have 101 neurons, and needs to be linearly activated\nmodel.add(Dense(101, activation='linear'))\nmodel.compile(loss='mean_squared_error',\n              optimizer='adam')\n# Diplay the model summary\nprint(\"model summary\")\nmodel.summary()\nprint(\"layer shapes of weights and bias arrays\")\nfor x in range(0,len(model.layers) - 1):\n    print(x)\n    print(model.get_layer(index=x+1).get_weights()[0].shape)\n    print(model.get_layer(index=x+1).get_weights()[1].shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58969e2e3bdca3b94da4ebd4e513a83455adf00a","scrolled":false,"collapsed":true},"cell_type":"code","source":"earlystopper = EarlyStopping(patience=2, verbose=1) \ncheckpointer = ModelCheckpoint('tgs-salt-columns-autoassoc', verbose=1, save_best_only=True)\nresults = model.fit(X_train[:,:,0], X_train[:,:,0], validation_split=0.1, batch_size=100, epochs=300, \n                    callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2316034edcb7227673fd9b69264ca9c0d0e87f14","collapsed":true},"cell_type":"code","source":"# Run all the data through the created network\nmodel = load_model('tgs-salt-columns-autoassoc')\npreds_train_t = model.predict(X_train[:,:,0], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24defa25c00d0d91b38e559515e78c63f4d26e2b"},"cell_type":"markdown","source":"Let's see how we did! If the auto-associative  network worked perfectly, then the output image should look EXACTLY like the input image.\n\nNaturally, this won't be the case if the number of neurons in the \"center\" layer is less than 101."},{"metadata":{"trusted":true,"_uuid":"6302c46fc76d8a43cb87d01c43c60c3c8f0ac98b","collapsed":true},"cell_type":"code","source":"# RUN THIS ONE A FEW TIMES TO VIEW DIFFERENT IMAGES AND JUDGE FOR YOURSELF\n# Pick a random example\nix = random.randint(0, int(len(train_ids)))\nix = int((ix-1)*im_width) # subtract 1 to give radix 0\n\nprint(\"original image from reconstructed input columns\")\nimage = np.zeros((im_width,im_height), dtype=np.uint8)\nfor x in range(0,im_width):\n    for y in range(0,im_height):\n        image[y,x]=X_train[ix+x,y]\nplt.imshow(np.dstack((image, image, image)))\nplt.show()\n\nprint(\"auto-associated image (encoded image) from reconstructed output columns\")\npred_mask = np.zeros((im_width,im_height))\nfor x in range(0,im_width):\n    for y in range(0,im_height):\n        pred_mask[y,x]=preds_train_t[ix+x,y]\n# USE SCALING ONLY FOR AUTO-ASSOC, NOT FOR SALT DETECTION or IF CONTRAST ENHANCEMENT NEEDED\nscale = pred_mask.max() - pred_mask.min()\npred_mask = (pred_mask - pred_mask.min()) / scale\nplt.imshow(np.dstack((pred_mask, pred_mask, pred_mask)))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}