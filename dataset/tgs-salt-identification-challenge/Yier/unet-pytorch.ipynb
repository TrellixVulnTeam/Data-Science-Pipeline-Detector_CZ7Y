{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport math\nimport glob\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as T\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel_path = 'unet_model.pth'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! unzip -q /kaggle/input/tgs-salt-identification-challenge/competition_data.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = \"/kaggle/working/competition_data/train/images\"\nmask_path = \"/kaggle/working/competition_data/train/masks\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['6caec01e67','2bfa664017','1544a0e952']\nimages = [Image.open(os.path.join(image_path, name+'.png')) for name in names]\nmasks = [Image.open(os.path.join(mask_path, name+'.png')) for name in names]\n\ntransforms = T.Compose([T.Grayscale(), T.ToTensor()])\nx = torch.stack([transforms(image) for image in images])\ny = torch.stack([transforms(mask) for mask in masks])\n\nfig = plt.figure( figsize=(9, 9))\n\nax = fig.add_subplot(331)\nplt.imshow(images[0])\nax = fig.add_subplot(332)\nplt.imshow(masks[0])\nax = fig.add_subplot(333)\nax.imshow(x[0].squeeze(), cmap=\"Greys\")\nax.imshow(y[0].squeeze(), alpha=0.5, cmap=\"Greens\")\n\nax = fig.add_subplot(334)\nplt.imshow(images[1])\nax = fig.add_subplot(335)\nplt.imshow(masks[1])\nax = fig.add_subplot(336)\nax.imshow(x[1].squeeze(), cmap=\"Greys\")\nax.imshow(y[1].squeeze(), alpha=0.5, cmap=\"Greens\")\n\nax = fig.add_subplot(337)\nplt.imshow(images[2])\nax = fig.add_subplot(338)\nplt.imshow(masks[2])\nax = fig.add_subplot(339)\nax.imshow(x[2].squeeze(), cmap=\"Greys\")\nax.imshow(y[2].squeeze(), alpha=0.5, cmap=\"Greens\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class segmentDataset(Dataset):\n    def __init__(self, image_path, mask_path):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        \n        image_list= glob.glob(image_path +'/*.png')\n        sample_names = []\n        for file in image_list:\n            sample_names.append(file.split('/')[-1].split('.')[0])\n            \n        self.sample_names = sample_names\n        \n        self.transforms = T.Compose([T.Grayscale(), T.ToTensor()])\n            \n    def __getitem__(self, idx):\n        image = Image.open(os.path.join(self.image_path, self.sample_names[idx]+'.png') )\n        mask = Image.open(os.path.join(self.mask_path, self.sample_names[idx]+'.png') )\n        return self.transforms(image), self.transforms(mask)\n\n    def __len__(self):\n        return len(self.sample_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = segmentDataset(image_path, mask_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_images = 64\ngrid_width = 8\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, idx in enumerate(range(max_images)):\n    image, mask = train_dataset[idx]\n    \n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(image.squeeze(), cmap=\"Greys\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Greens\")\n   \n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture\n\nThe model architecture is modified from https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class convBlock(nn.Module):\n    def __init__(self, in_channels, filters, size, stride = 1, activation = True):\n        super(convBlock, self).__init__()\n        self.activation = activation\n        self.conv = nn.Conv2d(in_channels, filters, size, stride = stride, padding = size//2)\n        self.norm = nn.BatchNorm2d(filters)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        if self.activation:\n            return F.relu(x)\n        else:\n            return x\n    \nclass residualBlock(nn.Module):\n    def __init__(self, in_channels, filters, size = 3):\n        super(residualBlock, self).__init__()\n\n        self.norm = nn.BatchNorm2d(in_channels)\n        self.conv1 = convBlock(in_channels, filters, size)\n        self.conv2 = convBlock(filters, filters, size, activation=False)\n\n    def forward(self, x):\n        residual = x  \n        x = F.relu(x)\n        x = self.norm(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        #x += residual\n        return x \n    \nclass deconvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size = 2, stride = 2):\n        super(deconvBlock, self).__init__()\n        \n        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride)\n\n    def forward(self, x1, x2):\n        xd = self.deconv(x1)\n        x = torch.cat([xd, x2], dim = 1)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UnetModel(nn.Module):\n\n    def __init__(self, filters = 16, dropout = 0.5):\n        super(UnetModel, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, filters, 3, padding = 1),\n            residualBlock(filters, filters),\n            residualBlock(filters, filters),\n            nn.ReLU()\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(dropout/2),\n            nn.Conv2d(filters, filters * 2, 3, padding = 1),\n            residualBlock(filters * 2, filters * 2),\n            residualBlock(filters * 2, filters * 2),\n            nn.ReLU()\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 2, filters * 4, 3, padding = 1),\n            residualBlock(filters * 4, filters * 4),\n            residualBlock(filters * 4, filters * 4),\n            nn.ReLU()\n        )\n        \n        self.conv4 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 4, filters * 8, 3, padding = 1),\n            residualBlock(filters * 8, filters * 8),\n            residualBlock(filters * 8, filters * 8),\n            nn.ReLU()\n        )\n            \n\n        self.middle = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 8, filters * 16, 3, padding = 3//2),\n            residualBlock(filters * 16, filters * 16),\n            residualBlock(filters * 16, filters * 16),\n            nn.ReLU()\n        )\n        \n        self.deconv4 = deconvBlock(filters * 16, filters * 8, 2)\n        self.upconv4 = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 16, filters * 8, 3, padding = 1),\n            residualBlock(filters * 8, filters * 8),\n            residualBlock(filters * 8, filters * 8),\n            nn.ReLU()\n        )\n  \n\n        self.deconv3 = deconvBlock(filters * 8, filters * 4, 3)\n        self.upconv3 = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 8, filters * 4, 3, padding = 1),\n            residualBlock(filters * 4, filters * 4),\n            residualBlock(filters * 4, filters * 4),\n            nn.ReLU()\n        )\n        \n        self.deconv2 = deconvBlock(filters * 4, filters * 2, 2)\n        self.upconv2 = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 4, filters * 2, 3, padding = 1),\n            residualBlock(filters * 2, filters * 2),\n            residualBlock(filters * 2, filters * 2),\n            nn.ReLU()\n        )\n\n        self.deconv1 = deconvBlock(filters * 2, filters, 3)\n        self.upconv1 = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(filters * 2, filters, 3, padding = 1),\n            residualBlock(filters, filters),\n            residualBlock(filters, filters),\n            nn.ReLU(),\n            nn.Dropout(dropout/2),\n            nn.Conv2d(filters, 1, 3, padding = 1)\n        )\n\n    def forward(self, x):\n        conv1 = self.conv1(x) \n        # 101 -> 50\n        conv2 = self.conv2(conv1) \n        # 50 -> 25\n        conv3 = self.conv3(conv2) \n        # 25 -> 12\n        conv4 = self.conv4(conv3) \n        # 12 - 6\n        x = self.middle(conv4) \n        \n        # 6 -> 12\n        x = self.deconv4(x, conv4)\n        x = self.upconv4(x)\n        # 12 -> 25\n        x = self.deconv3(x, conv3)\n        x = self.upconv3(x)\n        # 25 -> 50\n        x = self.deconv2(x, conv2)\n        x = self.upconv2(x)\n        # 50 -> 101\n        x = self.deconv1(x, conv1)\n        x = self.upconv1(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training\nNote that for each pixel we get a value between 0 to 1.\n0 represents no salt and 1 represents salt.\nWe take 0.5 as the threshold to decide whether to classify a pixel as 0 or 1.\n\nChoose binary cross entropy as the loss function.\n\nThe learning rate is adjusted by monitoring IoU score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_iou_score(outputs, labels):\n    A = labels.squeeze().bool()\n    pred = torch.where(outputs<0., torch.zeros_like(outputs), torch.ones_like(outputs))\n    B = pred.squeeze().bool()\n    intersection = (A & B).float().sum((1,2))\n    union = (A| B).float().sum((1, 2)) \n    iou = (intersection + 1e-6) / (union + 1e-6)  \n    return iou\n  \ndef train_one_batch(model, x, y):\n    x, y = x.to(device), y.to(device)\n\n    outputs = model(x)\n    loss = loss_fn(outputs, y)\n    iou = get_iou_score(outputs, y).mean()\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    return loss.item(), iou.item()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 200\nBATCH_SIZE = 64\n\nmodel = UnetModel().to(device)\nmodel.train()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n\nloss_fn = nn.BCEWithLogitsLoss()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\nsteps  = train_dataset.__len__()// BATCH_SIZE\nprint(steps,\"steps per epoch\")\n\nstart = time.time()\ntrain_losses = []\ntrain_ious = []\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print('-' * 10)\n    print('Epoch {}/{}'.format(epoch, NUM_EPOCHS))\n    running_iou = []\n    running_loss = []\n    for step, (x, y) in enumerate(train_dataloader):\n        loss, iou = train_one_batch(model, x, y)\n        running_iou.append(iou)\n        running_loss.append(loss)\n        print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}'.format(100*(step+1)/steps, loss,iou), end = \"\") \n        \n    print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}\\t{}'.format(100*(step+1)/steps,np.mean(running_loss),np.mean(running_iou), timeSince(start)))\n    scheduler.step(np.mean(running_iou))\n    \n    train_losses.append(loss)\n    train_ious.append(iou)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_losses, label = 'loss')\nplt.plot(train_ious, label = 'IoU')\nplt.xlabel('Epoch')\nplt.ylabel('Metric')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save weights    \ntorch.save(model.cpu().state_dict(), model_path)\n\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['6caec01e67','2bfa664017','1544a0e952']\nimages = [Image.open(os.path.join(image_path, name+'.png')) for name in names]\nmasks = [Image.open(os.path.join(mask_path, name+'.png')) for name in names]\n\ntransforms = T.Compose([T.Grayscale(), T.ToTensor()])\nx = torch.stack([transforms(image) for image in images])\ny = torch.stack([transforms(mask) for mask in masks])\n\noutputs = model(x)\npreds = torch.where(outputs<0., torch.zeros_like(outputs), torch.ones_like(outputs))\nious = get_iou_score(outputs, y)\n\nfig = plt.figure( figsize=(9, 12))\n\nax = fig.add_subplot(331)\nplt.imshow(images[0])\nax = fig.add_subplot(332)\nax.imshow(x[0].squeeze(), cmap=\"Greys\")\nax.imshow(y[0].squeeze(), alpha=0.5, cmap=\"Greens\")\nax = fig.add_subplot(333)\nax.imshow(x[0].squeeze(), cmap=\"Greys\")\nax.imshow(preds[0].squeeze(), alpha=0.5, cmap=\"OrRd\")\nax.set_title(\"IoU: \" + str(round(ious[0].item(), 2)), loc = 'left')\n\nax = fig.add_subplot(334)\nplt.imshow(images[1])\nax = fig.add_subplot(335)\nax.imshow(x[1].squeeze(), cmap=\"Greys\")\nax.imshow(y[1].squeeze(), alpha=0.5, cmap=\"Greens\")\nax = fig.add_subplot(336)\nax.imshow(x[1].squeeze(), cmap=\"Greys\")\nax.imshow(preds[1].squeeze(), alpha=0.5, cmap=\"OrRd\")\nax.set_title(\"IoU: \" + str(round(ious[1].item(), 2)), loc = 'left')\n\nax = fig.add_subplot(337)\nplt.imshow(images[2])\nax = fig.add_subplot(338)\nax.imshow(x[2].squeeze(), cmap=\"Greys\")\nax.imshow(y[2].squeeze(), alpha=0.5, cmap=\"Greens\")\nax = fig.add_subplot(339)\nax.imshow(x[2].squeeze(), cmap=\"Greys\")\nax.imshow(preds[2].squeeze(), alpha=0.5, cmap=\"OrRd\")\nax.set_title(\"IoU: \" + str(round(ious[2].item(), 2)), loc = 'left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainiter = iter(train_dataloader)\nimages, masks = next(trainiter)\n\noutputs = model(images)\npreds = torch.where(outputs<0., torch.zeros_like(outputs), torch.ones_like(outputs))\nious = get_iou_score(outputs, masks).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_images = outputs.size(0)\ngrid_width = 8\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, _data in enumerate(zip(images, masks, preds, ious)):\n    image, mask, pred, iou = _data\n    \n    ax = axs[int(i / grid_width), i % grid_width]\n    \n    ax.imshow(image.squeeze(), cmap = \"Greys\")\n    ax.imshow(mask.squeeze(), alpha = 0.5, cmap = \"Greens\")\n    ax.imshow(pred.squeeze(), alpha = 0.3, cmap = \"OrRd\")\n    ax.set_title(\"IoU: \" + str(round(iou, 2)), loc = 'left')\n    \n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks\n\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ntransforms = T.Compose([T.Grayscale(), T.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = \"/kaggle/working/competition_data/test/images\"\nsub_df = pd.read_csv('/kaggle/working/competition_data/sample_submission.csv')\nn = sub_df.shape[0]\n\nrle_mask = []\nfor idx in range(n):\n    ## load image\n    sample_name = sub_df['id'][idx]\n    image = Image.open(os.path.join(image_path, sample_name+'.png') )\n    image = transforms(image)\n    ## predict\n    out = model(image.unsqueeze(0)).squeeze()\n    pred = torch.where(out<0., torch.zeros_like(out), torch.ones_like(out))\n    ## write mask\n    rle_mask.append(rle_encode(pred.numpy()))\n    print(\"\\rprogress {}/{}\".format(idx+1, n), end = \"\")\n    \nsub_df['rle_mask'] = rle_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}