{"cells":[{"metadata":{"_uuid":"c8fc6c3e84f282d0e6f664caf9848297b6c62a16"},"cell_type":"markdown","source":"Unet, resnet34, and Kfold are combined in this script. But the result is pretty bad :( I would be appreciate if someone could give me some hints to enhance the score. \n\nThanks~ happy competition :)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport six\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.regularizers import l2\nfrom keras import optimizers\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\n\n\nimport time\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c36d16775556c3e358edefab5710dc1541ccfc4"},"cell_type":"code","source":"cv_total = 2\n\nepochs_1 = 1\nbatch_size_1 = 32\n\nepochs_2 = 1\nbatch_size_2 = 32\n\nversion = 5_100_32\nbasic_name_ori = f'Unet_resnet_v{version}'\nsave_model_name = basic_name_ori + '.model'\nsubmission_file = basic_name_ori + '.csv'\n\nprint(save_model_name)\nprint(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63c469280793719bf311d51e6ba2cdaea157d175"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 101\n\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a64babef03b9a0dbc94387a1dad54971c3e028d"},"cell_type":"code","source":"# Loading of training/testing ids and depths\ntrain_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9db7884caf13152e6088e0766bd003a2db7fd423"},"cell_type":"code","source":"def extract_hypercolumn(model, layer_indexes, instance):\n    layers = [model.layers[li].output for li in layer_indexes]\n    get_feature = K.function([model.layers[0].input],layers)\n    for layer in layers:\n        print(layer.name)\n        layer_output = get_feature([x])[0]\n        feature_maps = get_feature(instance)\n        hypercolumns = []\n    for convmap in feature_maps:\n        print(convmap.shape)\n        conv_out = convmap[0, :, :, :]\n        feat_map = conv_out.transpose((2,0,1))\n        print(\"F sz :\",feat_map.shape)\n        for fmap in feat_map: \n            #print(fmap.shape)\n            upscaled =resize(fmap, (224, 224), mode='constant', preserve_range=True)\n            hypercolumns.append(upscaled)\n    return np.asarray(hypercolumns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57800564d49019c734186603483eaedb5129e4e"},"cell_type":"markdown","source":"from keras import backend as K\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom skimage.transform import resize\nimport sklearn.cluster as cluster\n\n\n\n#model = VGG16(weights='imagenet')\nmodel = VGG16(weights='../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n\nfor idx in tqdm_notebook(train_df.index):\n    img = image.load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    \n    # with a Sequential model\n    get_feature = K.function([model.layers[0].input],[model.layers[2].output])\n    #layer_output = get_feature([x])[0]\n    feat = get_feature([x])[0]\n    \n    conv_outputs = feat[0, :, :, :]\n    conv_outputs.shape\n    \n    feat_map = conv_outputs.transpose((2,0,1))\n    feat_map.shape\n    layers_extract = [2]\n    hc = extract_hypercolumn(model, layers_extract, [x])\n    avg_hc=np.average(hc, axis=0)\n    layers_extract = [5,8,12]\n    hc2 = extract_hypercolumn(model, layers_extract, [x])\n    m = hc2.transpose(1,2,0).reshape(101 * 101, -1)\n    kmeans = cluster.KMeans(n_clusters=2, max_iter=300, n_jobs=5, precompute_distances=True)\n    cluster_labels = kmeans.fit_predict(m)\n    imcluster = np.zeros((101,101))\n    imcluster = imcluster.reshape((101*,))\n    imcluster = cluster_labels\n    train_df[\"images\"] = [np.array(imcluster)]"},{"metadata":{"trusted":true,"_uuid":"80c3768717007fb5f087d3e01619f1a9f9a3beac"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f55103f7daad6f03ec874c643077fe686c31bee"},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c916289a52d0576816502db92470c5d47926c9e6"},"cell_type":"markdown","source":"#### calculate mask type for stratify, the difficuly of training different mask type is different. \n* Reference  from Heng's discussion, search \"error analysis\" in the following link\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984#382657****"},{"metadata":{"trusted":true,"_uuid":"030d17f898e7d9b7eccfe8726dd1fb1407ec98cd"},"cell_type":"code","source":"#### Reference  from Heng's discussion\n# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984#382657\ndef get_mask_type(mask):\n    border = 10\n    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n\n    cover = (mask>0.5).sum()\n    if cover < 8:\n        return 0 # empty\n    if cover == ((mask*outer) > 0.5).sum():\n        return 1 #border\n    if np.all(mask==mask[0]):\n        return 2 #vertical\n\n    percentage = cover/(101*101)\n    if percentage < 0.15:\n        return 3\n    elif percentage < 0.25:\n        return 4\n    elif percentage < 0.50:\n        return 5\n    elif percentage < 0.75:\n        return 6\n    else:\n        return 7\n\ndef histcoverage(coverage):\n    histall = np.zeros((1,8))\n    for c in coverage:\n        histall[0,c] += 1\n    return histall\n\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_target, 2)\n\ntrain_df[\"coverage_class\"] = train_df.masks.map(get_mask_type)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbed9f883dd9c6ce22767c6a86bce4fe12b8a45"},"cell_type":"code","source":"train_all = []\nevaluate_all = []\nskf = StratifiedKFold(n_splits=cv_total, random_state=1234, shuffle=True)\nfor train_index, evaluate_index in skf.split(train_df.index.values, train_df.coverage_class):\n    train_all.append(train_index)\n    evaluate_all.append(evaluate_index)\n    print(train_index.shape,evaluate_index.shape) # the shape is slightly different in different cv, it's OK","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"738cb1bf628e12bb471a5a5d36cd68814004f5a5"},"cell_type":"code","source":"def get_cv_data(cv_index):\n    train_index = train_all[cv_index-1]\n    evaluate_index = evaluate_all[cv_index-1]\n    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    return x_train,y_train,x_valid,y_valid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2956f050f3aa78830ca41df34ff948deea9c6a82"},"cell_type":"markdown","source":"# Show  some examples of different mask"},{"metadata":{"trusted":true,"_uuid":"ecf001b66bdc494b3869c25ce26760705f25a9a0"},"cell_type":"code","source":"cv_index = 1\ntrain_index = train_all[cv_index-1]\nevaluate_index = evaluate_all[cv_index-1]\n\nprint(train_index.shape,evaluate_index.shape)\nhistall = histcoverage(train_df.coverage_class[train_index].values)\nprint(f'train cv{cv_index}, number of each mask class = \\n \\t{histall}')\nhistall_test = histcoverage(train_df.coverage_class[evaluate_index].values)\nprint(f'evaluate cv{cv_index}, number of each mask class = \\n \\t {histall_test}')\n\nfig, axes = plt.subplots(nrows=2, ncols=8, figsize=(24, 6), sharex=True, sharey=True)\n\n# show mask class example\nfor c in range(8):\n    j= 0\n    for i in train_index:\n        if train_df.coverage_class[i] == c:\n            axes[j,c].imshow(np.array(train_df.masks[i])  )\n            axes[j,c].set_axis_off()\n            axes[j,c].set_title(f'class {c}')\n            j += 1\n            if(j>=2):\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fb577cdf27f365d4a912728c2a7654d0e60fac8"},"cell_type":"code","source":"def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02967d71ee7f936254ab54acf2aa7c2e038a2b21"},"cell_type":"code","source":"size = (3, 3)\ndef build_model(input_layer, start_neurons, DropoutRatio = 0.5):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, size, activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, size, activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, size, activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, size, activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, size, activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, size, strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, size, strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, size, strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, size, strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n    output_layer =  Activation('sigmoid')(output_layer_noActi)\n    \n    return output_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd5e479d3aa211bcb5fe32ce9c4d71cd012eefa"},"cell_type":"code","source":"def get_iou_vector(A, B):\n    A = np.squeeze(A) # new added \n    B = np.squeeze(B) # new added\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n            metric.append(1)\n            continue\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0)  )/ (np.sum(union > 0) )\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred> 0.5 ], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred > 0.0 ], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0a9b8b1c1bbd5a8ae63b5fc283ee068afbd304f"},"cell_type":"code","source":"# code download from: https://github.com/bermanmaxim/LovaszSoftmax\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\ndef lovasz_loss(y_true, y_pred):\n    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n    #logits = K.log(y_pred / (1. - y_pred))\n    logits = y_pred #Jiaxin\n    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9cc638e5822c8a75e88df5c565178487a482ad7"},"cell_type":"markdown","source":"# ResNet 34"},{"metadata":{"trusted":true,"_uuid":"39557aced78bfa762c3cef1adc22960c302facc0"},"cell_type":"code","source":"# https://github.com/raghakot/keras-resnet/blob/master/resnet.py\ndef _bn_relu(input):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n    return Activation(\"relu\")(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(input)\n        return _bn_relu(conv)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    \"\"\"Helper to build a BN -> relu -> conv block.\n    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        activation = _bn_relu(input)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(activation)\n\n    return f\n\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\ndef basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n                           strides=init_strides,\n                           padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n                                  strides=init_strides)(input)\n\n        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n        return _shortcut(input, residual)\n\n    return f\n\ndef _residual_block(block_function, filters, repetitions, is_first_layer=False):\n    \"\"\"Builds a residual block with repeating bottleneck blocks.\n    \"\"\"\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            input = block_function(filters=filters, init_strides=init_strides,\n                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n        return input\n\n    return f\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    if K.image_dim_ordering() == 'tf':\n        ROW_AXIS = 1\n        COL_AXIS = 2\n        CHANNEL_AXIS = 3\n    else:\n        CHANNEL_AXIS = 1\n        ROW_AXIS = 2\n        COL_AXIS = 3\n\n\ndef _get_block(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError('Invalid {}'.format(identifier))\n        return res\n    return identifier\n\n\nclass ResnetBuilder(object):\n    @staticmethod\n    def build(input_shape, block_fn, repetitions,input_tensor):\n        _handle_dim_ordering()\n        if len(input_shape) != 3:\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n\n        # Permute dimension order if necessary\n        if K.image_dim_ordering() == 'tf':\n            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n\n        # Load function from str if needed.\n        block_fn = _get_block(block_fn)\n        \n        if input_tensor is None:\n            img_input = Input(shape=input_shape)\n        else:\n            if not K.is_keras_tensor(input_tensor):\n                img_input = Input(tensor=input_tensor, shape=input_shape)\n            else:\n                img_input = input_tensor\n                \n        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(img_input)\n        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n\n        block = pool1\n        filters = 64\n        for i, r in enumerate(repetitions):\n            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n            filters *= 2\n\n        # Last activation\n        block = _bn_relu(block)\n\n        model = Model(inputs=img_input, outputs=block)\n        return model\n\n    @staticmethod\n    def build_resnet_34(input_shape,input_tensor):\n        return ResnetBuilder.build(input_shape, basic_block, [3, 4, 6, 3],input_tensor)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37c7a59d45009a4d0fd7d36595113e6f7d16045d"},"cell_type":"markdown","source":"# U-Net with ResNet34 Encoder"},{"metadata":{"trusted":true,"_uuid":"55b23b16b3aa68ac4f744b6605e280b831346ecc"},"cell_type":"code","source":"def build_complie_model(lr = 0.1):\n    input_layer = Input((img_size_target, img_size_target, 1))\n    output_layer = build_model(input_layer, 16,0.5)\n\n    model1 = Model(input_layer, output_layer)\n\n    c = optimizers.adam(lr = lr)\n    model1.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric])\n    return model1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd53be0773daa9664a8ec7946e0e4ed94e88226"},"cell_type":"code","source":"def UResNet34(input_shape=(101, 101, 1), classes=1, decoder_filters=16, decoder_block_type='upsampling',\n                       encoder_weights=\"imagenet\", input_tensor=None, activation='sigmoid', **kwargs):\n\n    backbone = ResnetBuilder.build_resnet_34(input_shape=input_shape,input_tensor=input_tensor)\n    \n    input_layer = backbone.input #input = backbone.input\n    output_layer = build_model(input_layer, 16,0.5) #x\n    model1 = Model(input_layer, output_layer)\n    c = optimizers.adam(lr = 0.1)\n\n    model1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n    #model1.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric])\n    model1.name = 'u-resnet34'\n    \n    return model1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37d0e7d4709525205025b97a5f046cf0cb642389"},"cell_type":"code","source":"def plot_history(history,metric_name):\n    fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\n    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax_loss.legend()\n    ax_score.plot(history.epoch, history.history[metric_name], label=\"Train score\")\n    ax_score.plot(history.epoch, history.history[\"val_\" + metric_name], label=\"Validation score\")\n    ax_score.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"590dd6ad091ef612c0bca4b6bfa2eecc6aa96bbd"},"cell_type":"code","source":"def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test = model.predict(x_test).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict(x_test_reflect).reshape(-1, img_size_target, img_size_target)\n    preds_test += np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    return preds_test/2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea305bb18df6e3849874dc957c85d0edf51de06"},"cell_type":"markdown","source":"# Training 1"},{"metadata":{"trusted":true,"_uuid":"30622932f68888e895a9b8cac91810a1bb3c5e75"},"cell_type":"code","source":"# training\nious = [0] * cv_total\nfor cv_index in range(cv_total):\n    basic_name = f'Unet_resnet_v{version}_cv{cv_index+1}'\n    print('############################################\\n', basic_name)\n    save_model_name = basic_name + '.model'\n    \n    x_train, y_train, x_valid, y_valid =  get_cv_data(cv_index+1)\n    \n    #Data augmentation\n    x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n    y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n\n    #model = build_complie_model(lr = 0.01)\n    model1 = UResNet34(input_shape = (1,img_size_target,img_size_target))\n\n    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\n    \n    reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',\n                                  factor=0.5, patience=3, min_lr=0.01, verbose=1)\n\n    #epochs = 50 #small number for demonstration \n    #batch_size = 32\n    history = model1.fit(x_train, y_train,\n                        validation_data=[x_valid, y_valid], \n                        epochs=epochs_1,\n                        batch_size=batch_size_1,\n                        callbacks=[ model_checkpoint,reduce_lr], \n                        verbose=1)\n        \n    model1.load_weights(save_model_name)\n    \n    preds_valid = predict_result(model1,x_valid,img_size_target)\n    ious[cv_index] = get_iou_vector(y_valid, (preds_valid > 0.5))\n    \n#model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c986938bb4ebd0056185874b5ad2300b56d45405"},"cell_type":"markdown","source":"# Training 2"},{"metadata":{"trusted":true,"_uuid":"d1a503461a7d8d1a0540162eecc6cc01a9c8b669"},"cell_type":"code","source":"model1 = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\n# remove layter activation layer and use losvasz loss\ninput_x = model1.layers[0].input\n\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.adam(lr = 0.1)\n\n# lovasz_loss need input range (-∞，+∞), so cancel the last \"sigmoid\" activation  \n# Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\nmodel.compile(loss = \"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric_2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"628f1d6dfebe4ca6d611769b58e70aee4ae8b1ca"},"cell_type":"code","source":"#early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=5, min_lr=0.01, verbose=1)\n#epochs = 50\n#batch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs = epochs_2,\n                    batch_size = batch_size_2,\n                    verbose = 1,\n                    callbacks = [model_checkpoint,reduce_lr]\n                               #,early_stopping], \n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbcdf47edafbada2c70ce706033235812dc2a7b4"},"cell_type":"markdown","source":"# training\nious = [0] * cv_total\nfor cv_index in range(cv_total):\n    basic_name = f'Unet_resnet_v{version}_cv{cv_index+1}'\n    print('############################################\\n', basic_name)\n    save_model_name = basic_name + '.model'\n    \n    x_train, y_train, x_valid, y_valid =  get_cv_data(cv_index+1)\n    \n    #Data augmentation\n    x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n    y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n\n    early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\n    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n                                       mode = 'max', save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n    #epochs = 50\n    #batch_size = 32\n\n    history = model.fit(x_train, y_train,\n                        validation_data=[x_valid, y_valid], \n                        epochs=epochs_2,\n                        batch_size=batch_size_2,\n                        callbacks=[model_checkpoint,reduce_lr,early_stopping], \n                        verbose=1)\n    plot_history(history,'my_iou_metric_2')\n    model.load_weights(save_model_name)\n    \n    preds_valid = predict_result(model,x_valid,img_size_target)\n    ious[cv_index] = get_iou_vector(y_valid, (preds_valid > 0.5))\n    \n#model1.summary()"},{"metadata":{"trusted":true,"_uuid":"9de0c7646478423ddede8f16c631ca9c842d040e"},"cell_type":"code","source":"for cv_index in range(cv_total):\n    print(f\"cv {cv_index} ious = {ious[cv_index]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad6a747a0e32945572cafe870b48d34ab05dc46"},"cell_type":"code","source":"\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452d89840bc86c630f13c3d63d0700ab4ad27b1b"},"cell_type":"code","source":"x_test = np.array([(np.array(load_img(\"../input/tgs-salt-identification-challenge/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f68651e1ce6ad9a461c8f1a25a1250fd489adfed"},"cell_type":"code","source":"# average the predictions from different folds\nt1 = time.time()\npreds_test = np.zeros(np.squeeze(x_test).shape)\nfor cv_index in range(cv_total):\n    basic_name = f'Unet_resnet_v{version}_cv{cv_index+1}'\n    model.load_weights(basic_name + '.model')\n    preds_test += predict_result(model,x_test,img_size_target) /cv_total\n    \nt2 = time.time()\nprint(f\"Usedtime = {t2-t1} s\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ccac81a492b9caaff4a25401165e145cf2c6f8e"},"cell_type":"code","source":"\nt1 = time.time()\nthreshold  = 0.5 # some value in range 0.4- 0.5 may be better \npred_dict = {idx: rle_encode(np.round(preds_test[i]) > threshold) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\nt2 = time.time()\n\nprint(f\"Usedtime = {t2-t1} s\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770d7d596656f4f1ad17a6063ad662ac80e11b24"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c89c406884ee54bee2c57aff51b116c157553ae9"},"cell_type":"code","source":"t_finish = time.time()\nprint(f\"Kernel run time = {(t_finish-t_start)/3600} hours\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a3009187c164635fbe163bd8ce406c2e309b1e5f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}