{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport sys\nimport json\nimport codecs\nimport numpy as np\n\nsys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\nBERT_PRETRAINED_DIR = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n\n#Keras_bert packages\nimport keras\nfrom keras_bert.keras_bert.bert import get_model\nfrom keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras_bert.keras_bert import Tokenizer\nfrom keras_bert.keras_bert import get_base_dict, get_model, gen_batch_inputs\nfrom keras.callbacks import EarlyStopping\n## To create and visualize a model\nfrom tqdm import tqdm, tqdm_pandas\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda, Add, Flatten\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Uploading pretrained BERT model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nmax_len = 72\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True, seq_len=max_len)\n\nmodel.summary(line_length=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenization**\n\nLoad dictionary from BERT_PRETRAINED_DIR for tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"bsz = 64 #batch-size\ndict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\ntoken_dict = {}\nwith codecs.open(dict_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\ntokenizer = Tokenizer(token_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Uploading input train file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv').sample(frac=0.4,random_state = 42)\ntrain_labels = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing the input-data : Cleaning and Pre-processing**\n\n"},{"metadata":{},"cell_type":"markdown","source":"Removing Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_Stopwords(pd_frame):\n    from nltk.stem import WordNetLemmatizer \n    lemmatizer = WordNetLemmatizer()\n    stop_words = {'not', 'won', 'but', 'this', 'most', \"isn't\", 'have', 'just', 'themselves', 'too', 'isn', 'hadn', 'about', 'from', 'both', \"don't\", 'hasn', \"mightn't\", 'your', 'his', 'than', 'so', 'now', 'until', 'they', 'ain', 'does', 'itself', 'or', 'off', \"aren't\", 'haven', 'i', 'you', 'he', 'why', 'it', 'under', 'd', 'mightn', 'up', 'each', 'down', 'y', 'o', 're', 'wouldn', \"should've\", 'no', 'which', 'aren', 'a', \"you'll\", \"mustn't\", 'doing', \"didn't\", 'same', 't', 'whom', \"shan't\", 'an', 'don', \"wasn't\", 'its', 'those', 'own', 'yours', 'myself', 'and', 'has', 'wasn', 'll', \"hasn't\", 'was', 'in', 'few', 'other', \"couldn't\", 'then', 'be', 'being', 'nor', \"needn't\", 'can', \"won't\", 'couldn', 'weren', 'been', 'for', \"shouldn't\", 'there', 'needn', 'yourself', 'how', 'her', 'herself', 'below', \"you're\", 'when', 'very', \"haven't\", 'into', 'didn', 'them', 'to', 'above', 'shan', 'some', 'are', 'on', 'is', 'their', 'at', 'am', 'hers', 'doesn', 'between', 'while', 's', 'should', 'theirs', 'himself', \"that'll\", 'ours', 'yourselves', 'what', 'again', 'had', 'ma', 'our', \"you'd\", 'my', 'out', \"she's\", 'she', 'if', \"weren't\", 'that', 'these', 'will', 'with', 'against', 'do', 'ourselves', 'all', 'who', 'as', 'here', \"wouldn't\", 've', 'through', 'the', 'after', \"hadn't\", 'of', 'having', 'once', 'only', 'because', 'where', \"it's\", 'by', 'shouldn', \"doesn't\", 'we', 'during', 'over', 'any', \"you've\", 'him', 'me', 'more', 'did', 'further', 'such', 'm', 'mustn', 'before', 'were'}\n    main_np = np.empty([len(pd_frame), 72])\n    for i in tqdm(range(len(pd_frame))):\n        new_str = ''\n        tot_length = 70\n        sent = pd_frame.iloc[i]\n        for j in sent.split():\n            if j not in stop_words:\n                new_str += ' ' + lemmatizer.lemmatize(j.lower())\n        tokens = tokenizer.encode(new_str, max_len = 72)[0]\n        for k in range(72):\n            main_np[i,k] = tokens[k]\n    return main_np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model fitting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\n\nadam = Adam(lr=2e-5,decay=0.01)\nmaxlen = 72\n\n#checkpointer = ModelCheckpoint(filepath='best_model.hdf5', verbose=5, save_best_only=True)\n\nes = EarlyStopping(monitor=auc, mode='min', verbose=1)\n#start_model = Sequential()\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\n\nprint('begin_build')\nfull_model = Model(inputs=model.input, outputs=pool_output)\nfull_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[auc])\nprint('begin training')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_input = remove_Stopwords(train['comment_text'])\nseg_input = np.zeros((token_input.shape[0],maxlen))\nmask_input = np.ones((token_input.shape[0],maxlen))\nfull_model.fit([token_input, seg_input, mask_input], train_labels, validation_split=0.3, epochs = 1, \n               callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"token_input2 = remove_Stopwords(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')['comment_text'])\nseg_input2 = np.zeros((token_input2.shape[0],maxlen))\nmask_input2 = np.ones((token_input2.shape[0],maxlen))\npred = full_model.predict([token_input2, seg_input2, mask_input2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.utils.plot_model(full_model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_info = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nnew_df = {'id':test_info['id'], 'prediction' : pred}\ndef extract_pred(pred):\n    ans = []\n    for i in pred:\n        ans.append(i[0])\n    return ans\ndf = pd.DataFrame({\"id\": test_info[\"id\"], \"prediction\": extract_pred(pred)})\ndf.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}