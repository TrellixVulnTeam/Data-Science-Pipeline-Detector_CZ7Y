{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport numpy as np \nimport pandas as pd \nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\nimport gc\nimport logging\nimport datetime\nimport warnings\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport keras.layers as L\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D, Bidirectional\nfrom keras.layers import LSTM, Lambda, concatenate, BatchNormalization, Embedding\nfrom keras.layers import TimeDistributed\n\nimport nltk\nEMBED_SIZE = 300\nMAX_LEN = 220\nMAX_FEATURES = 100000\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\nOUTPUT_PATH = '../input/output/'\nJIGSAW_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\nEMB_PATH = '../input/domain-embedding/ft_skip_300_D'\nJIGSAW_PATH_TRAIN = '../input/preprocessed-jigsaw/toxic_train_preprocessed.csv'\nJIGSAW_PATH_TEST='../input/preprocessed-jigsaw/toxic_test_preprocessed.csv'\n\ndef get_logger():\n    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger('main')\n    logger.setLevel(logging.DEBUG)\n    return logger\n    \nlogger = get_logger()\ndef load_data():\n    logger.info('Loading kaggle toxic data..')\n    train=pd.read_csv(JIGSAW_PATH_TRAIN)\n    test=pd.read_csv(JIGSAW_PATH_TEST)\n    logger.info('Training data shape:{}, Test data shape:{}'.format(train.shape,test.shape))\n    return train, test\n\ndef remove_stopwords(data):\n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n    data['comment_text'] = data['comment_text'].progress_apply(lambda x: str(x).lower())\n    data['comment_text'] = data['comment_text'].progress_apply(lambda x: \" \".join(x for x in str(x).split() \n        if x not in stop or not x.isdigit()))\n    return data\n\ndef run_tokenizer(train, test):\n    logger.info('Fitting tokenizer')\n    tokenizer = Tokenizer(num_words=MAX_FEATURES) \n    tokenizer.fit_on_texts(list(train['comment_text']))# + list(test['comment_text'])\n    word_index = tokenizer.word_index\n    X_train = tokenizer.texts_to_sequences(list(train['comment_text']))\n    y_train = train['target'].values\n    X_test = tokenizer.texts_to_sequences(list(test['comment_text']))\n    \n    X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n    X_test = pad_sequences(X_test, maxlen=MAX_LEN)\n    \n    del tokenizer\n    gc.collect()\n    return X_train, X_test, y_train, word_index\n\ndef build_embedding_matrix(word_index,EMBED_SIZE, embed_dir=EMB_PATH):\n    logger.info('Loading and preparing pre-trained word embedding ...')\n    embedding_model = KeyedVectors.load(embed_dir)\n    embedding_matrix = np.zeros((len(word_index) + 1,EMBED_SIZE))\n    uknown=[]\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_model[word]\n        except:\n            embedding_matrix[i] = np.random.random(EMBED_SIZE)\n            uknown.append(word)\n    del embedding_model\n    print('No. of known words:',len(uknown))\n    return embedding_matrix\n\ndef get_global_embedding(word_index,EMBED_SIZE):\n    import collections\n    uknown=[]\n    word2emb=collections.defaultdict(int)\n    fglove=open('../input/glove840b300dtxt/glove.840B.300d.txt',\"rb\")\n    for line in fglove:\n        cols=line.strip().split()\n        word=cols[0]\n        embedding=np.array(cols[1:],dtype='float32')\n        word2emb[word]=embedding\n    fglove.close()\n    vocab_size=len(word_index)+1\n    emb_matrix=np.zeros((vocab_size,EMBED_SIZE))\n    for w, i in word_index.items():\n        vect = word2emb.get(w)\n        if vect is not None:\n            emb_matrix[i] = vect\n        else:\n            emb_matrix[i]=np.random.random(EMBED_SIZE)\n            uknown.append(word)\n    \n    del word2emb\n    gc.collect()\n    print('No. of known words:',len(uknown))\n    return emb_matrix\ndef build_model(emb_matrix,word_index):\n\tinputs = Input(shape=(MAX_LEN,), dtype='int32')\n\tembedding_layer=Embedding(len(word_index)+1,EMBED_SIZE,\n\t\tweights=[emb_matrix],input_length=MAX_LEN,\n\t\ttrainable=True)\n\tembedded_sequences = embedding_layer(inputs)\n\tl_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n\tl_pool1 = MaxPooling1D(5)(l_cov1)\n\tl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n\tl_pool2 = MaxPooling1D(5)(l_cov2)\n\tl_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n\t#l_pool3 = MaxPooling1D(35)(l_cov3)  \n\tl_flat = GlobalMaxPool1D()(l_cov3)# global max pooling\n\tl_dense = Dense(128, activation='relu')(l_flat)\n\tpreds = Dense(1, activation='sigmoid')(l_dense)\n\tmodel = Model(inputs, preds, name=\"jigsaw\")\n\treturn model\n\ndef submit(preds):\n    print('Prepare submission')\n    submission = pd.read_csv(os.path.join(JIGSAW_PATH,'sample_submission.csv'), index_col='id')\n    submission['prediction'] = preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv', index=False)\n    print('saved to ')\n\n\ndef main():\n    train, test = load_data()\n    logger.info('Stop word removal ....')\n    train=remove_stopwords(train)\n    test=remove_stopwords(test)\n#    toxic_words=[line.rstrip('\\n') for line in open('Data/detected_toxic_words.txt')]\n#    import gensim\n#    model=gensim.models.Word2Vec.load('Models/embeddings/toxic_w2v_300D')\n#    train_comment_text=train.comment_text\n#    for comment in train_comment_text:\n#        words=nltk.word_tokenize(comment)\n#        for w in words:\n#            for toxic in toxic_words:\n#                print(w,'-',toxic,':',model.wv.similarity(w.lower(),toxic))\n#            break\n#        break\n    X_train, X_test, y_train, word_index = run_tokenizer(train, test)\n    embedding_matrix = build_embedding_matrix(word_index,300)\n    logger.info('########## Data Statistics ############')\n    logger.info('X-train shape:{}'.format(X_train.shape))\n    logger.info('Y-train shape:{}'.format(y_train.shape))\n    logger.info('X-test shape:{}'.format(X_test.shape))\n    logger.info('Vocabulary size:{}'.format(len(word_index)+1))\n    logger.info('Embedding shape:{}'.format(embedding_matrix.shape))\n    model=build_model(embedding_matrix, word_index)\n    model.compile(loss='binary_crossentropy', optimizer='adam',\n                           metrics=['accuracy'])\n    model.summary()\n    model.fit(X_train, y_train, validation_split=0.2, epochs=10,batch_size=64)\n    algorithm='simple_cnn'\n    model.save('../input/Output/MODEL_{}.h5'.format(algorithm))\n    pred=model.predict(X_test)[:,0]\n    submit(pred)\n    # logger.info('loading embeding matrix')\n    # embedding_matrix=np.load('fast_text_domain_trained.npy')\n#    preds = run_model(X_train, X_test, y_train, embedding_matrix, word_index)\n    # model = load_model('mod_0.hdf5')\n    # sub_preds = model.predict(X_test)[:, 0]\n#    submit(preds)\n    \nif __name__ == \"__main__\":\n    main()\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"../input/output\"))\nOUTPUT_PATH = '../input/output/'\nJIGSAW_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\ndef load_data():\n    print('Load train and test data')\n    train = pd.read_csv(os.path.join(OUTPUT_PATH,'domain_ft.csv'))\n#     test = pd.read_csv(os.path.join(JIGSAW_PATH,'test.csv'), index_col='id')\n    return train\ndef submit(preds):\n    print('Prepare submission')\n    submission = pd.read_csv(os.path.join(JIGSAW_PATH,'sample_submission.csv'), index_col='id')\n    submission['prediction'] = preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv', index=False)\n    print('saved to ')\nsub=load_data()\npred=sub['prediction'].values\nsubmit(pred)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}