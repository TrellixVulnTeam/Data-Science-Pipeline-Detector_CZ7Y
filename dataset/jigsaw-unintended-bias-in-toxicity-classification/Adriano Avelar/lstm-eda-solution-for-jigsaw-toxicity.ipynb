{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kaggle-datasets/179715/404258/toxicity.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1557355694&Signature=YiAIra%2B04%2FeJCV8RCLMlgp3jsMsorIG4xJx4Fc54FXVK9MjrV6ZqVzrktUXwfc0iK2BfPCvV12DBY0C7B9GycwuM7Vnt%2BgZP8LF8I2afZLJKgjPNVAY7iQBkXCdISstzhn%2FrrTS5EV0M2qrLr2V8SxSakUsNrnXE354VC5jQrVjCbjTQo0cfbGZYg9akO7LS9D5StlmlEgYVMP2VpyYzZ90oPB%2FbW3uvpgh0APnzEfHrjxxPDo3fY0Tc3hHvNZOVBqh%2B9CQkJOaL7wkrjzzxsI0nXNMnaxpJdMdMtBCI9tYDM6eG3Yqn07qoGNVk3UKVsMOqbE%2F36W0kcpSKmGomiA%3D%3D\" width=1900px height=400px/>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Jigsaw Unintended Bias in Toxicity Classification\n<h5>Detect toxicity across a diverse range of conversations</h5>\n<h3> Kernel description: </h3>\nIn this competition the intention is to detect toxic comments and minimize unintended model bias, for that we to optimize a metric designed to measure unintended bias\n"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents:\n\n**1. [Problem Definition](#id1)** <br>\n**2. [Data Background](#id2)** <br>\n**3. [Load the Dataset](#id3)** <br>\n**4. [Data Pre-processing](#id4)** <br>\n**5. [Model](#id5)** <br>\n**6. [Visualization and Analysis of Results](#id6)** <br>\n**7. [Submittion](#id7)** <br>\n**8. [References](#ref)** <br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id1\"></a> <br> \n# **1. Problem Definition:** \n\nWhen the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where, unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n\nIn this competition, our challenge is to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id2\"></a> <br> \n# **2. Data Background:** \nAt the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nIn the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (**target**), and models should predict the **target toxicity** for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment.\n\n** Very Importante! **\n<p style=\"color:red;\">For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).</p>\n\nThe data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n\n* severe_toxicity\n* obscene\n* threat\n* insult\n* identity_attack\n* sexual_explicit"},{"metadata":{},"cell_type":"markdown","source":"Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. **Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation**.\n\n*Identities with MORE than 500 examples*\n\n* **male**\n* **female**\n* **psychiatric_or_mental_illness**\n* **homosexual_gay_or_lesbian**\n* **christian**\n* **jewish**\n* **muslim**\n* **black**\n* **white**\n\n*Identities with LESS than 500 examples*\n* transgender\n* other_gender\n* heterosexual\n* bisexual\n* other_sexual_orientation\n* hindu\n* buddhist\n* atheist\n* other_religion\n* asian\n* latino\n* other_race_or_ethnicity\n* physical_disability\n* intellectual_or_learning_disability\n* other_disability"},{"metadata":{},"cell_type":"markdown","source":"<h3>*Examples*</h3>\nHere are a few examples of comments and their associated toxicity and identity labels. Label values range from 0.0 - 1.0 represented the fraction of raters who believed the label fit the comment.\n\n**Comment:** i'm a white woman in my late 60's and believe me, they are not too crazy about me either!!<br>\n**Toxicity Labels:** All 0.0<br>\n**Identity Mention Labels:** female: 1.0, white: 1.0 (all others 0.0)<br>\n\n**Comment:** Why would you assume that the nurses in this story were women?<br>\n**Toxicity Labels:** All 0.0<br>\n**Identity Mention Labels:** female: 0.8 (all others 0.0)<br>\n\n**Comment:** Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.<br>\n**Toxicity Labels:** All 0.0<br>\n**Identity Mention Labels:** homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id3\"></a> <br> \n# **3. Load the Dataset** \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing import text, sequence\nfrom keras import backend as K\nfrom keras.models import load_model\nimport keras\nimport pickle\n\nimport os\nfrom tqdm import tqdm\ntqdm.pandas()\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>3.1 Heads of the data</h3>\nThe first thing we have to  do is loading the data and take a quick look at the number of rows and collumns in dataset. This is the first contact with the data. Do this after carefully reading the background on the data and the meaning of the attibutes provided by the problem.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_num_comments = train.shape[0]\nunique_comments = train['comment_text'].nunique()\n\n\nprint('Train set: %d (Entries) and %d (Attributes).' % (train.shape[0], train.shape[1]))\nprint('Test set: %d (Entries) and %d (Attributes).' % (test.shape[0], test.shape[1]))\n\nprint('Number of Unique Comments {}'.format(unique_comments))\nprint('Percentage of Unique Comments %.2f%%' %( (unique_comments/total_num_comments)*100 ))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>3.2 Data Visualization</h3>\nCheck Distribution of comment Lenght and Word Numbers.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_length'] = train['comment_text'].apply(lambda x : len(x))\nplt.figure(figsize=(16,4))\nsns.distplot(train['comment_length'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most comments are short. However, there is an increase when comments arrive close to 1000, perhaps because that is the maximum number of characters allowed in the tool that the data was collected.\n"},{"metadata":{},"cell_type":"markdown","source":"A similar behavior happens with the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest['comment_length'] = test['comment_text'].apply(lambda x : len(x))\nplt.figure(figsize=(16,4))\nsns.distplot(test['comment_length'])\nplt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the number of words in the attribute **comment_text**."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train['word_count'] = train['comment_text'].apply(lambda x : len(x.split(' ')))''\ntest['word_count'] = test['comment_text'].apply(lambda x : len(x.split(' ')))\nbin_size = max(train['word_count'].max(), test['word_count'].max())//10\nplt.figure(figsize=(20, 6))\nsns.distplot(train['word_count'], bins=bin_size)\nsns.distplot(test['word_count'], bins=bin_size)\nplt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['toxic_class'] = train['target'] >= 0.5\nplt.figure(figsize=(16,4))\nsns.countplot(train['toxic_class'])\nplt.title('Toxic vs Non Toxic Comments')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the figure \"Toxic vs Non Toxic Comments\" we can see that the vast majority of comments are non-toxic. Let's check if there is any relationship between the toxic comments and the size of the comments."},{"metadata":{},"cell_type":"markdown","source":"Another interesting step is to know how toxic comments are distilled over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\ntarget_df = train.sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count'], 'target':['mean']})\ntarget_df.columns = ['Date', 'Count', 'Toxicity Rate']'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''plt.figure(figsize=(16,4))\nsns.lineplot(x=target_df['Date'], y=target_df['Toxicity Rate'])\nplt.title('Toxicity over time')\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a period in late 2015, the toxicity rate was low for some reason. Maybe the end of the year will make people less toxic.But, the same behavior doesn't repeat at the end of 2016."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''plt.figure(figsize=(16,4))\nsns.lineplot(x=target_df['Date'], y=target_df['Count'])\nplt.title('Count of toxicity comments over time')\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id4\"></a> <br> \n# **4. Data Pre-processing** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take care of dataframe memory\nmem_usg = train.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[[\"target\", \"comment_text\"]]\nmem_usg = train.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train[\"comment_text\"]\nlabel_data = train[\"target\"]\ntest_data = test[\"comment_text\"]\ntrain_data.shape, label_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_data) + list(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = tokenizer.texts_to_sequences(train_data)\ntest_data = tokenizer.texts_to_sequences(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 200\ntrain_data = sequence.pad_sequences(train_data, maxlen=MAX_LEN)\ntest_data = sequence.pad_sequences(test_data, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = max_features or len(tokenizer.word_index) + 1\nmax_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_data), type(label_data.values), type(test_data)\nlabel_data = label_data.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id5\"></a> <br> \n# **5. Model** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras Model\n# Model Parameters\nNUM_HIDDEN = 512\nEMB_SIZE = 256\nLABEL_SIZE = 1\nMAX_FEATURES = max_features\nDROP_OUT_RATE = 0.25\nDENSE_ACTIVATION = \"sigmoid\"\nNUM_EPOCHS = 1\n\n# Optimization Parameters\nBATCH_SIZE = 512\nLOSS_FUNC = \"binary_crossentropy\"\nOPTIMIZER_FUNC = \"adam\"\nMETRICS = [\"accuracy\"]\n\nclass LSTMModel:\n    \n    def __init__(self):\n        self.model = self.build_graph()\n        self.compile_model()\n    \n    def build_graph(self):\n        model = keras.models.Sequential([\n            keras.layers.Embedding(MAX_FEATURES, EMB_SIZE),\n            keras.layers.CuDNNLSTM(NUM_HIDDEN),\n            keras.layers.Dropout(rate=DROP_OUT_RATE),\n            keras.layers.Dense(LABEL_SIZE, activation=DENSE_ACTIVATION)])\n        return model\n    \n    def compile_model(self):\n        self.model.compile(\n            loss=LOSS_FUNC,\n            optimizer=OPTIMIZER_FUNC,\n            metrics=METRICS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gc\n\nX_train = train_data\ny_train = label_data\nX_test = test_data\n\nKFold_N = 5\nfrom sklearn.model_selection import KFold\nsplits = list( KFold(n_splits=KFold_N).split(X_train,y_train) )\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\nimport numpy as np\n\n\noof_preds = np.zeros((X_train.shape[0]))\ntest_preds = np.zeros((X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in range(KFold_N):\n    K.clear_session()\n    tr_ind, val_ind = splits[fold]\n    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    model = LSTMModel().model#build_model()\n    model.fit(X_train[tr_ind],\n        y_train[tr_ind]>0.5,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n        callbacks = [es,ckpt])\n\n    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n    test_preds += model.predict(X_test)[:,0]\n    \ntest_preds /= KFold_N    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>=0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id7\"></a> <br> \n# **7. Submittion** "},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ref\"></a> <br> \n# **8. References** "},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">Please, votes up if you like this Kernel.</h1>\nI get lot of plots from https://www.kaggle.com/dimitreoliveira/toxicity-bias-extensive-eda-and-bi-lstm. Up Vote his awesome work."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}