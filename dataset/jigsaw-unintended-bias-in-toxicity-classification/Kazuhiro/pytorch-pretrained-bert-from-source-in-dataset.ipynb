{"cells":[{"metadata":{},"cell_type":"markdown","source":"- import pytorch-pretrained-bert from source in dataset\n\nthe source is cloned from [huggingface/pytorch\\-pretrained\\-BERT: ðŸ“–The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google/CMU Transformer\\-XL\\.](https://github.com/huggingface/pytorch-pretrained-BERT)\n\n- convert tf checkpoints to pytorch model\n\n\n- feature extraction with BERT like [huggingface/pytorch\\-pretrained\\-BERT: ðŸ“–The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google/CMU Transformer\\-XL\\.](https://github.com/huggingface/pytorch-pretrained-BERT)\n\nThis kernel is based off of [Import functions from Kaggle script](https://www.kaggle.com/rtatman/import-functions-from-kaggle-script)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = \"../working/\"\nos.listdir(WORK_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module we'll need to import our custom module\nimport shutil\n\n# copy our file into the working directory\nshutil.copytree(\"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT/pytorch_pretrained_bert\", os.path.join(WORK_DIR, \"pytorch_pretrained_bert\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/bert-pretrained-models/cased_l-12_h-768_a-12/cased_L-12_H-768_A-12')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None)\n\n# Tokenized input\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n\n# Mask a token that we will try to predict back with `BertForMaskedLM`\nmasked_index = 8\ntokenized_text[masked_index] = '[MASK]'\nprint(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert token to vocabulary indices\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n\n# Convert inputs to PyTorch tensors\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(BERT_MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model (weights)\nmodel = BertModel.from_pretrained(\n    WORK_DIR)\nmodel.eval()\n\n# If you have a GPU, put everything on cuda\ntokens_tensor = tokens_tensor.to('cuda')\nsegments_tensors = segments_tensors.to('cuda')\nmodel.to('cuda')\n\n# Predict hidden states features for each layer\nwith torch.no_grad():\n    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n# We have a hidden states for each of the 12 layers in model bert-base-uncased\nassert len(encoded_layers) == 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[e.size() for e in encoded_layers]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}