{"cells":[{"metadata":{},"cell_type":"markdown","source":"- lstm-gru with predicted ids by lstm-gru\n- gpt2 max\n- base uncased f1, 2\n- base CASED f4, 5\n- large uncased 5 f0\n- large CASED 2 f1\n\nめっちゃ速くなってる？もっかい投げる\n\nなんか速くなってるからもう一個アンサンブルする"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!cd ../input/apex-master/apex-master/apex-master/ && pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nimport sys\n# copy our file into the working directory\nsys.path.insert(0, \"../input/pytorch-pretrained-bert-2/pytorch-pretrained-bert-master/pytorch-pretrained-BERT-master/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM-GRU bs-256 crawl ROOV TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = False\n\nbatch_size = 256\n\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nn_seeds = 1\nn_splits = 10\nn_epochs = 15\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nENSEMBLE_START_EPOCH = 3\n\nMAX_LEN = 300\n\nEMB_DROPOUT = 0.3\nMIDDLE_DROPOUT = 0.3\n\nif DEBUG:\n    DEBUG_DATA_SIZE = 1000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = \"../input/lstmgru-bs256-crawl-no-appos-predicted-ids-r/results\"\nos.listdir(MODEL_PATH)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import sequence\nfrom keras import preprocessing\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nimport joblib\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nimport sys, os\n\n@contextmanager\ndef suppress_stdout():\n    with open(os.devnull, \"w\") as devnull:\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:  \n            yield\n        finally:\n            sys.stdout = old_stdout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# これだと、'はembeddingに結構入ってるのに除外されちゃう。　よくないので ' だけ抜いた\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test = test['comment_text'].apply(lambda x: clean_text(x))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith open('../input/reducing-oov-of-crawl300d2m-no-appos-result/jigsaw-crawl-300d-2M.joblib', 'rb') as f:\n    crawl_emb_dict = joblib.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce OOV"},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nfrom typing import Dict, List\ndef build_vocab(texts: pd.DataFrame) -> Dict[str, int]:\n    \"\"\"\n    \n    Parameters\n    -----\n    texts: pandas.Series\n        question textの列\n        \n    Returns\n    -----\n    dict: \n        単語とカウント\n    \n    \"\"\"\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef check_coverage(vocab: Dict[str, int], embeddings_index: Dict) -> List[str]:\n    \"\"\"\n    Parameters\n    -----\n    vocab: dict\n        単語とカウント\n    embeddings_index: dict\n        load_embedの出力\n        \n    Returns:\n        list:\n            embeddingsに入ってない単語\n    \"\"\"\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in tqdm(vocab.keys()):\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / float(len(vocab))))\n    print('Found embeddings for  {:.2%} of all text'.format(float(nb_known_words) / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words\n\nfrom nltk.stem import PorterStemmer\np_stemmer = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nl_stemmer = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\ns_stemmer = SnowballStemmer(\"english\")\n\nimport copy\ndef edits1(word):\n    \"\"\"\n    wordの編集距離1の単語のリストを返す\n    \"\"\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef known(words, embed): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in embed)\n\ndef spellcheck(word, word_rank_dict):\n    return min(known(edits1(word), word_rank_dict), key=lambda w: word_rank_dict[w])\n\n\nimport unicodedata\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\",\n                 \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\",\n                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\",\n                 \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"',\n                 '“': '\"', \"£\": \"e\", '∞': 'infinity',\n                 'θ': 'theta', '÷': '/', 'α': 'alpha',\n                 '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n                 '∅': '', '³': '3', 'π': 'pi', }\ndef process_stemmer(vocab, embed):\n    oov_word_set = set()\n    for word in tqdm(vocab.keys()):\n        vector = embed.get(word, None)\n        if vector is not None:\n            continue\n\n        vector = embed.get(word.lower(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(word.upper(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(word.capitalize(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        corr_word = punct_mapping.get(word, None)\n        if corr_word is not None:\n            vector = embed.get(corr_word, None)\n            if vector is not None:\n                embed[word] = vector\n                continue\n        \n        try:\n            vector = embed.get(p_stemmer.stem(word), None)\n        except:\n            vector = embed.get(p_stemmer.stem(word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        try:\n            vector = embed.get(l_stemmer.stem(word), None)\n        except:\n            vector = embed.get(l_stemmer.stem(word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        try:\n            vector = embed.get(s_stemmer.stem(word), None)\n        except:\n            vector = embed.get(s_stemmer.stem(word.decode('utf-8')), None)     \n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        oov_word_set.add(word)\n            \n    return embed, oov_word_set\n\ndef process_small_capital(vocab, embed, oov_set):\n    oov_word_set = set()\n    for word in tqdm(vocab.keys()):\n        if word not in oov_set:\n            continue\n        \n        char_list = []\n        any_small_capitial = False\n        for char in word:\n            try:\n                uni_name = unicodedata.name(char)\n            except ValueError:\n                continue\n\n            if 'LATIN LETTER SMALL CAPITAL' in uni_name:\n                char = uni_name[-1]\n                any_small_capitial = True\n            if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n                char = 'F'\n                any_small_capitial = True\n\n            char_list.append(char)\n\n        if not any_small_capitial:\n            oov_word_set.add(word)\n            continue\n            \n        legit_word = ''.join(char_list)\n        \n        vector = embed.get(legit_word, None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.lower(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.upper(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.capitalize(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        corr_word = punct_mapping.get(legit_word, None)\n        if corr_word is not None:\n            vector = embed.get(corr_word, None)\n            if vector is not None:\n                embed[word] = vector\n                continue\n        \n        try:\n            vector = embed.get(p_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(p_stemmer.stem(legit_word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        try:\n            vector = embed.get(l_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(l_stemmer.stem(legit_word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        try:\n            vector = embed.get(s_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(s_stemmer.stem(legit_word.decode('utf-8')), None)\n                    \n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        oov_word_set.add(word)\n        \n    return embed, oov_word_set\n\ndef process_spellcheck(vocab, embed, word_rank_dict, oov_set):\n    oov_word_set = set()\n    \n    for word in tqdm(vocab.keys()):\n        if word not in oov_set:\n            continue\n            \n        try:\n            vector = embed.get(spellcheck(word, word_rank_dict), None)\n        except:\n            continue\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        oov_word_set.add(word)\n            \n    return embed, oov_word_set\n\ndef make_word_rank(embed):\n    word_rank = {}\n    for i, word in enumerate(embed):\n        word_rank[word] = i\n    return word_rank\n\ndef head(enumerable, n=10):\n    for i, item in enumerate(enumerable):\n        print(str(i) + '\\n', item)\n        if i > n:\n            return\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nvocab = build_vocab(x_test)\noov = check_coverage(vocab, crawl_emb_dict)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nemb, oov_stemmer = process_stemmer(vocab, crawl_emb_dict)\noov = check_coverage(vocab, emb)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nemb, oov_small_capital = process_small_capital(vocab, emb, oov_stemmer)\noov = check_coverage(vocab, emb)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nword_rank = make_word_rank(emb)\nemb, oov_spell = process_spellcheck(vocab, emb, word_rank, oov_small_capital)\noov = check_coverage(vocab, emb)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence features"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/googleprofanitywords/list.txt', 'r') as f:\n    p_words = f.readlines()\n    \nset_puncts = set(puncts)\n\np_word_set = set([t.replace('\\n', '') for t in p_words])\n\ndef sentence_fetures(text):\n    word_list = text.split()\n    word_count = len(word_list)\n    n_upper = len([word for word in word_list if any([c.isupper() for c in word])])\n    n_unique = len(set(word_list))\n    n_ex = word_list.count('!')\n    n_que = word_list.count('?')\n    n_puncts = len([word for word in word_list if word in set_puncts])\n    n_prof = len([word for word in word_list if word in p_word_set])\n    n_oov = len([word for word in word_list if word not in crawl_emb_dict])\n    \n    return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov\n\nsentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']\nfrom collections import defaultdict\nfeature_dict = defaultdict(list)\nfor text in progress_bar(x_test):\n    feature_list = sentence_fetures(text)\n    for i_feature, feature_name in enumerate(sentence_feature_cols):\n        feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])\n        \nsentence_df = pd.DataFrame.from_dict(feature_dict)\nfor col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:\n    sentence_df[col + '_ratio'] = sentence_df[col] / sentence_df['word_count']\nsentence_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\ntmp = sentence_df.hist(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_feature_mat = sentence_df.drop(columns=['n_oov', 'n_oov_ratio']).values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nx_test = x_test.apply(lambda x: clean_numbers(x))\n\ntokenizer = preprocessing.text.Tokenizer(filters=\"\", lower=False)\ntokenizer.fit_on_texts(list(x_test))\n\nx_test = tokenizer.texts_to_sequences(x_test)\n\nmax_features = len(tokenizer.word_index) + 1\nmax_features\n\ndef build_matrix(word_index, embedding_dict):\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_dict[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\nemb_mat_crawl, oov = build_matrix(tokenizer.word_index, emb)\ndel crawl_emb_dict, oov, emb\ngc.collect()\n\nemb_mat_torch = torch.tensor(emb_mat_crawl, dtype=torch.float32).cuda()\ndel emb_mat_crawl\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nclass DynamicBucketIterator(object):\n    def __init__(self, data, label, capacity, pad_token, shuffle, length_quantile, max_batch_size, for_bert):\n        self.data = data\n        self.label = label\n        self.pad_token = pad_token\n        self.capacity = capacity\n        self.shuffle = shuffle\n        self.length_quantile = length_quantile\n        self.for_bert = for_bert\n        \n        self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))\n        \n        old_separator_index = 0\n        self.separator_index_list = [0]\n        for i_sample in range(len(self.data)):\n            sample_index = self.index_sorted[i_sample]\n            sample = self.data[sample_index]\n            current_batch_size = i_sample - old_separator_index + 1\n            if min(len(sample), MAX_LEN) * current_batch_size <= self.capacity and current_batch_size <= max_batch_size:\n                pass\n            else:\n                old_separator_index = i_sample\n                self.separator_index_list.append(i_sample)\n                \n        self.separator_index_list.append(len(self.data)) # [0, ..., start_separator_index, end_separator_index, ..., len(data)]\n        \n        if not self.shuffle:\n            self.bucket_index = range(self.__len__())\n        \n        self.reset_index()\n\n    def reset_index(self):\n        self.i_batch = 0\n        \n        if self.shuffle:\n            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))\n            self.bucket_index = np.random.permutation(self.__len__())\n    \n    def __len__(self):\n        return len(self.separator_index_list) - 1\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        try:\n            i_bucket = self.bucket_index[self.i_batch]\n        except IndexError as e:\n            self.reset_index()\n            raise StopIteration\n            \n        start_index, end_index = self.separator_index_list[i_bucket : i_bucket + 2]\n        \n        index_batch = self.index_sorted[start_index : end_index]\n\n        raw_batch_data = [self.data[i] for i in index_batch]\n        \n        batch_label = self.label[index_batch]\n        \n        math.ceil(1)\n        \n        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))\n        max_len = min([max_len, MAX_LEN])\n        if max_len == 0:\n            max_len = 1\n        \n        if self.for_bert:\n            segment_id_batch = np.zeros((len(raw_batch_data), max_len))\n            padded_batch = []\n            input_mask_batch = []\n            for sample in raw_batch_data:\n                input_mask = [1] * len(sample) + [0] * (max_len - len(sample))\n                input_mask_batch.append(input_mask[:max_len])\n\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch\n        \n        else:\n            padded_batch = []\n            for sample in raw_batch_data:\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, batch_label, index_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass GRULayer(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(GRULayer, self).__init__()\n        \n        self.gru = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          bias=True,\n                          bidirectional=True,\n                          batch_first=True)\n        \n        self.init_weights()\n        \n    def init_weights(self):\n        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n        for k in ih:\n            nn.init.xavier_uniform_(k)\n        for k in hh:\n            nn.init.orthogonal_(k)\n        for k in b:\n            nn.init.constant_(k, 0)\n\n    def forward(self, x):\n        gru_outputs, gru_state = self.gru(x)\n        return gru_outputs, gru_state\n    \n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embed_size, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.embedding_dropout = SpatialDropout(EMB_DROPOUT)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.middle_dropout = SpatialDropout(MIDDLE_DROPOUT)\n        \n        self.lstm2 = GRULayer(LSTM_UNITS * 2, LSTM_UNITS)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = LSTM_UNITS * 2 + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden) # bi-max, bi-ave, sentence_features\n            \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x, sentence_features):\n        h_embedding = self.embedding_dropout(x)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        \n        h_lstm1 = self.middle_dropout(h_lstm1)\n        \n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n            \n        h_sentence = self.linear_sentence1(sentence_features)\n        h_cat = torch.cat((max_pool, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nOOF_TRAIN_COL = 'oof_train'\nSUBGROUP_AUC_COL = 'subgroup_auc'\nBPSN_AUC_COL = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC_COL = 'bnsp_auc'  # stands for background negative, subgroup positive\nfrom sklearn import metrics\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n'model'\ndef compute_subgroup_auc(df, subgroup_col, label_col, oof_col):\n    subgroup_examples = df[df[subgroup_col]]\n    return compute_auc(subgroup_examples[label_col], subgroup_examples[oof_col])\n\ndef compute_bpsn_auc(df, subgroup_col, label_col, oof_col):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup_col] & ~df[label_col]]\n    non_subgroup_positive_examples = df[~df[subgroup_col] & df[label_col]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label_col], examples[oof_col])\n\ndef compute_bnsp_auc(df, subgroup_col, label_col, oof_col):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup_col] & df[label_col]]\n    non_subgroup_negative_examples = df[~df[subgroup_col] & ~df[label_col]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label_col], examples[oof_col])\n\ndef compute_bias_metrics_for_model(df,\n                                   subgroup_list,\n                                   oof_col,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    record_list = []\n    for subgroup in subgroup_list:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(df[df[subgroup]])\n        }\n        record[SUBGROUP_AUC_COL] = compute_subgroup_auc(df, subgroup, label_col, oof_col)\n        record[BPSN_AUC_COL] = compute_bpsn_auc(df, subgroup, label_col, oof_col)\n        record[BNSP_AUC_COL] = compute_bnsp_auc(df, subgroup, label_col, oof_col)\n        record_list.append(record)\n    return pd.DataFrame(record_list).sort_values('subgroup_auc', ascending=True)\n\nTOXICITY_COLUMN = 'target'\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC_COL], POWER),\n        power_mean(bias_df[BPSN_AUC_COL], POWER),\n        power_mean(bias_df[BNSP_AUC_COL], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\ndef get_various_auc(valid_df, y_pred):\n    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n    valid_df.loc[:, OOF_TRAIN_COL] = y_pred\n    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n    bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, OOF_TRAIN_COL, TOXICITY_COLUMN)\n    overall_auc = calculate_overall_auc(valid_df, OOF_TRAIN_COL)\n    return get_final_metric(bias_metrics_df, overall_auc), overall_auc, bias_metrics_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\nauc_array = np.zeros((n_seeds, n_splits, n_epochs))\n\ntest_array = np.zeros((n_seeds, n_splits, n_epochs, len(x_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # 使ってるのは i_foldだけ\n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{i_seed}-fold{i_fold}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False,\n                                            length_quantile=1, max_batch_size=10000000, for_bert=False)\n        \n        print(torch.cuda.memory_allocated())\n        for i_epoch in range(ENSEMBLE_START_EPOCH, n_epochs):\n            start_time = time.time()\n            model = NeuralNet(EMBED_SIZE, len(aux_col_list), sentence_feature_mat.shape[-1])\n            load_path = os.path.join(MODEL_PATH, f'seed{i_seed}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n            model.load_state_dict(torch.load(load_path))\n            model.cuda()\n            \n            model.eval()\n            \n            epoch_test_pred = np.zeros(len(x_test))\n            mem_list = []\n            for batch in test_loader:\n                x_batch = batch[0]\n                y_batch = batch[1]\n                index_batch = batch[2]\n                \n                y_true_batch = y_batch[:, :1+len(aux_col_list)]\n                sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n                sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n                \n                if len(x_batch) >= 32:\n                    y_pred = model(emb_mat_torch[x_batch], sentence_feature_batch)\n                else:\n                    # イミフだけど、len(x_batch)が32未満だとバグる\n                    len_last = len(x_batch)\n                    x_batch.extend([[0 for _ in range(len(x_batch[0]))] for _ in range(32)])\n                    y_pred = model(emb_mat_torch[x_batch[:32]][:len_last], sentence_feature_batch)\n                \n                mem_list.append(torch.cuda.memory_allocated())\n                epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n                \n                del y_pred\n                torch.cuda.empty_cache()\n                            \n            test_array[i_seed, i_fold, i_epoch] = epoch_test_pred\n            \n            elapsed_time = time.time() - start_time\n            \n            del model\n            gc.collect()\n            torch.cuda.empty_cache()\n            print(f'epoch {i_epoch}: {elapsed_time: 0.3f}, {max(mem_list)/10**9}')\n            \n\n        del epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_rnn = np.mean(test_array[:, :, ENSEMBLE_START_EPOCH:], axis=(0, 1, 2))\nplt.hist(test_pred_rnn, bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_loader, emb_mat_torch, test_array\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GPT-2 max"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_feature_mat = sentence_df[SENTENCE_FEAURE_USED].values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2Model\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = GPT2Tokenizer(vocab_file='../input/gpt2-models/vocab.json', merges_file='../input/gpt2-models/merges.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_test = x_test.progress_apply(lambda x: tokenizer.encode(x[:MAX_LEN]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.gpt2 = GPT2Model.from_pretrained('../input/gpt2-models/')\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        h_gpt2, _ = self.gpt2(x_features) # (bsz, seq_len, hidden_size)\n        \n        h_gpt2, _ = torch.max(h_gpt2, 1) # (bsz, hidden_size)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((h_gpt2, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = '../input/gpt2-f1e1-max-results'\nos.listdir(MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [1]\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    \n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=False)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in progress_bar(test_loader):\n        x_batch = torch.tensor(batch[0], dtype=torch.long).cuda()\n        y_batch = batch[1]\n        index_batch = batch[2]\n        \n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n        y_pred = model(x_batch, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_batch, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_gpt2_max = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_gpt2_max, bins=20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# base uncased f1, 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nMAX_LEN = 220\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import sequence\nfrom keras import preprocessing\nfrom tqdm import tqdm\ntqdm.pandas()\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nimport joblib\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_feature_mat = sentence_df[SENTENCE_FEAURE_USED].values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = \"../working/\"\nos.listdir(WORK_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_DO_LOWER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text, max_len, tokenizer):\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# cleanしたのは x_test_cleanedに入ってて、x_test はそのまま\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\nx_test = x_test.progress_apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(len(t) for t in x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from glob import glob\ndef get_model_path(i_seed, i_fold, i_epoch, base_path):\n    for dir_name in glob(base_path + '*'):\n        if (i_seed == int(re.match('.*s\\d', dir_name).group()[-1]) and\n            i_fold == int(re.match('.*f\\d', dir_name).group()[-1]) and\n            i_epoch == int(re.match('.*e\\d', dir_name).group()[-1])\n           ):\n            return dir_name\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_path(0, 0, 1, '../input/base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [0]\n\ni_epoch = 2\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = '../input/bert-base-uncased-ftpe18-e2-5e6'\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_base_uncased_ftp13 = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_base_uncased_ftp13, bins=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [1, 2]\n\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = get_model_path(i_seed, i_fold, i_epoch, '../input/base-uncased')\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test_pred_base_uncased = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_base_uncased, bins=20)\n\ndel test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# base CASED f4, 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_list = [4, 5]\n\ni_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/cased_l-12_h-768_a-12/cased_L-12_H-768_A-12/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_DO_LOWER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text, max_len, tokenizer):\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]\nx_test = x_test.progress_apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(len(t) for t in x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_path(i_seed, 4, 1, '../input/base-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_dict = {}\n\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = get_model_path(i_seed, i_fold, i_epoch, '../input/base-cased')\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n    #                 timer.stamp(f'x_features')\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n    #                 timer.stamp(f'after_prediction')\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_base_CASED = np.mean(list(test_dict.values()), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Large uncased f0e1"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8\nn_seeds = 1\nn_splits = 10\nn_epochs = 1\n\nTRAIN_ON_N_SPLITS = 1\n\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nBERT_HIDDEN_SIZE = 1024\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nENSEMBLE_START_EPOCH = 0\n\nOUT_DROPOUT = 0.3\n\nMAX_LEN = 220\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = \"../input/bert-large-5\"\nos.listdir(MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport re\n# これだと、'はembeddingに結構入ってるのに除外されちゃう。　よくないので ' だけ抜いた\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test_clean = test['comment_text'].apply(lambda x: clean_text(x)).apply(lambda x: clean_numbers(x))\ndel test\n\ngc.collect()\n\nx_test_clean.head(20)\n\ndef add_cls_sep(text):\n    return \"[CLS] \" + text + \" [SEP]\"\n\nx_test = x_test_clean.apply(lambda x: add_cls_sep(x))\n\nx_test.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_DO_LOWER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)\n\ntqdm.pandas()\nx_test = x_test.progress_apply(lambda x: tokenizer.tokenize(x))\n\nx_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x)[:MAX_LEN])\n\nx_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_feature_mat = sentence_df.drop(columns=['n_oov']).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_array = np.zeros(len(x_test))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # 使ってるのは i_foldだけ\n        if i_fold >= TRAIN_ON_N_SPLITS:\n            break\n            \n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{0}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n        \n        print(torch.cuda.memory_allocated())\n        mb = master_bar(range(n_epochs))\n\n        i_epoch = 1\n        \n        model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n        load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{0}-epoch{i_epoch*4+3}.torchModelState')\n        model.load_state_dict(torch.load(load_path))\n        model.cuda()\n        print(i_epoch)\n        start_time = time.time()\n\n        model.eval()\n\n        epoch_test_pred = np.zeros(len(x_test))\n        for batch in progress_bar(test_loader):\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            index_batch = batch[4]\n            y_true_batch = y_batch[:, :1+len(aux_col_list)]\n            sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n            sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n#                 print('x_features', torch.cuda.memory_allocated())\n#                 timer.stamp(f'x_features')\n            y_pred = model(x_features, sentence_feature_batch)\n#                 print('after_prediction', torch.cuda.memory_allocated())\n#                 timer.stamp(f'after_prediction')\n\n            epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            del x_features, y_pred\n            torch.cuda.empty_cache()\n\n        test_array = epoch_test_pred\n        elapsed_time = time.time() - start_time\n        del model, epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_large_uncased = test_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_pred_large_uncased, bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_loader\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT Large CASED f0"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8\nn_seeds = 1\nn_splits = 10\nn_epochs = 1\n\nTRAIN_ON_N_SPLITS = 1\n\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nBERT_HIDDEN_SIZE = 1024\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/cased_l-24_h-1024_a-16/cased_L-24_H-1024_A-16/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nENSEMBLE_START_EPOCH = 0\n\nOUT_DROPOUT = 0.3\n\nMAX_LEN = 220\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = \"../input/large-cased-1\"\nos.listdir(MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport re\n# これだと、'はembeddingに結構入ってるのに除外されちゃう。　よくないので ' だけ抜いた\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_test_clean = test['comment_text'].apply(lambda x: clean_text(x)).apply(lambda x: clean_numbers(x))\ndel test\n\ngc.collect()\n\nx_test_clean.head(20)\n\ndef add_cls_sep(text):\n    return \"[CLS] \" + text + \" [SEP]\"\n\nx_test = x_test_clean.apply(lambda x: add_cls_sep(x))\n\nx_test.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_DO_LOWER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)\n\ntqdm.pandas()\nx_test = x_test.progress_apply(lambda x: tokenizer.tokenize(x))\n\nx_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x)[:MAX_LEN])\n\nx_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_feature_mat = sentence_df.drop(columns=['n_oov']).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_array = np.zeros(len(x_test))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # 使ってるのは i_foldだけ\n        if i_fold >= TRAIN_ON_N_SPLITS:\n            break\n            \n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{0}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n        \n        print(torch.cuda.memory_allocated())\n        mb = master_bar(range(n_epochs))\n\n        i_epoch = 1\n        \n        model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n        load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{0}-epoch{i_epoch*4+3}.torchModelState')\n        model.load_state_dict(torch.load(load_path))\n        model.cuda()\n        print(i_epoch)\n        start_time = time.time()\n\n        model.eval()\n\n        epoch_test_pred = np.zeros(len(x_test))\n        for batch in progress_bar(test_loader):\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            index_batch = batch[4]\n            y_true_batch = y_batch[:, :1+len(aux_col_list)]\n            sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n            sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n#                 print('x_features', torch.cuda.memory_allocated())\n#                 timer.stamp(f'x_features')\n            y_pred = model(x_features, sentence_feature_batch)\n#                 print('after_prediction', torch.cuda.memory_allocated())\n#                 timer.stamp(f'after_prediction')\n\n            epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            del x_features, y_pred\n            torch.cuda.empty_cache()\n\n        test_array = epoch_test_pred\n        elapsed_time = time.time() - start_time\n        del model, epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_large_CASED = test_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_pred_large_CASED, bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_list = [test_pred_rnn,\n             test_pred_large_uncased, test_pred_large_CASED,\n             test_pred_base_uncased_ftp13,\n             test_pred_base_uncased, test_pred_base_CASED,\n             test_pred_gpt2_max]\n\ncorr_mat = np.corrcoef(pred_list)\nprint(corr_mat)\n\nimport seaborn as sns\nsns.heatmap(corr_mat, cmap='viridis')\n\ntest_pred = np.average(pred_list,\n                       weights=[0.5, 2, 2, 1, 1.5, 1.5, 0.5], axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_pred, bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\ndf_submit.prediction = test_pred\ndf_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}