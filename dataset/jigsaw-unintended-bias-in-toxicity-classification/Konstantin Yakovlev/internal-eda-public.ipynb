{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 10\npd.options.display.max_colwidth = 20\n\n## Validation\n# :validation_df - DataFrame to make validation # type: pandas DataFrame\n# :preds_df - DataFrame with predictions        # type: pandas DataFrame (columns ['id','prediction'])\n# :verbose - print or not full report           # type: bool\ndef local_validation(validation_df, preds_df, verbose=False):\n    validation_df = validation_df.merge(preds_df[['id', 'prediction']], on=['id'], how='left').dropna()\n       \n    for col in ['target'] + identity_columns:\n        validation_df[col] = np.where(validation_df[col] >= 0.5, True, False)\n\n    SUBGROUP_AUC = 'subgroup_auc'\n    BPSN_AUC = 'bpsn_auc'  \n    BNSP_AUC = 'bnsp_auc'  \n    TOXICITY_COLUMN = 'target'\n\n    def compute_auc(y_true, y_pred):\n        try:\n            return metrics.roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n    \n    def compute_subgroup_auc(df, subgroup, label, model_name):\n        return compute_auc(df[df[subgroup]][label], df[df[subgroup]][model_name])\n    \n    def compute_bpsn_auc(df, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n        subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n        non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n        examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n        return compute_auc(examples[label], examples[model_name])\n    \n    def compute_bnsp_auc(df, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n        subgroup_positive_examples = df[df[subgroup] & df[label]]\n        non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n        examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n        return compute_auc(examples[label], examples[model_name])\n    \n    def compute_bias_metrics_for_model(dataset, subgroups, model, label_col, include_asegs=False):\n        \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n        records = []\n        for subgroup in subgroups:\n            record = {\n                'subgroup': subgroup,\n                'subgroup_size': len(dataset[dataset[subgroup]])\n            }\n            record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n            record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n            record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n            records.append(record)\n        return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n    \n    def calculate_overall_auc(df, model_name):\n        return metrics.roc_auc_score(df[TOXICITY_COLUMN], df[model_name])\n    \n    def power_mean(series, p):\n        return np.power(sum(np.power(series, p)) / len(series), 1 / p)\n    \n    def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n        bias_score = np.average([\n            power_mean(bias_df[SUBGROUP_AUC], POWER),\n            power_mean(bias_df[BPSN_AUC], POWER),\n            power_mean(bias_df[BNSP_AUC], POWER)\n        ])\n        return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n    bias_metrics_df = compute_bias_metrics_for_model(validation_df, identity_columns, 'prediction', 'target')\n    if verbose:\n        print(bias_metrics_df)\n    print(get_final_metric(bias_metrics_df, calculate_overall_auc(validation_df, 'prediction')))\n\ndef validate_df(df, preds, verbose=True, val_df='train'):\n    df = df.copy()\n    df['prediction'] = preds\n    if val_df=='train':\n        local_validation(train, df, verbose)  \n    else:\n        local_validation(test, df, verbose)  \n## ----------------------------------------------------------------------------------------------------\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kernel Summary\n\nWe can divide this competition pipeline into **5 parts**.\n\n### 1 - Data Augmentation\nThe main question - can we find a good, labeled dataset to augment our data?\nThe only one that I have in mind is the past Jigsaw competition.\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n\nOther possibilities:\n* Translations\n* Sentence permutation\n\nWe didn't try anything of this till now.\n\n\n### 2 - Text preprocessing\n**Key findings here:**\n* We have to do different preprocess for Bert and for embeddings;\n* We can use different preprocess functions (that brings diversity for final stacking);\n* We can't use data from the test set, as it will be different for 2nd stage and we can't risk;\n* It is a VERY important step.\n\n\n### 3 - Modeling\n**Key findings here:**\n* Bert is better than LSTM, but takes much more time;\n* Stacking works better with different models (Bert/LSTM/CuDNNGRU);\n* Till now we tested 3 models Fatsai Bert / Fastai LSTM / Keras CuDNNGRU;\n* More options here - Character Level CNNs in Keras / Elmo / ULMfit;\n* Bag of words doesn't perform well even on stacking level;\n* We do NOT have much time left to do experiments, so probably we have to concentrate our efforts on what we have done.\n\n\n### 4 - Stacking\n**Key findings here:**\n* LGBM works best on this level;\n* No overfiting (we have base \"regularization\" by converting float32 to float16 ))));\n* We can add more models here - Log/Linear regression / Catboost / XGBM etc.;\n* We can pretrain models and use kernel just for predictions.\n\n\n### 5 - Ensemble\n**Key findings here:**\n* If we will use more than 1 model in \"part 4\" we can do ensembling;\n* SDG works best for \"weighted\" result;\n* We need really good additional models in \"part 4\" to this step makes sense.\n\n\n### Summary\nOur final submission is as strong as strong each part. \nBetter preprocess -> better model performance -> better stacking -> better result.\nWe should do updates and improvements in each step.\n\nLet's see what we have and what we can do step by step."},{"metadata":{},"cell_type":"markdown","source":"----------------"},{"metadata":{},"cell_type":"markdown","source":"# Part 1 - Data Augmentation\n#### !IMPORTANT -> All augmentated data should be placed in the same fold with original to control leakage.\n"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Data augmentation from past challenge\nWhat we have from past Jigsaw challenge.\n\nWe have \"matching\" on these labels (name could differ, but the meaning is the same):\n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\nprint('Total labeled Items:', len(df))\nprint('Lables and Data', list(df))\nprint('#'*10, 'Number of items by label')\nfor col in list(df)[2:]:\n    print('#'*5,col, len(df[df[col]>=0.5]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much data, but it is well labeled. \nWhat we can do with this set -> preprocess with one of our Preprocessors and train Bert on it.\nPredict all our TRAIN and TEST sets and use it as meta-features for stacking.\n* No weighting, as we do not have identity labels here\n* No need to hard tune Bert for it\n\nWe need prof-of-concept check (just simple draft to see if we can have any boost)"},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Translation\nWe can use some API to translate PART of the train data set.\nOnly comment with positive identity matching.\n\nIt could give us vocabulary diversity for 2nd stage test data.\n\n**Example**\n* Where does the demand for self driving cars come from?  Do we really have a shortage of drivers? **Is this not an industry in search of a market that does not exist?**\n* ¿De dónde viene la demanda de autos de auto conducción? ¿Realmente tenemos una escasez de conductores? ¿No es esto una industria en busca de un mercado que no existe?\n* Where does the demand for auto driving cars come from? Do we really have a shortage of drivers? **Is not this an industry looking for a market that does not exist?**\n\nWe will NOT translate TEST comments as we will not have such option (no online access) for the 2nd stage.\n\nGoogle cloud and Microsoft have trial API -> We need to test it.\n\n\nVocabulary deversity:\n~~~~\ngay /ɡeɪ/\nnoun\na homosexual, especially a man.\nsynonyms:\thomosexual, lesbian, gay person, lesbigay;\n~~~~"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv').fillna(0)\nidentity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n                    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\ndf = df[df[identity_columns].sum(axis=1)>0]\nprint('Comment for translation:', len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Sentence permutation\nIdea is very simple. 1 + 2 = 3 = 2 + 1\n\n**Example**\n* Where does the demand for self driving cars come from?  Do we really have a shortage of drivers? Is this not an industry in search of a market that does not exist?\n* Is this not an industry in search of a market that does not exist? Where does the demand for self driving cars come from?  Do we really have a shortage of drivers?\n\nWe could try to devide comments by sentences and then do a swap.\n\nIn this case we can do it on TRAIN and TEST data (just don't forget to average final score).\n\nAs we have huge number of comments we can do it only for train comments with positive identity matching and all test set.\n"},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Synonymization\nIt is a bad option. The main problem here that we can swap words out of context.\nTo do it in the right way will be a time-consuming task."},{"metadata":{},"cell_type":"markdown","source":"## 1.5 Summary\nI do not think we have any other real option for data augmentation.\n\nPlease correct me if I'm wrong.\n\n** literature **\n\n* classification techniques for noisy and imbalanced data\nhttps://fau.digital.flvc.org/islandora/object/fau%3A4271/datastream/OBJ/download/Classification_techniques_for_noisy_and_imbalanced_data.pdf\n\n* Imbalanced text classification: A term weighting approach\nhttps://ccc.inaoep.mx/~villasen/bib/Imbalanced%20text%20classification-%20A%20term%20weighting%20approach.pdf\n\n* Addressing the problem of Unbalanced Data sets in Sentiment Analysis\nhttps://www.academia.edu/5505329/Addressing_the_problem_of_Unbalanced_Data_sets_in_Sentiment_Analysis\n\n* Handling of Imbalanced Data in Text. Classification: Category-Based Term Weights\nhttps://link.springer.com/chapter/10.1007%2F978-1-84628-754-1_10\n"},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"# Part 2 - Text preprocessing\n\nActually, we have 4 options for preprocessing. You can find it in our shared kernel.\n\nhttps://www.kaggle.com/kyakovlev/jigsaw-preprocess-collections\n\n* \"Classic preprocess\"\nhttps://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\n* \"Modified classic\"\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\n\n* Own preprocess for Bert (\"Experimental preprocess for Bert\")\n\n* Own preprocess for embeddings (\"Experimental preprocess\")\n\n\n\"Modified classic\" we can use for benchmarks as it serves for bert and for embeddings (I will prepare a \"single step\" model for a benchmark with the possibility to \"probe\" LB)."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Noise reduction\n\nMain things we can do is to reduce noise in data.\n\n**Links - Isolate domain name / split by words and wrap link string**\n* **Example:**\n* https://www.theguardian.com/uk-news/2017/jul/26/charlie-gard-us-pro-life-rightwing-ethics\"\n* theguardian.com [ uk news charlie gard us pro life rightwing ethics ]\n\n\n**Spam chars - chars are not present in our Vocabulary (Fastext or Glove)**\n* \"\\u200b\\u200eعدويهصقأناخلىبمغر\" etc.\n\n\n**Contractions**\n* Some Vocabularies have vectors for contractions, some not. Use contraction dict matching to convert missing ones.\n\n\n**Word normalization**\n* Mother's -> Mother\n* Mothers -> Mother (if plural form is not in Vocabulary)\n* «Mother» - > \" Mother \"\n* etc.\n\n\n**Punctuation isolation**\n* \"Mother\" -> \" Mother \"\n* Mother.Father -> Mother . Father\n\n\n**Emoji**\n* Depends on Vocabulary \n* Convert to text\n* or remove\n\n\n**Bad words matching**\n* @$$ -> ass\n* F*ck -> Fuck\n* etc\n\nAll normalization has to be done to match Vocabulary of the model (FastText/Glove/Bert)\n"},{"metadata":{},"cell_type":"markdown","source":"-----"},{"metadata":{},"cell_type":"markdown","source":"# EDA\nBefore we can pass to modeling, let's check what kind of data we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv').fillna(0)\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns in TRAIN data set', list(train))\nprint('Columns in TEST data set', list(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we can see here is we can use only 'comment_text' directly for training.\n\nOther columns we can use in 'indirect' mode (weights, aux-output, etc.)"},{"metadata":{},"cell_type":"markdown","source":"## Aux-output\nLets check what kind of columns we can use as aux-output (make predictions and use it on stacking level)."},{"metadata":{"trusted":true},"cell_type":"code","source":"set_1 = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\nset_2 = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', \n        'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', \n         'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', \n         'other_disability', 'other_gender', 'other_race_or_ethnicity', \n         'other_religion', 'other_sexual_orientation', 'physical_disability', \n         'psychiatric_or_mental_illness', 'transgender', 'white']\nset_3 = ['created_date', 'publication_id', 'parent_id', 'article_id','rating']\nset_4 = ['funny', 'wow', 'sad', 'likes', 'disagree']\n\nfrom sklearn.linear_model import LinearRegression\nX = train[set_1]\ny = np.where((train['target']>=0.5),1,0)\n\nlr = LinearRegression().fit(X, y)\nprint('LR overall AUC for set_1:', metrics.roc_auc_score(y, lr.predict(X)))\nvalidate_df(train, lr.predict(X), verbose=True, val_df='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How we can interpret these results?**\n\t\nIF we can do a perfect prediction for aux-columns in SET 1\nwe can have  0.9863711832271177 BIASed AUC\n\nIt means that this columns ARE important.\n"},{"metadata":{},"cell_type":"markdown","source":"### Identities"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[set_2]\ny = np.where((train['target']>=0.5),1,0)\n\nlr = LinearRegression().fit(X, y)\nprint('LR overall AUC for set_1:', metrics.roc_auc_score(y, lr.predict(X)))\nvalidate_df(train, lr.predict(X), verbose=True, val_df='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How we can interpret these results?**\n\t\nIdentity mark -> not always a toxicity. \n\nKey word here -> NOT ALWAYS, please see that for SOME identity groups subgroup_auc is high.\nIt means that for some groups identity keywords are toxic ones.\n\nProbably it is good idea to include it in aux output for some model."},{"metadata":{},"cell_type":"markdown","source":"## Emotions"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[set_4]\ny = np.where((train['target']>=0.5),1,0)\n\nlr = LinearRegression().fit(X, y)\nprint('LR overall AUC for set_1:', metrics.roc_auc_score(y, lr.predict(X)))\nvalidate_df(train, lr.predict(X), verbose=True, val_df='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOTHING)).\nHow we can use this information?\n\n['funny', 'wow', 'sad', 'likes', 'disagree']\n\nIt is very valuable information -> we just need to find a way to use it.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Information about comment and article\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['created_date'] = pd.to_datetime(train['created_date'])\ntrain['year'] = train['created_date'].dt.year.astype(np.int8)\ntrain['month'] = train['created_date'].dt.month.astype(np.int8)\ntrain['day'] = train['created_date'].dt.day.astype(np.int8)\ntrain['weekday'] = train['created_date'].dt.weekday.astype(np.int8)\ntrain['time_block'] = (train['created_date'].dt.year - 2015).astype(np.int8)*12 + (train['created_date'].dt.month).astype(np.int8)\n\nset_3 = ['year', 'month', 'day', 'weekday', 'publication_id', 'parent_id', 'article_id']\n\nX = train[set_3]\ny = np.where((train['target']>=0.5),1,0)\n\nlr = LinearRegression().fit(X, y)\nprint('LR overall AUC for set_1:', metrics.roc_auc_score(y, lr.predict(X)))\nvalidate_df(train, lr.predict(X), verbose=True, val_df='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that this data is useless,  but it's not. It is some kind of Categorical data and we need to treat it in that way."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = train.groupby('month').agg({'target':'mean'}).reset_index()\nplt.figure(figsize=(10, 6))\nplt.plot(temp_df['month'], temp_df['target']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = train.groupby('weekday').agg({'target':'mean'}).reset_index()\nplt.figure(figsize=(10, 6))\nplt.plot(temp_df['weekday'], temp_df['target']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = train.groupby('time_block').agg({'target':'mean'}).reset_index()\nplt.figure(figsize=(10, 6))\nplt.plot(temp_df['time_block'], temp_df['target']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(train['id']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}