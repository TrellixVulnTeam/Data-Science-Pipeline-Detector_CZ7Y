{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Scikit Learn\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Flatten, MaxPooling1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport gc\n\nprint(\"Libraries loaded\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/thousandvoices/simple-lstm"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/jigsaw-unintended-bias-in-toxicity-classification'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a06d6485c4fb4b16cde71699486723c3f0035241"},"cell_type":"markdown","source":"**Read the data******"},{"metadata":{"_uuid":"719db1055eea23f98cbda39c6bf227bd7e6b0645","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n\n# train_df =   train_df.head(1000)\n# test_df =   test_df.head(1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train_1 = train[train['target']==1]\n# Train_0 = train[train['target']==0]\n# train =  pd.concat([Train_1,Train_0.head(len(Train_1))], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[train['target']==1].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50d7f1c0fdf4aea206afdacb9db5007a2c62c2ee"},"cell_type":"markdown","source":"**Check the target column break down**"},{"metadata":{"_uuid":"2afd3095b6715f7704a4848a9d067a3bc129316a","trusted":true},"cell_type":"code","source":"# train['target'] = np.where(train['target'] >= 0.5, 1, 0)\n# train[\"target\"].value_counts()\n\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bcd51b27e18083305894ab7ad4d0512ff6acc6d"},"cell_type":"markdown","source":"**Word Cloud**"},{"metadata":{"_uuid":"f0d7f44deca7e7b8b962238a59f0df4cbb9e00d9"},"cell_type":"markdown","source":"* **1) Word cloud of Non toxic comments**"},{"metadata":{"_uuid":"f3b0eeaf667f088235e9f618f7b4a75c588e24f2","trusted":true},"cell_type":"code","source":"# stopwords = set(STOPWORDS)\n\n\n# wordcloud = WordCloud(\n#                           background_color='white',\n#                           stopwords=stopwords,\n#                           max_words=200,\n#                           max_font_size=40, \n#                           random_state=42\n#                          ).generate(str(train[train.target==0]['comment_text']))\n\n# print(wordcloud)\n# fig = plt.figure(1)\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()\n# fig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64ed082363b637ed1d8cce75fb2678dc20776780"},"cell_type":"markdown","source":"* **2) Word cloud of toxic comments**"},{"metadata":{"_uuid":"68a5c228ccc86c6f914f8e7cd4f540c44bd2da4f","trusted":true},"cell_type":"code","source":"# stopwords = set(STOPWORDS)\n\n# wordcloud = WordCloud(\n#                           background_color='white',\n#                           stopwords=stopwords,\n#                           max_words=200,\n#                           max_font_size=40, \n#                           random_state=42\n#                          ).generate(str(train[train.target==1]['comment_text']))\n\n# print(wordcloud)\n# fig = plt.figure(1)\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()\n# fig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6d9634e5004e9ffd5854938d3b9ed631b20b831","trusted":true},"cell_type":"code","source":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n## some config values \nembed_size = 50 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"comment_text\"].fillna(\"_na_\").values\nval_X = val_df[\"comment_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"comment_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(train_X)+list(val_X)+list(test_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"717fb2b8fc1283bb8c2db5f90a8125a35694ebfb","trusted":true},"cell_type":"code","source":"\n# inp = Input(shape=(maxlen,))\n# x = Embedding(max_features, embed_size)(inp)\n# x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n# x = GlobalMaxPool1D()(x)\n# x = Dense(16, activation=\"relu\")(x)\n# x = Dropout(0.1)(x)\n# x = Dense(1, activation=\"sigmoid\")(x)\n# model = Model(inputs=inp, outputs=x)\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# print(model.summary())\n# model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n\n\n# define the model\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size, input_length=maxlen))\n\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\nmodel.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve, auc,  f1_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score\n# Making the Confusion Matrix\ndef get_metrics(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n\n    class_names=[0,1] # name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n\n    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n    # Model Precision: what percentage of positive tuples are labeled as such?\n    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n    # Model Recall: what percentage of positive tuples are labelled as such?\n    print(\" True positive rate or (Recall or Sensitivity) :\",metrics.recall_score(y_test, y_pred))\n\n    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n    specificity = tn / (tn+fp)\n\n    #Specitivity. or True negative rate\n    print(\" True Negative rate or Specitivity :\",specificity)\n\n    false_negative = fn / (fn+tp)\n\n    #False negative rate\n    print(\" False Negative rate :\",false_negative)\n\n    #False positive rate\n    print(\" False positive rate (Type 1 error) :\",1 - specificity)\n    \n    print('F Score', f1_score(y_test, y_pred))\n    print(cm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# val_pred_y = model.predict_classes([val_X], batch_size=1024, verbose=1)\n# get_metrics(val_y,val_pred_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"fast_text = ['../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec']\n\nfast_text_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in fast_text], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## some config values \nembed_size = 50 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\nmodel = Sequential()\nmodel.add(Embedding(len(fast_text_matrix), 300, input_length=maxlen, weights=[fast_text_matrix], trainable=False))\n\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\n\n# model.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\nmodel.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(val_X, val_y))\n\nget_metrics(val_y,model.predict_classes([val_X], batch_size=1024, verbose=1))\n\nfast_text_val_pred_y = model.predict([test_X], batch_size=1024, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del fast_text_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nglove = ['../input/glove840b300dtxt/glove.840B.300d.txt']\nglove_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in glove], axis=-1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## some config values \nembed_size = 50 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\nmodel = Sequential()\nmodel.add(Embedding(len(glove_matrix), 300, input_length=maxlen, weights=[glove_matrix], trainable=False))\n\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n\nmodel.add(GlobalMaxPooling1D())\n\n# model.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\nmodel.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(val_X, val_y))\n\nget_metrics(val_y,model.predict_classes([val_X], batch_size=1024, verbose=1))\n\nglove_val_pred_y = model.predict([test_X], batch_size=1024, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del glove_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions =  (fast_text_val_pred_y + glove_val_pred_y) / 2\nfinal_predictions = np.where(final_predictions >= 0.5, 1, 0)\noutput = pd.DataFrame({\"id\":test_df[\"id\"].values})\noutput['prediction'] = final_predictions\noutput.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}