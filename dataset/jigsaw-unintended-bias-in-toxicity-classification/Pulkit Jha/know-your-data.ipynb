{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom collections import defaultdict\nimport os\nimport re\nimport warnings\nimport random\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\n\nprint(os.listdir(\"../input\"))\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_colwidth', -1)\nwarnings.filterwarnings('ignore')\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nsns.set_style('darkgrid')\nsns.set_palette('deep')\nfigSize = (10, 6)\n\npercentiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# *** Read Data ***\ntrainDf = pd.read_csv('../input/train.csv')\ntestDf  = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# *** Get a View of Data ***\ntrainDf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# *** Train and Test Shape ***\nprint('Train Data - Number of Rows : {} Number of Columns : {}'.format(trainDf.shape[0], trainDf.shape[1]))\nprint('Test Data  - Number of Rows : {} Number of Columns : {}'.format(testDf.shape[0], testDf.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Different Data Types Distribution in Train Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# *** Check Data Types ***\ndTypeDist = defaultdict(int)\nfor aDict in trainDf.dtypes.reset_index().rename(columns = {'index' : 'colName', 0 : 'dType'}).to_dict(orient='records'):\n    dTypeDist[str(aDict['dType']).replace('dtype(', '').replace(')', '')] += 1\n\ndTypeDistDf = pd.DataFrame.from_dict(dTypeDist, orient = 'index'). \\\n                 reset_index(). \\\n                 rename(columns = {'index' : 'dType', 0 : 'colCount'})\n                 \n#print(dTypeDistDf)\n\nfig, ax = plt.subplots(figsize = figSize)\ngraph = sns.barplot(x = dTypeDistDf['dType'], y = dTypeDistDf['colCount'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Missing Valu Distribution"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# *** Check Missing Values ***\nmissingValsDf = trainDf.isnull().sum().reset_index(). \\\n                        rename(columns = {'index' : 'colName', 0 : '#MissingValues'})\n\nfig, ax = plt.subplots(figsize = (20, 8))\ngraph = sns.lineplot(x = 'colName', y = '#MissingValues', data = missingValsDf)\nxtks = plt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Distribution of Unique Values"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# *** Unique Values ***\nuniqValsDf = trainDf.nunique(). \\\n                    reset_index(). \\\n                    rename(columns = {'index' : 'colName', 0 : '#UniqValues'})\n\nfig, ax = plt.subplots(figsize = (20, 8))\ngraph = sns.lineplot(x = 'colName', y = '#UniqValues', data = uniqValsDf)\nxtks = plt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     - Few comments are getting repeated"},{"metadata":{},"cell_type":"markdown","source":"- Distribution of target"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"targetDistDf = trainDf['target'].describe(percentiles=percentiles). \\\n                                 reset_index(). \\\n                                 rename(columns = {'index' : 'Metric', 'target' : 'Value'})\ntargetDistDf = targetDistDf.ix[1:, :]\n\nfig, ax = plt.subplots(figsize = figSize)\ngraph = sns.barplot(x = targetDistDf['Metric'], y = targetDistDf['Value'], color = 'cadetblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train Basic Keras Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n\nstop_words = set(stopwords.words('english'))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanText(aStentence):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    text = aStentence.lower()\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\",\", \" commas \", text)\n    text = re.sub(r\"\\.\", \" fullStop \", text)\n    text = re.sub(r\"!\", \" exclamationmark \", text)\n    text = re.sub(r\"\\?\", \" questionmark \", text)\n    text = re.sub(r\"'\", \" singleQoute \", text)\n    text = re.sub(r'\"', \" doubleQoute \", text)\n    text = re.sub(r'\\n', \" newLine \", text)\n    text = re.sub(\"[^A-za-z]\",\" \", text)\n    wordTokens = word_tokenize(text)\n    text = ' '.join([CONTRACTION_MAP[aWord] if aWord in CONTRACTION_MAP else aWord for aWord in wordTokens])\n    text = ' '.join([aWord for aWord in text.split() if aWord not in stop_words])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['comment_text'] = trainDf['comment_text'].map(lambda x : ' '.join([w.lower() for w in cleanText(x).split() if w!='']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['uniformNumber'] = trainDf['comment_text'].map(lambda x : random.uniform(0, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = list(trainDf[trainDf['uniformNumber'] <= 0.7]['comment_text'])\ntrain_labels = list(trainDf[trainDf['uniformNumber'] <= 0.7]['target'])\n\nvalidation_sentences = list(trainDf[trainDf['uniformNumber'] > 0.7]['comment_text'])\nvalidation_labels = list(trainDf[trainDf['uniformNumber'] > 0.7]['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 1000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\n#word_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 3\nhistory = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(validation_padded, validation_labels), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDf['comment_text'] = testDf['comment_text'].map(lambda x : ' '.join([w.lower() for w in cleanText(x).split() if w!='']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = list(testDf['comment_text'])\ntest_sequences = tokenizer.texts_to_sequences(test_sequences)\ntest_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict(test_padded).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subDf  = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subDf['prediction'] = pd.Series(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subDf.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}