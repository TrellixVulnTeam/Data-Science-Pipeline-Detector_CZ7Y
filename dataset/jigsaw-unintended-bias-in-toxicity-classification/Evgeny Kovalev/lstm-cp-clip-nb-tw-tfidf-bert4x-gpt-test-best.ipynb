{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors\n\nimport warnings\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nimport string\nimport re\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport gc\nimport os\n\nimport copy\n\nimport pickle\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\ndef build_matrix(word_index, path, dim=300):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, dim))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n    \nclass SoftmaxPooling(nn.Module):\n    def __init__(self, dim=1):\n        super(self.__class__, self).__init__()\n        self.dim = dim\n        \n    def forward(self, x):\n        return (x * x.softmax(dim=self.dim)).sum(dim=self.dim)\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n\n        self.linear_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        )\n        \n        self.linear_aux_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        )\n        \n        self.softmaxpool = SoftmaxPooling()\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        # softmax pooling\n        soft_pool = self.softmaxpool(h_lstm2)\n        \n        h_conc = torch.cat((max_pool, avg_pool, soft_pool), 1)\n        hidden = h_conc\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\", axis=1):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=axis)\n    # CAREFUL WITH WHAT IS BELOW, IT IS DIFFERENT FOR LSTM BLENDING AND LSTM/BERT BLENDING\n    elif type_ == \"harmonic\":\n        res = np.average([1 / p for p in predictions], weights=weights, axis=axis)\n        return 1 / res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=axis\n        )\n        res = np.exp(numerator / sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=axis)\n        return res / (len(res) + 1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nSEED = 1234\nBATCH_SIZE = 512\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim'\nNUMBERBATCH_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/numberbatch-en.gensim'\nPARAGRAM_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/paragram_300_sl999.gensim'\nTWITTER_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/glove.twitter.27B.200d.gensim'\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM Crawl + Paragram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# only here the values are like this\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 768","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = TreebankWordTokenizer()\n\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n\nloss_weight = 3.209226860170181\n\nmax_features = 400000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/bilstm-crawl-paragram-0/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nparagram_matrix, unknown_words_paragram = build_matrix(tokenizer.word_index, PARAGRAM_EMBEDDING_PATH)\nprint('n unknown words (paragram): ', len(unknown_words_paragram))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, paragram_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel paragram_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_seq = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 300\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test_seq]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test_seq, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=test_loader, valid_dl=test_loader, collate_fn=test_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model_cp(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('../input/bilstm-crawl-paragram-{}/model_1_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds = pd.DataFrame()\ntest_model_preds['lstm_cp_0_3'] = test_model_cp(0, 3)[:, 0]\ntest_model_preds['lstm_cp_1_3'] = test_model_cp(1, 3)[:, 0]\ntest_model_preds['lstm_cp_2_3'] = test_model_cp(2, 3)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM clip target [0.05, 0.95]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# since this one - like this\nLSTM_UNITS = 256\nDENSE_HIDDEN_UNITS = 1536","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model_clip(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('../input/lstm-clip-{}/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds['lstm_clip_0_2'] = test_model_clip(0, 2)[:, 0]\ntest_model_preds['lstm_clip_1_3'] = test_model_clip(1, 3)[:, 0]\ntest_model_preds['lstm_clip_2_3'] = test_model_clip(2, 3)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM Numberbatch"},{"metadata":{"trusted":true},"cell_type":"code","source":"numberbatch_matrix, unknown_words_numberbatch = build_matrix(tokenizer.word_index, NUMBERBATCH_EMBEDDING_PATH)\nprint('n unknown words (numberbatch): ', len(unknown_words_numberbatch))\n\nembedding_matrix = numberbatch_matrix\nprint(embedding_matrix.shape)\n\ndel numberbatch_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model_nb(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('../input/lstm-numberbatch-{}/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim))\n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds['lstm_nb_0_4'] = test_model_nb(0, 4)[:, 0]\ntest_model_preds['lstm_nb_1_4'] = test_model_nb(1, 4)[:, 0]\ntest_model_preds['lstm_nb_2_4'] = test_model_nb(2, 4)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM Twitter"},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_matrix, unknown_words_twitter = build_matrix(tokenizer.word_index, TWITTER_EMBEDDING_PATH, dim=200)\nprint('n unknown words (twitter): ', len(unknown_words_twitter))\n\nembedding_matrix = twitter_matrix\nprint(embedding_matrix.shape)\n\ndel twitter_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model_tw(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('../input/lstm-twitter-{}/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds['lstm_tw_0_3'] = test_model_tw(0, 3)[:, 0]\ntest_model_preds['lstm_tw_1_3'] = test_model_tw(1, 3)[:, 0]\ntest_model_preds['lstm_tw_2_3'] = test_model_tw(2, 3)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nrepl = {\n    \"&lt;3\": \" good \",\n    \":d\": \" good \",\n    \":dd\": \" good \",\n    \":p\": \" good \",\n    \"8)\": \" good \",\n    \":-)\": \" good \",\n    \":)\": \" good \",\n    \";)\": \" good \",\n    \"(-:\": \" good \",\n    \"(:\": \" good \",\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" bad \",\n    \":(\": \" bad \",\n    \":s\": \" bad \",\n    \":-s\": \" bad \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\n\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s):\n    return re_tok.sub(r' \\1 ', s).split()\n\ncont_patterns = [\n        (b'US', b'United States'),\n        (b'IT', b'Information Technology'),\n        (b'(W|w)on\\'t', b'will not'),\n        (b'(C|c)an\\'t', b'can not'),\n        (b'(I|i)\\'m', b'i am'),\n        (b'(A|a)in\\'t', b'is not'),\n        (b'(\\w+)\\'ll', b'\\g<1> will'),\n        (b'(\\w+)n\\'t', b'\\g<1> not'),\n        (b'(\\w+)\\'ve', b'\\g<1> have'),\n        (b'(\\w+)\\'s', b'\\g<1> is'),\n        (b'(\\w+)\\'re', b'\\g<1> are'),\n        (b'(\\w+)\\'d', b'\\g<1> would'),\n    ]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n\ndef prepare_for_char_n_gram(text):\n    \"\"\" Simple text clean up process\"\"\"\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower(), encoding=\"utf-8\")\n    # 2. Drop \\n and  \\t\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    # 3. Replace english contractions\n    for (pattern, repl) in patterns:\n        clean = re.sub(pattern, repl, clean)\n    # 4. Drop puntuation\n    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n    clean = re.sub(b\"\\d+\", b\" \", clean)\n    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n    clean = re.sub(b'\\s+', b' ', clean)\n    # Remove ending space if any\n    clean = re.sub(b'\\s+$', b'', clean)\n    # 7. Now replace words by words surrounded by # signs\n    # e.g. my name is bond would become #my# #name# #is# #bond#\n    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n\n    return str(clean, 'utf-8')\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))\n\ndef get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Count number of \\n\n#     df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n    # TODO chars per row\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    # Number of F words - f..k contains folk, fork,\n    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n    # Number of S word\n    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n    # Number of D words\n    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n    # Number of occurence of You, insulting someone usually needs someone called : you\n    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n    # Just to check you really refered to my mother ;-)\n    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n    # Just checking for toxic 19th century vocabulary\n    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n    # Some Sentences start with a <:> so it may help\n    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n    # Check for dates 18:44, 8 December 2010\n    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n    # Check for date short 8 December 2010\n    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n    # Check for http links\n#     df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n    # check for mail\n    df[\"has_mail\"] = df[\"comment_text\"].apply(\n        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n    )\n    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n\n    # Now clean comments\n    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n\n    # Get the new length in words and characters\n    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n    # Number of different characters used in a comment\n    # Using the f word only will reduce the number of letters required in the comment\n    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n        lambda x: 1 + min(99, len(x)))\n\nfts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n    \ndef preprocess(df):\n    keys = [i for i in repl.keys()]\n\n    new_data = []\n    ltr = df[\"comment_text\"].tolist()\n    for i in tqdm(ltr):\n        arr = str(i).split()\n        xx = \"\"\n        for j in arr:\n            j = str(j).lower()\n            if j[:4] == 'http' or j[:3] == 'www':\n                continue\n            if j in keys:\n                # print(\"inn\")\n                j = repl[j]\n            xx += j + \" \"\n        new_data.append(xx)\n    df[\"new_comment_text\"] = new_data\n    \n    trate = df[\"new_comment_text\"].tolist()\n    for i, c in enumerate(trate):\n        trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n    df[\"comment_text\"] = trate\n    df.drop([\"new_comment_text\"], axis=1, inplace=True)\n\n    df_text = df['comment_text']\n    \n    \n    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n    \n    get_indicators_and_clean_comments(df)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = preprocess(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n\ntest_text = test_df['clean_comment'].apply(lambda x: re.sub('#', '', x)).fillna('')\n\nword_vectorizer = TfidfVectorizer(\n        sublinear_tf=True,\n        strip_accents='unicode',\n        analyzer='word',\n        min_df=5,\n        ngram_range=(1, 2),\n        max_features=60000)\n\nwith open('../input/jigsaw-tfidf-models/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\ntest_word_features = word_vectorizer.transform(test_text)\ntest_features = hstack([test_df[fts], test_word_features]).tocsr()\ndel test_word_features\n\nwith open('../input/jigsaw-tfidf-models/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \ntest_model_preds['tfidf_gbm'] = gbm.predict(test_features)\n\ntext = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s):\n    return text.sub(r' \\1 ', s)\n\nword_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=50000)\n\nwith open('../input/simple-tfidf-models/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\ntest_tfidf = word_vectorizer.transform(test_df['comment_text'].fillna(''))\n\nlr = LogisticRegression(solver='lbfgs', random_state=13)\nwith open('../input/simple-tfidf-models/lr_model.pickle', 'rb') as handle:\n    lr = pickle.load(handle)\n\ntest_model_preds['tfidf_lr_simple'] = lr.predict_proba(test_tfidf)[:, 1]\n\nwith open('../input/simple-tfidf-models/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \ntest_model_preds['tfidf_gbm_simple'] = gbm.predict(test_tfidf)\n\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n        \n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n\nNbSvm = NbSvmClassifier(C=1.5, dual=True, n_jobs=-1)\nwith open('../input/simple-tfidf-nbsvm/nbsvm_model.pickle', 'rb') as handle:\n    NbSvm = pickle.load(handle)\n    \ntest_model_preds['tfidf_nbsvm_simple'] = NbSvm.predict_proba(test_tfidf)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"! md5sum ../input/jigsaw2019code/*.py\n! ls ../input/\n! find ../input/ -name config.json | grep -v deps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deps_path = '../input/kagglejigsaw2019deps/kaggle-jigsaw-2019-deps/kaggle-jigsaw-2019-deps/'\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" $deps_path/apex\n! pip install $deps_path/pytorch-pretrained-BERT/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/jigsaw2019code/bert.py \\\n  _runs/bert-base-uncased-pretrained-ep1 \\\n  --model ../input/jigsaw2019bertbaseuncasedpretrainedep1/bert-base-uncased-pretrained-ep1/bert-base-uncased-pretrained-ep1 \\\n  --submission --test-size $test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/jigsaw2019code/bert.py \\\n  _runs/gpt2-ep2-lr8e-5 \\\n  --bucket 0 \\\n  --model ../input/gpt2ep2lr8e5/gpt2-ep2-lr8e-5/gpt2-ep2-lr8e-5/ \\\n  --submission --test-size $test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/jigsaw2019code/bert.py \\\n  _runs/bert-base-cased-pretrained-ep1 \\\n  --model ../input/bertbasecasedpretrainedep1/bert-base-cased-pretrained-ep1/bert-base-cased-pretrained-ep1/ \\\n  --submission --test-size $test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/jigsaw2019code/bert.py \\\n  _runs/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16 \\\n  --model ../input/bertluncased1redo/ \\\n  --submission --test-size $test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/jigsaw2019code/bert.py \\\n  _runs/bert-base-uncased-fresh-ep1 \\\n  --model ../input/bertbaseuncasedfreshep1/bert-base-uncased-fresh-ep1/bert-base-uncased-fresh-ep1/ \\\n  --submission --test-size $test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_uncased1 = pd.read_csv('_runs/bert-base-uncased-pretrained-ep1/submission.csv')['prediction']\nbert_cased1 = pd.read_csv('_runs/bert-base-cased-pretrained-ep1/submission.csv')['prediction']\nbert_uncased_fresh1 = pd.read_csv('_runs/bert-base-uncased-fresh-ep1/submission.csv')['prediction']\nbert_large1 = pd.read_csv('_runs/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16/submission.csv')['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds['bert_uncased1'] = bert_uncased1\ntest_model_preds['bert_cased1'] = bert_cased1\ntest_model_preds['bert_uncased_fresh1'] = bert_uncased_fresh1\ntest_model_preds['bert_large1'] = bert_large1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2"},{"metadata":{"trusted":true},"cell_type":"code","source":"gpt22 = pd.read_csv('_runs/gpt2-ep2-lr8e-5/submission.csv')['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds['gpt22'] = gpt22","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_preds.to_csv('test_model_preds.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cp _runs/bert-base-cased-pretrained-ep1/submission.csv bert-base-cased-pretrained-ep1.csv\n! cp _runs/bert-base-uncased-pretrained-ep1/submission.csv bert-base-uncased-pretrained-ep1.csv\n! cp _runs/bert-base-uncased-fresh-ep1/submission.csv bert-base-uncased-fresh-ep1.csv\n! cp _runs/gpt2-ep2-lr8e-5/submission.csv gpt2-ep2-lr8e-5.csv\n! cp _runs/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16/submission.csv \\\n  resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blend"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_weights = {\n    'cp': [50, 40, 40],\n    'clip': [40, 40, 44],\n    'nb': [40, 39, 44],\n    'tw': [40, 39, 40],\n    'tfidf': [42, 44, 41, 37],\n    'bert': [59, 30, -28, 65],\n    'blend': [17, 33, -13, -1, -3, 132, 42]\n}\n\n\ndef custom_predict(X_models, weights=best_weights, type_='linear'):\n    model_cp = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_cp')]], weights=(np.array(weights['cp']) + 1e-15) / (sum(weights['cp']) + 1e-15), type_=type_)\n    model_clip = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_clip')]], weights=(np.array(weights['clip']) + 1e-15) / (sum(weights['clip']) + 1e-15), type_=type_)\n    model_nb = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_nb')]], weights=(np.array(weights['nb']) + 1e-15) / (sum(weights['nb']) + 1e-15), type_=type_)\n    model_tw = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_tw')]], weights=(np.array(weights['tw']) + 1e-15) / (sum(weights['tw']) + 1e-15), type_=type_)\n    model_tfidf = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('tfidf')]], weights=(np.array(weights['tfidf']) + 1e-15) / (sum(weights['tfidf']) + 1e-15), type_=type_)\n    model_bert = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('bert')]], weights=(np.array(weights['bert']) + 1e-15) / (sum(weights['bert']) + 1e-15), type_=type_)\n    model_gpt = X_models['gpt22']\n    \n    models = [model_cp, model_clip, model_nb, model_tw, model_tfidf, model_bert, model_gpt]\n    model_blend = np.zeros_like(model_bert) + 1e-15\n    for i in range(len(models)):\n        model_blend += weights['blend'][i] * models[i]\n    model_blend /= (sum(weights['blend']) + 1e-15)\n    return model_blend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': custom_predict(test_model_preds)\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -rf _runs","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}