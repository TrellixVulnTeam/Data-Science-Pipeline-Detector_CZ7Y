{"cells":[{"metadata":{},"cell_type":"markdown","source":"Best score 0.93417\n- V8 0.92941 - maxlen 230 (nearly timeout!)\n- V7 timeout - maxlen 240\n- V6 timeout - 8x1 models, maxlen 250\n- V5 0.92929 - 8x1 models, maxlen 220 "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Hat tip to @thousandvoices - based on https://www.kaggle.com/thousandvoices/simple-lstm\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/glove840b300dtxt\"))\nprint(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\nprint(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\nEMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\nNUM_MODELS = 8\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 1\nMAX_LEN = 220\nDROPOUT_RATE = 0.3\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '\"\"“”’`\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n  \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CONTRACTIONS = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n                \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n                \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n                \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n                \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n                \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n                \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n                \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n                \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"it's\": \"it is\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\ndef clean_contractions(str, mapping):\n    \"\"\"Expand contractions\"\"\"\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        str = str.replace(s, \"'\")\n    res = ' '.join([mapping[t] if t in mapping else t for t in str.split(' ')])\n    return res\n\ndef clean_special_chars(text, punct, replacement=' '):\n    '''Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution'''\n    for p in punct:\n        text = text.replace(p, replacement)\n    return text\n\ndef preprocess(data):\n    data = data.astype(str)\n    data = data.apply(lambda x: clean_contractions(x, mapping=CONTRACTIONS))\n    data = data.apply(lambda x: clean_special_chars(x, punct=CHARS_TO_REMOVE))\n    return data\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \ndef build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(DROPOUT_RATE)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(SEED)\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_train = preprocess(train_df[TEXT_COLUMN])\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = preprocess(test_df[TEXT_COLUMN])\n\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights /= sample_weights.mean()\nprint(sample_weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\n# word_index: dictionary mapping words (str) to their rank/index (int).\n# Only set after fit_on_texts was called.\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\nprint(embedding_matrix.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train models in blended ensemble\ncheckpoint_predictions = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    seed_everything(SEED + model_idx)\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        print('model {} epoch {}'.format(model_idx + 1, global_epoch + 1))\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n            ]\n        )\n        # make inference\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('len(checkpoint_predictions)={}, len(weights)={}'.format(len(checkpoint_predictions), len(weights)))\npredictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission\nsubmission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint(os.listdir(\".\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}