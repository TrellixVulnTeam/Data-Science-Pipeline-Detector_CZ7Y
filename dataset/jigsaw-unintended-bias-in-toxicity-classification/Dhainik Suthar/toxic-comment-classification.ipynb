{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !unzip -u '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/train.csv (2).zip'\n# !unzip -u '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/test.csv.zip'","metadata":{"id":"53Pum6kMjS_2","outputId":"c2ab99d6-a819-4acb-d953-29951529a9ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing Modules","metadata":{"id":"aYUccjPRj7fN"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import SnowballStemmer, PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier, RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.linear_model import SGDRegressor \nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"id":"kR81JZcki0QX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read train and test","metadata":{"id":"EgaeRojDj_Ky"}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ndf_test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","metadata":{"id":"uVlnfyNyi_0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"yovNUoabjCzO","outputId":"871255e6-8d8a-4310-be65-a315dfee153f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shape of train and test","metadata":{"id":"1dWiqnyLkCMp"}},{"cell_type":"code","source":"print(\"train data shape : \", df_train.shape)\nprint(\"Test data shape : \", df_test.shape)","metadata":{"id":"pizNHlfmjq9Q","outputId":"f07ae967-7054-4bc8-e55c-2b7b5304ccc7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Null value**","metadata":{"id":"HLbxqUKIkHu9"}},{"cell_type":"code","source":"pd.DataFrame(df_train.isnull().sum()/ df_train.shape[0])","metadata":{"id":"nR46YvJcj1cG","outputId":"67f63076-df0e-43d8-8a88-1676c5442da7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are lot null values but we don't care we only care about that all, comment_text and target","metadata":{"id":"CH83rKJtk6fP"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.distplot(df_train['target'], kde=True, hist=False)\nplt.title(\"Distribution of target\")\nplt.grid()\nplt.show()","metadata":{"id":"Hgbgi-z-k51H","outputId":"1d09e416-d731-494e-b7ce-9545f752f8d1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_train['target'].apply(lambda x: 1 if x>=0.5 else 0)\ntotal = float(len(data))\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nsns.countplot(data)\nplt.title('Target Countplot')\n\nfor p in ax.patches:\n  height = p.get_height()\n  ax.text(p.get_x() + p.get_width()/2.0, height+4, '{:1.2f}%'.format(100*height/total))\n","metadata":{"id":"fm9g-mMpk4I1","outputId":"4fcd66b7-321f-404d-e281-5454bbc13fe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data is unbalanced. \n\nthere is only 8% non-toxic comment and 92% toxic comment","metadata":{"id":"b63fQk_emkbD"}},{"cell_type":"code","source":"df_train.columns","metadata":{"id":"DQLnjXl8l9uG","outputId":"4febbd1c-b006-44e0-e836-7ac350c9e21b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distplot_feature(features,title, data):\n  plt.figure(figsize=(10, 8))\n  plt.title(title)\n  for feature in features:\n    sns.distplot(data[feature], kde=True, hist=False, label=feature)\n  plt.legend()\n  plt.xlabel(\" \")\n  plt.show()","metadata":{"id":"Y4q5XEmHnDZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [ 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat' ]\n\ndistplot_feature(features, \"Distribution of additional toxicity features on non-toxic comment\" ,df_train[df_train['target']<0.5])","metadata":{"id":"ldzSJkmPn3Jb","outputId":"6f8f4827-5e83-49ed-fce1-1628dfc6cbfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [ 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat' ]\n\ndistplot_feature(features, \"Distribution of additional toxicity features on toxic comment\" ,df_train[df_train['target']>0.5])","metadata":{"id":"k_xxCk11oAYI","outputId":"7e02b8a4-27fe-4a16-e5bc-3b2b887fbda9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more insulting comment","metadata":{"id":"f_rPu8FBqqYp"}},{"cell_type":"code","source":"def comment_type(data):\n  data = [ data['severe_toxicity'], data['obscene'], data['identity_attack'], data['insult'], data['threat'] ]\n  data = np.argmax(data)\n  if data == 0: \n    return 'severe_toxicity'\n  elif data == 1:\n    return 'obscene'\n  elif data == 2:\n    return 'identity_attack'\n  elif data == 3:\n    return 'insult'\n  else:\n    return 'threat'","metadata":{"id":"fX8rBBUorPC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_train[df_train['target']>=0.5].apply(comment_type, axis=1)\n\ntotal = float(len(data))\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nsns.countplot(data)\nplt.title(\"Percentage of type of toxicity \")\nfor p in ax.patches:\n  height = p.get_height()\n  ax.text(p.get_x() + p.get_width()/2, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')","metadata":{"id":"nm44-YGZtRpr","outputId":"c518a4e3-a926-44b0-e6a3-0b55c65db7cf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from 8% toxic comment, 78% of the toxic comment made are insults, 6.75% are obscene, 10.62% are identity_attack, 3.73% are threat, 0.16% are severe toxicity.","metadata":{"id":"7QJilrf0ziAt"}},{"cell_type":"markdown","source":"**Gender attributes**\n\nmale\n\nfemale\n\nhomosexual_gay_or_lesbian\n\nbisexual\n\nheterosexual\n\nother_gender \n\ntransgender","metadata":{"id":"67oNVaAP0R6u"}},{"cell_type":"code","source":"features = ['male', 'female', 'homosexual_gay_or_lesbian', 'bisexual', 'heterosexual', 'other_gender', 'transgender']\ndistplot_feature(features, \"Distribution of gender features on toxic comment\", df_train[df_train['target'] > 0.5].dropna(how='any', axis=0))","metadata":{"id":"7CuGsKYC0pVP","outputId":"316247cf-560b-4aae-9b09-6489ecab3222"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = (df_train['male'] > 0.5) | (df_train['female'] > 0.5) | (df_train['homosexual_gay_or_lesbian'] > 0.5) | (df_train['bisexual'] > 0.5) | (df_train['heterosexual'] > 0.5) | (df_train['other_gender'] > 0.5) | (df_train['transgender'] > 0.5)\ndata = df_train[data]\ndata = data[data['target'] > 0.5]\ndata = data.apply(comment_type, axis=1)\n\ntotal = len(data)\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nsns.countplot(data)\nplt.title(\"Percentage of type of toxicity in comments gender reference made \")\nfor p in ax.patches:\n  height = p.get_height()\n  ax.text(p.get_x() + p.get_width()/2, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')","metadata":{"id":"ZGKbxIpn2IV6","outputId":"39d95665-d874-48fb-fb24-59c33726a915"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see from the plot that the toxic comments where sexual orientation references are made are mostly used for insult and identity attacks.","metadata":{"id":"_aGM9Wbc_dHI"}},{"cell_type":"code","source":"df_train.columns","metadata":{"id":"BRCQUbpY9l1X","outputId":"2db6ee2c-5bb5-45b3-80f4-159c90d335e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['hindu', 'jewish', 'latino', 'muslim', 'atheist', 'other_religion']\ndistplot_feature(features, \"Distriution of religion on toxic comment\", df_train[df_train['target'] > 0.5])","metadata":{"id":"7hzkwKdh6avH","outputId":"afba649e-9e1d-4643-eb48-1f3f5febde7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = (df_train['hindu'] > 0.5) | (df_train['jewish'] > 0.5) | (df_train['latino'] > 0.5) | (df_train['muslim'] > 0.5) | (df_train['atheist'] > 0.5) | (df_train['other_religion'] > 0.5) \ndata = df_train[data]\ndata = data[data['target'] > 0.5]\ndata = data.apply(comment_type, axis=1)\n\ntotal = len(data)\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nsns.countplot(data)\nplt.title(\"Percentage of type of toxicity in comments religion reference made \")\nfor p in ax.patches:\n  height = p.get_height()\n  ax.text(p.get_x() + p.get_width()/2, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')","metadata":{"id":"iBNXJxfE-UiS","outputId":"1bffe0e9-d183-4290-c302-b7ca6ad25f9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see from the plot that the toxic comments where religion references are made are mostly used for identity attacks and insults.","metadata":{"id":"Joy7yDkR_mbt"}},{"cell_type":"code","source":"","metadata":{"id":"rANj3qix_ID4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features generated by users feedback\n\n*   funny\n*   sad\n*   wow\n*   likes\n*   disagree","metadata":{"id":"WDsM6b0VsVP_"}},{"cell_type":"code","source":"def count_plot(feature, title, data):\n  data = data[feature]\n  fig, ax = plt.subplots(1, 1, figsize=(16, 5))\n  total = float(len(data))\n  sns.countplot(data, order=data.value_counts().index[:15])\n  for p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2, height+3, '{:1.2f}%'.format(100*height/total))","metadata":{"id":"pKZq8w8Nscsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot('funny', \"Percentage of funny votes given\", df_train)\ncount_plot('funny', \"Percentage of funny votes given on toxic comment\", df_train[df_train['target'] >= 0.5])","metadata":{"id":"bJHFOENrtwPv","outputId":"5e839873-c423-40ac-e9ea-7024d9000145"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot('sad', \"Percentage of sad votes given\", df_train)\ncount_plot('sad', \"Percentage of sad votes given on toxic comment\", df_train[df_train['target'] >= 0.5])","metadata":{"id":"8wFRWDyRuHFH","outputId":"335fe2ab-0d4e-47c3-8e1a-4c010b9597d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot('wow', \"Percentage of wow votes given\", df_train)\ncount_plot('wow', \"Percentage of wow votes given on toxic comment\", df_train[df_train['target'] >= 0.5])","metadata":{"id":"nuqeZAxbvLw_","outputId":"b68b288f-7e68-42de-d62f-417ae7c08a60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot('likes', \"Percentage of likes votes given\", df_train)\ncount_plot('likes', \"Percentage of likes votes given on toxic comment\", df_train[df_train['target'] >= 0.5])","metadata":{"id":"5Laqjy6PvSAB","outputId":"1e0d31f5-a155-4ced-ad71-3586475409a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot('disagree', \"Percentage of disagree votes given\", df_train)\ncount_plot('disagree', \"Percentage of disagree votes given on toxic comment\", df_train[df_train['target'] >= 0.5])","metadata":{"id":"sP3v0NtuvW5V","outputId":"d0213347-c095-4e07-afaa-afa10dd07a01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wordcloud of comment text**","metadata":{"id":"zsUME0dhvoT-"}},{"cell_type":"code","source":"def show_wordcloud(data, title=None):\n  wordcloud = WordCloud(\n      background_color = 'white',\n      stopwords = set(STOPWORDS),\n      max_words = 50,\n      scale = 5,\n      random_state = 1\n  ).generate(str(data))\n\n  fig = plt.figure(figsize=(10, 10))\n  plt.axis('off')\n  if title:\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n\n  plt.imshow(wordcloud)\n  plt.show()","metadata":{"id":"w9288drPvh8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df_train['comment_text'].sample(20000), title=\"Prevalent words in comment data\")","metadata":{"id":"P_8J8AabwiYT","outputId":"3ea17416-4390-49a5-dac3-43e6e1a5cc3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df_train[df_train['target'] > 0.75]['comment_text'].sample(20000), title=\"Prevalent words in comment data where target>0.75\")","metadata":{"id":"AGDBVTUqygjz","outputId":"0124b164-b112-4e9e-c6d7-f64534b62681"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df_train[df_train['target'] < 0.25]['comment_text'].sample(20000), title=\"Prevalent words in comment data where target<0.25\")","metadata":{"id":"hAGLp2lQzO-B","outputId":"c3619417-dd33-4138-e86d-446c80823b5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df_train[df_train['threat'] < 0.25]['comment_text'], title=\"Prevalent words in comment data where threat score<0.25\")\nshow_wordcloud(df_train[df_train['threat'] > 0.75]['comment_text'], title=\"Prevalent words in comment data where threat score>0.75\")","metadata":{"id":"jl6XnQFNz3sC","outputId":"c04aeb11-11bc-4e5b-f08c-9482a8ae1fe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df_train[df_train['insult'] < 0.25]['comment_text'], title=\"Prevalent words in comment data where insult score<0.25\")\nshow_wordcloud(df_train[df_train['insult'] > 0.75]['comment_text'], title=\"Prevalent words in comment data where insult score>0.75\")","metadata":{"id":"vTNULUEG0KEO","outputId":"68e08233-71b5-4244-b568-777c022953f1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess text**","metadata":{"id":"Lf7bo7AU1KnK"}},{"cell_type":"code","source":"nltk.download('stopwords')\nstemmer = SnowballStemmer(\"english\")\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(str):\n  str = str.lower()\n  str = re.sub('[^A-Za-z0-9]+', ' ', str)\n  words = str.split()\n  new_str = []\n  for word in words:\n    if word not in stop_words:\n      new_str.append(stemmer.stem(word))\n  new_str = ' '.join(new_str)\n  return new_str\n","metadata":{"id":"f_yA4ovI0ut2","outputId":"32f56d4d-33a3-4b9f-fc31-209a3140a9b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train['preprocessed_text'] = df_train['comment_text'].apply(preprocess)","metadata":{"id":"lxRNeYcz2q5z","outputId":"7c4d723b-f738-402a-e872-a7d51f587297"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"6pMsO_5Z4ydY","outputId":"964454fc-ab69-44e2-c140-45b772039ec1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_test['preprocessed_text'] = df_test['comment_text'].apply(preprocess)","metadata":{"id":"orCLSIaH45Th","outputId":"b9b420af-738e-4998-8b2a-f3aeb5a6a767"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"id":"y_y6En2Y5H0p","outputId":"d8e896af-02fe-485e-9998-0e1746118d16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message = df_train['preprocessed_text']\ntarget = df_train['target'] \n\ntrain_message, val_message, train_target , val_target = train_test_split(message, target, test_size=0.1)\n\nprint(\"train_message : \", train_message.shape)\nprint(\"train_target : \", train_target.shape)\nprint(\"val_message : \", val_message.shape)\nprint(\"val_target : \", val_target.shape)","metadata":{"id":"QN0FW8TY5Jgu","outputId":"3c50bfdb-000c-4e31-d574-fe8db44e70e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_message = df_test['preprocessed_text']\n\nprint(\"test_message : \", test_message.shape)","metadata":{"id":"p2LR_w7p7TEp","outputId":"9330beda-0ddb-4f22-b935-069a135129b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_message.to_pickle('train_message.pkl')\n# train_target.to_pickle('train_target.pkl')\n# val_message.to_pickle('val_message.pkl')\n# val_target.to_pickle('val_target.pkl')\n# test_message.to_pickle('test_message.pkl')\n\n# !cp '/content/train_message.pkl' '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment'\n# !cp '/content/train_target.pkl' '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment'\n# !cp '/content/val_message.pkl' '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment'\n# !cp '/content/val_target.pkl' '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment'\n# !cp '/content/test_message.pkl' '/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment'","metadata":{"id":"ESd4E214Aewy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_message = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/train_message.pkl')\n# train_target = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/train_target.pkl')\n# val_message = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/val_message.pkl')\n# val_target = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/val_target.pkl')\n# test_message = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/datasets/Toxic Comment/test_message.pkl')","metadata":{"id":"UfWlbIf75fSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count Vectorizer**","metadata":{"id":"mSFwjVad_HlD"}},{"cell_type":"code","source":"%%time\ncv = CountVectorizer(max_features = 30000, ngram_range =(1, 2) )\ntrain_message_count = cv.fit_transform(train_message)\nval_message_count = cv.transform(val_message)\ntest_message_count = cv.transform(test_message)","metadata":{"id":"avWScuvN9N2m","outputId":"1bd60b43-6902-4928-b747-08bd7b59a755"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\npenalty = ['l1', 'l2']\nxticks = []\ntrain_errors = []\nval_errors = []\nbest_model = None\nbest_error = 100\nfor a in alpha:\n    for p in penalty:\n        xticks.append(str(a) + ' ' + p)\n        print(str(a) + ' ' + p + \" :\")\n        \n        model = SGDRegressor(alpha=a, penalty=p) \n        model.fit(train_message_count, train_target) \n        \n        preds = model.predict(train_message_count) \n        err = mean_squared_error(train_target, preds) \n        train_errors.append(err)\n        print(\"Mean Squared Error on train set: \", err)\n        \n        preds = model.predict(val_message_count) \n        err = mean_squared_error(val_target, preds) \n        val_errors.append(err)\n        print(\"Mean Squared Error on cv set: \", err)\n        \n        if err < best_error: \n            best_error = err\n            best_model = model\n        \n        print(\"*\"*20)","metadata":{"id":"pMDSlmed_fTs","outputId":"4f564256-02bd-4b12-92be-b606fa64b565"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(range(len(alpha) * len(penalty) ), train_errors)\nplt.plot(range(len(alpha) * len(penalty)), val_errors)\nplt.suptitle(\"Mean squared error vs Hyper parameter\")\nplt.legend(['train', 'val'])\nplt.xticks(range(len(alpha) * len(penalty)), xticks, rotation=45)\nplt.xlabel('Hyper parameter(alpha + penalty )')\nplt.ylabel(\"Mean squared error \")\nplt.show()","metadata":{"id":"fCD-Sr3M7kM5","outputId":"010bebaf-50d5-415c-d7a5-4c4a68ab44a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_names = cv.get_feature_names()\nweights = best_model.coef_\ndf = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\ndf.sort_values(\"weights\", ascending=False).iloc[0:20,:]","metadata":{"id":"CkeoyMVH-dX0","outputId":"68c3e259-1da3-4457-a535-9de466975d57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_depth = [3, 5, 7]\nmin_samp = [10, 100, 500, 1000]\ntrain_errors = []\nval_errors = []\n\nbest_model = None\nbest_error = 10\n\nfor d in max_depth:\n  for s in min_samp:\n    dt = DecisionTreeRegressor(max_depth = d, min_samples_leaf = s)\n    dt.fit(train_message_count, train_target)\n\n    pred = dt.predict(train_message_count) \n    print(\"max_depth : \", d, \"  min_samples : \", s)\n    error = mean_squared_error(pred, train_target)\n    print(\"Train mse : \", error)\n    train_errors.append(error)\n\n    pred = dt.predict(val_message_count) \n    error = mean_squared_error(pred, val_target)\n    print(\"val mse : \", error)\n    val_errors.append(error)\n\n    if error < best_error:\n      best_model = dt\n      best_error = error\n      print('*'*30)","metadata":{"id":"g99BOlon_4jH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(range(len(max_depth) * len(min_samp) ), train_errors)\nplt.plot(range(len(max_depth) * len(min_samp)), val_errors)\nplt.suptitle(\"Mean squared error vs Hyper parameter\")\nplt.legend(['train', 'error'])\nplt.xlabel('Hyper parameter(max_depth + min_samples )')\nplt.ylabel(\"Mean squared error \")\nplt.show()","metadata":{"id":"umqXOYweCFtl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_names = cv.get_feature_names()\nweights = best_model.feature_importances_\ndf = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\ndf.sort_values(\"weights\", ascending=False).iloc[0:20,:]","metadata":{"id":"GT08ddSGDQPc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TfIdf vectorizer","metadata":{"id":"Pb7_JbFnTZVD"}},{"cell_type":"code","source":"%%time\ntfidf = TfidfVectorizer(max_features = 30000, ngram_range =(1, 2) )\ntrain_message_tfidf = tfidf.fit_transform(train_message)\nval_message_tfidf = tfidf.transform(val_message)\ntest_message_tfidf = tfidf.transform(test_message)","metadata":{"id":"FksZRwJcDRX1","outputId":"fea90565-22b1-44e4-d43a-417ef45d5a88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\npenalty = ['l1', 'l2']\nxticks = []\ntrain_errors = []\nval_errors = []\nbest_model = None\nbest_error = 100\nfor a in alpha:\n    for p in penalty:\n        xticks.append(str(a) + ' ' + p)\n        print(str(a) + ' ' + p + \" :\")\n        \n        model = SGDRegressor(alpha=a, penalty=p) \n        model.fit(train_message_tfidf, train_target) # Train\n        \n        preds = model.predict(train_message_tfidf) # Get predictions\n        err = mean_squared_error(train_target, preds) # Calculate error on trainset\n        train_errors.append(err)\n        print(\"Mean Squared Error on train set: \", err)\n        \n        preds = model.predict(val_message_tfidf) # Get predictions on CV set\n        err = mean_squared_error(val_target, preds) # Calculate error on cv set\n        val_errors.append(err)\n        print(\"Mean Squared Error on cv set: \", err)\n        \n        if err < best_error: # Get best model trained\n            best_error = err\n            best_model = model\n        \n        print(\"*\"*20)","metadata":{"id":"OfBGMs2vP-8t","outputId":"cd8da67c-4645-4392-c995-17ef49c22c7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(range(len(alpha) * len(penalty) ), train_errors)\nplt.plot(range(len(alpha) * len(penalty)), val_errors)\nplt.suptitle(\"Mean squared error vs Hyper parameter\")\nplt.legend(['train', 'error'])\nplt.xlabel('Hyper parameter(alpha + penalty )')\nplt.ylabel(\"Mean squared error \")\nplt.show()","metadata":{"id":"CKOYK6ANYo74","outputId":"fcf51a4f-dfe4-4d1b-912d-ebceb066204a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_names = tfidf.get_feature_names()\nweights = best_model.coef_\ndf = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\ndf.sort_values(\"weights\", ascending=False).iloc[0:20,:]","metadata":{"id":"aSen1U1X-R3D","outputId":"5ff9578e-9783-444f-9faa-aeccf666c8c9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision tree on tfidf","metadata":{"id":"UhMhROmQDa7F"}},{"cell_type":"code","source":"max_depth = [3, 5, 7]\nmin_samp = [10, 100, 1000]\ntrain_errors = []\nval_errors = []\n\nbest_model = None\nbest_error = 10\n\nfor d in max_depth:\n  for s in min_samp:\n    dt = DecisionTreeRegressor(max_depth = d, min_samples_leaf = s)\n    dt.fit(train_message_tfidf, train_target)\n\n    pred = dt.predict(train_message_tfidf) \n    print(\"max_depth : \", d, \"  min_samples : \", s)\n    error = mean_squared_error(pred, train_target)\n    print(\"Train mse : \", error)\n    train_errors.append(error)\n\n    pred = dt.predict(val_message_tfidf) \n    error = mean_squared_error(pred, val_target)\n    print(\"val mse : \", error)\n    val_errors.append(error)\n\n    if error < best_error:\n      best_model = dt\n      best_error = error\n    print('*'*30)","metadata":{"id":"N20isFfu_jkB","outputId":"6e15d2a9-9abb-467f-e884-8384f70d8e00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nplt.plot(range(len(max_depth) * len(min_samp) ), train_errors)\nplt.plot(range(len(max_depth) * len(min_samp)), val_errors)\nplt.suptitle(\"Mean squared error vs Hyper parameter\")\nplt.legend(['train', 'error'])\nplt.xlabel('Hyper parameter(max_depth + min_samples )')\nplt.ylabel(\"Mean squared error \")\nplt.show()","metadata":{"id":"NWJViSeiDryP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_names = cv.get_feature_names()\nweights = best_model.feature_importances_\ndf = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\ndf.sort_values(\"weights\", ascending=False).iloc[0:20,:]","metadata":{"id":"Q1GoYq64EfI9"},"execution_count":null,"outputs":[]}]}