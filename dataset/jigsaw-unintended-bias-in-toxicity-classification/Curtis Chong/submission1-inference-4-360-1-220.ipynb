{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/privatepytorchpretrainedbert/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nimport math\nimport gc\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            first = int(math.ceil(max_seq_length * 0.25)) #first 25%\n            last = max_seq_length - first\n            print(first)\n            print(last)\n            tokens_first = tokens_a[:first]\n            tokens_last = tokens_a[-last:]\n            tokens_a = tokens_first + tokens_last\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n       \n        all_tokens.append(one_token)\n    print(\"Number of sequences longer: \", longer)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"all_preds = []\n\nmodel_weights = [\"../input/bestbert-360-e1/bestbert_360_len1.bin\",\n                 \"../input/bestbert-360-m2-e1/bestbert_360_2_len1.bin\",\n                 \"../input/bert-360-lin-inc-lr-m2-e1/bestbert_360_lin_dec_m2_1.bin\",\n                 \"../input/bert360-lin-dec-e1-m1/bert360_lin_dec_pytorch1.bin\",\n                 \"../input/bert-2-epoch/bert_pytorch_2_epoch/bert_pytorch.bin\"]\n\nmodel_paths = [\"../input/bertpretraineduncasedmodel/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12\",\n               \"../input/bertpretraineduncasedmodel/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12\",\n               \"../input/bertpretraineduncasedmodel/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12\",\n               \"../input/bertpretraineduncasedmodel/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12\",\n               \"../input/bertpretraineduncasedmodel/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12\"]\n\nmodel_configs = ['../input/cicero-bert-config/cicero_bert_config.json',\n                 '../input/cicero-bert-config/cicero_bert_config.json',\n                 '../input/cicero-bert-config/cicero_bert_config.json',\n                 '../input/cicero-bert-config/cicero_bert_config.json',\n                 '../input/cicero-bert-config/cicero_bert_config.json']\n\nmax_seq_lengths = [360, 360, 360, 360, 220]\n\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str)\n\nfor idx in range(len(model_weights)):\n    \n    MAX_SEQUENCE_LENGTH = max_seq_lengths[idx]\n    BATCH_SIZE = 32\n    BERT_MODEL_PATH = model_paths[idx]\n\n    bert_config = BertConfig(model_configs[idx])\n    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n    \n    X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n    \n    \n    \n    model = BertForSequenceClassification(bert_config, num_labels=7)\n    model.load_state_dict(torch.load(model_weights[idx]))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    \n    test_preds = np.zeros((len(X_test)))\n    test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n    tk0 = tqdm(test_loader)\n    for i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\n    all_preds.append(torch.sigmoid(torch.tensor(test_preds)).numpy().ravel())\n    \n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': np.mean(all_preds, axis=0)\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}