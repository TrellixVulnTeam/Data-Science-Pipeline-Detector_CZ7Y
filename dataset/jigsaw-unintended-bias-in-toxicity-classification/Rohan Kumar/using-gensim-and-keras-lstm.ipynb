{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Bidirectional, Flatten, Activation, Embedding, Concatenate, Input, Dense, Dropout, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Conv1D, MaxPool1D, Embedding,BatchNormalization, LSTM,merge, Conv2D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import Sequential, Model\nimport keras.utils as ku \nimport numpy as np\nfrom keras import regularizers\nimport pickle as pkl\ntokenizer = Tokenizer()\nfrom html import unescape\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom nltk.corpus import stopwords\n#import swifter\nstop=set(stopwords.words('english'))\nstem=SnowballStemmer('english')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/train.csv')#, sep='\\t')\ndf_un=pd.read_csv('../input/test.csv')#, sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bin'] = df.target>=0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def re_clean(text):\n    #c.append('')\n    #if len(c)%1000==0:\n        #pass\n        #print(len(c))\n    text=text.lower()\n    symbol = \"\"\"!#$%^&*();:\\t\\\\\\\"!\\{\\}\\[\\]<>-\\?\\-\\\\\\\"â€”\\.,1234567890\"\"\"\n    text=re.sub(\"\\'ll\", ' will', text)\n    text=re.sub(\"\\'ve\", ' have', text)\n    text=re.sub(\"\\'s\", ' is', text)\n    text=re.sub('[{}]'.format(symbol),' ', text)\n    #text=re.sub('[\\W]',' ', text)\n    text=re.sub('\\n',' ', text)\n    text=re.sub(' +',' ', text)\n    text=re.sub('^\\s', '', text)\n    #text=' '.join([stem.stem(i) for i in text.split() if not i in stop])\n    #text=[i for i in text.split() if i not in stop]\n    return text #' '.join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['text']=df.comment_text.apply(re_clean)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_un['text']=df_un.comment_text.apply(re_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#tok_corp = list(df.text)+list(df_un.text)\ntok_corp= [[i for i in sent.split() if i] for sent in list(df.text)+list(df_un.text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lens = [len(doc) for doc in tok_corp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m= max(lens)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\nimport gensim\nmodel300=gensim.models.Word2Vec(tok_corp, min_count=5 ,window=5,size = 300, sg=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tok_corp\ndel lens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model300.save('w2v_news300.word2vec')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer.fit_on_texts(list(df.text)+list(df_un.text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train_seq_trunc = tokenizer.texts_to_sequences(df.text)\nX_test_seq_trunc = tokenizer.texts_to_sequences(df_un.text)\nX_train_seq_trunc = pad_sequences(X_train_seq_trunc, maxlen=m, truncating='post', padding='post')\nX_test_seq_trunc = pad_sequences(X_test_seq_trunc, maxlen=m, truncating='post', padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny_train = df.bin\nX_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=37)\n\nassert X_valid_emb.shape[0] == y_valid_emb.shape[0]\nassert X_train_emb.shape[0] == y_train_emb.shape[0]\n\nprint('Shape of validation set:',X_valid_emb.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train_seq_trunc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_WORDS=len(tokenizer.word_index)\nGLOVE_DIM=300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_matrix = np.zeros((NB_WORDS+1, GLOVE_DIM))\n\nfor w, i in tokenizer.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < NB_WORDS:\n        try:\n            vect = model300.wv.get_vector(w)\n            emb_matrix[i] = vect\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        except:\n            pass\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_filters = 32\nfilter_sizes = [3,4,5]\ndropout = 0.5\nembedding_dim = 300\nvocabulary_size = NB_WORDS\nsequence_length = m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropout=0.2\ninp1=Input((m,))\nemb=Embedding(NB_WORDS+1,300, weights=[emb_matrix],trainable=False)(inp1)\nlstm1=Bidirectional(LSTM(128, return_sequences=True))(emb)\ndrop1=Dropout(dropout)(lstm1)\nlstm2=Bidirectional(LSTM(128, return_sequences=True))(drop1)\ndrop2=Dropout(dropout)(lstm2)\nlstm2=Bidirectional(LSTM(128, return_sequences=False))(drop2)\n#dense_lstm=Dense(128, activation='relu')(lstm2)\n#reshape = Reshape((sequence_length,embedding_dim,1))(emb)\n#conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n#conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n#conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n#maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n#maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n#maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n#concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])#, maxpool_3])\n#flatten = Flatten()(concatenated_tensor)\n#dense_con=Dense(128, activation='relu')(flatten)\n#conc=conc=merge.multiply(([dense_con, dense_lstm]))\nbatch1=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n                              gamma_initializer='ones', moving_mean_initializer='zeros', \n                              moving_variance_initializer='ones', beta_regularizer=None,\n                              gamma_regularizer=None, beta_constraint=None,\n                              gamma_constraint=None)(lstm2)\ndense3=Dense(128, activation='relu')(batch1)\ndrop4=Dropout(0.5)(dense3)\ndense4=Dense(128, activation='relu')(drop4)\n\"\"\"batch2=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n                          gamma_initializer='ones', moving_mean_initializer='zeros', \n                          moving_variance_initializer='ones', beta_regularizer=None,\n                          gamma_regularizer=None, beta_constraint=None,\n                          gamma_constraint=None)(dense4)\"\"\"\n#conc=Concatenate()([dense7, out1])\ndrop5=Dropout(0.5)(dense4)\ndense7=Dense(128, activation='relu')(drop5)\nout=Dense(1, activation='sigmoid')(dense7)\nmodel= Model(inp1, out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\ncheckpoint = ModelCheckpoint('jigsaw.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\nmodel.compile(loss='binary_crossentropy', optimizer=adam,\n              metrics=['accuracy'], )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train_emb\n                       , y_train_emb\n                       , epochs=10\n                       , batch_size=1024\n                       , validation_data=(X_valid_emb, y_valid_emb)\n                       , verbose=1, callbacks=[checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=load_model('jigsaw.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre=model.predict(X_test_seq_trunc, verbose=1)\ndf3=pd.DataFrame(pre, index=df_un.id, columns=['prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}