{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json, string, re, random, pickle, gc, operator, time, sys\nfrom contextlib import contextmanager\nfrom collections import Counter\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader, Sampler\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nfrom keras_preprocessing.text import text_to_word_sequence\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-08T06:37:15.52689Z","iopub.execute_input":"2021-12-08T06:37:15.527191Z","iopub.status.idle":"2021-12-08T06:37:15.548248Z","shell.execute_reply.started":"2021-12-08T06:37:15.527141Z","shell.execute_reply":"2021-12-08T06:37:15.547117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.197592Z","iopub.execute_input":"2021-12-07T23:52:31.198079Z","iopub.status.idle":"2021-12-07T23:52:31.203835Z","shell.execute_reply.started":"2021-12-07T23:52:31.198028Z","shell.execute_reply":"2021-12-07T23:52:31.202772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def manual_seed(seed=420, cuda=False):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nmanual_seed(cuda=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.205455Z","iopub.execute_input":"2021-12-07T23:52:31.205953Z","iopub.status.idle":"2021-12-07T23:52:31.21695Z","shell.execute_reply.started":"2021-12-07T23:52:31.205756Z","shell.execute_reply":"2021-12-07T23:52:31.216214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(msg):\n    start = time.time()\n    print(f'[{msg}] start...')\n    yield\n    elapsed = time.time() - start\n    hours, rem = divmod(elapsed, 3600)\n    minutes, seconds = divmod(rem, 60)\n    elapsed_str = \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n    print(f'[{msg}] done in {elapsed_str}.')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.218557Z","iopub.execute_input":"2021-12-07T23:52:31.219543Z","iopub.status.idle":"2021-12-07T23:52:31.22814Z","shell.execute_reply.started":"2021-12-07T23:52:31.218826Z","shell.execute_reply":"2021-12-07T23:52:31.227148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_VOCAB_SIZE = 30520\nMAX_LEN = 256\nTARGET_COLUMNS = ['target'] #, 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.231827Z","iopub.execute_input":"2021-12-07T23:52:31.232171Z","iopub.status.idle":"2021-12-07T23:52:31.238628Z","shell.execute_reply.started":"2021-12-07T23:52:31.232097Z","shell.execute_reply":"2021-12-07T23:52:31.23774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"identity_columns = ['asian', 'atheist',\n       'bisexual', 'black', 'buddhist', 'christian', 'female',\n       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n       'muslim', 'other_disability', 'other_gender',\n       'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.240538Z","iopub.execute_input":"2021-12-07T23:52:31.241236Z","iopub.status.idle":"2021-12-07T23:52:31.247441Z","shell.execute_reply.started":"2021-12-07T23:52:31.241174Z","shell.execute_reply":"2021-12-07T23:52:31.246734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_characters = {\n    \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", \"…\": \"...\", \"&\": \" and \", \"“\": '\"', \"”\": '\"',\n    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\", \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",\n    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \n    \"ᴀ\": \"a\", \"ʙ\": \"b\", \"ᴄ\": \"c\", \"ᴅ\": \"d\", \"ᴇ\": \"e\", \"ғ\": \"f\", \"ɢ\": \"g\", \"ʜ\": \"h\", \"ɪ\": \"i\", \n    \"ᴊ\": \"j\", \"ᴋ\": \"k\", \"ʟ\": \"l\", \"ᴍ\": \"m\", \"ɴ\": \"n\", \"ᴏ\": \"o\", \"ᴘ\": \"p\", \"ǫ\": \"q\", \"ʀ\": \"r\", \n    \"s\": \"s\", \"ᴛ\": \"t\", \"ᴜ\": \"u\", \"ᴠ\": \"v\", \"ᴡ\": \"w\", \"x\": \"x\", \"ʏ\": \"y\", \"ᴢ\": \"z\"\n}\ncontractions = json.load(open('../input/english-contractions/contractions.json', 'r'))\ncontractions = {key.lower():value.lower() for key, value in contractions.items()}\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\nspecial_characters_re = re.compile('({})'.format('|'.join(special_characters.keys())))\nspecial_characters_map = lambda match: special_characters[match.group(0)]\ncontractions_re = re.compile('({})'.format('|'.join(contractions.keys())))\ncontractions_map = lambda match: contractions[match.group(0)]\npunct_table = str.maketrans(punct, ' '*len(punct))\n\ndef tokenize(text):\n    text = text.lower()\n    text = special_characters_re.sub(special_characters_map, text)\n    text = contractions_re.sub(contractions_map, text)\n    text = text.translate(punct_table)\n    tokens = text_to_word_sequence(text, lower=False)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:31.249121Z","iopub.execute_input":"2021-12-07T23:52:31.24971Z","iopub.status.idle":"2021-12-07T23:52:31.280334Z","shell.execute_reply.started":"2021-12-07T23:52:31.24966Z","shell.execute_reply":"2021-12-07T23:52:31.279699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('Process train.csv'):\n    print(\"Loading File...\")\n    train_df = pd.read_csv('../input/text-augmentation/augmented_train.csv')\n    train_df2 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n    print(\"Filling NaNs with 0s\")\n    train_df.fillna(0.0, inplace=True)\n    print(f'Training data size: {len(train_df)}')\n    print(\"Extracting Data...\")\n    train_text, train_target = train_df['comment_text_augmented'].values, torch.from_numpy(train_df[TARGET_COLUMNS].astype(np.float32).values)\n    #train_target = torch.where(train_target >= 0.5, torch.ones_like(train_target), torch.zeros_like(train_target))\n    train_has_identities = (train_df[identity_columns].values >= 0.5).any(axis=1)\n    train_is_toxic = (train_df['target'].values >= 0.5)\n    train_weights = (1 + ~train_is_toxic) * train_has_identities + train_is_toxic * ~train_has_identities + 1\n    train_weights = train_weights.astype(np.float32) / train_weights.mean()\n    train_weights = torch.from_numpy(train_weights)\n    del train_df, train_has_identities, train_is_toxic\n    gc.collect()\n    print(\"Tokenizing...\")\n    for index, comment in enumerate(tqdm(train_text)):\n        train_text[index] = tokenize(comment)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:52:41.552263Z","iopub.execute_input":"2021-12-07T23:52:41.552591Z","iopub.status.idle":"2021-12-07T23:57:30.520342Z","shell.execute_reply.started":"2021-12-07T23:52:41.552535Z","shell.execute_reply":"2021-12-07T23:57:30.519538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('Process test.csv'):\n    print(\"Loading File...\")\n    test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    test_df2 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    print(f'Testing data size: {len(test_df)}')\n    print(\"Extracting Data...\")\n    test_ids, test_text = torch.from_numpy(test_df2['id'].astype(np.int32).values), test_df['comment_text'].values\n    del test_df\n    gc.collect()\n    print(\"Tokenizing...\")\n    for index, comment in enumerate(tqdm(test_text)):\n        test_text[index] = tokenize(comment)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:57:30.522176Z","iopub.execute_input":"2021-12-07T23:57:30.522638Z","iopub.status.idle":"2021-12-07T23:57:48.111084Z","shell.execute_reply.started":"2021-12-07T23:57:30.522586Z","shell.execute_reply":"2021-12-07T23:57:48.11025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('Calculating Lengths'):\n    oversized = 0\n    print('Trimming')\n    for index, comment in enumerate(train_text):\n        if len(comment) > MAX_LEN:\n            train_text[index] = comment[:MAX_LEN]\n            oversized += 1\n    train_lengths = torch.tensor([len(comment) for comment in train_text], dtype=torch.int16)\n    for index, comment in enumerate(test_text):\n        if len(comment) > MAX_LEN:\n            test_text[index] = comment[:MAX_LEN]\n            oversized += 1\n    test_lengths = torch.tensor([len(comment) for comment in test_text], dtype=torch.int16)\n    print(f'{oversized} comment(s) ({oversized*100/(len(train_text)+len(test_text))}%) are longers than {MAX_LEN}')\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:57:48.112411Z","iopub.execute_input":"2021-12-07T23:57:48.11271Z","iopub.status.idle":"2021-12-07T23:57:51.465681Z","shell.execute_reply.started":"2021-12-07T23:57:48.112662Z","shell.execute_reply":"2021-12-07T23:57:51.464863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('Indexing tokens'):\n    vocab = Counter()\n    print('Counting training tokens...')\n    vocab.update(token for comment in tqdm(train_text) for token in comment)\n    print('Counting testing tokens...')\n    vocab.update(token for comment in tqdm(test_text) for token in comment)\n    print(f'Full vocabulary size is {len(vocab)} covering {sum(vocab.values())} tokens.')\n    print('Top 20 words:', vocab.most_common(20))\n    top_words, top_freq = zip(*vocab.most_common(min(len(vocab),MAX_VOCAB_SIZE)))\n    print(f'Top-{len(top_words)} covers {sum(top_freq)*100/sum(vocab.values())}%.')\n    del vocab\n    del top_freq\n    print(\"Building token index...\")\n    token2index = {token:index for index, token in enumerate(['<PAD>', '<UNK>'] + list(top_words))}\n    del top_words\n    gc.collect()\n    print(\"Indexing training data...\")\n    train_input = torch.zeros(len(train_text), MAX_LEN, dtype=torch.int32)\n    for index, comment in enumerate(tqdm(train_text)):\n        train_input[index,:len(comment)] = torch.tensor([token2index.get(token,1) for token in comment], dtype=torch.int32) \n    del train_text\n    gc.collect()\n    print(\"Indexing testing data...\")\n    test_input = torch.zeros(len(test_text), MAX_LEN, dtype=torch.int32)\n    for index, comment in enumerate(tqdm(test_text)):\n        test_input[index,:len(comment)] = torch.tensor([token2index.get(token,1) for token in comment], dtype=torch.int32) \n    del test_text\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:57:51.467603Z","iopub.execute_input":"2021-12-07T23:57:51.467938Z","iopub.status.idle":"2021-12-07T23:59:19.65438Z","shell.execute_reply.started":"2021-12-07T23:57:51.467892Z","shell.execute_reply":"2021-12-07T23:59:19.653637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('Resolving zero length elements'):\n    zero_mask = (train_lengths == 0)\n    print(f'found {zero_mask.sum().item()} zero length elements in training data')\n    train_lengths[zero_mask] = 1\n    train_input[zero_mask, 0] = 1\n    zero_mask = (test_lengths == 0)\n    print(f'found {zero_mask.sum().item()} zero length elements in testing data')\n    test_lengths[zero_mask] = 1\n    test_input[zero_mask, 0] = 1","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:59:19.660591Z","iopub.execute_input":"2021-12-07T23:59:19.662757Z","iopub.status.idle":"2021-12-07T23:59:19.722168Z","shell.execute_reply.started":"2021-12-07T23:59:19.662705Z","shell.execute_reply":"2021-12-07T23:59:19.72146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_files = [\n    {'file': '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', 'size': 300},\n    {'file': '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'size': 300}\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:59:19.726328Z","iopub.execute_input":"2021-12-07T23:59:19.728523Z","iopub.status.idle":"2021-12-07T23:59:19.734747Z","shell.execute_reply.started":"2021-12-07T23:59:19.728452Z","shell.execute_reply":"2021-12-07T23:59:19.733951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vector_size = sum(file['size'] for file in embedding_files)\nword_embeddings = torch.empty(len(token2index), vector_size, dtype=torch.float32)\nword_embeddings[0] = 0\nstart = 0\nfor file in embedding_files:\n    with timer(f'Loading {file[\"file\"]}'):\n        w2v = pickle.load(open(file[\"file\"], 'rb'))\n    size = file['size']\n    end = start + size\n    unk_gen = lambda: F.normalize(torch.randn(size), p=2, dim=0)\n    word_embeddings[1, start:end] = unk_gen()\n    not_found = []\n    for token, index in tqdm(token2index.items()):\n        if index < 2: continue\n        try:\n            word_embeddings[index, start:end] = torch.from_numpy(w2v[token])\n        except KeyError:\n            word_embeddings[index, start:end] = unk_gen()\n            not_found.append((index, token))\n    print(f'Could not find vectors for {len(not_found)} token(s) ({len(not_found)*100/(len(token2index)-2)}%)')\n    not_found.sort()\n    print('Top-10 not found words:\\n'+'\\n'.join(f'{str(index+1)}- {token}' for index, token in not_found[:10]))\n    del w2v\n    del not_found\n    gc.collect()\n    start = end\nprint(word_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T23:59:19.739899Z","iopub.execute_input":"2021-12-07T23:59:19.742576Z","iopub.status.idle":"2021-12-08T00:00:08.484577Z","shell.execute_reply.started":"2021-12-07T23:59:19.742453Z","shell.execute_reply":"2021-12-08T00:00:08.483896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we try an alternative way for embedding\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm, trange\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport gc\n'''\nBERT_MODEL = 'bert-base-uncased'\nCASED = 'uncased' in BERT_MODEL\nINPUT = '../input/jigsaw-bert-preprocessed-input/'\nTEXT_COL = 'comment_text'\nMAXLEN = 250\nos.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\nBERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'\ndef get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_FP)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat\nembedding_matrix = get_bert_embed_matrix()'''\n#embedding_matrix = torch.from_numpy(embedding_matrix)\n#print(embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:48:28.891947Z","iopub.execute_input":"2021-12-07T07:48:28.892333Z","iopub.status.idle":"2021-12-07T07:48:28.900891Z","shell.execute_reply.started":"2021-12-07T07:48:28.892275Z","shell.execute_reply":"2021-12-07T07:48:28.899952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LengthSortedBatchSampler(Sampler):\n    def __init__(self, lengths, batch_size):\n        self.lengths = lengths\n        self.batch_size = batch_size\n    \n    def __iter__(self):\n        shuffled_indices = torch.randperm(self.lengths.size(0))\n        shuffled_lengths = self.lengths[shuffled_indices]\n        shuffled_lengths, sorted_indices = shuffled_lengths.sort()\n        shuffled_indices = shuffled_indices[sorted_indices]\n        batches = self.batch_size * torch.randperm(len(self))\n        for batch_start in batches:\n            yield shuffled_indices[batch_start:batch_start+self.batch_size]\n\n    def __len__(self):\n        return (len(self.lengths) + self.batch_size - 1) // self.batch_size","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:08.485953Z","iopub.execute_input":"2021-12-08T00:00:08.486244Z","iopub.status.idle":"2021-12-08T00:00:08.49499Z","shell.execute_reply.started":"2021-12-08T00:00:08.486194Z","shell.execute_reply":"2021-12-08T00:00:08.494109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 512\ntrain_dataset = TensorDataset(train_lengths, train_input, train_target, train_weights)\ntrain_dataloader = DataLoader(train_dataset, batch_sampler=LengthSortedBatchSampler(train_lengths, BATCH_SIZE))\ntest_dataset = TensorDataset(test_lengths, test_input)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:08.496358Z","iopub.execute_input":"2021-12-08T00:00:08.496909Z","iopub.status.idle":"2021-12-08T00:00:08.509284Z","shell.execute_reply.started":"2021-12-08T00:00:08.496857Z","shell.execute_reply":"2021-12-08T00:00:08.508609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, embeddings, hidden_size=128, layer_count=2, linear_layer_count=2, num_outputs=len(TARGET_COLUMNS)):\n        super(Model, self).__init__()\n        #vocab_size, emb_dim = embeddings.shape\n        vocab_size, emb_dim = embeddings.size()\n        self.embed = nn.Embedding.from_pretrained(embeddings)\n        self.embed_dropout = SpatialDropout(0.2)\n        self.gru = nn.GRU(emb_dim, hidden_size, num_layers=layer_count, batch_first=True, bidirectional=True)\n        feats_size = 2*2*hidden_size # bidirectional * (mean+max) * hidden_size\n        self.linear_layers = nn.ModuleList([nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feats_size, feats_size),\n            nn.PReLU()\n        ) for _ in range(linear_layer_count)])\n        self.head = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feats_size, num_outputs)\n        )\n\n    def forward(self, sentences, lengths):\n        embeddings = self.embed(sentences)\n        embeddings = self.embed_dropout(embeddings)\n        packed_input = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, h = self.gru(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n        output_max, _ = output.max(dim=1)\n        output_mean = output.sum(dim=1)/(lengths.unsqueeze(1).float()+(1e-8))\n        feats = torch.cat([output_max, output_mean], dim=1)\n        for layer in self.linear_layers:\n            feats = feats + layer(feats)\n        return self.head(feats)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:08.511112Z","iopub.execute_input":"2021-12-08T00:00:08.511783Z","iopub.status.idle":"2021-12-08T00:00:08.530034Z","shell.execute_reply.started":"2021-12-08T00:00:08.511419Z","shell.execute_reply":"2021-12-08T00:00:08.529197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make 1st target more important\ntarget_weights = torch.ones(len(TARGET_COLUMNS), dtype=torch.float32)\ntarget_weights[0] = len(TARGET_COLUMNS)+1\ntarget_weights /= 2\nprint(target_weights)\ntarget_weights = target_weights.cuda()\n\ndef loss_fn(predicted, target, weights):\n    return ((F.binary_cross_entropy_with_logits(predicted, target, reduction='none') * weights.unsqueeze(1)) * target_weights).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:08.533959Z","iopub.execute_input":"2021-12-08T00:00:08.534288Z","iopub.status.idle":"2021-12-08T00:00:12.022827Z","shell.execute_reply.started":"2021-12-08T00:00:08.534235Z","shell.execute_reply":"2021-12-08T00:00:12.021942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_one_epoch(model, optimizer, epoch):\n    model.train()\n    interactive = is_interactive()\n    epoch_loss = 0\n    batch_it = tqdm(train_dataloader, desc=\"Epoch {}/{}\".format(epoch, num_epochs))\n    for batch_lengths, batch_sentences, batch_target, batch_weights in batch_it:\n        optimizer.zero_grad()\n        predicted = model(batch_sentences.cuda().long(), batch_lengths.cuda())\n        loss = loss_fn(predicted, batch_target.cuda(), batch_weights.cuda())\n        loss.backward()\n        optimizer.step()\n        batch_loss = loss.item()\n        if interactive:\n            batch_it.set_postfix({'batch loss': batch_loss})\n        epoch_loss += batch_loss * batch_lengths.size(0)\n    if interactive:\n        batch_it.close()\n    epoch_loss /= train_lengths.size(0)\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:12.024267Z","iopub.execute_input":"2021-12-08T00:00:12.024571Z","iopub.status.idle":"2021-12-08T00:00:12.034138Z","shell.execute_reply.started":"2021-12-08T00:00:12.024523Z","shell.execute_reply":"2021-12-08T00:00:12.033242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(model):\n    model.eval()\n    test_size = test_ids.size(0)\n    predictions = torch.empty(test_size, dtype=torch.float)\n    index = 0\n    for batch_lengths, batch_sentences in tqdm(test_dataloader, desc=\"Test\"):\n        predicted = torch.sigmoid(model(batch_sentences.cuda().long(), batch_lengths.cuda()))\n        index_to = index + predicted.size(0)\n        predictions[index:index_to] = predicted[:,0].cpu().detach()\n        index = index_to\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:12.0354Z","iopub.execute_input":"2021-12-08T00:00:12.035994Z","iopub.status.idle":"2021-12-08T00:00:12.048839Z","shell.execute_reply.started":"2021-12-08T00:00:12.035902Z","shell.execute_reply":"2021-12-08T00:00:12.048133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract(history, key, default=None):\n        return [elem.get(key, default) for elem in history]\n\ndef view_history(history):\n    plt.plot(extract(history, 'epoch'), extract(history, 'loss'))\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()\n    print(history)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:00:12.050518Z","iopub.execute_input":"2021-12-08T00:00:12.051287Z","iopub.status.idle":"2021-12-08T00:00:12.059336Z","shell.execute_reply.started":"2021-12-08T00:00:12.050974Z","shell.execute_reply":"2021-12-08T00:00:12.058697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_models = 6\npredictions = torch.zeros(test_ids.size(0), dtype=torch.float)\ntotal_weight = 0\nfor index in range(1,num_models+1): \n    print(f'Start training model#{index}')\n    #model = Model(embeddings=word_embeddings, hidden_size=128, layer_count=2, linear_layer_count=2).cuda()\n    model = Model(embeddings=word_embeddings, hidden_size=128, layer_count=2, linear_layer_count=2).cuda()\n    \n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.6**epoch)\n    num_epochs = 10\n    history = []\n    for epoch in range(1,num_epochs+1):\n        epoch_loss = fit_one_epoch(model, optimizer, epoch)\n        print(\"Epoch {}/{}: Average Loss={}\".format(epoch, num_epochs, epoch_loss), flush=True)\n        history.append({'epoch': epoch, 'loss': epoch_loss})\n        epoch_predictions = get_predictions(model)\n        epoch_weight = 2**epoch\n        predictions += epoch_weight * epoch_predictions\n        total_weight += epoch_weight\n        lr_scheduler.step()\n    view_history(history)\npredictions /= total_weight","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:13.607769Z","iopub.execute_input":"2021-12-08T02:51:13.608079Z","iopub.status.idle":"2021-12-08T06:03:43.280954Z","shell.execute_reply.started":"2021-12-08T02:51:13.608029Z","shell.execute_reply":"2021-12-08T06:03:43.277328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T06:25:17.402865Z","iopub.execute_input":"2021-12-08T06:25:17.403154Z","iopub.status.idle":"2021-12-08T06:25:17.525511Z","shell.execute_reply.started":"2021-12-08T06:25:17.403105Z","shell.execute_reply":"2021-12-08T06:25:17.524149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del identity_columns, train_target, train_weights, test_ids, train_lengths, test_lengths, token2index, train_input, test_input, zero_mask\ndel word_embeddings, train_dataset, train_dataloader, test_dataset, test_dataloader, target_weights, model, optimizer, lr_scheduler\ndel history, epoch_predictions\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T13:39:24.926695Z","iopub.execute_input":"2021-12-07T13:39:24.927243Z","iopub.status.idle":"2021-12-07T13:39:25.07715Z","shell.execute_reply.started":"2021-12-07T13:39:24.927187Z","shell.execute_reply":"2021-12-07T13:39:25.076419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:01:55.185432Z","iopub.execute_input":"2021-12-07T22:01:55.185757Z","iopub.status.idle":"2021-12-07T22:01:55.190651Z","shell.execute_reply.started":"2021-12-07T22:01:55.185707Z","shell.execute_reply":"2021-12-07T22:01:55.189678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport warnings\nimport torch.utils.data\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T01:05:59.034061Z","iopub.execute_input":"2021-12-08T01:05:59.034416Z","iopub.status.idle":"2021-12-08T01:05:59.23868Z","shell.execute_reply.started":"2021-12-08T01:05:59.034358Z","shell.execute_reply":"2021-12-08T01:05:59.237943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T01:05:50.332684Z","iopub.execute_input":"2021-12-08T01:05:50.332998Z","iopub.status.idle":"2021-12-08T01:05:50.339607Z","shell.execute_reply.started":"2021-12-08T01:05:50.332945Z","shell.execute_reply":"2021-12-08T01:05:50.338643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig('../input/finetuned-bert-for-jigsaw-toxicity-classification/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T01:06:02.411531Z","iopub.execute_input":"2021-12-08T01:06:02.411853Z","iopub.status.idle":"2021-12-08T01:06:02.475085Z","shell.execute_reply.started":"2021-12-08T01:06:02.411797Z","shell.execute_reply":"2021-12-08T01:06:02.474319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest2_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T01:06:04.056399Z","iopub.execute_input":"2021-12-08T01:06:04.056748Z","iopub.status.idle":"2021-12-08T01:06:04.337474Z","shell.execute_reply.started":"2021-12-08T01:06:04.056695Z","shell.execute_reply":"2021-12-08T01:06:04.336739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"../input/finetuned-bert-for-jigsaw-toxicity-classification/bert_pytorch.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T10:20:04.567819Z","iopub.execute_input":"2021-12-07T10:20:04.568164Z","iopub.status.idle":"2021-12-07T10:20:12.42694Z","shell.execute_reply.started":"2021-12-07T10:20:04.568106Z","shell.execute_reply":"2021-12-07T10:20:12.426124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T10:20:22.740507Z","iopub.execute_input":"2021-12-07T10:20:22.740806Z","iopub.status.idle":"2021-12-07T10:32:13.146565Z","shell.execute_reply.started":"2021-12-07T10:20:22.740755Z","shell.execute_reply":"2021-12-07T10:32:13.145878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_predictions = 0.5*test_pred + 0.5*predictions.numpy()\nfinal_predictions = predictions.numpy()\n#final_predictions = 0.5*test_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:47:48.923395Z","iopub.execute_input":"2021-12-08T02:47:48.923724Z","iopub.status.idle":"2021-12-08T02:47:48.927619Z","shell.execute_reply.started":"2021-12-08T02:47:48.923673Z","shell.execute_reply":"2021-12-08T02:47:48.926781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test2_df['id'], 'prediction': final_predictions})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:47:50.372174Z","iopub.execute_input":"2021-12-08T02:47:50.372459Z","iopub.status.idle":"2021-12-08T02:47:50.793732Z","shell.execute_reply.started":"2021-12-08T02:47:50.372409Z","shell.execute_reply":"2021-12-08T02:47:50.792957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:47:51.942182Z","iopub.execute_input":"2021-12-08T02:47:51.942468Z","iopub.status.idle":"2021-12-08T02:47:51.950439Z","shell.execute_reply.started":"2021-12-08T02:47:51.94242Z","shell.execute_reply":"2021-12-08T02:47:51.949672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T12:02:36.893783Z","iopub.execute_input":"2021-12-07T12:02:36.894102Z","iopub.status.idle":"2021-12-07T12:02:36.913427Z","shell.execute_reply.started":"2021-12-07T12:02:36.894049Z","shell.execute_reply":"2021-12-07T12:02:36.912569Z"},"trusted":true},"execution_count":null,"outputs":[]}]}