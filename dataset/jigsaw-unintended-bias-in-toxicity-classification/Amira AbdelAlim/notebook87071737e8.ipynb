{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nimport unicodedata\nimport html\nstop_words = stopwords.words('english')\nfrom keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data= pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data=Data.iloc[0:100000,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_Data= Data['comment_text']\ny_Data= Data['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del Data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data, train_labels, test_labels = train_test_split(x_Data, \n                                                                    y_Data,\n                                                                    test_size=0.33,\n                                                                    random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Unprocessed_train_data= train_data.to_list()\nUnprocessed_test_data= test_data.to_list()\n#Unprocessed_train_labels= train_labels.to_list()\n#Unprocessed_test_labels= test_labels.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_data\ndel test_data\ndel x_Data\ndel y_Data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Pre-Processing","metadata":{}},{"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', ' ', str(text))\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n  return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_corpus(corpus):\n  return [normalize_text(t) for t in corpus]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data= normalize_corpus(Unprocessed_train_data)\ntest_data= normalize_corpus(Unprocessed_test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BOW Model Using Binary Features:","metadata":{}},{"cell_type":"code","source":"vocab_sz = 10000 # None means all\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\ntok.fit_on_texts(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_data, mode='binary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = tok.texts_to_matrix(test_data, mode='binary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.asarray(train_labels).astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_labels\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.asarray(test_labels).astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_labels\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model using Binary Features:","metadata":{}},{"cell_type":"code","source":"from keras import regularizers\nmodel_1 = models.Sequential()\nmodel_1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel_1.add(layers.Dropout(0.6))\nmodel_1.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.02), activation='relu'))\nmodel_1.add(layers.Dense(1, activation='sigmoid'))\nmodel_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.MeanSquaredError(name=\"mean_squared_error\"),\n              metrics=[metrics.MeanSquaredError(name=\"mean_squared_error\")])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.fit(x_train, y_train,\n                    epochs=5,\n                    batch_size=8,\n                    validation_data=(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Glove pretrained embeddings:","metadata":{}},{"cell_type":"code","source":"train_data= normalize_corpus(Unprocessed_train_data)\ntest_data= normalize_corpus(Unprocessed_test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_sz = 10000 \ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\ntok.fit_on_texts(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('glove.6B.100d.txt', mode='rt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))\n# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_sz, 100))\nfor word, i in tok.word_index.items():\n    if i == vocab_sz:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# integer encode the documents\ntrain_data_sequence =tok.texts_to_sequences(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_sequence =tok.texts_to_sequences(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pad documents to a max length of 30 words\nmax_length = 100\nx_train_= pad_sequences(train_data_sequence, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_data_sequence\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test= pad_sequences(test_data_sequence, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_data_sequence\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM Model","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM, GRU\nn_latent_factors = 100\nmodel_2 = models.Sequential()\nmodel_2.add(layers.Embedding(vocab_sz, n_latent_factors, weights=[embedding_matrix], input_length=max_length, trainable=False))\nmodel_2.add(LSTM(100, return_sequences=True))\nmodel_2.add(LSTM(100))\nmodel_2.add(layers.Dense(100, activation='relu'))\nmodel_2.add(layers.Dense(16, activation='relu'))\nmodel_2.add(layers.Dense(16, activation='relu'))\nmodel_2.add(layers.Dense(1, activation='sigmoid'))\nmodel_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.compile(optimizer=optimizers.RMSprop(lr=0.0001),\n              loss=losses.MeanSquaredError(name=\"mean_squared_error\"),\n              metrics=[metrics.MeanSquaredError(name=\"mean_squared_error\")])\nmodel_2.fit(x_train_,y_train,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU Model","metadata":{}},{"cell_type":"code","source":"model_3 = models.Sequential()\nmodel_3.add(layers.Embedding(vocab_sz, n_latent_factors, weights=[embedding_matrix], input_length=max_length, trainable=False))\nmodel_3.add(GRU(100, return_sequences=True))\nmodel_3.add(GRU(100))\nmodel_3.add(layers.Dense(100, activation='relu'))\nmodel_3.add(layers.Dense(16, activation='relu'))\nmodel_3.add(layers.Dense(16, activation='relu'))\nmodel_3.add(layers.Dense(1, activation='sigmoid'))\nmodel_3.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.compile(optimizer=optimizers.RMSprop(lr=0.0001),\n              loss=losses.MeanSquaredError(name=\"mean_squared_error\"),\n              metrics=[metrics.MeanSquaredError(name=\"mean_squared_error\")])\nmodel_3.fit(x_train_,y_train,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion: The best performance was obtained using the GRU Model which keeps improving its performance by increasing the training epoques;\n## The second best performance is the LSTM Model;","metadata":{}}]}