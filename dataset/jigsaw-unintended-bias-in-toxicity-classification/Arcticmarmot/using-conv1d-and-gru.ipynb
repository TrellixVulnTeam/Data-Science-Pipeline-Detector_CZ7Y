{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#access data\ntrain_frame = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_frame = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nprint(train_frame.shape)\nprint(test_frame.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmaxlen = 220\nmax_words = 100000\n\nx_train = train_frame['comment_text']\nx_test = test_frame['comment_text']\ny_train = train_frame['target']\n\n#preprocessing\n#from https://www.kaggle.com/tanreinama/pretext-lstm-tuning-v3-with-ensemble-tune\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'\n\nmapping = {\n    \"·¥Ä\": \"a\", \" ô\": \"b\", \"·¥Ñ\": \"c\", \"·¥Ö\": \"d\", \"·¥á\": \"e\", \"“ì\": \"f\", \"…¢\": \"g\", \" ú\": \"h\", \"…™\": \"i\", \n    \"·¥ä\": \"j\", \"·¥ã\": \"k\", \" ü\": \"l\", \"·¥ç\": \"m\", \"…¥\": \"n\", \"·¥è\": \"o\", \"·¥ò\": \"p\", \"«´\": \"q\", \" Ä\": \"r\", \n    \"s\": \"s\", \"·¥õ\": \"t\", \"·¥ú\": \"u\", \"·¥†\": \"v\", \"·¥°\": \"w\", \"x\": \"x\", \" è\": \"y\", \"·¥¢\": \"z\",\"Êàë\":' I ',\"‰Ω†\":' you ',\n    \"üòÇ\":' kidding ','üò≠':' sad ','üò†':' angry ','üòÅ':' excited ','üëé':' discriminate ','üëç':' great ','üòÑ':' happy ',\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\":\"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"trump's\": \"trump is\", \"obama's\": \"obama is\", \"canada's\": \"canada is\", \"today's\": \"today is\"}\ndef del_punct(x):\n    for p in punct:\n        if p in x:\n            x.replace(p,' ')\n    return x\ndef fix_quote(x):\n    x_list = x.split()\n    for ele in x_list:\n        if ele.startswith(\"'\"):\n            x.replace(ele,ele[1:],1)\n        if ele.endswith(\"'\"):\n            x.replace(ele,ele[:-1],1)\n    return x\ndef trans_map(x):\n    for k,v in mapping.items():\n        if k in x:\n            x.replace(k,v)\n    return x\ndef preprocessing(x):\n    x = x.apply(trans_map)\n    x = x.apply(fix_quote)\n    x = x.apply(del_punct)\n    return x\nx_train = preprocessing(x_train)\nx_test = preprocessing(x_test)\nprint('preprocessing completed')\n\ntokenizer = Tokenizer(num_words = max_words)    \ntokenizer.fit_on_texts(list(x_train)+list(x_test))\nprint('tokenizer completed')\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = pad_sequences(x_train,maxlen = maxlen)\nx_test = pad_sequences(x_test,maxlen =maxlen)\nword_index = tokenizer.word_index\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_dir = '../input/glove6b-300d/glove.6B.300d.txt'\nembeddings_index = {}\ncount = 0\nunfound = []\nwith open(glove_dir) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:],dtype = 'float32')\n        embeddings_index[word] = coefs\nprint('embeddings_index completed')\nembedding_dim = 300\nembedding_matrix = np.zeros((max_words,embedding_dim))\nfor word,i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n        else:\n            unfound.append(i)\n\nprint('matrix completed')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_dict = dict([(v,k) for k,v in word_index.items()])\nprint(len(unfound))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\nfrom keras import callbacks\nfrom keras import Input\n\n\ndef get_model():\n    model = models.Sequential()\n    model.add(layers.Embedding(100000,300,input_length = 220,weights = [embedding_matrix],trainable = False))\n    model.add(layers.Conv1D(16,8,activation = 'relu'))\n    model.add(layers.MaxPool1D(3))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Conv1D(16,8,activation = 'relu'))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Bidirectional(layers.GRU(128)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(1,activation = 'sigmoid'))\n    model.summary()\n    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['acc'])\n    return model\ndef get_model2():\n    input_tensor = Input(shape = (220,))\n    emb = layers.Embedding(100000,300,input_length = 220,weights = [embedding_matrix],trainable = False)(input_tensor)\n    x = layers.Conv1D(16,8,activation = 'relu')(emb)\n    x = layers.MaxPool1D(3)(x)\n    x = layers.Conv1D(16,8,activation = 'relu')(x)\n    x = layers.MaxPool1D(3)(x)\n    x = layers.Bidirectional(layers.CuDNNLSTM(128,return_sequences = True))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(300,1,padding = 'same')(x)\n    y = layers.concatenate([emb,x],axis = 1)\n    y = layers.Bidirectional(layers.CuDNNLSTM(128))(y)\n    y = layers.BatchNormalization()(y)\n    output = layers.Dense(1,activation = 'sigmoid')(y)\n    model = models.Model(input_tensor,output)\n    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['acc'])\n    model.summary()\n    return model\ndef get_model3():\n    input_tensor = Input(shape = (220,))\n    emb = layers.Embedding(100000,300,input_length = 220,weights = [embedding_matrix],trainable = False)(input_tensor)\n    conv = layers.Conv1D(128,2,activation = 'relu',padding = 'same')(emb)\n    conv = layers.MaxPool1D(5)(conv)\n    conv = layers.BatchNormalization()(conv)\n    conv = layers.Conv1D(256,3,activation = 'relu',padding = 'same')(conv)\n    conv = layers.MaxPool1D(5)(conv)\n    conv = layers.BatchNormalization()(conv)\n    lstm = layers.Bidirectional(layers.CuDNNLSTM(128,return_sequences = True))(emb)\n    lstm = layers.Bidirectional(layers.CuDNNLSTM(128))(lstm)\n    lstm = layers.BatchNormalization()(lstm)\n    add = layers.add([conv,lstm])\n    output = layers.Flatten()(add)\n    output = layers.Dense(128,activation = 'relu')(output)\n    output = layers.Dense(1,activation = 'sigmoid')(output)\n    model = models.Model(input_tensor,output)\n    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['acc'])\n    model.summary()\n    return model\ndef lr_func(epoch):\n    return 0.01*(10**(-epoch))\nmodel = get_model3()\nmodel_check_point = callbacks.ModelCheckpoint('./model1_5.h5',monitor = 'val_loss',save_best_only = True)\nlr_shedule = callbacks.LearningRateScheduler(lr_func)\nhistory = model.fit(x_train[100000:],y_train[100000:]>0.5,epochs = 8,batch_size = 2048,validation_data = [x_train[:100000],y_train[:100000]>0.5],callbacks = [model_check_point])\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import roc_auc_score\n#y_test = model.predict(x_train[:100000])\n#roc_auc_score(y_train[:100000]>0.5,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('./model1_5.h5')\nresult = model.predict(x_test)\nresult = pd.DataFrame({'id':sub.id,'prediction':result[:,0]})\nresult.to_csv(\"submission.csv\", index=False, columns=['id', 'prediction'])\nprint('submit successful')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}