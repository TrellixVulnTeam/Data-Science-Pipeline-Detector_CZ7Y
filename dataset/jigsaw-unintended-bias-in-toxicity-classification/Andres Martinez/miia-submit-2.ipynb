{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project 3\n\n\n# Conversations Toxicity Detection\n\nJigsaw Unintended Bias in Toxicity Classification \n\nDetect toxicity across a diverse range of conversations\n\n\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data#\n\n\nTry Colab\nhttps://albahnsen.com/2018/07/22/how-to-download-kaggle-data-into-google-colab/\n\n\n## Description\n\n## Background\nAt the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nIn the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).\n\nThe data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n\n- severe_toxicity\n- obscene\n- threat\n- insult\n- identity_attack\n- sexual_explicit\n\nAdditionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold.\n\n- male\n- female\n- transgender\n- other_gender\n- heterosexual\n- homosexual_gay_or_lesbian\n- bisexual\n- other_sexual_orientation\n- christian\n- jewish\n- muslim\n- hindu\n- buddhist\n- atheist\n- other_religion\n- black\n- white\n- asian\n- latino\n- other_race_or_ethnicity\n- physical_disability\n- intellectual_or_learning_disability\n- psychiatric_or_mental_illness\n- other_disability\n\nNote that the data contains different comments that can have the exact same text. Different comments that have the same text may have been labeled with different targets or subgroups.\n\n## Examples\nHere are a few examples of comments and their associated toxicity and identity labels. Label values range from 0.0 - 1.0 represented the fraction of raters who believed the label fit the comment.\n\n- Comment: i'm a white woman in my late 60's and believe me, they are not too crazy about me either!!\n\nToxicity Labels: All 0.0\nIdentity Mention Labels: female: 1.0, white: 1.0 (all others 0.0)\n- Comment: Why would you assume that the nurses in this story were women?\n\nToxicity Labels: All 0.0\nIdentity Mention Labels: female: 0.8 (all others 0.0)\n- Comment: Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.\n\nToxicity Labels: All 0.0\nIdentity Mention Labels: homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)\n\nIn addition to the labels described above, the dataset also provides metadata from Jigsaw's annotation: toxicity_annotator_count and identity_annotator_count, and metadata from Civil Comments: created_date, publication_id, parent_id, article_id, rating, funny, wow, sad, likes, disagree. Civil Comments' label rating is the civility rating Civil Comments users gave the comment.\n\n## Labelling Schema\nTo obtain the toxicity labels, each comment was shown to up to 10 annotators*. Annotators were asked to: \"Rate the toxicity of this comment\"\n\n- Very Toxic (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- Toxic (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- Hard to Say\n- Not Toxic\n\nThese ratings were then aggregated with the target value representing the fraction of annotations that annotations fell within the former two categories.\n\nTo collect the identity labels, annotators were asked to indicate all identities that were mentioned in the comment. An example question that was asked as part of this annotation effort was: \"What genders are mentioned in the comment?\"\n\n- Male\n- Female\n- Transgender\n- Other gender\n- No gender mentioned\n\nAgain, these were aggregated into fractional values representing the fraction of raters who said the identity was mentioned in the comment.\n\nThe distributions of labels and subgroup between Train and Test can be assumed to be similar, but not exact.\n\n*Note: Some comments were seen by many more than 10 annotators (up to thousands), due to sampling and strategies used to enforce rater accuracy.\n\n## File descriptions\n- train.csv - the training set, which includes subgroups\n- test.csv - the test set, which does not include subgroups\n- sample_submission.csv - a sample submission file in the correct format\n\n\n# Evaluation\n\n- 50% Create a solution using with a Machine Learning algorithm - Presentation - Only show what you did different or what other teams can learn from your solution\n- 50% Performance in the Kaggle competition (Normalized acording to class performance in the private leaderboard)\n\n_____________\n\n\n\n### Owners\n\nThe project has been developed by the following:\n\n* Andres Felipe Martinez Tunarroza\n* Jorge Luis Medina Herrada\n* Ana Milena Rodriguez Gómez\n* Nicolas David Gil Quijano\n\nCredtis to **thousandvoices** ([source](https://www.kaggle.com/thousandvoices/simple-lstm))\n\nData Mining. University of the Andes.\n\nMay, 2019."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Global variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = ['../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n                   '../input/glove840b300dtxt/glove.840B.300d.txt']\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\nIDENTITY_COLUMNS = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n                    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\nlist_stopwords = {'the','be','to','of','and','a','in','that','have','I','is','it','for','not','on','with','he','as',\n                  'you','do','at', 'this','but','his','by','from'}\n\nstopword_Common_names = {'Oliver','Noah','Jack','Liam','Harry', 'Mason','Jacob','Charlie','William','Thomas',\n                         'Ethan','George','Michael','Oscar','Alexander','James','Daniel','Amelia','Emma','Olivia',\n                         'Isla','Sophia','Emily','Isabella','Poppy','Ava','Mia','Isabella','Jessica','Abigail',\n                         'Lily','Madison','Sophie','Charlotte'}\n\nlist_stopwords = set.union(stopword_Common_names,list_stopwords,stopwords.words('english'))\n\nids = {'contractions':[\"' m\",\"' re\",\"' ve\",\"' s\",\"' ll\",\"' d\",\"' n't\",\"'m\",\"'re\",\"'ve\",\"'s\",\"'ll\",\"'d\",\"'n't\",\"n't\",\"[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\"],\n      'normal':[\" am\",\" are\",\" have\",\" is\",\" will\",\" had\",\" not\",\" am\",\" are\",\" have\",\" is\",\" will\",\" had\",\" not\",\" not\", \" \"]}\n\nids = dict(zip(ids['contractions'], ids['normal']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    \n    aux = []\n    for i in np.arange(df.shape[0]):\n        querywords = pd.DataFrame(df).iloc[i][0].split()          \n        resultwords  = [word for word in querywords if word.lower() not in list_stopwords]\n        aux.append(' '.join(resultwords))\n\n    x_t = pd.DataFrame(aux, index=df.index)[0]\n    \n    return x_t\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neural network arquitecture\n\n* Embedding layer\n* Dropout layer\n* LSTM layer (Bidirectional with *CuDNNLSTM* implementation)\n* Pooling layer (GlobalMaxPooling1D and GlobalAveragePooling1D)\n* Dense layer\n\nOhter information:\n* `loss='binary_crossentropy'`\n* `optimizer='adam'`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_ = preprocessing(pd.DataFrame(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_ = preprocessing(pd.DataFrame(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weights calculation to fit the neural network "},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights /= sample_weights.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neural Network training and prediction"},{"metadata":{"trusted":false},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results Consolidation"},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submit"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({'id': test_df.id,\n                                     'prediction': predictions})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}