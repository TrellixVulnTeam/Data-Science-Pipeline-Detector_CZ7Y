{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nHere is a very simple starter for using LightGBM in this version of the toxic comments challenge."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the data and extract the necessary fields\n\nI'm ignoring all the other columns right now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = train.loc[:, 'target'].values\ntexts = [text for doc_id, text in train.loc[:, 'comment_text'].iteritems()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas is often not very efficient when objects are involved, so we may as well delete the dataframe to save memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the Text\n\nTo start, I'm just going to use the Scikit-learn CountVectorizer. This is probably not optimal, but that's the point of a starter notebook. I just chose some fairly random parameters without trying to tune anything."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n\ncount_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=50, max_df=0.2)\ncount_vectorizer.fit(texts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized_texts = 1.0 * count_vectorizer.transform(texts)  # 1.0 since LGBM wants floats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a train-validation Split\n\nNot doing k-fold right now since I'm just getting started. Also note that there will be a bit of data leakage from running the vectorizer before doing the split. I'll just ignore this for now since it will typically be a small effect for such a large dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(\n    vectorized_texts, targets, test_size=0.2, random_state=80745, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create LightGBM datasets and initialize parameters\n\nI haven't done any tuning of this right now. One important thing to do in the future will be \nto add the official evaluation AUC definition as a metric so we can get a real evaluation of\nwhat the score should be."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\ntrain_data = lgb.Dataset(X_train, Y_train)\nvalid_data = lgb.Dataset(X_valid, Y_valid, reference=train_data)\n\nparam = {\n    'num_leaves':31,\n    'num_trees':150,\n    'objective':'cross_entropy',\n    'metric': ['auc']\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bdt = lgb.train(param, train_data, 100, valid_sets=[valid_data])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_texts = [text for doc_id, text in test_data.loc[:, 'comment_text'].iteritems()]\ntest_vectorized_texts = 1.0 * count_vectorizer.transform(test_texts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = bdt.predict(test_vectorized_texts)\ntest_data['prediction'] = predictions\nfinal_result = test_data[['prediction']].to_csv('lightgbm_primer_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ...And we're done.\n\nThere's a ton of other work to be done. Preprocessing the text, tuning the model, building new features, etc. might all bring significant improvements to the model. This likely isn't going to as well right now as some of the deep learning models, but with the right feature engineering it might be quite competitive."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}