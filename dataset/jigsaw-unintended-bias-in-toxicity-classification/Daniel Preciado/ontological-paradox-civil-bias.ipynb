{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom nltk.corpus import stopwords\nfrom multiprocessing import Pool, cpu_count\nfrom warnings import filterwarnings as fw; fw(\"ignore\")\n\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\ntrain.shape, test.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mcol = [train.columns[i] for i in [11, 13, 14, 17, 19, 21, 22, 29, 31]]\ntrain.drop(columns=[c for c in train.columns if c not in ['id', 'comment_text', 'target']+mcol], inplace=True)\n\nfor c in ['target']+mcol:\n    train[c] = np.where(train[c] >= 0.5, 1, 0)\ntrain['weights'] = train[mcol[0]].astype(str).str.cat(train[mcol[1:]].astype(str), sep='')\ntrain['weights'] = train['weights'].map(lambda x: int(x, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_df(df):\n    sw = set(stopwords.words(\"english\"))\n    df = pd.DataFrame(df)\n    df['np'] = df['comment_text'].map(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    df['nu'] = df['comment_text'].map(lambda x: len([w for w in str(x).split(' ') if w.isupper()]))\n    df['nt'] = df['comment_text'].map(lambda x: len([w for w in str(x).split(' ') if w.istitle()]))\n    df['len'] = df['comment_text'].map(lambda x: len(str(x)))\n    df['wc'] = df['comment_text'].map(lambda x: len(str(x).split(' ')))\n    df['wcu'] = df['comment_text'].map(lambda x: len(set(str(x).split(' '))))\n    df['mwl'] = df['comment_text'].map(lambda x: np.mean([len(w) for w in str(x).split(' ')]))\n    df['wcu%'] = df['wcu'] / df['wc']\n    df['comment_text'] = df['comment_text'].str.lower()\n    df['comment_text'] = df['comment_text'].str.replace('[^a-z ]',' ', regex=True)\n    df['comment_text'] = df['comment_text'].str.replace('    ',' ', regex=False)\n    df['comment_text'] = df['comment_text'].str.replace('   ',' ', regex=False)\n    df['comment_text'] = df['comment_text'].str.replace('  ',' ', regex=False)\n    df['sw'] = df['comment_text'].map(lambda x: len([w for w in str(x).split(' ') if w in sw]))\n    df['swu'] = df['comment_text'].map(lambda x: len(set([w for w in str(x).split(' ') if w in sw])))\n    df['lenc'] = df['comment_text'].map(lambda x: len(str(x)))\n    df['wcc'] = df['comment_text'].map(lambda x: len(str(x).split(' ')))\n    df['wcuc'] = df['comment_text'].map(lambda x: len(set(str(x).split(' '))))\n    df['mwlc'] = df['comment_text'].map(lambda x: np.mean([len(w) for w in str(x).split(' ')]))\n    df['wcuc%'] = df['wcuc'] / df['wcc']\n    return df\n\ndef multi_transform(df):\n    p = Pool(cpu_count())\n    df = p.map(transform_df, np.array_split(df, cpu_count()))\n    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n    p.close(); p.join()\n    return df\n\ntrain = multi_transform(train)\ntest = multi_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\n\ncol = ['np', 'nu', 'nt', 'len', 'wc', 'wcu', 'mwl','wcu%', 'sw', 'swu', 'lenc', 'wcc', 'wcuc', 'mwlc', 'wcuc%', 'tscore']\nword_toxicity = {}\n\ndef get_word_toxicity(s,toxicity):\n    global word_toxicity\n    for w in str(s).split(' '):\n        if w in word_toxicity:\n            word_toxicity[w]['Count'] += 1\n            word_toxicity[w]['ToxicitySum'] += toxicity\n            word_toxicity[w]['Toxicity'] = word_toxicity[w]['ToxicitySum'] / word_toxicity[w]['Count']\n        else:\n            word_toxicity[w] = {'Count': 1, 'ToxicitySum': toxicity, 'Toxicity':  toxicity}\n\n_ = train.progress_apply(lambda r:  get_word_toxicity(r['comment_text'], r['target']), axis=1)\n\ndef score_text1(s):\n    global word_toxicity\n    score = 0.\n    max_toxicity = 0.\n    wc = len(str(s).split(' '))\n    for w in str(s).split(' '):\n        if w in word_toxicity:\n            if word_toxicity[w]['Count'] > 1:\n                if word_toxicity[w]['Toxicity'] > max_toxicity:\n                    max_toxicity = word_toxicity[w]['Toxicity']\n                score += word_toxicity[w]['Toxicity']\n    return ((score / wc) * 0.9) + (max_toxicity * 0.1) \n\ntrain['tscore'] = train.progress_apply(lambda r:  score_text1(r['comment_text']), axis=1)\ntest['tscore'] = test.progress_apply(lambda r:  score_text1(r['comment_text']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vsize = 70\n\ndef load_vectors(path, size=30):\n    f = open(path, 'r', encoding='utf-8')\n    d = {}\n    for l in f:\n        w = l.rstrip().split(' ')\n        d[w[0]] = list(map(float, w[1:size+1]))\n    return d\n\nw2v = load_vectors('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', vsize)\n\ndef get_comment_vectors(s, size=30):\n    s = str(s).split(' ')\n    v = np.zeros(size)\n    denom = 0\n    for w in s:\n        if w in w2v:\n            v = np.add(v, w2v[w])\n            denom += 1\n    if denom > 0:\n        v /= denom\n    return v\n\ntrainx = [get_comment_vectors(s, vsize) for s in train['comment_text'].values]\ntrainx = pd.DataFrame(trainx, columns=['crawl_300d_2M_vec+' + str(i) for i in range(vsize)])\ntrain = pd.concat((train, trainx), axis=1)\n\ntestx = [get_comment_vectors(s, vsize) for s in test['comment_text'].values]\ntestx = pd.DataFrame(testx, columns=['crawl_300d_2M_vec+' + str(i) for i in range(vsize)])\ntest = pd.concat((test, testx), axis=1)\n\ncol += ['crawl_300d_2M_vec+' + str(i) for i in range(vsize)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef subgroup_auc(df, subgroup):\n    subgroup_examples = df[df[subgroup]]\n    return auc(subgroup_examples['target'], subgroup_examples['pred'])\n\ndef bpsn_auc(df, subgroup):\n    subgroup_negative_examples = df[df[subgroup] & ~df['target']]\n    non_subgroup_positive_examples = df[~df[subgroup] & df['target']]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return auc(examples['target'], examples['pred'])\n\ndef bnsp_auc(df, subgroup):\n    subgroup_positive_examples = df[df[subgroup] & df['target']]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df['target']]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return auc(examples['target'], examples['pred'])\n\ndef bm_for_model(df):\n    global mcol\n    records = []\n    for subgroup in mcol:\n        record = {'subgroup': subgroup, 'subgroup_size': len(df[subgroup])}\n        record['subgroup_auc'] = subgroup_auc(df,subgroup)\n        record['bpsn_auc'] = bpsn_auc(df, subgroup)\n        record['bnsp_auc'] = bnsp_auc(df, subgroup)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\ndef power_mean(series, p=-5):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(df):\n    global mcol\n    for c in ['target'] + mcol:\n        df[c] = np.where(df[c] >= 0.5, True, False)\n    overall_auc = metrics.roc_auc_score(df['target'], df['pred'])\n    df = bm_for_model(df)\n    bias_score = np.average([power_mean(df['subgroup_auc']), power_mean(df['bpsn_auc']), power_mean(df['bnsp_auc'])])\n    return (overall_auc * 0.25) + ((1 - 0.25) * bias_score)\n\ndef lgb_toxic_metric(preds, dtrain):\n    global mcol\n    labels = dtrain.get_label()\n    weights =  dtrain.get_weight()\n    df = pd.DataFrame(weights, columns=['weights'])\n    df['weights'] = df['weights'].map(lambda x: np.binary_repr(x, width=len(mcol)))\n    for i in range(len(mcol)):\n        df[mcol[i]] = df['weights'].map(lambda x: int(str(x)[i]))\n    df['target'] = labels\n    df['pred'] = preds\n    score = get_final_metric(df)\n    return 'toxic_metric', score, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nparams = {'learning_rate':0.2, 'max_depth':8, 'objective':'binary', 'metric':'auc', 'num_leaves':32, 'feature_fraction':0.9,'bagging_fraction':0.8, 'bagging_freq':5}\n\nfolds = 3\ntest['prediction'] = 0.0\nfor fold in range(folds):\n    x1, x2, y1, y2, w1, w2 = model_selection.train_test_split(train[col], train['target'], train['weights'], test_size=0.3, random_state=fold+7)\n    model = lgb.train(params, lgb.Dataset(x1, label=y1, weight=w1), 100, lgb.Dataset(x2, label=y2, weight=w2), early_stopping_rounds=10,  verbose_eval=2, feval=lgb_toxic_metric)\n    test['prediction'] += model.predict(test[col], num_iteration=model.best_iteration)\ntest['prediction'] = (test['prediction'] / folds).clip(0,1)\ntest['prediction'] = test['prediction'].map(lambda x: x * 0.05 if x < 0.14 else x * 1.15).clip(0.000001,0.999999) #Blend kernel submission factors ** magic numbers **\ntest[['id', 'prediction']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}