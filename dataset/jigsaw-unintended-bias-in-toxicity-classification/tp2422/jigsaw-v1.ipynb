{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\n\n# nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\nimport unicodedata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_accented_chars(train['comment_text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_chars(text, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\nremove_special_chars(train['comment_text'][0], remove_digits=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_stemmer(text):\n\tps = nltk.porter.PorterStemmer()\n\ttext = ' '.join([ps.stem(word) for word in text.split()])\n\treturn text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_stemmer(train['comment_text'][0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\ndef lemmatize_text(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    text = ' '.join(wordnet_lemmatizer.lemmatize(elem) for elem in text.split(' '))    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatize_text(train['comment_text'][0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stopwords(train['comment_text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_corpus(corpus, html_stripping=False, contraction_expansion=True,\n                     accented_char_removal=False, text_lower_case=True, \n                     text_lemmatization=False, special_char_removal=True, \n                     stopword_removal=True, remove_digits=True):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n        # expand contractions    \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        # remove special characters and\\or digits    \n        if special_char_removal:\n            # insert spaces between special characters to isolate them    \n            special_char_pattern = re.compile(r'([{.(-)!}])')\n            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n            doc = remove_special_chars(doc, remove_digits=remove_digits)  \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_contractions(text):\n    return ' '.join([CONTRACTION_MAP[elem] if elem.lower() in CONTRACTION_MAP else elem for elem in text.lower().split(\" \")])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sample Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_sample = train[0:20]\n# new_train = train_sample['comment_text'].apply(lambda x: normalize_corpus(x.split(\" \")))\n\n# sample_train_df = pd.DataFrame(new_train)\n# sample_train_df['target'] = train['target'][0:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating TDF-IDF Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# entire train data, processing faster\n# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', stop_words='english')\n# tfidf_matrix = vectorizer.fit_transform(train['comment_text'])\n# feature_names = vectorizer.get_feature_names()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrix = vectorizer.fit_transform(train['comment_text']).todense()\n# train_tfidf = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Baseline Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfidf_vec = TfidfVectorizer(stop_words='english')\n# # tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n# tfidf_vec.fit_transform(train['comment_text'].values.tolist() + test['comment_text'].values.tolist())\n# train_tfidf = tfidf_vec.transform(train['comment_text'].values.tolist())\n# test_tfidf = tfidf_vec.transform(test['comment_text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.target\n# tfidf_train = tfidf_vec.fit_transform(train['comment_text'])\n\n# X_train, X_test, y_train, y_test = train_test_split(train['comment_text'], y, test_size=0.33,random_state=53)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = pd.concat([train['comment_text'], test['comment_text']])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 2),\n    max_features=50000)\nword_vectorizer.fit(all_text)\nprint('Word TFIDF 1/3')\ntrain_word_features = word_vectorizer.transform(train['comment_text'])\nprint('Word TFIDF 2/3')\ntest_word_features = word_vectorizer.transform(test['comment_text'])\nprint('Word TFIDF 3/3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\n\ntrain['new_target'] = np.where(train['target'] > 0, 1, 0)\n\ntrain_target = train['new_target']\nmodel = LogisticRegression(solver='sag')\nsfm = SelectFromModel(model, threshold=0.2)\nprint(train_word_features.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sparse_matrix = sfm.fit_transform(train_word_features, train_target)\nprint(train_sparse_matrix.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sparse_matrix, valid_sparse_matrix, y_train, y_valid = train_test_split(train_sparse_matrix, train_target, test_size=0.05, random_state=144)\n\n\ntest_sparse_matrix = sfm.transform(test_word_features)\n\nd_train = lgb.Dataset(train_sparse_matrix, label=y_train)\nd_valid = lgb.Dataset(valid_sparse_matrix, label=y_valid)\nwatchlist = [d_train, d_valid]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'learning_rate': 0.2,\n              'application': 'binary',\n              'num_leaves': 31,\n              'verbosity': -1,\n              'metric': 'auc',\n              'data_random_seed': 2,\n              'bagging_fraction': 0.8,\n              'feature_fraction': 0.6,\n              'nthread': 4,\n              'lambda_l1': 1,\n              'lambda_l2': 1}\nmodel = lgb.train(params,\n                  train_set=d_train,\n                  num_boost_round=140,\n                  valid_sets=watchlist,\n                  verbose_eval=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({'id': test['id']})\nsubmission['prediction'] = model.predict(test_sparse_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Pipeline**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}