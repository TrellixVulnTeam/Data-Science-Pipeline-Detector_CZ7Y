{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nimport torch\nimport torch.utils.data\nimport os\nimport warnings\nwarnings.filterwarnings(action='once')\n\ndevice = torch.device('cuda')\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n\n\nMAX_SEQUENCE_LENGTH = 200\nSEED = 42 \nBATCH_SIZE = 32\nINFER_BATCH_SIZE = 64\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n#LARGE_BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n#bert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\nbert_config = BertConfig('../input/bertinference/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\nBERT_SMALL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nBERT_LARGE_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\n\nstr_='imports done'\nos.system('echo '+str_)\n\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))   \n\nstr_='convert_lines done'\nos.system('echo '+str_)\n\ngc.collect()\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"../input/bertinference/pytorch_bert_6.bin\"))\nmodel.to(device)\n\nstr_='convert_lines done'\nos.system('echo '+str_)\n\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\ntest_preds = np.zeros((len(X_test)))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=INFER_BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * INFER_BATCH_SIZE:(i + 1) * INFER_BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\npredictions_bert = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n\nstr_='bert predicted'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GloVe, Fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\nimport numpy as np \nimport pandas as pd\nfrom IPython.display import clear_output\n#from nltk.corpus import stopwords    \nimport nltk\n#nltk.download('stopwords')\nfrom tqdm import tqdm as tqdm \ntqdm.pandas()\nimport os\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Bidirectional, concatenate, Embedding, Input, CuDNNLSTM, Dropout, SpatialDropout1D,GlobalMaxPooling1D,GlobalAveragePooling1D, add\nfrom keras.models import Sequential,Model\nfrom keras import regularizers, optimizers\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.regularizers import l1_l2\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for keys in train_df.columns:\n    if sum(train_df[keys].isna())!=0:\n        train_df[keys] = train_df[keys].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class pre_processing():\n\n    def __init__(self, data):\n        self.data = data\n        \n    def clean_contraction(self, data):\n        contraction_mapping = {\n            \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n            'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n            'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n            'can;t': 'cannot','can;t;ve': 'cannot have',\n            'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n            \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n            'couldn;t;ve': 'could not have','couldn´t': 'could not',\n            'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n            'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n            'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n            'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n            \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n            'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n            \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n            \"he'd've\": 'he would have',\"he'll\": 'he will',\n            \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n            'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n            'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n            \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n            'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n            'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n            'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n            'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n            \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n            'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n            'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n            'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n            'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n            'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n            'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n            \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n            'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n            'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n            'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n            \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n            'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n            'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n            \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n            'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n            'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n            \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n            'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n            \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n            'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n            'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n            'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n            'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n            'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n            \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n            'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n            \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n            'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n            'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n            'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n            'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n            \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n            'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n            'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n            'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n            'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n            'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n            '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n            \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n            \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n            \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n            \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n            \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n            \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n            'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n            'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n            'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n            'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n            'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n            'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n            'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n            'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n        }   \n        \n        def clean_contractions(text, mapping):\n            specials = [\"’\", \"‘\", \"´\", \"`\"]\n            for s in specials:\n                text = text.replace(s, \"'\")\n            text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n            return text\n        \n        data = data.progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n        return data\n        \n    def remove_punc(self, data):\n        punct_mapping = {\"_\":\" \", \"`\":\" \"}\n        punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n        punct += '©^®` <→°€™› ♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n        def clean_special_chars(text, punct, mapping):\n            for p in mapping:\n                text = text.replace(p, mapping[p])    \n            for p in punct:\n                text = text.replace(p, f' {p} ')     \n            return text\n\n            data = data.astype(str)\n            data = data.progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n        return data\n    \n    def lower(self, data):\n        data = data.astype(str).progress_apply(lambda x: x.lower())\n        return data\n    \n    def remove_stop_words(self, data):\n        #stop = set(stopwords.words('english'))\n        stop = {'a',\n         'about',\n         'above',\n         'after',\n         'again',\n         'against',\n         'ain',\n         'all',\n         'am',\n         'an',\n         'and',\n         'any',\n         'are',\n         'aren',\n         \"aren't\",\n         'as',\n         'at',\n         'be',\n         'because',\n         'been',\n         'before',\n         'being',\n         'below',\n         'between',\n         'both',\n         'but',\n         'by',\n         'can',\n         'couldn',\n         \"couldn't\",\n         'd',\n         'did',\n         'didn',\n         \"didn't\",\n         'do',\n         'does',\n         'doesn',\n         \"doesn't\",\n         'doing',\n         'don',\n         \"don't\",\n         'down',\n         'during',\n         'each',\n         'few',\n         'for',\n         'from',\n         'further',\n         'had',\n         'hadn',\n         \"hadn't\",\n         'has',\n         'hasn',\n         \"hasn't\",\n         'have',\n         'haven',\n         \"haven't\",\n         'having',\n         'he',\n         'her',\n         'here',\n         'hers',\n         'herself',\n         'him',\n         'himself',\n         'his',\n         'how',\n         'i',\n         'if',\n         'in',\n         'into',\n         'is',\n         'isn',\n         \"isn't\",\n         'it',\n         \"it's\",\n         'its',\n         'itself',\n         'just',\n         'll',\n         'm',\n         'ma',\n         'me',\n         'mightn',\n         \"mightn't\",\n         'more',\n         'most',\n         'mustn',\n         \"mustn't\",\n         'my',\n         'myself',\n         'needn',\n         \"needn't\",\n         'no',\n         'nor',\n         'not',\n         'now',\n         'o',\n         'of',\n         'off',\n         'on',\n         'once',\n         'only',\n         'or',\n         'other',\n         'our',\n         'ours',\n         'ourselves',\n         'out',\n         'over',\n         'own',\n         're',\n         's',\n         'same',\n         'shan',\n         \"shan't\",\n         'she',\n         \"she's\",\n         'should',\n         \"should've\",\n         'shouldn',\n         \"shouldn't\",\n         'so',\n         'some',\n         'such',\n         't',\n         'than',\n         'that',\n         \"that'll\",\n         'the',\n         'their',\n         'theirs',\n         'them',\n         'themselves',\n         'then',\n         'there',\n         'these',\n         'they',\n         'this',\n         'those',\n         'through',\n         'to',\n         'too',\n         'under',\n         'until',\n         'up',\n         've',\n         'very',\n         'was',\n         'wasn',\n         \"wasn't\",\n         'we',\n         'were',\n         'weren',\n         \"weren't\",\n         'what',\n         'when',\n         'where',\n         'which',\n         'while',\n         'who',\n         'whom',\n         'why',\n         'will',\n         'with',\n         'won',\n         \"won't\",\n         'wouldn',\n         \"wouldn't\",\n         'y',\n         'you',\n         \"you'd\",\n         \"you'll\",\n         \"you're\",\n         \"you've\",\n         'your',\n         'yours',\n         'yourself',\n         'yourselves'}\n        data = data.progress_apply(lambda x: (' ').join([i for i in x.split(' ') if i not in stop]))\n        return data\n    '''\n    def replace_profanity(self, data):\n        swear_words = [\n       ]\n        profanity1 = [i[:-2] for i in profanity]\n        profanity = [i for i in profanity1 if i not in ['as', 'bust', 'caw','chin', 'mil', 'pant', 'shot', 'spun', 'bone','rap', 'dam', 'cip', 'cnu', 'coo', 'cu', 'fa', 'ga', 'horn', 'no', 'pro', 'se', 'ti', 'x', 'xx']]\n        profanity_mapping = {}\n        for i in profanity:\n            profanity_mapping[i] = 'fuck'\n            \n        def rep_prof(text, mapping):\n            text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n            return text                \n\n        data = data.progress_apply(lambda x: rep_prof(x, profanity_mapping))\n        return data\n    '''\n    \n    def clean_spaces(self, data):\n        data = data.progress_apply(lambda x: correct_spaces(x))\n        return data\n        \n    def preprocess(self):\n        print('correct contractions')\n        self.data = self.clean_contraction(self.data)\n        #print('remove punc')\n        #self.data = self.remove_punc(self.data)\n        #print('lower')\n        #self.data = self.lower(self.data)\n        #print('remove stop_word')\n        #self.data = self.remove_stop_words(self.data)\n        #print('replacing profanity')\n        #self.data = self.replace_profanity(self.data)\n        #print('cleaning spaces')\n        #self.data = self.clean_spaces(self.data)\n        print('done')\n        return self.data\n\ntrain_df['comment_text'] = pre_processing(train_df['comment_text']).preprocess()\ntest_df['comment_text'] = pre_processing(test_df['comment_text']).preprocess()\n\nstr_='lstm preprocess done'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTARGET_COLUMN = ['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\nx_train = train_df['comment_text'].astype(str)\nx_test = test_df['comment_text'].astype(str)\n\ntokenizer = Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(tqdm(list(x_train) + list(x_test)))\n\nx_train = tokenizer.texts_to_sequences(tqdm(x_train))\nx_test = tokenizer.texts_to_sequences(x_test)\n\nMAX_LEN = 220\n\nx_train = pad_sequences(tqdm(x_train), maxlen=MAX_LEN)\nx_test = pad_sequences(x_test, maxlen=MAX_LEN)\n\ny_train = train_df['target'].values\ny_identity = (train_df[IDENTITY_COLUMNS].values>0.5).astype(int)\ny_aux_train = train_df[AUX_COLUMNS].values\n\nstr_='lstm tokenizer done'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n    \nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1).values\nsample_weights += train_df[TARGET_COLUMN].mul((~train_df[IDENTITY_COLUMNS]).sum(axis=1), axis=0).values.ravel()\nsample_weights += (~train_df[TARGET_COLUMN]).mul(train_df[IDENTITY_COLUMNS].sum(axis=1), axis=0).values.ravel()*5\nsample_weights /= sample_weights.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_PATHS = ['../input/embeddings/crawl-300d-2M.vec',\n                 '../input/embeddings/glove.840B.300d.txt']\n\ndef get_coefs(word, *arr):\n    \"\"\"\n    Get word, word_embedding from a pretrained embedding file\n    \"\"\"\n    return word, np.asarray(arr,dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_PATHS], axis=-1)\n\nstr_='embedding matrix done'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\nEPOCHS=4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n        #super(Attention, self).build(input_shape) \n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n\ndef build_model_1(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    \n    y = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    y = add([y, Dense(128*4, activation='relu')(y)])\n    y = add([y, Dense(128*4, activation='relu')(y)])\n\n    z = Attention(MAX_LEN)(x)\n    z = add([z, Dense(128*2, activation='relu')(y)])\n    z = add([z, Dense(128*2, activation='relu')(y)])    \n    \n    result = concatenate([y, z])\n    result = Dense(1, activation='sigmoid')(y)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(y)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model\n\n\n\ndef build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str_='training starting'\nos.system('echo '+str_)\n\ncheckpoint_predictions = []\nweights = []\n\nmodel = build_model(embedding_matrix, y_aux_train.shape[-1])\nfor epoch in range(EPOCHS):\n    str_='starting epoch: ' + str(epoch)\n    os.system('echo '+str_)\n    model.fit(\n        x_train,\n        [y_train, y_aux_train],\n        batch_size=BATCH_SIZE,\n        epochs=1,\n        verbose=1,\n        sample_weight=[sample_weights, np.ones_like(sample_weights)],\n        callbacks = [\n                LearningRateScheduler(lambda _: 1e-3*(0.55**epoch))\n        ]\n    )\n    preds = model.predict(x_test, batch_size=2048)\n    checkpoint_predictions.append(preds[0].flatten())\n    weights.append(2*epoch)\n    str_='finished epoch: ' + str(epoch)\n    os.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictions_lstm = np.average(checkpoint_predictions, weights=weights, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ENSEMBLE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=0)\n    elif type_ == \"harmonic\":\n        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n        return 1 / res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=0\n        )\n        res = np.exp(numerator / sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n        return res / (len(res) + 1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model1 0.97301\n#predictions = ensemble_predictions([predictions_bert, predictions_lstm], weights=[0.6, 0.4], type_='rank') \n# model\npredictions = ensemble_predictions([predictions_bert, predictions_lstm], weights=[0.6, 0.4], type_='rank')\n#predictions = ensemble_predictions([predictions_bert, predictions_lstm], weights=[0.666, 0.334], type_='rank')\n#predictions = ensemble_predictions([predictions_bert, predictions_lstm], weights=[0.666, 0.334], type_='rank') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''submission_lstm = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': predictions_lstm\n})\nsubmission_bert = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': predictions_bert\n})\npredictions = submission_lstm['prediction'].rank(pct=True)*0.4 + submission_bert['prediction'].rank(pct=True)*0.6'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}