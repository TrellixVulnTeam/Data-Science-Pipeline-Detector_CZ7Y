{"cells":[{"metadata":{"_cell_guid":"c34537ca-8b77-da22-7ac7-d5b2d4fc8b0e"},"cell_type":"markdown","source":"### Jigsaw Unintended Bias in Toxicity Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load modules\nimport pandas as pd # Data manipulation\nimport matplotlib.pyplot as plt # Data visualization\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7501ef11-0507-9055-bfe0-797873794917","trusted":true},"cell_type":"code","source":"# Read in the data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some notes for my own reference:\n* train.iloc[167] gets the 167th row\n* [benchmark kernel](https://www.kaggle.com/dborkan/benchmark-kernel))\n* I think it would be cool to parse the url: [like this](https://stackoverflow.com/questions/9626535/get-protocol-host-name-from-url). It seems like some URLs would always be toxic (e.g. conspiracy sites, nazi shit)\n"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n\n# Function to convert tweet text to bag of words :)\ndef tweet_to_words(raw_tweet):\n    tweet_text = BeautifulSoup(raw_tweet, \"lxml\").get_text() # Remove HTML\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", tweet_text) # Remove any non-letter characters\n    words = letters_only.lower().split() # Convert to lower case and split around spaces\n    stops = set(stopwords.words(\"english\")) # Make set from list -- faster to search!\n    meaningful_words = [w for w in words if not w in stops] # Remove stopwords\n    return(\" \".join(meaningful_words)) # Join into one string around spaces\n\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Make list of words from tweets\ndef list_of_words(data):\n\n    # Ensure that comment_text entries are all strings\n    data['comment_text'] = data['comment_text'].astype(str)\n    \n    num_tweets = data[\"comment_text\"].size\n    clean_tweets = []\n\n    for i in range(0, num_tweets):\n        clean_tweets.append(tweet_to_words(data[\"comment_text\"][i]))\n        if(i % 500 == 0):\n            print(\"at observation {}\".format(i))\n            \n    return clean_tweets\n\n# Convert list of tweets to features based on bag of words of size n\ndef word_counts(clean_tweets, n):\n    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=n)\n    train_data_features = vectorizer.fit_transform(clean_tweets)\n    return train_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Smaller\n#smaller_train = train.head(5000) # Just use 50,000 rows for now\n#smaller_clean = list_of_words(smaller_train)\n#clean_counts = word_counts(smaller_clean, 200)\n\n# All training data\nclean = list_of_words(train)\nclean_counts = word_counts(clean, 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now train a random forest classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do this to fix weird error\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(train[\"target\"]) # Use smaller_train if doing so\n\nprint(\"scores encoded\")\n\nforest = RandomForestClassifier(n_estimators = 50) # 100 trees\nprint(\"forest made\")\nforest = forest.fit(clean_counts, training_scores_encoded) # Features are word vectors, target is toxicity\nprint(\"forest fitted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now run on test data to make submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_clean = list_of_words(test)\ntest_features = word_counts(test_clean, 200)\n\n# Use random forest to make sentiment label predictions\nresult = forest.predict(test_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\noutput = pd.DataFrame(data = {\"id\":test[\"id\"], \"prediction\":result})\n\n# Use pandas to write output csv file\noutput.to_csv(\"submission.csv\", index=False, quoting=3)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}