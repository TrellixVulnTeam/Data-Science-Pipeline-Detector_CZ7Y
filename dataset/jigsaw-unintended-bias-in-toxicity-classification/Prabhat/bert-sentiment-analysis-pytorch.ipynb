{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pytorch-transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#from __future__ import print_function, division\nimport torch.nn as nn\nimport torch\nfrom pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM,BertConfig\nimport logging\n\n#from __future__ import print_function, division\n\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nimport os\nimport copy\nimport gc\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nlogging.basicConfig(level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/pretrained-bert-including-scripts/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsample = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n#sample = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n#sample = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[0:50000] \ntest = test[0:5000] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-uncased-large-pytorch/bert-large-uncased-pytorch_model.bin'\nBERT_CONFIG = '../input/bert-uncased-large-pytorch/bert-large-uncased-config.json'\n#BERT_TOKEN = '../input/bert-uncased-large-pytorch/bert-large-uncased-vocab.txt'\nBERT_TOKEN = '../input/pretrained-bert-including-scripts/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/vocab.txt'\nBERT_ALL_MODEL = '../input/pretrained-bert-including-scripts/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16'\nBERT_CONFIG_NEW = '../input/pretrained-bert-including-scripts/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/bert_config.json'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n            \"\"\"\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n        \n\nclass BertForSequenceClassification(nn.Module):\n    def __init__(self, modelpath,num_labels=2):\n        super(BertForSequenceClassification, self).__init__()\n        self.num_labels = num_labels\n        self.bert = BertModel.from_pretrained(modelconfig.modelpath)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        nn.init.xavier_normal_(self.classifier.weight)\n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits\n    def freeze_bert_encoder(self):\n        for param in self.bert.parameters():\n            param.requires_grad = False\n    \n    def unfreeze_bert_encoder(self):\n        for param in self.bert.parameters():\n            param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Review_Dataset(Dataset):\n    def __init__(self,config,review_label_list,tokenizer):   \n        self.review_label_list = review_label_list\n        #self.max_seq_length = max_seq_length\n        self.modelconfig = config\n        self.tokenizer = tokenizer\n        \n    def __getitem__(self,index):\n        #print(self.modelconfig.tokenizer)\n        tokenized_review = self.tokenizer.tokenize(self.review_label_list[0][index])\n        if len(tokenized_review) > self.modelconfig.max_seq_length:\n            tokenized_review = tokenized_review[:self.modelconfig.max_seq_length]\n        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n        padding = [0] * (self.modelconfig.max_seq_length - len(ids_review))\n        ids_review += padding\n        assert len(ids_review) == self.modelconfig.max_seq_length\n        ids_review = torch.tensor(ids_review)\n        labels = self.review_label_list[1][index]        \n        list_of_labels = [torch.from_numpy(np.array(labels))]\n        return ids_review, list_of_labels[0]\n    \n    def __len__(self):\n        return len(self.review_label_list[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler,num_epochs=25):\n    since = time.time()\n    \n    print('starting')\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 100\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()\n            else:\n                model.eval()\n            running_loss = 0.0\n            labels_corrects = 0\n            for inputs, labels in dataloaders_dict[phase]:\n                inputs = inputs.to(device) \n                labels = labels.to(device)\n                labels = labels.float()\n                #inputs = inputs.float()\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    #print(model)\n                    outputs = model(inputs)\n                    outputs = F.softmax(outputs,dim=1)\n                    outputs = outputs.float()\n                    #loss = criterion(outputs, torch.max(labels.float(), 1)[1])\n                    #print(labels)\n                    #print(outputs)\n                    loss = criterion(outputs, labels)\n                    if phase == 'train':                       \n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                labels_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(labels, 1)[1])\n            epoch_loss = running_loss / dataset_sizes[phase]\n            labels_acc = labels_corrects.double() / dataset_sizes[phase]\n            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n            print('{} label_acc: {:.4f}'.format(phase, labels_acc))\n            if phase == 'val' and epoch_loss < best_loss:\n                print('saving with loss of {}'.format(epoch_loss),'improved over previous {}'.format(best_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), 'bert_model_test.pth')\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(float(best_loss)))\n    model.load_state_dict(best_model_wts)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef divide_dataset(maindf,reviewcol,labelcollist,batch_size,tokenizer,testsize=0.20):\n    review= maindf[reviewcol]\n    label = maindf[labelcollist]\n    review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=testsize, random_state=42) \n    #print(review_train.shape, review_test.shape, label_train.shape, label_test.shape)\n    review_train = review_train.values.tolist()\n    review_test = review_test.values.tolist()\n    label_train = label_train.values.tolist()\n    label_test = label_test.values.tolist()\n    #print(review_train[0:4])\n    #print(review_test[0:4])\n    #print(label_train[0:4])\n    #print(label_test[0:4])\n    \n    #label_train = pd.get_dummies(y_train).values.tolist()\n    #label_test = pd.get_dummies(y_test).values.tolist()\n    #train_lists = [review_train, label_train]\n    #test_lists = [label_train, label_test]\n    training_dataset = Review_Dataset(modelconfig,[review_train, label_train],tokenizer )\n    test_dataset = Review_Dataset(modelconfig,[review_test, label_test],tokenizer)\n    \n    dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n                   }\n    dataset_sizes = {'train':len(review_train),'val':len(review_test)}\n    gc.enable()\n    del maindf,training_dataset,test_dataset,review_train, review_test, label_train, label_test\n    gc.collect()\n    return dataloaders_dict,dataset_sizes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelconfig = Config(\n    review_column='comment_text',\n    label_columns = ['severe_toxicity','obscene','threat','insult','identity_attack','sexual_explicit'],\n    batch_size=32,\n    test_size = 0.2,\n    max_seq_length = 256,\n    num_epochs = 3,\n    modelpath = 'bert-base-uncased',\n    vocabpath = 'bert-base-uncased',\n    \n    \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(modelconfig.vocabpath)\ndataloaders_dict,dataset_sizes = divide_dataset(train,modelconfig.review_column,modelconfig.label_columns,\n                                                modelconfig.batch_size,tokenizer,modelconfig.test_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrlast = .001\nlrmain = .00001\nconfig = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\nnum_labels = 6\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification(modelpath=modelconfig.modelpath,num_labels=num_labels)\nmodel = model.to(device)\noptim1 = optim.Adam(\n    [\n        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n       \n   ])\n\n#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n# Observe that all parameters are being optimized\noptimizer_ft = optim1\ncriterion = nn.BCEWithLogitsLoss()\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}