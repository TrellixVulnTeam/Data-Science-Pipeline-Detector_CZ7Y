{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Please install the packages in this way :\n# It will be directly installed in current enviorenmet \n#import sys\n#!{sys.executable} -m pip install pydot\n#!{sys.executable} -m pip install sklearn\n#!{sys.executable} -m pip install bert-tensorflow","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport keras.layers as layers\nimport pydot\n\nfrom collections import Counter\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\n#from tensorflow.python.keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import LearningRateScheduler\n\nfrom keras.layers import Input, Embedding, BatchNormalization, LSTM, Dense, Concatenate,Bidirectional\nfrom keras.models import Model\n\nfrom keras.utils import plot_model","execution_count":2,"outputs":[{"output_type":"stream","text":"WARNING: Logging before flag parsing goes to stderr.\nW0606 08:03:15.648323 139765616489856 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce TensorFlow logging output.\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n# Instantiate the elmo model\nelmo_module = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=False)\n\n# Initialize session\n\n#K.set_session(sess)\n\n#K.set_learning_phase(1)\n\n#sess.run(tf.global_variables_initializer())\n#sess.run(tf.tables_initializer())","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's import the dataset here\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n#train=train.sample(100000)\n#test=test.sample(10000)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (len(train))","execution_count":6,"outputs":[{"output_type":"stream","text":"1804874\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## using sklearn for splitting the dataset in train and validation set\n## test set has been already provided\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(train,test_size = 0.05, random_state = 23)    ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lengh of the validation set\nprint (len(val))","execution_count":8,"outputs":[{"output_type":"stream","text":"90244\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This function if for preprocessing the dataset\ndef preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking if GPU is working fine or not\n\nimport tensorflow as tf\ntf.test.is_gpu_available()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing the parameter of max word length, I am taking it 100 to make algorithm faster\ntime_steps = 100\n\n\n# building vocabulary from dataset\ndef build_vocabulary(sentence_list):\n    unique_words = \" \".join(sentence_list).strip().split()\n    word_count = Counter(unique_words).most_common()\n    vocabulary = {}\n    for word, _ in word_count:\n        vocabulary[word] = len(vocabulary)        \n\n    return vocabulary\n\n\n# Get vocabulary vectors from document list\n# Vocabulary vector, Unknown word is 1 and padding is 0\n# INPUT: raw sentence list\n# OUTPUT: vocabulary vectors list\ndef get_voc_vec(document_list, vocabulary):    \n    voc_ind_sentence_list = []\n    for document in document_list:\n        voc_idx_sentence = []\n        word_list = document.split()\n        \n        for w in range(time_steps):\n            if w < len(word_list):\n                # pickup vocabulary id and convert unknown word into 1\n                voc_idx_sentence.append(vocabulary.get(word_list[w], -1) + 2)\n            else:\n                # padding with 0\n                voc_idx_sentence.append(0)\n            \n        voc_ind_sentence_list.append(voc_idx_sentence)\n        \n    return np.array(voc_ind_sentence_list)\n\n\nvocabulary = build_vocabulary(train['comment_text'])\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## these are the identity columns, \n\nidentity_columns = ['asian', 'atheist',\n       'bisexual', 'black', 'buddhist', 'christian', 'female',\n       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n       'muslim', 'other_disability', 'other_gender',\n       'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']\n\ny_identities_train = (train[identity_columns] >= 0.5).astype(int).values\ny_identities_val = (val[identity_columns] >= 0.5).astype(int).values","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_labelling(identity_columns,train):  \n    '''\n    credits go to: https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution/ \n    '''\n    #identity_columns = ['asian', 'atheist',\n    #   'bisexual', 'black', 'buddhist', 'christian', 'female',\n    #   'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n    #   'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n    #   'muslim', 'other_disability', 'other_gender',\n    #   'other_race_or_ethnicity', 'other_religion',\n    #   'other_sexual_orientation', 'physical_disability',\n    #   'psychiatric_or_mental_illness', 'transgender', 'white']\n       \n    # Overall\n    weights = np.ones((len(train),)) / 4\n    # Subgroup\n    weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n    # Background Positive, Subgroup Negative\n    weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    # Background Negative, Subgroup Positive\n    weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    loss_weight = 1.0 / weights.mean()\n    #y_train = (train['target'].values>=0.5).astype(np.int).T\n    y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n    Y_n_t=np.hstack([y_train, y_aux_train])\n    return Y_n_t, loss_weight","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nY_n_t, loss_weight_train = run_labelling(identity_columns,train)\nY_n_v, loss_weight_val = run_labelling(identity_columns,val)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del y_train\nprint(Y_n_t)","execution_count":15,"outputs":[{"output_type":"stream","text":"[[0.         0.25       0.         ... 0.16666667 0.16666667 0.        ]\n [0.         0.75       0.16666667 ... 0.16666667 0.         0.        ]\n [0.         0.25       0.16666667 ... 0.         0.16666667 0.        ]\n ...\n [0.         0.75       0.         ... 0.         0.         0.        ]\n [0.         0.25       0.         ... 0.         0.         0.        ]\n [0.         0.25       0.         ... 0.         0.         0.        ]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mini-batch generator\ndef batch_iter(data, labels, batch_size, shuffle=False):\n    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n    print(\"batch_size\", batch_size)\n    print(\"num_batches_per_epoch\", num_batches_per_epoch)\n\n    def data_generator():\n        data_size = len(data)\n\n        while True:\n            # Shuffle the data at each epoch\n            if shuffle:\n                shuffle_indices = np.random.permutation(np.arange(data_size))\n                shuffled_data = data[shuffle_indices]\n                shuffled_labels = labels[shuffle_indices]\n            else:\n                shuffled_data = data\n                shuffled_labels = labels\n\n            for batch_num in range(num_batches_per_epoch):\n                start_index = batch_num * batch_size\n                end_index = min((batch_num + 1) * batch_size, data_size)\n                ###\n                shuffled_D=shuffled_data[start_index: end_index]\n                shuffled_D.tolist()\n                shuffled_D = [' '.join(t.split()[0:time_steps]) for t in shuffled_D]\n                shuffled_D = np.array(shuffled_D)                \n                X_voc = get_voc_vec(shuffled_D, vocabulary)\n                                \n                sentence_split_list = []\n                sentence_split_length_list = []\n                \n                \n\n                for sentence in shuffled_D:\n                    \n                    \n                    \n                    sentence_split = sentence.split()\n                    sentence_split_length = len(sentence_split)\n                    sentence_split += [\"NaN\"] * (time_steps - sentence_split_length)\n                    \n                    sentence_split_list.append((\" \").join(sentence_split))\n                    sentence_split_length_list.append(sentence_split_length)\n        \n                X_elmo = np.array(sentence_split_list)\n\n                X = [X_voc, X_elmo]\n                \n                y = shuffled_labels[start_index: end_index]\n                y_train=y[:,[0, 1]]\n                y_aux_train=y[:,[2,3,4,5,6,7]]\n                \n                Lables=[y_train,y_aux_train]\n                yield X,Lables \n\n    return num_batches_per_epoch, data_generator()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create datasets (Only take up to time_steps words for memory)\n\n#train_text = train['comment_text'].tolist()\n#train_text = [' '.join(t.split()[0:time_steps]) for t in train_text]\n#train_text = np.array(train_text)\n#train_label = np.array(train_df['polarity'].tolist())\n#train_label=(train['target'].values>=0.5).astype(np.int).T\n\n#val_text = val['comment_text'].tolist()\n#val_text = [' '.join(t.split()[0:time_steps]) for t in val_text]\n#val_text = np.array(val_text)\n#train_label = np.array(train_df['polarity'].tolist())\n#val_label =(val['target'].values>=0.5).astype(np.int).T\n\n\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# mini-batch size\nbatch_size = 256\n\ntrain_steps, train_batches = batch_iter(train['comment_text'],\n                                        Y_n_t,\n                                        batch_size)\nvalid_steps, valid_batches = batch_iter(val['comment_text'],\n                                        Y_n_v,\n                                        batch_size)\n\n","execution_count":18,"outputs":[{"output_type":"stream","text":"batch_size 256\nnum_batches_per_epoch 6698\nbatch_size 256\nnum_batches_per_epoch 353\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_aux_targets=Y_n_t.shape[-1]-2\n\ndel train,  val,  Y_n_t","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embed elmo method\ndef make_elmo_embedding(x):\n    embeddings = elmo_module(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"elmo\"]\n    \n    return embeddings","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.losses import binary_crossentropy\n\n#from keras.losses import binary_crossentropy\ndef custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 768\n\n#num_aux_targets=Y_n_t.shape[-1]-2\nloss_weight =loss_weight_train\n\n# elmo embedding dimension\nelmo_dim = 1024\ntime_steps = 100\n\n# Input Layers\n\n#word_input = Input(shape=(100,), dtype='int32')  # (batch_size, sent_length)\nword_input = Input(shape=(100, ), dtype='int32')  # (batch_size, sent_length)\n#elmo_input = Input(shape=(100,), dtype='string')  # (batch_size, sent_length, elmo_size)\nelmo_input = Input(shape=(None, ), dtype='string')  # (batch_size, sent_length, elmo_size)\n\n\n# Hidden Layers\nword_embedding = Embedding(input_dim=len(vocabulary), output_dim=128, mask_zero=False)(word_input)\nelmo_embedding = layers.Lambda(make_elmo_embedding, output_shape=(100, elmo_dim))(elmo_input)\nword_embedding = Concatenate()([word_embedding, elmo_embedding])\nword_embedding = BatchNormalization()(word_embedding)\n\nx = SpatialDropout1D(0.3)(word_embedding)\n#x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,merge_mode='concat')(word_embedding))\n#model.add( Bidirectional( LSTM(lstm_out = 196, dropout_U = 0.2, dropout_W = 0.2)))\n\n\n    \nx = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\nx = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n\natt = Attention(100)(x)\nhidden = concatenate([att, GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n\nhidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\nhidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\nresult = Dense(1, activation='sigmoid')(hidden)\naux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \nmodel = Model(inputs=[word_input, elmo_input], outputs=[result, aux_result])\nmodel.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n\n\n\n\n\n##dense = Dense(1, activation='sigmoid')(hidden)\n# Output Layer\n#predict = Dense(units=1, activation='sigmoid')(x)\n#pred = layers.Dense(1, activation='sigmoid')(x) #### num Aux target\n#aux_result = layers.Dense(num_aux_targets, activation='sigmoid')(x)\n\n\n\n#model = Model(inputs=[word_input, elmo_input], outputs=pred)\n#model = Model(inputs=[word_input, elmo_input], outputs=[pred,aux_result])\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n#model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight,1.0], optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nplot_model(model, to_file=\"model.png\", show_shapes=True)\n","execution_count":23,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 100)          0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, None)         0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 128)     206828928   input_1[0][0]                    \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 100, 1024)    0           input_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 100, 1152)    0           embedding_1[0][0]                \n                                                                 lambda_1[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 100, 1152)    4608        concatenate_1[0][0]              \n__________________________________________________________________________________________________\nspatial_dropout1d_1 (SpatialDro (None, 100, 1152)    0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 100, 256)     1312768     spatial_dropout1d_1[0][0]        \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) (None, 100, 256)     395264      bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nattention_1 (Attention)         (None, 256)          356         bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 768)          0           attention_1[0][0]                \n                                                                 global_max_pooling1d_1[0][0]     \n                                                                 global_average_pooling1d_1[0][0] \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 768)          590592      concatenate_2[0][0]              \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 768)          0           concatenate_2[0][0]              \n                                                                 dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 768)          590592      add_1[0][0]                      \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 768)          0           add_1[0][0]                      \n                                                                 dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            769         add_2[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 6)            4614        add_2[0][0]                      \n==================================================================================================\nTotal params: 209,728,491\nTrainable params: 209,726,187\nNon-trainable params: 2,304\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid_batches)","execution_count":25,"outputs":[{"output_type":"stream","text":"<generator object batch_iter.<locals>.data_generator at 0x7f1cff7015c8>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#from tensorflow.python.keras.callbacks import TensorBoard, LearningRateScheduler\nNUM_EPOCHS = 1\nlogfile_path = './log'\ntb_cb = TensorBoard(log_dir=logfile_path, histogram_freq=0)\ncheckpoint_predictions = []\ncheckpoint_val_preds = []\nweights = []\nNUM_MODELS = 1\nsess = tf.Session()\ninitialize_vars(sess)\nfor model_idx in range(NUM_MODELS):\n    for global_epoch in range(NUM_EPOCHS):\n        history = model.fit_generator(train_batches, train_steps,\n                              epochs=2, \n                              validation_data=valid_batches,\n                              validation_steps=valid_steps,\n                              #callbacks=[tb_cb]\n                              callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))]\n                              )\n        \n        #checkpoint_predictions.append([0].flatten())\n        #checkpoint_val_preds.append(model.predict(x_train[val_ind], batch_size=2048)[0].flatten())\n        #print(model.predict_generator(valid_batches, steps=valid_steps))\n        checkpoint_val_preds.append(model.predict_generator(valid_batches, steps=valid_steps)[0].flatten())\n        weights.append(2 ** global_epoch)\n        \n        #[0].flatten()\n        \n#","execution_count":26,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:107: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 206828928 elements. This may consume a large amount of memory.\n  num_elements)\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/2\n6698/6698 [==============================] - 14764s 2s/step - loss: 0.3212 - dense_3_loss: 0.0658 - dense_4_loss: 0.1112 - val_loss: 0.3006 - val_dense_3_loss: 0.0606 - val_dense_4_loss: 0.1071\nEpoch 2/2\n6698/6698 [==============================] - 14775s 2s/step - loss: 0.2478 - dense_3_loss: 0.0453 - dense_4_loss: 0.1034 - val_loss: 0.3255 - val_dense_3_loss: 0.0680 - val_dense_4_loss: 0.1085\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(checkpoint_val_preds)","execution_count":27,"outputs":[{"output_type":"stream","text":"[array([0.00033218, 0.00017622, 0.00011253, ..., 0.01226839, 0.00314671,\n       0.00031343], dtype=float32)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('ElmoModel.h5')\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before this please rerun the model building steps \n#initialize_vars(sess)\n#model.load_weights('BertModel.h5')","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = np.average(checkpoint_val_preds, weights=weights, axis=0)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val_preds)\nprint(len(val_preds))","execution_count":31,"outputs":[{"output_type":"stream","text":"[0.00033218 0.00017622 0.00011253 ... 0.01226839 0.00314671 0.00031343]\n90244\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef power_mean(x, p=2):\n    return np.power(np.mean(np.power(x, p)),1/p)\n\ndef get_s_auc(y_true,y_pred,y_identity):\n    mask = y_identity==1\n    try:\n        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        s_auc = 1\n    return s_auc\n\ndef get_bpsn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n    try:\n        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bpsn_auc = 1\n    return bpsn_auc\n\ndef get_bspn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n    try:\n        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bspn_auc = 1\n    return bspn_auc\n\ndef get_total_auc(y_true,y_pred,y_identities):\n\n    N = y_identities.shape[1]\n    \n    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n\n    M_s_auc = power_mean(saucs)\n    M_bpsns_auc = power_mean(bpsns)\n    M_bspns_auc = power_mean(bspns)\n    rauc = roc_auc_score(y_true,y_pred)\n\n\n    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + rauc\n    total_auc/= 4\n\n    return total_auc","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val=Y_n_v[:,[0, 1]]\ny_val[:,0].shape, val_preds.shape,y_identities_val.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"((90244,), (90244,), (90244, 24))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mini-batch generator\ndef test_batch_iter(data, batch_size, shuffle=False):\n    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n    print(\"batch_size\", batch_size)\n    print(\"num_batches_per_epoch\", num_batches_per_epoch)\n\n    def data_generator():\n        data_size = len(data)\n\n        while True:\n            # Shuffle the data at each epoch\n            if shuffle:\n                shuffle_indices = np.random.permutation(np.arange(data_size))\n                shuffled_data = data[shuffle_indices]\n                #shuffled_labels = labels[shuffle_indices]\n            else:\n                shuffled_data = data\n                #shuffled_labels = labels\n\n            for batch_num in range(num_batches_per_epoch):\n                start_index = batch_num * batch_size\n                end_index = min((batch_num + 1) * batch_size, data_size)\n                ###\n                shuffled_D=shuffled_data[start_index: end_index]\n                shuffled_D.tolist()\n                shuffled_D = [' '.join(t.split()[0:time_steps]) for t in shuffled_D]\n                shuffled_D = np.array(shuffled_D)                \n                X_voc = get_voc_vec(shuffled_D, vocabulary)\n                                \n                sentence_split_list = []\n                sentence_split_length_list = []\n                \n                \n\n                for sentence in shuffled_D:\n                    \n                    \n                    \n                    sentence_split = sentence.split()\n                    sentence_split_length = len(sentence_split)\n                    sentence_split += [\"NaN\"] * (time_steps - sentence_split_length)\n                    \n                    sentence_split_list.append((\" \").join(sentence_split))\n                    sentence_split_length_list.append(sentence_split_length)\n        \n                X_elmo = np.array(sentence_split_list)\n\n                X = [X_voc, X_elmo]\n                \n                #y = shuffled_labels[start_index: end_index]\n                #y_train=y[:,[0, 1]]\n                #y_aux_train=y[:,[2,3,4,5,6,7]]\n                \n                #Lables=[y_train,y_aux_train]\n                #yield X,Lables\n                yield X\n\n    return num_batches_per_epoch, data_generator()","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_steps, test_batches = test_batch_iter(test['comment_text'],\n                                        \n                                        batch_size)\n","execution_count":37,"outputs":[{"output_type":"stream","text":"batch_size 256\nnum_batches_per_epoch 381\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_generator(test_batches, steps=test_steps)[0].flatten()","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('../input/sample_submission.csv')\ndf_submit.prediction = predictions\ndf_submit.to_csv('submission.csv', index=False)","execution_count":42,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"tf_gpu140","language":"python","name":"tf_gpu140"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}