{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train=pd.read_csv('../input/train.csv').sample(100000,random_state=0)\ndf_train=pd.read_csv('../input/train.csv', nrows=100000)\ndf_train['label']=np.where(df_train.target>=0.5,1,0)\ndf_train['label']=df_train['label'].astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# df_train=pd.read_csv('../input/data-augmentation/train.csv')\ndf_test=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test.shape,df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n    'can;t': 'cannot','can;t;ve': 'cannot have',\n    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not','doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n    \"he'd've\": 'he would have',\"he'll\": 'he will',\n    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not','isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not','oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is','what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ','ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n    }\n\ncontraction_mapping=dict((k.lower(), v.lower()) for k,v in contraction_mapping.items())\n\n# df_train['treated_comment']=df_train['comment_text']\ndef lowercase(sen):\n    y=\" \".join(x.lower() for x in sen.split())\n    return y\n\n\ndef clean_contractions(text, mapping):   # 总觉得哪里不对....有待细化\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\n\n# df_train['treated_comment'].head()\n\nimport re\ndef clean_text(x):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', x) # re.sub is time-comsuming\n    return text\n\n\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\ndef stop_remove(sen):\n    y=\" \".join(x for x in sen.split() if x not in stop)\n    return y\n\ndef number_remove(x):\n    y=re.sub('\\d','',x)\n    return y\n\n               \ndef preprocess(x):\n    x=lowercase(x)\n    x=clean_contractions(x,contraction_mapping)\n    x=clean_text(x)\n    x=stop_remove(x)\n    x=number_remove(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_train['comment_text']=df_train['comment_text'].apply(preprocess)\n\n# df_train['label'] = np.where(df_train['target'] >= .5, 1, 0)\n# df_train['label']=df_train['label'].astype('int8')\n\ndf_test['comment_text']=df_test['comment_text'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train=df_train.sample(100000,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation "},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom random import shuffle\nrandom.seed(1)\n\n#stop words list\nstop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n\t\t\t'into', 'through', 'during', 'before', 'after', \n\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n\t\t\t'should', 'now', '']\n\n#cleaning up text\nimport re\ndef get_only_chars(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"’\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    return clean_line\n\n########################################################################\n# Synonym replacement\n# Replace n words in the sentence with synonyms from wordnet\n########################################################################\n\n#for the first time you use wordnet\n#import nltk\n#nltk.download('wordnet')\nfrom nltk.corpus import wordnet \n\ndef synonym_replacement(words, n):\n\tnew_words = words.copy()\n\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n\trandom.shuffle(random_word_list)\n\tnum_replaced = 0\n\tfor random_word in random_word_list:\n\t\tsynonyms = get_synonyms(random_word)\n\t\tif len(synonyms) >= 1:\n\t\t\tsynonym = random.choice(list(synonyms))\n\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n\t\t\tnum_replaced += 1\n\t\tif num_replaced >= n: #only replace up to n words\n\t\t\tbreak\n\n\t#this is stupid but we need it, trust me\n\tsentence = ' '.join(new_words)\n\tnew_words = sentence.split(' ')\n\n\treturn new_words\n\ndef get_synonyms(word):\n\tsynonyms = set()\n\tfor syn in wordnet.synsets(word): \n\t\tfor l in syn.lemmas(): \n\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n\t\t\tsynonyms.add(synonym) \n\tif word in synonyms:\n\t\tsynonyms.remove(word)\n\treturn list(synonyms)\n\n########################################################################\n# Random deletion\n# Randomly delete words from the sentence with probability p\n########################################################################\n\ndef random_deletion(words, p):\n\n\t#obviously, if there's only one word, don't delete it\n\tif len(words) == 1:\n\t\treturn words\n\n\t#randomly delete words with probability p\n\tnew_words = []\n\tfor word in words:\n\t\tr = random.uniform(0, 1)\n\t\tif r > p:\n\t\t\tnew_words.append(word)\n\n\t#if you end up deleting all words, just return a random word\n\tif len(new_words) == 0:\n\t\trand_int = random.randint(0, len(words)-1)\n\t\treturn [words[rand_int]]\n\n\treturn new_words\n\n########################################################################\n# Random swap\n# Randomly swap two words in the sentence n times\n########################################################################\n\ndef random_swap(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tnew_words = swap_word(new_words)\n\treturn new_words\n\ndef swap_word(new_words):\n\trandom_idx_1 = random.randint(0, len(new_words)-1)\n\trandom_idx_2 = random_idx_1\n\tcounter = 0\n\twhile random_idx_2 == random_idx_1:\n\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n\t\tcounter += 1\n\t\tif counter > 3:\n\t\t\treturn new_words\n\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n\treturn new_words\n\n########################################################################\n# Random insertion\n# Randomly insert n words into the sentence\n########################################################################\n\ndef random_insertion(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tadd_word(new_words)\n\treturn new_words\n\ndef add_word(new_words):\n\tsynonyms = []\n\tcounter = 0\n\twhile len(synonyms) < 1:\n\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n\t\tsynonyms = get_synonyms(random_word)\n\t\tcounter += 1\n\t\tif counter >= 10:\n\t\t\treturn\n\trandom_synonym = synonyms[0]\n\trandom_idx = random.randint(0, len(new_words)-1)\n\tnew_words.insert(random_idx, random_synonym)\n\n########################################################################\n# main data augmentation function\n########################################################################\n\ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n\t\n\tsentence = get_only_chars(sentence)\n\twords = sentence.split(' ')\n\twords = [word for word in words if word is not '']\n\tnum_words = len(words)\n\t\n\taugmented_sentences = []\n\tnum_new_per_technique = int(num_aug/4)+1\n\tn_sr = max(1, int(alpha_sr*num_words))\n\tn_ri = max(1, int(alpha_ri*num_words))\n\tn_rs = max(1, int(alpha_rs*num_words))\n\n\t#sr\n\tfor _ in range(num_new_per_technique):\n\t\ta_words = synonym_replacement(words, n_sr)\n\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#ri\n\tfor _ in range(num_new_per_technique):\n\t\ta_words = random_insertion(words, n_ri)\n\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rs\n\tfor _ in range(num_new_per_technique):\n\t\ta_words = random_swap(words, n_rs)\n\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rd\n\tfor _ in range(num_new_per_technique):\n\t\ta_words = random_deletion(words, p_rd)\n\t\taugmented_sentences.append(' '.join(a_words))\n\n\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n\tshuffle(augmented_sentences)\n\n\t#trim so that we have the desired number of augmented sentences\n\tif num_aug >= 1:\n\t\taugmented_sentences = augmented_sentences[:num_aug]\n\telse:\n\t\tkeep_prob = num_aug / len(augmented_sentences)\n\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n\t#append the original sentence\n# \taugmented_sentences.append(sentence)\n\n\treturn augmented_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.comment_text.iloc[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda(df_train.comment_text.iloc[4], alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aug_toxic(trainDF,sr=0.3,ri=0.1,rs=0.1,rd=0.1,num_aug=4):\n    aug=[]\n    for i in range(trainDF.shape[0]):\n        aug_textlist=eda(trainDF.comment_text.iloc[i], \n                         alpha_sr=sr,\n                         alpha_ri=ri, \n                         alpha_rs=rs, \n                         p_rd=rd, \n                         num_aug=num_aug)\n        aug.extend(aug_textlist)\n#     return aug\n    df_aug=pd.DataFrame({'comment_text':aug,'label':np.ones((len(aug),),dtype='int8')})\n\n    return df_aug\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ndf_aug=aug_toxic(df_train[df_train.label==1]) # Toxic augment dataframe \n\n# df_augment=df_aug.append(df_train[['comment_text','label']][df_train.label==0]) # concat the augment and the non-toxic\n\n# df_train=df_augment\ndf_augment=df_train[['comment_text','label']].append(df_aug)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n\n# split the dataset into training and validation datasets \ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_augment['comment_text'], df_augment['label']\n                                                                      ,test_size=0.25,random_state=1)\n\n# label encode the target variable \nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_x.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# word level tf-idf \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer1=TfidfVectorizer(ngram_range=(1,2), \n#                             tokenizer=tokenize,\n                      min_df=3, strip_accents='unicode', use_idf=1,\n                      smooth_idf=1, sublinear_tf=1\n                        ,max_features=valid_x.shape[0]\n                           )\n# vectorizer1 = TfidfVectorizer()\ntrain_vectors = vectorizer1.fit_transform(train_x)\nvalid_vectors = vectorizer1.transform(valid_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of train and test is :',\n      train_vectors.shape, valid_vectors.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=2)\n# X_train_res, y_train_res = sm.fit_sample(train_vectors, train_y.ravel()) # 对训练集的tfidf用smote过采样\n# print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n# print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\n# print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n# print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NB-SVM"},{"metadata":{},"cell_type":"markdown","source":"## Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\n# the basic naive-bayes feature equation\ndef NBSVM_predict(x,TV_x):\n    '''\n    x: train tf-idf ,default:train_vectors\n    TV_x: tf-idf of the  you want :valid vectors or  test vectors\n    '''\n    \n    def pr(y_i, y):\n        p = x[y == y_i].sum(0)\n        \n        return (p+1) / ((y == y_i).sum()+1)\n\n\n    # fit a model for one independent at a time\n    def get_mdl(y):\n    #     y = y.values\n        r = np.log(pr(1, y) / pr(0, y))\n        m = LogisticRegression(C=4, dual=True)\n        x_nb = x.multiply(r)\n        return m.fit(x_nb, y), r\n\n    labels = [\"label\"]\n    # the variable to store predictions\n    pred_valid = np.zeros((TV_x.shape[0], len(labels)))\n\n    for i, j in enumerate(labels):\n        print('fit', j)\n        t = (train_y == 1)*1\n        m, r = get_mdl(t)\n        print(TV_x.shape)\n        print(r.shape)\n        pred_valid[:, i] = m.predict_proba(TV_x.multiply(r))[:, 1]\n\n\n    y_pred_valid=pred_valid.flatten() # probability of predicted\n    return y_pred_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# define input for funcition-report\ny_pred_valid=NBSVM_predict(train_vectors ,valid_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\nfrom sklearn import metrics\nimport scikitplot \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\ndef report(y_true,y_pred_valid):\n    '''\n    y_true: true label\n    y_pred_valid: predict of valid or test\n    '''\n    \n    print('The accuracy is:',accuracy_score(y_true, y_pred_valid.round()))\n    print('The F1 score is %f'%metrics.f1_score(y_true, y_pred_valid.round())  )\n    print('The recall socre is %f'%metrics.recall_score(y_true, y_pred_valid.round()))\n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_valid)\n    print(\"The AUC score is :\",metrics.auc(fpr, tpr))\n    skplt.metrics.plot_confusion_matrix(y_true, y_pred_valid.round(), normalize=True)\n    \n    y_probas = np.vstack(((1-y_pred_valid),y_pred_valid)).T # predicted probabilities generated by sklearn classifier\n    scikitplot.metrics.plot_roc(y_true, y_probas)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report(valid_y,y_pred_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rate of unintended bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input: text of df_train\ny_pred_train=NBSVM_predict(train_vectors,train_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_overall_x=vectorizer1.transform(df_train['comment_text'])\ntrain_overall_predicted = NBSVM_predict(train_vectors,train_overall_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=df_train.label\nreport(y_true,train_overall_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['predict']=train_overall_predicted.round()\ndf_train['predict']=df_train['predict'].astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identity_columns = [\n#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\nidentity_columns =['asian', 'latino', 'black', 'white', 'other_race_or_ethnicity',\n                   'atheist', 'buddhist', 'hindu', 'jewish', 'muslim', 'christian', \n                   'other_religion', 'female', 'male', 'other_gender', 'heterosexual', \n                   'bisexual', 'transgender', 'homosexual_gay_or_lesbian', 'other_sexual_orientation',\n                   'intellectual_or_learning_disability', 'physical_disability', \n                   'psychiatric_or_mental_illness', 'other_disability']\nidentity=(df_train[identity_columns].fillna(0).values>0.5).sum(axis=1).astype(bool).astype(np.int) \n\ndf_train['identity']=identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len1=df_train[np.logical_and(df_train['label']== 0,df_train['identity']==1)].shape[0]\nlen2=df_train[np.logical_and(df_train['label']== 0,df_train['identity']==1)][df_train['predict']==1].shape[0]\nprint('fpr in indentity',len2/len1*100)\n# df_train[(df_train.label==0)&(df_train.predict==0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len3=df_train[np.logical_and(df_train['label']== 0,df_train['predict']==1)].shape[0]\nlen4=df_train[df_train.label==0].shape[0]\nprint('fpr in all',len3/len4*100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict  "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx = train_vectors\ntest_x = vectorizer1.transform(df_test['comment_text'])\ntest_predicted = NBSVM_predict(x,test_x).round()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# from sklearn.naive_bayes import MultinomialNB,GaussianNB\n# clf1 = MultinomialNB(alpha=1)\n# clf1.fit(train_vectors, train_y)\n# from  sklearn.metrics  import accuracy_score\n# predicted = clf1.predict(valid_vectors)\n\n# accuracy2, auc2 = evaluate(clf1, valid_vectors, valid_y)\n# print(\"测试集正确率：%.4f%%\\n\" % (accuracy2* 100))\n# print(\"测试集AUC值：%.6f\\n\" % (auc2))\n\n# import sklearn\n# from sklearn.metrics import classification_report\n\n# def evaluate(model, X, y):\n#     \"\"\"评估数据集，并返回评估结果，包括：正确率、AUC值\n#     \"\"\"\n#     accuracy = model.score(X, y)\n#     fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, model.predict_proba(X)[:, 1], pos_label=1)\n#     return accuracy, sklearn.metrics.auc(fpr, tpr)\n\n# from sklearn.metrics import confusion_matrix\n# # y_pred_valid=clf1.predict(valid_vectors)\n# # confusion_matrix(valid_y, y_pred_valid)\n# import scikitplot as skplt\n# skplt.metrics.plot_confusion_matrix(valid_y, y_pred_valid.round(), normalize=True)\n\n# # confusion_matrix(valid_y, y_pred_valid)\n# confusion_matrix(valid_y,pred_valid.round())\n\n# import scikitplot \n# import matplotlib.pyplot as plt\n\n# y_true = valid_y# ground truth labels\n# y_probas = clf1.predict_proba(valid_vectors)# predicted probabilities generated by sklearn classifier\n# scikitplot.metrics.plot_roc(y_true, y_probas)\n# plt.show()\n\n# from sklearn import metrics\n# print('The F1 score is %f'%metrics.f1_score(y_true, y_pred_valid)  )\n# print('The recall socre is %f'%metrics.recall_score(y_true, y_pred_valid))\n\n# from sklearn.metrics import classification_report\n# target_names = ['Non-toxic', 'toxic']\n# print(classification_report(y_true, y_pred_valid, target_names=target_names))\n\n\n# test_x=vectorizer1.transform(df_test['comment_text'])\n# test_predicted = clf1.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('../input/sample_submission.csv')\ndf_submit.prediction = test_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}