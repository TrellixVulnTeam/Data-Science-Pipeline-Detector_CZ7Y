{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**: With this kernel, I wished to experiment with the effects of feature engineering on a model designed to minimize bias in toxicity detection using BERT Embeddings + LSTM. \n\nA series of functions used in the kernel are drawn from the original BERT Embeddings + LSTM kernel by Dieter https://www.kaggle.com/christofhenkel/bert-embeddings-lstm/. They are credited to the owner wherever possible."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\nimport os\nimport gc\nimport re\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm, trange\nimport pickle\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n# pytorch bert imports\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n# keras imports\nfrom keras.utils import np_utils\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import CuDNNLSTM, Activation, Dense, Dropout, Input, Embedding, concatenate, Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import SpatialDropout1D, Dropout, add, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nimport keras.layers as L\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PRETRAINED_DIR = '../input/bert-base-uncased-model/'\nINPUT_DIR = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\nBERT_VOCAB_DIR = '../input/bert-base-uncased-vocab-file/vocab.txt'\nMAX_LENGTH = 250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the bert encoded training and test data\ntrain_data = pd.read_csv(INPUT_DIR + 'train.csv')\ntest_data = pd.read_csv(INPUT_DIR + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering for the training data\nregex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\ntrain_data['capitals'] = train_data['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\ntrain_data['exclamation_points'] = train_data['comment_text'].apply(lambda x: len(regex.findall(x)))\ntrain_data['total_length'] = train_data['comment_text'].apply(len)\n\n# Feature Engineering for the test data\ntest_data['capitals'] = test_data['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\ntest_data['exclamation_points'] = test_data['comment_text'].apply(lambda x: len(regex.findall(x)))\ntest_data['total_length'] = test_data['comment_text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = ['capitals','exclamation_points','total_length']\nidentity_columns = ['male','female','homosexual_gay_or_lesbian','christian','jewish','muslim',\n                    'black','white','psychiatric_or_mental_illness']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Customizing the weights\ny_ids= (train_data[identity_columns] >= 0.5).astype(int).values\n# Overall\nweights = np.ones((len(train_data),)) / 4\n# Subgroup\nweights += (train_data[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train_data['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_data[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train_data['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_data[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train_data['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train_data[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conversion of continuous target columns to categorical\nfor column in identity_columns + ['target']:\n    train_data[column]= np.where(train_data[column] >= 0.5, True, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_preprocessing(text):\n    filter_char = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n    text = text.lower()\n    text = text.replace(filter_char,'')\n    text = text.replace('[^a-zA-Z0-9 ]', '')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['comment_text'] = train_data['comment_text'].apply(nlp_preprocessing)\ntest_data['comment_text'] = test_data['comment_text'].apply(nlp_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising BERT tokenizer\ntokenizer = BertTokenizer(vocab_file=BERT_VOCAB_DIR)\ndef tokenization(row):\n    row = tokenizer.tokenize(row)\n    row = tokenizer.convert_tokens_to_ids(row)\n    return row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['comment_text'] = train_data['comment_text'].apply(tokenization)\ntest_data['comment_text'] = test_data['comment_text'].apply(tokenization)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_ids(doc):\n    doc = [str(i) for i in doc]\n    return ' '.join(doc)\ntrain_data['comment_text'] = train_data['comment_text'].apply(string_ids)\ntest_data['comment_text'] = test_data['comment_text'].apply(string_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.zeros((train_data.shape[0],MAX_LENGTH),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(train_data['comment_text']))):\n    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n    inp_len = len(input_ids)\n    x_train[i,:inp_len] = np.array(input_ids)\n    \nx_test = np.zeros((test_data.shape[0],MAX_LENGTH),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(test_data['comment_text']))):\n\n    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n    inp_len = len(input_ids)\n    x_test[i,:inp_len] = np.array(input_ids)\n    \nwith open('temporary.pickle', mode='wb') as f:\n    pickle.dump(x_test, f) # use temporary file to reduce memory\n\n# Removing extra variables to free up the memory\ndel x_test\ndel test_data\ndel train_data\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss_func(y_true, y_preds):\n    loss = binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_preds) * y_true[:,1]\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_PRETRAINED_DIR)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = get_bert_embed_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets, loss_weight):\n    '''\n    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/\n    '''\n    words = Input(shape=(MAX_LENGTH,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.5)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss_func,'binary_crossentropy'], loss_weights=[loss_weight, 1.0],\n                  optimizer=Adam(lr = 0.001))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_idx, val_idx = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nLSTM_UNITS = 128\nHIDDEN_UNITS = 4 * LSTM_UNITS\nmodel_predictions = []\nmodel_val_preds = []\nweights = []\n\n# Model Training and Prediction Phase\nmodel = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\nfor epoch in range(epochs):\n    model.fit(x_train[tr_idx],[y_train[tr_idx], y_aux_train[tr_idx]],\n              validation_data = (x_train[val_idx],[y_train[val_idx], y_aux_train[val_idx]]),\n              batch_size=512,\n              epochs=1,\n              verbose=1,\n              callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** epoch))])\n    with open('temporary.pickle', mode='rb') as f:\n        x_test = pickle.load(f) \n    model_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n    model_val_preds.append(model.predict(x_train[val_idx], batch_size=2048)[0].flatten())\n    del x_test\n    gc.collect()\n    weights.append(2 ** epoch)\ndel model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = np.average(model_val_preds, weights = weights, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Following section is drawn from a set of functions used on https://www.kaggle.com/christofhenkel/bert-embeddings-lstm/ \"\"\"\n\nfrom sklearn.metrics import roc_auc_score\n\ndef get_s_auc(y_true,y_pred,y_identity):\n    mask = y_identity==1\n    try:\n        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        s_auc = 1\n    return s_auc\n\ndef get_bspn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n    try:\n        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bspn_auc = 1\n    return bspn_auc\n\ndef get_bpsn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n    try:\n        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bpsn_auc = 1\n    return bpsn_auc\n\ndef get_total_auc(y_true,y_pred,y_identities):\n    N = y_identities.shape[1]\n    \n    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n\n    M_s_auc = np.power(np.mean(np.power(saucs, -5)),1/-5)\n    M_bpsns_auc = np.power(np.mean(np.power(bpsns, -5)),1/-5)\n    M_bspns_auc = np.power(np.mean(np.power(bspns, -5)),1/-5)\n    r_auc = roc_auc_score(y_true,y_pred)\n    \n    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + r_auc\n    total_auc/= 4\n\n    return total_auc\n\nget_total_auc(y_train[val_idx][:,0],val_preds,y_ids[val_idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submission Stage:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average predictions for the model\npredictions = np.average(model_predictions, weights=weights, axis=0)\n\ndf_submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\ndf_submission.drop(['comment_text'],axis = 1, inplace = True)\ndf_submission['prediction'] = predictions\ndf_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------------------------------------------------------------------------------------------------------------------------"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}