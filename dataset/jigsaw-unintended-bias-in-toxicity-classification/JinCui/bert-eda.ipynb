{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_rows', 20)\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm, tqdm_notebook\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 320\nSEED = 1234\nEPOCHS = 1\nData_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\"\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOXICITY_COLUMN = 'target'\n\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\ntrain_df['comment_text'] = train_df['comment_text'].astype(str) \n\ntest_df = pd.read_csv(os.path.join(Data_dir, \"test.csv\"))\ntest_df['comment_text'] = test_df['comment_text'].astype(str)\n\nprint('loaded %d train data' % len(train_df))\nprint('loaded %d test data' % len(test_df))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\ntrain_df[identity_columns] = (train_df[identity_columns]>=0.5).astype(float)\ntrain_df['target']=(train_df['target']>=0.5).astype(float)\ntrain_df['target'].value_counts()\n\ntrain_df[['target']+identity_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words distribution\nsns.distplot(train_df['comment_text'].apply(lambda t:len(t.split(\" \"))+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\ndef plot_wordcloud(text, mask=None, max_words=300, max_font_size=100, figure_size=(10,12), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n\nplot_wordcloud(train_df[\"comment_text\"], title=\"All Comments\")\nplot_wordcloud(train_df[\"comment_text\"][train_df['target']==1], title=\"Toxic Comments\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n\nsequences = convert_lines(train_df[\"comment_text\"],MAX_SEQUENCE_LENGTH,tokenizer)\nsequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nskf = StratifiedKFold(n_splits=5, random_state=True, shuffle=True)\nsplits = list(skf.split(train_df, train_df['target']))\n\n\nwith open('skf_5_splits.pkl', 'wb') as f:\n    pickle.dump(splits, f)\n    \nwith open('bert_large_sequences_{}.pkl'.format(MAX_SEQUENCE_LENGTH), 'wb') as t:\n    pickle.dump(sequences, t)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}