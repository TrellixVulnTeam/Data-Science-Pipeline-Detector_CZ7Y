{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom keras.preprocessing import text, sequence\nfrom keras import backend as K\nfrom keras.models import load_model\nimport keras\nimport pickle\nprint(K.tensorflow_backend._get_available_gpus())\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train[\"comment_text\"]\nlabel_data = train[\"target\"]\ntest_data = test[\"comment_text\"]\ntrain_data.shape, label_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vectorize Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_data) + list(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = tokenizer.texts_to_sequences(train_data)\ntest_data = tokenizer.texts_to_sequences(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 200\ntrain_data = sequence.pad_sequences(train_data, maxlen=MAX_LEN)\ntest_data = sequence.pad_sequences(test_data, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = max_features or len(tokenizer.word_index) + 1\nmax_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_data), type(label_data.values), type(test_data)\nlabel_data = label_data.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud of train file\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[\"comment_text\"], title=\"Word Cloud of Train Comments\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud of train file of toxic comments\ntrain['target'] = np.where(train['target'] >= 0.5, 1, 0)\ntrain_toxic = train[train.target == 1]\n\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_toxic[\"comment_text\"], title=\"Word Cloud of Toxic Comments\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud of train file of non-toxic comments\ntrain['target'] = np.where(train['target'] >= 0.5, 1, 0)\ntrain_nontoxic = train[train.target == 0]\nplot_wordcloud(train_nontoxic[\"comment_text\"], title=\"Word Cloud of Non-Toxic Comments\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras Model\n# Model Parameters\nNUM_HIDDEN = 256\nEMB_SIZE = 256\nLABEL_SIZE = 1\nMAX_FEATURES = max_features\nDROP_OUT_RATE = 0.2\nDENSE_ACTIVATION = \"sigmoid\"\nNUM_EPOCH = 1\n\n# Optimization Parameters\nBATCH_SIZE = 1000\nLOSS_FUNC = \"binary_crossentropy\"\nOPTIMIZER_FUNC = \"adam\"\nMETRICS = [\"accuracy\"]\n\nclass LSTMModel:\n    \n    def __init__(self):\n        self.model = self.build_graph()\n        self.compile_model()\n    \n    def build_graph(self):\n        model = keras.models.Sequential([\n            keras.layers.Embedding(MAX_FEATURES, EMB_SIZE),\n            keras.layers.CuDNNLSTM(NUM_HIDDEN),\n            keras.layers.Dropout(rate=DROP_OUT_RATE),\n            keras.layers.Dense(LABEL_SIZE, activation=DENSE_ACTIVATION)])\n        return model\n    \n    def compile_model(self):\n        self.model.compile(\n            loss=LOSS_FUNC,\n            optimizer=OPTIMIZER_FUNC,\n            metrics=METRICS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTMModel().model\nmodel.fit(\n    train_data, \n    label_data, \n    batch_size = BATCH_SIZE, \n    epochs = NUM_EPOCH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc\nauc_keras = auc(fpr_keras, tpr_keras)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlim(0, 0.2)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Loss and accuracy graphs\n\nimport matplotlib.pyplot as plt\n\nhistory = model.fit(\n    train_data, \n    label_data, \n    validation_split=0.25, epochs=5, batch_size=1000, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plot training & validation accuracy values\nplt.xlim(0, 4)\nplt.ylim(0.6, 0.8)\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC curve\nimport sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\ny_pred = model.predict(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tag label_data as binary\n# y_pred=np.where(y_pred>=0.5,1,0)\nlabel_data=np.where(label_data>=0.5,1,0)\nfpr, tpr, threshold = metrics.roc_curve(label_data, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\n# label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.02, 1])\nplt.ylim([0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tag both label and predicted as binary (with margin as 0.5)\n\ny_pred=np.where(y_pred>=0.5,1,0)\nlabel_data=np.where(label_data>=0.5,1,0)\nfpr, tpr, threshold = metrics.roc_curve(label_data, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.02, 1])\nplt.ylim([0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract frequency of transactions along with toxicity for different probability buckets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_in = '../input/sample_submission.csv'\nsubmission_out = 'submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(submission_in, index_col='id')\nsubmission['prediction'] = result\nsubmission.reset_index(drop=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(submission_out, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DO PREDICTION ON TRAIN FILE. ANALYSE FALSE POSITIVES"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get data for train file so that we can evaluate false positives\nsubmission_in = '../input/train.csv'\n#Filter ID and pre\nsubmission_out = 'submission_train.csv'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make test data as train so that we can check false positives of classification\ntest1_data = train[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(test1_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1_data = tokenizer.texts_to_sequences(test1_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 200\ntest1_data = sequence.pad_sequences(test1_data, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = model.predict(test1_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_train = pd.read_csv(submission_in, index_col='id')\nsubmission_train['prediction'] = result1\nsubmission_train.reset_index(drop=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submisson file has a prediction column as well\n\nsubmission_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter only the ID, target value and prediction value\nfiltered_submission=submission_train.filter(items=['id', 'target','prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter fields which show difference in classification between target and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_submission['prediction'] = [ 1 if prediction>=0.5 else 0 for prediction in filtered_submission['prediction'] ]\nfiltered_submission['target'] = [ 1 if target>=0.5 else 0 for target in filtered_submission['target'] ]\nfiltered_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ANALYSE FALSE POSITIVES IN THE PREDICTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_submission.groupby([\"target\",\"prediction\"]).count()[['id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FN are more than FPs. Precision 13%, Recall 3%\n\n#Word cloud of train file of false positives and f\nfiltered_submission_toxic = filtered_submission[filtered_submission.target == 1]\n\nplot_wordcloud(train_toxic[\"comment_text\"], title=\"Word Cloud of Toxic Comments\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def toxicwordcloud(subset=train[train.target>0.7], title = \"Words Frequented\"):\n    stopword=set(STOPWORDS)\n#     toxic_mask=np.array(Image.open(picture))\n#     toxic_mask=toxic_mask[:,:,1]\n    text=subset.comment_text.values\n    wc= WordCloud(background_color=\"black\",max_words=4000,stopwords=stopword)\n    wc.generate(\" \".join(text))\n    plt.figure(figsize=(8,8))\n    plt.xticks([])\n    plt.yticks([])\n    plt.axis('off')\n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicwordcloud(subset=train[train.target>0.7], title = \"Words Frequented\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}