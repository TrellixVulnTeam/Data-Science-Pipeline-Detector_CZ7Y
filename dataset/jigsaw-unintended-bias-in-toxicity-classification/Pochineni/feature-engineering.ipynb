{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\n\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom nltk import word_tokenize, pos_tag\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate new Features using train data\ntrain['total_length'] = train['comment_text'].apply(len)\ntrain['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ntrain['caps_vs_length'] = train.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\ntrain['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))\ntrain['num_question_marks'] = train['comment_text'].apply(lambda comment: comment.count('?'))\ntrain['num_punctuation'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\ntrain['num_symbols'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\ntrain['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\ntrain['num_unique_words'] = train['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\ntrain['words_vs_unique'] = train['num_unique_words'] / train['num_words']\ntrain['num_smilies'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n#Sentence count in each comment:\ntrain['count_sent']=train['comment_text'].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#title case words count\ntrain['count_words_title'] = train['comment_text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ntrain['count_stopwords'] = train['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ntrain['mean_word_len'] = train['comment_text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks', 'num_punctuation', 'num_words', \n            'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols', 'count_sent', 'count_words_title', 'count_stopwords', 'mean_word_len')\ncolumns = ('target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit','identity_annotator_count', 'toxicity_annotator_count')\nrows = [{c:train[f].corr(train[c]) for c in 'columns'} for f in features]\ntrain_correlations = pd.DataFrame(rows, index=features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation between new features and target variable\ntrain_correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation using heat map\nplt.figure(figsize=(10, 6))\nsns.set(font_scale=1)\nax = sns.heatmap(train_correlations, vmin=-0.1, vmax=0.1, center=0.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features which seem correlated to target : capitals, num_exclamation_marks, num_question_marks, num_punctuation, num_unique_words, num_smilies, num_symbols,count_words_title,count_stopwords,mean_word_len"},{"metadata":{},"cell_type":"markdown","source":"READ :\n\nDocument matrix: https://scikit-learn.org/stable/modules/feature_extraction.html\n\nPOS : https://www.kaggle.com/tarunpaparaju/jigsaw-competition-part-of-speech-tagging; https://www.kaggle.com/ryches/22nd-place-solution-6-models-pos-tagging\n\nNamed Entity Recognition : "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add term document matrix, POS and named entity recognition as features\n\ncv = CountVectorizer()\ncount_feats_user= cv.fit_transform(train['comment_text'].apply(lambda x : str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_feats_user.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## POS\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_data) + list(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tag(tokenizer.fit_on_texts)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}