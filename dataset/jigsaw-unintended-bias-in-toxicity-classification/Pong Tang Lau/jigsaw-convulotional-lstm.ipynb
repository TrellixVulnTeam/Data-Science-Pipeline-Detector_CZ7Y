{"cells":[{"metadata":{"_uuid":"dda5ba85-5401-49ff-a42c-3c49016ca619","_cell_guid":"6ca2ca63-1d23-4a43-bc4d-3731bf9942e2","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nprint(tf.__version__)\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime as dt\n\nembedding_dim = 100\n#1804874\nTRAIN_SIZE = 1804874\nTEST_PORTION = .05\nMAX_LENGTH = 200\nnum_epochs = 30\nconv_feature_size = 256\nBATCH_SIZE = 1024\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = LSTM_UNITS*4\n\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nAUX_COLUMNS = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# corpus = project_lib.load_data_to_list(\"./data/train.csv\")\ntrain_csv = \"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\"\ntest_csv = \"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\"\n\ntrain_csv_df = pd.read_csv(train_csv, nrows=TRAIN_SIZE)\ntest_csv_df = pd.read_csv(test_csv)\nprint('loaded %d records' % len(train_csv_df))\n\n# Make sure all comment_text values are strings\ntrain_csv_df['comment_text'] = train_csv_df['comment_text'].astype(str) \ntest_csv_df['comment_text'] = test_csv_df['comment_text'].astype(str) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try to understand the data here, what they actually look like, the distribution, or if some are more useful than the others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# see what the data looks like\nprint(train_csv_df.head())\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n#plot the distribution for labels\nsns.distplot(train_csv_df['target'].values)\n\nsum_true_label = (train_csv_df['target'].values>=0.5).sum()\nprint(\"number of true labels = \",sum_true_label)\n\n\ntrain_csv_df = train_csv_df.fillna(0)\nsum_aux_label = (train_csv_df[AUX_COLUMNS]>0).sum(axis=0)\nprint(\"number of auxiliary labels = \\n\",sum_aux_label)\n\nnum_of_col_without_aux = ((train_csv_df[AUX_COLUMNS]>0).sum(axis=1)==0).sum()\nprint(\"number of columns without auxiliary labels = \",num_of_col_without_aux)\n\n\nsum_identity_label = (train_csv_df[IDENTITY_COLUMNS]>0).sum(axis=0)\nprint(IDENTITY_COLUMNS)\nprint(\"\\nnumber of identity labels = \\n\",sum_identity_label)\n\nnum_of_col_without_identity = ((train_csv_df[IDENTITY_COLUMNS]>0).sum(axis=1)==0).sum()\nprint(\"number of columns without identity labels = \",num_of_col_without_identity)\n\n\nnum_of_col_without_other_label = ((train_csv_df[AUX_COLUMNS+ IDENTITY_COLUMNS]>0).sum(axis=1)==0).sum()\nprint(\"\\nnumber of columns without identity/auxiliary labels = \", num_of_col_without_other_label)\n\nnum_of_col_without_any_label = ((train_csv_df[['target']+AUX_COLUMNS+ IDENTITY_COLUMNS]>0.0).sum(axis=1)==0).sum()\nprint(\"number of columns without any labels = \", num_of_col_without_any_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter out the rows without any labels, as the competition focus on the unintended bias rather than abosolute label accuracy\ntrain_csv_df = train_csv_df.loc[~(train_csv_df[['target']+AUX_COLUMNS+ IDENTITY_COLUMNS]==0.0).all(axis=1)]\n\nprint(\"length of fitlered dataset = \", len(train_csv_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nimport re\nfrom nltk.corpus import stopwords\nimport spacy\nnlp = spacy.load('en', parse = True, tag=True, entity=True)\nimport emoji\nimport multiprocessing\nfrom sklearn import model_selection\n\n# Lemmatization with spacy\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\n# Remove everything apart from number and english letters\ndef remove_special_characters(text, remove_digits=False):\n    pattern = '\\n+|\\n\\r+'\n    text = re.sub(pattern, ' ', text)\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    pattern = '\\s+'\n    text = re.sub(pattern, ' ', text)\n    return text\n\n# Stopwords from nltk library, exempting a list of negation words due to their sentimental value\ndef stopwords_removal(sentence):        \n    # stop words removal actually lowers the submission score as it potentially removes sentimental information.\n    stop_words = set(stopwords.words('english'))\n    negation_words = ['not','but',\"mightn\",\"haven't\",\"hadn\",\"isn\",\"didn\",\"shan't\",\"weren\",\n                      \"don't\",\"doesn't\",\"mustn't\",\"hadn't\",\"shouldn\",\"wouldn't\",\"ain\",\"mightn't\",\n                      \"no\",\"won't\",\"hasn\",\"needn't\",\"didn't\",\"doesn\",\"against\",\"aren't\",\"hasn't\",\"don\",\n                      \"nor\",\"wasn't\",\"shouldn't\",\"weren't\",\"couldn't\",\"couldn\",\"won\"]\n    for word in negation_words:\n        stop_words.remove(word)\n    filtered_sentence = \"\"\n    words = sentence.split()\n    for word in words:\n        if not word in stop_words:\n            filtered_sentence += word + ' '    \n    return filtered_sentence\n\n# Some comments use alternative spellings for agressive words\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    #(\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    #(\"a**\", \" ass\"),\n]\n\nREGEX_REPLACER = [\n    (re.compile('\\W'+pat.replace(\"*\", \"\\S\")+'\\W', flags=re.IGNORECASE), ' '+repl+' ')\n    for pat, repl in WORDS_REPLACER\n]\n\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\n\n# Convert emoji to words/phrases\ndef my_demojize(string: str) -> str:\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n\n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\n# function to call the above processings, lemmatization is commented out\n# as it cost too much time and in some cases harms the performance\ndef text_preprocess(sentence):\n    sentence = my_demojize(sentence)\n    sentence = remove_special_characters(sentence, True)\n    #sentence = RE_SPACE.sub(\" \", sentence)\n    sentence = sentence.lower()\n    for pattern, repl in REGEX_REPLACER:\n        sentence = pattern.sub(repl, sentence)\n    #sentence = lemmatize_text(sentence)\n    sentence = RE_MULTI_SPACE.sub(\" \", sentence).strip().replace(r'\\n',  ' ')\n    return sentence\n\n# Process the loaded data, tokenization, fitting and padding\ndef generate_dataset(train_set_df, test_set_df):\n    print(\"Generating dataset\")\n    start_time = dt.now()\n    \n    train_set_df = train_set_df.sample(frac=1).reset_index(drop=True)\n    test_set_df = test_set_df[:int(test_set_df.shape[0]*TRAIN_SIZE/1804874)]\n\n    trunc_type = 'post'\n    padding_type = 'post'\n    oov_tok = \"<OOV>\"\n\n    target_labels = train_set_df[\"target\"].values.tolist()\n    \n    identities = train_set_df[IDENTITY_COLUMNS].fillna(0)\n    sample_weights = np.ones(len(identities), dtype=np.float32)\n    sample_weights += target_labels * ((identities).sum(axis=1))\n    #sample_weights += identities.sum(axis=1)\n    #sample_weights += target_labels * ((1-identities).sum(axis=1))\n    #sample_weights += (np.ones(len(target_labels))-target_labels) * identities.sum(axis=1) * 5\n    sample_weights /= sample_weights.mean()\n    train_set_df[\"sample_weights\"] = sample_weights\n    \n    \n    sentences = train_set_df[\"comment_text\"].values.tolist()\n    test_sentences = test_set_df[\"comment_text\"].values.tolist()\n    print(\"pre-process text\")\n    with multiprocessing.Pool(processes=2) as pool:\n        processed_sentences = pool.map(text_preprocess, sentences)\n        processed_test_sentences = pool.map(text_preprocess, test_sentences)   \n    \n    print(\"tokenizing...\")\n    tokenizer = Tokenizer(oov_token=oov_tok)\n    tokenizer.fit_on_texts(processed_sentences+processed_test_sentences)\n\n    word_index = tokenizer.word_index\n    no_of_vocab = len(word_index)\n\n    print(\"fitting...\")\n    sequences = tokenizer.texts_to_sequences(processed_sentences)\n    print(\"padding...\")\n    padded = pad_sequences(sequences, padding=padding_type, truncating=trunc_type, maxlen=MAX_LENGTH)\n    train_set_df[\"padded_sequences\"] = pd.Series(list(padded), index=train_set_df.index)\n        \n    training_df, validation_df = model_selection.train_test_split(train_set_df, test_size=TEST_PORTION)\n    print('%d train comments, %d validate comments' % (len(training_df), len(validation_df)))\n    \n    validation_sequences = np.array(validation_df[\"padded_sequences\"].to_numpy().tolist())\n    training_sequences = np.array(training_df[\"padded_sequences\"].to_numpy().tolist())\n\n    \n    test_sequences = tokenizer.texts_to_sequences(processed_test_sentences)\n    test_sequences = pad_sequences(test_sequences, padding=padding_type, truncating=trunc_type,\n                                   maxlen=MAX_LENGTH)\n\n    test_set_df[\"padded_sequences\"] = pd.Series(list(test_sequences), index=test_set_df.index)\n    \n    print(\"Dataset generated, time elapsed =\", dt.now()-start_time)\n\n    return no_of_vocab, word_index, test_set_df, training_df, validation_df\n\n\n\n\n\nvocab_size, word_indices, test_df, train_df, val_df = generate_dataset(train_csv_df, test_csv_df)\n\nval_sequences = np.array(val_df[\"padded_sequences\"].to_numpy().tolist())\ntrain_sequences = np.array(train_df[\"padded_sequences\"].to_numpy().tolist())\nval_labels = val_df['target'].values\naux_val_labels = val_df[['target']+AUX_COLUMNS].values\ntrain_labels = train_df['target'].values\naux_train_labels = train_df[['target']+AUX_COLUMNS].values\n\npadded_test_sequences = np.array(test_df[\"padded_sequences\"].to_numpy().tolist())\ntest_id = test_df[\"id\"].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\\\n                                    Conv1D, CuDNNLSTM, SpatialDropout1D, add, Input, concatenate\n\n# Setup embedding model from GloVe\ndef create_embedding_matrix(word_index, embedding_dimension=100):\n    embeddings_index = {}\n    with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as f:\n        for line in f:\n            values = line.split();\n            word = values[0];\n            coefs = np.asarray(values[1:], dtype='float32');\n            embeddings_index[word] = coefs;        \n            \n    embeddings_matrix = np.zeros((len(word_index)+1, embedding_dimension));\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word);\n        if embedding_vector is not None:\n            embeddings_matrix[i] = embedding_vector;\n            \n    return embeddings_matrix\n\n\n# Build the convlutional LSTM model, using the auxiliary labels as loss to blend in the information \ndef create_model_functional_api(no_of_vocab, embedding_dimension, word_index):\n    words = Input(shape=(None,))\n\n    embedding_matrix = create_embedding_matrix(word_index, embedding_dimension)\n\n    x = Embedding(no_of_vocab+1, embedding_dimension, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Conv1D(128,5)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    aux_result = Dense(6, activation='sigmoid')(hidden)\n\n    tf_model = Model(inputs=words, outputs=[aux_result])\n    tf_model.compile(loss='binary_crossentropy', optimizer='adam')\n    tf_model.summary()\n\n    return tf_model\n\nmodel = create_model_functional_api(vocab_size, embedding_dim, word_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Start\")\nstart_time = dt.now()\n\nsample_weights = train_df[\"sample_weights\"].values\ncombined_weigths =[sample_weights]\n\nhistory = model.fit(train_sequences, [aux_train_labels], epochs=num_epochs, batch_size=BATCH_SIZE,\n                    validation_data=(val_sequences, [aux_val_labels]),\n                    sample_weight=combined_weigths\n                    )\n\nprint(\"Training Complete, time elapsed =\", dt.now()-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn import metrics\n\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + IDENTITY_COLUMNS:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\nflat_list = []\nfor sublist in model.predict(val_sequences):\n    flat_list.append(sublist[0])\n\nval_df[\"prediction\"] = flat_list\n#val_df[\"prediction\"] = model.predict(val_sequences)[0]\n\nbooled_val_df = convert_dataframe_to_bool(val_df)\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\nbias_metrics_df = compute_bias_metrics_for_model(booled_val_df, IDENTITY_COLUMNS, 'prediction', 'target')\nbias_metrics_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_overall_auc(df, model_name):\n    true_labels = df['target']\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    print(\"SUBGROUP_AUC mean =\",power_mean(bias_df[SUBGROUP_AUC], POWER))\n    print(\"BPSN_AUC mean =\",power_mean(bias_df[BPSN_AUC], POWER))\n    print(\"BNSP_AUC mean =\",power_mean(bias_df[BNSP_AUC], POWER))\n    print(\"OVERALL_AUC mean =\",overall_auc)\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \nget_final_metric(bias_metrics_df, calculate_overall_auc(booled_val_df, 'prediction'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction = model.predict(padded_test_sequences)[0]\nprediction = model.predict(padded_test_sequences)\n\nflat_list = []\nfor sublist in prediction:\n    flat_list.append(sublist[0])\n\npd_submission = pd.DataFrame({\"id\": test_id, \"prediction\": flat_list})\npd_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}