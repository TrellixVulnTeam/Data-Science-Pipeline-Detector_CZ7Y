{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ast import literal_eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/toxic-span-detection/tsd_train.csv\")\ntrain[\"spans\"] = train.spans.apply(literal_eval)\ntrain.head(5)\n\ntrial = pd.read_csv(\"/kaggle/input/toxic-span-detection/tsd_trial.csv\")\ntrial[\"spans\"] = trial.spans.apply(literal_eval)\ntrial.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered = df[np.logical_not(np.logical_or(df['comment_text'].isin(train['text'].values) , \n                df['comment_text'].isin(trial['text'].values)))]\nfiltered = filtered.query(\"target > 0.5\")['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(filtered) , filtered[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ktrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare(text , span):\n    text = text.replace('\"' , \"'\")\n    \n    sentences = sent_tokenize(text)\n    tokens = []\n    length = []\n    pointer = 0\n    \n    for sentence in sentences:\n        result = word_tokenize(sentence)\n        tokens.extend(word_tokenize(sentence))\n        length.append((pointer, pointer + len(result)))\n        pointer += len(result)\n        \n    pointer = 0\n    itemSpan = []\n    marking = [None for I in range(len(text))]\n\n    count = 0\n    \n\n    try:\n        for token in tokens:\n            start = text[pointer:].index(token) + pointer\n            end = start + len(token) - 1\n            pointer = end\n            itemSpan.append((start , end))\n\n            for L in range(start , end + 1):\n                marking[L] = count\n            count += 1\n    except:\n        print(tokens)\n        print(token)\n        print(text)\n        raise\n\n    try:\n        labels = [\"False\" for token in tokens]\n\n        for M in span:\n            if marking[M] is not None:\n                labels[marking[M]] = \"True\"\n    except:\n        print(tokens)\n        print(token)\n        print(text)\n        print(M)\n        print(marking)\n        print(labels)\n        raise\n\n    assert len(tokens) == len(labels)\n    return {'tokens' : tokens , 'labels' : labels , \n          'spans' : itemSpan , 'text' : text ,\n           'pointer' : length}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize , sent_tokenize\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial_processed = []\n\ntrial_spans = trial['spans'].values\ntrial_texts = trial['text'].values\nfor M in range(len(trial)):\n    trial_processed.append(prepare(trial_texts[M] , trial_spans[M]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_processed = []\n\ntrain_spans = train['spans'].values\ntrain_texts = train['text'].values\nfor M in range(len(train)):\n    train_processed.append(prepare(train_texts[M] , train_spans[M]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = []\ntrain_y = []\n\nfor sentences in train_processed:\n    for pointer in sentences['pointer']:\n        train_X.append(sentences['tokens'][pointer[0] : pointer[1]])\n        train_y.append(sentences['labels'][pointer[0] : pointer[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = []\ntest_y = []\n\nfor sentences in trial_processed:\n    for pointer in sentences['pointer']:\n        test_X.append(sentences['tokens'][pointer[0] : pointer[1]])\n        test_y.append(sentences['labels'][pointer[0] : pointer[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ktrain\nfrom ktrain import text as txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train , test , preproc = txt.entities_from_array(train_X , train_y, test_X , test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = txt.sequence_tagger('bilstm-bert', preproc,\n                            bert_model='bert-base-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = ktrain.get_learner(model, train_data=train, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit(0.01, 1, cycle_len=4, checkpoint_folder='/tmp/saved_weights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = ktrain.get_predictor(model, preproc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_labels = []\n\nfor row in tqdm(filtered[:20_000]):\n    predicted_labels.append(predictor.predict(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_labels[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"added_X = [[I[0] for I in row] for row in predicted_labels]\nadded_y = [[I[1] for I in row] for row in predicted_labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_X = added_X + train_X\ncombined_y = added_y + train_y\nlen(combined_X) , len(combined_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for M in range(len(combined_X)):\n    assert len(combined_X[M]) == len(combined_y[M])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train , test , preproc = txt.entities_from_array(combined_X , combined_y, test_X , test_y)\nmodel = txt.sequence_tagger('bilstm-bert', preproc,\n                            bert_model='bert-base-cased')\nlearner = ktrain.get_learner(model, train_data=train, batch_size=128)\nlearner.fit(0.01, 1, cycle_len=4, checkpoint_folder='/tmp/weights2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = ktrain.get_predictor(model, preproc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_spans(text , tokens):\n    text = text.replace('\"' , \"'\")\n    string = list(text)\n\n    pointer = 0\n    itemSpan = []\n    marking = [None for I in range(len(text))]\n\n    count = 0\n    \n    for token in tokens:\n        start = text[pointer:].index(token) + pointer\n        end = start + len(token) - 1\n        pointer = end\n        itemSpan.append((start , end))\n\n    return itemSpan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(ground, prediction):\n    if len(ground) == 0:\n        if len(prediction) == 0:\n            return 1\n        else:\n            return 0\n        \n    if len(prediction) == 0:\n        return 0\n    \n    precision = len(set(ground) & set(prediction)) / len(set(prediction))\n    recall = len(set(ground) & set(prediction)) / len(set(ground))\n\n    return (2 * precision * recall) / (precision + recall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = 0\nprocessed = 0\nfor K in tqdm(range(len(trial_spans))):\n    try:\n        results = []\n        for pointer in trial_processed[K]['pointer']:\n            result = predictor.predict(\" \".join(trial_processed[K]['tokens'][pointer[0]:pointer[1]]))\n            results.extend([I for I in result])\n            \n        spans = extract_spans(trial_processed[K]['text'] , [I[0] for I in results])\n        prediction = []\n\n        for M in range(len(results)):\n            if results[M][1] == 'True':\n                for Z in range(spans[M][0] , spans[M][1] + 1):\n                    prediction.append(Z)\n\n        total += accuracy(trial_spans[K] , prediction)\n        processed += 1\n    except:\n        print(K)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total / processed","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}