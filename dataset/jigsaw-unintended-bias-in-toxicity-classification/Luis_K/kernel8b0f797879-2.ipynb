{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project 3\n\n\n# Conversations Toxicity Detection\n\n## Preproccesing and CuDNNLSTM Model\n\nJigsaw Unintended Bias in Toxicity Classification \n\nDetect toxicity across a diverse range of conversations\n\n\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data#\n"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport operator \n\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import KeyedVectors\n\nfrom sklearn import model_selection\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import LeakyReLU, CuDNNLSTM\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import EarlyStopping\n\nimport seaborn as sns","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":17,"outputs":[{"output_type":"stream","text":"['outprocess3', 'jigsaw-unintended-bias-in-toxicity-classification', 'fasttext-crawl-300d-2m']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Import Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/outprocess3/train_preprocess2.csv\")\ntest = pd.read_csv(\"../input/outprocess3/test_preprocess2.csv\")\nprint(\"Train shape : \",train.shape)\ntest.head()\n","execution_count":18,"outputs":[{"output_type":"stream","text":"Train shape :  (1804874, 2)\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"        id                                       comment_text\n0  7000000  Jeff Sessions is another one of trump is Orwel...\n1  7000001  I actually inspected the infrastructure on Gra...\n2  7000002  No it will not . That ' s just wishful thinkin...\n3  7000003  Instead of wringing our hands and nibbling the...\n4  7000004  how many of you commenters have garbage piled ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7000000</td>\n      <td>Jeff Sessions is another one of trump is Orwel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7000001</td>\n      <td>I actually inspected the infrastructure on Gra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000002</td>\n      <td>No it will not . That ' s just wishful thinkin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7000003</td>\n      <td>Instead of wringing our hands and nibbling the...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7000004</td>\n      <td>how many of you commenters have garbage piled ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Further Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntrain_orig.head()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train,train_orig[['target']]],axis=1)\ntrain.head()","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"      id                                       comment_text    target\n0  59848  This is so cool . it is like , ' would you wan...  0.000000\n1  59849  Thank you ! ! This would make my life a lot le...  0.000000\n2  59852  This is such an urgent design problem ; kudos ...  0.000000\n3  59855  Is this something I ' ll be able to install on...  0.000000\n4  59856              haha you guys are a bunch of losers .  0.893617","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>This is so cool . it is like , ' would you wan...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>Thank you ! ! This would make my life a lot le...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>This is such an urgent design problem ; kudos ...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>Is this something I ' ll be able to install on...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>haha you guys are a bunch of losers .</td>\n      <td>0.893617</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(train_orig)\ngc.collect()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"490"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Convert target to binary flag"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = np.where(train['target'] >= 0.5, True, False)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split into train/validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, validate_df = model_selection.train_test_split(train, test_size=0.1)\nprint('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))","execution_count":23,"outputs":[{"output_type":"stream","text":"1624386 train comments, 180488 validate comments\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"              id  ...   target\n1525131  5987739  ...    False\n104146    369546  ...    False\n1757018  6275451  ...    False\n580114    952106  ...    False\n490475    845174  ...    False\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1525131</th>\n      <td>5987739</td>\n      <td>So , the Globe journalist used Gerrymandering ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>104146</th>\n      <td>369546</td>\n      <td>You want to tell that to every college student...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1757018</th>\n      <td>6275451</td>\n      <td>There was no informed consent given to search ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>580114</th>\n      <td>952106</td>\n      <td>Would these protesters feel the same if they o...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>490475</th>\n      <td>845174</td>\n      <td>It is also a state mandate in a very repressiv...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Tokenize the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['comment_text'].describe()","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"count         1624368\nunique        1600928\ntop       Well said .\nfreq              176\nName: comment_text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NUM_WORDS = 100000\nTOXICITY_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'\n\n# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train_df[['comment_text']])\n\n# All comments must be truncated or padded to be the same length.\nMAX_SEQUENCE_LENGTH = 256\ndef pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create our embedding matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"577"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDINGS_DIMENSION = 300\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_common_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(ft_common_crawl)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words_in_embedding = 0\n\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index.vocab:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector        \n        num_words_in_embedding += 1","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = pad_text(train_df[[TEXT_COLUMN]], tokenizer)\ntrain_labels = train_df[[TOXICITY_COLUMN]]\nvalidate_text = pad_text(validate_df[[TEXT_COLUMN]], tokenizer)\nvalidate_labels = validate_df[[TOXICITY_COLUMN]]","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"162"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model Architecture\n\nAdding dropout / 1d conv / concatenated poolings based on the architecture presented @ https://www.kaggle.com/tunguz/bi-gru-cnn-poolings-gpu-kernel-version\n\nNow based on: https://www.kaggle.com/samarthsarin/toxication-with-embeddings-and-keras-lstm"},{"metadata":{"trusted":true},"cell_type":"code","source":"NODES = 64\nvocab_size = len(tokenizer.word_index) + 1\n\n\nmodel = Sequential()\n\nmodel.add(Embedding(vocab_size,EMBEDDINGS_DIMENSION,input_length = MAX_SEQUENCE_LENGTH,weights = [embedding_matrix],trainable = False))\n\nmodel.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\nmodel.add(Conv1D(64,7,padding='same'))\nmodel.add(GlobalAveragePooling1D())\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU())\n\nmodel.add(Dense(NODES,activation = 'relu'))\n\nmodel.add(Dense(1,activation = 'sigmoid'))\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 1024\nNUM_EPOCHS = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n    train_text,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    validation_data=(validate_text, validate_labels),\n    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict & Submit"},{"metadata":{},"cell_type":"markdown","source":"Let's submit this as our first submission. Once we have a reasonable pipeline setup, we can move on to looking at the competition metric in more detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}