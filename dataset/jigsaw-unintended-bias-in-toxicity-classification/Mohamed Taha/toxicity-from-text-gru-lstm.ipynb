{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Variable Exploration","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning and Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling from the data","metadata":{}},{"cell_type":"code","source":"train_sample = train.sample(frac = 0.1, random_state = 42, axis = 'index')\ntrain_sample.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_text(text):\n    \n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    \n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n#     # strip html\n#     soup = BeautifulSoup(text, 'html.parser')\n#     text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    \n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    \n    return text.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample['clean_comment_text'] = train_sample['comment_text'].apply(clean_text)\ntest['clean_comment_text'] = test['comment_text'].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample['clean_comment_text'].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sequence Length Analysis","metadata":{}},{"cell_type":"code","source":"plt.hist(list(train_sample['clean_comment_text'].str.split().map(lambda x: len(x))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.median(np.array(train_sample['clean_comment_text'].str.split().map(lambda x: len(x))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Representation","metadata":{}},{"cell_type":"markdown","source":"## Loading the embedding matrix","metadata":{}},{"cell_type":"code","source":"embedding_dict = pd.read_pickle('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Tokenization as Sequences","metadata":{}},{"cell_type":"code","source":"# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_sample['clean_comment_text']) + list(test['comment_text']))\n\nmax_len = 18\nX_train_seq = tokenizer.texts_to_sequences(train_sample['clean_comment_text'])\nX_test_seq = tokenizer.texts_to_sequences(test['comment_text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_sample['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,300))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    \n            \nembedding_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"# Setting up the model\n\nfrom keras import layers\nfrom keras.models import Sequential\n\ndef setup_lstm_model(max_len, n_latent_factors):\n    \n    model = Sequential()\n    model.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                               input_length = max_len, trainable = False))\n    model.add(layers.LSTM(units = max_len, return_sequences = True))\n    model.add(layers.GlobalAveragePooling1D())\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = setup_lstm_model(max_len = max_len, n_latent_factors = 300)\nlstm_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final hyperparameter configurations\nlstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nbatch_size = 512\nepochs = 5\n\nlstm_model.fit(X_train_seq, y_train, epochs=epochs, batch_size = batch_size, validation_data = (X_val_seq, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU","metadata":{}},{"cell_type":"code","source":"def setup_gru_model(max_len, n_latent_factors):\n    \n    model = Sequential()\n    model.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                               input_length = max_len, trainable = False))\n    model.add(layers.GRU(units = max_len, return_sequences = True))\n    model.add(layers.GlobalAveragePooling1D())\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_model = setup_gru_model(max_len = max_len, n_latent_factors = 300)\ngru_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final hyperparameter configurations and fitting\ngru_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nbatch_size = 512\nepochs = 5\n\ngru_model.fit(X_train_seq, y_train, epochs=epochs, batch_size = batch_size, validation_data = (X_val_seq, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}