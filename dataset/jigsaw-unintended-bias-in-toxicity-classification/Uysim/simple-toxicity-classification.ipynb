{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel response to competitiion [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). It is my first hand on **Text Classification**. I will use Tensorflow and Keras for this project."},{"metadata":{},"cell_type":"markdown","source":"I have built few great kernel for beginner with deep learning that you can check it out: \n* https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification\n* https://www.kaggle.com/uysimty/learn-titanic-survival\n* https://www.kaggle.com/uysimty/keras-predict-google-stock-using-lstm"},{"metadata":{},"cell_type":"markdown","source":"Because of our analyst part taking too long. So it is disable me from submit to compotition. So I have other kernel that take only processing part which return 92.3 acurracy score.\n\n* https://www.kaggle.com/uysimty/simple-toxicity-classification-submission"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom gensim.models import KeyedVectors\nimport operator\nfrom tqdm import tqdm\ntqdm.pandas()\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=25\nbatch_size=128\nmax_words=100000\nmax_seq_size=256","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\")) \nprint(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\nprint(os.listdir(\"../input/quoratextemb\"))\nprint(os.listdir(\"../input/quoratextemb/embeddings\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df  = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub_df   = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Take care of dataframe memory "},{"metadata":{"trusted":true},"cell_type":"code","source":"mem_usg = train_df.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll select only the columns that we need to reduce some memory usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[[\"target\", \"comment_text\"]]\nmem_usg = train_df.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See? we have more free memory"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n"},{"metadata":{},"cell_type":"markdown","source":"### Load Embedding"},{"metadata":{},"cell_type":"markdown","source":"To increase our covarage we try to combine few embedding together in order for us to more vocab covrage. \nIn term of memory optimize, we will convert our vector to ```float16``` to reduce some memory usage. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for combine the vector files that have given \ndef combine_embedding(vec_files):\n    \n    # convert victor to float16 to make it use less memory\n    def get_coefs(word, *arr): \n        return word, np.asarray(arr, dtype='float16')\n\n    # make our embed smaller by get_coefs\n    def optimize_embedding(embedding): \n        optimized_embedding = {}\n        for word in embedding.vocab:\n            optimized_embedding[word] = np.asarray(embedding[word], dtype='float16')\n        return optimized_embedding\n\n    \n    # load embed vector from file\n    def load_embed(file):\n        print(\"Loading {}\".format(file))\n\n        if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n            return dict(get_coefs(*o.strip().split(\" \")) for o in open(file) if len(o) > 100)\n        \n        elif file == '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec':\n            return optimize_embedding(KeyedVectors.load_word2vec_format(file))\n\n        else:\n            return dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n        \n    combined_embedding = {}\n    for file in vec_files:\n        combined_embedding.update(load_embed(file))\n    return combined_embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_files = [\n    \"../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\",\n    \"../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt\",\n    \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = combine_embedding(vec_files)\ncovered_vocabs = set(list(embedding_index.keys()))\nembedding_index.clear()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count occurance of words "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for count how many time word accure in our data\ndef count_words_from(series):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    sentences =  series.str.split()\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Coverage"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for check coverage of our vocab\ndef check_coverage_for(vocab):\n    a = 0\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        if word in covered_vocabs:\n            a += 1\n            k += vocab[word]\n        else:\n            oov[word] = vocab[word]\n            i += vocab[word]\n\n    print('Found embeddings for {:.2%} of vocab'.format(a / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this methods help to clean up some memory while improve the coverage. As it will release the varaible the send of method\ndef check_current_coverage(num=50):\n    vocab = count_words_from( train_df[\"comment_text\"] )\n    coverage = check_coverage_for(vocab)\n    return coverage[:num]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the first coverage "},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see only 18% of our vocab has been covered. But 92% of our text has already cover. From the top first uncovered we see we have some problem with contractions. Let's get rid of it. "},{"metadata":{},"cell_type":"markdown","source":"### Clean contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"Trump's\": \"trump is\", \"Obama's\": \"obama is\", \"Canada's\": \"canada is\", \"today's\": \"today is\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"known_contractions = []\nfor contract in contraction_mapping:\n    if contract in covered_vocabs:\n        known_contractions.append(contract)\nprint(known_contractions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our embedding have known some contractions. So we will remove that known contractions from our dictionary and let's our embedding handle it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for cont in known_contractions:\n    contraction_mapping.pop(cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    words = [contraction_mapping[word] if word in contraction_mapping else word for word in text.split(\" \")]\n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_contractions(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let check coverage again"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our covarage have increase a little. But at least we are good for next step. From the top uncovered we see that we have problem with some specials charator. "},{"metadata":{},"cell_type":"markdown","source":"#### Clean special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specail_signs = { \"…\": \"...\", \"₂\": \"2\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_puncts = []\nfor p in punct:\n    if p not in covered_vocabs:\n        unknown_puncts.append(p)\nprint(' '.join(unknown_puncts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All contraction are known"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text):\n    for s in specail_signs: \n        text = text.replace(s, specail_signs[s])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_special_chars(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check coverage again"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the excited part that we have our coverage increase much. Our vocab coverage has increase to 65% and Most of our text (99.6%) has been covered"},{"metadata":{},"cell_type":"markdown","source":"# Clean Special Caps"},{"metadata":{},"cell_type":"markdown","source":"We see some like like ```ʜᴏᴍᴇ```, ```ᴜᴘ```, ```ᴄʜᴇᴄᴋ``` etc ... We need to convert it to up, home, check ...."},{"metadata":{"trusted":true},"cell_type":"code","source":"special_caps_mapping = { \n    \"ᴀ\": \"a\", \"ʙ\": \"b\", \"ᴄ\": \"c\", \"ᴅ\": \"d\", \"ᴇ\": \"e\", \"ғ\": \"f\", \"ɢ\": \"g\", \"ʜ\": \"h\", \"ɪ\": \"i\", \"ᴊ\": \"j\", \"ᴋ\": \"k\", \"ʟ\": \"l\", \"ᴍ\": \"m\",\n    \"ɴ\": \"n\", \"ᴏ\": \"o\", \"ᴘ\": \"p\", \"ǫ\": \"q\", \"ʀ\": \"r\", \"s\": \"s\", \"ᴛ\": \"t\", \"ᴜ\": \"u\", \"ᴠ\": \"v\", \"ᴡ\": \"w\", \"x\": \"x\", \"ʏ\": \"y\", \"ᴢ\": \"z\",\n    \"𝘊\": \"C\", \"𝘦\": \"e\", \"𝘳\": \"r\", \"𝘢\": \"a\", \"𝘵\": \"t\", \"𝘰\": \"o\", \"𝘤\": \"c\", \"𝘺\": \"y\", \"𝘴\": \"s\", \"𝘪\": \"i\", \"𝘧\": \"f\", \"𝘮\": \"m\", \"𝘣\": \"b\",\n    \"м\": \"m\", \"υ\": \"u\", \"т\": \"t\", \"ѕ\": \"s\", \"𝙀\": \"E\", \"𝒛\": \"z\", \"𝑲\": \"K\", \"𝑳\": \"L\", \"𝑾\": \"W\", \"𝒋\": \"j\", \"𝟒\": \"4\",\n    \"𝙒\": \"W\", \"𝘾\": \"C\", \"𝘽\": \"B\", \"𝑱\": \"J\", \"𝑹\": \"R\", \"𝑫\": \"D\", \"𝑵\": \"N\", \"𝑪\": \"C\", \"𝑯\": \"H\", \"𝒒\": \"q\", \"𝑮\": \"G\", \"𝗕\": \"B\", \"𝗴\": \"g\", \n    \"𝟐\": \"2\", \"𝗸\": \"k\", \"𝗟\": \"L\", \"𝗠\": \"M\", \"𝗷\": \"j\", \"𝐎\": \"O\", \"𝐍\": \"N\", \"𝐊\": \"K\", \"𝑭\": \"F\", \"Е\": \"E\"\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_small_caps(text):\n    for char in special_caps_mapping:\n        text = text.replace(char, special_caps_mapping[char])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_small_caps(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Emoji "},{"metadata":{"trusted":true},"cell_type":"code","source":"emojis = \"🍕🐵😑😢🐶️😜😎👊😁😍💖💵👎😀😂🔥😄🏻💥😋👏😱🚌ᴵ͞🌟😊😳😧🙀😐😕👍😮😃😘💩💯⛽🚄😖🏼🚲😟😈💪🙏🎯🌹😇💔😡👌🙄😠😉😤⛺🙂😏🍾🎉😞🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪🐰🐇🐱🙆😨🙃💕💗💚🙈😴🏿🤗🇺🇸⤵🏆🎃😩👮💙🐾🐕😆🌠🐟💫💰💎🖐🙅⛲🍰🤐👆🙌💛🙁👀🙊🙉🚬🤓😵😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓😣⏺😌🤑🌏😯😲💞🚓🔔📚🏀👐💤🍇🏡❔⁉👠》🇹🇼🌸🌞🎲😛💋💀🎄💜🤢َِ🗑💃📣👿༼つ༽😰🤣🐝🎅🍺🎵🌎͟🤡🤥😬🤧🚀🤴😝💨🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦🍀😫🤤🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪🇫🇷🇩🇪😷🇨🇦🌐📺🐋💘💓💐🌋🌄🌅👺🐷🚶🤘ͦ💸👂👃🎫🚢🚂🏃👽😙🎾👹⎌🏒⛸🏄🐀🚑🤷🤙🐒🐈ﷻ🦄🚗🐳👇⛷👋🦊🐽🎻🎹⛓🏹🍷🦆♾🎸🤕🤒⛑🎁🏝🦁🙋😶🔫👁💲🗯👑🚿💡😦🏐🇰🇵👾🐄🎈🔨🐎🤞🐸💟🎰🌝🛳🍭👣🏉💭🎥🐴👨🤳🦍🍩😗🏂👳🍗🕉🐲🍒🐑⏰💊🌤🍊🔹🤚🍎𝑷🐂💅💢💒🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻🤖🎎😼🕷👼📉🍟🍦🌈🔭《🐊🐍🐦🐡💳ἱ🙇🥜🔼\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emojis(text):\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: remove_emojis(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Unprocessable Symbols"},{"metadata":{},"cell_type":"markdown","source":"Do some clean up for memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"del covered_vocabs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's clean up of testing set"},{"metadata":{},"cell_type":"markdown","source":"* Convert to lower case\n* Clean contractions\n* Clean special charactor\n* Convert small caps"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_up_text_with_all_process(text):\n    text = text.lower()\n    text = clean_contractions(text)\n    text = clean_special_chars(text)\n    text = clean_small_caps(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"comment_text\"] = test_df[\"comment_text\"].progress_apply(lambda text: clean_up_text_with_all_process(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform Text "},{"metadata":{"trusted":true},"cell_type":"code","source":"tranformer = Tokenizer(lower = True, filters='', num_words=max_words)\ntranformer.fit_on_texts( list(train_df[\"comment_text\"].values) + list(test_df[\"comment_text\"].values) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_x = tranformer.texts_to_sequences(train_df[\"comment_text\"].values)\ntransformed_x = pad_sequences(transformed_x, maxlen = max_seq_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform predicting set"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_predict = tranformer.texts_to_sequences(test_df[\"comment_text\"])\nx_predict = pad_sequences(x_predict, maxlen = max_seq_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Martix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(word_index, total_vocab, embedding_size):\n    embedding_index = combine_embedding(vec_files)\n    matrix = np.zeros((total_vocab, embedding_size))\n    for word, index in tqdm(word_index.items()):\n        try:\n            matrix[index] = embedding_index[word]\n        except KeyError:\n            pass\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tranformer.word_index\ntotal_vocab = len(word_index) + 1\nembedding_size = 300\nembedding_matrix = build_embedding_matrix(tranformer.word_index, total_vocab, embedding_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean up some memory"},{"metadata":{},"cell_type":"markdown","source":"Let free up some memory before to other hard job. I'll clean ```vocab``` and ```coverage``` up in order for us to have enough memory to continue"},{"metadata":{"trusted":true},"cell_type":"code","source":"del tranformer\ndel word_index\ndel embedding_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select features and Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (train_df['target'].values > 0.5).astype(int)\nx_train, x_test, y_train, y_test = train_test_split(transformed_x, y, random_state=10, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let clean memory again,  I'll clean ```word_index``` and ```embedding_index``` up in order for us to have enough memory for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel y\ndel test_df\ndel transformed_x\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.nn import relu, sigmoid\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_input = Input(shape=(max_seq_size,), dtype='int32')\nembedding_layer = Embedding(total_vocab,\n                            embedding_size,\n                            weights=[embedding_matrix],\n                            input_length=max_seq_size,\n                            trainable=False)\n\nx_layer = embedding_layer(sequence_input)\nx_layer = SpatialDropout1D(0.2)(x_layer)\nx_layer = Bidirectional(CuDNNGRU(64, return_sequences=True))(x_layer)   \nx_layer = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_layer)\n\navg_pool1 = GlobalAveragePooling1D()(x_layer)\nmax_pool1 = GlobalMaxPooling1D()(x_layer)     \n\nx_layer = concatenate([avg_pool1, max_pool1])\n\npreds = Dense(1, activation=sigmoid)(x_layer)\n\nmodel = Model(sequence_input, preds)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=10, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Virtualize Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\n\nax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(x_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[\"prediction\"] = y_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Credit**\n* https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2\n* https://www.kaggle.com/thousandvoices/simple-lstm"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}