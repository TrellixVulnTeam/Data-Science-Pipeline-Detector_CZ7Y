{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install & Load Library"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ignore warnings"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport gc\nimport re\nimport sys\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nfrom nltk.tokenize.treebank import TreebankWordTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Util functions"},{"metadata":{},"cell_type":"markdown","source":"* set seed functions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set seed\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"* preprocessing of public kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# preprocessing of public kernel\nclass Preprocess_pb_kernel:\n    \n    def __init__(self):\n        self.tokenizer_preprocess = TreebankWordTokenizer()\n   \n        symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n        symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n\n        self.isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n        self.remove_dict = {ord(c):f'' for c in symbols_to_delete}\n    \n    def _handle_punctuation(self, x):\n        x = x.translate(self.remove_dict)\n        x = x.translate(self.isolate_dict)\n        return x\n\n    def _handle_contractions(self, x):\n        x = self.tokenizer_preprocess.tokenize(x)\n        return x\n\n    def _fix_quote(self, x):\n        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n        x = ' '.join(x)\n        return x\n    \n    def preprocess(self, text):\n        text = self._handle_punctuation(text)\n        text = self._handle_contractions(text)\n        text = self._fix_quote(text)\n\n        return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocessing of sogna kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# preprocessing of public kernel\nclass Preprocess_sogna_kernel:\n    \n    def __init__(self):\n        contraction_mapping = {\n            \"'cause\": 'because',\n            ',cause': 'because',\n            ';cause': 'because',\n            '´cause': 'because',\n            '’cause': 'because',\n\n            \"ain't\": 'am not',\n            'ain,t': 'am not', \n            'ain;t': 'am not',\n            'ain´t': 'am not',\n            'ain’t': 'am not',\n\n            \"aren't\": 'are not', \"arn't\": \"are not\",\n            'aren,t': 'are not', \"arn,t\": \"are not\",\n            'aren;t': 'are not', \"arn;t\": \"are not\",\n            'aren´t': 'are not', \"arn´t\": \"are not\",\n            'aren’t': 'are not', \"arn’t\": \"are not\",\n\n            \"isn't\": 'is not', \"is'nt\": \"is not\",\n            'isn,t': 'is not', \"is,nt\": \"is not\",\n            'isn;t': 'is not', \"is;nt\": \"is not\",\n            'isn´t': 'is not', \"is´nt\": \"is not\",\n            'isn’t': 'is not', \"is’nt\": \"is not\",\n\n            \"wasn't\": 'was not', \"was'nt\": \"was not\",\n            'wasn,t': 'was not', \"was,nt\": \"was not\",\n            'wasn;t': 'was not', \"was;nt\": \"was not\",\n            'wasn´t': 'was not', \"was´nt\": \"was not\",\n            'wasn’t': 'was not', \"was’nt\": \"was not\",\n\n            \"weren't\": 'were not',\n            'weren,t': 'were not',\n            'weren;t': 'were not',\n            'weren´t': 'were not',\n            'weren’t': 'were not',\n\n            \"didn't\": 'did not', \"d'int\": \"did not\", \"did'nt\": \"did not\", \"din't\": \"did not\",\n            'didn,t': 'did not', \"d,int\": \"did not\", \"did,nt\": \"did not\", \"din,t\": \"did not\",\n            'didn;t': 'did not', \"d;int\": \"did not\", \"did;nt\": \"did not\", \"din;t\": \"did not\",\n            'didn´t': 'did not', \"d´int\": \"did not\", \"did´nt\": \"did not\", \"din´t\": \"did not\",\n            'didn’t': 'did not', \"d’int\": \"did not\", \"did’nt\": \"did not\", \"din’t\": \"did not\",\n\n            \"doesn't\": 'does not', \"doens't\": \"does not\", \"dosen't\": \"does not\", \"dosn't\": \"does not\",\n            'doesn,t': 'does not', \"doens,t\": \"does not\", \"dosen,t\": \"does not\", \"dosn,t\": \"does not\",\n            'doesn;t': 'does not', \"doens;t\": \"does not\", \"dosen;t\": \"does not\", \"dosn;t\": \"does not\",\n            'doesn´t': 'does not', \"doens´t\": \"does not\", \"dosen´t\": \"does not\", \"dosn´t\": \"does not\",\n            'doesn’t': 'does not', \"doens’t\": \"does not\", \"dosen’t\": \"does not\", \"dosn’t\": \"does not\",\n\n            \"don't\": 'do not', \"dont't\": \"do not\",\n            'don,t': 'do not', \"dont,t\": \"do not\",\n            'don;t': 'do not', \"dont;t\": \"do not\",\n            'don´t': 'do not', \"dont´t\": \"do not\",\n            'don’t': 'do not', \"dont’t\": \"do not\",\n\n            'don\"\"t': \"do not\",\n\n            \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"havn't\": 'have not',\n            'hadn,t': 'had not', 'hadn,t,ve': 'had not have', 'hasn,t': 'has not', 'haven,t': 'have not', \"havn,t\": 'have not',\n            'hadn;t': 'had not', 'hadn;t;ve': 'had not have', 'hasn;t': 'has not', 'haven;t': 'have not', \"havn;t\": 'have not',\n            'hadn´t': 'had not', 'hadn´t´ve': 'had not have', 'hasn´t': 'has not', 'haven´t': 'have not', \"havn´t\": 'have not',\n            'hadn’t': 'had not', 'hadn’t’ve': 'had not have', 'hasn’t': 'has not', 'haven’t': 'have not', \"havn’t\": 'have not',\n\n            \"won't\": 'will not', \"will've\": \"will have\", \"won't've\": \"will not have\",\n            'won,t': 'will not', \"will,ve\": \"will have\", \"won,t,ve\": \"will not have\",\n            'won;t': 'will not', \"will;ve\": \"will have\", \"won;t;ve\": \"will not have\",\n            'won´t': 'will not', \"will´ve\": \"will have\", \"won´t´ve\": \"will not have\",\n            'won’t': 'will not', \"will’ve\": \"will have\", \"won’t’ve\": \"will not have\",\n\n            \"wouldn't\": 'would not', \"would've\": \"would have\", \"wouldn't've\": \"would not have\",\n            'wouldn,t': 'would not', \"would,ve\": \"would have\", \"wouldn,t,ve\": \"would not have\",\n            'wouldn;t': 'would not', \"would;ve\": \"would have\", \"wouldn;t;ve\": \"would not have\",\n            'wouldn´t': 'would not', \"would´ve\": \"would have\", \"wouldn´t´ve\": \"would not have\",\n            'wouldn’t': 'would not', \"would’ve\": \"would have\", \"wouldn’t’ve\": \"would not have\",\n\n            \"can't\": 'cannot',\"can't've\": 'cannot have',\n            'can,t': 'cannot','can,t,ve': 'cannot have',\n            'can;t': 'cannot','can;t;ve': 'cannot have',\n            'can´t': 'cannot','can´t´ve': 'cannot have',\n            'can’t': 'cannot','can’t’ve': 'cannot have',\n\n            \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"could'nt\": 'could not',\n            'could,ve': 'could have', 'couldn,t': 'could not', 'couldn,t,ve': 'could not have', \"could,nt\": 'could not',\n            'could;ve': 'could have', 'couldn;t': 'could not', 'couldn;t;ve': 'could not have', \"could;nt\": 'could not',\n            'could´ve': 'could have', 'couldn´t': 'could not', 'couldn´t´ve': 'could not have', \"could´nt\": 'could not',\n            'could’ve': 'could have', 'couldn’t': 'could not', 'couldn’t’ve': 'could not have', \"could’nt\": 'could not',\n\n            \"sha'n't\": 'shall not', \"shan't\": 'shall not', \"shan't've\": \"shall not have\", \n            'sha,n,t': 'shall not', 'shan,t': 'shall not', \"shan,t,ve\": \"shall not have\", \n            'sha;n;t': 'shall not', 'shan;t': 'shall not', \"shan;t;ve\": \"shall not have\", \n\n            \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": \"should not have\", \"shoudn't\": \"should not\",\n            'should,ve': 'should have', 'shouldn,t': 'should not', \"shouldn,t,ve\": \"should not have\", \"shoudn,t\": \"should not\",\n            'should;ve': 'should have', 'shouldn;t': 'should not', \"shouldn;t;ve\": \"should not have\", \"shoudn;t\": \"should not\",\n\n            \"mayn't\": 'may not',\n            'mayn,t': 'may not',\n            'mayn;t': 'may not',\n            'mayn´t': 'may not',\n            'mayn’t': 'may not',\n\n            \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": \"might not have\", \n            'might,ve': 'might have', 'mightn,t': 'might not', \"mightn,t,ve\": \"might not have\", \n            'might;ve': 'might have', 'mightn;t': 'might not', \"mightn;t;ve\": \"might not have\", \n\n            \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": \"must not have\",  \n            'must,ve': 'must have', 'mustn,t': 'must not', \"mustn,t,ve\": \"must not have\", \n            'must;ve': 'must have', 'mustn;t': 'must not', \"mustn;t;ve\": \"must not have\", \n\n            \"needn't\": 'need not', \"needn't've\": \"need not have\",\n            'needn,t': 'need not', \"needn,t,ve\": \"need not have\",\n            'needn;t': 'need not', \"needn;t;ve\": \"need not have\",\n\n            \"oughtn't\": 'ought not', \"oughtn't've\": \"ought not have\",\n            'oughtn,t': 'ought not', \"oughtn,t,ve\": \"ought not have\",\n            'oughtn;t': 'ought not', \"oughtn;t;ve\": \"ought not have\",\n\n            \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he's\": 'he is', \"he'll've\": \"he will have\",\n            'he,d': 'he would', 'he,d,ve': 'he would have', 'he,ll': 'he will', 'he,s': 'he is', \"he,ll,ve\": \"he will have\",\n            'he;d': 'he would', 'he;d;ve': 'he would have', 'he;ll': 'he will', 'he;s': 'he is', \"he;ll;ve\": \"he will have\",\n            'he´d': 'he would', 'he´d´ve': 'he would have', 'he´ll': 'he will', 'he´s': 'he is', \"he´ll´ve\": \"he will have\",\n            'he’d': 'he would', 'he’d’ve': 'he would have', 'he’ll': 'he will', 'he’s': 'he is', \"he’ll’ve\": \"he will have\",\n\n            \"she'd\": 'she would', \"she'll\": 'she will' ,\"she's\": 'she is', \"she'd've\": \"she would have\", \"she'll've\": \"she will have\",\n            'she,d': 'she would', 'she,ll': 'she will', 'she,s': 'she is', \"she,d,ve\": \"she would have\", \"she,ll,ve\": \"she will have\",\n            'she;d': 'she would', 'she;ll': 'she will', 'she;s': 'she is', \"she;d;ve\": \"she would have\", \"she;ll;ve\": \"she will have\",\n            'she´d': 'she would', 'she´ll': 'she will', 'she´s': 'she is', \"she´d´ve\": \"she would have\", \"she´ll´ve\": \"she will have\",\n            'she’d': 'she would', 'she’ll': 'she will', 'she’s': 'she is', \"she’d’ve\": \"she would have\", \"she’ll’ve\": \"she will have\",\n\n            \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"i've\": 'i have', \"i'd've\": \"i would have\", \"i'll've\": \"i will have\", \"i'ma\": \"i am\", \"i'am\": 'i am', \"i'l\": \"i will\", \"i'v\": 'i have',\n            'i,d': 'i would', 'i,ll': 'i will', 'i,m': 'i am', 'i,ve': 'i have', \"i,d,ve\": \"i would have\", \"i,ll,ve\": \"i will have\", \"i,ma\": \"i am\", \"i,am\": 'i am', \"i,l\": \"i will\", \"i,v\": 'i have',\n            'i;d': 'i would', 'i;ll': 'i will', 'i;m': 'i am', 'i;ve': 'i have', \"i;d;ve\": \"i would have\", \"i;ll;ve\": \"i will have\", \"i;ma\": \"i am\", \"i;am\": 'i am', \"i;l\": \"i will\", \"i;v\": 'i have',\n            'i´d': 'i would', 'i´ll': 'i will', 'i´m': 'i am', 'i´ve': 'i have', \"i´d´ve\": \"i would have\", \"i´ll´ve\": \"i will have\", \"i´ma\": \"i am\", \"i´am\": 'i am', \"i´l\": \"i will\", \"i´v\": 'i have',\n            'i’d': 'i would', 'i’ll': 'i will', 'i’m': 'i am', 'i’ve': 'i have', \"i’d’ve\": \"i would have\", \"i’ll’ve\": \"i will have\", \"i’ma\": \"i am\", \"i’am\": 'i am', \"i’l\": \"i will\", \"i’v\": 'i have',\n\n            'i\"\"m': 'i am',\n\n            \"we'd\": 'we would', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"we'd've\": \"we would have\", \"we'll've\": \"we will have\",\n            'we,d': 'we would', 'we,ll': 'we will', 'we,re': 'we are', 'we,ve': 'we have', \"we,d,ve\": \"we would have\", \"we,ll,ve\": \"we will have\",\n            'we;d': 'we would', 'we;ll': 'we will', 'we;re': 'we are', 'we;ve': 'we have', \"we;d;ve\": \"we would have\", \"we;ll;ve\": \"we will have\",\n            'we´d': 'we would', 'we´ll': 'we will', 'we´re': 'we are', 'we´ve': 'we have', \"we´d´ve\": \"we would have\", \"we´ll´ve\": \"we will have\",\n            'we’d': 'we would', 'we’ll': 'we will', 'we’re': 'we are', 'we’ve': 'we have', \"we’d’ve\": \"we would have\", \"we’ll’ve\": \"we will have\",\n\n            \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": \"you have\", \"your'e\": \"you are\", \"u're\": \"you are\", \"ya'll\": \"you all\", \"you'r\": \"you are\",\n            'you,d': 'you would', 'you,ll': 'you will', 'you,re': 'you are', \"you,ve\": \"you have\", \"your,e\": \"you are\", \"u,re\": \"you are\", \"ya,ll\": \"you all\", \"you,r\": \"you are\", \n            'you;d': 'you would', 'you;ll': 'you will', 'you;re': 'you are', \"you;ve\": \"you have\", \"your;e\": \"you are\", \"u;re\": \"you are\", \"ya;ll\": \"you all\", \"you;r\": \"you are\",\n            'you´d': 'you would', 'you´ll': 'you will', 'you´re': 'you are', \"you´ve\": \"you have\", \"your´e\": \"you are\", \"u´re\": \"you are\", \"ya´ll\": \"you all\", \"you´r\": \"you are\",\n            'you’d': 'you would', 'you’ll': 'you will', 'you’re': 'you are', \"you’ve\": \"you have\", \"your’e\": \"you are\", \"u’re\": \"you are\", \"ya’ll\": \"you all\", \"you’r\": \"you are\",\n\n            \"y'all\": \"you all\", \"y'know\": \"you know\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n            \"y,all\": \"you all\", \"y,know\": \"you know\", \"y,all,d\": \"you all would\", \"y,all,d,ve\": \"you all would have\",\"y,all,re\": \"you all are\",\"y,all,ve\": \"you all have\", \n            \"y;all\": \"you all\", \"y;know\": \"you know\", \"y;all;d\": \"you all would\", \"y;all;d;ve\": \"you all would have\",\"y;all;re\": \"you all are\",\"y;all;ve\": \"you all have\", \n            \"y´all\": \"you all\", \"y´know\": \"you know\", \"y´all´d\": \"you all would\", \"y´all´d´ve\": \"you all would have\",\"y´all´re\": \"you all are\",\"y´all´ve\": \"you all have\", \n            \"y’all\": \"you all\", \"y’know\": \"you know\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\",\"y’all’re\": \"you all are\",\"y’all’ve\": \"you all have\", \n\n            \"you.i\": \"you i\",\n\n            \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"they'd've\": \"they would have\", \"they'll've\": \"they will have\",\n            'they,d': 'they would', 'they,ll': 'they will', 'they,re': 'they are', 'they,ve': 'they have', \"they,d,ve\": \"they would have\", \"they,ll,ve\": \"they will have\",\n            'they;d': 'they would', 'they;ll': 'they will', 'they;re': 'they are', 'they;ve': 'they have', \"they;d;ve\": \"they would have\", \"they;ll;ve\": \"they will have\",\n            'they´d': 'they would', 'they´ll': 'they will', 'they´re': 'they are', 'they´ve': 'they have', \"they´d´ve\": \"they would have\", \"they´ll´ve\": \"they will have\",\n            'they’d': 'they would', 'they’ll': 'they will', 'they’re': 'they are', 'they’ve': 'they have', \"they’d’ve\": \"they would have\", \"they’ll’ve\": \"they will have\",\n\n            \"it'd\": 'it would', \"it'll\": 'it will', \"it's\": 'it is', \"it'd've\": \"it would have\", \"it'll've\": \"it will have\",\n            'it,d': 'it would', 'it,ll': 'it will', 'it,s': 'it is', \"it,d,ve\": \"it would have\", \"it,ll,ve\": \"it will have\", \n            'it;d': 'it would', 'it;ll': 'it will', 'it;s': 'it is', \"it;d;ve\": \"it would have\", \"it;ll;ve\": \"it will have\",\n            'it´d': 'it would', 'it´ll': 'it will', 'it´s': 'it is', \"it´d´ve\": \"it would have\", \"it´ll´ve\": \"it will have\",\n            'it’d': 'it would', 'it’ll': 'it will', 'it’s': 'it is', \"it’d’ve\": \"it would have\", \"it’ll’ve\": \"it will have\",\n\n            \"this'll\": \"this all\", \"this's\": \"this is\",\n            \"this,ll\": \"this all\", \"this,s\": \"this is\",\n            \"this;ll\": \"this all\", \"this;s\": \"this is\",\n\n            \"that'd\": 'that would', \"that's\": 'that is', \"that'll\": \"that will\", \"that'd've\": \"that would have\",\n            'that,d': 'that would', 'that,s': 'that is', \"that,ll\": \"that will\", \"that,d,ve\": \"that would have\",\n            'that;d': 'that would', 'that;s': 'that is', \"that;ll\": \"that will\", \"that;d;ve\": \"that would have\",\n            'that´d': 'that would', 'that´s': 'that is', \"that´ll\": \"that will\", \"that´d´ve\": \"that would have\",\n            'that’d': 'that would', 'that’s': 'that is', \"that’ll\": \"that will\", \"that’d’ve\": \"that would have\",\n\n            \"there'd\": 'there had', \"there's\": 'there is', \"there'll\": \"there will\",\"there're\": \"there are\", \"there'd've\": \"there would have\",\n            'there,d': 'there had', 'there,s': 'there is', \"there,ll\": \"there will\",\"there,re\": \"there are\", \"there,d,ve\": \"there would have\",\n            'there;d': 'there had', 'there;s': 'there is', \"there;ll\": \"there will\",\"there;re\": \"there are\", \"there;d;ve\": \"there would have\",\n            'there´d': 'there had', 'there´s': 'there is', \"there´ll\": \"there will\",\"there´re\": \"there are\", \"there´d´ve\": \"there would have\",\n            'there’d': 'there had', 'there’s': 'there is', \"there’ll\": \"there will\",\"there’re\": \"there are\", \"there’d’ve\": \"there would have\",\n\n            \"here's\": \"here is\", \"here're\": \"here are\",\n            \"here,s\": \"here is\", \"here,re\": \"here are\",\n            \"here;s\": \"here is\", \"here;re\": \"here are\",\n\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n\n            \"where'd\": 'where did', \"where's\": 'where is', \"where've\": \"where have\", \"where're\": \"where are\",\n            'where,d': 'where did', 'where,s': 'where is', \"where,ve\": \"where have\", \"where,re\": \"where are\",\n            'where;d': 'where did', 'where;s': 'where is', \"where;ve\": \"where have\", \"where;re\": \"where are\",\n\n            \"who'll\": 'who will', \"who's\": 'who is', \"who'd\": \"who would\", \"who're\": \"who are\",\"who've\": \"who have\", \"who'll've\": \"who will have\",\n            'who,ll': 'who will', 'who,s': 'who is', \"who,d\": \"who would\", \"who,re\": \"who are\",\"who,ve\": \"who have\", \"who,ll,ve\": \"who will have\",\n            'who;ll': 'who will', 'who;s': 'who is', \"who;d\": \"who would\", \"who;re\": \"who are\",\"who;ve\": \"who have\", \"who;ll;ve\": \"who will have\",\n\n            \"how'd\": 'how did', \"how'll\": 'how will', \"how's\": 'how is', \"how'd'y\": \"how do you\",\n            'how,d': 'how did', 'how,ll': 'how will', 'how,s': 'how is', \"how,d,y\": \"how do you\",\n            'how;d': 'how did', 'how;ll': 'how will', 'how;s': 'how is', \"how;d;y\": \"how do you\",\n\n            \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"what'll've\": \"what will have\",\n            'what,ll': 'what will', 'what,re': 'what are', 'what,s': 'what is', 'what,ve': 'what have', \"what,ll,ve\": \"what will have\",\n            'what;ll': 'what will', 'what;re': 'what are', 'what;s': 'what is', 'what;ve': 'what have', \"what;ll;ve\": \"what will have\",\n\n            \"why'd\": \"why would\", \"why'll\": \"why will\", \"why're\": \"why are\", \"why's\": \"why is\", \"why've\": \"why have\",\n            \"why,d\": \"why would\", \"why,ll\": \"why will\", \"why,re\": \"why are\", \"why,s\": \"why is\", \"why,ve\": \"why have\",\n            \"why;d\": \"why would\", \"why;ll\": \"why will\", \"why;re\": \"why are\", \"why;s\": \"why is\", \"why;ve\": \"why have\",\n\n            \"let's\": 'let us', 'let,s': 'let us', 'let;s': 'let us',\n\n            \"ma'am\": 'madam', 'ma,am': 'madam', 'ma;am': 'madam',\n\n            \"wan't\": 'want', \"wan,t\": 'want', \"wan;t\": 'want',\n\n            \"agains't\": \"against\", \"agains,t\": \"against\", \"agains;t\": \"against\",\n\n            \"c'mon\": \"common\", \"c,mon\": \"common\", \"c;mon\": \"common\",\n\n            \"gov't\": \"government\", \"gov,t\": \"government\", \"gov;t\": \"government\",\n\n            'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at', 'ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\n            'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','CO₂':'carbon dioxide','ғɪʀsᴛ':'first','ᴇɴᴅ':'end',\n            'ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra','Gʀᴇᴀᴛ':'great',\n            'sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\n            'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\n            'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', \n            'ɴᴇᴇᴅ':' need ','ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start',\n        }\n        \n        self.contraction_mapping = {**contraction_mapping, **self._get_upper_contraction(contraction_mapping)}\n    \n    \n        self.spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n\n        self.special_punc_mappings = {\n            \"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '−': '-', '•': '-', \"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '−': '-', \n\n            '”': '\"', '″': '\"', '“': '\"', '“': '\"', '”': '\"', '“': '\"', \n            \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"’\": \"'\", \"`\": \"'\",\n\n            \"₹\": \"e\", \"€\": \"e\", \"™\": \"tm\",  \"×\": \"x\", \"²\": \"2\", \"£\": \"e\", 'à': 'a', '³': '3', \n            \"√\": \" sqrt \", 'α': 'alpha', 'β': 'beta', 'θ': 'theta', 'π': 'pi', '∞': 'infinity', '÷': '/', '∅': '',\n\n            '،':'', '„':'', \"°\": \"\", 'करना': '', 'है': '',\n            '…': ' ... '\n        }\n\n        self.punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\n    def _get_upper_contraction(self, mapping):\n        result_dict = {}\n        for key, value in mapping.items():\n            if key[0].isalpha():\n                key = key[0].upper() + key[1:]\n                value = value[0].upper() + value[1:]\n                result_dict[key] = value\n        return result_dict\n    \n\n    def _clean_contractions(self, text, mapping):\n        specials = [\"’\", \"‘\", \"´\", \"`\"]\n        for s in specials:\n            text = text.replace(s, \"'\")\n        text = ' '.join([mapping[t] if t in mapping else t for t in text.split()])\n        return text\n    \n    \n    # remove space\n    def _remove_space(self, text):\n        for space in self.spaces:\n            text = text.replace(space, ' ')\n\n        text = text.strip()\n        text = re.sub('\\s+', ' ', text)\n\n        return text\n    \n    # remove special punctuations\n    def _clean_special_punctuations(self, text):\n        for punc in self.special_punc_mappings:\n            text = text.replace(punc, self.special_punc_mappings[punc])\n\n        return text\n    \n    def _spacing_punctuations(self, text):\n        for p in self.punct:\n            text = text.replace(p, f' {p} ')\n\n        return text\n    \n    def _preprocess_punctuations(self, text):\n        text = self._remove_space(text)\n        text = self._clean_special_punctuations(text)\n        text = self._spacing_punctuations(text)\n        text = self._remove_space(text)\n\n        return text\n    \n    def preprocess(self, text):\n        text = self._clean_contractions(text, self.contraction_mapping)\n        text = self._preprocess_punctuations(text)\n        \n        return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"### LSTM"},{"metadata":{},"cell_type":"markdown","source":"* spatial dropout"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2) # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1) # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x) # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1) # (N, T, 1, K)\n        x = x.squeeze(2) # (N, T, K)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* embedding"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Embedding_LSTM(nn.Module):\n    def __init__(self, embedding_matrix):\n        super(Embedding_LSTM, self).__init__()\n        \n        self.embedding = nn.Embedding(embedding_matrix.shape[0], EMBED_SIZE * 2)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.embedding_dropout(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* encoder"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Encoder_LSTM(nn.Module):\n    def __init__(self, num_aux_targets):\n        super(Encoder_LSTM, self).__init__()\n    \n        self.cell1 = nn.LSTM(EMBED_SIZE * 2, HIDDEN_SIZE, bidirectional=True, batch_first=True)\n        self.cell2 = nn.LSTM(HIDDEN_SIZE * 2, HIDDEN_SIZE, bidirectional=True, batch_first=True)\n        \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n                \n    def forward(self, x):\n        cell1, _ = self.cell1(x)\n        cell2, _ = self.cell2(cell1)\n        \n        avg_pool = torch.mean(cell2, 1)\n        max_pool, _ = torch.max(cell2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* lstm model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Model_LSTM(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(Model_LSTM, self).__init__()\n        \n        self.embedding = Embedding_LSTM(embedding_matrix)\n        self.encoder = Encoder_LSTM(num_aux_targets)\n\n    def forward(self, x):\n        \n        x = self.embedding(x)\n        out = self.encoder(x) \n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceTemplate:\n    def __init__(self, preprocess_funcs=None, debug=False):\n        self.preprocess_funcs = preprocess_funcs\n        self.debug = debug\n    \n    # Data Related Functions\n    def _load_text(self, category=\"test\"):\n        \n        # load raw text\n        print(f\"Load {category} Text ...\")\n        path = f'../input/jigsaw-unintended-bias-in-toxicity-classification/{category}.csv'\n        text = pd.read_csv(path).comment_text.astype(str)\n        \n        # debug mode\n        if self.debug:\n            print(\"Debug Mode ...\")\n            text = text[:10000]\n        \n        # preprocessing\n        if self.preprocess_funcs:\n            print(\"Preprocessing ...\")\n            \n            for i, preprocess_func in enumerate(self.preprocess_funcs):\n                print(f\"---- Preprocessing {i}'th\")\n                text = text.apply(lambda v: preprocess_func.preprocess(v))\n        else:\n            print(\"No preprocessing ...\")\n            \n        print('\\n')\n        return text\n    \n    # Inference Related Functions\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* lstm inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceLstm(InferenceTemplate):\n\n    def make_data(self, preprocess_type=None):\n        \n        \n        # load text\n        if preprocess_type == 'sogna':\n            print(\"Train Text Preprocessing ... [sogna]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_sogna.csv')['comment_text_sogna'].astype(str)\n        elif preprocess_type == 'pb':\n            print(\"Train Text Preprocessing ... [pb]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_pb.csv')['comment_text_pb'].astype(str)\n        elif preprocess_type == 'sogna_pb':\n            print(\"Train Text Preprocessing ... [sogna + pb]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_sogna_pb.csv')['comment_text_sogna_pb'].astype(str)\n        else:\n            train_text = self._load_text('train')\n            \n        if self.debug:\n            train_text = train_text[:10000]\n        \n        # train_text = self._load_text('train')\n        test_text = self._load_text('test')\n        \n        # tokenizer\n        tokenizer = Tokenizer(lower=False, filters=\"\")\n        tokenizer.fit_on_texts(list(train_text) + list(test_text))\n        \n        # get token (sequence)\n        test_text = tokenizer.texts_to_sequences(list(test_text))\n        \n        return test_text, tokenizer\n    \n    \n    # Embedding Related Functions\n    def _load_embeddings(self, path):\n        with open(path,'rb') as f:\n            emb_arr = pickle.load(f)\n        return emb_arr\n    \n    def _build_matrix(self, word_index, path):\n        embedding_index = self._load_embeddings(path)\n        embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n        unknown_words = []\n\n        for word, i in word_index.items():\n            try: embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try: embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try: embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError: unknown_words.append(word)\n\n        return embedding_matrix, unknown_words\n    \n    def make_embedding(self, tokenizer):\n        \n        # crawl\n        crawl_matrix, unkown_words = self._build_matrix(tokenizer.word_index,\n                                                        CRAWL_EMBEDDING_PATH)\n        \n        print('n unknown words (crawl): ', len(unkown_words))\n        \n        # glove\n        glove_matrix, unkown_words = self._build_matrix(tokenizer.word_index, \n                                                        GLOVE_EMBEDDING_PATH)\n        \n        print('n unknown words (glove): ', len(unkown_words))\n        \n        # embedding_matrix\n        embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n        print(\"Embedding Matrix Shape: \", embedding_matrix.shape)\n        \n        del crawl_matrix, glove_matrix, unkown_words\n        gc.collect()\n        \n        return embedding_matrix\n    \n    # Model Related Functions\n    def make_model(self, embedding_matrix, path):\n        \n        # template\n        model = Model_LSTM(embedding_matrix, 7)\n        \n        # load weight\n        model.encoder.load_state_dict(torch.load(path)['model_state_dict'])\n        return model\n    \n    # Inference Related Functions\n    def _get_padding_size(self, lengths):\n        padding_size = 0\n        _max = np.max(lengths)\n        _threshold = np.ceil(np.quantile(lengths, 0.95))\n\n        if _max >= MAX_LEN:\n            padding_size = MAX_LEN\n\n        else:\n            padding_size = _max\n\n        return int(padding_size)\n    \n    def inference(self, test, model):\n        \n        model.cuda()\n        model.eval()\n        \n        total_batch = int(len(test) / BATCH_SIZE)\n        test_preds = np.zeros((len(test), 8))\n        \n        print(\"total batch: {}, test size: {}, batch size: {}\".format(total_batch, len(test), BATCH_SIZE))\n        \n        for i in range(total_batch + 1):\n\n            # get batch\n            batch_X = test[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n\n            # when data size divides by batch size\n            if len(batch_X) == 0: break\n\n            # adaptive padding size\n            padding_size = self._get_padding_size([len(v) for v in batch_X])\n            batch_X = pad_sequences(batch_X, maxlen=padding_size, padding='post')\n\n            # torch tensor\n            batch_X = torch.tensor(batch_X, dtype=torch.long).cuda()\n\n            # prediction\n            pred_y = model(batch_X).detach().squeeze(dim=-1)\n\n            # predict test target\n            test_preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE, :] = self._sigmoid(pred_y.cpu().numpy())\n            \n            del batch_X, pred_y\n            torch.cuda.empty_cache()\n            \n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* bert inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceBert(InferenceTemplate):\n    \n    # Data Related Functions\n    def _convert_lines(self, example, max_seq_length, tokenizer):\n        max_seq_length -=2\n        all_tokens = []\n        longer = 0\n        for text in tqdm_notebook(example):\n            tokens_a = tokenizer.tokenize(text)\n            if len(tokens_a) > max_seq_length:\n                tokens_a = tokens_a[:max_seq_length]\n                longer += 1\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n\n        print(\"Num Of Lines Over Max Sequences: {}/{}\".format(longer, len(all_tokens)))\n        return np.array(all_tokens)\n    \n    def make_data(self):\n        \n        # load text\n        test_text = self._load_text('test')\n            \n        # tokenize\n        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\n        \n        # get token (sequence)\n        test_text = self._convert_lines(test_text.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n        \n        return test_text\n    \n    # Model Related Functions\n    def make_model(self, path):\n        ckpt = torch.load(f'{path}', map_location='cuda:0')\n        \n        # bert config\n        bert_config = ckpt['bert_config']\n        \n        # model\n        model = BertForSequenceClassification(bert_config, num_labels=8)\n        model.load_state_dict(ckpt['model_state_dict'])\n        \n        return model\n    \n    # Inference Related Functions\n    def inference(self, test, model):\n        \n        model = model.to(device)\n\n        test_preds = np.zeros((len(test)))\n        test_dataset = data.TensorDataset(torch.tensor(test, dtype=torch.long))\n        test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        for param in model.parameters():\n            param.requires_grad=False\n        \n        model.eval()\n        \n        tq = tqdm_notebook(test_loader)\n        for i, (x_batch,)  in enumerate(tq):\n            pred = model(x_batch.to(device),\n                         attention_mask = (x_batch > 0).to(device), \n                         labels=None)\n            \n            # print(pred[:, 0].detach().cpu().squeeze().numpy())\n\n            test_preds[i*BATCH_SIZE:(i+1)*BATCH_SIZE]= self._sigmoid(pred[:,0].detach().cpu().squeeze().numpy())\n\n            del x_batch, pred\n            torch.cuda.empty_cache()\n\n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* gpt2 inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceGPT2(InferenceTemplate):\n    \n    # Data Related Functions\n    def _convert_lines(self, example, max_seq_length, tokenizer):\n        all_tokens = []\n        longer = 0\n        for text in tqdm_notebook(example):\n            tokens_a = tokenizer.tokenize(text)\n            if len(tokens_a) > max_seq_length:\n                tokens_a = tokens_a[:max_seq_length]\n                longer += 1\n            one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n\n        print(\"Num Of Lines Over Max Sequences: {}/{}\".format(longer, len(all_tokens)))\n        return np.array(all_tokens)\n    \n    def make_data(self):\n        \n        # load text\n        test_text = self._load_text('test')\n            \n        # tokenize\n        tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL_PATH, cache_dir=None)\n        \n        # get token (sequence)\n        test_text = self._convert_lines(test_text.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n        \n        return test_text\n    \n    # Model Related Functions\n    def make_model(self, path):\n        ckpt = torch.load(f'{path}')\n        \n        # bert config\n        bert_config = ckpt['bert_config']\n        \n        # model\n        model = GPT2ClassificationHeadModel(bert_config)\n        model.load_state_dict(ckpt['model_state_dict'])\n        \n        return model\n    \n    # Inference Related Functions\n    def inference(self, test, model):\n        \n        test_preds = np.zeros((len(test)))\n        test_dataset = data.TensorDataset(torch.tensor(test, dtype=torch.long))\n        test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        for param in model.parameters():\n            param.requires_grad=False\n        \n        model.cuda()\n        model.eval()\n        \n        tq = tqdm_notebook(test_loader)\n        for i, (x_batch,)  in enumerate(tq):\n            pred = model(x_batch.cuda())\n            \n            # print(self._sigmoid(pred[:, 0].detach().cpu().squeeze().numpy()))\n            \n            test_preds[i*BATCH_SIZE:(i+1)*BATCH_SIZE]= self._sigmoid(pred[:,0].detach().cpu().squeeze().numpy())\n\n            del x_batch, pred\n            torch.cuda.empty_cache()\n\n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nseed_everything(random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pb = Preprocess_pb_kernel()\npreprocess_sogna = Preprocess_sogna_kernel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nCRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m/pickled-crawl300d2m/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d/pickled-glove840b300d/glove.840B.300d.pkl'\nEMBED_SIZE = 300\nHIDDEN_SIZE = 128\nDENSE_HIDDEN_UNITS = 4 * HIDDEN_SIZE\nBATCH_SIZE = 512 * 8\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference class"},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_lstm = InferenceLstm(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make data and tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text, tokenizer = inference_lstm.make_data(preprocess_type='sogna')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = inference_lstm.make_embedding(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nall_test_preds = []\n\nfor model_idx in range(2):\n    test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(4)]\n\n    for epoch in range(4):  \n        path = f'../input/sub-toxic-lstm-weight/lstm_wh_model_idx_{model_idx}_epcoh_{epoch}.pt'\n        model = inference_lstm.make_model(embedding_matrix, path=path)\n        test_preds.append(inference_lstm.inference(test_text, model))\n\n    all_test_preds.append(np.average(test_preds, weights=checkpoint_weights, axis=0))\n\ntest_preds_lstm = np.mean(all_test_preds, axis=0)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embedding_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2"},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"trusted":true},"cell_type":"code","source":"PYTORCH_BERT_DIR = \"../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master/\"\nsys.path.insert(0, PYTORCH_BERT_DIR)\n\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2ClassificationHeadModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna, preprocess_pb]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nGPT2_MODEL_PATH = '../input/gpt2-pretrained-models/gpt2-models/'\nMAX_SEQUENCE_LENGTH = 260\nBATCH_SIZE = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_gpt2 = InferenceGPT2(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = inference_gpt2.make_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_gpt2.make_model('../input/sub-toxic-gpt2-weight/gpt2_dis_260len_2epoch_32batch_4accum_8e-05lr_0.005warmup_0.2dropout_all_full.pt')\ntest_preds_gpt2 = inference_gpt2.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* delete library"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"lib_modules_list = [s for s in list(sys.modules.keys()) if \"pytorch_pretrained_bert\" in s]\n\ndel GPT2Tokenizer, GPT2ClassificationHeadModel\nfor m in lib_modules_list :\n    del sys.modules[m]\n\nsys.path.remove(PYTORCH_BERT_DIR)\nsys.path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT"},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"trusted":true},"cell_type":"code","source":"PYTORCH_BERT_DIR = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT/\"\nsys.path.insert(0, PYTORCH_BERT_DIR)\n\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipe"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna, preprocess_pb]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nMAX_SEQUENCE_LENGTH = 300\nBATCH_SIZE = 256","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference class"},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_bert = InferenceBert(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = inference_bert.make_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_300len_2epoch_64batch_2accum_2e-05lr_all_full_0.01warm_1.3weight.pt')\ntest_preds_bert_94560 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_pretrained_300len_2epoch_64batch_2accum_2e-05lr_all_full_0.01warm_94501.pt')\ntest_preds_bert_94501 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_300len_2epoch_32batch_1accum_2e-05lr_full.pt')\ntest_preds_bert_94427 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* delete library"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"lib_modules_list = [s for s in list(sys.modules.keys()) if \"pytorch_pretrained_bert\" in s]\n\ndel BertTokenizer, BertForSequenceClassification\nfor m in lib_modules_list :\n    del sys.modules[m]\n\nsys.path.remove(PYTORCH_BERT_DIR)\nsys.path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT - KERAS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pprint\nimport time\nimport json\nimport os\nimport sys\nimport collections\nimport csv\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm_notebook\n\nos.environ['TF_KERAS'] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('pip install --no-index --find-links=\"../input/kerasbert/keras-bert-lib/\" keras-bert')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n\n# import python modules defined by BERT\nimport run_classifier\nimport modeling\nimport optimization\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\nBERT_PRETRAINED_PATH = '../input/bert-seq-360/'\nBERT_MODEL_PATH = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = os.listdir(BERT_PRETRAINED_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'keras-bert-toxic-vocab-360-02.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter\nMAX_SEQUENCE_LENGTH = 360\nLR = 2e-5\nloss_weight = 3.2092275837114372","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_FILE = os.path.join(BERT_MODEL_PATH, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_MODEL_PATH, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_MODEL_PATH, 'bert_model.ckpt')\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer_preprocess = TreebankWordTokenizer()\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer_preprocess.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\n# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_test['comment_text'] = df_test['comment_text'].astype(str) \ndf_test['comment_text'] = df_test['comment_text'].apply(lambda x:preprocess(x))\nX_test = convert_lines(df_test['comment_text'].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n\nX_seg_input = np.zeros((X_test.shape[0], MAX_SEQUENCE_LENGTH))\nX_mask_input = np.ones((X_test.shape[0], MAX_SEQUENCE_LENGTH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import load_trained_model_from_checkpoint\n\nbase_model = load_trained_model_from_checkpoint(CONFIG_FILE, checkpoint_file, training=True, seq_len=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef custom_target_loss(y_true, y_pred):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    target_loss = binary_crossentropy(K.reshape(y_true[:, 0], shape=(-1, 1)), y_pred) * y_true[:, 1]\n    return (target_loss * loss_weight)\n\ndef custom_aux_loss(y_true, y_pred):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    aux_loss = binary_crossentropy(y_true, y_pred)\n\n    return aux_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python import keras\nfrom keras_bert import calc_train_steps\n\nextract = base_model.get_layer('Extract').output\n\nfinal_layer = keras.layers.Dense(256, activation='relu')(extract)\nfinal_layer = keras.layers.Dropout(0.25)(final_layer)\n\ntarget_layer = keras.layers.Dense(1, activation='sigmoid', name='target_layer')(final_layer)\naux_layer = keras.layers.Dense(6, activation='sigmoid', name='aux_layer')(final_layer)\n\nmodel = keras.models.Model(inputs=base_model.input, outputs=[target_layer, aux_layer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(os.path.join(BERT_PRETRAINED_PATH, MODEL_NAME))\nmodel.compile(loss=[custom_target_loss, custom_aux_loss], optimizer=tf.train.AdamOptimizer(learning_rate=LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_939 = model.predict([X_test, X_seg_input, X_mask_input], batch_size=128, verbose=1, use_multiprocessing=True)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_gpt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94560","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94501","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94427","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_939","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.DataFrame(index=range(len(test_text)))\nmeta['lstm_939'] = test_preds_lstm\nmeta['gpt2_941'] = test_preds_gpt2\nmeta['bert_94560'] = test_preds_bert_94560\nmeta['bert_94501'] = test_preds_bert_94501\nmeta['bert_94427'] = test_preds_bert_94427\nmeta['bert_939'] = test_preds_bert_939\n\nmeta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nfinal_preds = (meta.bert_94560 ** 2) * 0.4 + (meta.bert_94501 ** 2) * 0.17 + (meta.bert_94427 ** 2) * 0.12 + (meta.gpt2_941 ** 2) * 0.11 + (meta.lstm_939 ** 2) * 0.1 + (meta.bert_939 ** 2) * 0.1\nsubmission.prediction = final_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}