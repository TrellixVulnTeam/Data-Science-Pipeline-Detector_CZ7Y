{"cells":[{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"HfRYwIBy2GGy","outputId":"a3038db3-5ce4-4559-ff8c-e10eaa3e6d8c","trusted":true},"cell_type":"code","source":"# importing required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom keras.layers import Activation,Input,CuDNNLSTM,CuDNNGRU, Embedding, LSTM, Dropout, BatchNormalization, Dense, concatenate, Flatten, Conv1D, MaxPool1D, LeakyReLU, ELU, SpatialDropout1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.utils import to_categorical,plot_model\nfrom keras.preprocessing.text import Tokenizer, one_hot\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, load_model,Sequential\nfrom keras import regularizers\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\nfrom keras.initializers import he_normal,Orthogonal\nfrom keras.regularizers import l2\nfrom keras.constraints import max_norm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix,classification_report\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"pRu6oJRfRFkq"},"cell_type":"markdown","source":"# Reading data"},{"metadata":{"colab":{},"colab_type":"code","id":"B9EFbxALRFkr","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explorarory Data Analysis"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"colab_type":"code","id":"T7VYcWotqwqo","outputId":"6bb91b8f-fa70-4cce-f37d-14ebc5fa9299","trusted":true},"cell_type":"code","source":"train.columns[train.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"colab_type":"code","id":"SFbLOYA-rIHK","outputId":"881675ed-edba-41bb-9d95-16014ec620b3","trusted":true},"cell_type":"code","source":"print(\"Target column has\", train['target'].isna().sum(), \"missing values\")\nprint(\"comment_text column has\", train['comment_text'].isna().sum(), \"missing values\")","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"colab_type":"code","id":"BP4Sn_j1rIYT","outputId":"674ec696-fc28-4153-f126-ce6f3f3565bf","trusted":true},"cell_type":"code","source":"# see distribution of the comment_text length\ntrain['comment_text'].str.len().hist()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"ifG1hMS0rIcX","outputId":"af004e7c-d9f1-472a-b706-b078a07ed5c3","trusted":true},"cell_type":"code","source":"test.columns[test.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"Exl_-Nxqqwwz","outputId":"ac60d49d-c1f7-4bb4-aef2-382ac1f13b73","trusted":true},"cell_type":"code","source":"print(\"comment_text column has\",test['comment_text'].isna().sum(), \"missing values\")","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"colab_type":"code","id":"h-kwAkL8qw4z","outputId":"bc6a6c38-8c8f-4653-a39c-d82dd27b4265","trusted":true},"cell_type":"code","source":"test.comment_text.str.len().hist()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"colab_type":"code","id":"HEAYSntvqw1v","outputId":"83af3dc2-403c-4806-d966-b05eaf2e526b","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":93},"colab_type":"code","id":"4WhvTsd0qwuu","outputId":"4ffa1795-c56d-4e6a-fcbc-6850b1029148","trusted":true},"cell_type":"code","source":"data = train.sample(frac=0.5)\nprint(\"Mean value for the target column:  \", data[\"target\"].mean())\nprint(\"Number of targets higher than 0.5: \", data[(data['target'] >0.5)][\"target\"].count())\nprint(\"Number of targets higher than 0.0: \", data[(data['target'] >0.0)][\"target\"].count())\nprint(\"Number of comments:                \", len(data))","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"pULS0HqRsJQo","outputId":"a13a7a37-3470-444c-ec63-e57c6b742ed8","trusted":true},"cell_type":"code","source":"data['label'] = np.where(data['target']>0,1,0)\nlabel_list = list(data['label'].unique())\nlabel_list\nprint(\"Number of targets higher than 0.0: \", data[(data['target'] >0.0)][\"target\"].count())","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"LYzj4Iz1sJVg","trusted":true},"cell_type":"code","source":"# this is a fraction of the sample fraction. Essentially we should always keep this between 0.7-0.9\ntraining_frac = 0.8\ntrain_len = int(len(data)*training_frac)\nvalid_len = int(len(data)*(1.0-training_frac))\n\ntrain = data.iloc[:train_len, :]\nvalid = data.iloc[:valid_len, :]","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"FLXGGhP7sJOG","trusted":true},"cell_type":"code","source":"train0 = train[train[\"label\"]==0]\ntrain1 = train[train[\"label\"]==1]\ntrain0[\"count\"] = train0['comment_text'].str.split().str.len()\ntrain1[\"count\"] = train1['comment_text'].str.split().str.len()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","id":"7fmoWsdgseHh","outputId":"12d23087-e75c-4c48-b0fc-352952288f99","trusted":true},"cell_type":"code","source":"import seaborn as sns\nx = pd.Series(train0[\"count\"])\nax = sns.distplot(x, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","id":"uuR4c8jRqwoX","outputId":"016179fa-6ca5-45c6-f66f-36baad66cab6","trusted":true},"cell_type":"code","source":"import seaborn as sns\nx = pd.Series(train1[\"count\"])\nax = sns.distplot(x, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811},"colab_type":"code","id":"Dn00HLYlsmIN","outputId":"f449990e-3282-419c-c269-89373ea7ffec","trusted":true},"cell_type":"code","source":"total_comments = train.shape[0]\nn_unique_comments = train['comment_text'].nunique()\nn_comments_both = len(set(train['comment_text'].unique()) & set(test['comment_text'].unique()))\nprint('Train set: %d rows and %d columns.' % (total_comments, train.shape[1]))\ndisplay(train.head())\ndisplay(train.describe())","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"colab_type":"code","id":"mXQczG04smNq","outputId":"b34e24de-27a1-4a3e-e9f9-cff0c1028bb0","trusted":true},"cell_type":"code","source":"\nprint('Test set: %d rows and %d columns.' % (test.shape[0], test.shape[1]))\ndisplay(test.head())\nprint('Number of unique comments: %d or %.2f%%'% (n_unique_comments, (n_unique_comments / total_comments * 100)))\nprint('Number of comments that are both in train and test sets: %d'% n_comments_both)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"colab_type":"code","id":"PFuM86f0smSD","outputId":"c492d688-5d47-4afc-a604-9a4bc10e9774","trusted":true},"cell_type":"code","source":"train['comment_length'] = train['comment_text'].apply(lambda x : len(x))\ntest['comment_length'] = test['comment_text'].apply(lambda x : len(x))\ntrain['word_count'] = train['comment_text'].apply(lambda x : len(x.split(' ')))\ntest['word_count'] = test['comment_text'].apply(lambda x : len(x.split(' ')))\nbin_size = max(train['comment_length'].max(), test['comment_length'].max())//10\n\nplt.figure(figsize=(20, 6))\nsns.distplot(train['comment_length'], bins=bin_size)\nsns.distplot(test['comment_length'], bins=bin_size)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"colab_type":"code","id":"bRbE46PysmoI","outputId":"b04006fc-4f92-448f-9d95-4afc58c89ee8","trusted":true},"cell_type":"code","source":"bin_size = max(train['word_count'].max(), test['word_count'].max())//10\nplt.figure(figsize=(20, 6))\nsns.distplot(train['word_count'], bins=bin_size)\nsns.distplot(test['word_count'], bins=bin_size)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"colab_type":"code","id":"ryDb2xUCsmxn","outputId":"b23d661d-1718-4bf3-c3ab-47a8d9bb0b06","trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\nsns.distplot(train['target'], bins=20, ax=ax1).set_title(\"Complete set\")\nsns.distplot(train[train['target'] > 0]['target'].values, bins=20, ax=ax2).set_title(\"Only toxic comments\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"colab_type":"code","id":"ZHJ2GUTUsm1U","outputId":"b851d059-82b4-4664-9a33-41162ac069c5","trusted":true},"cell_type":"code","source":"train['is_toxic'] = train['target'].apply(lambda x : 1 if (x > 0.5) else 0)\nplt.figure(figsize=(8, 6))\nsns.countplot(train['is_toxic'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"colab_type":"code","id":"3yufQesDu9AF","outputId":"c2868a9c-bbe0-4398-e625-e9844fd63efb","trusted":true},"cell_type":"code","source":"# Lets also see how many missing values (in percentage) we are dealing with\nmiss_val_train_df = train.isnull().sum(axis=0) / train_len\nmiss_val_train_df = miss_val_train_df[miss_val_train_df > 0] * 100\nmiss_val_train_df","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Pf7FCjmfsmuU","trusted":true},"cell_type":"code","source":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']\n# getting the dataframe with identities tagged\ntrain_labeled = train.loc[:, ['target'] + identities ].dropna()\n# lets define toxicity as a comment with a score being equal or .5\n# in that case we divide it into two dataframe so we can count toxic vs non toxic comment per identity\ntoxic = train_labeled[train_labeled['target'] >= .5][identities]\nnon_toxic = train_labeled[train_labeled['target'] < .5][identities]","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"eY_RJzcFsmsA","trusted":true},"cell_type":"code","source":"# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\ntoxic_count = toxic.where(train_labeled == 0, other = 1).sum()\nnon_toxic_count = non_toxic.where(train_labeled == 0, other = 1).sum()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":959},"colab_type":"code","id":"2urQT6B-smmJ","outputId":"1198b294-0c59-422f-c5ff-2e28a86da489","trusted":true},"cell_type":"code","source":"toxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\ntoxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\ntoxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20})","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"Jn_kBxaAsmMD","outputId":"896bd32e-3d77-484f-9152-df10cd84dc8a","trusted":true},"cell_type":"code","source":"# First we multiply each identity with the target\nweighted_toxic = train_labeled.iloc[:, 1:].multiply(train_labeled.iloc[:, 0], axis=\"index\").sum() \n# changing the value of identity to 1 or 0 only and get comment count per identity group\nidentity_label_count = train_labeled[identities].where(train_labeled == 0, other = 1).sum()\n# then we divide the target weighted value by the number of time each identity appears\nweighted_toxic = weighted_toxic / identity_label_count\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\n# plot the data using seaborn like before\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = weighted_toxic.values , y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxicity')\nplt.title('Weighted Analysis of Most Frequent Identities')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"c7Sur1jxxNhb","trusted":true},"cell_type":"code","source":"# lets take the dataset with identitiy tags, created date, and target column\nwith_date = train.loc[:, ['created_date', 'target'] + identities].dropna()\n# next we will create a weighted dataframe for each identity tag (like we did before)\n# first we divide each identity tag with the total value it has in the dataset\nweighted = with_date.iloc[:, 2:] / with_date.iloc[:, 2:].sum()\n# then we multiplty this value with the target \ntarget_weighted = weighted.multiply(with_date.iloc[:, 1], axis=\"index\")\n# lets add a column to count the number of comments\ntarget_weighted['comment_count'] = 1\n# now we add the date to our newly created dataframe (also parse the text date as datetime)\ntarget_weighted['created_date'] = pd.to_datetime(with_date['created_date'].apply(lambda dt: dt[:10]))\n# now we can do a group by of the created date to count the number of times a identity appears for that date\nidentity_weight_per_date = target_weighted.groupby(['created_date']).sum().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"nleNXVjRx0EW","trusted":true},"cell_type":"code","source":"# lets group most of the identities into three major categories as follows for simplified analysis\nraces = ['black','white','asian','latino','other_race_or_ethnicity']\nreligions = ['atheist', 'buddhist', 'christian', 'hindu', 'muslim', 'jewish','other_religion']\nsexual_orientation = ['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"colab_type":"code","id":"pyJUwgWYx0UE","outputId":"1e288837-b636-4846-8339-b4d90924a37f","trusted":true},"cell_type":"code","source":"# lets create a column to aggregate our weighted toxicity score per identity group\nidentity_weight_per_date['races_total'] = identity_weight_per_date[races].sum(axis=1)\nidentity_weight_per_date['religions_total'] = identity_weight_per_date[religions].sum(axis=1)\nidentity_weight_per_date['sexual_orientation_total'] = identity_weight_per_date[sexual_orientation].sum(axis=1)\n# and then plot a time-series line plot per identity group\nidentity_weight_per_date[['races_total', 'religions_total', 'sexual_orientation_total']].plot(figsize=(15,7), linewidth=1, fontsize=15) \nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"colab_type":"code","id":"ysbfNzOax0Yi","outputId":"696ab93d-390b-4bbe-8105-eb3f1d6d7a25","trusted":true},"cell_type":"code","source":"identity_weight_per_date['comment_count'].plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.xlabel('Comment Date', fontsize = 15)\nplt.ylabel('Total Comments', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"colab_type":"code","id":"E3A7NYRnx0Bi","outputId":"02a658d6-256d-4af4-e225-27d0db4f55c4","trusted":true},"cell_type":"code","source":"# lets divide by the comment count for the date to get a relative weighted toxic score\nidentity_weight_per_date['races_rel'] = identity_weight_per_date['races_total'] / identity_weight_per_date['comment_count']\nidentity_weight_per_date['religions_rel'] = identity_weight_per_date['religions_total'] / identity_weight_per_date['comment_count']\nidentity_weight_per_date['sexual_orientation_rel'] = identity_weight_per_date['sexual_orientation_total']  / identity_weight_per_date['comment_count']\n# now lets plot the data\nidentity_weight_per_date[['races_rel', 'religions_rel', 'sexual_orientation_rel']].plot(figsize=(15,7), linewidth=1, fontsize=20) \nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"colab_type":"code","id":"TJ0WmXCZxz-m","outputId":"32a2f6bd-6848-401c-8dcd-c023c5d01c3a","trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of races\nidentity_weight_per_date[races].div(identity_weight_per_date['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"colab_type":"code","id":"DcD8CgbmxNet","outputId":"9185d16f-be68-4ec1-eb76-96bb471990b1","trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of religions\nidentity_weight_per_date[religions].div(identity_weight_per_date['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"colab_type":"code","id":"1-B_sSfCyoBB","outputId":"97e33a66-f0ce-4994-a410-f9b9fded555f","trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of sexual orientation\nidentity_weight_per_date[sexual_orientation].div(identity_weight_per_date['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=20)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"2P-AcxsORFlC"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"cJveshdVRFlD","outputId":"72995302-8290-4839-85b4-e43f399e9012","trusted":true},"cell_type":"code","source":"\n\n# We clean the essay text data\n# For this task, we have defined some helper functions\n# The same function and code snippet will be used to clean project title\n# https://stackoverflow.com/a/47091490/4084039\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# https://gist.github.com/sebleier/554280\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n# Cleaning Text feature\npreprocessed_text = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train['comment_text'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_text.append(sent.lower().strip())\ntrain[\"clean_text\"] = preprocessed_text","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"hal7cK0MRFlG","outputId":"4b1593d8-1e2d-42fc-e4db-581f7d2565c6","trusted":true},"cell_type":"code","source":"# Combining all the above statemennts \npreprocessed_comments_test = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(test['comment_text'].values):\n    sent = decontracted(sentence)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_comments_test.append(sent.lower().strip())","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"kaV3A4XqRFlJ","trusted":true},"cell_type":"code","source":"test['comment_text'] = preprocessed_comments_test","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"yytRFz1kRFlM","trusted":true},"cell_type":"code","source":"train_len = len(train.index)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"colab_type":"code","id":"lWQLDf7MRFlO","outputId":"04cdc8b8-23cb-4c21-bd66-c804bec18bdd","trusted":true},"cell_type":"code","source":"miss_val_train_df = train.isnull().sum(axis=0) / train_len\nmiss_val_train_df = miss_val_train_df[miss_val_train_df > 0] * 100\nmiss_val_train_df","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"78IGIV26RFlQ","trusted":true},"cell_type":"code","source":"identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"fv81sIB-RFlT","trusted":true},"cell_type":"code","source":"for column in identity_columns + ['target']:\n    train[column] = np.where(train[column] >= 0.5, True, False)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"tjAGg3RPRFlV","trusted":true},"cell_type":"code","source":"y = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"OgyaF9TyRFlb"},"cell_type":"markdown","source":"### spliting into train and test set"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"colab_type":"code","id":"806IVB0QRFlc","outputId":"b32673e5-b4b1-4d02-c197-3b1c3b685174","trusted":true},"cell_type":"code","source":"# We split our dataset into train,cross-validation and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(train,y,test_size=0.30,random_state=43)\n\nprint(x_train.shape)\nprint(x_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"tf4j6TfZRFle","trusted":true},"cell_type":"code","source":"MAX_VOCAB_SIZE = 100000\nTOXICITY_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'\nMAX_SEQUENCE_LENGTH = 300\n\n# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(train[TEXT_COLUMN])\n\n# All comments must be truncated or padded to be the same length.\ndef padding_text(texts, tokenizer):\n    return sequence.pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\ntrain_text = padding_text(x_train[TEXT_COLUMN], tokenizer)\ntrain_y = to_categorical(x_train[TOXICITY_COLUMN])\nvalidate_text = padding_text(x_valid[TEXT_COLUMN], tokenizer)\nvalidate_y = to_categorical(x_valid[TOXICITY_COLUMN])\n# for submission purpose\ntest_text = padding_text(test[TEXT_COLUMN], tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\nimport os\nprint(os.listdir(\"../input/fasttext-crawl-300d-2m\"))","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"1k9xf272RFlq","outputId":"70b524c6-07c2-45c1-ca34-12a9c28c5388","trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec' ,encoding='utf8') as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nlen(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"aViHeDowRFlu","trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,300))\nnum_words_in_embedding = 0\nfor word, i in tokenizer.word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    num_words_in_embedding += 1\n    # words not found in embedding index will be all-zeros.\n    embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"colab_type":"code","id":"_BZ8HsRLRFlw","outputId":"553e3123-417d-47ed-dcf4-871a63b5e6dd","trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport pickle\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix,classification_report\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image,YouTubeVideo,HTML\nfrom keras.models import Sequential, Model\nfrom keras.utils import to_categorical,plot_model\nfrom keras.layers import Dense, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.initializers import he_normal\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding,CuDNNLSTM,CuDNNGRU, Flatten, Input, concatenate, Conv1D, GlobalMaxPooling1D, SpatialDropout1D, GlobalAveragePooling1D, Bidirectional\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.initializers import Orthogonal\nfrom keras.preprocessing.text import one_hot\nfrom keras.constraints import max_norm","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"colab_type":"code","id":"Aw24S5fZRFl0","outputId":"1ccf0feb-27cc-4a30-8ce0-5f7439814baa","trusted":true},"cell_type":"code","source":"input_text_bgru = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\nembedding_layer_bgru = Embedding(len(tokenizer.word_index) + 1,\n                                    300,\n                                    weights=[embedding_matrix],\n                                    input_length=MAX_SEQUENCE_LENGTH,\n                                    trainable=False)\ng = embedding_layer_bgru(input_text_bgru)\ng = SpatialDropout1D(0.4)(g)\ng = Bidirectional(CuDNNGRU(64, return_sequences=True))(g)\ng = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(g)\navg_pool = GlobalAveragePooling1D()(g)\nmax_pool = GlobalMaxPooling1D()(g)\ng = concatenate([avg_pool, max_pool])\ng = Dense(128, activation='relu')(g)\nbgru_output = Dense(2, activation='softmax')(g)\nmodel = Model(inputs=[input_text_bgru], outputs=[bgru_output])","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"colab_type":"code","id":"TnodSCv7RFl3","outputId":"4baa2930-76f1-445e-8fff-03d1f4af7ac8","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ndef auc1(y_true, y_pred):\n    if len(np.unique(y_true[:,1])) == 1:\n        return 0.5\n    else:\n        return roc_auc_score(y_true, y_pred)\n\ndef auroc(y_true, y_pred):\n    return tf.py_func(auc1, (y_true, y_pred), tf.double)\n\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=[auroc])","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":568},"colab_type":"code","id":"8AgT7kAHUffr","outputId":"b87ecc27-3e38-471c-b817-c6b0e20d1302","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"colab_type":"code","id":"E8Xn7p9PRFl5","outputId":"95ad31ed-bcbc-4ba0-a156-e75b36076537","trusted":true},"cell_type":"code","source":"history = model.fit(train_text,train_y,\n              batch_size=1024,\n              epochs=10,\n              validation_data=(validate_text, validate_y))","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"lHrEufoYwIyL"},"cell_type":"markdown","source":""},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"colab_type":"code","id":"jixbaiFCRFl8","outputId":"0257b02f-876b-4691-dec5-6d279fdbd844","trusted":true},"cell_type":"code","source":"plt.plot(history.history['auroc'], 'g')\nplt.plot(history.history['val_auroc'], 'r')\nplt.legend({'Train ROCAUC': 'g', 'Test ROCAUC':'r'})\nplt.show()\n\n\nplt.plot(history.history['loss'], 'g')\nplt.plot(history.history['val_loss'], 'r')\nplt.legend({'Train Loss': 'g', 'Test Loss':'r'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"qyM3jPyoRFl-","trusted":true},"cell_type":"code","source":"predictions = model.predict(test_text)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"uicvxEz7RFmA","trusted":true},"cell_type":"code","source":"submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nsubmit.prediction = predictions\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"jigsaw 3.ipynb","provenance":[],"toc_visible":true,"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}