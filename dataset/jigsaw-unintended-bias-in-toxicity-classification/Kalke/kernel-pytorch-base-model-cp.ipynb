{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport io\nimport os\nimport math\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nimport os\nimport random\nimport re\nimport string\nimport time\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.optim.optimizer import Optimizer\nimport tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nTRAIN_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\nTEST_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\nSAMPLE_SUBMISSION = '../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300\nmax_features = 100000\nmaxlen = 220\nbatch_size = 2048\ntrain_epochs = 5\nn_splits = 5\n\nseed = 1029\n\n@contextmanager\ndef timer(msg):\n    t0 = time.time()\n    print(f'[{msg}] start.')\n    yield\n    elspsed_time = time.time() - t0\n    print(f'[{msg}] done in {elspsed_time / 60:.2f} min.')\n    \ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nmisspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n    \ndef _get_missspell(misspell_dict):\n    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n    return misspell_dict , misspell_re\n\ndef replace_typeical_misspell(text):\n    misspellings , misspellings_re = _get_missspell(misspell_dict)\n    def replace(match):\n        return misspellings[match.group(0)]\n    return misspellings_re.sub(replace, text)\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n    x = str(x)\n    for punct in puncts + list(string.punctuation):\n        if punct in x:\n            x = x.replace(punct,f'{punct}')\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+',' ',x)\n\ndef get_coef(word,*arr):\n    return word,np.asarray(arr,dtype='float32')\n\ndef load_fasttext(word_index):\n    embeddings_index = dict(get_coef(*o.strip().split(' ')) for o in open(EMB_PATH))\n    nb_words = min(max_features, len(word_index))\n    \n    embeddings_matrix = np.zeros((nb_words,embed_size))\n    \n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embeddings_matrix[i] = embedding_vector\n    return embeddings_matrix\n\ndef load_and_prec():\n    train = pd.read_csv(TRAIN_PATH,index_col='id')\n    test = pd.read_csv(TEST_PATH,index_col='id')\n    \n    TEXT_COL = 'comment_text'\n    #lower\n    train[TEXT_COL] = train[TEXT_COL].str.lower()\n    test[TEXT_COL] = test[TEXT_COL].str.lower()\n    \n    #clean misspellings\n    train[TEXT_COL] = train[TEXT_COL].apply(replace_typeical_misspell)\n    test[TEXT_COL] = test[TEXT_COL].apply(replace_typeical_misspell)\n    \n    #clean text\n    train[TEXT_COL] = train[TEXT_COL].apply(clean_text)\n    test[TEXT_COL] = test[TEXT_COL].apply(clean_text)\n    \n    #clean numbers\n    train[TEXT_COL] = train[TEXT_COL].apply(clean_numbers)\n    test[TEXT_COL] = test[TEXT_COL].apply(clean_numbers)\n    \n    #fill up all missing values\n    train_x = train[TEXT_COL].fillna(\"_##_\").values\n    test_x = test[TEXT_COL].fillna(\"_##_\").values\n    \n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_x))\n    train_x = tokenizer.texts_to_sequences(train_x)\n    test_x = tokenizer.texts_to_sequences(test_x)\n    \n    #pad_the sentences\n    train_x = pad_sequences(train_x, maxlen=maxlen)\n    test_x = pad_sequences(test_x, maxlen=maxlen)\n    \n    train_y = (train['target'].values > 0.5).astype(int)\n    \n    np.random.seed(seed)\n    train_idx = np.random.permutation(len(train_x))\n    \n    train_x = train_x[train_idx]\n    train_y = train_y[train_idx]\n    \n    return train_x, train_y, test_x, tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self,embedding_matrix):\n        super(NeuralNet,self).__init__()\n        lstm_hidden_size = 120\n        gru_hidden_size = 60\n        self.gru_hidden_size = gru_hidden_size\n        \n        self.embedding = nn.Embedding(max_features,embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = nn.Dropout2d(0.1)\n        \n        self.lstm = nn.LSTM(embed_size,lstm_hidden_size,bidirectional=True,batch_first=True)\n        self.gru = nn.GRU(lstm_hidden_size*2,gru_hidden_size,bidirectional=True,batch_first=True)\n        \n        self.linear = nn.Linear(gru_hidden_size*6,16)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(16,1)\n        \n    def forward(self,x):\n        h_embedding = self.embedding(x)\n        h_embedding = torch.unsqueeze(h_embedding.transpose(1,2),2)\n        h_embedding = torch.squeeze(self.embedding_dropout(h_embedding)).transpose(1,2)\n        \n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, hh_gru = self.gru(h_lstm)\n\n        hh_gru = hh_gru.view(-1 , self.gru_hidden_size * 2)\n        avg_pool = torch.mean(h_gru,1)\n        max_pool,_ = torch.max(h_gru,1)\n        \n        conc = torch.cat((hh_gru,avg_pool,max_pool),1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out\n    \ndef sigmoid(x):\n    return 1/ (1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n    \nwith timer('load data'):\n    train_x, train_y, test_x, word_index = load_and_prec()\n    embedding_matrix = load_fasttext(word_index)\n        \nwith timer('train'):\n    train_preds = np.zeros((len(train_x)))\n    test_preds = np.zeros(len(test_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('train'):\n    train_preds = np.zeros((len(train_x)))\n    test_preds = np.zeros(len(test_x))\n        \n    seed_torch(seed)\n        \n    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda()\n    test = torch.utils.data.TensorDataset(x_test_cuda)\n    test_loader = torch.utils.data.DataLoader(test,batch_size=batch_size, shuffle = False)\n        \n    splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x,train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_idx, valid_idx) in enumerate(splits):\n    x_train_fold = torch.tensor(train_x[train_idx],dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis],dtype=torch.float32).cuda()\n            \n    x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(train_y[valid_idx,np.newaxis],dtype=torch.float32).cuda()\n            \n    model = NeuralNet(embedding_matrix)\n    model.cuda()\n            \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n            \n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold,y_val_fold)\n            \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n            \n    print(f'Fold {fold + 1}')\n    for epoch in range(train_epochs):\n        start_time = time.time()\n                \n        model.train()\n        avg_loss = 0.\n                \n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(x_batch)\n                    \n            loss = loss_fn(y_pred, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n                    \n        model.eval()\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros(len(test_x))\n        avg_val_loss = 0.\n                \n        #raw_sum = 0\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            #\n            with torch.no_grad():\n                y_pred = model(x_batch).detach()\n                \n                #print('i is {i}','batch_size is {batch_size}')        \n                \n                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n                #print('the i-->{} and the raw_sum--_>{}'.format(i,raw_sum))\n                \n                valid_preds_fold[i*batch_size:(i+1)*batch_size] = sigmoid(y_pred.cpu().numpy())[:,0]\n                #raw_sum +=y_pred.size(0)\n                \n                \n            elapsed_time = time.time() - start_time\n            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n                    epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n                \n        for i,(x_batch,) in enumerate(test_loader):\n            with torch.no_grad():\n                y_pred = model(x_batch).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        train_preds[valid_idx] = valid_preds_fold\n        test_preds += test_preds_fold / len(splits)\n        \n    print(f'cv score: {roc_auc_score(train_y, train_preds):<8.5f}')\n    \nwith timer('submit'):\n    submission = pd.read_csv(SAMPLE_SUBMISSION,index_col='id')\n    submission['prediction'] = test_preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('../input/submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('submit'):\n    submission = pd.read_csv(SAMPLE_SUBMISSION,index_col='id')\n    submission['prediction'] = test_preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val_fold.size(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val_fold.size(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(360976-360448)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}