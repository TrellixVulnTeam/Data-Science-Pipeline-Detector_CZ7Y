{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\nimport six\n### IMPORTS\nimport torch\nimport os\nfrom torch.utils import data\nfrom tqdm import tqdm\nimport torch.nn.init as init\nfrom sklearn.metrics import average_precision_score,f1_score\nfrom datetime import datetime\nimport logging\nimport pickle\nfrom sklearn import model_selection\nfrom datetime import datetime\nfrom sklearn import metrics\nimport sys\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch, BertModel\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport shutil\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"## TOKENIZATION CODE"},{"metadata":{"trusted":true},"cell_type":"code","source":"### TOKENIZER\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(\"utf-8\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n    ids = []\n    for token in tokens:\n        ids.append(vocab[token])\n    return ids\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_tokens_to_ids(self.vocab, tokens)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenization.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(\"C\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', \n                    'christian', 'jewish','muslim', 'black',\n                    'white', 'psychiatric_or_mental_illness']\nTOXICITY_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = \"../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/\"\ninit_checkpoint_pt = os.path.join(BERT_MODEL_PATH, \"pytorch_model.bin\")\nbert_config_file = os.path.join(BERT_MODEL_PATH, \"bert_config.json\")\nvocab_file = os.path.join(BERT_MODEL_PATH, \"vocab.txt\")\ndo_lower_case = True\ntokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\ndevice = torch.device('cuda')\nmodel_save_path = '../input/trained-models/model_3_new.cpt'\nmax_seq_len = 500\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = \"../working/\"\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertModel.from_pretrained('../working/').to('cuda')\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## BERT EMBEDDINGS CODE"},{"metadata":{"trusted":true},"cell_type":"code","source":"### fetching bert embeddings for the list\ndef Bert_embedder(sentences):\n    tokenized_list = []\n    indexed_list = []\n    segments = []\n    tokens_tensors = []\n    segments_tensors = []\n    for sents in sentences:\n            tokens = tokenizer.tokenize(sents)\n            if len(tokens) > max_seq_len:\n                tokens_up = tokens[0:max_seq_len]\n            else:\n                \n                tokens_up = tokens\n            tokens_up = [\"[CLS]\"] + tokens_up + [\"[SEP]\"]\n                \n            tokenized_list.append(tokens_up)\n\n    \n    ### Padding the tokens\n    pad_token = 1\n    X_lengths = [len(sentence) for sentence in tokenized_list]\n    padded_tokenized_list = [[]]\n    longest_sent = max(X_lengths)\n    batch_size = len(tokenized_list)\n    indexed_list = np.zeros((batch_size, longest_sent)) * pad_token\n    \n        \n        \n    for i, tokenized_text in enumerate(tokenized_list):\n        ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n        for j, toks in enumerate(ids):\n            indexed_list[i, j] = toks\n    \n    \n    for indexed_tokens in indexed_list:\n        segments.append([0]*longest_sent)\n    \n    for indexed_tokens in indexed_list:\n        tokens_tensors.append(torch.tensor([indexed_tokens]).to('cuda').long())\n    for segment in segments :   \n        segments_tensors.append(torch.tensor([segment]).to('cuda').long())\n    \n    embeddings = torch.Tensor().to('cuda')\n    for i, (tokens_tensor, segments_tensor) in enumerate(zip(tokens_tensors,segments_tensors )): \n        with torch.no_grad():\n            encoded_layers, _ = model(tokens_tensor, segments_tensor)\n            embeddings = torch.cat((embeddings, encoded_layers[-1]), 0)\n            \n    return embeddings, X_lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Toxic_classifier(torch.nn.Module):\n    def __init__(self, input_dim, out_dim):\n        super(Toxic_classifier, self).__init__()\n        #self.lstm = torch.nn.LSTM(input_dim, out_dim, bidirectional=True, batch_first=True)\n        self.lstm = torch.nn.LSTM(input_dim, out_dim, bidirectional=True, batch_first=True, dropout=0.5)\n        self.Weights = torch.nn.Parameter(torch.Tensor(2*out_dim))\n        self.Weights.data.uniform_(-0.1, 0.1)\n        self.soft = torch.nn.Softmax()\n        self.fc_layers = torch.nn.Linear(512, 128, bias=True)\n        #self.linear = torch.nn.Linear(128, 64, bias=False)\n        self.linear_layers = torch.nn.Linear(128, 2, bias=True)\n        \n               \n    def forward(self, inputs, max_seq_len):\n        lstm_out, _= self.lstm(inputs)\n        prod = torch.einsum('ijk,k->ij', (lstm_out, self.Weights))  \n        soft_out = self.soft(prod)\n            #print(soft_out)\n        attn_state_prod = torch.einsum('ij,ijl->ijl', (soft_out, lstm_out))\n        sum_state = torch.sum(attn_state_prod, dim=1)\n        fc_state = torch.relu(self.fc_layers(sum_state))\n        #temp_state = torch.relu(self.linear(fc_state))\n           \n            #print(fc_state[action].size())\n        final_out = self.linear_layers(fc_state)\n        return final_out\n\n        \n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Toxic_model = torch.load(model_save_path)\nToxic_model.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PREDICTION PART"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create dataset and dataloader for loading batches \nclass Dataset(data.Dataset):\n    \n    def __init__(self, data):\n        self.data = data\n       \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        record = self.data.iloc[index]\n\n        return record.to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = Dataset(test)\ndataset_loader_test = data.DataLoader(val_dataset,batch_size=256, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['prediction'] = 0\nvalidate_latest = pd.DataFrame()\nsubmission = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in tqdm(dataset_loader_test):\n    #print(torch.softmax(Toxic_model(batch['comment_text'], 400), 1))\n    validate_latest = pd.DataFrame()\n    embeddings, lengths =  Bert_embedder(batch['comment_text'])\n    y_out = Toxic_model(embeddings, max_seq_len)\n    #print(y_out[:,1].detach().cpu().numpy())\n    y_out = torch.softmax(y_out, 1)\n    batch_ids = batch['id'].detach().cpu().numpy().tolist()\n    output = y_out[:,1].detach().cpu().numpy().tolist()\n    validate_latest['id'] = batch_ids\n    validate_latest['prediction'] = output\n    submission = submission.append(validate_latest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}