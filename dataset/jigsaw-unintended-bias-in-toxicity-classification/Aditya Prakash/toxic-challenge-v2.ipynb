{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport sys, os, re, csv, codecs\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, Activation, Dropout, CuDNNLSTM, GlobalMaxPool1D,CuDNNGRU\nfrom keras.layers import Embedding, Bidirectional, Concatenate, SpatialDropout1D\nfrom keras.models import Model\nfrom keras import regularizers, initializers, constraints, layers, optimizers\nimport keras\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'].isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.where(train['target'].values >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_list = preprocess(train['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(list(train_list))\ntrain_tokenize_list = tokenizer.texts_to_sequences(list(train_list))\n#test_tokenize_list = tokenizer.texts_to_sequences(list(test_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sen_length = [len(i) for i in train_tokenize_list]\nplt.hist(sen_length, bins=np.arange(0, 200, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 190\ntrain_pad_list = pad_sequences(train_tokenize_list, maxlen= max_len)\n#test_pad_list = pad_sequences(test_tokenize_list, maxlen= max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n]\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding_index = load_embeddings('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#type(embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import operator \n\n# def check_coverage(vocab,embeddings_index):\n#     a = {}\n#     oov = {}\n#     k = 0\n#     i = 0\n#     for word in vocab:\n#         try:\n#             a[word] = embeddings_index[word]\n#             k += vocab[word]\n#         except:\n\n#             oov[word] = vocab[word]\n#             i += vocab[word]\n#             pass\n\n#     print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n#     print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n#     sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n#     return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import tqdm\n#oov = check_coverage(tokenizer.word_docs,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inp = Input(shape=(max_len,))\n# embed_size = 300\n# e = Embedding(*embedding_matrix.shape, weights = [embedding_matrix], trainable = False)(inp)\n# x = CuDNNLSTM(128, return_sequences= True, name = 'lstm_layer')(e)\n# x = CuDNNLSTM(128, return_sequences= True, name = 'lstm_layer2')(x)\n# x = GlobalMaxPool1D()(x)\n# x = Dropout(0.1)(x)\n# x = Dense(64, activation='relu')(x)\n# x = Dropout(0.1)(x)\n# x = Dense(1, activation='sigmoid')(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(max_len,))\nx = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(inp)\nx = SpatialDropout1D(0.3)(x)\nx1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\nx2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\nmax_pool1 = GlobalMaxPool1D()(x1)\nmax_pool2 = GlobalMaxPool1D()(x2)\nconc = Concatenate()([max_pool1, max_pool2])\npredictions = Dense(1, activation='sigmoid')(conc)\nmodel = Model(inputs=inp, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = Model(inputs=inp, outputs=x)\nmodel.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5'\nmcCallBack = keras.callbacks.ModelCheckpoint(filepath,save_best_only=True,mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nepochs = 3\nmodel.fit(train_pad_list, y, epochs= epochs, batch_size=batch_size, validation_split=0.1, callbacks= [mcCallBack])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_list = test['comment_text']\ntest_tokenize_list = tokenizer.texts_to_sequences(list(test_list))\ntest_pad_list = pad_sequences(test_tokenize_list, maxlen= max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(test_pad_list, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nsample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub['prediction'] = y_test\n#sample_sub.to_csv('../input/submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# import sys\n# import requests\n# from tqdm import tqdm\n\n# # if len(sys.argv) != 2:\n# #     print('You must enter the model name as a parameter, e.g.: download_model.py 117M')\n# #     sys.exit(1)\n\n# model = '117M'\n\n# subdir = os.path.join('models', model)\n# if not os.path.exists(subdir):\n#     os.makedirs(subdir)\n# subdir = subdir.replace('\\\\','/') # needed for Windows\n\n# for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n\n#     r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n\n#     with open(os.path.join(subdir, filename), 'wb') as f:\n#         file_size = int(r.headers[\"content-length\"])\n#         chunk_size = 1000\n#         with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n#             # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n#             for chunk in r.iter_content(chunk_size=chunk_size):\n#                 f.write(chunk)\n#                 pbar.update(chunk_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.makedirs('models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}