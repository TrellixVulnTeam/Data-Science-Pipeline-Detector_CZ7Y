{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport sys\nimport time\nimport json\nimport random\nimport unicodedata\nimport multiprocessing\nfrom functools import partial, lru_cache\n\nimport emoji\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom nltk import TweetTokenizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.sequence import pad_sequences\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n\nCUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"‘\": \"'\",\n        \"’\": \"'\",\n        \"`\": \"'\",\n        \"“\": '\"',\n        \"”\": '\"',\n        \"«\": '\"',\n        \"»\": '\"',\n        \"ɢ\": \"G\",\n        \"ɪ\": \"I\",\n        \"ɴ\": \"N\",\n        \"ʀ\": \"R\",\n        \"ʏ\": \"Y\",\n        \"ʙ\": \"B\",\n        \"ʜ\": \"H\",\n        \"ʟ\": \"L\",\n        \"ғ\": \"F\",\n        \"ᴀ\": \"A\",\n        \"ᴄ\": \"C\",\n        \"ᴅ\": \"D\",\n        \"ᴇ\": \"E\",\n        \"ᴊ\": \"J\",\n        \"ᴋ\": \"K\",\n        \"ᴍ\": \"M\",\n        \"Μ\": \"M\",\n        \"ᴏ\": \"O\",\n        \"ᴘ\": \"P\",\n        \"ᴛ\": \"T\",\n        \"ᴜ\": \"U\",\n        \"ᴡ\": \"W\",\n        \"ᴠ\": \"V\",\n        \"ĸ\": \"K\",\n        \"в\": \"B\",\n        \"м\": \"M\",\n        \"н\": \"H\",\n        \"т\": \"T\",\n        \"ѕ\": \"S\",\n        \"—\": \"-\",\n        \"–\": \"-\",\n    }\n)\n\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\nHEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\n\ndef my_demojize(string: str) -> str:\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n\n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\n\ndef normalize(text: str) -> str:\n    text = my_demojize(text)\n\n    text = RE_SPACE.sub(\" \", text)\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.translate(TABLE)\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n\n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n\n    return text\n\n\nPORTER_STEMMER = PorterStemmer()\nLANCASTER_STEMMER = LancasterStemmer()\nSNOWBALL_STEMMER = SnowballStemmer(\"english\")\n\ndef word_forms(word):\n    yield word\n    yield word.lower()\n    yield word.upper()\n    yield word.capitalize()\n    yield PORTER_STEMMER.stem(word)\n    yield LANCASTER_STEMMER.stem(word)\n    yield SNOWBALL_STEMMER.stem(word)\n    \ndef maybe_get_embedding(word, model):\n    for form in word_forms(word):\n        if form in model:\n            return model[form]\n\n    word = word.strip(\"-'\")\n    for form in word_forms(word):\n        if form in model:\n            return model[form]\n\n    return None\n\n\ndef gensim_to_embedding_matrix(word2index, path):\n    model = KeyedVectors.load(path, mmap=\"r\")\n    embedding_matrix = np.zeros((max(word2index.values()) + 1, model.vector_size), dtype=np.float32)\n    unknown_words = []\n\n    for word, i in word2index.items():\n        maybe_embedding = maybe_get_embedding(word, model)\n        if maybe_embedding is not None:\n            embedding_matrix[i] = maybe_embedding\n        else:\n            unknown_words.append(word)\n\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# test = test.head(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nwith multiprocessing.Pool(processes=2) as pool:\n     text_list = pool.map(normalize, test.comment_text.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n\ntest_word_sequences = []\nword_dict = {}\nword_index = 1\n\nfor doc in text_list:\n    word_seq = []\n    for token in tknzr.tokenize(doc):\n        if token not in word_dict:\n            word_dict[token] = word_index\n            word_index += 1\n        word_seq.append(word_dict[token])\n    test_word_sequences.append(word_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\ntest_lengths = torch.from_numpy(np.array([len(x) for x in test_word_sequences]))\nmaxlen = test_lengths.max() \nprint(f\"Max len = {maxlen}\")\nmaxlen = min(maxlen, 400)\n\nx_test_padded = torch.tensor(pad_sequences(test_word_sequences, maxlen=maxlen)).long()\ntest_collator = SequenceBucketCollator(torch.max, sequence_index=0, length_index=1)\n\ndel text_list, test_word_sequences, tknzr\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nglove_matrix, _ = gensim_to_embedding_matrix(\n    word_dict,\n    \"../input/gensim-embeddings-dataset/glove.840B.300d.gensim\",\n)\n\ncrawl_matrix, _ = gensim_to_embedding_matrix(\n    word_dict, \n    \"../input/gensim-embeddings-dataset/crawl-300d-2M.gensim\",\n)\n\npara_matrix, _ = gensim_to_embedding_matrix(\n    word_dict, \n    \"../input/gensim-embeddings-dataset/paragram_300_sl999.gensim\",\n)\n\nw2v_matrix, _ = gensim_to_embedding_matrix(\n    word_dict, \n    \"../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim\",\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\ndef one_hot_char_embeddings(word2index, char_vectorizer):\n    words = [\"\"] * (max(word2index.values()) + 1)\n    for word, i in word2index.items():\n        words[i] = word\n\n    return char_vectorizer.transform(words).toarray().astype(np.float32)\n\nchar_matrix = one_hot_char_embeddings(\n    word_dict,\n    joblib.load('../input/jigsaw-solution-ver-1/char_vectorizer.pkl'),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 6 * LSTM_UNITS\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)\n        x = x.permute(0, 3, 2, 1)\n        x = super(SpatialDropout, self).forward(x)\n        x = x.permute(0, 3, 2, 1)\n        x = x.squeeze(2)\n        return x\n    \n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, output_aux_sub=11):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(embedding_matrix.shape[0], embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS + 6 + output_aux_sub, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, 6)\n        self.linear_sub_out = nn.Linear(DENSE_HIDDEN_UNITS, output_aux_sub)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        avg_pool1 = torch.mean(h_lstm1, 1)\n        avg_pool2 = torch.mean(h_lstm2, 1)\n        max_pool2, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((avg_pool1, max_pool2, avg_pool2), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n\n        aux_result = self.linear_aux_out(hidden)\n        sub_result = self.linear_sub_out(hidden)\n        result = self.linear_out(torch.cat((hidden, aux_result, sub_result), 1))\n        out = torch.cat([result, aux_result, sub_result], 1)\n        return out\n    \n    \ndef get_lstm_preds(model_name, embedding_matrix):\n    model = NeuralNet(embedding_matrix)\n    temp_dict = torch.load('../input/jigsaw-solution-ver-1/' + model_name)\n    temp_dict['embedding.weight'] = torch.tensor(embedding_matrix)\n    model.load_state_dict(temp_dict)\n    model = model.cuda()\n    for param in model.parameters():\n        param.requires_grad=False\n    model = model.eval()\n    \n    batch_size = 256\n    test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_collator)\n    \n    preds = np.zeros((len(test_dataset), 18), dtype=np.float32)\n    with torch.no_grad():\n        for i, (x_batch) in enumerate(test_loader):\n            X_1 = x_batch[0].cuda()\n            y_pred = torch.sigmoid(model(X_1)).cpu().numpy()\n            preds[i * batch_size:(i + 1) * batch_size] = y_pred\n            \n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM inference"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nlstm_models = ['Notebook_100_5.bin',\n               'Notebook_100_6.bin', \n               'Notebook_100_7.bin',\n               'Notebook_100_8.bin',\n               'Notebook_100_9.bin', \n               'Notebook_100_10.bin', \n               'Notebook_100_11.bin',\n               'Notebook_100_12.bin']\n\nembedding_matrix = np.concatenate([glove_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n\nall_lstm_preds = []\nfor model_name in lstm_models:\n    print(model_name)\n    preds = get_lstm_preds(model_name, embedding_matrix)\n    all_lstm_preds.append(preds)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(embedding_matrix.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lstm_models = ['Notebook_100_13.bin']\n\n# embedding_matrix = np.concatenate([glove_matrix, crawl_matrix, para_matrix, char_matrix], axis=1)\n\n# for model_name in lstm_models:\n#     print(model_name)\n#     preds = get_lstm_preds(model_name, embedding_matrix)\n#     all_lstm_preds.append(preds)\n#     gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nlstm_models = ['Notebook_100_1.bin']\n\nembedding_matrix = np.concatenate([para_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n\nfor model_name in lstm_models:\n    print(model_name)\n    preds = get_lstm_preds(model_name, embedding_matrix)\n    all_lstm_preds.append(preds)\n    \nlstm_models = ['Notebook_100_2.bin']\n\nembedding_matrix = np.concatenate([glove_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n\nfor model_name in lstm_models:\n    print(model_name)\n    preds = get_lstm_preds(model_name, embedding_matrix)\n    all_lstm_preds.append(preds)\n    \nlstm_models = ['Notebook_100_3.bin']\n\nembedding_matrix = np.concatenate([glove_matrix, para_matrix, w2v_matrix, char_matrix], axis=1)\n\nfor model_name in lstm_models:\n    print(model_name)\n    preds = get_lstm_preds(model_name, embedding_matrix)\n    all_lstm_preds.append(preds)\n\nlstm_models = ['Notebook_100_4.bin']\n\nembedding_matrix = np.concatenate([glove_matrix, para_matrix, crawl_matrix, char_matrix], axis=1)\n\nfor model_name in lstm_models:\n    print(model_name)\n    preds = get_lstm_preds(model_name, embedding_matrix)\n    all_lstm_preds.append(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def simple_magic(preds):\n    return (\n        preds[:, 0] + preds[:, 1] * 0.05 - preds[:, -1] * 0.05 - preds[:, 4] * 0.05\n    )\n\ndef sophisticated_magic(preds):\n    return (\n        preds[:, 0] + preds[:, 1] * 0.05 - preds[:, -1] * 0.03 - preds[:, 4] * 0.03\n    ) - preds[:, 14] * (1 - preds[:, 0]) * 0.05 - preds[:, 10] * (1 - preds[:, 0]) * 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from scipy import stats\n\nall_lstm_preds = np.vstack([sophisticated_magic(x) for x in all_lstm_preds])\n# preds_lstm = np.median(all_lstm_preds, axis=0)  # MEDIAN ensemble\n# preds_lstm = stats.trim_mean(all_lstm_preds, 0.1, axis=0) # 10%-trimmed-mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"STORE = {\n    'lstm': all_lstm_preds.mean(0),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del crawl_matrix, glove_matrix, para_matrix, w2v_matrix, embedding_matrix\ndel test_lengths, maxlen, x_test_padded, test_collator\ndel lstm_models, all_lstm_preds\ndel word_dict, word_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT inference"},{"metadata":{"trusted":false},"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertForSequenceClassification\n\nUNCASED_BERT_MODEL_PATH = \"../input/transformer-tokenizers/bert-base-uncased/\"\nCASED_BERT_MODEL_PATH = \"../input/transformer-tokenizers/bert-base-cased/\"\n\n\ndef clip_to_max_len(batch):\n    X, lengths = map(torch.stack, zip(*batch))\n    max_len = torch.max(lengths).item()\n    return X[:, :max_len]\n\ndef prepare_bert(bin_file, is_cased):\n    bert_path = CASED_BERT_MODEL_PATH if is_cased else UNCASED_BERT_MODEL_PATH \n    config = BertConfig(os.path.join(bert_path, \"bert_config.json\"))\n    model = BertForSequenceClassification(config, num_labels=18)\n    model.load_state_dict(torch.load(bin_file, map_location=\"cpu\"))\n    for p in model.parameters():\n        p.requires_grad = False\n    model = model.eval()\n    model = model.cuda()\n    return model\n\ndef prepare_tokenizer(is_cased):\n    if is_cased:\n        return BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\n    else:\n        return BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\n    \n    \ndef apply_bert(model, loader):\n    preds = np.zeros((len(loader.dataset), 18), dtype=np.float32)\n    for i, X in enumerate(loader):\n        X = X.cuda()\n        p = torch.sigmoid(model(X, attention_mask=(X > 0)))\n        preds[i * loader.batch_size : (i + 1) * loader.batch_size] = p.cpu().numpy()\n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## UNCASED"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ntokenizer = prepare_tokenizer(is_cased=False)\nMAX_LEN = 400 - 2\n\ndef convert_line(text):\n    tokens_a = tokenizer.tokenize(text)[:MAX_LEN]\n    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"])\n    one_token += [0] * (MAX_LEN - len(tokens_a))\n    return one_token\n\nwith multiprocessing.Pool(processes=2) as pool:\n    sequences = pool.map(convert_line, test.comment_text)    \nsequences = np.array(sequences)\n\nlengths = np.argmax(sequences == 0, axis=1)\nlengths[lengths == 0] = sequences.shape[1]\n\nids = lengths.argsort(kind=\"stable\")\ninverse_ids = test.id.values[ids].argsort(kind=\"stable\")\n\nsequences = torch.from_numpy(sequences)\nlengths = torch.from_numpy(lengths)\n\ntest_dataset = data.TensorDataset(sequences, lengths)\ntest_loader = data.DataLoader(data.Subset(test_dataset, ids), batch_size=128, collate_fn=clip_to_max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nUNCASED_BERT_MODELS = [\n    \"BERT_exp_BERT_3_decay_5_epoch_1\",\n    \"final-pipe1-raw_1\",\n    \"final-pipe1-raw_2\",\n    \"final-pipe1-raw_3\",\n    \"final-pipe2-raw_1\",\n    \"final-pipe2-raw_2\",\n    \"final-pipe2-raw_3\",\n    \"final-pipe4-wiki_raw_1\",\n    \"final-pipe4-wiki_raw_2\",\n    \"final-pipe4-wiki_raw_3\",\n]\n\nfor path in UNCASED_BERT_MODELS:\n    model = prepare_bert(f\"../input/toxic-models-zoo/{path}.bin\", is_cased=False)\n    preds = apply_bert(model, test_loader)\n    STORE[path] = sophisticated_magic(preds)[inverse_ids]\n    \n    \nfor path in [\"final-pipe2-raw_4\", \"BERT_exp_BERT_3_epoch_1\"]:\n    model = prepare_bert(f\"../input/jigsaw-solution-ver-1/{path}.bin\", is_cased=False)\n    preds = apply_bert(model, test_loader)\n    STORE[path] = sophisticated_magic(preds)[inverse_ids]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CASED"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ntokenizer = prepare_tokenizer(is_cased=True)\n\ndef convert_line(text):\n    tokens_a = tokenizer.tokenize(text)[:MAX_LEN]\n    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"])\n    one_token += [0] * (MAX_LEN - len(tokens_a))\n    return one_token\n\nwith multiprocessing.Pool(processes=2) as pool:\n    sequences = pool.map(convert_line, test.comment_text)    \nsequences = np.array(sequences)\n\nlengths = np.argmax(sequences == 0, axis=1)\nlengths[lengths == 0] = sequences.shape[1]\n\nids = lengths.argsort(kind=\"stable\")\ninverse_ids = test.id.values[ids].argsort(kind=\"stable\")\n\nsequences = torch.from_numpy(sequences)\nlengths = torch.from_numpy(lengths)\n\ntest_dataset = data.TensorDataset(sequences, lengths)\ntest_loader = data.DataLoader(data.Subset(test_dataset, ids), batch_size=128, collate_fn=clip_to_max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nCASED_BERT_MODELS = [\n    \"BERT_exp_BERT_3_cased_decay_epoch_1\",\n    \"final-pipe2-cased_1\",\n    \"final-pipe2-cased_2\",\n    \"final-pipe2-cased_3\",\n#     \"final-pipe2-cased_4\",\n#     \"final-pipe2-cased_5\",\n#     \"final-pipe2-cased_6\",\n    \"final-pipe5-wiki_cased_1\",\n    \"final-pipe5-wiki_cased_2\",\n    \"final-pipe5-wiki_cased_3\",\n]\n\nfor path in CASED_BERT_MODELS:\n    model = prepare_bert(f\"../input/toxic-models-zoo/{path}.bin\", is_cased=True)\n    preds = apply_bert(model, test_loader)\n    STORE[path] = sophisticated_magic(preds)[inverse_ids]\n    \n    \nfor path in [\"final-pipe2-cased_10\"]:\n    model = prepare_bert(f\"../input/jigsaw-solution-ver-1/{path}.bin\", is_cased=True)\n    preds = apply_bert(model, test_loader)\n    STORE[path] = sophisticated_magic(preds)[inverse_ids]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2CNN"},{"metadata":{"trusted":false},"cell_type":"code","source":"import regex as re\nfrom io import open\n\n@lru_cache()\ndef bytes_to_unicode():\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass MonkeyPatchedGPT2Tokenizer(object):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        vocab_file = os.path.join(pretrained_model_name_or_path, \"vocab.json\")\n        merges_file = os.path.join(pretrained_model_name_or_path, \"merges.txt\")\n\n        max_len = 1024\n        kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        special_tokens = kwargs.pop('special_tokens', [])\n        tokenizer = cls(vocab_file, merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n        return tokenizer\n\n    def __init__(self, vocab_file, merges_file, errors='replace', special_tokens=None, max_len=None):\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.encoder = json.load(open(vocab_file))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        bpe_data = open(merges_file, encoding='utf-8').read().split('\\n')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized  of contractions\n        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n        self.special_tokens = {}\n        self.special_tokens_decoder = {}\n        self.set_special_tokens(special_tokens)\n\n    def __len__(self):\n        return len(self.encoder) + len(self.special_tokens)\n\n    def set_special_tokens(self, special_tokens):\n        if not special_tokens:\n            self.special_tokens = {}\n            self.special_tokens_decoder = {}\n            return\n        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def tokenize(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        ids = []\n        if isinstance(tokens, str):\n            if tokens in self.special_tokens:\n                return self.special_tokens[tokens]\n            else:\n                return self.encoder.get(tokens, 0)\n        for token in tokens:\n            if token in self.special_tokens:\n                ids.append(self.special_tokens[token])\n            else:\n                ids.append(self.encoder.get(token, 0))\n        if len(ids) > self.max_len:\n            print(\n                \"Token indices sequence length is longer than the specified maximum \"\n                \" sequence length for this OpenAI GPT model ({} > {}). Running this\"\n                \" sequence through the model will result in indexing errors\".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        \"\"\"Converts a sequence of ids in BPE tokens using the vocab.\"\"\"\n        tokens = []\n        for i in ids:\n            if i in self.special_tokens_decoder:\n                if not skip_special_tokens:\n                    tokens.append(self.special_tokens_decoder[i])\n            else:\n                tokens.append(self.decoder[i])\n        return tokens\n\n    def encode(self, text):\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n    def decode(self, tokens, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        text = ''.join(self.convert_ids_to_tokens(tokens, skip_special_tokens=skip_special_tokens))\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n        if clean_up_tokenization_spaces:\n            text = text.replace('<unk>', '')\n            text = text.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ','\n                    ).replace(\" ' \", \"'\").replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" do not\", \" don't\"\n                    ).replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 're\", \"'re\")\n        return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from pytorch_pretrained_bert import GPT2Config, GPT2Model\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel\n\nMAX_LEN = 250\nGPT2_TOKENIZER = MonkeyPatchedGPT2Tokenizer.from_pretrained(\"../input/transformer-tokenizers/gpt2/\")\n\ndef convert_line_gpt2(text):\n    tokens_a = GPT2_TOKENIZER.tokenize(text)[:MAX_LEN]\n    one_token = GPT2_TOKENIZER.convert_tokens_to_ids(tokens_a)\n    one_token += [0] * (MAX_LEN - len(tokens_a))\n    return one_token\n\n\ndef apply_gpt(model, loader):\n    preds = np.zeros((len(loader.dataset), 18), dtype=np.float32)\n    for i, X in enumerate(loader):\n        p = torch.sigmoid(model(X.cuda()))\n        preds[i * loader.batch_size : (i + 1) * loader.batch_size] = p.cpu().numpy()\n    return preds\n\n\nclass GPT2CNN(GPT2PreTrainedModel):\n    def __init__(self, config, num_labels):\n        super().__init__(config)\n        self.transformer = GPT2Model(config)  \n        self.cnn1 = nn.Conv1d(768, 256, kernel_size=3, padding=1)\n        self.cnn2 = nn.Conv1d(256, num_labels, kernel_size=3, padding=1)\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        x, _ = self.transformer(input_ids, position_ids, token_type_ids, past)\n        x = x.permute(0, 2, 1)\n        x = F.relu(self.cnn1(x))\n        x = self.cnn2(x)\n        output, _ = torch.max(x, 2)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nwith multiprocessing.Pool(processes=2) as pool:\n    sequences = np.array(pool.map(convert_line_gpt2, test.comment_text))\n\nlengths = np.argmax(sequences == 0, axis=1)\nlengths[lengths == 0] = sequences.shape[1]\n\nsequences = torch.from_numpy(sequences)\nlengths = torch.from_numpy(lengths)\n\ntest_dataset = data.TensorDataset(sequences, lengths)\ntest_loader = data.DataLoader(test_dataset, batch_size=64, collate_fn=clip_to_max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nGPT2CNN_MODELS = [\n    \"BERT_exp_GPT2_CNN_epoch_1\",\n    \"BERT_exp_GPT2_CNN_seed_epoch_1\",\n#     \"final-pipe6-gpt_wiki_1\",\n]\n\nfor path in GPT2CNN_MODELS:\n    model = GPT2CNN(GPT2Config(), num_labels=18)\n    model.load_state_dict(\n        torch.load(f\"../input/toxic-models-zoo/{path}.bin\", map_location=\"cpu\")\n    )\n    for p in model.parameters():\n        p.requires_grad = False\n    model = model.eval()\n    model = model.cuda()\n    preds = apply_gpt(model, test_loader)\n    STORE[path] = sophisticated_magic(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MERGING"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(STORE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.corr('kendall')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_preds = (\n    ### LSTM\n    4.5 * df[\"lstm\"] +\n    \n    ### BERT\n    # best 90%-model\n    df[\"BERT_exp_BERT_3_decay_5_epoch_1\"] +\n    df[\"BERT_exp_BERT_3_cased_decay_epoch_1\"] +\n    \n    # GPT-CNN\n    2 * df[\"BERT_exp_GPT2_CNN_epoch_1\"] + \n    2 * df[\"BERT_exp_GPT2_CNN_seed_epoch_1\"] +\n#     2 * df[\"final-pipe6-gpt_wiki_1\"] +\n    \n    # fint-tune\n    df[\"final-pipe1-raw_1\"] +\n    df[\"final-pipe1-raw_2\"] +\n    df[\"final-pipe1-raw_3\"] +\n    # base-uncased\n    df[\"final-pipe2-raw_1\"] +\n    df[\"final-pipe2-raw_2\"] +\n    df[\"final-pipe2-raw_3\"] +\n    # cased (note: 4,5,6 don't really improve results)\n    df[\"final-pipe2-cased_1\"] +\n    df[\"final-pipe2-cased_2\"] +\n    df[\"final-pipe2-cased_3\"] + \n#     df[\"final-pipe2-cased_4\"] +\n#     df[\"final-pipe2-cased_5\"] +\n#     df[\"final-pipe2-cased_6\"] +\n    # Wiki-ft\n    df[\"final-pipe4-wiki_raw_1\"] +\n    df[\"final-pipe4-wiki_raw_2\"] +\n    df[\"final-pipe4-wiki_raw_3\"] +\n    # Wiki-cased\n    df[\"final-pipe5-wiki_cased_1\"] +\n    df[\"final-pipe5-wiki_cased_2\"] +\n    df[\"final-pipe5-wiki_cased_3\"] + \n    \n    df[\"final-pipe2-raw_4\"] +\n    df[\"final-pipe2-cased_10\"] +\n    df[\"BERT_exp_BERT_3_epoch_1\"]\n)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test['id'],\n    'prediction': final_preds,\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(torch.cuda.max_memory_allocated(0) / 1024 / 1024)\nprint(torch.cuda.max_memory_cached(0) / 1024 / 1024)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2ef73e2d8c5049ce810360bb86a40940":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5dcc22d0866741f496171bfb934934f3","IPY_MODEL_958033834a5641dfbc14e7774bcaef55"],"layout":"IPY_MODEL_dc66dc54b7bc450e957dcea73aa6e9a5"}},"413cedc04d434b7996dd8fa53d09acdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}},"51d4baedd8db47a1821b7204d341d644":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dcc22d0866741f496171bfb934934f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75bcd3df05964fafb4e0c644cd591d32","max":1521,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f34fbad09d024e4dacbff6e49cb08465","value":1521}},"75bcd3df05964fafb4e0c644cd591d32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"958033834a5641dfbc14e7774bcaef55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51d4baedd8db47a1821b7204d341d644","placeholder":"​","style":"IPY_MODEL_e5014a071523488d810bee510426b075","value":"100% 1521/1521 [13:42&lt;00:00,  2.19it/s]"}},"af27b39960e14e3ba4a10fbb7ebb5537":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b793ca5992ee4e8bbe036c874914f68a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1577e0de2ce4f23a7d0c48306364eaf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d67a99a6b61a44ec84f666a0518f0be3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc66dc54b7bc450e957dcea73aa6e9a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3387ba9000144b89e2854d3a065a590":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f48339f077ad4181af835f3e1fc4108e","IPY_MODEL_f5a3a1dd2637419bb748f01410eab086"],"layout":"IPY_MODEL_af27b39960e14e3ba4a10fbb7ebb5537"}},"e5014a071523488d810bee510426b075":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}},"f34fbad09d024e4dacbff6e49cb08465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f48339f077ad4181af835f3e1fc4108e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d67a99a6b61a44ec84f666a0518f0be3","max":1521,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b793ca5992ee4e8bbe036c874914f68a","value":1521}},"f5a3a1dd2637419bb748f01410eab086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1577e0de2ce4f23a7d0c48306364eaf","placeholder":"​","style":"IPY_MODEL_413cedc04d434b7996dd8fa53d09acdd","value":"100% 1521/1521 [13:42&lt;00:00,  2.19it/s]"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}