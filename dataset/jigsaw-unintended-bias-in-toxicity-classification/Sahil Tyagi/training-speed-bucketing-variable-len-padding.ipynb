{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Bucketing** - A technique that can significantly improve model's training time.\n<br>Idea is to pad sequences on the batch level instead of padding sequences on the whole data."},{"metadata":{},"cell_type":"markdown","source":"Inspiration -> [Quora insincere questions classification discussion](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-516532)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np, tensorflow as tf\nfrom keras.preprocessing import text, sequence","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nBATCH_SIZE = 512\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = preprocess(train[TEXT_COL])\ny_train = np.where(train['target'] >= 0.5, 1, 0)\nx_test = preprocess(test[TEXT_COL])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train_seq = tokenizer.texts_to_sequences(x_train)\nx_test_seq = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pd.DataFrame.from_dict({\n    'text': x_train,\n    'as_numbers': x_train_seq\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train['length'] = x_train.as_numbers.str.len()\nx_train['target'] = y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bucketing**\n<br>Excerpt from the reference sample itself\n<br>\n> The key point to keep in mind is that we should not “bias” the order in which different sequence lengths are sampled any more than necessary to achieve bucketing. E.g., sorting our data by sequence length might seem like a good solution, but then each epoch would be trained on short sequences before longer sequences, which could harm results. Here is one solution, which uses a predetermined batch_size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BucketedDataIterator():\n    def __init__(self, df, num_buckets = 5):\n        df = df.sort_values('length').reset_index(drop=True)\n        self.size = len(df) / num_buckets\n        self.dfs = []\n        for bucket in range(num_buckets):\n            self.dfs.append(df.loc[bucket*self.size: (bucket+1)*self.size - 1])\n        self.num_buckets = num_buckets\n\n        # cursor[i] will be the cursor for the ith bucket\n        self.cursor = np.array([0] * num_buckets)\n        self.shuffle()\n\n        self.epochs = 0\n\n    def shuffle(self):\n        #sorts dataframe by sequence length, but keeps it random within the same length\n        for i in range(self.num_buckets):\n            self.dfs[i] = self.dfs[i].sample(frac=1).reset_index(drop=True)\n            self.cursor[i] = 0\n\n    def next_batch(self, n):\n        if np.any(self.cursor+n+1 > self.size):\n            self.epochs += 1\n            self.shuffle()\n\n        i = np.random.randint(0,self.num_buckets)\n\n        res = self.dfs[i].loc[self.cursor[i]:self.cursor[i]+n-1]\n        self.cursor[i] += n\n\n        # Pad sequences with 0s so they are all the same length\n        maxlen = max(res['length'])\n        x = np.zeros([n, maxlen], dtype=np.int32)\n        for i, x_i in enumerate(x):\n            x_i[:res['length'].values[i]] = res['as_numbers'].values[i]\n\n        return x, res['target'], res['length']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = BucketedDataIterator(x_train, 5)\npadding = 0\nfor i in range(100):\n    lengths = tr.next_batch(BATCH_SIZE)[2].values\n    max_len = max(lengths)\n    padding += np.sum(max_len - lengths)\nprint(\"Average padding with bucketing:\", padding/(BATCH_SIZE*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN) is used then the padding results will be\nprint(\"Average padding without bucketing :\", np.sum(MAX_LEN - x_train['length'])/len(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sample usage to extract batch for training\nbatch = tr.next_batch(BATCH_SIZE)\nx = batch[0]\ny = batch[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference : [Variable length sequences](https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}