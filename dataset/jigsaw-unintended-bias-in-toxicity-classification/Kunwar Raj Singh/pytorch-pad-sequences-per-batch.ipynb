{"cells":[{"metadata":{},"cell_type":"markdown","source":"The [winning solution of Quora Insincere Questions Classification](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-516532) used a clever way to pad sequences per batch on the fly. <br> I would like to share the same for PyTorch.<br> This should help in improving run times without affecting model performance ( in theory )."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils import data\nimport numpy as np\nfrom keras.preprocessing import sequence","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(data.Dataset):\n    '''\n    Simple Dataset\n    '''\n    def __init__(self,X,y=None):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return [self.X[idx],self.y[idx]]\n        return self.X[idx]","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyCollator(object):\n    '''\n    Yields a batch from a list of Items\n    Args:\n    test : Set True when using with test data loader. Defaults to False\n    percentile : Trim sequences by this percentile\n    '''\n    def __init__(self,test=False,percentile=100):\n        self.test = test\n        self.percentile = percentile\n    def __call__(self, batch):\n        if not self.test:\n            data = [item[0] for item in batch]\n            target = [item[1] for item in batch]\n        else:\n            data = batch\n        lens = [len(x) for x in data]\n        max_len = np.percentile(lens,self.percentile)\n        data = sequence.pad_sequences(data,maxlen=int(max_len))\n        data = torch.tensor(data,dtype=torch.long)\n        if not self.test:\n            target = torch.tensor(target,dtype=torch.float32)\n            return [data,target]\n        return [data]","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a sample dataset to test our new collate function"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 1024\nsizes = np.random.normal(loc=200,scale=50,size=(sample_size,)).astype(np.int32)\nX = [np.ones((sizes[i])) for i in range(sample_size)]\nY = np.random.rand(sample_size).round()","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we choose to pad this data by maximum length in the whole data, this is the length all sequences will be padded to"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"sizes.max()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"352"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"However, this is not ideal. <br>\nLet's try padding the sequence to maximum length per batch instead of the whole dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\ndataset = TextDataset(X,Y)\ntest_dataset = TextDataset(X)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sequence sizes are smaller overall! <br>**\n*Note that the size reduction depends on the distribution of sequence sizes in the actual dataset. <br>*\n*Here, I've used a normally distributed dummy dataset. *"},{"metadata":{"trusted":true},"cell_type":"code","source":"collate = MyCollator(percentile=100)\nloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True ,collate_fn=collate)\nfor X,Y in loader:\n    print(X.shape,Y.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"torch.Size([128, 331]) torch.Size([128])\ntorch.Size([128, 304]) torch.Size([128])\ntorch.Size([128, 287]) torch.Size([128])\ntorch.Size([128, 341]) torch.Size([128])\ntorch.Size([128, 333]) torch.Size([128])\ntorch.Size([128, 352]) torch.Size([128])\ntorch.Size([128, 323]) torch.Size([128])\ntorch.Size([128, 314]) torch.Size([128])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Example : Running on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_collate = MyCollator(test=True,percentile=100)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False , collate_fn=test_collate)\nfor X in test_loader:\n    print(X[0].shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"torch.Size([128, 314])\ntorch.Size([128, 341])\ntorch.Size([128, 331])\ntorch.Size([128, 352])\ntorch.Size([128, 313])\ntorch.Size([128, 321])\ntorch.Size([128, 314])\ntorch.Size([128, 333])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"To Furthur reduce running times, you can choose to pad by **Nth** percentile of lenghts, keeping **N** close to 100. This may or may not affect model performance, your mileage may vary. <br>\nFor example, **N = 95** gave a good balance between speed and performance for quora challenge's winning team. <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"collate = MyCollator(percentile=95)\nloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True ,collate_fn=collate)\nfor X,Y in loader:\n    print(X.shape,Y.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"torch.Size([128, 289]) torch.Size([128])\ntorch.Size([128, 267]) torch.Size([128])\ntorch.Size([128, 274]) torch.Size([128])\ntorch.Size([128, 277]) torch.Size([128])\ntorch.Size([128, 280]) torch.Size([128])\ntorch.Size([128, 285]) torch.Size([128])\ntorch.Size([128, 287]) torch.Size([128])\ntorch.Size([128, 266]) torch.Size([128])\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}