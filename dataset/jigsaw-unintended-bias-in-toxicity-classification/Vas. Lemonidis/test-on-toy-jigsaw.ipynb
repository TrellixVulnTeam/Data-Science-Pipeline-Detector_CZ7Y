{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThis is just a simple tuned version of @Kunwar Raj Singh's great kernel.\nAll Credits go to https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai\nYou can go on improving LB by the suggestions mentioned here-> https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version\n'''\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\n\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n\nCRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n    \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_model(learn,test,output_dim,lr=0.0088,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.4**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets, max_features):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        \ndef preprocess(data):\n    '''\n    Credit goes to \n    https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\ndef train_and_test_on_indices(indices: list=None, x_train: pd.DataFrame=None):\n    train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n    if indices is not None:\n        train = train.iloc[indices]\n    if x_train is None:\n        x_train = train['comment_text']\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    x_train = preprocess(x_train)\n    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\n    x_test = preprocess(test['comment_text'])\n    \n    identity_columns = [\n        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n    # Overall\n    weights = np.ones((len(x_train),)) / 4\n    # Subgroup\n    weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n    # Background Positive, Subgroup Negative\n    weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    # Background Negative, Subgroup Positive\n    weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    loss_weight = 1.0 / weights.mean()\n    y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n    max_features = None\n    tokenizer = text.Tokenizer()\n    tokenizer.fit_on_texts(list(x_train) + list(x_test))\n    \n    x_train = tokenizer.texts_to_sequences(x_train)\n    x_test = tokenizer.texts_to_sequences(x_test)\n    x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n    x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n    \n    max_features = max_features or len(tokenizer.word_index) + 1\n    crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n    print('n unknown words (crawl): ', len(unknown_words_crawl))\n    glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n    print('n unknown words (glove): ', len(unknown_words_glove))\n    \n    embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n    embedding_matrix.shape\n    \n    del crawl_matrix\n    del glove_matrix\n    gc.collect()\n    \n    x_train_torch = torch.tensor(x_train, dtype=torch.long)\n    y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n    \n    x_test_torch = torch.tensor(x_test, dtype=torch.long)\n    \n    batch_size = 512\n    \n    train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n    valid_dataset = data.TensorDataset(x_train_torch[:batch_size], y_train_torch[:batch_size])\n    test_dataset = data.TensorDataset(x_test_torch)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    \n    databunch = DataBunch(train_dl=train_loader,valid_dl=valid_loader)\n    \n    def custom_loss(data, targets):\n        ''' Define custom loss function for weighted BCE on 'target' column '''\n        bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n        bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n        return (bce_loss_1 * loss_weight) + bce_loss_2\n        \n    all_test_preds = []\n    \n    for model_idx in range(NUM_MODELS):\n        print('Model ', model_idx)\n        seed_everything(1234 + model_idx)\n        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1], max_features)\n        learn = Learner(databunch,model,loss_func=custom_loss)\n        test_preds = train_model(learn,test_dataset,output_dim=7)    \n        all_test_preds.append(test_preds)\n        \n        \n    submission = pd.DataFrame.from_dict({\n        'id': test['id'],\n        'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n    })\n    return submission\n\nif __name__ == '__main__':\n    print('hi')\n    with open('submission.csv', 'w') as out:\n        out.write('')\n#     submission = train_and_test_on_indices()\n#     submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pickle\nwith open('../input/augmented-toy-jigsaw/train_augmented', 'rb') as inp:\n    inp_data = pickle.load(inp)\nindices = inp_data[0]\nx_train = inp_data[1]\ny = inp_data[2]\nx_aug_train = inp_data[3]\nimport pandas as pd\nx_train_series = pd.Series(x_train)\nsubmission = train_and_test_on_indices(indices, x_train_series)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aug_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aug_train[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}