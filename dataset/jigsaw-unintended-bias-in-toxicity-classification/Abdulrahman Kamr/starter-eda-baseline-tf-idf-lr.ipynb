{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\nimport html\nimport unicodedata\n\n# sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n\n# Keras\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\n\n# Standard plotly imports\nimport plotly.offline as py \nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\nstop_words = set(stopwords.words(\"english\"))\n\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'\n","metadata":{"id":"O3E5JNmOSR0r","execution":{"iopub.status.busy":"2021-11-19T12:01:14.526416Z","iopub.execute_input":"2021-11-19T12:01:14.526723Z","iopub.status.idle":"2021-11-19T12:01:24.245448Z","shell.execute_reply.started":"2021-11-19T12:01:14.526652Z","shell.execute_reply":"2021-11-19T12:01:24.244571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:01:24.24698Z","iopub.execute_input":"2021-11-19T12:01:24.247304Z","iopub.status.idle":"2021-11-19T12:01:24.261334Z","shell.execute_reply.started":"2021-11-19T12:01:24.247262Z","shell.execute_reply":"2021-11-19T12:01:24.260491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_data = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\ntrain_data.head()\n","metadata":{"id":"Ue5ZQBltWJoi","execution":{"iopub.status.busy":"2021-11-19T12:01:24.263053Z","iopub.execute_input":"2021-11-19T12:01:24.263578Z","iopub.status.idle":"2021-11-19T12:01:42.497138Z","shell.execute_reply.started":"2021-11-19T12:01:24.263541Z","shell.execute_reply":"2021-11-19T12:01:42.496298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"id":"LZKWTNGYx5Lp","execution":{"iopub.status.busy":"2021-11-19T12:01:42.498714Z","iopub.execute_input":"2021-11-19T12:01:42.499217Z","iopub.status.idle":"2021-11-19T12:01:42.515313Z","shell.execute_reply.started":"2021-11-19T12:01:42.499178Z","shell.execute_reply":"2021-11-19T12:01:42.514173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"id":"J6MaQX7cx5Oa","execution":{"iopub.status.busy":"2021-11-19T12:01:42.5167Z","iopub.execute_input":"2021-11-19T12:01:42.517174Z","iopub.status.idle":"2021-11-19T12:01:42.528486Z","shell.execute_reply.started":"2021-11-19T12:01:42.517134Z","shell.execute_reply":"2021-11-19T12:01:42.527615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns","metadata":{"id":"9DIQy1wG5ywo","execution":{"iopub.status.busy":"2021-11-19T12:01:42.530181Z","iopub.execute_input":"2021-11-19T12:01:42.530612Z","iopub.status.idle":"2021-11-19T12:01:42.540514Z","shell.execute_reply.started":"2021-11-19T12:01:42.53057Z","shell.execute_reply":"2021-11-19T12:01:42.539634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['toxic'] = np.where(train_data['target'] >= .5, 'Toxic', 'Non-Toxic')","metadata":{"id":"Ma8gNwtZx5Qq","execution":{"iopub.status.busy":"2021-11-19T12:01:42.543862Z","iopub.execute_input":"2021-11-19T12:01:42.544147Z","iopub.status.idle":"2021-11-19T12:01:42.827456Z","shell.execute_reply.started":"2021-11-19T12:01:42.544103Z","shell.execute_reply":"2021-11-19T12:01:42.826386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA\n","metadata":{"id":"wMuJL8FEwIS4"}},{"cell_type":"code","source":"train_data['comment_text'].isnull().sum() ,train_data['target'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:01:42.831066Z","iopub.execute_input":"2021-11-19T12:01:42.831334Z","iopub.status.idle":"2021-11-19T12:01:43.206685Z","shell.execute_reply.started":"2021-11-19T12:01:42.831306Z","shell.execute_reply":"2021-11-19T12:01:43.205864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution for words in comment text","metadata":{"id":"py-NlFguytXG"}},{"cell_type":"code","source":"train_data['num_words'] = [len(sent.split()) for sent in train_data['comment_text']]\ntrain_data[['comment_text', 'num_words']].head()","metadata":{"id":"eMulCCHovhh7","execution":{"iopub.status.busy":"2021-11-19T12:01:43.211372Z","iopub.execute_input":"2021-11-19T12:01:43.213787Z","iopub.status.idle":"2021-11-19T12:01:50.002225Z","shell.execute_reply.started":"2021-11-19T12:01:43.213739Z","shell.execute_reply":"2021-11-19T12:01:50.000804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# maximum and minimum number of words per sentencs\nmax(train_data['num_words']), min(train_data['num_words'])","metadata":{"id":"ZVXlFIkFzjqa","execution":{"iopub.status.busy":"2021-11-19T12:01:50.003552Z","iopub.execute_input":"2021-11-19T12:01:50.003904Z","iopub.status.idle":"2021-11-19T12:01:50.441579Z","shell.execute_reply.started":"2021-11-19T12:01:50.003867Z","shell.execute_reply":"2021-11-19T12:01:50.44077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of words in toxic and non toxic sentence","metadata":{"id":"S1NqG1mKz94n"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train_data[train_data['toxic'] == 'Toxic']['num_words'], kde= True, ax=ax[0], color= 'r')\nsns.histplot(train_data[train_data['toxic'] == 'Non-Toxic']['num_words'], kde= True, ax=ax[1], color= 'b')\nax[0].set_title(\"Toxic Distribution\")\nax[1].set_title(\"Non Toxic Distribution\")\nplt.show();","metadata":{"id":"7p-EBcekvhng","execution":{"iopub.status.busy":"2021-11-19T12:01:50.442787Z","iopub.execute_input":"2021-11-19T12:01:50.443126Z","iopub.status.idle":"2021-11-19T12:02:01.213888Z","shell.execute_reply.started":"2021-11-19T12:01:50.443091Z","shell.execute_reply":"2021-11-19T12:02:01.213042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribuition of Toxicity of comments","metadata":{"id":"EbQJoAyp4jS1"}},{"cell_type":"code","source":"train_data['toxic'].value_counts()","metadata":{"id":"5T96rUd45KlH","execution":{"iopub.status.busy":"2021-11-19T12:02:01.214936Z","iopub.execute_input":"2021-11-19T12:02:01.215244Z","iopub.status.idle":"2021-11-19T12:02:01.638097Z","shell.execute_reply.started":"2021-11-19T12:02:01.21521Z","shell.execute_reply":"2021-11-19T12:02:01.637228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train_data['toxic'])\nplt.title('Distribuition of Toxicity of comments');","metadata":{"id":"NgdagCRwvhqq","execution":{"iopub.status.busy":"2021-11-19T12:02:01.639516Z","iopub.execute_input":"2021-11-19T12:02:01.640114Z","iopub.status.idle":"2021-11-19T12:02:03.534173Z","shell.execute_reply.started":"2021-11-19T12:02:01.640074Z","shell.execute_reply":"2021-11-19T12:02:03.533367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:03.535371Z","iopub.execute_input":"2021-11-19T12:02:03.535873Z","iopub.status.idle":"2021-11-19T12:02:03.540988Z","shell.execute_reply.started":"2021-11-19T12:02:03.535832Z","shell.execute_reply":"2021-11-19T12:02:03.540143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the dataframe with identities tagged\ntrain_labeled_df = train_data.loc[:, ['target'] + identities ].dropna()\n\n# lets define toxicity as a comment with a score being equal or .5\ntoxic_df = train_labeled_df[train_labeled_df['target'] >= .5][identities]\nnon_toxic_df = train_labeled_df[train_labeled_df['target'] < .5][identities]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:03.542436Z","iopub.execute_input":"2021-11-19T12:02:03.542903Z","iopub.status.idle":"2021-11-19T12:02:03.890026Z","shell.execute_reply.started":"2021-11-19T12:02:03.542817Z","shell.execute_reply":"2021-11-19T12:02:03.889189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\ntoxic_count = toxic_df.apply(lambda x : x > 0 ).sum()\nnon_toxic_count = non_toxic_df.apply(lambda x : x > 0 ).sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:03.891336Z","iopub.execute_input":"2021-11-19T12:02:03.891683Z","iopub.status.idle":"2021-11-19T12:02:03.948432Z","shell.execute_reply.started":"2021-11-19T12:02:03.891645Z","shell.execute_reply":"2021-11-19T12:02:03.947629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows= 2,ncols= 1,figsize=(15,15))\n\nsns.barplot(x= toxic_count, y= identities, ax=ax[0])\nsns.barplot(x= non_toxic_count, y= identities, ax=ax[1])\n\nax[0].set_title(\"Toxic Distribution\")\nax[1].set_title(\"Non Toxic Distribution\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:03.949699Z","iopub.execute_input":"2021-11-19T12:02:03.950052Z","iopub.status.idle":"2021-11-19T12:02:04.697495Z","shell.execute_reply.started":"2021-11-19T12:02:03.950018Z","shell.execute_reply":"2021-11-19T12:02:04.696684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can concat the two series together to get a toxic count vs non toxic count for each identity\ntoxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\n\ntoxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\ntoxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20});","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:04.698653Z","iopub.execute_input":"2021-11-19T12:02:04.699126Z","iopub.status.idle":"2021-11-19T12:02:05.691222Z","shell.execute_reply.started":"2021-11-19T12:02:04.699088Z","shell.execute_reply":"2021-11-19T12:02:05.689839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Clouds","metadata":{}},{"cell_type":"code","source":"word_cloud_non_toxic = train_data[train_data['toxic'] == \"Non-Toxic\"]['comment_text']\nword_cloud_toxic = train_data[train_data['toxic'] == \"Toxic\"]['comment_text']\n\nwordcloud_non = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(word_cloud_non_toxic))\n\nwordcloud_toxic = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(word_cloud_toxic))\n\nfig = plt.figure(figsize=[20,5])\nplt.suptitle('Non Normalized Text', size = 25)\n\nfig.add_subplot(1, 2, 1).set_title(\"Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_toxic, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nfig.add_subplot(1, 2, 2).set_title(\"Non Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_non, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:05.692597Z","iopub.execute_input":"2021-11-19T12:02:05.69316Z","iopub.status.idle":"2021-11-19T12:02:07.300642Z","shell.execute_reply.started":"2021-11-19T12:02:05.693123Z","shell.execute_reply":"2021-11-19T12:02:07.299897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make lists for normalized text\nclean_toxic_text = [normalize_text(sent) for sent in train_data[train_data['toxic'] == \"Toxic\"]['comment_text'][:10000]]\nclean_non_toxic_text = [normalize_text(sent) for sent in train_data[train_data['toxic'] == \"Non-Toxic\"]['comment_text'][:10000]]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:07.301732Z","iopub.execute_input":"2021-11-19T12:02:07.302179Z","iopub.status.idle":"2021-11-19T12:02:25.944585Z","shell.execute_reply.started":"2021-11-19T12:02:07.302142Z","shell.execute_reply":"2021-11-19T12:02:25.943772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_non = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(clean_non_toxic_text))\n\nwordcloud_toxic = WordCloud(\n                          background_color='black',\n                          stopwords=stop_words,\n                          max_words=2000,\n                          max_font_size=100, \n                          random_state=42\n                         ).generate(str(clean_toxic_text))\n\nfig = plt.figure(figsize=[20,5])\nplt.suptitle('Normalized Text', size = 25)\n\nfig.add_subplot(1, 2, 1).set_title(\"Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_toxic, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nfig.add_subplot(1, 2, 2).set_title(\"Non Toxic Wordcloud\", fontsize=10)\nplt.imshow(wordcloud_non, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:02:25.945866Z","iopub.execute_input":"2021-11-19T12:02:25.946183Z","iopub.status.idle":"2021-11-19T12:02:32.834205Z","shell.execute_reply.started":"2021-11-19T12:02:25.94615Z","shell.execute_reply":"2021-11-19T12:02:32.831181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"#### Logistic Regression + TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"%%time\n\nVectorize = TfidfVectorizer()\nX = Vectorize.fit_transform(train_data[\"comment_text\"])\nX_test_date = Vectorize.transform(test_data[\"comment_text\"])\n\ny = np.array(train_data['target'].apply(lambda x: x > .5).astype('int'))","metadata":{"id":"_kyNrpeJDWcN","execution":{"iopub.status.busy":"2021-11-19T12:02:32.835544Z","iopub.execute_input":"2021-11-19T12:02:32.836051Z","iopub.status.idle":"2021-11-19T12:04:23.004108Z","shell.execute_reply.started":"2021-11-19T12:02:32.836017Z","shell.execute_reply":"2021-11-19T12:04:23.003219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape, X_test_date.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:04:23.007661Z","iopub.execute_input":"2021-11-19T12:04:23.00794Z","iopub.status.idle":"2021-11-19T12:04:23.0149Z","shell.execute_reply.started":"2021-11-19T12:04:23.007911Z","shell.execute_reply":"2021-11-19T12:04:23.012974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:04:23.016508Z","iopub.execute_input":"2021-11-19T12:04:23.017187Z","iopub.status.idle":"2021-11-19T12:04:23.530434Z","shell.execute_reply.started":"2021-11-19T12:04:23.017034Z","shell.execute_reply":"2021-11-19T12:04:23.529506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on tf-idf\nlr = LogisticRegression(C=1.0)\nlr.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:04:23.53374Z","iopub.execute_input":"2021-11-19T12:04:23.534029Z","iopub.status.idle":"2021-11-19T12:05:16.696715Z","shell.execute_reply.started":"2021-11-19T12:04:23.534002Z","shell.execute_reply":"2021-11-19T12:05:16.695756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corss_scores = cross_val_score(lr, X, y, cv=5, scoring= 'roc_auc')\ncorss_scores.mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:05:16.698108Z","iopub.execute_input":"2021-11-19T12:05:16.69843Z","iopub.status.idle":"2021-11-19T12:10:28.915519Z","shell.execute_reply.started":"2021-11-19T12:05:16.698395Z","shell.execute_reply":"2021-11-19T12:10:28.914743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lr.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:10:28.916814Z","iopub.execute_input":"2021-11-19T12:10:28.917177Z","iopub.status.idle":"2021-11-19T12:10:29.013957Z","shell.execute_reply.started":"2021-11-19T12:10:28.917138Z","shell.execute_reply":"2021-11-19T12:10:29.013061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, cmap='Reds')\nplt.xlabel('predicted value')\nplt.ylabel('true value');","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:10:29.015215Z","iopub.execute_input":"2021-11-19T12:10:29.015716Z","iopub.status.idle":"2021-11-19T12:10:30.161578Z","shell.execute_reply.started":"2021-11-19T12:10:29.015676Z","shell.execute_reply":"2021-11-19T12:10:30.160702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification report**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T12:10:30.162756Z","iopub.execute_input":"2021-11-19T12:10:30.163105Z","iopub.status.idle":"2021-11-19T12:10:31.563089Z","shell.execute_reply.started":"2021-11-19T12:10:30.163068Z","shell.execute_reply":"2021-11-19T12:10:31.562156Z"},"trusted":true},"execution_count":null,"outputs":[]}]}