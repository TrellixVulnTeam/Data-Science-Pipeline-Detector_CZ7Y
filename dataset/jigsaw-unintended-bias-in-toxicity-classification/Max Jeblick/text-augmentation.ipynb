{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Text augmentation with word2vec\n\nThe following script can be used for text augmentation.\nBasically, for each word in the corpus, a candidate list of synonyms is created by considering nearby words (as defined by the word2vec distance). This list is then filtered, only keeping those synonyms for which the POS tag is the same as the original word. \n\nThe idea described above is adapted from the nicely written article https://towardsdatascience.com/data-augmentation-for-natural-language-processing-6ae928313a3f\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport operator\nimport pandas as pd \nimport os\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', -1)\nimport numpy as np\nfrom nltk import pos_tag\nimport gensim\nfrom gensim.models import KeyedVectors\nimport re\nfrom nltk.corpus import wordnet as wn\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = KeyedVectors.load_word2vec_format(EMB_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspired from https://towardsdatascience.com/data-augmentation-for-natural-language-processing-6ae928313a3f\ndef get_similar_words(word_list, word_index, threshold=0.8):\n    word_tags = pos_tag(word_list)\n    similar_words = {}\n    for idx, word in tqdm(enumerate(word_list)):\n        if word in word2vec.wv.vocab:\n            #get words with highest cosine similarity\n            replacements = word2vec.wv.most_similar(positive=word, topn=5)\n            #keep only words that pass the threshold\n            replacements = [replacements[i][0] for i in range(5) if replacements[i][1] > threshold]\n            #check for POS tag equality, dismiss if unequal\n            replacements = [elem for elem in replacements if pos_tag([elem.lower()])[0][1] == word_tags[idx][1]]\n            # Dismiss upper-case similar replacements and OOV words.\n            replacements = [elem for elem in replacements if elem.lower() != word.lower() and elem in word_index and elem in word2vec.wv]\n            if len(replacements) > 0:\n                similar_words[word] = replacements\n            if idx % 500 == 0:\n                print(word, replacements)\n    # One could also reverse the dictionary or postprocess it further\n    return similar_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=None, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(word_index.items(), key=operator.itemgetter(1))[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = list(word_index.keys())[:20]\nprint(sample)\nfor i in range(10):\n    print(get_similar_words(sample, word_index, threshold=i * 0.1))\n    print(f'threshold at: {i*0.1}')\n    print('*'*50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar_words_dict = get_similar_words(list(word_index.keys())[:50_000], word_index, threshold=0.3)\nlen(similar_words_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" for word,synonyms in list(similar_words_dict.items())[:20]:\n    print(word,synonyms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\n\ndef get_synonym_list(word, dictionary):\n    if word in dictionary.keys():\n        return dictionary[word]\n    return [word]\n\ndef replace_text(text):\n    return ' '.join([np.random.choice(get_synonym_list(word, similar_words_dict)) for word in text.split(' ')])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_text('anyone likes 1 apple and 2 oranges ?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text_augmented'] = train['comment_text'].apply(lambda x: replace_text(x))\ntest['comment_text_augmented'] = test['comment_text'].apply(lambda x: replace_text(x))\ntrain[['comment_text', 'comment_text_augmented']].head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('augmented_train.csv', index=False)\ntest.to_csv('augmented_test.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}