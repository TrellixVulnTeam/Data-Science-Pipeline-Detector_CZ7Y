{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.layers import *\nfrom keras.models import Model\n\n\n# For reproducibility.\nseed = 7\nnp.random.seed(seed)\ntf.set_random_seed(seed)\nsession_conf = tf.ConfigProto(\n    intra_op_parallelism_threads=1,\n    inter_op_parallelism_threads=1\n)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\nprint(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"# Data Loading\n\nwe load the dataset and apply some transformations to use it in a deep learning model."},{"metadata":{"_uuid":"ed0303324540aeb4e1068268abf27fdbef0ad5f9","trusted":true},"cell_type":"code","source":"print(\"Loading data...\")\ndf_train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\nprint(\"Train shape:\", df_train.shape)\ndf_test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\",encoding=\"latin-1\")\nprint(\"Test shape:\", df_test.shape)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.rename(columns=({\"comment_text\":\"Reviews\"}))\ndf_train = df_train.rename(columns=({\"target\":\"Label\"}))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.rename(columns=({\"comment_text\":\"Reviews\"}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = df_train[:10000]\n#df_test = df_test[:10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06fb1f160cb506879ee2cbb2fde5b8551a533e87"},"cell_type":"markdown","source":"In addition to the datasets, we load a pretrained word embeddings."},{"metadata":{"_uuid":"f9f6690b12a951d72e741e221c627f835d3d9a39","trusted":true},"cell_type":"code","source":"EMBEDDING_FILE =  '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n\ndef load_embeddings(filename):\n    embeddings = {}\n    with open(filename) as f:\n        for line in f:\n            values = line.rstrip().split(' ')\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\nembeddings = load_embeddings(EMBEDDING_FILE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5defa17c3f32e993e862b7ddaa4f190ab4dc9cac"},"cell_type":"markdown","source":"# Preprocessings\n\nAfter loading the datasets, we will preprocess the datasets. In this time, we will apply following techniques:\n\n* Nigation handling\n* Replacing numbers\n* Tokenization\n* Zero padding\n\nNegation handling is the process of converting negation abbreviation to a canonical format. For example, \"aren't\" is converted to \"are not\". It is helpful for sentiment analysis.\n\nReplacing numbers is the process of converting numbers to a specific character. For example, \"1924\" and \"123\" are converted to \"0\". It is helpful for reducing the vocabulary size.\n\nTokenization is the process of taking a text or set of texts and breaking it up into its individual words. In this step, we will tokenize text with the help of splitting text by space or punctuation marks.\n\nZero padding is the process of pad \"0\" to the dataset for the purpose of ensuring that all sentences has the same length."},{"metadata":{"_uuid":"5c16fa877155eb7a95a6640a1ad6f4e3f8a126c8"},"cell_type":"markdown","source":"## Negation handling"},{"metadata":{"_uuid":"c663ce4f9bdc2f497cf43ae21b91acc9cf6e854c","trusted":true},"cell_type":"code","source":"df_train.Reviews = df_train.Reviews.str.replace(\"n't\", 'not')\ndf_test.Reviews = df_test.Reviews.str.replace(\"n't\", 'not')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\n\ndf_train[\"Label\"] = lb_make.fit_transform(df_train[\"Label\"])"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count=len(df_train['Label'].value_counts())\ntarget_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Reviews'] = df_train['Reviews'].astype(str)\ndf_test['Reviews'] = df_test['Reviews'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fee8a5b73523cd85eebd8aeea43f0c70c938f7ab"},"cell_type":"markdown","source":"## Replacing numbers"},{"metadata":{"_uuid":"fb4e2e2b3e346ce386ce738d8f9c755780c5ccf2","trusted":true},"cell_type":"code","source":"df_train.Reviews = df_train.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\ndf_test.Reviews = df_test.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = df_train['Reviews'].values\nx_test  = df_test['Reviews'].values\ny_train = df_train['Label'].values\nx = np.r_[x_train, x_test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"805e69b93d2932c51f87877b53a0a9c140ac7782"},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{"_uuid":"b5d5360f1094d30dd69500b861b8a92142a01059","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(lower=True, filters='\\n\\t')\ntokenizer.fit_on_texts(x)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test  = tokenizer.texts_to_sequences(x_test)\nvocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\nprint('vocabulary size: {}'.format(vocab_size))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57e6e123b1fe89811c78dd1e503c30f4c0997c4b"},"cell_type":"markdown","source":"## Zero padding"},{"metadata":{"_uuid":"aec770d09524cd17362fee54da2d4bf4dc5feb05","trusted":true},"cell_type":"code","source":"maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\nprint('maxlen: {}'.format(maxlen))\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"873900aeb0026c0fce20d8df63fb839556e52d04","trusted":true},"cell_type":"code","source":"def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n    embedding_matrix = np.zeros([vocab_size, dim])\n    for word, i in word_index.items():\n        if i >= vocab_size:\n            continue\n        vector = embeddings.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector\n    return embedding_matrix\n\nembedding_size = 200\nembedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n                                     vocab_size, embedding_size)\nprint('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10e0b9d14f0c03d97e227e3c186e6ae91575a3fb"},"cell_type":"markdown","source":"# Building a model\n\nIn this time, we will use attention based LSTM model. First of all, we should define the attention layer as follows:"},{"metadata":{"_uuid":"e4baa9ebcbe6a13a7529c096a92b8fbe2092ed70","trusted":true},"cell_type":"code","source":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69d95ad878c49aa95143f6e309d56a49276312c"},"cell_type":"markdown","source":"After defining the attention layer, we will define the entire model:"},{"metadata":{"_uuid":"9c9b7132a66f89ad2cc2c3a6b9bbe6e639b56518","trusted":true},"cell_type":"code","source":"def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n    input_words = Input((maxlen, ))\n    x_words = Embedding(vocab_size,\n                        embedding_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.3)(x_words)\n    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n    x = Attention(maxlen)(x_words)\n    x = Dropout(0.2)(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(target_count, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\nmodel = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\nmodel.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce6567202d90efe96afb0f0740e72d6b04b59526"},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"_uuid":"18f7d4608277cc33958538f544edcb51a9b73549","trusted":true},"cell_type":"code","source":"save_file = 'model.h5'\nhistory = model.fit(x_train, y_train,\n                    epochs=2, verbose=1,\n                    batch_size=1024, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cabc0bc4674270fa6f2d508f70fab16466ef646"},"cell_type":"markdown","source":"# Making a submission file\n\nAfter training the model, we make a submission file by predicting for the test dataset."},{"metadata":{"_uuid":"e37e6c8afa82926ba70582f9241f711dfaee5e55","trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024)\ny_pred = y_pred.argmax(axis=1).astype(int)\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5230270f381badaaca5e9d47b623aa51d183a248","trusted":true},"cell_type":"code","source":"df_test['prediction'] = y_pred\ndf_test[['id', 'prediction']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}