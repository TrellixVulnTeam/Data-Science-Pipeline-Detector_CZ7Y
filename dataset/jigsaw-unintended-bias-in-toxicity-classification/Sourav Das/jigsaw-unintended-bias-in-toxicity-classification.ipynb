{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n#import lightgbm as lgb\nfrom scipy.sparse import vstack, hstack, csr_matrix, spmatrix\nfrom scipy.stats import binom\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nimport datetime\nimport gc\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,roc_auc_score,accuracy_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n#from nltk.stem.porter import PorterStemmer\n#from nltk.corpus import stopwords\n#from nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import TimeSeriesSplit\nimport math\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport xgboost as xgb\nimport seaborn as sns\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\nfrom nltk.corpus import stopwords\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim import corpora\nfrom gensim.models import LdaModel\n\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics.classification import accuracy_score, log_loss\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom tqdm import tqdm\nimport nltk\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Flatten, concatenate, GlobalMaxPooling1D, add \nfrom keras.layers import LSTM, SpatialDropout1D, Input, Dense, Bidirectional, CuDNNLSTM, GlobalAveragePooling1D\nfrom keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor\nfrom keras.utils import np_utils\nfrom scipy.sparse import coo_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# denoting duplicate rows\ndata[data.duplicated(['comment_text','target'], keep=False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['id']==240344]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['id']==282368]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_bool(df, col_name, col_bool):\n    df[col_bool] = np.where(df[col_name] >= 0.5, 1, 0)       \n\ndef convert_dataframe_to_bool(df, columns, col_bool):        \n    bool_df = df.copy()\n    convert_to_bool(bool_df, columns, col_bool)\n    #for col in columns:\n        #convert_to_bool(bool_df, col)\n    return bool_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(0, inplace = True)\n\ndata = convert_dataframe_to_bool(data, ['target'], 'target_bool')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal number of points in both classes:\")\n\ndata['target_bool'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\ndata.hist(column='target')\nplt.xlabel(\"Target/Toxicity level\")\nplt.ylabel(\"No of comments\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_data=data.sort_values('publication_id', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final=sorted_data.drop_duplicates(subset={\"comment_text\",\"target_bool\"}, keep='first', inplace=False)\nfinal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking to see how much % of data still remains\n\n(final['id'].size*1.0)/(data['id'].size*1.0)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['target_bool'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n    'can;t': 'cannot','can;t;ve': 'cannot have',\n    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n    'doesn’t': 'does not',\"don't\": 'do not',\"Don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not',\"They're\":'they are','don’t': 'do not','Don’t': 'do not',\n    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n    \"he'd've\": 'he would have',\"he'll\": 'he will',\n    \"he's\": 'he is',\"He's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n    'how’s': 'how is',\"i'd\": 'i would',\"I'd\": 'i would',\"i'll\": 'i will',\"I'll\": 'i will',\"i'm\": 'i am',\"I'm\": 'i am',\"i've\": 'i have',\"I've\": 'i have','i,d': 'i would','i,ll': 'i will',\n    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\"It's\":'it is',\n    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am','I’m': 'i am',\n    'i’ve': 'i have','I’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have',\"We've\": 'we have','we,d': 'we would','we,ll': 'we will',\n    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have','We’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\" :‑)\":'smiley',\\\n    \" :)\":'smiley',\" :-]\":'smiley',\" :]\":'smiley',\" :‑D\":'laughing',\" :D\":'laughing',\" =D\":'laughing',\" :‑(\":'sad',\" :(\":'sad',\" ;)\":'wink',\" :P\":'cheeky'\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\nps.stem('inducing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn = SnowballStemmer(\"english\")\nsn.stem('inducing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = WordNetLemmatizer()\nps.stem(lm.lemmatize('studies'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\".join([lm.lemmatize(token) for token in \"Thank you!! This would make my life a lot less anxiety inducing. Keep it up, and don't let anyone get in your way!\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer() \nlemmatizer = WordNetLemmatizer()\nsn = SnowballStemmer(\"english\")\n\n\ndef clean_contractions(text, mapping):\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    for s in punct:\n        text = text.replace(s, \" \")\n    text = text.replace('\\n', \" \")\n    text = re.sub('[^A-Za-z]+', ' ', text)\n    #for t in text.split():\n        #text = text.replace(t, lemmatizer.lemmatize(t))\n        #text = text.replace(t, sn.stem(t))\n    #text = ' '.join(e.lower() for e in text.split() if e.lower() not in stopwords)\n    text = ' '.join(e.lower() for e in text.split())\n    return text\n\nprint(\"cleaning started at \",datetime.datetime.now())\nfinal['treated_comment'] = final['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\nprint(\"cleaning ended at \",datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['treated_comment'] = final['treated_comment'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['comment_text'] = final['comment_text'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['treated_comment'].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['comment_text'].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['treated_comment', 'comment_text', 'target_bool', 'target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num=final.shape[0]\nfinal=final.sort_values(by=['created_date'], ascending=True)\ntrain=final[col_list].iloc[:math.ceil(num*0.8),:]\ntest=final[col_list].iloc[math.ceil(num*0.8):,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target_bool'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target_bool'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, cv_df, y_train, y_cv = train_test_split(train, train['target_bool'],\n                                                  stratify=train['target_bool'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=test\ny_test=test['target_bool']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(cv_df.shape)\nprint(y_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', df_train.shape[0])\nprint('Number of data points in test data:', df_test.shape[0])\nprint('Number of data points of Y label in train data:', y_train.shape[0])\nprint('Number of data points of Y label in test data:', y_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 100000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df_train['treated_comment'])\n\n\nsequences_text_train = tokenizer.texts_to_sequences(df_train['treated_comment'])\nsequences_text_test = tokenizer.texts_to_sequences(df_test['treated_comment'])\nsequences_text_cv = tokenizer.texts_to_sequences(cv_df['treated_comment'])\n\nmax_len = max(len(l) for l in sequences_text_train)\npad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)\npad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len)\npad_cv = sequence.pad_sequences(sequences_text_cv, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = Input(shape=(None,))\nx = Embedding(max_words, 128, input_length=max_len)(words)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n\nhidden = concatenate([\n     GlobalMaxPooling1D()(x),\n     GlobalAveragePooling1D()(x),\n])\nhidden = add([hidden, Dense(4 * 128, activation='relu')(hidden)])\nhidden = add([hidden, Dense(4 * 128, activation='relu')(hidden)])\nresult = Dense(1, activation='sigmoid')(hidden)\n\nmodel = Model(inputs=words, outputs=[result])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae','acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nmodel = Sequential()\nmodel.add(Embedding(max_words, 128, input_length=max_len))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\n\n# model compile\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae','acc'])\nmodel.summary()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(datetime.datetime.now())\n\ndf_train=''\ndf_test=''\ncv_df=''\n\nhistory = model.fit(pad_train, y_train.values,verbose=2, epochs=8, batch_size=2048,\n                    validation_data=(pad_cv, y_cv.values))\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(pad_test)\ny_pred=np.where(np.asarray(y_pred) >= 0.5, 1, 0)\n\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\n\ny_prob = model.predict(pad_test)\nprint(\"\\tROC_AUC: %1.3f\" % roc_auc_score(y_test, y_prob))\n\n\nprint(datetime.datetime.now())\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\ntest_data_cleaned = test_data['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\nsequences_text_test_data = tokenizer.texts_to_sequences(test_data_cleaned)\npad_test_data = sequence.pad_sequences(sequences_text_test_data, maxlen=max_len)\ntest_data_pred=model.predict(pad_test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\ndf_submit.prediction = test_data_pred\ndf_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}