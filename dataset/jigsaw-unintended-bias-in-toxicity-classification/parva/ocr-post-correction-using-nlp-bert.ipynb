{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Install requirements"},{"metadata":{},"cell_type":"markdown","source":"1. pip install pytorch-pretrained-bert without internet"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\nfrom pytorch_pretrained_bert.modeling import BertModel","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"BERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer(vocab_file='../input/torch-bert-weights/bert-base-uncased-vocab.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read Image to Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom pytesseract import image_to_string\nimport re\nimport nltk\n# from enchant.checker import SpellChecker\nfrom difflib import SequenceMatcher\nfilename = '/kaggle/input/a-sample-for-ocr/sample.png'\ntext = image_to_string(Image.open(filename))\ntext_original = str(text)\nprint (text_original)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Error Detection By BERT:\nBERT tokenizer breaks the text to wordpieces which are in its vocab. Thus, if a word is broken in small pieces (shown with #), then it is miss-spelled. we use this fact to detect errors. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets tokenize some text (I intentionally mispelled 'plastic' to check berts subword information handling)\ntokens = tokenizer.tokenize(text_original)\ntokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_tokens=[]\ni=0\nwhile i<len(tokens):\n    if tokens[i].startswith(\"##\"):\n        print(tokens[i])\n        head=tokens[i-1]\n        new_tokens=new_tokens[:-1]\n        incorrect_str=head+tokens[i][2:]\n        i=i+1\n        while tokens[i].startswith(\"##\"):\n            incorrect_str=incorrect_str+tokens[i][2:]\n            i=i+1\n        \n        new_tokens.append((incorrect_str,1))\n    else:\n        new_tokens.append((tokens[i],0))\n        i=i+1\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the errors are found which are the tuples with second value of 1. \nWe want to use BERT again to predict the correct values for them. \nTo do so, we replace the incorrect word by [MASK]. BERT is able to predict values for [MASK] tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_text_str=\"\"\nincorrect_words=[]\nfor word in new_tokens:\n    if word[1]==0:\n        new_text_str=new_text_str+word[0]+\" \"\n    else:\n        incorrect_words.append(word[0])\n        new_text_str=new_text_str+'[MASK]'+\" \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_text_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load, train and predict using pre-trained model\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenized_text = tokenizer.tokenize(new_text_str)\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nMASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n# Create the segments tensors\nsegs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\nsegments_ids=[]\nprev=-1\nfor k, s in enumerate(segs):\n    segments_ids = segments_ids + [k] * (s-prev)\n    prev=s\nsegments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\nsegments_tensors = torch.tensor([segments_ids])\n# prepare Torch inputs \ntokens_tensor = torch.tensor([indexed_tokens])\n# Load pre-trained model\nmodel = BertForMaskedLM.from_pretrained(BERT_FP)\n\npredictions = model(tokens_tensor, segments_tensors)\n# Predict all tokens\nwith torch.no_grad():\n    predictions = model(tokens_tensor, segments_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incorrect_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_original","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.topk(predictions[0, MASKIDS[1]], k=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict words for mask using BERT; \n#refine prediction by comparing with original word\nimport nltk\n\ndef predict_word(text_original, predictions, maskids):\n    pred_words=[]\n    for i in range(len(MASKIDS)):\n        list2=[]\n        preds = torch.topk(predictions[0, MASKIDS[i]], k=50) \n        indices = preds[1].tolist()\n        list1 = tokenizer.convert_ids_to_tokens(indices)\n        print(list1)\n        for predicted in list1:\n            dist=nltk.edit_distance(predicted,incorrect_words[i])\n            list2.append((predicted,dist))\n        \n        sorted_list2 = sorted(list2, key=lambda tup: tup[1])\n        final_predicted_word=sorted_list2[0]\n        \n        text_original=text_original.replace(incorrect_words[i],final_predicted_word[0])\n    \n    return text_original\n\n            \n        \npredict_word(text_original, predictions, MASKIDS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}