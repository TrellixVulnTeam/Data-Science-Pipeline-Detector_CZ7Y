{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Jigsaw EDA</font></center></h1>\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/75/Jigsaw.svg\" width=\"300\"></img>\n\n<br>\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n- <a href='#3'>Data exploration</a>   \n    - <a href='#31'>Target feature</a>   \n    - <a href='#32'>Sensitive topics features</a>   \n    - <a href='#33'>Feedback information</a>   \n    - <a href='#34'>Comments data wordclouds</a>   \n    - <a href='#35'>Comments data topic modelling</a>  \n- <a href='#4'>Prepare the model</a>    \n    - <a href='#41'>Build vocabulary</a>   \n    - <a href='#42'>Embedding index and embedding matrix</a>   \n    - <a href='#43'>Check coverage</a>   \n    - <a href='#44'>Transform to lowercase</a>   \n    - <a href='#45'>Remove contractions</a>   \n    - <a href='#46'>Remove punctuation</a>  \n    - <a href='#47'>Tokenize</a>  \n- <a href='#5'>Conclusions</a>      \n- <a href='#6'>References</a>   "},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n## Competition objective\n\nThe competition objective is to build models that detect toxicity and reduce unwanted bias. \nFor example, if a certain minority name is frequently associated with toxic comments, some models might associate the presence of the minority name in a message that is not toxic wiht toxicity and wrongly classify the comment as toxic.\n\n## Background\n\nAt the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\n## References\n\nPlease consult the <a href='#5'>References</a> section for the datasets, Kernels and articles used in this Kernel. Special mention for tje Kernels of @anebzt [8] and @christofhenkel [9].\n"},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>  \n\n## Load packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import gc\nimport os\nimport warnings\nimport operator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom wordcloud import WordCloud, STOPWORDS\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport nltk\nfrom gensim import corpora, models\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom keras.preprocessing.text import Tokenizer\n\npyLDAvis.enable_notebook()\nnp.random.seed(2018)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\nJIGSAW_PATH = \"../input/jigsaw-unintended-bias-in-toxicity-classification/\"\ntrain = pd.read_csv(os.path.join(JIGSAW_PATH,'train.csv'), index_col='id')\ntest = pd.read_csv(os.path.join(JIGSAW_PATH,'test.csv'), index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"print(\"Train and test shape: {} {}\".format(train.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# <a id='3'>Data exploration</a>  \n\nThe comments are stored in `train` and `test` in `comment_text` column.  \nAdditionally, in `train` we have flags for the presence in the comments of a certain sensitive topic.\nThe topic is related to five categories: race or ethnicity, gender, sexual orientation, religion, disability, as following:\n* **race or ethnicity**: asian, black, jewish, latino, other_race_or_ethnicity, white  \n* **gender**: female, male, transgender, other_gender  \n* **sexual orientation**: bisexual, heterosexual, homosexual_gay_or_lesbian, other_sexual_orientation  \n* **religion**: atheist,buddhist,  christian, hindu, muslim, other_religion  \n* **disability**: intellectual_or_learning_disability, other_disability, physical_disability, psychiatric_or_mental_illness  \n\nWe also have few article/comment identification information:\n* created_date  \n* publication_id   \n* parent_id  \n* article_id \n\nSeveral user feedback information associated with the comments are provided:\n* rating  \n* funny  \n* wow  \n* sad  \n* likes  \n* disagree  \n* sexual_explicit  \n\nIn the datasets are also 2 fields relative to annotations:\n* identity_annotator_count  \n* toxicity_annotator_count\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"##  <a id='31'>Target feature</a>\n\nLet's check the distribution of `target` value in the train set."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.title(\"Distribution of target in the train set\")\nsns.distplot(train['target'],kde=True,hist=False, bins=120, label='target')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's represent similarly the distribution of the additional toxicity features."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def plot_features_distribution(features, title):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        sns.distplot(train.loc[~train[feature].isnull(),feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['severe_toxicity', 'obscene','identity_attack','insult','threat']\nplot_features_distribution(features, \"Distribution of additional toxicity features in the train set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='32'>Sensitive topics features</a>\n\nLet's check now the distribution of sensitive topics features values."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['asian', 'black', 'jewish', 'latino', 'other_race_or_ethnicity', 'white']\nplot_features_distribution(features, \"Distribution of race and ethnicity features values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['female', 'male', 'transgender', 'other_gender']\nplot_features_distribution(features, \"Distribution of gender features values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['bisexual', 'heterosexual', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\nplot_features_distribution(features, \"Distribution of sexual orientation features values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['atheist','buddhist',  'christian', 'hindu', 'muslim', 'other_religion']\nplot_features_distribution(features, \"Distribution of religion features values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['intellectual_or_learning_disability', 'other_disability', 'physical_disability', 'psychiatric_or_mental_illness']\nplot_features_distribution(features, \"Distribution of disability features values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='33'>Feedback information</a>\n\nLet's show the feedback values distribution."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def plot_count(feature, title,size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(train))\n    g = sns.countplot(train[feature], order = train[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('rating','rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('funny','funny votes given',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('wow','wow votes given',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('sad','sad votes given',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('likes','likes given',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_count('disagree','disagree given',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"features = ['sexual_explicit']\nplot_features_distribution(features, \"Distribution of sexual explicit values in the train set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='34'>Comments data wordclouds</a>\n\nLet's show the wordcloud of frequent used words in the comments. A maximum of 50 words are shown."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show the prevalent words in the train set (we will use a **20,000** comments sample and show top **50** words)."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train['comment_text'].sample(20000), title = 'Prevalent words in comments - train data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show now the frequent used words in comments for which insult score under **0.25** and above **0.75**."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['insult'] < 0.25]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['insult'] > 0.75]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We show now the most frequent words for comments with threat score bellow **0.25** and above **0.75**."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['threat'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with threat score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['threat'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with threat score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show the wordcloud of frequent words used in comments with obscene **score < 0.25** and obscene **score > 0.75**."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['obscene']< 0.25]['comment_text'], \n               title = 'Prevalent words in comments with obscene score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['obscene'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with obscene score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show now the prevalent words in comments with target (toxicity)  score under **0.25** and over **0.75**."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['target'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with target score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"show_wordcloud(train.loc[train['target'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with target score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='35'>Comments data topic modelling</a>\n\n\n\n### Train data\n\nLet's perform topic modelling on a subset of the comments from train data."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n            result.append(token)\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's verify the processor for one comment."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"comment_sample = train['comment_text'][:1].values[0]\nprint('Original comment: {}'.format(comment_sample))\nprint('Tokenized comment: {}'.format(preprocess(comment_sample)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will apply preprocess on a sample of 200000 comments."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\npreprocessed_comments = train['comment_text'].sample(200000).map(preprocess)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show few documents preprocessed."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"preprocessed_comments.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create now a dictionary. We set also some filters."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\ndictionary = gensim.corpora.Dictionary(preprocessed_comments)\ndictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=75000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's generate a bag-of-words from the dictioanry and the corpus of comments (documents). \nThen, we generate a corpus and apply TF-IDF."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\nbow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_comments]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now run LDA (Latent Dirichelet Allocation algorithm) with the result of TF-IDF applied to the bag-of-words corpus to generate 20 topics."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\nlda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20,\n                                    id2word=dictionary, passes=2, workers=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's represent the first 10 topics, each with 5 words."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"topics = lda_model.print_topics(num_words=5)\nfor i, topic in enumerate(topics[:10]):\n    print(\"Train topic {}: {}\".format(i, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use the LDA model to predict the type of topic for one document. \nWe select one specific document (with index 5)."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"bd5 = bow_corpus[5]\nfor i in range(len(bd5)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bd5[i][0], dictionary[bd5[i][0]],bd5[i][1]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for index, score in sorted(lda_model[bd5], key=lambda tup: -1*tup[1]):\n    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We extracted the topics using LDA.\nLet's represent the topics using the `pyLDAvis` tool."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We save the pyLDAvis graph as a html page."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"pyLDAvis.save_html(vis, \"LDAVis_train.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To access this page, select **Output** from the side menu of this Notebook.  \nSelect Press **Download** button next to the **LDAVis_train.html** item to display the widget.\n\nTo display this graph widget in the Notebook you will need to uncomment the next cell."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#vis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the most important topics per each comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"def topic_sentences(ldamodel=lda_model, corpus=bow_corpus, \\\n                        texts=preprocessed_comments):\n    # initialization\n    sent_topics_df = pd.DataFrame()\n\n    # get main topic in each comment\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # get the dominanttopic, % contribution and keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4),\\\n                                                                topic_keywords]), ignore_index=True)\n            else:\n                break\n    text = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, text], axis=1)\n    return(sent_topics_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_sents_keywords = topic_sentences(ldamodel=lda_model, corpus=bow_corpus, \\\n                                                  texts=preprocessed_comments)\ndominant_topic =topic_sents_keywords.reset_index()\ndominant_topic.columns = ['Comment', 'Dominant Topic', 'Topic Percent Contribution', 'Keywords','Text']\ndominant_topic.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test data\n\nLet's repeat the same procedure for topic modelling but for test data now."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\npreprocessed_comments = test['comment_text'].map(preprocess)\ndictionary = gensim.corpora.Dictionary(preprocessed_comments)\ndictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=75000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_comments]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\nlda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20,\n                                    id2word=dictionary, passes=2, workers=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's print the top 10 topics, each with 5 words."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"topics = lda_model.print_topics(num_words=5)\nfor i, topic in enumerate(topics[:10]):\n    print(\"Test topic {}: {}\".format(i, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We save the pyLDAvis graph as a html page. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.save_html(vis, \"LDAVis_test.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To access this page, select **Output** from the side menu of this Notebook.  \nSelect Press **Download** button next to the **LDAVis_test.html**  item to display the widget.\n\nTo display this graph widget in the Notebook you will need to uncomment the next cell."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#vis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>Prepare the model</a>  \n\nIn preparation for the model using Deep Learning, there are two rules formulated by @christofhenkel [8][9]:\n* Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings  \n* Get your vocabulary as close to the embeddings as possible   \n\n"},{"metadata":{},"cell_type":"markdown","source":"## <a id='41'>Build vocabulary</a>  \n\nWe start with building the vocabulary. First we set few constants, as following:  \n* **EMBED_SIZE** - embedding size - the size of word vector - should match the embedding source (GloVe);  \n* **MAX_FEATURES** - Maximum number of features - the number of unique words to use or number of rows in the embedding vector;  \n* **MAXLEN** - The maximum length of comments text"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBED_SIZE = 300 # size of word vector; this should be set to 300 to match the embedding source\nMAX_FEATURES = 100000 # how many unique words to use (i.e num rows in embedding vector)\nMAXLEN = 220 # max length of comments text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following function is from [8][9]. It builds the vocabulary by browsing all comments, splits in sentences, sentences in words. An accumulator is created, with the value associated to each word equal with the accumulated value."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocabulary(texts):\n    \"\"\"\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: list of list of words\n    output: dictionary of words and their count\n    \"\"\"\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in tqdm_notebook(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We concatenate train and test and build an unique vocabulary with both datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# populate the vocabulary\ndf = pd.concat([train ,test], sort=False)\nvocabulary = build_vocabulary(df['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the first 10 elements of the vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the first 10 elements and their count\nprint({k: vocabulary[k] for k in list(vocabulary)[:10]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='42'>Embedding index and embedding matrix</a>\n\nLet's build the embedding index (this is a dictionary with keys the embeddings and the values are arrays of their embedding representations) and embedding matrix (a matrix representation of the embeddings).  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(file):\n    \"\"\"\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: embeddings file\n    output: embedding index\n    \"\"\"\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nGLOVE_PATH = '../input/glove840b300dtxt/'\nprint(\"Extracting GloVe embedding started\")\nembed_glove = load_embeddings(os.path.join(GLOVE_PATH,'glove.840B.300d.txt'))\nprint(\"Embedding completed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the size of embeeding structure loaded."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now create the embeeding matrix using the word index and the embeeding index created. We are using the MAX_FEATURES to limit the number of features thus the size of embeeding matrix. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_matrix(word_index, embeddings_index):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: word index, embedding index\n    output: embedding matrix\n    '''\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    EMBED_SIZE = all_embs.shape[1]\n    nb_words = min(MAX_FEATURES, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n    for word, i in tqdm_notebook(word_index.items()):\n        if i >= MAX_FEATURES:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='43'>Check coverage</a>\n\nWith the following function, we check coverage of embeddings for the vocabulary created from the train and test dataset. We will sue this function repeatedly after each pre-processing operation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_coverage(vocab, embeddings_index):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: vocabulary, embedding index\n    output: list of unknown words; also prints the vocabulary coverage of embeddings and the % of comments text covered by the embeddings\n    '''\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in tqdm_notebook(vocab.keys()):\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n    print('Found embeddings for {:.3%} of vocabulary'.format(len(known_words)/len(vocab)))\n    print('Found embeddings for {:.3%} of all text'.format(nb_known_words/(nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the initial coverage of vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Verify the intial vocabulary coverage\")\noov_glove = check_coverage(vocabulary, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding only cover ~15% of the vocabulary and this accounts for 89.6% from the entire comments texts.  \nLet's check what kind of words are missing from the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the missing words are punctuation and upper case words. Let's remove punctuation and apply lowerisation (i.e. turn all words to lowercase).\n\n## <a id='44'>Transform to lowercase</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lower(embedding, vocab):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: vocabulary, embedding matrix\n    output: modify the embeddings to include the lower case from vocabulary\n    '''\n    count = 0\n    for word in tqdm_notebook(vocab):\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda x: x.lower())\ntest['comment_text'] = test['comment_text'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check coverage for vocabulary with lower case\")\noov_glove = check_coverage(vocabulary, embed_glove)\nadd_lower(embed_glove, vocabulary) # operates on the same vocabulary\noov_glove = check_coverage(vocabulary, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check again the words not included in embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that most of the remaining include contractions and words with punctuation. We continue with removing contractions.\n\n## <a id='45'>Remove contractions</a> \n\nContractions are modified forms of words and expressions. We will map, using the following dictionary, the contracted forms on words existent in embeddings.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\nlen(contraction_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def known_contractions(embed):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: embedding matrix\n    output: known contractions (from embeddings)\n    '''\n    known = []\n    for contract in tqdm_notebook(contraction_mapping):\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Known contractions in GloVe embeddings:\")\nprint(known_contractions(embed_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: current text, contraction mappings\n    output: modify the comments to use the base form from contraction mapping\n    '''\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest['comment_text'] = test['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will build again the vocabulary and check again the coverage."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train ,test], sort=False)\nvocab = build_vocabulary(df['comment_text'])\nprint(\"Check embeddings after applying contraction mapping\")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that, with lowercase transformation and contraction treatment, the improvement in termns of coverage is not significant. Punctuation, which we didn't treated yet, seems to be really important.\n\n## <a id='46'>Remove punctuation</a>  \n\nWe remove as well punctuation."},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\npunct_mapping += '©^®` <→°€™› ♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n\ndef unknown_punct(embed, punct):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: current text, contraction mappings\n    output: unknown punctuation\n    '''\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Find unknown punctuation:\")\nprint(unknown_punct(embed_glove, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"puncts = {\"‘\": \"'\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '…': ' '}\n\ndef clean_special_chars(text, punct, mapping):\n    '''\n    credits to: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n    credits to: https://www.kaggle.com/anebzt/quora-preprocessing-model\n    input: current text, punctuations, punctuation mapping\n    output: cleaned text\n    '''\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ') \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda x: clean_special_chars(x, punct_mapping, puncts))\ntest['comment_text'] = test['comment_text'].apply(lambda x: clean_special_chars(x, punct_mapping, puncts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's rebuid the vocabulary after replacing the punctuation and check again the coverage."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf = pd.concat([train ,test], sort=False)\nvocab = build_vocabulary(df['comment_text'])\nprint(\"Check coverage after punctuation replacement\")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a significant improvement by extracting punctuation. 99.7% from the text is covered by the embeddings and 57% of the vocabulary. Punctuation appears to be very important."},{"metadata":{},"cell_type":"markdown","source":"## <a id='47'>Tokenize</a>\n\nWe apply tokenization for train and test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(list(train))\ntrain = tokenizer.texts_to_sequences(train)\ntest = tokenizer.texts_to_sequences(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready to create a model. Stay tuned!"},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>Conclusions</a>  \n\nWe applied succesively several techniques to improve the vocabulary coverage with GloVe for the entire corpus of texts in `comment_text`, as following:\n* transform to lowercase;  \n* remove contractions;  \n* remove punctuation;  \n\n\nBy just using these simple text transformation, we were able to found embeddings for 54.401% of vocabulary and for 99.718% of all text, from the orginial coverage, before these processings, of 15% of the vocabularyand 89.6% from all text."},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>References</a>  \n\n[1] https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/  \n[2] https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24  \n[3] https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d  \n[4] https://www.aclweb.org/anthology/W14-3110   \n[5] https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/  \n[6] https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets  \n[7] https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  \n[8] https://www.kaggle.com/anebzt/quora-preprocessing-model  \n[9] https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings  \n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}