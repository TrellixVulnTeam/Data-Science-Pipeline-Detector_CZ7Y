{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport nltk\nfrom tqdm import tqdm_notebook\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm_notebook().pandas()\n\ndef echo(s):\n    print(s)\n    os.system('echo \\\"' + str(s) + '\\\"')\n\nimport os\necho(os.listdir(\"../input\"))\n\nimport pickle\nimport psutil\nimport multiprocessing as mp\nimport itertools\ncores = psutil.cpu_count()\necho(f\"There are {cores} CPU cores.\")\npartitions = 4\necho(f\"Using {partitions} partitions.\")\n\nimport emoji\nimport unidecode\nimport re\nimport random\nfrom string import punctuation\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nfrom typing import List\nfrom IPython.display import display, HTML\n\ndef iterate_counter(fn):\n    global counter\n    counter = 0\n    def f(*args, **kwargs):\n        global counter\n        res = fn(*args, **kwargs)\n        counter += 1\n        if counter % 100000 == 0 and counter > 1:\n            echo(f\"Processed has iterated {counter} times.\")\n        return res\n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n    tqdm = nop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List\nimport numpy as np\n\n\"\"\"\nInterface definitions\n\"\"\"\nclass BasePreprocess():\n    def __init__(self):\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_preprocess(self, sentence: str) -> List[str]:\n        \"\"\"\n        @param sentence - a string of untokenized sentence.\n        @return sentence - preprocessed sentence\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_batch_preprocess(self, text: List[str]) -> List[List[str]]:\n        \"\"\"\n        @param text - a list of string of untokenized sentence.\n        @return tokens - a list of preprocessed sentence\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n\nclass BaseTokenizer():\n    def __init__(self):\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_tokens(self, sentence: str) -> List[str]:\n        \"\"\"\n        @param sentence - a string of untokenized sentence.\n        @return tokens - a list of string of tokens\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_batch_tokens(self, text: List[str]) -> List[List[str]]:\n        \"\"\"\n        @param text - a list of string of untokenized sentence.\n        @return tokens - a list of list of string of tokens\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n\nclass BaseEmbedding():\n    def __init__(self):\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_embedding(self, tokens: List[str]) -> np.ndarray:\n        \"\"\"\n        @param tokens - a list of string of tokens\n        @return embedding - the embedding of the sentence.\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_batch_embedding(self, text_tokens: List[List[str]]) -> np.ndarray:\n        \"\"\"\n        @param text_tokens - a list of tokens\n        @return embedding - the embedding of the list of tokens.\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n\nclass BaseModel():\n    def __init__(self):\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def predict(self, embed: np.ndarray) -> np.float32:\n        \"\"\"\n        @param embed - the embedding of the sentence.\n        @return prediction - the prediction of the embedding.\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")\n    def get_batch_predict(self, batch_embed: np.ndarray) -> np.ndarray:\n        \"\"\"\n        @param batch_embed -  the embedding of the list of tokens.\n        @return  prediction - the prediction of the list of embeddings.\n        \"\"\"\n        raise NotImplementedError(\"This is an interface, dont call me\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicBroccoliPreprocess(BasePreprocess):\n    def __init__(self):\n        self.stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n        self.stopwords = '((^|[\\s]+)' + '([\\s]+|$))|((^|[\\s]+)'.join(nltk.corpus.stopwords.words('english')) + '([\\s]+|$))'\n    \n    def urls(self, sentence):\n        '''\n        Keeps only alphanumerics in URLs of the sentence.\n        - Related to spam or propaganda.\n        - Negatively correlated with target.\n        '''\n        tokens = sentence.split()\n        sentence, url_count = re.subn(\n            r'[^\\s]*((:\\/\\/)|(www.))[^\\s]*',\n            lambda x: re.sub(r'(%20)|([^A-Za-z0-9])', ' ', x.group(0)),\n            sentence\n        )\n        return sentence, {\n            'urls': url_count\n        }\n    \n    def uncommon_chars(self, sentence, sentence_length):\n        '''\n        Uncommon characters are characters that are neither alphanumeric nor common punctuation from python string library.\n        Replaces some common uncommon characters and replaces the rest with spaces.\n        - Related to spam or propaganda.\n        - Negatively correlated with target.\n        '''\n        uncommon_pattern = re.compile(r\"([^a-zA-Z0-9\\s\\\\\" + r'\\\\'.join(punctuation) + r\"]+)\")\n        char_count = 0\n        for match in uncommon_pattern.finditer(sentence):\n            char_count += len(match.group(0))\n        pct_uncommon = char_count / sentence_length\n        sentence = emoji.demojize(sentence)\n        sentence = re.sub(r':[\\w]+:', lambda x: x.group(0).replace('_', ' '), sentence)\n        sentence = unidecode.unidecode(sentence)\n        return sentence, {\n            'uncommon_chars': char_count,\n            'uncommon_pct': pct_uncommon\n        }\n    \n    def uppercase(self, sentence):\n        '''\n        Convert sentence to lowercase and count the number of capital letters.\n        - Highly correlated with toxicity subattributes.\n        - Somewhat correlates with spam.\n        '''\n        caps_count = 0\n        full_caps_tokens_count = 0\n        tokens = sentence.split()\n        for token in tokens:\n            if token.isupper():\n                full_caps_tokens_count += 1\n            for c in token:\n                if c.isupper():\n                    caps_count += 1\n        sentence = sentence.lower()\n        return sentence, {\n            'caps': caps_count,\n            'full_caps': full_caps_tokens_count\n        }\n\n    def contractions(self, sentence, use_extra=True):\n        '''\n        Replaces some of the most common contractions with their expanded forms.\n        - Usable only after converted to lowercase\n        - Converts ` to ' because a lot of the contractions use `\n        '''\n        sentence = re.sub('`', '\\'', sentence)\n        contractions_mapping = {\n            \"don't\": \"do not\", \"i'm\": \"i am\", \"can't\": \"cannot\", \"doesn't\": \"does not\", \"didn't\": \"did not\", \"you're\": \"you are\",\n            \"isn't\": \"is not\", \"won't\": \"will not\", \"i've\": \"i have\", \"they're\": \"they are\", \"aren't\": \"are not\", \"wouldn't\": \"would not\",\n            \"wasn't\": \"was not\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"we're\": \"we are\", \"couldn't\": \"could not\", \"haven't\": \"have not\",\n            \"shouldn't\": \"should not\", \"you've\": \"you have\", \"hasn't\": \"has not\", \"you'll\": \"you will\", \"we've\": \"we have\", \"we'll\": \"we will\",\n            \"they've\": \"they have\", \"weren't\": \"were not\", \"you'd\": \"you would\", \"they'll\": \"they will\", \"he'll\": \"he will\", \"ain't\": \"am not\",\n            \"they'd\": \"they would\", \"he'd\": \"he would\", \"gov't\": \"government\", \"we'd\": \"we would\", \"it'll\": \"it will\", \"hadn't\": \"had not\",\n            \"would've\": \"would have\", \"she'll\": \"she will\", \"that'll\": \"that will\", \"who've\": \"who have\", \"she'd\": \"she would\", \"it'd\": \"it would\",\n            \"it'd\": \"it would\", \"should've\": \"should have\", \"y'all\": \"you all\", \"who'd\": \"who would\", \"could've\": \"could have\",\n            \"there'd\": \"there would\", \"cont'd\": \"continued\", \"there'll\": \"there will\", \"ma'am\": \"madam\", \"how'd\": \"how did\", \"who'll\": \"who will\",\n            \"needn't\": \"need not\", \"must've\": \"must have\", \"that'd\": \"that would\", \"y'know\": \"you know\", \"ya'll\": \"you all\", \"mustn't\": \"must not\",\n            \"where'd\": \"where did\", \"might've\": \"might have\", \"who're\": \"who are\", \"that've\": \"that have\", \"ne'er\": \"never\",\n            \"there're\": \"there are\", \"this'll\": \"this will\", \"what'll\": \"what will\", \"what'd\": \"what did\", \"what're\": \"what are\",\n            \"there've\": \"there have\", \"where've\": \"where have\", \"what've\": \"what have\", \"that're\": \"that are\"\n        }\n        if use_extra:\n            extra = { # from https://gist.github.com/nealrs/96342d8231b75cf4bb82\n              \"ain't\": \"am not\",\n              \"aren't\": \"are not\",\n              \"can't\": \"cannot\",\n              \"can't've\": \"cannot have\",\n              \"'cause\": \"because\",\n              \"could've\": \"could have\",\n              \"couldn't\": \"could not\",\n              \"couldn't've\": \"could not have\",\n              \"didn't\": \"did not\",\n              \"doesn't\": \"does not\",\n              \"don't\": \"do not\",\n              \"hadn't\": \"had not\",\n              \"hadn't've\": \"had not have\",\n              \"hasn't\": \"has not\",\n              \"haven't\": \"have not\",\n              \"he'd\": \"he would\",\n              \"he'd've\": \"he would have\",\n              \"he'll\": \"he will\",\n              \"he'll've\": \"he will have\",\n              \"he's\": \"he is\",\n              \"how'd\": \"how did\",\n              \"how'd'y\": \"how do you\",\n              \"how'll\": \"how will\",\n              \"how's\": \"how is\",\n              \"i'd\": \"i would\",\n              \"i'd've\": \"i would have\",\n              \"i'll\": \"i will\",\n              \"i'll've\": \"i will have\",\n              \"i'm\": \"i am\",\n              \"i've\": \"i have\",\n              \"isn't\": \"is not\",\n              \"it'd\": \"it had\",\n              \"it'd've\": \"it would have\",\n              \"it'll\": \"it will\",\n              \"it'll've\": \"it will have\",\n              \"it's\": \"it is\",\n              \"let's\": \"let us\",\n              \"ma'am\": \"madam\",\n              \"mayn't\": \"may not\",\n              \"might've\": \"might have\",\n              \"mightn't\": \"might not\",\n              \"mightn't've\": \"might not have\",\n              \"must've\": \"must have\",\n              \"mustn't\": \"must not\",\n              \"mustn't've\": \"must not have\",\n              \"needn't\": \"need not\",\n              \"needn't've\": \"need not have\",\n              \"o'clock\": \"of the clock\",\n              \"oughtn't\": \"ought not\",\n              \"oughtn't've\": \"ought not have\",\n              \"shan't\": \"shall not\",\n              \"sha'n't\": \"shall not\",\n              \"shan't've\": \"shall not have\",\n              \"she'd\": \"she would\",\n              \"she'd've\": \"she would have\",\n              \"she'll\": \"she will\",\n              \"she'll've\": \"she will have\",\n              \"she's\": \"she is\",\n              \"should've\": \"should have\",\n              \"shouldn't\": \"should not\",\n              \"shouldn't've\": \"should not have\",\n              \"so've\": \"so have\",\n              \"so's\": \"so is\",\n              \"that'd\": \"that would\",\n              \"that'd've\": \"that would have\",\n              \"that's\": \"that is\",\n              \"there'd\": \"there had\",\n              \"there'd've\": \"there would have\",\n              \"there's\": \"there is\",\n              \"they'd\": \"they would\",\n              \"they'd've\": \"they would have\",\n              \"they'll\": \"they will\",\n              \"they'll've\": \"they will have\",\n              \"they're\": \"they are\",\n              \"they've\": \"they have\",\n              \"to've\": \"to have\",\n              \"wasn't\": \"was not\",\n              \"we'd\": \"we had\",\n              \"we'd've\": \"we would have\",\n              \"we'll\": \"we will\",\n              \"we'll've\": \"we will have\",\n              \"we're\": \"we are\",\n              \"we've\": \"we have\",\n              \"weren't\": \"were not\",\n              \"what'll\": \"what will\",\n              \"what'll've\": \"what will have\",\n              \"what're\": \"what are\",\n              \"what's\": \"what is\",\n              \"what've\": \"what have\",\n              \"when's\": \"when is\",\n              \"when've\": \"when have\",\n              \"where'd\": \"where did\",\n              \"where's\": \"where is\",\n              \"where've\": \"where have\",\n              \"who'll\": \"who will\",\n              \"who'll've\": \"who will have\",\n              \"who's\": \"who is\",\n              \"who've\": \"who have\",\n              \"why's\": \"why is\",\n              \"why've\": \"why have\",\n              \"will've\": \"will have\",\n              \"won't\": \"will not\",\n              \"won't've\": \"will not have\",\n              \"would've\": \"would have\",\n              \"wouldn't\": \"would not\",\n              \"wouldn't've\": \"would not have\",\n              \"y'all\": \"you all\",\n              \"y'alls\": \"you alls\",\n              \"y'all'd\": \"you all would\",\n              \"y'all'd've\": \"you all would have\",\n              \"y'all're\": \"you all are\",\n              \"y'all've\": \"you all have\",\n              \"you'd\": \"you had\",\n              \"you'd've\": \"you would have\",\n              \"you'll\": \"you you will\",\n              \"you'll've\": \"you you will have\",\n              \"you're\": \"you are\",\n              \"you've\": \"you have\"\n            }\n            contractions_mapping = {**contractions_mapping, **extra}\n        contractions_pattern = re.compile('(%s)' % '|'.join(contractions_mapping.keys()))\n        sentence = contractions_pattern.sub(lambda x: contractions_mapping[x.group(0)], sentence)\n        return sentence\n    \n    def list_items(self, sentence):\n        if re.match('\\(1\\)|\\(1\\.\\)|1\\)|1\\.\\)', sentence) and re.match('\\(2\\)|\\(2\\.\\)|2\\)|2\\.\\)', sentence):\n            sentence = re.sub('\\(1\\)|\\(1\\.\\)|1\\)|1\\.\\)', ' @LIST ', sentence)\n            sentence = re.sub('\\(2\\)|\\(2\\.\\)|2\\)|2\\.\\)', ' @LIST ', sentence)\n            for i in range(3, 20):\n                if re.match(f'\\({i}\\)|\\({i}\\.\\)|{i}\\)|{i}\\.\\)', sentence):\n                    sentence = re.sub('\\({i}\\)|\\({i}\\.\\)|{i}\\)|{i}\\.\\)', ' @LIST ', sentence)\n                else:\n                    break\n        if re.match('\\(a\\)|\\(a\\.\\)|a\\)|a\\.\\)', sentence) and re.match('\\(b\\)|\\(b\\.\\)|b\\)|b\\.\\)', sentence):\n            sentence = re.sub('\\(a\\)|\\(a\\.\\)|a\\)|a\\.\\)', ' @LIST ', sentence)\n            sentence = re.sub('\\(b\\)|\\(b\\.\\)|b\\)|b\\.\\)', ' @LIST ', sentence)\n            for i in range(2, 26):\n                c = chr(ord('a') + i)\n                if re.match(f'\\({c}\\)|\\({c}\\.\\)|{c}\\)|{c}\\.\\)', sentence):\n                    comment = re.sub('\\({c}\\)|\\({c}\\.\\)|{c}\\)|{c}\\.\\)', ' @LIST ', sentence)\n                else:\n                    break\n        roman_numerals = ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv']\n        if re.match('\\(i\\)|\\(i\\.\\)|i\\)|i\\.\\)', sentence) and re.match('\\(ii\\)|\\(ii\\.\\)|ii\\)|ii\\.\\)', sentence):\n            sentence = re.sub('\\(i\\)|\\(i\\.\\)|i\\)|i\\.\\)', ' @LIST ', sentence)\n            sentence = re.sub('\\(ii\\)|\\(ii\\.\\)|ii\\)|ii\\.\\)', ' @LIST ', sentence)\n            for i in range(2, len(roman_numerals)):\n                num = roman_numerals[i]\n                if re.match(f'\\({num}\\)|\\({num}\\.\\)|{num}\\)|{num}\\.\\)', sentence):\n                    sentence = re.sub('\\({num}\\)|\\({num}\\.\\)|{num}\\)|{num}\\.\\)', ' @LIST ', sentence)\n                else:\n                    break\n        sentence = re.sub('#[0-9]+', ' # @LIST ', sentence)\n        return sentence\n    \n    def text_emojis(self, sentence):\n        emojis = {\n            'happy': [':)', ':-)'],\n            'frown': [':(', '):'],\n            'wink': [';)', ';-)']\n        }\n        for key in emojis:\n            pattern = re.compile(r'(' + r'|'.join(['\\\\' + '\\\\'.join(token) for token in emojis[key]]) + ')')\n            sentence = pattern.sub(' ' + key + ' ', sentence)\n        return sentence\n\n    def closing_puncts(self, sentence):\n        sentence = re.sub('[\\[\\{\\<]', '(', sentence)\n        sentence = re.sub('[\\]\\}\\>]', ')', sentence)\n        sentence = self.list_items(sentence)\n        sentence = self.text_emojis(sentence)\n        sentence = re.sub('[\\(\\)]', ' ', sentence)\n        return sentence\n    \n    def exclamations_questions(self, sentence):\n        sentence = re.sub('![\\s]+!', '!!', sentence)\n        sentence = re.sub('![\\s]+!', '!!', sentence) # twice to substitute for all\n        sentence = re.sub('\\?[\\s]+\\?', '??', sentence)\n        sentence = re.sub('\\?[\\s]+\\?', '??', sentence)\n        sentence = re.sub('\\?[\\s]+!', '?!', sentence)\n        sentence = re.sub('![\\s]+\\?', '!?', sentence)\n        sentence = re.sub(r'[!\\?]+', ' \\g<0> ', sentence)\n        sentence = re.sub(r'[!\\?]+', lambda x: ''.join(sorted(x.group(0), reverse=True)), sentence)\n        sentence = re.sub('(!+)|(\\?+)', ' \\g<0> ', sentence)\n        return sentence\n    \n    def censored(self, sentence):\n        puncts = '#$%&*+@\\\\^`|~'\n        sentence = re.sub('[\\\\' + '\\\\'.join(puncts) + ']+', lambda x: ' @CENSOR ' if len(set(x.group(0))) >= 3 else x.group(0), sentence)\n        return sentence\n    \n    def asterisks(self, sentence):\n        def process(token):\n            token = token.group(0)\n            if not (re.search('[a-zA-Z]', token) is None):\n                has_bold = False\n                if token[0] == '*' and token[-1] == '*':\n                    has_bold = True\n                    token = token[1:len(token)-1]\n                token = re.sub('\\*', '-', token)\n                if has_bold:\n                    token = '*' + token + '*'\n            return token\n        sentence = re.sub('[\\S]*[\\*]+[\\S]*', process, sentence)\n        return sentence\n    \n    def remove_puncts(self, sentence):\n        puncts = \"+=\\\\^`|~\"\n        sentence = re.sub('((sh^t)|(f^ck)|(f^^k)|(bi^ch)|(s^^t))', lambda x: x.group(0).replace('^', '-'), sentence)\n        sentence = re.sub('[\\\\' + '\\\\'.join(puncts) + ']', ' ', sentence)\n        return sentence\n    \n    def compress_puncts(self, sentence):\n        puncts = '[\\\\' + '\\\\'.join(punctuation.replace(\"'\", '').replace('\"', '')) + ']'\n        pattern = re.compile(puncts + '[\\s]+' + puncts)\n        sentence = pattern.sub(lambda x: re.sub('[\\s]+', '', x.group(0)), sentence)\n        sentence = pattern.sub(lambda x: re.sub('[\\s]+', '', x.group(0)), sentence)\n        pattern = re.compile(puncts + '+')\n        sentence = pattern.sub(lambda x: ''.join(sorted(x.group(0))), sentence)\n        return sentence\n    \n    def token_counts(self, sentence):\n        '''\n        Count the number of unique tokens of the regex form [\\w'].\n        - Note that we use token instead of word because \\w has _ in it.\n        '''\n        tokens = {}\n        for match in re.finditer(r'[\\w\\']+', sentence):\n            token = match.group(0)\n            if token[0] == '\\'':\n                token = token[1:]\n            if len(token) > 0 and token[-1] == '\\'':\n                token = token[:-1]\n            if len(token) > 0:\n                tokens[token] = tokens.get(token, 0) + 1\n        unique_count = len(tokens.keys())\n        total_count = sum(tokens.values())\n        return {\n            'unique_tokens': unique_count,\n            'tokens': total_count\n        }\n    \n    def numbers(self, sentence):\n        sentence = re.sub('[0-9]+', ' @NUMBER ', sentence)\n        return sentence\n    \n    def process_punctuations(self, sentence, sentence_length):\n        '''\n        Process the punctuations to clean up a lot of dirty internet spams.\n        '''\n        count_puncts = '!?*/^+'\n        puncts_stats = {}\n        for punct in count_puncts:\n            puncts_stats[punct] = 0\n        for match in re.finditer('[\\\\' + '\\\\'.join(count_puncts) + ']', sentence):\n            punct = match.group(0)\n            puncts_stats[punct] += 1\n        for punct in count_puncts:\n            puncts_stats[punct + '_vs_count'] = puncts_stats[punct] / sentence_length\n        sentence = self.closing_puncts(sentence)\n        sentence = self.exclamations_questions(sentence)\n        sentence = self.censored(sentence)\n        sentence = re.sub('(^|[\\s])#([\\s]|$)', lambda x: x.group(0).replace('#', 'number'), sentence)\n        sentence = re.sub('(^|[\\s])\\$[\\S]*', ' $ @NUMBER ', sentence)\n        sentence = re.sub('[\\S]*%', ' @NUMBER % ', sentence)\n        sentence = re.sub('[0-9 ]+/[0-9 ]+', ' @NUMBER ', sentence)\n        sentence = re.sub('(^|[\\s])&([\\s,]|$)', lambda x: x.group(0).replace('&', 'and'), sentence)\n        sentence = re.sub('&', 'n', sentence)\n        sentence = self.asterisks(sentence)\n        sentence = self.remove_puncts(sentence)\n        sentence = self.compress_puncts(sentence)\n        return sentence, puncts_stats\n    \n    def common(self, sentence):\n        sentence = re.sub(self.stopwords, ' ', sentence)\n        return sentence\n    \n    def stem(self, sentence):\n        sentence = re.sub('[a-z]+', lambda x: self.stemmer.stem(x.group(0)), sentence)\n        return sentence\n    \n    def possessives(self, sentence):\n        sentence = re.sub(r\"[\\w]+'s[\\W]+\", lambda x: x.group(0).replace(\"'s\", ''), sentence)\n        return sentence\n    \n    def manual(self, sentence):\n        words = {\n            'trolly': 'troll'\n        }\n        manual_corrections = re.compile('(%s)' % '|'.join(words.keys()))\n        sentence = manual_corrections.sub(lambda x: words[x.group(0)], sentence)\n        return sentence\n    \n    def get_preprocess(self, sentence: str) -> (str, dict):\n        sentence_length = len(sentence)\n        token_count = self.token_counts(sentence)\n        sentence, new_lines_count = re.subn('\\n', ' ', sentence)\n        sentence, url_count = self.urls(sentence)\n        sentence, uncommon_count = self.uncommon_chars(sentence, sentence_length)\n        sentence, caps_count = self.uppercase(sentence)\n        sentence = self.contractions(sentence)\n        sentence = self.possessives(sentence)\n        sentence, punct_stats = self.process_punctuations(sentence, sentence_length)\n        sentence = self.numbers(sentence)\n        sentence = self.manual(sentence)\n        #sentence = self.common(sentence)\n        #sentence = self.stem(sentence)\n        sentence = re.sub('[\\s]+', ' ', sentence)\n        return sentence, {\n            'length': sentence_length,\n            'newlines': new_lines_count,\n            **token_count,\n            **url_count,\n            **uncommon_count,\n            **caps_count,\n            **punct_stats\n        }\n    \n    def process_token(self, token):\n        token = re.sub('&', 'n', token) # The new & symbol since casual tokenizer doesn't keep it together\n        if not (re.search('[a-zA-Z]', token) is None):\n            token = re.sub('\\*', '-', token) # The new * censor since casual tokenizer doesn't keep it together\n        token = self.compress_puncts(token)\n        return token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple tokenization using NLTK tokenize casual\nclass SimpleTweetTokenizer(BaseTokenizer):\n    def __init__(self):\n        self.tokenizer = nltk.tokenize.casual.TweetTokenizer(\n            preserve_case=True,\n            reduce_len=True,\n            strip_handles=False\n        )\n    \n    def get_tokens(self, sentence: str) -> List[str]:\n        tokens = self.tokenizer.tokenize(sentence)\n        return tokens\n\n    def tokens_parallelize(self, part):\n        return [self.get_tokens(sent) for sent in part]\n    \n    def get_batch_tokens(self, text: List[str]) -> List[List[str]]:\n        '''\n        part_size = len(text) // partitions\n        parts = [text[i:i + part_size] for i in range(0, len(text), part_size)]\n        with mp.Pool(processes=cores) as pool:\n            pooled = pool.map(self.tokens_parallelize, parts)\n            batch_tokens = list(itertools.chain.from_iterable(pooled))\n            return batch_tokens\n        '''\n        return [self.get_tokens(sent) for sent in text]\n    \n    def get_segmentized_tokens(self, sentence: str, size: int) -> List[List[str]]:\n        tokens = self.tokenizer.tokenize(sentence)\n        segmentized_tokens = []\n        loc = len(tokens) - size\n        while loc >= 0:\n            segmentized_tokens.append(tokens[loc:loc + size])\n            loc -= size\n        if loc > -size:\n            segmentized_tokens.append(tokens[:loc + size])\n        segmentized_tokens.reverse()\n        return segmentized_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GloveFastTextEmbedding(BaseEmbedding):\n    def __init__(self, preprocessor, unk_freq=None):\n        '''\n        @param unk_freq - change a word to token <UNK> if its frequency in data\n                          is less than unk_freq times. Use None or 0 if want all tokens.\n        '''\n        self.vocab = {}\n        self.preprocessor = preprocessor\n        if not unk_freq:\n            unk_freq = 0\n        self.unk_freq = unk_freq\n        self.finalized = False\n    \n    def __len__(self):\n        return len(self.vocab)\n    \n    def _load_from_pickle(self):\n        with open('embedding_vocab_weights.pickle', 'rb') as f:\n            self.vocab = pickle.load(f)\n        with open('embedding_vocab.pickle', 'rb') as f:\n            self.indexer = pickle.load(f)\n        self.finalized = True\n        print(f\"Loaded {len(self.vocab)} embeddings.\")\n        \n    def _parse(self, line):\n        '''\n        @param line - a line of text from glove or fasttext pretrained embedding file\n        @return (token, np.ndarray) - the token of the line and its corresponding embedding weights\n        '''\n        mapping = line.strip().split(\" \")\n        token = mapping[0]\n        weights = np.array(mapping[1:]).astype(np.float32)\n        return (token, weights)\n    \n    def _add_token(self, token, weight_type, weights=None):\n        if weight_type != \"glove\" and weight_type != \"fasttext\" and weight_type != \"torch\":\n            raise ValueError(\"weight_type should be one of the three values: glove, fasttext, or torch\")\n        if weights is None and (weight_type == \"glove\" or weight_type == \"fasttext\"):\n            raise ValueError(\"weights should not be None for a glove or fasttext embedding\")\n        if not token in self.vocab:\n            self.vocab[token] = {\n                \"count\": 0,\n                \"glove\": None,\n                \"fasttext\": None\n            }\n        if weight_type != \"torch\":\n            self.vocab[token][weight_type] = weights\n        self.vocab[token][\"count\"] += 1\n        \n    '''\n    Add all the tokens from train and test first, then add existing\n    pretrained word embeddings from GLoVe and FastText to reduce\n    the amount of space used.\n    \n    Procedure:\n    add_token (for all vocabulary words)\n    build_vocab\n    finalize\n    '''\n\n    def add_token(self, token):\n        if self.finalized:\n            raise RuntimeError(\"Vocabulary has been finalized, no new token may be added\")\n        if self.unk_freq > 0 and token == '@UNK':\n            raise ValueError(\"Detected \\\"@UNK\\\" token in the dataset\")\n        if token == '@START':\n            raise ValueError('Detected \"@START\" token in the dataset')\n        if token == '@PAD':\n            raise ValueError('Detected \"@PAD\" token in the dataset')\n        if token == '@END':\n            raise ValueError('Detected \"@END\" token in the dataset')\n        self._add_token(token, weight_type=\"torch\")\n\n    def build_vocab(self):\n        echo(\"Adding FastText vocabulary...\")\n        with open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec') as f:\n            tokens_count = int(f.readline().split()[0])\n            actual_count = 0\n            for i in range(tokens_count):\n                line = f.readline()\n                token, weights = self._parse(line)\n                processed_token = self.preprocessor.process_token(token)\n                if token in self.vocab:\n                    if self.vocab[token]['fasttext'] is None or token == processed_token:\n                        self._add_token(token, weight_type='fasttext', weights=weights)\n                        actual_count += 1\n            echo(f\"Added {actual_count} tokens from FastText\")\n        echo(\"Adding GLoVe vocabulary...\")\n        with open('../input/glove840b300dtxt/glove.840B.300d.txt') as f:\n            tokens_count = 0\n            for line in f:\n                token, weights = self._parse(line)\n                processed_token = self.preprocessor.process_token(token)\n                if token in self.vocab:\n                    if self.vocab[token]['glove'] is None or token == processed_token:\n                        self._add_token(token, weight_type='glove', weights=weights)\n                        tokens_count += 1\n            echo(f\"Added {tokens_count} tokens from GLoVe\")\n    \n    def finalize(self):\n        unk_tokens = []\n        self.no_embed_tokens = []\n        for token in self.vocab:\n            if self.vocab[token]['glove'] is None \\\n                    and self.vocab[token]['fasttext'] is None:\n                if self.vocab[token]['count'] >= self.unk_freq:\n                    self.no_embed_tokens.append(token)\n                else:\n                    unk_tokens.append(token)\n        self._add_token('@UNK', weight_type=\"torch\")\n        self._add_token('@START', weight_type='torch')\n        self._add_token('@PAD', weight_type='torch')\n        self._add_token('@END', weight_type='torch')\n        for token in unk_tokens:\n            self.vocab[\"@UNK\"][\"count\"] += self.vocab[token][\"count\"]\n            self.vocab.pop(token)\n        \n        self.indexer = []\n        for token in self.vocab:\n            self.vocab[token][\"index\"] = torch.LongTensor([len(self.indexer)])\n            self.indexer.append(token)\n        '''\n        self.torch_embedding_layer = nn.Embedding(\n            num_embeddings=len(self),\n            embedding_dim=100\n        )\n        self.torch_embedding_layer.weight.requires_grad = False\n        '''\n        \n        echo(f\"Finalizing {len(self.vocab)} tokens for vocabulary\")\n        for token in self.vocab:\n            token_dict = self.vocab[token]\n            token_glove = token_dict['glove']\n            token_fasttest = token_dict['fasttext']\n            #token_norm = self.torch_embedding_layer(token_dict['index']).view(-1).data.numpy().astype(np.float32)\n            self.vocab[token][\"embedding\"] = np.concatenate([\n                (token_glove if token_glove is not None else np.zeros(300, dtype=np.float32)),\n                (token_fasttest if token_fasttest is not None else np.zeros(300, dtype=np.float32))\n            ])\n            token_dict.pop('glove')\n            token_dict.pop('fasttext')\n        self.finalized = True\n        echo(f\"Finalized {len(self.vocab)} tokens for vocabulary\")\n        echo(f\"{len(self.no_embed_tokens)} tokens do not have pretrained embeddings.\")\n    \n    def get_token_embedding(self, token) -> (np.ndarray, torch.LongTensor):\n        if not self.finalized:\n            raise RuntimeError(\"Vocabulary must be finalized to get its embeddings\")\n        if not token in self.vocab:\n            token = '@UNK'\n        token_embedding = self.vocab[token]['embedding']\n        token_index = self.vocab[token]['index']\n        return token_embedding, token_index\n    \n    def get_embedding(self, tokens: List[str], size=185, pad=True) -> (np.ndarray, torch.Tensor):\n        '''\n        Note: resulting size of the embeddings list will be size + 2 due to appended @START and @END tokens\n        '''\n        if len(tokens) > size:\n            # raise ValueError(f\"Given more than {size} tokens when the amount of tokens were expected to be less than {size}\")\n            tokens = tokens[:size]\n        start_embedding, start_idx = self.get_token_embedding('@START')\n        start_embedding = np.expand_dims(start_embedding, axis=0)\n        pad_embedding, pad_idx = self.get_token_embedding('@PAD')\n        pad_embeddings = np.array([pad_embedding.copy() for i in range(size - len(tokens))])\n        pad_idxs = torch.LongTensor([pad_idx.clone() for i in range(size - len(tokens))])\n        end_embedding, end_idx = self.get_token_embedding('@END')\n        end_embedding = np.expand_dims(end_embedding, axis=0)\n        if pad_embeddings.size > 0:\n            start_embedding = np.concatenate([start_embedding, pad_embeddings])\n            start_idx = torch.cat([start_idx, pad_idxs])\n        if len(tokens) == 0:\n            all_embeddings = np.concatenate([start_embedding, end_embedding])\n            all_idxs = torch.cat([start_idx, end_idx])\n            return all_embeddings, all_idxs\n        embeddings, embeddings_idxs = zip(*(self.get_token_embedding(token) for token in tokens))\n        embeddings = np.array(list(embeddings))\n        embeddings_idxs = torch.LongTensor(list(embeddings_idxs))\n        all_embeddings = np.concatenate([start_embedding, embeddings, end_embedding])\n        all_idxs = torch.cat([start_idx, embeddings_idxs, end_idx])\n        return all_embeddings, all_idxs\n    \n    def embeddings_parallelize(self, part, args):\n        size, pad = args\n        return np.array([self.get_embedding(tokens, size, pad) for tokens in part])\n    \n    def get_batch_embedding(self, text_tokens: List[List[str]], size=185, pad=True) -> (np.ndarray, torch.Tensor):\n        '''\n        part_size = len(text_tokens) // partitions\n        parts = [text_tokens[i:i + part_size] for i in range(0, len(text_tokens), part_size)]\n        with mp.Pool(processes=cores) as pool:\n            pooled = pool.starmap(self.embeddings_parallelize, zip(parts, itertools.repeat((size, pad))))\n            embeddings = np.concatenate(pooled)\n            return embeddings\n        '''\n        better_size = min(max([len(tokens) for tokens in text_tokens]), size)\n        batch_embeddings, batch_idxs = zip(*(self.get_embedding(tokens, better_size, pad) for tokens in text_tokens))\n        return np.stack(list(batch_embeddings)), torch.stack(list(batch_idxs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import types\n#embedding.get_embedding = types.MethodType(GloveFastTextEmbedding.get_embedding, embedding)\n#embedding.embeddings_parallelize = types.MethodType(GloveFastTextEmbedding.embeddings_parallelize, embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicBroccoliModel(nn.Module, BaseModel):\n    def __init__(self, embeddings_size, stats_features_dim):\n        super(ToxicBroccoliModel, self).__init__()\n    \n        # Add trainable embeddings\n        self.embed = nn.Embedding(embeddings_size, 100)\n    \n        # Add entire channel dropout for embeddings\n        self.dropout = nn.Dropout2d(p=0.2)\n        \n        feature_hidden_size = 64\n        \n        # Bi-LSTM\n        self.lstm = nn.LSTM(\n            input_size=700,\n            hidden_size=feature_hidden_size * 2,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.lstm2 = nn.LSTM(\n            input_size=feature_hidden_size * 4,\n            hidden_size=feature_hidden_size,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Simple Convolution of the first Bi-LSTM layer\n        conv_channels = 64\n        self.conv = nn.Conv1d(\n            in_channels=feature_hidden_size * 4,\n            out_channels=conv_channels,\n            kernel_size=5\n        )\n        self.convmaxpool = nn.MaxPool1d(kernel_size=187) # Sentence length of 200 with start and end tags\n        self.convavgpool = nn.AvgPool1d(kernel_size=187) # Same as above\n        \n        # Standard Attention Layer\n        # As found on https://www.aclweb.org/anthology/S18-1040\n        self.attention = self.Attention(feature_hidden_size * 4)\n        \n        # Max pool and average tool for fine tuning\n        # As found on https://arxiv.org/pdf/1801.06146.pdf, section 3.3\n        self.maxpool = nn.MaxPool1d(kernel_size=187) # Sentence length of 200 with start and end tags\n        self.avgpool = nn.AvgPool1d(kernel_size=187) # Same as above\n        \n        # Dense/Fully-connected/Feed-forward layer for statistical features\n        statistical_features = stats_features_dim\n        statistical_features_output = 64\n        self.sff = nn.Linear(\n            in_features=statistical_features,\n            out_features=statistical_features_output,\n            bias=False\n        )\n        nn.init.xavier_uniform_(self.sff.weight)\n        self.sfbn = nn.BatchNorm1d(\n            num_features=statistical_features_output\n        )\n        self.sfact = nn.Tanh()\n        \n        # Dense/Fully-connected/Feed-forward layer\n        firstout = 128\n        lastout = 1\n        self.ff1 = nn.Linear(\n            #in_features=feature_hidden_size * 2,\n            #in_features=feature_hidden_size * 2 + statistical_features_output,\n            #in_features=feature_hidden_size * 2 + conv_channels * 2, # If using CNN in combination with LSTM\n            #in_features=feature_hidden_size * 6, # Both attention and lstm output, or max + avg pool + lstm output\n            in_features=feature_hidden_size * 10 + conv_channels * 2 + statistical_features_output, # Everything\n            out_features=firstout,\n            bias=False\n        )\n        self.ff1act = nn.PReLU()\n        self.ffdropout = nn.Dropout(p=0.5)\n        self.ffbn = nn.BatchNorm1d(\n            num_features=firstout\n        )\n        self.ff2 = nn.Linear(\n            in_features=firstout,\n            out_features=lastout\n        )\n        nn.init.xavier_uniform_(self.ff2.weight)\n        self.act = nn.Sigmoid()\n        \n        # Custom differentiable ROCAUC Loss\n        # self.loss = self.ROCAUC()\n        \n        # BCELoss\n        self.loss = self.BCELoss()\n        \n        # Optimizer\n        self.optim = optim.Adam(\n            params=self.parameters(),\n            lr=1e-3\n        )\n        \n    def Attention(self, hidden_size):\n        self.attention_weights = nn.Linear(\n            in_features=hidden_size,\n            out_features=1\n        )\n        nn.init.xavier_uniform_(self.attention_weights.weight)\n        self.attention_softmax = nn.Softmax(dim=1)\n        \n        # hidden_states - [batch_size, sequence_length, hidden_size * 2] for Bi-LSTM\n        def attention(hidden_states, get_layers=False):\n            e = self.attention_weights(hidden_states) # [batch_size, sequence_length, 1]\n            w = self.attention_softmax(e) # [batch_size, sequence_length, 1]\n            r = torch.squeeze(torch.matmul(torch.unsqueeze(w, dim=2), torch.unsqueeze(hidden_states, dim=2)), dim=2) # [batch_size, sequence_length, hidden_size * 2]\n            r = torch.transpose(r, 1, 2) # [batch_size, hidden_size * 2, sequence_length]\n            s = torch.sum(r, dim=2) # [batch_size, hidden_size]\n            if get_layers:\n                return s, r\n            return s, None\n        return attention\n    \n    def ROCAUC(self, gamma=0.2, p=3):\n        '''\n        Differentiable ROCAUC as a loss function (minimize the loss)\n        Copied from tflearn.objectives.roc_auc_score\n        \n        Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n        Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n        Refer to equations (7) and (8)\n        '''\n        def loss(pred: torch.Tensor, gold: torch.Tensor):\n            gold = gold.byte()\n            pos = torch.masked_select(pred, gold)\n            neg = torch.masked_select(pred, ~gold)\n            \n            pos = torch.unsqueeze(pos, 0)\n            neg = torch.unsqueeze(neg, 1)\n            \n            diff = torch.zeros((pos * neg).shape).cuda() + pos - neg - gamma\n            diff = diff[diff < 0]\n            \n            return torch.sum(torch.pow(torch.neg(diff), p))\n        return loss\n    \n    def BCELoss(self):\n        loss_fn = nn.BCELoss()\n        def loss(pred: torch.Tensor, gold: torch.Tensor):\n            gold = gold.float()\n            return loss_fn(pred, gold)\n        return loss\n\n    def spatial_dropout(self, x):\n        v = torch.transpose(x, 1, 2)\n        v = torch.unsqueeze(v, dim=2)\n        v = self.dropout(v)\n        v = torch.squeeze(v, dim=2)\n        v = torch.transpose(v, 1, 2)\n        return v\n    \n    def sf_layer(self, sf, get_layers=False):\n        sfo = self.sff(sf)\n        #sfo = self.sfbn(sfo)\n        sf = self.sfact(sfo)\n        if get_layers:\n            return sf, sfo\n        return sf, None\n    \n    def forward(self, x, idxs, sf, get_layers=False):\n        '''\n        @param x - features of dimension [batch_size, tokens_length, embedding_size]\n        @param idxs - indices for tokens in the sentence to get their trainable embeddings [batch_size, tokens_length, 1]\n        @param sf - statistical features of dimension [batch_size, statistical_features]\n        @return prediction - probability of positive target\n        '''\n        emb = self.embed(idxs) # [batch_size, sequence_length, trainable_embedding_size]\n        x = torch.cat([x, emb], 2) # [batch_size, sequence_length, embedding_size]\n        \n        v = self.spatial_dropout(x) # [batch_size, sequence_length, embedding_size]\n        v1, _ = self.lstm(v) # v1 - [batch_size, sequence_length, 2 * hidden_size]\n        \n        v2, (hn2, cn2) = self.lstm2(v1) # v2 - [batch_size, sequence_length, 4 * hidden_size], hn - [2, batch_size, 2 * hidden_size]\n        hnf = torch.transpose(hn2, 0, 1).flatten(start_dim=1) # [batch_size, 4 * hidden_size]\n        sfo, sflayer = self.sf_layer(sf, get_layers) # [batch_size, statistical_features_output]\n        \n        # full = torch.cat([hnf, sfo], dim=1) # [batch_size, 2 * hidden_sizes + statistical_features_output]\n        \n        conv = self.conv(torch.transpose(v1, 1, 2))\n        cmaxv = torch.squeeze(self.convmaxpool(conv), dim=2)\n        cavgv = torch.squeeze(self.convavgpool(conv), dim=2)\n        # full = torch.cat([hnf, cmaxv, cavgv], dim=1) # [batch_size, 2 * hidden_sizes + 2 * conv_channels]\n        \n        attn, attn_layer = self.attention(v1, get_layers) # [batch_size, 4 * hidden_size]\n        # full = torch.cat([hnf, attn], dim=1) # [batch_size, 6 * hidden_size]\n        \n        maxv = torch.squeeze(self.maxpool(torch.transpose(v2, 1, 2)), dim=2) # [batch_size, 2 * hidden_state]\n        avgv = torch.squeeze(self.avgpool(torch.transpose(v2, 1, 2)), dim=2) # [batch_size, 2 * hidden_state]\n        # full = torch.cat([hnf, maxv, avgv], dim=1) # [batch_size, 6 * hidden_size]\n        \n        full = torch.cat([hnf, cmaxv, cavgv, attn, maxv, avgv, sfo], dim=1) # [batch_size, 10 * hidden_size + 2 * conv_channels + statistical_features_output]\n        \n        ffo1 = self.ff1(full) # [batch_size, 128]\n        ffo1 = self.ffdropout(ffo1) # [batch_size, 128]\n        ffo1 = self.ffbn(ffo1) # [batch_size, 128]\n        ffo2 = self.ff2(ffo1) # [batch_size, 1]\n        p = self.act(ffo2) # [batch_size, 1]\n        p = torch.squeeze(p.transpose(0, 1)) # [batch_size]\n        \n        if get_layers:\n            layers = []\n            layers.append(x)\n            layers.append(v1)\n            layers.append(v2)\n            layers.append(conv)\n            layers.append(cmaxv)\n            layers.append(cavgv)\n            layers.append(attn_layer)\n            layers.append(maxv)\n            layers.append(avgv)\n            layers.append(sflayer)\n            layers.append(ffo1)\n            return p, layers\n        return p, None\n    \n    def _predict(self, embed: np.ndarray, idxs: torch.Tensor, stats_feat: np.ndarray, get_layers=False) -> torch.Tensor:\n        self.zero_grad()\n        x = torch.unsqueeze(torch.FloatTensor(embed), 0).cuda()\n        sf = torch.unsqueeze(torch.FloatTensor(stats_feat), 0).cuda()\n        idxs = torch.unsqueeze(idxs, 0).cuda()\n        p, layers = self.forward(x, idxs, sf, get_layers)\n        if get_layers:\n            for idx in range(len(layers)):\n                layers[idx] = torch.squeeze(layers[idx], 0)\n            return p, layers\n        return p\n    \n    def _get_batch_predict(self, batch_embed: np.ndarray, batch_idxs: torch.Tensor, batch_stats_feat: np.ndarray) -> torch.Tensor:\n        self.zero_grad()\n        x = torch.FloatTensor(batch_embed).cuda()\n        sf = torch.FloatTensor(batch_stats_feat).cuda()\n        idxs = batch_idxs.cuda()\n        p, _ = self.forward(x, idxs, sf)\n        return p\n        \n    def predict(self, embed: np.ndarray, idxs: torch.Tensor, stats_feat: np.ndarray, get_layers=False) -> np.float32:\n        p, layers = self._predict(embed, idxs, stats_feat, get_layers)\n        p = p.cpu().detach().numpy()\n        if get_layers:\n            for idx in range(len(layers)):\n                layers[idx] = layers[idx].cpu().detach().numpy()\n            return p, layers\n        return p\n    \n    def get_batch_predict(self, batch_embed: np.ndarray, batch_idxs: torch.Tensor, batch_stats_feat: np.ndarray) -> np.ndarray:\n        p = self._get_batch_predict(batch_embed, batch_idxs, batch_stats_feat)\n        p = p.cpu().detach().numpy()\n        return p\n    \n    def train_model(self, batch: np.ndarray, batch_idxs: torch.Tensor, batch_stats_feat, gold: np.ndarray) -> np.float32:\n        self.train()\n        p = self._get_batch_predict(batch, batch_idxs, batch_stats_feat)\n        gold = torch.tensor(gold).cuda()\n        loss = self.loss(p, gold).cuda()\n        loss.backward()\n        self.optim.step()\n        return loss.float().cpu().detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import types\n#model.forward = types.MethodType(ToxicBroccoliModel.forward, model)\n#embedding.embeddings_parallelize = types.MethodType(GloveFastTextEmbedding.embeddings_parallelize, embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'] = train['target'].apply(lambda x: 1 if x >= 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_values = []\n# Parallelize this\n@iterate_counter\ndef preprocess_row(row):\n    preprocessor = ToxicBroccoliPreprocess()\n    text, values = preprocessor.get_preprocess(row['comment_text'])\n    preprocessed_values.append({'id': row['id'], **values})\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\necho(\"Preprocessing training data...\")\ntrain['processed_text'] = train.apply(preprocess_row, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\necho(\"Preprocessing testing data...\")\ntest['processed_text'] = test.apply(preprocess_row, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ToxicBroccoliPreprocess()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_values = pd.DataFrame(preprocessed_values, dtype=np.float32).set_index('id')\npreprocessed_values.index = preprocessed_values.index.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_values.to_csv('preprocessed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_features_dim = preprocessed_values.shape[1]\necho(f\"Number of statistical features: {stats_features_dim}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = SimpleTweetTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding = GloveFastTextEmbedding(ToxicBroccoliPreprocess(), 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"possible_token_lengths = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor idx, row in train.iterrows():\n    comment = row['processed_text']\n    tokens = tokenizer.get_tokens(comment)\n    if len(tokens) == 0:\n        echo(f\"Found empty sentence: {comment}\")\n        echo(f\"Original sentence: {row['comment_text']}\")\n    possible_token_lengths.append(len(tokens))\n    for token in tokens:\n        embedding.add_token(token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor idx, row in test.iterrows():\n    comment = row['processed_text']\n    tokens = tokenizer.get_tokens(comment)\n    if len(tokens) == 0:\n        echo(f\"Found empty sentence: {comment}\")\n        echo(f\"Original sentence: {row['comment_text']}\")\n    possible_token_lengths.append(len(tokens))\n    for token in tokens:\n        embedding.add_token(token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding.build_vocab()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_occurrences = [v['count'] for v in embedding.vocab.values()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(word_occurrences, [1, 5, 40, 44, 45, 55, 75])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding.finalize()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"echo(f\"Has no pretrained embeddings: {embedding.no_embed_tokens}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(possible_token_lengths, [50, 90, 95, 99, 100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"echo(f\"Percent of coverage: {1 - len(embedding.no_embed_tokens) / len(embedding)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = train[train['label'] == 1]\nneg_train = train[train['label'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation ROC AUC (thanks to Google's Benchmark model at https://www.kaggle.com/dborkan/benchmark-kernel/notebook)"},{"metadata":{"trusted":true},"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\nMODEL_NAME = 'prediction'\nTOXICITY_COLUMN = 'label'\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_metrics(validate_df):\n    # List all identities\n    identity_columns = [\n        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n    for col in identity_columns:\n        validate_df[col] = np.where(validate_df[col] >= 0.5, True, False)\n    \n    bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n    display(HTML(bias_metrics_df.to_html()))\n    echo(f\"Current final ROCAUC: {get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_model(model, df, get_loss=False):\n    model.eval()\n    batch_size = 512\n    preds = []\n    loss = 0\n    echo(f\"Predicting...\")\n    for i in range(0, df.shape[0], batch_size):\n        batch = df.iloc[i:i + batch_size]\n        \n        '''\n        batch_indices = {}\n        batch_tokens = []\n        for index, row in batch.iterrows():\n            segmented_tokens = tokenizer.get_segmentized_tokens(row['processed_text'], 65)\n            batch_indices[index] = [len(batch_indices) + j for j in range(len(segmented_tokens))]\n            batch_tokens.extend(segmented_tokens)\n        batch_embeddings = embedding.get_batch_embedding(batch_tokens, size=65)\n        '''\n\n        batch_tokens = tokenizer.get_batch_tokens(batch['processed_text'].values)\n        batch_embeddings, batch_idxs = embedding.get_batch_embedding(batch_tokens)\n        batch_statistical_features = preprocessed_values.loc[batch['id'].values].values\n\n        p = model.get_batch_predict(batch_embeddings, batch_idxs, batch_statistical_features)\n        if get_loss:\n            if not 'label' in batch.columns:\n                raise RuntimeError(\"Cannot get loss without 'label' gold column\")\n            loss += model.loss(torch.tensor(p).cuda(), torch.tensor(batch['label'].values).cuda()).cpu().detach().numpy()\n        #for index, row in batch.iterrows():\n        for j in range(batch.shape[0]):\n            '''\n            p_row = p[batch_indices[index]]\n            p_row = np.sum(p_row) / p_row.shape[0]\n            '''\n            preds.append({\n                'id': batch.iloc[j]['id'], #row['id'],\n                'prediction': p[j] # p_row\n            })\n        if i % 40960 == 0 and i > 0:\n            echo(f\"Predicted {i} samples...\")\n    preds = pd.DataFrame(preds).set_index(\"id\")\n    return preds, loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, pos, neg):\n    eval_df = pd.concat([pos, neg])\n    eval_df = eval_df.reindex(np.random.permutation(eval_df.index))\n    \n    pred_df, loss = predict_model(model, eval_df, get_loss=True)\n    if pred_df.shape[0] != eval_df.shape[0]:\n        raise RuntimeError('eval_df and pred_df has different number of rows, indicating data has been lost')\n    df = eval_df.join(pred_df, on=\"id\", sort=True)\n    \n    true_pos = 0\n    true_neg = 0\n    false_neg = 0\n    false_pos = 0\n    for index, row in df.iterrows():\n        if row['label'] == 1:\n            if row['prediction'] >= 0.5:\n                true_pos += 1\n            else:\n                false_neg += 1\n        else:\n            if row['prediction'] >= 0.5:\n                false_pos += 1\n            else:\n                true_neg += 1\n    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n    accuracy = (true_pos + true_neg) / df.shape[0]\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    compute_metrics(df)\n    echo(f\"--- Evaluation on {eval_df.shape[0]} samples ---\")\n    echo(f\"Accuracy: {accuracy}\")\n    echo(f\"Precision: {precision}\")\n    echo(f\"Recall: {recall}\")\n    echo(f\"F1: {f1}\")\n    echo(f\"Loss: {loss}\")\n    echo(\"----------------------------------\")\n    return (accuracy, f1, loss, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, pos, neg, eval_per=None, eval_pos=None, eval_neg=None):\n    batch_size = 512\n    df = pd.concat([neg, pos])\n    df = df.reindex(np.random.permutation(df.index))\n    total_loss = 0\n    prev_loss = 0\n    for i in range(0, df.shape[0], batch_size):\n        batch = df.iloc[i:i + batch_size]\n        \n        '''\n        gold = []\n        batch_tokens = []\n        for index, row in batch.iterrows():\n            segmentized_tokens = tokenizer.get_segmentized_tokens(row['processed_text'], 65)\n            gold.extend([row['label'] for k in range(len(segmentized_tokens))])\n            batch_tokens.extend(segmentized_tokens)\n            \n        gold = np.asarray(gold, dtype=np.float32)\n        batch_embeddings = embedding.get_batch_embedding(batch_tokens, size=65)\n        loss = model.train(batch_embeddings, gold) / gold.shape[0]\n        '''\n        \n        gold = batch['label'].values\n        batch_tokens = tokenizer.get_batch_tokens(batch['processed_text'].values)\n        batch_embeddings, batch_idxs = embedding.get_batch_embedding(batch_tokens)\n        batch_statistical_features = preprocessed_values.loc[batch['id'].values].values\n        loss = model.train_model(batch_embeddings, batch_idxs, batch_statistical_features, gold)\n\n        total_loss += loss\n        if i % 163840 == 0 and i > 0:\n            echo(f\"Loss on {i} samples: {total_loss}\")\n            echo(f\"Change on loss: {total_loss - prev_loss}\")\n            prev_loss = float(total_loss)\n        if eval_per is not None and i % eval_per == 0 and i > 0:\n            eval_model(model, eval_pos, eval_neg)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_model(pos, neg):\n    pos_train, pos_test = model_selection.train_test_split(pos, test_size=0.2)\n    neg_train, neg_test = model_selection.train_test_split(neg, test_size=0.2)\n    \n    epochs = 2\n    model = ToxicBroccoliModel(len(embedding), stats_features_dim).cuda()\n    for i in range(epochs):\n        echo(f\"--- Epoch {i} ---\")\n        train_model(model, pos_train, neg_train, eval_per=None, eval_pos=pos_test, eval_neg=neg_test)\n        accuracy, f1, loss, df = eval_model(model, pos_test, neg_test)\n        df.set_index(\"id\").to_csv(f\"epoch_{i}_validation_output.csv\")\n        torch.save(model.state_dict(), f'./toxic-broccoli-lstm-validation-epoch-{i}.pt')\n        echo(f\"-----------------\")\n    '''\n    from sklearn.model_selection import KFold\n    folds = 3\n    epochs = 3\n    negkf = KFold(n_splits=folds).split(neg)\n    poskf = KFold(n_splits=folds).split(pos)\n    losses = [[] for i in range(epochs)]\n    accuracies = [[] for i in range(epochs)]\n    f1s = [[] for i in range(epochs)]\n    for f in range(folds):\n        echo(f\"--- Fold {f} ---\")\n        pos_train, pos_test = next(poskf)\n        neg_train, neg_test = next(negkf)\n        pos_train = pos.iloc[pos_train]\n        pos_test = pos.iloc[pos_test]\n        neg_train = neg.iloc[neg_train]\n        neg_test = neg.iloc[neg_test]\n        model = ToxicBroccoliModel().cuda()\n        for i in range(epochs):\n            loss = train_model(model, pos_train, neg_train, eval_per=100000, eval_pos=pos_test.sample, eval_neg=neg_test.sample)\n            echo(f\"--- Epoch {i} ---\")\n            accuracy, f1 = eval_model(model, pos_test, neg_test)\n            losses[i].append(loss)\n            accuracies[i].append(accuracy)\n            f1s[i].append(f1)\n            echo(f\"-----------------\")\n        echo(f\"----------------\")\n    for i in range(epochs):\n        echo(f\"Epoch {i} loss: {sum(losses[i]) / folds}\")\n        echo(f\"Epoch {i} accuracy: {sum(accuracies[i]) / folds}\")\n        echo(f\"Epoch {i} f1: {sum(f1s[i]) / folds}\")\n    return (losses, accuracies, f1s)\n    '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nvalidate_model(pos_train, neg_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ToxicBroccoliModel(len(embedding), stats_features_dim).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nepoch = 2\nfor i in range(epoch):\n    echo(f\"------ EPOCH {i} ------\")\n    train_model(model, pos_train, neg_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntraindf, _ = predict_model(model, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.to_csv('train_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf, _ = predict_model(model, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"echo(\"Model's state_dict:\")\necho(model)\necho(\"\")\necho(\"Optimizer's state_dict:\")\necho(model.optim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), './toxic-broccoli-lstm-preprocessed-all.pt')\ntorch.save(model.optim.state_dict(), './toxic-broccoli-lstm-preprocessed-all-optim.pt')\necho(os.listdir(\"./\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('embedding_vocab_weights.pickle', 'wb') as f:\n    pickle.dump(embedding.vocab, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('embedding_vocab.pickle', 'wb') as f:\n    pickle.dump(embedding.indexer, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndef attn_transfer(example_text):\n    model.eval()\n    preprocessed_text, pv = preprocessor.get_preprocess(example_text)\n    example_tokens = tokenizer.get_tokens(preprocessed_text)\n    example_embed, example_embed_idx = embedding.get_embedding(example_tokens)\n    plt.rcParams['figure.figsize'] = (20, len(example_tokens))\n    echo(preprocessed_text)\n    echo(pv)\n    p, layers = model.predict(example_embed, example_embed_idx, np.array([*pv.values()]), get_layers=True)\n    echo('')\n    echo(\"Embedding layer\")\n    pad_length = min(5, 187 - len(example_tokens) + 1)\n    tokens = [\"@PAD\" for i in range(pad_length - 1)] + example_tokens + ['@END']\n    plt.imshow(np.power(layers[0][-len(example_tokens) - pad_length:], 2), interpolation='bicubic', aspect=10)\n    plt.yticks([*range(len(tokens))], tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Embedding Features\")\n    plt.show()\n    echo(\"First LSTM layer\")\n    plt.imshow(np.power(layers[1][-len(example_tokens) - pad_length:], 2), interpolation='bicubic', aspect=2.5)\n    plt.yticks([*range(len(tokens))], tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    sns.heatmap(layers[1][-len(example_tokens) - pad_length:] * np.abs(layers[1][-len(example_tokens) - pad_length:]),\n                vmin=-0.5, vmax=0.5, center=0, yticklabels=tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    echo(\"Attention layer summed\")\n    plt.imshow(np.power(np.expand_dims(np.sum(layers[6], axis=1), 0), 2), interpolation='bicubic', aspect=5)\n    plt.ylabel(\"Totaled Weight\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    echo(\"Attention layer\")\n    plt.imshow(np.power(layers[6].transpose()[-len(example_tokens) - pad_length:], 2), interpolation='bicubic', aspect=5)\n    plt.yticks([*range(len(tokens))], tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    attn_norm = layers[6].transpose()[-len(example_tokens) - pad_length:] * np.abs(layers[6].transpose()[-len(example_tokens) - pad_length:])\n    sns.heatmap(attn_norm, vmin=np.amin(attn_norm), vmax=np.amax(attn_norm), center=0, yticklabels=tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    echo(\"Second LSTM layer\")\n    plt.imshow(np.power(layers[2][-len(example_tokens) - pad_length:], 2), interpolation='bicubic', aspect=2.5)\n    plt.yticks([*range(len(tokens))], tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    sns.heatmap(layers[2][-len(example_tokens) - pad_length:] * np.abs(layers[2][-len(example_tokens) - pad_length:]),\n                vmin=-0.5, vmax=0.5, center=0, yticklabels=tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    echo(\"Second LSTM maxpool and avgpool layer\")\n    plt.imshow(np.power(np.expand_dims(layers[7], 0), 2), interpolation='bicubic', aspect=2.5)\n    plt.ylabel(\"Pooled Max\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    plt.imshow(np.power(np.expand_dims(layers[8], 0), 2), interpolation='bicubic', aspect=2.5)\n    plt.ylabel(\"Pooled Average\")\n    plt.xlabel(\"Hidden State Features\")\n    plt.show()\n    echo(\"Convolution layer (for the first LSTM)\")\n    plt.imshow(np.power(layers[3].transpose()[-len(example_tokens) - pad_length:], 2), interpolation='bicubic', aspect=1)\n    plt.yticks([*range(len(tokens))], tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Channels\")\n    plt.show()\n    sns.heatmap(layers[3].transpose()[-len(example_tokens) - pad_length:] * np.abs(layers[3].transpose()[-len(example_tokens) - pad_length:]),\n                vmin=-5, vmax=5, center=0, yticklabels=tokens)\n    plt.ylabel(\"Tokens\")\n    plt.xlabel(\"Channels\")\n    plt.show()\n    echo(\"Convolution maxpool and avgpool layer\")\n    plt.imshow(np.power(np.expand_dims(layers[4], 0), 2), interpolation='bicubic', aspect=1)\n    plt.ylabel(\"Pooled Max\")\n    plt.xlabel(\"Channels Features\")\n    plt.show()\n    plt.imshow(np.power(np.expand_dims(layers[5], 0), 2), interpolation='bicubic', aspect=1)\n    plt.ylabel(\"Pooled Average\")\n    plt.xlabel(\"Channels\")\n    plt.show()\n    echo(\"Statistical features layer\")\n    plt.imshow(np.power(np.expand_dims(layers[9], 0), 2), interpolation='bicubic', aspect=1)\n    plt.ylabel(\"Activation Weights\")\n    plt.xlabel(\"Hidden Features\")\n    plt.show()\n    echo(\"Feed forward layer\")\n    plt.imshow(np.power(np.expand_dims(layers[10], 0), 2), interpolation='bicubic', aspect=2.5)\n    plt.ylabel(\"Activation Weights\")\n    plt.xlabel(\"Hidden Features\")\n    plt.show()\n    print(f\"Toxicity Probability: {p}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following examples display the attention weights outputs of the model. Warning: the sentences can be considered vulgar, profane, and offensive."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct Positive\nattn_transfer('i wish i was funny like a black person.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# False Negative\nattn_transfer('The Troll is strong with this one.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# False Positive\nattn_transfer('Gary:  is it sexist to ask people to vote for you because you are a woman?   Is it racist to ask for special treatment because you are black?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct negative\nattn_transfer('Nice! Loved your \"How to Win the Lottery\" talk at XOXO a few years ago.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Very borderline examples\nattn_transfer('Be nice to see the harassment comments, or just plain hateful ones, go away')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attn_transfer('Wouldn\\'t be the first criminal elected or re-elected in the US.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}