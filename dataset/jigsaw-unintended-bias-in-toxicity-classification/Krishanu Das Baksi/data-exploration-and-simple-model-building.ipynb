{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## This notebook is primarily for exploration purposes and training simple predictive models on the data. \n### PS: You wont find any high end DNNs like LSTMs or GRUs in this notebook\n\nThis notebook is for people who are just starting out in data sciences and text mining. Although Deep Learning models such as LSTMs are currently dominating the NLP landscape, it might be difficult for beginners to get a grasp over them without understanding the basics of text mining, also beginners wont totally appreciate the modern NN based architectures if they are not fully aware of the different classical machine learning methodologies and their pitfalls.\n\nIn this notebook, I am going to explain in simple steps how to go about exploring text data and how to make simple models like simple Logistic regression, SVM and Naiive Bayes Classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we need to load the data into a dataframe\n\ndf = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets also have a look at the test data\n\ntdf = pd.read_csv(\"../input/test.csv\")\nprint(tdf.head())\ndel(tdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the test data is composed of only the comments, and our algorithm has to predict the toxicity score of the text between 0 and 1, o being the lowest and 1 being the highest."},{"metadata":{},"cell_type":"markdown","source":"### Lets have a look at some random comment texts and their labels to better understand the bias mentioned in the competition description"},{"metadata":{},"cell_type":"markdown","source":"### Neutral/ slightly toxic comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's first have a look at some of the comments whose scores were above 0.0\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] > 0.], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non toxic comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's have a look at completely non toxic comments\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] == 0.], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Severely toxic comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets have a look at very toxic comments: target > 0.75\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] >= .75], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now lets have a look at the distribution of the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20, 5))\nplt.hist(df['target'], bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the majority of the comments are non toxic (score == 0.0). Therefore, we sould have a look at the toxic ones separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.hist(df[df['target'] > 0.0]['target'], bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are very few comments with high scores, therefore, we should count all the comments with low scores (e.g., target > 0.10) as toxic"},{"metadata":{},"cell_type":"markdown","source":"## Making the model\nNow we should start making a model to predict the target in unseen texts"},{"metadata":{},"cell_type":"markdown","source":"#### Training and Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf = df.loc[:30000, [\"comment_text\", \"target\"]]\ntdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(tdf[\"comment_text\"], tdf[\"target\"], test_size = .10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n\n#### First I am going to import Count Vectorizer and TFIDF Vectorizers which are used to convert the texts into feature vector forms which can be used as by the machine learning algorithm. \nFor more information please visit https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\nand https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n\nThese are used to convert the text data from strings which cannot be used directly by the machine learning algorithms into floats which can be used as features.\nIn short these algorithms count the occurence of every word token in the texts and convert the word scores for each text into a float. For more details refer to the abovementioned links."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nNow I will build a simple model for binary text classification. The two classes being toxic and non toxic.\nI will explore different classic machine learning algorithms for this purpose. \nHowever I wont go into RNNs in this notebook. Very soon I will release another notebook with LSTMs, word embeddings etc."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression with Count Vectoriser"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvect = CountVectorizer(min_df = 0.1, ngram_range=(1, 3), analyzer=\"word\").fit(X_train)\nX_trcv = cvect.transform(X_train)\nX_tscv = cvect.transform(X_test)\n\n# In order to convert the coninuous values of the target to binary, as logistic regression can accept only binary values (0 or 1) as the target values\n# Here we choose 0.1 as a cutoff as we want to classify even slightly toxic comments as toxic\ny_train_lg = np.array(y_train > 0.1, dtype=np.float)\ny_test_lg = np.array(y_test > 0.1, dtype=np.float)\n\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":34,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"Training Accuracy: 0.753\nTesting Accuracy: 0.7677440853048983\nTest Precision: 0.5263157894736842\nTest Recall: 0.02861230329041488\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Classifier\nWe can see that the accuracy scores are not that bad. However, the precision and recall scores are just unacceptable. This is because of the imbalanced targets, as most of the targets have label 0 and few have label 1. So even a dumb classifier which always predicts the most common class would give a respectable accuracy score. So we need to compare our classifier's performance with once such most-common-class-classifier. Lets see how to do that..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\ndclf = DummyClassifier(strategy=\"most_frequent\").fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(dclf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(dclf.score(X_tscv, y_test_lg)))\npredicted = dclf.predict(X_tscv)\n\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":33,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.7527777777777778\nTesting Accuracy: 0.7670776407864045\nTest Precision: 0.0\nTest Recall: 0.0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Performance:\nTherefore, we see that our classifier is not much better than a simple baseline model which just predicts all outputs to be the most frequent class.\nTherefore, we need better models. Hence we will explore other models better suited for text classification purposes viz Naive Bayes Classifier and Support Vector Machines"},{"metadata":{},"cell_type":"markdown","source":"#### Bernoulli Naive Bayes using Count Vectors as features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n\nclf = BernoulliNB().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":35,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.6543703703703704\nTesting Accuracy: 0.659780073308897\nTest Precision: 0.29411764705882354\nTest Recall: 0.3290414878397711\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Improvement\nWe can see that the precision and recall scores have gone up, but the total accuracy score has gone down.\nNext we will run:\n#### Multinomial NB with TFIDF and Count Vector features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":36,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.7500370370370371\nTesting Accuracy: 0.7637454181939354\nTest Precision: 0.4603174603174603\nTest Recall: 0.08297567954220315\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The precision and recall scores again went down. But the accuracy increased and became equal to the Dummy classifier\nLets use TFIDF features now"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfvect = TfidfVectorizer().fit(X_train)\n\nX_trtf = tfvect.transform(X_train)\nX_tstf = tfvect.transform(X_test)\n\nclf = MultinomialNB().fit(X_trtf, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trtf, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tstf, y_test_lg)))\npredicted = clf.predict(X_tstf)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":38,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.7651481481481481\nTesting Accuracy: 0.7727424191936021\nTest Precision: 0.9473684210526315\nTest Recall: 0.02575107296137339\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Improvement:\nIn this model the precision and accuracy have improved, however, the recall is very low. Lets now play around with SVM Models, which are also used very often in text classification"},{"metadata":{},"cell_type":"markdown","source":"#### SVM Classifier with a linear kernel and TFIDF vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf = SVC(kernel=\"linear\").fit(X_trtf, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trtf, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tstf, y_test_lg)))\npredicted = clf.predict(X_tstf)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":39,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.8606666666666667\nTesting Accuracy: 0.8153948683772076\nTest Precision: 0.7675276752767528\nTest Recall: 0.29756795422031473\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Major Improvement\nThe above classifier has major improvements in terms of test accuracy, precision and recall."},{"metadata":{},"cell_type":"markdown","source":"#### SVM Classifier with linear kernel and Count Vectors\nNow lets try the last classifier on our list. "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC(kernel=\"linear\").fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","execution_count":40,"outputs":[{"output_type":"stream","text":"Training Accuracy: 0.7527777777777778\nTesting Accuracy: 0.7670776407864045\nTest Precision: 0.0\nTest Recall: 0.0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Again the above classifier underperformed. It is almost always impossible to say which classifier will give the best results. Its better to try different classifiers and go with the one with the best performance."},{"metadata":{},"cell_type":"markdown","source":"### End\nUnfortunately I have to end this notebook here. However, if this is helpful, please comment. I will do further analysis using simple machine learning algorithms and share with you.\n\nWithin a few weeks I will publish a notebook on how to train an LSTM Classifier for this task. Stay tuned."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}