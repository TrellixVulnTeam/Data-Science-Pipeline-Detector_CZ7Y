{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')\n","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  mask |= (ar1 == a)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":4,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1804874 entries, 59848 to 6334010\nData columns (total 44 columns):\ntarget                                 float64\ncomment_text                           object\nsevere_toxicity                        float64\nobscene                                float64\nidentity_attack                        float64\ninsult                                 float64\nthreat                                 float64\nasian                                  float64\natheist                                float64\nbisexual                               float64\nblack                                  float64\nbuddhist                               float64\nchristian                              float64\nfemale                                 float64\nheterosexual                           float64\nhindu                                  float64\nhomosexual_gay_or_lesbian              float64\nintellectual_or_learning_disability    float64\njewish                                 float64\nlatino                                 float64\nmale                                   float64\nmuslim                                 float64\nother_disability                       float64\nother_gender                           float64\nother_race_or_ethnicity                float64\nother_religion                         float64\nother_sexual_orientation               float64\nphysical_disability                    float64\npsychiatric_or_mental_illness          float64\ntransgender                            float64\nwhite                                  float64\ncreated_date                           object\npublication_id                         int64\nparent_id                              float64\narticle_id                             int64\nrating                                 object\nfunny                                  int64\nwow                                    int64\nsad                                    int64\nlikes                                  int64\ndisagree                               int64\nsexual_explicit                        float64\nidentity_annotator_count               int64\ntoxicity_annotator_count               int64\ndtypes: float64(32), int64(9), object(3)\nmemory usage: 619.7+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1,300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embeddings_index[word]\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n    return embedding_matrix","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gc\n\nmaxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\ny_train = train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\n\ndel tokenizer\ngc.collect()\n\n\n","execution_count":6,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"fitting tokenizer\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = load_embeddings()\n","execution_count":7,"outputs":[{"output_type":"stream","text":"2000001it [02:15, 14708.35it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(word_index, embeddings_index)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_index\ngc.collect()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport keras.layers as L\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\ndef build_model(verbose = False, compile = True):\n    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n    embedding_layer = L.Embedding(len(word_index) + 1,\n                                300,\n                                weights=[embedding_matrix],\n                                input_length=maxlen,\n                                trainable=False)\n    x = embedding_layer(sequence_input)\n    x = L.SpatialDropout1D(0.2)(x)\n    x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n\n    att = Attention(maxlen)(x)\n    avg_pool1 = L.GlobalAveragePooling1D()(x)\n    max_pool1 = L.GlobalMaxPooling1D()(x)\n\n    x = L.concatenate([att,avg_pool1, max_pool1])\n\n    preds = L.Dense(1, activation='sigmoid')(x)\n\n\n    model = Model(sequence_input, preds)\n    if verbose:\n        model.summary()\n    if compile:\n        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n    return model\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import KFold\n\nsplits = list(KFold(n_splits=5).split(X_train,y_train))\n\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\nimport numpy as np\nBATCH_SIZE = 2048\nNUM_EPOCHS = 100\n\noof_preds = np.zeros((X_train.shape[0]))\ntest_preds = np.zeros((X_test.shape[0]))\nfor fold in [0,1,2,3,4]:\n    K.clear_session()\n    tr_ind, val_ind = splits[fold]\n    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    model = build_model()\n    model.fit(X_train[tr_ind],\n        y_train[tr_ind]>0.5,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n        callbacks = [es,ckpt])\n\n    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n    test_preds += model.predict(X_test)[:,0]\ntest_preds /= 5","execution_count":12,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 1443899 samples, validate on 360975 samples\nEpoch 1/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.1186 - acc: 0.9580 - val_loss: 0.0919 - val_acc: 0.9652\nEpoch 2/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0932 - acc: 0.9644 - val_loss: 0.0891 - val_acc: 0.9655\nEpoch 3/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0891 - acc: 0.9656 - val_loss: 0.0872 - val_acc: 0.9665\nEpoch 4/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0863 - acc: 0.9667 - val_loss: 0.1019 - val_acc: 0.9587\nEpoch 5/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0848 - acc: 0.9672 - val_loss: 0.0870 - val_acc: 0.9665\nEpoch 6/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0827 - acc: 0.9679 - val_loss: 0.0878 - val_acc: 0.9658\nEpoch 7/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0817 - acc: 0.9683 - val_loss: 0.0943 - val_acc: 0.9625\nEpoch 8/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0799 - acc: 0.9689 - val_loss: 0.0885 - val_acc: 0.9654\nEpoch 00008: early stopping\nTrain on 1443899 samples, validate on 360975 samples\nEpoch 1/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.1163 - acc: 0.9587 - val_loss: 0.0923 - val_acc: 0.9647\nEpoch 2/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0937 - acc: 0.9642 - val_loss: 0.0883 - val_acc: 0.9661\nEpoch 3/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0893 - acc: 0.9657 - val_loss: 0.0879 - val_acc: 0.9660\nEpoch 4/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0869 - acc: 0.9666 - val_loss: 0.0868 - val_acc: 0.9666\nEpoch 5/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0849 - acc: 0.9671 - val_loss: 0.0925 - val_acc: 0.9631\nEpoch 6/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0830 - acc: 0.9679 - val_loss: 0.0918 - val_acc: 0.9633\nEpoch 7/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0817 - acc: 0.9683 - val_loss: 0.0878 - val_acc: 0.9665\nEpoch 00007: early stopping\nTrain on 1443899 samples, validate on 360975 samples\nEpoch 1/100\n1443899/1443899 [==============================] - 139s 96us/step - loss: 0.1185 - acc: 0.9571 - val_loss: 0.0874 - val_acc: 0.9666\nEpoch 2/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0944 - acc: 0.9639 - val_loss: 0.0853 - val_acc: 0.9670\nEpoch 3/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0902 - acc: 0.9654 - val_loss: 0.0832 - val_acc: 0.9686\nEpoch 4/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0876 - acc: 0.9661 - val_loss: 0.0860 - val_acc: 0.9663\nEpoch 5/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0857 - acc: 0.9667 - val_loss: 0.0843 - val_acc: 0.9669\nEpoch 6/100\n1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0840 - acc: 0.9674 - val_loss: 0.0826 - val_acc: 0.9683\nEpoch 7/100\n1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0827 - acc: 0.9679 - val_loss: 0.0832 - val_acc: 0.9678\nEpoch 8/100\n1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0812 - acc: 0.9684 - val_loss: 0.0829 - val_acc: 0.9686\nEpoch 9/100\n1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0802 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9679\nEpoch 00009: early stopping\nTrain on 1443899 samples, validate on 360975 samples\nEpoch 1/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.1165 - acc: 0.9586 - val_loss: 0.1015 - val_acc: 0.9601\nEpoch 2/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0927 - acc: 0.9646 - val_loss: 0.0926 - val_acc: 0.9640\nEpoch 3/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0885 - acc: 0.9660 - val_loss: 0.0904 - val_acc: 0.9652\nEpoch 4/100\n1443899/1443899 [==============================] - 138s 96us/step - loss: 0.0860 - acc: 0.9668 - val_loss: 0.0940 - val_acc: 0.9629\nEpoch 5/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0840 - acc: 0.9674 - val_loss: 0.0915 - val_acc: 0.9639\nEpoch 6/100\n1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0823 - acc: 0.9680 - val_loss: 0.0905 - val_acc: 0.9647\nEpoch 00006: early stopping\nTrain on 1443900 samples, validate on 360974 samples\nEpoch 1/100\n1443900/1443900 [==============================] - 139s 96us/step - loss: 0.1191 - acc: 0.9585 - val_loss: 0.1016 - val_acc: 0.9609\nEpoch 2/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0935 - acc: 0.9647 - val_loss: 0.0961 - val_acc: 0.9626\nEpoch 3/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0896 - acc: 0.9658 - val_loss: 0.0989 - val_acc: 0.9607\nEpoch 4/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0875 - acc: 0.9666 - val_loss: 0.0936 - val_acc: 0.9635\nEpoch 5/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0852 - acc: 0.9673 - val_loss: 0.0958 - val_acc: 0.9620\nEpoch 6/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0838 - acc: 0.9678 - val_loss: 0.0933 - val_acc: 0.9642\nEpoch 7/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0826 - acc: 0.9683 - val_loss: 0.0936 - val_acc: 0.9633\nEpoch 8/100\n1443900/1443900 [==============================] - 138s 95us/step - loss: 0.0813 - acc: 0.9688 - val_loss: 0.0938 - val_acc: 0.9630\nEpoch 9/100\n1443900/1443900 [==============================] - 138s 96us/step - loss: 0.0805 - acc: 0.9691 - val_loss: 0.0981 - val_acc: 0.9608\nEpoch 00009: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5,oof_preds)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"0.9705507564383304"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()\n#%%\n\n","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"        id  prediction\n0  7000000    0.001450\n1  7000001    0.000077\n2  7000002    0.002509\n3  7000003    0.000207\n4  7000004    0.979345","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7000000</td>\n      <td>0.001450</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7000001</td>\n      <td>0.000077</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000002</td>\n      <td>0.002509</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7000003</td>\n      <td>0.000207</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7000004</td>\n      <td>0.979345</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}