{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n# copy our file into the working directory\nsys.path.insert(0, \"../input/pytorch-pretrained-BERT/pytorch-pretrained-BERT/pytorch-pretrained-bert/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\n\nn_seeds = 1\nn_splits = 10\nn_epochs = 15\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nENSEMBLE_START_EPOCH = 3\n\nMAX_LEN = 220\n\nEMB_DROPOUT = 0.3\nMIDDLE_DROPOUT = 0.3\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n# 分散表現がuncasedだった場合は、単語を全て小文字で扱う\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nWORK_DIR = \"../working/\"\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nimport os\n\n# 標準出力をnullに変換\n@contextmanager\ndef suppress_stdout():\n    # nullの書き込み\n    with open(os.devnull, \"w\") as devnull:\n        # 標準出力をnullに変換\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:  \n            yield\n        finally:\n            sys.stdout = old_stdout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nraw_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n#test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nAUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat']\ntrain_data = raw_train[:100000]\ntrain_text = train_data['comment_text']\ny_target = (train_data['target'] >= 0.5).astype('float32')\n#print(y_target)\ny_aux_target = (train_data[AUX_COLUMNS] >= 0.5).astype('float32')\n#x_test = test['comment_text'][:300]\n#print(y_aux_target)\n#print(len(y_target.values))\n#print(y_aux_target.shape)\n#y_target = torch.tensor(y_target.values.reshape((-1, 1)), dtype=torch.float32).cuda()\n#y_aux_target = torch.tensor(y_aux_target.values, dtype=torch.float32).cuda()\ny_label = pd.concat([y_target, y_aux_target], axis=1)\n#print(y_label)\n#print(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del raw_train, train_data, y_target, y_aux_target\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport emoji\nimport unicodedata\n\n# 特定文字の変換器を作成\nCUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\x10\": None,\n        \"\\x9d\": None,\n        \"\\xa0\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"\\uf0d8\": None,\n        \"\\u2061\": None,\n        \"‘\": \"'\",\n        \"’\": \"'\",\n        \"`\": \"'\",\n        \"“\": '\"',\n        \"”\": '\"',\n        \"«\": '\"',\n        \"»\": '\"',\n        \"ɢ\": \"G\",\n        \"ɪ\": \"I\",\n        \"ɴ\": \"N\",\n        \"ʀ\": \"R\",\n        \"ʏ\": \"Y\",\n        \"ʙ\": \"B\",\n        \"ʜ\": \"H\",\n        \"ʟ\": \"L\",\n        \"ғ\": \"F\",\n        \"ᴀ\": \"A\",\n        \"ᴄ\": \"C\",\n        \"ᴅ\": \"D\",\n        \"ᴇ\": \"E\",\n        \"ᴊ\": \"J\",\n        \"ᴋ\": \"K\",\n        \"ᴍ\": \"M\",\n        \"Μ\": \"M\",\n        \"ᴏ\": \"O\",\n        \"ᴘ\": \"P\",\n        \"ᴛ\": \"T\",\n        \"ᴜ\": \"U\",\n        \"ᴡ\": \"W\",\n        \"ᴠ\": \"V\",\n        \"ĸ\": \"K\",\n        \"в\": \"B\",\n        \"м\": \"M\",\n        \"н\": \"H\",\n        \"т\": \"T\",\n        \"ѕ\": \"S\",\n        \"—\": \"-\",\n        \"–\": \"-\",\n    }\n)\n\n# 下品な単語の規制後と規制前の単語の辞書を作成\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\n# 下品な単語の規制部分の特殊文字を無効化し、大文字小文字を区別しない判別器を作成\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\n# 空白の判別器を作成\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\n# Unicodeから異体字セレクタ（直前の文字の種類を変えるコード）をkeyとする辞書を作成\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\n# 特定のUnicode（学習で使えない文字？）を別の文字で置換する辞書作成\nHEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n# 絵文字の判別器を作成\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\n# 絵文字のエイリアスを文章化\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\n# 文章内の絵文字をエイリアスの文章に変換\ndef my_demojize(string: str) -> str:\n    # sub関数でmatchした単語を、それを含む絵文字のエイリアスの文章に置換\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n\n    # テストデータの文章中の絵文字を、絵文字のエイリアスの文章に置換し、そこから異体字セレクタを削除\n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\n# 文章中の特定の特殊単語を解析用に変換\ndef normalize(text: str) -> str:\n    #text_len = len(text)\n    #print(text)\n    # 文章内の絵文字をエイリアスの文章に変換\n    text = my_demojize(text)\n\n    # 空白を全てスペースに変換\n    text = RE_SPACE.sub(\" \", text)\n    # 文字表現（Unicode）の規格を統一化\n    text = unicodedata.normalize(\"NFKD\", text)\n    # 文章中のTABLE_keyをvalueに変換\n    text = text.translate(TABLE)\n    # 連続した空白を一つのスペースに変換し、文章の両端の空白を削除\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n\n    # 下品な単語の規制された部分を修復\n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n        \n    #if text_len != len(text):\n        #print(text + \"\\n\")\n    \n    return text\n\n# 文章の特殊単語を一般単語の変換\ntrain_text = train_text.apply(lambda x: normalize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del UNICODE_EMOJI_MY, EMOJI_REGEXP, TABLE, HEBREW_TABLE, ARABIC_TABLE, CHINESE_TABLE, KANJI_TABLE, HIRAGANA_TABLE, KATAKANA_TABLE, NMS_TABLE, RE_MULTI_SPACE, RE_SPACE, REGEX_REPLACER, WORDS_REPLACER, CUSTOM_TABLE\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if \"EMJ\" in crawl_emb_dict:\n#    print(\"EMJ\")\n#if \"א\" in crawl_emb_dict:\n#    print(\"HEB\")\n#if \"ا\" in crawl_emb_dict:\n#    print(\"ARA\")\n#if \"是\" in crawl_emb_dict:\n#    print(\"CHI\")\n#if \"ッ\" in crawl_emb_dict:\n#    print(\"JAP\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '\\n', '\\r', \"'\", \"'\", 'θ', '÷', 'α', 'β', '∅', 'π', '₹', '´']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            # 特定文字の両側に空白付ける\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\nimport re\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\n# 句読点の両側にスペースを付与\ntrain_text = train_text.apply(lambda x: clean_text(x))\n\n# 文章から数字削除\ntrain_text = train_text.apply(lambda x: clean_numbers(x))\n\n# Bertの語彙を読み込み\n#with open(BERT_MODEL_PATH + 'vocab.txt', 'r') as f:\n#    raw_dict = f.readlines()\n\n# データ内の改行を削除と重複行の削除\n#crawl_emb_dict = set([t.replace('\\n', '') for t in raw_dict])\n#print(len(crawl_emb_dict))\n    \nimport joblib\n# ファイルデータ（単語の分散表現の辞書）を並列処理形式で保存\nwith open('../input/reducing-oov-of-crawl300d2m-no-appos-result/jigsaw-crawl-300d-2M.joblib', 'rb') as f:\n    crawl_emb_dict = joblib.load(f)\ncrawl_emb_dict = set(crawl_emb_dict.keys())\n#print(crawl_emb_dict)\n#for k in list(crawl_emb_dict)[:10]:\n#    print({k:crawl_emb_dict[k]})\n\n# グーグルが禁止している単語（禁句）集を取得\nwith open('../input/googleprofanitywords/google-profanity-words/google-profanity-words/profanity.js', 'r') as f:\n    p_words = f.readlines()\n    \nset_puncts = set(puncts)\n#print(set_puncts)\n\n# データ内の改行を削除と重複行の削除\np_word_set = set([t.replace('\\n', '') for t in p_words])\n#print(p_word_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del puncts, p_words#, raw_dict\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nfrom typing import Dict, List\nfrom tqdm import tqdm_notebook as tqdm\n\ndef check_coverage(texts, embeddings_word: Dict) -> List[str]:\n    known_words = []\n    unknown_words = []\n    for text in tqdm(texts):\n        text = text.split()\n        for word in text:\n            if word in embeddings_word:\n                known_words.append(word)\n                #print(word)\n            else:\n                unknown_words.append(word)\n\n    print('Found embeddings for {:.2%} of vocab'.format(float(len(set(known_words))) / (float(len(set(known_words))) + float(len(set(unknown_words))))))\n    print('Found embeddings for {:.2%} of all text'.format(float(len(known_words)) / (float(len(known_words)) + float(len(unknown_words)))))\n\n    return set(unknown_words)\n\n# ３通りのStemmer（接尾辞除去=語幹検出アルゴリズム）用の関数を呼び出し\nfrom nltk.stem import PorterStemmer\np_stemmer = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nl_stemmer = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\ns_stemmer = SnowballStemmer(\"english\")\n\nimport copy\ndef edits1(word):\n    \"\"\"\n    wordの編集距離1の単語のリストを返す\n    \"\"\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    # 単語を左右２つに分割したパターンを作成\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    # 単語から一文字を消したパターンを作成\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    # 単語から隣同士の文字を一組入れ替えたパターンを作成\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    # 単語内の一文字を別の文字（アルファベット中の全ての文字）にそれぞれ変換したパターンを作成\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    # 単語のどこかに別の文字（アルファベット中の全ての文字）を加えたパターンを作成\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef known(words, embed): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    # 単語が辞書内に入っていたら返す\n    return set(w for w in words if w in embed)\n\n# 単語の誤字脱字を復元\ndef spellcheck(word, word_rank_dict):\n    # 単語に一文字入れたり、隣同士の文字を入れ替えたりした物の内、単語辞書にある物の中から、一番文字数が少ない文字列を返す\n    return min(known(edits1(word), word_rank_dict), key=lambda word_rank_dict:word_rank_dict)\n\n\nimport unicodedata\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\",\n                 \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\",\n                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\",\n                 \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"',\n                 '“': '\"', \"£\": \"e\", '∞': 'infinity',\n                 'θ': 'theta', '÷': '/', 'α': 'alpha',\n                 '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n                 '∅': '', '³': '3', 'π': 'pi', 'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','ᴀ':'a', '😉':'wink','😂':'joy','😀':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\ndef process_stemmer(texts, embed):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            #print(word)\n            # 分散表現辞書から色々な表現で変換した文中の単語の分散表現を検索\n            if word in embed:\n                new_text += word + \" \"\n                #print(\"embed:\",word)\n                continue\n\n            # 単語を全て小文字化した物で検索\n            if word.lower() in embed:\n                new_text += word.lower() + \" \"\n                #print(\"lower:\",word)\n                continue\n\n            # 単語を全て大文字化した物で検索\n            if word.upper() in embed:\n                new_text += word.upper() + \" \"\n                #print(\"upper:\",word)\n                continue\n\n            # 単語の頭文字だけを大文字化した物で検索\n            if word.capitalize() in embed:\n                new_text += word.capitalize() + \" \"\n                #print(\"cap:\",word)\n                continue\n\n            # 特殊文字の分散表現を検索\n            corr_word = punct_mapping.get(word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",word)\n                continue\n\n            try:\n                # PorterStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = p_stemmer.stem(word)\n            except:\n                # 失敗したら、文字コードを変更して再実行\n                vector = p_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n                \n            try:\n                # LancasterStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = l_stemmer.stem(word)\n            except:\n                vector = l_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                # SnowballStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = s_stemmer.stem(word)\n            except:\n                vector = s_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            # 単語の分散表現が検索できなかった単語を記録\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef process_small_capital(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            #print(word)\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n\n            char_list = []\n            any_small_capitial = False\n            # 単語を一文字ずつ取得\n            for char in word:\n                try:\n                    # 文字に割り振られている名前（\"a\"=\"LATIN SMALL LETTER A\"）を取得\n                    uni_name = unicodedata.name(char)\n                except ValueError:\n                    continue\n\n                # 文字がラテン文字だった場合\n                if 'LATIN SMALL LETTER' or 'LATIN CAPITAL LETTER' in uni_name:\n                    # 文字に割り振られた名前の最後（文字が大文字化した物）を取得\n                    char = uni_name[-1]\n                    any_small_capitial = True\n                # キリル文字の\"ғ\"は、\"F\"に変換\n                if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n                    char = 'F'\n                    any_small_capitial = True\n\n                char_list.append(char)\n\n            # 単語内の全ての文字が、名前が割り当てられていない、ラテン文字ではなく、キリル文字の'ғ'でもない場合\n            if not any_small_capitial:\n                oov_word_set.add(word)\n                new_text += word + \" \"\n                #print(\"oov_small_cap:\",word)\n                continue\n\n            # 変換した文字を一つの単語に戻す\n            legit_word = ''.join(char_list)\n\n            if legit_word in embed:\n                new_text += legit_word + \" \"\n                #print(\"embed:\",legit_word)\n                continue\n\n            if legit_word.lower() in embed:\n                new_text += legit_word.lower() + \" \"\n                #print(\"lower:\",legit_word)\n                continue\n\n            if legit_word.upper() in embed:\n                new_text += legit_word.upper() + \" \"\n                #print(\"upper:\",legit_word)\n                continue\n\n            if legit_word.capitalize() in embed:\n                new_text += legit_word.capitalize() + \" \"\n                #print(\"cap:\",legit_word)\n                continue\n\n            corr_word = punct_mapping.get(legit_word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",legit_word)\n                continue\n\n            try:\n                vector = p_stemmer.stem(legit_word)\n            except:\n                vector = p_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n\n            try:\n                vector = l_stemmer.stem(legit_word)\n            except:\n                vector = l_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                vector = s_stemmer.stem(legit_word)\n            except:\n                vector = s_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n            \n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n\n        new_texts.append(new_text.strip())\n        \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef process_spellcheck(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n\n            try:\n                vector = spellcheck(word, embed)\n            except:\n                oov_word_set.add(word)\n                new_text += word + \" \"\n                #print(\"oov:\",word)\n                continue\n            if vector is not None:\n                new_text += vector + \" \"\n                #print(\"original:\",word)\n                #print(\"miss_sp:\",vector)\n                continue\n\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n            \n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\nfrom nltk import TweetTokenizer\n# Tweet専用の解析ツールを作成\n# reduce_len：単語の長さの標準化（短縮化）をするかどうかを設定\ntknzr = TweetTokenizer(reduce_len=True)\ndef twitter_stemmer(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n            \n            tokens = tknzr.tokenize(word)\n            if (tokens[0] == \"'\" or '\"') and len(tokens) > 1:\n                word = tokens[1]\n                #print(\"tokens_1:\")\n            else:\n                word = tokens[0]\n                #print(\"tokens_0:\")\n            #print(word)\n            # 分散表現辞書から色々な表現で変換した文中の単語の分散表現を検索\n            if word in embed:\n                new_text += word + \" \"\n                #print(\"embed:\",word)\n                continue\n\n            # 単語を全て小文字化した物で検索\n            if word.lower() in embed:\n                new_text += word.lower() + \" \"\n                #print(\"lower:\",word)\n                continue\n\n            # 単語を全て大文字化した物で検索\n            if word.upper() in embed:\n                new_text += word.upper() + \" \"\n                #print(\"upper:\",word)\n                continue\n\n            # 単語の頭文字だけを大文字化した物で検索\n            if word.capitalize() in embed:\n                new_text += word.capitalize() + \" \"\n                #print(\"cap:\",word)\n                continue\n\n            # 特殊文字の分散表現を検索\n            corr_word = punct_mapping.get(word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",word)\n                continue\n\n            try:\n                # PorterStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = p_stemmer.stem(word)\n            except:\n                # 失敗したら、文字コードを変更して再実行\n                vector = p_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n                \n            try:\n                # LancasterStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = l_stemmer.stem(word)\n            except:\n                vector = l_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                # SnowballStemmerアルゴリズムで抽出した単語の語幹で検索\n                vector = s_stemmer.stem(word)\n            except:\n                vector = s_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            # 単語の分散表現が検索できなかった単語を記録\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(\"0:None!\")\n            print(text)\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef bytes_to_unicode():\n    # ラテン文字を表すUnicodeを取得\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    # 0~255のUnicodeを取得\n    for b in range(2**8):\n        if b not in bs:\n            # 文字以外のUnicode（PCの命令コード）も取得\n            bs.append(b)\n            # ラテン文字拡張A（PCの命令コード+256に位置する文字）のUnicodeを取得\n            cs.append(2**8+n)\n            n += 1\n    # 配列内のUnicodeを単語に変換\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n# 単語の先頭文字から二文字ずつのタプルを取得\ndef get_pairs(word):\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n# ファイルからmerge文字取得\nMERGES_PATH = \"../input/transformer-tokenizers/gpt2/merges.txt\"\nbpe_data = open(MERGES_PATH, encoding='utf-8').read().split('\\n')[1:-1]\n# merge文字のfirstとsecondをタプル化\nbpe_merges = [tuple(merge.split()) for merge in bpe_data]\n# keyがmerge文字のタブル、valueが0からのidの辞書作成\nbpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\ndef bpe(token):\n    word = tuple(token)\n    # 単語の先頭文字から二文字ずつのタプルを取得\n    pairs = get_pairs(word)\n\n    if not pairs:\n        return token\n\n    while True:\n        # 単語中でマッチしたmerge文字のタプルから、idが一番小さいものを取得\n        bigram = min(pairs, key = lambda pair: bpe_ranks.get(pair, float('inf')))\n        if bigram not in bpe_ranks:\n            break\n        first, second = bigram\n        new_word = []\n        i = 0\n        while i < len(word):\n            try:\n                # i番目の文字から後ろのmerge文字のfirstのidを取得\n                j = word.index(first, i)\n                # i番目の文字からmerge文字のfirstの直前までの文字列を取得\n                new_word.extend(word[i:j])\n                i = j\n            except:\n                # merge文字のfirstが単語に含まれてない場合は、そのまま取得\n                new_word.extend(word[i:])\n                break\n\n            # merge文字のfirstとsecondの二文字が連続で続いた時も取得\n            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                new_word.append(first+second)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_word = tuple(new_word)\n        word = new_word\n        #print(\"new_word:\",new_word)\n        if len(word) == 1:\n            break\n        else:\n            pairs = get_pairs(word)\n    word = ' '.join(word)\n    return word\n\ndef merge_spellcheck(texts, embed, oov_set):\n    # key（ラテン文字とPC命令コード）、value（ラテン文字とラテン文字拡張）である辞書を取得\n    byte_encoder = bytes_to_unicode()\n    # 文章を単語（解析するパーツ）に分ける文字の判別器\n    #pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|[^('s)('t)('re)('ve)('m)('ll)('d)]+\"\"\")\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for pre_word in text:\n            if pre_word not in oov_set:\n                new_text += pre_word + \" \"\n                continue\n            #print(\"original:\",pre_word)\n            bpe_tokens = []\n            token_list = re.split(\"('s)|('t)|('re)|('ve)|('m)|('ll)|('d)\", pre_word)\n            token_list = [x for x in token_list if (x is not None) and (x != \"\")]\n            #print(\"token_list:\",token_list)\n            # 単語中の単語をmerge文字で細分化\n            for token in token_list:\n                #print(\"token:\",token)\n                # 単語中のラテン文字とPC命令コードをラテン文字拡張Aに変換した文字のみの単語を生成\n                token = ''.join(byte_encoder[b] for b in token.encode('utf-8'))\n                # merge文字によって分かち書きした単語を保存\n                bpe_tokens.extend(bpe_token for bpe_token in bpe(token).split(' '))\n                if (len(bpe_tokens) == 1) and (bpe_tokens[0] == token) and (len(token_list) == 0):\n                    oov_word_set.add(word)\n                    new_text += word + \" \"\n                    #print(\"no_tokens:\")\n                    continue\n                if len(bpe_tokens) == 1:\n                    bpe_tokens.append(\" \")\n\n                for word in bpe_tokens:\n                    #print(\"word:\",word)\n                    # 分散表現辞書から色々な表現で変換した文中の単語の分散表現を検索\n                    if word in embed:\n                        new_text += word + \" \"\n                        #print(\"embed:\",word)\n                        continue\n\n                    # 単語を全て小文字化した物で検索\n                    if word.lower() in embed:\n                        new_text += word.lower() + \" \"\n                        #print(\"lower:\",word)\n                        continue\n\n                    # 単語を全て大文字化した物で検索\n                    if word.upper() in embed:\n                        new_text += word.upper() + \" \"\n                        #print(\"upper:\",word)\n                        continue\n\n                    # 単語の頭文字だけを大文字化した物で検索\n                    if word.capitalize() in embed:\n                        new_text += word.capitalize() + \" \"\n                        #print(\"cap:\",word)\n                        continue\n\n                    # 特殊文字の分散表現を検索\n                    corr_word = punct_mapping.get(word, None)\n                    if corr_word is not None:\n                        new_text += corr_word + \" \"\n                        #print(\"punct:\",word)\n                        continue\n\n                    try:\n                        # PorterStemmerアルゴリズムで抽出した単語の語幹で検索\n                        vector = p_stemmer.stem(word)\n                    except:\n                        # 失敗したら、文字コードを変更して再実行\n                        vector = p_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"p_st:\",vector)\n                        continue\n\n                    try:\n                        # LancasterStemmerアルゴリズムで抽出した単語の語幹で検索\n                        vector = l_stemmer.stem(word)\n                    except:\n                        vector = l_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"l_st:\",vector)\n                        continue\n\n                    try:\n                        # SnowballStemmerアルゴリズムで抽出した単語の語幹で検索\n                        vector = s_stemmer.stem(word)\n                    except:\n                        vector = s_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"s_st:\",vector)\n                        continue\n\n                    # 単語の分散表現が検索できなかった単語を記録\n                    oov_word_set.add(word)\n                    new_text += word + \" \"\n                    #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef head(enumerable, n=10):\n    #print(enumerable)\n    for i, item in enumerate(enumerable):\n        print(str(i) + '\\n',item)\n        if i > n:\n            return\n\nprint(\"only_data:\")\n# 分散表現の辞書に登録されていない単語を取得\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n\n# テストデータの単語に接尾辞除去を用いて、分散表現の辞書作成\ntrain_text, oov_stemer = process_stemmer(train_text, crawl_emb_dict)\n#print(train_text)\nprint(\"stemmer_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_stemer)\n\n# 単語中の文字を解析して、分散表現を検索\ntrain_text, oov_small_capital = process_small_capital(train_text, crawl_emb_dict, oov_stemer)\nprint(\"small_capital_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_small_capital)\n\n# 単語の誤字脱字を解析し、分散表現を検索\n#train_text_test = train_text\ntrain_text, oov_spell = process_spellcheck(train_text, crawl_emb_dict, oov_small_capital)\nprint(\"spellcheck_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_spell)\n\n# twitterの分かち書きを使って、単語を変換し、分散表現を検索\n#train_text_test = train_text\ntrain_text, oov_twitter = twitter_stemmer(train_text, crawl_emb_dict, oov_spell)\nprint(\"twitter_stemmer_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_twitter)\n\n# merge文字を使って、単語を変換し、分散表現を検索\n#train_text_test = train_text\ntrain_text, oov_mearge = merge_spellcheck(train_text, crawl_emb_dict, oov_twitter)\nprint(\"merge_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del oov, oov_stemer, oov_small_capital, oov_spell, oov_mearge, oov_twitter, bpe_data, bpe_merges, bpe_ranks\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 文章の特徴データ生成\ndef sentence_fetures(text):\n    word_list = text.split()\n    #print(word_list)\n    # 単語数\n    word_count = len(word_list)\n    # 大文字を含む単語の数\n    n_upper = len([word for word in word_list if any([c.isupper() for c in word])])\n    # 含まれる単語の種類\n    n_unique = len(set(word_list))\n    # ビックリマークの数\n    n_ex = word_list.count('!')\n    #print(n_ex)\n    # クエスチョンマークの数\n    n_que = word_list.count('?')\n    # 特殊文字（句読点）の数\n    n_puncts = len([word for word in word_list if word in set_puncts])\n    # 禁句の数\n    n_prof = len([word for word in word_list if word in p_word_set])\n    # unknown単語の数\n    n_oov = len([word for word in word_list if word not in crawl_emb_dict])\n    \n    return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov\n\nfrom collections import defaultdict\nsentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']\n# key不要のvalueがlist型の辞書を作成\nfeature_dict = defaultdict(list)\n#print(raw_train)\nfor text in train_text:\n    #print(text)\n    # 文章の特徴データを取得\n    feature_list = sentence_fetures(text)\n    for i_feature, feature_name in enumerate(sentence_feature_cols):\n        # keyを特徴の名前、valueを文章の特徴値とした辞書を作成\n        feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])\n        \nsentence_df = pd.DataFrame.from_dict(feature_dict)\n#print(sentence_df['word_count'])\n# 各特徴データを単語の数で平均化\nfor col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:\n    sentence_df[col + '_ratio'] = sentence_df[col] / sentence_df['word_count']\n#print(sentence_df)\n    \n# 特徴データからunknownワード関連のデータを削除\nsentence_feature_mat = sentence_df.drop(columns=['n_oov', 'n_oov_ratio']).values\n#print(sentence_feature_mat)\n\n# 特徴データを標準化（平均0、分散1の集合化）\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(sentence_feature_mat)\nsentence_feature_mat = scaler.transform(sentence_feature_mat)\n#print(sentence_feature_mat)\n#print(scaler.var_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sentence_feature_cols, feature_dict, sentence_df, scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentence_feature_mat = torch.tensor(sentence_feature_mat, dtype=torch.float32).cuda()\nsentence_feature_size = sentence_feature_mat.shape[-1]\ny_label_size = y_label.shape[-1]\n#print(y_label_size)\n#print(sentence_feature_mat.shape)\n#print(sentence_feature_size)\nimport numpy as np\nlabel_data = torch.tensor(np.concatenate([y_label, sentence_feature_mat], axis=1), dtype=torch.float32).cuda()\n#print(y_target)\n#print(y_target.shape)\nn_samples = len(train_text)\ntrain_size = int(n_samples * 0.4)\nvalid_size = int(n_samples * 0.3)\n#train_index = list(range(0, train_size))\n#valid_index = list(range(train_size, train_size + valid_size))\n#test_index = list(range(train_size + valid_size, n_samples))\n#print(n_samples)\ntrain = pd.Series(list(train_text[0:train_size]))\ntrain_label = label_data[0:train_size]\nvalid = pd.Series(list(train_text[train_size:train_size + valid_size]))\nvalid_label = label_data[train_size:train_size + valid_size]\ntest = pd.Series(list(train_text[train_size + valid_size:n_samples]))\ntest_label = label_data[train_size + valid_size:n_samples]\n#print(len(train))\n#print(len(valid))\n#print(len(test))\n#print(valid_label)\n#print(label_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_label, n_samples, train_size, valid_size, sentence_feature_mat#, train_index, valid_index, test_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 文章データを整地\ndef tokenize(text, max_len, tokenizer):\n    # vocab.txtに乗っている文字だけを文章内に取得\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]\n\n# 以前のpandasファイルの進捗バーを初期化\n#import pandas as pd\n#from tqdm import tqdm\n#tqdm.pandas()\n#from tqdm import tqdm_notebook as tqdm\n#from tqdm._tqdm_notebook import tqdm_notebook\n#tqdm_notebook.pandas()\ntrain = train.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))\nvalid = valid.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))\ntest = test.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 文章データをvocab.txt内の分散表現に変換\ntrain = train.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\nvalid = valid.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\ntest = test.apply(lambda x: tokenizer.convert_tokens_to_ids(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()\n\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\n# printなどの標準出力メッセージを非表示\nwith suppress_stdout():\n    # 事前学習済みのBertモデル（TendorflowModel）をPytorch上に作成\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        # Bert（TendorflowModel）の事前学習時（checkpoint）の重みをモデルに設定\n        # ファイルの名前で出力は既に決定されている？\n        # ファイル内の変数名に'squad'がある時、事前学習の出力はClassification（最初の時系列データの出力）になる？\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n        BERT_MODEL_PATH + 'bert_config.json',\n        # 重みを設定したBERTモデルの設定ファイルをWORK_DIR上に保存\n        WORK_DIR + 'pytorch_model.bin')\n\n# 使用したBertの設定ファイル（PytorchModel）を別のフォルダに保存\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nclass NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, sentence_feature_size):\n        super(NeuralNet, self).__init__()\n        # WORK_DIR上の設定ファイルで事前学習済みのBERTを作成\n        #self.bert = BertModel.from_pretrained(WORK_DIR)\n        # bertモデルは学習しない\n        #for param in self.bert.parameters():\n            #param.requires_grad=False\n            #print(f'bert-{param.requires_grad}')\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        # Bertの特徴を学習\n        self.before_linear = nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE)\n        self.before_linear2 = nn.Linear(BERT_HIDDEN_SIZE, 50)\n        \n        # sentence_featureの特徴を学習\n        self.sentence_feature_linear = nn.Linear(sentence_feature_size, sentence_feature_size)\n        \n        # Bertとsentence_featureの抱き合わせを学習\n        n_hidden = 50 + sentence_feature_size\n        self.mix_linear = nn.Linear(n_hidden, n_hidden)\n        \n        # 出力\n        self.linear_out = nn.Linear(n_hidden, 1)\n        #nn.init.xavier_uniform_(self.linear_out.weight)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        #nn.init.xavier_uniform_(self.linear_aux_out.weight)\n        \n    def forward(self, bert_output, sentence_feature):\n        # encodeセルの最初の時系列（Classification）以外は出力しないBertモデル\n        # 変数前の*は、変数をアンパックする（引数を増やしてる？）\n        #_, bert_output = self.bert(*x_features, output_all_encoded_layers=False)\n        bert_output = self.dropout(bert_output)\n        \n        bert_relu  = F.relu(bert_output)\n        \n        before_nn = self.before_linear(bert_relu)\n        before_relu = F.relu(before_nn)\n        before_nn2 = self.before_linear2(before_relu)\n        before_relu2 = F.relu(before_nn2)\n        \n        sentence_feature_nn = self.sentence_feature_linear(sentence_feature)\n        sentence_feature_relu = F.relu(sentence_feature_nn)\n        \n        h_cat = torch.cat((before_relu2, sentence_feature_relu), 1)\n        mix_nn = self.mix_linear(h_cat)\n        mix_relu = F.relu(mix_nn)\n        \n        result = self.linear_out(mix_relu)\n        aux_result = self.linear_aux_out(mix_relu)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n# テストデータをバッチ毎に分けるジェネレータ関数\nclass DynamicBucketIterator(object):\n    # テストデータをバッチ毎のデータセットに分割\n    def __init__(self, data, label, capacity, pad_token, shuffle, length_quantile, max_batch_size, for_bert):\n        self.data = data\n        self.label = label\n        self.pad_token = pad_token\n        self.capacity = capacity\n        self.shuffle = shuffle\n        self.length_quantile = length_quantile\n        self.for_bert = for_bert\n        \n        # 文章が短い順に文章データのindexをソート\n        self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))\n        \n        old_separator_index = 0\n        self.separator_index_list = [0]\n        for i_sample in range(len(self.data)):\n            # 文章が短い順に文章データ取得\n            sample_index = self.index_sorted[i_sample]\n            sample = self.data[sample_index]\n            current_batch_size = i_sample - old_separator_index + 1\n            if min(len(sample), MAX_LEN) * current_batch_size <= self.capacity and current_batch_size <= max_batch_size:\n                pass\n            else:\n                # バッチの最後の文章データのindexを記録\n                old_separator_index = i_sample\n                self.separator_index_list.append(i_sample)\n                \n        # 文章データの最後のindexを記録\n        self.separator_index_list.append(len(self.data)) # [0, ..., start_separator_index, end_separator_index, ..., len(data)]\n        \n        if not self.shuffle:\n            # バッチ数取得\n            self.bucket_index = range(self.__len__())\n        \n        self.reset_index()\n\n    def reset_index(self):\n        self.i_batch = 0\n        \n        if self.shuffle:\n            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))\n            self.bucket_index = np.random.permutation(self.__len__())\n    \n    def __len__(self):\n        return len(self.separator_index_list) - 1\n    \n    # イテレータ関数により呼び出される関数\n    def __iter__(self):\n        return self\n    \n    # イテレータにバッチ毎のデータを返す\n    def __next__(self):\n        # バッチ数が全て呼び出されていたら、初期化して、イテレーション終了\n        try:\n            i_bucket = self.bucket_index[self.i_batch]\n        except IndexError as e:\n            self.reset_index()\n            raise StopIteration\n            \n        start_index, end_index = self.separator_index_list[i_bucket : i_bucket + 2]\n        \n        # データのindexを使用順に保存\n        index_batch = self.index_sorted[start_index : end_index]\n\n        raw_batch_data = [self.data[i] for i in index_batch]\n        \n        batch_label = self.label[index_batch]\n        # ???\n        math.ceil(1)\n        \n        # バッチ中で一番長い文章の単語数を取得\n        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))\n        max_len = min([max_len, MAX_LEN])\n        if max_len == 0:\n            max_len = 1\n        \n        # BERT用にデータを整形して、返す\n        if self.for_bert:\n            # バッチの文章データの空配列\n            segment_id_batch = np.zeros((len(raw_batch_data), max_len))\n            padded_batch = []\n            input_mask_batch = []\n            for sample in raw_batch_data:\n                # バッチ内で最長のデータに合わせて、単語が入ってる所に1、入ってない所に0\n                input_mask = [1] * len(sample) + [0] * (max_len - len(sample))\n                input_mask_batch.append(input_mask[:max_len])\n\n                # データ内をバッチ内で最長のデータに合わせて、0でパディング\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            # バッチ毎に、パディングされた文章データ、文章データの空配列、\n            # 文章データのmask、文章毎の学習の正解データ、文章データのindexを返す\n            return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch\n        \n        else:\n            padded_batch = []\n            for sample in raw_batch_data:\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, batch_label, index_batch\n        \ndef sigmoid(x):\n    return np.where(x<-709.0, 0.0, 1 / (1 + np.exp(-x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport torch.optim as optim\n\ntest_dict = {}\nfold_list = [0]\nepochs = 7\n\nstart_time = time.time()\n\n# encodeセルの最初の時系列（Classification）以外は出力しないBertモデル\n# WORK_DIR上の設定ファイルで事前学習済みのBERTを作成\nbert = BertModel.from_pretrained(WORK_DIR).cuda()\n#for param in bert.parameters():\n    #param.requires_grad=False\n    #print(f'bert-{param.requires_grad}')\n\n\nmodel = NeuralNet(y_label_size-1, sentence_feature_size)\n# 最適化手法のパラメータ設定\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nmodel.cuda()\n\n# クラス分類が1のデータのロス関数（BCEWithLogitsLoss）への影響度\n#pos_weight = (len(y_targetlabel) - y_target.sum(0)) / y_target.sum(0)\n#pos_weight[pos_weight == float(\"Inf\")] = 1\n#print(pos_weight)\nloss_fn=nn.BCEWithLogitsLoss(reduction='mean')#, pos_weight=pos_weight)\n\n#for param in model.parameters():\n    #print(param.requires_grad)\n\nhighest_accuracy = 0\nlowest_loss = len(valid) * 100\n#print(lowest_loss)\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n\n    train_loader = DynamicBucketIterator(train, \n                                        train_label,\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n    \n    #print(valid_label)\n    valid_loader = DynamicBucketIterator(valid, \n                                         valid_label,\n                                         capacity=MAX_LEN*batch_size,\n                                         pad_token=0,\n                                         shuffle=False,\n                                         length_quantile=1,\n                                         max_batch_size=2048,\n                                         for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    eval_start_time = time.time()\n    for epoch in range(epochs):\n        batch_i = 0\n        train_loss_validation = 0\n        model.train()\n        for batch in train_loader:\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            #print(y_batch)\n            y_targets_batch = y_batch[:, :y_label_size]\n            sentence_feature = y_batch[:, -sentence_feature_size:]\n            #print(y_targets_batch)\n            #print(sentence_feature)\n            index_batch = batch[4]\n            #sample_weight_batch = y_batch[:, len(y_batch[1])-1]\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n\n            with torch.no_grad():\n                # 変数前の*は、変数をアンパックする（引数を増やしてる？）\n                _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n            #print(bert_output.grad_fn)\n            # 勾配の初期化\n            optimizer.zero_grad()\n            out = model(bert_output, sentence_feature)\n            #print(model.linear_out.weight.grad)\n            #print(y_targets_batch)\n            test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())\n            batch_i += 1\n            \n            # クラス分類が1のデータのロス関数（BCEWithLogitsLoss）への影響度\n            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)\n            #pos_weight[pos_weight == float(\"Inf\")] = 1\n            #print(pos_weight)\n            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n            \n            loss = loss_fn(out, y_targets_batch)\n            #print(f'{epoch}-{batch_i}:{loss.item() / len(y_batch)}')\n            \n            train_loss_validation += loss.item()\n            # 勾配の計算\n            loss.backward()\n            #print(model.linear_out.weight.grad)\n            #print(bert_output.grad_fn.grad)\n            \n            # パラメータの更新\n            optimizer.step()\n            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features, out, loss, bert_output\n            torch.cuda.empty_cache()\n        print(\"train_loss_validation:\", train_loss_validation)\n        \n        valid_pred = np.zeros(len(valid))\n        batch_i = 0\n        loss_validation = 0\n        model.eval()\n        for batch in valid_loader:\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            #print(y_batch)\n            y_targets_batch = y_batch[:, :y_label_size]\n            sentence_feature = y_batch[:, -sentence_feature_size:]\n            #print(y_targets_batch)\n            #print(sentence_feature)\n            index_batch = batch[4]\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n\n            with torch.no_grad():\n                # 変数前の*は、変数をアンパックする（引数を増やしてる？）\n                _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n            #print(bert_output.grad_fn)\n            \n            y_pred = model(bert_output, sentence_feature)\n            \n            #print(\"y_pred:\", y_pred[:, 0])\n            #print(\"y_batch:\", y_batch[:, 0])\n            # クラス分類が1のデータのロス関数（BCEWithLogitsLoss）への影響度\n            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)\n            #pos_weight[pos_weight == float(\"Inf\")] = 1\n            #print(pos_weight)\n            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n            \n            loss = loss_fn(y_pred, y_targets_batch)\n            \n            loss_validation += loss.item()\n            #print(y_batch)\n            #test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())\n            batch_i += 1\n            \n            valid_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            \n            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features#, loss_fn\n            torch.cuda.empty_cache()\n        \n        valid_pred = (torch.from_numpy(valid_pred) >= 0.5).to(torch.float32).cuda()\n        correct = (valid_pred == valid_label[:, 0]).to(torch.float32).sum(0)\n        positive_correct = ((valid_pred == valid_label[:, 0]) & (valid_label[:, 0] == 1.0)).to(torch.float32).sum(0)\n        negative_correct = correct - positive_correct\n        accuracy = correct / len(valid_label)\n        positive_sum = valid_label[:, 0].sum(0)\n        positive_accuracy = positive_correct / positive_sum\n        negative_accuracy = negative_correct / (len(valid_label) - positive_sum)\n        if (accuracy >= highest_accuracy) | (lowest_loss >= loss_validation):\n            print(epoch)\n            print(accuracy)\n            print(positive_accuracy)\n            print(negative_accuracy)\n            highest_accuracy = accuracy\n            lowest_loss = loss_validation\n            print(\"lowest_loss:\", lowest_loss)\n            # 重みやパラメータの値のみ保存（保存されたメモリ番地などは保存しない）\n            torch.save(model.state_dict(), '../fine-tuning')\n            \n        print(\"loss_validation:\", loss_validation)\n        print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    del train_loader, valid_loader, correct, accuracy, batch_i, positive_correct, negative_correct, positive_sum, positive_accuracy, negative_accuracy\n    gc.collect()\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, valid, valid_label, train_label, optimizer, loss_fn\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dict = {}\nfold_list = [0]\n#min_out = torch.tensor(-709, dtype=torch.float32).cuda()\n\ni_epoch = 2\nstart_time = time.time()\n\nmodel.load_state_dict(torch.load('../fine-tuning'))\nmodel.eval()\n#for param in model.parameters():\n    #print(param.requires_grad)\n\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n\n    test_loader = DynamicBucketIterator(test, \n                                        test_label,\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    test_pred = np.zeros(len(test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        sentence_feature = y_batch[:, -sentence_feature_size:]\n        index_batch = batch[4]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n        #                 print('x_features', torch.cuda.memory_allocated())\n        \n        with torch.no_grad():\n            _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n        #print(bert_output.grad_fn)\n        \n        y_pred = model(bert_output, sentence_feature)\n        #                 print('after_prediction', torch.cuda.memory_allocated())\n        #y_pred = torch.where(y_pred < min_out, min_out, y_pred)\n        #print(y_pred)\n        test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_batch, segment_id_batch, input_mask_batch, index_batch, y_batch, sentence_feature, x_features, y_pred, bert_output\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = test_pred\n    #print(epoch_test_pred)\n    del model, test_loader\n    gc.collect()\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = (torch.from_numpy(test_pred) >= 0.5).to(torch.float32).cuda()\ncorrect = (test_pred == test_label[:, 0]).to(torch.float32).sum(0)\naccuracy = correct / len(test_label)\nprint(accuracy)\n\ndel accuracy, correct, test_pred\ngc.collect()\ntorch.cuda.empty_cache()\nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#min_out = torch.tensor(-700, dtype=torch.float32)\n#y_pred = torch.tensor([-709, -710], dtype=torch.float32)\n#pred = torch.where(y_pred < min_out, min_out, y_pred)\n#sigmoid(pred.detach().cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n#df_submit.prediction = test_dict[0]\n#df_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}