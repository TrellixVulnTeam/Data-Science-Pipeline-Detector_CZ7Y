{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **LSTM-CNN Model**\nThis model uses a Bidirectional Long short-term memory Convolution Neural Network.\n\nA layer with a long short-term memory component has the ability to remember the previous data in the input and make decisions based on that input. The bidirectional part represents the ability to understand the \"TEXT\" forwards and backwards.\n\nLSTM combined with a CNN has been show to increase the performance of the model. Refer to the link below for reference.\n\nhttp://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n\nThe setup for this Natural Language Process Application is setup with the following order:\n1. Clean Text by decontracting Contractions + special Chars \n    * They're ---> They are \n    * @)@)#_@#HI ---> HI\n2. Load Pretrained Embedding into embedding Matrix for input into model\n3. Generate LSTM-CNN Model\n\nKaggle competition uses ROC-AUC combined with a Bias ROC-AUC to evaluate this model. \nRefer to the following for details:\n\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n\nFurther Work:\n1. Create a training subgroup to train against identity subgroups in dataset.\n2. Investigate more ways of cleaning/preprocessing text. e.g. stemming,tagging words, etc.\n3. Apply model to other social media platform to assess toxicity levels on different sites.\n4. Create ROC-AUC assessment.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Inspired from toxication-with-embeddings-and-keras-lstm. https://www.kaggle.com/samarthsarin/toxication-with-embeddings-and-keras-lstm\n\nimport numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\n\n\nfrom keras.models import Sequential,Model\nfrom keras.layers import Embedding,Input,Activation,Flatten,CuDNNLSTM,Dense,Dropout,Bidirectional\nfrom keras.layers import Convolution1D,GlobalAveragePooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LeakyReLU\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport re\nimport gc\nimport pickle\nimport seaborn as sns\n%matplotlib inline\ntqdm.pandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reads CSV to df.\n\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cleans up the data in the 'Comment Text' Column by performing the following:**\n1. Decontracting the Contractions and turning text to lowercase.\n2. Removing Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleans up the Data set by decontracting contractions and removing special character/punctuations.\n\n#Decontraction inspired from https://gist.github.com/nealrs/96342d8231b75cf4bb82\ncList = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef deContract(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text.lower())\n\n\ndef clean_special_chars(text):\n    \n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, ' ')\n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\n\n\n\ndef preProcessing(x):\n    x=deContract(x)\n    x=clean_special_chars(x)\n    return x\n    \n\n\ntrain[\"comment_text\"] = train[\"comment_text\"].progress_apply(lambda x: preProcessing(x))\n\ntest[\"comment_text\"] = test[\"comment_text\"].progress_apply(lambda x: preProcessing(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transforms the target column to a bi-categorical column of 1s and 0s.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for transforming target to categorical set. Values > 0.5 are considered Toxic and vice versa.\ndef target(value):\n    if value>=0.5:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applys the transformation to the the target column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use apply function to apply target function.\ntrain['target'] = train['target'].progress_apply(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defines x features and y target.\nx = train['comment_text']\ny = train['target']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Path to pull in a \"Pickle\" version of the glove Pretraining.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Link to the Glove Pretrained Embedding\nGlovePath = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenizers the text into meaningful sequences to be INPUTs into the neural network.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizes Text.\ntoken = Tokenizer()\ntoken.fit_on_texts(x)\npad_seq = pad_sequences(token.texts_to_sequences(x),maxlen = 300)\nvocab_size=len(token.word_index)+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creates the embedding matrix to load into the neural network embedding layer.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function used to building the embedding matrix to load into the model.\ndef build_matrix(word_index, embedPath):\n    with open(embedPath, 'rb') as fp:\n        embedding_index = pickle.load(fp)\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Runs the function to build the matrix.\nembedding_matrix= build_matrix(token.word_index,GlovePath)\ndel (train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Function is used to build the neural network model.**\n\nThe model consists of the following:\n1. Embedding Layer : Uses pre-trained glove embedding.\n2. Bidirectional cuDNN LSTM : Long Short Term Memory is used because it is able to remember what it used in the sentence. CuDNN is the the Nvidia GPU library to utilize GPUs to speed up data analysis.\n3. Convolution 1D\n4. Global Average Pooling 1D\n5. Dense Layer\n6. Leaky ReLu\n7. Dense(64) Relu Layer\n8. Dense (1) sigmoid Layer\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates function load model. Created using bidirectional LSTM \ndef BuildModel(vocab_size,embedding_Matrix):\n    model = Sequential()\n    model.add(Embedding(vocab_size,300,input_length = 300,weights = [embedding_matrix],trainable = False))\n    model.add(Bidirectional(CuDNNLSTM(300,return_sequences=True)))\n    model.add(Convolution1D(64,7,padding='same'))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(1,activation = 'sigmoid'))\n    model.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splits up the training/validation sets.\nx_train,x_test,y_train,y_test = train_test_split(pad_seq,y,test_size = 0.15,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del (x,y)\ngc.collect()\n# Builds the model.\nmodel = BuildModel(vocab_size,embedding_matrix)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The following fits the data to the model and runs the machining portion.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collects the history of the model.\nhistory = model.fit(x_train,y_train,epochs = 5,batch_size=1000,validation_data=(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This last part summarizes the data and exports necessary components.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(1,6)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dumps history into json file for further analysis.\nimport json\njson = json.dumps(values)\n\nf = open(\"history.json\",\"w\")\nf.write(json)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plots the training accuracy vs the validation accuracy.\nplt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plots the train/val loss over the epochs.\nplt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save model to a file for future use.\nfilename = 'finalized_model.sav'\npickle.dump(model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizes the test comment text.\nX = test['comment_text']\ntest_pad_seq = pad_sequences(token.texts_to_sequences(X),maxlen = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uses the model to predict test df.\nprediction = model.predict(test_pad_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame([test['id']]).T\nsubmission['prediction'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}