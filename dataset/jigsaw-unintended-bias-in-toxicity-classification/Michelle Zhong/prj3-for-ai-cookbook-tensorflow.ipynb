{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from utils import *\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport time\nimport pandas as pd\nimport numpy as np\nfrom collections import *\nfrom sklearn.metrics import *\nfrom sklearn.datasets import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom nltk.corpus import stopwords\nimport re\n\nenglish_stopwords = stopwords.words('english')\ndef clearstring(string):\n    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n    string = string.split(' ')\n    string = filter(None, string)\n    string = [y.strip() for y in string if y.strip() not in english_stopwords]\n    string = ' '.join(string)\n    return string.lower()\n\ndef separate_dataset(trainset, ratio = 0.5):\n    datastring = []\n    datatarget = []\n    for i in range(len(trainset.data)):\n        data_ = trainset.data[i].split('\\n')\n        data_ = list(filter(None, data_))\n        data_ = random.sample(data_, int(len(data_) * ratio))\n        for n in range(len(data_)):\n            data_[n] = clearstring(data_[n])\n        datastring += data_\n        for n in range(len(data_)):\n            datatarget.append(trainset.target[i])\n    return datastring, datatarget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = load_files(container_path = '../input/alldata4ai/data_all/data_all/', encoding = 'UTF-8')\ntrainset.data, trainset.target = separate_dataset(trainset,1.0)\nprint (trainset.target_names)\nprint (len(trainset.data))\nprint (len(trainset.target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build dictionary for the text in order to transform the words to vectors\ndef build_dataset(words, n_words):\n    # count -> most frequently used tokens\n    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n    count.extend(Counter(words).most_common(n_words - 1))\n    # word -> int\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    # data -> corpus\n    data = list()\n    # unk refers to unknown tokens\n    unk_count = 0\n    for word in words:\n        index = dictionary.get(word, 0)\n        if index == 0:\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    # int -> word\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot matrix for embedding and initialize it\nONEHOT = np.zeros((len(trainset),len(trainset.target_names)))\nONEHOT[np.arange(len(trainset)),] = 1.0\n\n# split the dataset\nONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\nONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\ntrain_X, test_X, train_Y, test_Y, train_onehot, test_onehot = train_test_split(trainset.data, \n                                                                               trainset.target, \n                                                                               ONEHOT, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all tokens in the text\nconcat = \" \".join(trainset.data).split()\nvocabulary_size = len(list(set(concat)))\n# get the dictionary\ndata, count, dictionary, rev_dictionary = build_dataset(concat,vocabulary_size)\nprint('vocab from size: %d'%(vocabulary_size))\n#print('Most common words (+UNK)', count[:5])\n#print('Sample data', data[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# release the memory\nimport gc\ndel concat\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# emmmm...release it again in case lack of memory\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nclass Model:\n    def __init__(self, size_layer, num_layers, embedded_size,\n                 dict_size, dimension_output, learning_rate, attention_size=150):\n        \n        def cells(reuse=False):\n            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n        # for input data, appear with sess.run: feed_dict{} \n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n        # embedding, random\n        # todo: introduce embedding from pre-trained models\n        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n        # add emedding layer\n        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n        outputs, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)\n        attention_w = tf.get_variable(\"attention_v\", [attention_size], tf.float32)\n        query = tf.layers.dense(tf.expand_dims(last_state[-1].h, 1), attention_size)\n        keys = tf.layers.dense(outputs, attention_size)\n        align = tf.reduce_sum(attention_w * tf.tanh(keys + query), [2])\n        align = tf.nn.softmax(align)\n        outputs = tf.squeeze(tf.matmul(tf.transpose(outputs, [0, 2, 1]),\n                                             tf.expand_dims(align, 2)), 2)\n        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n        self.logits = tf.matmul(outputs, W) + b\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_layer = 128\nnum_layers = 2\nembedded_size = 128\ndimension_output = 2 # â€œtarget\"\nlearning_rate = 1e-3 \nmaxlen = 250 # max number of words in each comment\nbatch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(size_layer,num_layers,embedded_size,vocabulary_size+4,dimension_output,learning_rate)\nsess.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# todo: remove explicit for loop\ndef str_idx(corpus, dic, maxlen, UNK=3):\n    X = np.zeros((len(corpus),maxlen))\n    rowN = 0\n    for row in corpus:\n        colN = 0\n        for k in row.split(): \n            if colN < maxlen:\n                val = dic[k] if k in dic else UNK\n                X[rowN, colN]= val\n                colN += 1\n            else:\n                break\n        rowN += 1\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n'%(EPOCH))\n        break\n        \n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    for i in range(1, (len(train_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n        train_loss += loss\n        train_acc += acc\n    \n    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss = sess.run([model.accuracy, model.cost], \n                           feed_dict = {model.X : batch_x, model.Y : test_onehot[i:i+batch_size]})\n        test_loss += loss\n        test_acc += acc\n    \n    train_loss /= (len(train_X) // batch_size)\n    train_acc /= (len(train_X) // batch_size)\n    test_loss /= (len(test_X) // batch_size)\n    test_acc /= (len(test_X) // batch_size)\n    \n    if test_acc > CURRENT_ACC:\n        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n        \n    print('time taken:', time.time()-lasttime)\n    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n                                                                                          train_acc,test_loss,\n                                                                                          test_acc))\n    EPOCH += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cannot run due to out of memory\n# logits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})\n# print(classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}