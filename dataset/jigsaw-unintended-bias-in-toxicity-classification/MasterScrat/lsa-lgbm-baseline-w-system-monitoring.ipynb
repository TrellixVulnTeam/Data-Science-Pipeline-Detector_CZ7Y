{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LSA, LGBM Baseline + Experiments\n\n**In this notebook, I attempt to use TFIDF followed by SVD (AKA latent semantic analysis or LSA).** It doesn't seem to work well, probably because the problem requires a deeper analysis of the relationship between words and LSA doesn't keep track of their relative positions - we can't even compute trigrams as they don't fit in memory.\n\nHowever I experimented with a few interesting things. Things that worked:\n\n- **Measuring system usage** while the kernel is running. Head here to see how it works: https://www.kaggle.com/masterscrat/monitoring-system-usage\n\n- **Using `tqdm` works well for more things than I expected. **For example, it gives useful output when ingesting data in `TfidfVectorizer` or `TruncatedSVD`\n\n- **Logging output to standard output and *commit logs* at the same time.** This means you can keep track of the progression of your kernel while it's commiting. Quite useful in practice.\n\n- **Simple t-SNE viz**\n\nThings that didn't work out:\n\n- **Parallelizing TF-IDF.** In theory, you can use all available cores for the `TfidfVectorizer` `fit` operation. It does speed things up, but it causes out of memory errors when using the full dataset.\n\n- **Loading LGBM `Dataset`s from files.** Reportedly, this would prevent the massive memory use that occurs when LGBM starts up. In practice, it seems to use more memory, and fails with an out of memory error when using the full dataset (while things load properly using the regular way on that same dataset)."},{"metadata":{},"cell_type":"markdown","source":"- **v36** Added system monitoring, more LGB leaves, removed cruft\n\n- **v35** Made public"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PARAMETERS\nTOY_MODE = False\ntfidf_ngram_range = (1, 2) # trigrams don't fit in memory\nsvd_n_components = 64 # 96 doesn't fit in memory\nlgb_num_leaves = 65\n\nPARALLELIZE_TF_IDF = False # doesn't work\nLOAD_LGBM_DATA_FROM_FILES = False # doesn't work\n\ncompetition_files_path = '../input/jigsaw-unintended-bias-in-toxicity-classification/'","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### MONITORING ###\nimport psutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport multiprocessing\nfrom IPython.display import clear_output\nfrom collections import deque\n\nclass SystemMonitorProcess:\n    def __init__(self, start_timestamp, update_interval=0.1):\n        self.update_interval = update_interval\n        self.cpu_nums = psutil.cpu_count()\n        self.max_mem = psutil.virtual_memory().total\n        self.sysCpuLogs = deque()\n        self.sysMemLogs = deque()\n        self.timeLogs = deque()\n        self.start_time = start_timestamp\n\n    def get_system_info(self):\n        cpu_percent = psutil.cpu_percent(interval=0.0, percpu=False)\n        mem_percent = float(psutil.virtual_memory().used) / self.max_mem * 100\n        return cpu_percent, mem_percent\n        \n    def monitor(self):\n        while True:\n            time.sleep(self.update_interval)\n            sCpu, sMem = self.get_system_info()  \n            self.sysCpuLogs.append(sCpu)\n            self.sysMemLogs.append(sMem)\n            self.timeLogs.append(time.time() - self.start_time)\n            logs.update({\n                'sysCpuLogs': self.sysCpuLogs,\n                'sysMemLogs': self.sysMemLogs,\n                'time': self.timeLogs\n            })\n            \nclass SystemMonitor:\n    def __init__(self, update_interval=0.1):\n        self.graph = None\n        self.update_interval = update_interval\n        self.start_timestamp = time.time()\n        self.msgs = []\n    \n    def monitor(self):\n        self.graph = SystemMonitorProcess(self.start_timestamp, self.update_interval)\n        self.graph.monitor()\n        \n    def annotate(self, msg):\n        self.msgs.append([time.time() - self.start_timestamp, msg])\n        \n    def plot(self):\n        if not 'sysCpuLogs' in logs:\n            print('No data yet.')\n            return\n\n        fig = plt.figure(figsize=(20,3))\n        plt.ylabel('usage (%)')\n        \n        # FIXME display running time on primary X axis!\n        ax = plt.axes()\n        #plt.xlabel('running time (s)')\n        \n        ax2 = ax.twiny()\n        ax2.plot(list(logs['time']), logs['sysCpuLogs'], label=\"cpu\")\n        ax2.plot(list(logs['time']), logs['sysMemLogs'], label=\"mem\")\n        ax2.set_xticks([msg[0] for msg in self.msgs])\n        ax2.set_xticklabels([msg[1] for msg in self.msgs], rotation=90)\n\n        ax2.legend(loc='best')\n        plt.show()\n        \nsm = SystemMonitor(1.0) # polling frequency\nlogs = multiprocessing.Manager().dict()\nsmp = multiprocessing.Process(target=sm.monitor)\nsmp.start()","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport shlex\nimport gc\n\nfrom numpy.random import seed\nfrom tensorflow import set_random_seed\nseed(42)\nset_random_seed(42)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message both to standard output and to Kaggle commit logs\n# inspired from https://www.kaggle.com/alexktn/logs-in-commits/log\n# shlex.quote exists specifically to shell-escape strings!\ndef debug(msg):\n    os.system('echo ' + shlex.quote(msg))\n    sm.annotate(msg)\n    print(msg)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndebug('Started {}'.format(time.strftime('%d %b %Y at %H:%M:%S', time.localtime())))","execution_count":35,"outputs":[{"output_type":"stream","text":"Started 06 Apr 2019 at 22:23:07\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(competition_files_path + 'train.csv')\ntest = pd.read_csv(competition_files_path + 'test.csv')\n\nprint(train.shape)\nprint(test.shape)","execution_count":36,"outputs":[{"output_type":"stream","text":"(1804874, 45)\n(97320, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TOY_MODE:\n    train = train[:int(len(train)/32)]\n    test = test[:int(len(test)/32)]","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = train[['comment_text', 'target']]\ntest = test[['comment_text']]\n\ntrain['comment_text'] = train['comment_text'].astype(str)\ntest['comment_text'] = test['comment_text'].astype(str)","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug('Starting TFIDF, ngram range {}...'.format(tfidf_ngram_range))\n\ntfv = TfidfVectorizer(min_df=3,\n                      max_features=None, \n                      strip_accents='unicode', \n                      analyzer='word', \n                      token_pattern=r'(?u)\\b\\w+\\b',  \n                      ngram_range=tfidf_ngram_range,\n                      use_idf=1, \n                      smooth_idf=1, \n                      sublinear_tf=1\n                     )","execution_count":39,"outputs":[{"output_type":"stream","text":"Starting TFIDF, ngram range (1, 2)...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_text = list(train['comment_text'].values) + list(test['comment_text'].values)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif PARALLELIZE_TF_IDF:\n    debug('Fit...')\n    tfv.fit(tqdm(full_text));","execution_count":41,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 9.78 µs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Tried out parallel transform, but fails with out of memory, to investigate.\n# https://github.com/rafaelvalero/ParallelTextProcessing/blob/master/parallelizing_text_processing.ipynb\n\nif PARALLELIZE_TF_IDF:\n    import multiprocessing\n    from multiprocessing import Pool\n    import scipy.sparse as sp\n\n    num_cores = multiprocessing.cpu_count()\n\n    def chunks(l, n):\n        for i in range(0, len(l), n):\n            yield l[i:i + n]\n\n    def parallelize_dataframe(df, func):\n        pool = Pool(num_cores)\n        df = sp.vstack(pool.map(func, chunks(df, 512)), format='csr')\n        pool.close()\n        pool.join()\n        return df\n\n    def test_func(data):\n        tfidf_matrix = tfv.transform(data)\n        return tfidf_matrix\n\n    tfidf_col = parallelize_dataframe(full_text, test_func)","execution_count":42,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 11 µs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not PARALLELIZE_TF_IDF:\n    tfidf_col = tfv.fit_transform(tqdm(full_text))","execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=59443), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b982eb039904f5daf08ddb9ea088d8f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del full_text, tfv","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf_col.shape)\ntfidf_col","execution_count":45,"outputs":[{"output_type":"stream","text":"(59443, 176887)\n","name":"stdout"},{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"<59443x176887 sparse matrix of type '<class 'numpy.float64'>'\n\twith 4739092 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug('Starting SVD, reducing feature dimension from {} to {}...'.format(tfidf_col.shape[1], svd_n_components))","execution_count":46,"outputs":[{"output_type":"stream","text":"Starting SVD, reducing feature dimension from 176887 to 64...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd_ = TruncatedSVD(n_components=svd_n_components, random_state=1337)\n\nsvd_col = svd_.fit_transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('TFIDF_')\n\ndel svd_, tfidf_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(svd_col).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = svd_col[0:len(train)]\nX_test = svd_col[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\nX_embedded = TSNE(n_components=2).fit_transform(X_train[0:1500])\ndftsne = pd.DataFrame(X_embedded, columns=['x','y'])\ndftsne['class'] = train['target'] > 0.5\n\n# I'm no t-SNE expert but this looks pretty unpromising\nax = sns.lmplot('x', 'y', dftsne, hue='class', fit_reg=False, height=8, scatter_kws={'alpha':0.7,'s':60})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug('Starting LGBM...');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import roc_auc_score\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst\n\nlgb_params = {'application': 'regression', \n              'boosting': 'gbdt',\n              'metric': 'rmse',\n              'num_leaves': lgb_num_leaves,\n              'max_depth': -1,\n              'learning_rate': 0.01,\n              'bagging_fraction': 0.85,\n              'feature_fraction': 0.8,\n              'min_split_gain': 0.02,\n              'min_child_samples': 150,\n              'min_child_weight': 0.02,\n              'lambda_l2': 0.0475,\n              'verbosity': -1,\n              'data_random_seed': 17,\n              'early_stop': 600,\n              'max_bin': 255,\n              'verbose_eval': 500,\n              'num_rounds': 10000,\n              \n              # only used when loading from files\n              'has_header': True,\n              'label_column': 'name:label',\n              'use_two_round_loading': True\n             }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV code from https://www.kaggle.com/skooch/petfinder-simple-lgbm-baseline\n\nfrom sklearn.model_selection import KFold\n\nN_SPLITS = 5\n\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = KFold(n_splits=N_SPLITS, random_state=42, shuffle=True)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    roc_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], N_SPLITS))\n    feature_importance_df = pd.DataFrame()\n    \n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('{} fold {}/{}'.format(label, i, N_SPLITS))\n        \n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        \n        ###\n        pred_val_y, pred_test_y, importances, roc = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        ###\n        \n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            roc_scores.append(roc)\n            debug(label + ' CV score {}/{}: ROC {}'.format(i, N_SPLITS, roc))\n            \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        \n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n        i += 1\n        \n    print('{} CV RMSE scores : {}'.format(label, cv_scores))\n    print('{} CV mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} CV std RMSE score : {}'.format(label, np.std(cv_scores)))\n    print('{} CV ROC scores : {}'.format(label,  roc_scores))\n    print('{} CV mean ROC score : {}'.format(label, np.mean(roc_scores)))\n    print('{} CV std ROC score : {}'.format(label, np.std(roc_scores)))\n    \n    pred_full_test = pred_full_test / float(N_SPLITS)\n    \n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'roc': roc_scores,\n               'importance': feature_importance_df\n              }\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_importance(feature_importance, columns):\n    feature_imp = pd.DataFrame(sorted(zip(feature_importance, columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 20))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()\n    plt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndef run_lgb(train_X, train_y, test_X, test_y, test_X2, params):\n    \n    if LOAD_LGBM_DATA_FROM_FILES:\n        # Tried dumping data to CSV before loading to LGBM, as recommended from:\n        # https://github.com/Microsoft/LightGBM/issues/1032\n        # BUT didn't work, made problem worse\n        \n        # TODO try to *append* to CSV instead of doing concat in memory\n        # https://stackoverflow.com/questions/17530542/how-to-add-pandas-data-to-an-existing-csv-file\n\n        train_X = pd.concat([train_y, train_X], axis=1)\n        train_X = train_X.rename(columns={ train_X.columns[0]: \"label\" })\n        train_X.to_csv('train_X.csv')\n        del train_X\n\n        test_X = pd.concat([test_y, test_X], axis=1)\n        test_X = test_X.rename(columns={ test_X.columns[0]: \"label\" })\n        test_X.to_csv('test_X.csv')\n        del test_X\n\n        gc.collect()\n\n        d_train = lgb.Dataset('train_X.csv', free_raw_data=True)\n        d_train.raw_data = None\n\n        d_valid = lgb.Dataset('test_X.csv', free_raw_data=True)\n        d_valid.raw_data = None\n\n        gc.collect()\n    \n    else:\n        d_train = lgb.Dataset(train_X, label=train_y, free_raw_data=True)\n        d_valid = lgb.Dataset(test_X, label=test_y, free_raw_data=True)\n        del train_X, train_y\n    \n    watchlist = [d_train, d_valid]\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n        \n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop,\n                      )\n    \n    print('Computing score...')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    roc = roc_auc_score(test_y > 0.5, pred_test_y)\n    \n    print('Predicting on test set...')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    \n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), roc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = run_cv_model(X_train, X_test, train['target']  > 0.5, run_lgb, lgb_params, rmse, 'LGB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance\nplot_importance(results['importance']['importance'], results['importance']['feature'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save results"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = [r[0] for r in results['test']]\nsub = pd.read_csv(competition_files_path + 'sample_submission.csv')\n\nif not TOY_MODE:\n    assert sub.shape[0] == len(test_predictions)\n    debug('Saving...')\n    sub['prediction'] = test_predictions\n    sub.to_csv('submission.csv', index=False)\n    sub.head()\n    \nelse:\n    print(\"Toy mode, won't save.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm.plot()\nsmp.terminate()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}