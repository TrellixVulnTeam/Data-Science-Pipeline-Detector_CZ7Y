{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks for @christofhenkel @abhishek @iezepov for their great work:\n\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\nhttps://www.kaggle.com/abhishek/pytorch-bert-inference\nhttps://www.kaggle.com/iezepov/starter-gensim-word-embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/russian-bert-pavlov/\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#%reload_ext autoreload\n#%autoreload \n#%matplotlib inline\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors\n#import fastai\n#from fastai.train import Learner\n#from fastai.train import DataBunch\n#from fastai.callbacks import *\n#from fastai.basic_data import DatasetType\n#import fastprogress\n#from fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED =1234","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport torch\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nMAX_SEQUENCE_LENGTH = 256\nSEED = 1234\nBATCH_SIZE = 512\nBERT_MODEL_PATH = '../input/russian-bert-pavlov/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\nbert_config = BertConfig('../input/berttinkoff/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '../input/fasttest-common-crawl-russian/cc.ru.300.vec'\n#GLOVE_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\nNUM_MODELS = 1\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 200\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BERT Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_dir = \"../input/datatinkoff/\"\n\nd = 473141\nnum_to_load= 373141  #\n\nvalid_size= 100000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_text(x):\n    \n    \n    return \"{0} . {1} . {2}\".format(x['header'], x['text'], x['job_title']) \n\n\ntrain_df  =  pd.read_csv(os.path.join(Data_dir,\"sr_train.csv\")).sample(d,random_state=SEED)\n\n\ntrain_df.loc[train_df.job_title.isnull(), 'job_title']  = 'Пусто'\n\ntrain_df.loc[train_df.header.isnull(), 'header']  = 'Пусто'\n\ntrain_df = train_df.tail(100000)\n\n\ntrain_df['text_1'] = train_df[['header', 'text', 'job_title']].apply(lambda x: merge_text(x), axis=1)\ntrain_df['text_1'] = train_df['text_1'].progress_apply(lambda x:str(x))\nX_train = convert_lines(train_df[\"text_1\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n\n\n#sequences = convert_lines(train_df[\"text_1\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n\n\n#test_df = pd.read_csv(os.path.join(Data_dir,\"sr_test.csv\"))\n#train_df['comment_text'] = train_df['comment_text'].astype(str) \n\n\n\ntest_df  =  pd.read_csv(os.path.join(Data_dir,\"sr_test.csv\"))#.sample(d,random_state=SEED)\n\n\ntest_df.loc[test_df.job_title.isnull(), 'job_title']  = 'Пусто'\n\ntest_df.loc[test_df.header.isnull(), 'header']  = 'Пусто'\n\n\n\ntest_df['text_1'] = test_df[['header', 'text', 'job_title']].apply(lambda x: merge_text(x), axis=1)\ntest_df['text_1'] = test_df['text_1'].apply(lambda x:str(x))\nX_test = convert_lines(test_df[\"text_1\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n\n\n\n##test_df['comment_text'] = test_df['comment_text'].astype(str) \n#X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"../input/berttinkoff/bert_pytorch.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros((len(X_test)  ,  4))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    #test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 1].detach().cpu().squeeze().numpy()\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 2].detach().cpu().squeeze().numpy()\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 3].detach().cpu().squeeze().numpy()\n    \n\n#test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n\n\nMODEL_NAME = ['like', 'skip', 'dislike',  'view']\nfor i in range(len(MODEL_NAME)):\n    test_df[MODEL_NAME[i]]=torch.sigmoid(torch.tensor(test_preds[:,i])).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_bert_test = pd.DataFrame.from_dict({\n#    'id': test_df['id'],\n##    'prediction': test_pred\n#})\ntest_df.to_csv(\"bertestk.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = np.zeros((len(X_train)  ,  4))\ntrain = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long))\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=512, shuffle=False)\ntk0 = tqdm(train_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    #train_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n    \n    train_preds[i * 512:(i + 1) * 512,0]=pred[:,0].detach().cpu().squeeze().numpy()\n    train_preds[i * 512:(i + 1) * 512,1]=pred[:,1].detach().cpu().squeeze().numpy()\n    train_preds[i * 512:(i + 1) * 512,2]=pred[:,2].detach().cpu().squeeze().numpy()\n    train_preds[i * 512:(i + 1) * 512,3]=pred[:,3].detach().cpu().squeeze().numpy()\n    \n\n#train_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n\nMODEL_NAME = ['like', 'skip', 'dislike',  'view']\nfor i in range(len(MODEL_NAME)):\n    train_df[MODEL_NAME[i]]=torch.sigmoid(torch.tensor(valid_preds[:,i])).numpy()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df.to_csv(\"berttrain100000k.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_df = reduce_mem_usage(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n##symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##tokenizer = TreebankWordTokenizer()\n\n##isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n##remove_dict = {ord(c):f'' for c in symbols_to_delete}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##x_train = train_df['text_1'].progress_apply(lambda x:preprocess(x))\n##y_aux_train = train_df[['like', 'skip','dislike', 'view']]\n##x_test = test_df['text_1'].progress_apply(lambda x:preprocess(x))\n##col = ['like', 'skip','dislike', 'view']\n##identity_columns = [\n #   'like', 'skip','dislike', 'view']\n# Overall\n##weights = np.ones((len(x_train),)) / 4\n# Subgroup\n##weights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\n##weights += (( (train_df[col].values>=0.5).astype(bool).astype(np.int) +\n ##  (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\n##weights += (( (train_df[col].values<0.5).astype(bool).astype(np.int) +\n #3  (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n##loss_weight = 1.0 / weights.mean()\n\n##y_train = np.vstack([(train_df[col].values>=0.5).astype(np.int),weights]).T\n\n##max_features = 410047","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###tokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n###crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n####print('n unknown words (crawl): ', len(unknown_words_crawl))\n\n#glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n#print('n unknown words (glove): ', len(unknown_words_glove))\n\n####max_features = max_features or len(tokenizer.word_index) + 1\n####max_features\n\n####embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n###embedding_matrix.shape\n\n###del crawl_matrix\n#del glove_matrix\n####gc.collect()\n\n####y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##x_train = tokenizer.texts_to_sequences(x_train)\n##x_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n \n###maxlen = 300\n###x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n\n###x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###batch_size = 512\n###test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n###train_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\n###valid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\n###train_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n   ###                                     sequence_index=0, \n      ###                                  length_index=1, \n         ##                               label_index=2)\n####test_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\n####train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n###valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n###test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\n###databunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###all_test_preds = []\n\n##for model_idx in range(NUM_MODELS):\n ##   print('Model ', model_idx)\n  ##  seed_everything(1 + model_idx)\n  ##  model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n  # # learn = Learner(databunch, model, loss_func=custom_loss)\n  ##  test_preds = train_model(learn,test_dataset,output_dim=7)    \n    ##all_test_preds.append(test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_lstm = pd.DataFrame.from_dict({#\n#    'id': test_df['id'],\n#    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n#})\n\n\n\n###MODEL_NAME = ['like', 'skip', 'dislike',  'view']\n##for i in range(len(MODEL_NAME)):\n##    train_df[MODEL_NAME[i]]=torch.sigmoid(torch.tensor(valid_preds[:,i])).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Blending part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.read_csv(#\n #   \"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\"\n#)\n\n#weights = [0.333, 0.667]\n#weights = [0.5, 0.5]\n#submission[\"prediction\"] = ensemble_predictions(\n#    [submission_bert.prediction.values, submission_lstm.prediction.values],\n##    weights,\n #   type_=\"rank\",\n#)\n#submission.to_csv(\"submission.csv\", index=False)\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}