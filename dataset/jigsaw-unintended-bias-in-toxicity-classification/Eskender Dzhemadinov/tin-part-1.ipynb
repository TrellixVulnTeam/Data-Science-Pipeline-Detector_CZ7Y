{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Текста, имена\n#! git clone https://github.com/NVIDIA/apex\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/tinkoffapex/apex-master\n#! cd apex\n#! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/tinkoffapex/apex-master\"))\n#print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n#print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n#print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install apex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Nvidia Apex\n\n#!git clone https://github.com/NVIDIA/apex.git && cd apex && python setup.py install --cuda_ext --cpp_ext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndevice=torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1804874\n#MAX_SEQUENCE_LENGTH = 256\nMAX_SEQUENCE_LENGTH =256\nSEED = 1234\nEPOCHS = 3\nData_dir=\"../input/datatinkoff/\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\" \n#num_to_load= 1600000                         #Train size to match time limit\n#valid_size= 204874                         #Validation Size\n\n\n#d = 5000\n#num_to_load = 4000\n#valid_size = 1000\nd = 473141\nnum_to_load= 373141  #\n\nvalid_size= 100000\n\n\n\n\n#valid_size= 5000\n\n\n\n#num_to_load= 20000                       #Train size to match time limit\n#valid_size= 10000\nTOXICITY_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the Bart Pytorch repo to the PATH\n# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\n\n\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam, OpenAIAdam\nprint(os.listdir(\"../working\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translate model from tensorflow to pytorch\n#BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nBERT_MODEL_PATH   = \"../input/russian-bert-pavlov/\"\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n     BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n#shutil.copyfile(\"../input/weightbert/1_bert_pytorch.bin\", WORK_DIR + 'pytorch_model.bin')\n#shutil.copyfile(BERT_MODEL_PATH + 'bert_pytorch.bin', WORK_DIR + 'bert_pytorch.bin')\n#comment\n#shutil.copyfile(\"../input/bert-part-one/bert_pytorch.bin\", WORK_DIR + 'pytorch_model.bin')\nshutil.copyfile( BERT_MODEL_PATH+ 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the Bert configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig(BERT_MODEL_PATH+'bert_config.json')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT_MODEL_PATH   = \"../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/\"\nBERT_MODEL_PATH   = \"../input/russian-bert-pavlov/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer1 = TreebankWordTokenizer()\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer1.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num \n#d = 1804874\n\n#k = d - valid_size \n#num_to_load= 1000000                         #Train size to match time limit#\n#valid_size= 104874 \n#/kaggle/input/datatinkoff/sr_train.csv\n#/kaggle/input/datatinkoff/sr_test.csv\n\ntrain_df = pd.read_csv(os.path.join(Data_dir,\"sr_train.csv\")).sample(d,random_state=SEED)\nprint(train_df.shape)\n#print(train_df.target.mean())\n#print(train_df[train_df.target > 0].count())\n#test = train_df[k:]\n\n#train = train_df[:k].sort_values(by=['target'], ascending=False)\n\n#train_df = train[:num_to_load].sample(frac=1, random_state=42).append(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[train_df.job_title.isnull(), 'job_title']  = 'Пусто'\n\ntrain_df.loc[train_df.header.isnull(), 'header']  = 'Пусто'\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('loaded %d records' % len(train_df))\n\n\n\n# Make sure all comment_text values are strings\n#train_df['comment_text'] = train_df['comment_text'].astype(str) \n\n#num_to_load= len(train_df[:-100000][train_df.target > 0])                     #Train size to match time limit\n#valid_size= len(train_df[-100000:]) \n\n\n#train_df  = train_df[:-100000][train_df.target > 0].append(train_df[-100000:])\ndef merge_text(x):\n    \n    \n    return \"{0} . {1} . {2}\".format(x['header'], x['text'], x['job_title']) \ntrain_df['text_1'] = train_df[['header', 'text', \"job_title\"]].apply(lambda x: merge_text(x), axis=1)\ntrain_df['text_1'] = train_df['text_1'].progress_apply(lambda x:str(x))\n\n\nsequences = convert_lines(train_df[\"text_1\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n#train_df=train_df.fillna(0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.loc[train_df.dislike ==1, 'target']  =0\n#train_df.loc[train_df.like ==1, 'target']  =1\n#train_df.loc[train_df.view ==1, 'target']  =0.7\n#train_df.loc[train_df.skip ==1, 'target']  =0.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# List all identities\nidentity_columns = [\n    'like', \t'skip', \t'dislike', \t'view' \t]\ny_columns= ['like', \t'skip', \t'dislike', \t'view'  ]\n\ntrain_df = train_df.drop(['text_1', 'header', 'text', 'job_title'],axis=1)\n# convert target to 0,1\n#train_df['target']=(train_df['target']>=0.5).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = sequences[:num_to_load]                \ny = train_df[y_columns].values[:num_to_load]\nX_val = sequences[num_to_load:]                \ny_val = train_df[y_columns].values[num_to_load:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=train_df.tail(valid_size).copy()\n#train_df=train_df.head(num_to_load)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#del train_df\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X\ndel y\ndel sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    #alpha=1\n    #reduce= False\n    def __init__(self, alpha=0.25, gamma=1, logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        \n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n        \nkl = FocalLoss()\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ngc.collect()\n\n\noutput_model_file = \"bert_pytorch.bin\"\n\nlr=2e-5\nbatch_size = 32\naccumulation_steps=1\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n#model = BertForSequenceClassification.from_pretrained(\"../input/bert-part-one/\",cache_dir=None,num_labels=len(y_columns))\nmodel = BertForSequenceClassification.from_pretrained(\"../working/\",cache_dir=None,num_labels=len(y_columns))\n\nmodel.zero_grad()\nmodel = model.to(device)\n\n\n\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n\noptimizer = OpenAIAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\n\n\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()\n\ntq = tqdm_notebook(range(EPOCHS))\nprint(tq)\ncnt = 1\nfor epoch in tq:\n    cnt+=1\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    \n    \n    #if cnt==2:\n    #    checkpoint = torch.load('../input/weightbert/bert_pytorch.bin')\n    ##    model.load_state_dict(checkpoint['model_state_dict'])\n    #    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    #    epoch = checkpoint['epoch']\n    #    loss = checkpoint['loss']\n    \n    for i,(x_batch, y_batch) in tk0:\n        if i <= 200000:\n        \n            optimizer.zero_grad()\n            \n            y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n            #print(y_pred)\n            loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n            #loss = ls.forward(self, y_pred,y_batch.to(device))\n            #loss = forward(y_pred,y_batch.to(device))\n            #####loss = kl.forward( y_pred,y_batch.to(device))\n            #loss  = F.mse_loss(y_pred,y_batch.to(device)) #81\n            #loss  = F.soft_margin_loss(y_pred,y_batch.to(device)) \n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n                optimizer.step()                            # Now we can do an optimizer step\n                optimizer.zero_grad()\n            if lossf:\n                #lossf = 0.95*lossf+0.05*loss.item()\n                lossf = 0.98*lossf+0.02*loss.item()\n            else:\n                lossf = loss.item()\n            tk0.set_postfix(loss = lossf)\n            avg_loss += loss.item() / len(train_loader)\n            avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n    \n    \ntorch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss\n            }, \"{0}\".format(output_model_file))\n    #break\n    \n\n\n#torch.save(model.state_dict(), output_model_file)\n#torch.save({\n #           'epoch': epoch,\n  #          'model_state_dict': model.state_dict(),\n   #         'optimizer_state_dict': optimizer.state_dict(),\n    #        'loss': loss\n     #       }, \"1_{0}\".format(output_model_file))\n\n#torch.save(model.state_dict(), output_model_file)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_model_file = 'bert_pytorch.bin'\nmodel = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\ncheckpoint = torch.load(output_model_file)\n#model.load_state_dict(output_model_file)\n#model.load_state_dict(torch.load(output_model_file ))\n#model.load_state_dict(torch.load(output_model_file ))\n#checkpoint = torch.load('1_bert_pytorch.bin')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n#model.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.eval()\nvalid_preds = np.zeros((len(X_val) ,   len(y_columns)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    #print(i)\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    #print(pred[:,1].detach().cpu().squeeze().numpy().shape)\n    valid_preds[i*32:(i+1)*32,0]=pred[:,0].detach().cpu().squeeze().numpy()\n    valid_preds[i*32:(i+1)*32,1]=pred[:,1].detach().cpu().squeeze().numpy()\n    valid_preds[i*32:(i+1)*32,2]=pred[:,2].detach().cpu().squeeze().numpy()\n    valid_preds[i*32:(i+1)*32,3]=pred[:,3].detach().cpu().squeeze().numpy()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../working/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From baseline kernel\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid_preds[:,0].shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nMODEL_NAME = ['model1', 'model2', 'model3', 'model4']\nfor i in range(len(MODEL_NAME)):\n    test_df[MODEL_NAME[i]]=torch.sigmoid(torch.tensor(valid_preds[:,i])).numpy()\n#TOXICITY_COLUMN = 'target'\ntest_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp = ['like', 'skip','dislike', 'view']\nfor i in range(len(MODEL_NAME)):\n    TOXICITY_COLUMN = sp[i]\n    bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME[i], sp[i])\n    bias_metrics_df\n    get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score\n#like \tskip \tdislike \tview\ny_true = np.array(test_df['like'])\ny_scores = np.array(test_df['model1'])\nroc_auc_score(y_true, y_scores)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}