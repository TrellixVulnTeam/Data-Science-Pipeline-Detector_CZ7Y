{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport gc\nimport unicodedata\nimport six\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nK.set_epsilon(1e-7)\nimport re\nimport codecs\nimport sys\nimport string\nimport codecs\nimport numpy as np\nimport re\nimport pandas as pd\nfrom tqdm import *\nsys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\nfrom keras_bert.keras_bert.bert import get_model\nimport tokenization \nfrom keras_bert.keras_bert import get_custom_objects\n#from keras_bert.keras_bert.optimizers import AdamWarmup\n\n\ndef bert_get_result():\n    maxlen = 220\n    bsz=512\n    print('begin_build')\n    def checkpoint_loader(checkpoint_file):\n        def _loader(name):\n            return tf.train.load_variable(checkpoint_file, name)\n        return _loader\n\n\n    def load_trained_model_from_checkpoint(config_file,\n                                      #checkpoint_file,\n                                           training=False,\n                                           seq_len=None):\n        \"\"\"Load trained official model from checkpoint.\n        :param config_file: The path to the JSON configuration file.\n        :param checkpoint_file: The path to the checkpoint files, should end with '.ckpt'.\n        :param training: If training, the whole model will be returned.\n                         Otherwise, the MLM and NSP parts will be ignored.\n        :param seq_len: If it is not None and it is shorter than the value in the config file, the weights in\n                        position embeddings will be sliced to fit the new length.\n        :return:\n        \"\"\"\n        with open(config_file, 'r') as reader:\n            config = json.loads(reader.read())\n        if seq_len is None:\n            seq_len = config['max_position_embeddings']\n        else:\n            seq_len = min(seq_len, config['max_position_embeddings'])\n        #loader = checkpoint_loader(checkpoint_file)\n        model = get_model(\n            token_num=config['vocab_size'],\n            pos_num=seq_len,\n            seq_len=seq_len,\n            embed_dim=config['hidden_size'],\n            transformer_num=config['num_hidden_layers'],\n            head_num=config['num_attention_heads'],\n            feed_forward_dim=config['intermediate_size'],\n            training=training,\n        )\n        if not training:\n            inputs, outputs = model\n            model = keras.models.Model(inputs=inputs, outputs=outputs)\n\n        return model\n\n    def convert_lines(example, max_seq_length,tokenizer):\n        max_seq_length -=2\n        all_tokens = []\n        longer = 0\n        for i in tqdm(range(len(example))):\n          tokens_aa = tokenizer.tokenize(example[i])\n          if len(tokens_aa)>max_seq_length:\n            tokens_a = tokens_aa[:int(max_seq_length/2)]+tokens_aa[-int(max_seq_length/2):]\n            longer += 1\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n          else:\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_aa+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_aa))\n            all_tokens.append(one_token)\n        print(longer)\n        return np.array(all_tokens)\n\n    test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')#[:1024]#.sample(512*2)\n    #test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    \n      \n    ###\n    ###\n    ## new models\n    print('new models')\n    symbols_to_delete = '→★©®●ː☆¶）иʿ。ﬂﬁ₁♭年▪←ʒ、（月■⇌ɹˤ³の¤‿عدويهصقناخلىبمغرʀɴשלוםביエンᴵאעכח‐ικξتحكسةفزط‑地谷улкноה歌мυтэпрдˢᵒʳʸᴺʷᵗʰᵉᵘοςתמדףנרךצט成都ех小土》करमा英文レクサス外国人бьыгя不つзц会下有的加大子ツشءʲшчюж戦щ明קљћ我出生天一家新ʁսհןجі‒公美阿ספ白マルハニチロ社ζ和中法本士相信政治堂版っфچیリ事「」シχψմեայինրւդک《ლさようならعدويهصقناخلىبمغرʀɴשלוםביエンᴵאעכח‐ικξتحكسةفزط‑地谷улкноה歌мυтэпрдˢᵒʳʸᴺʷᵗʰᵉᵘοςתמדףנרךצט成都ех小土》करमा英文レクサス外国人бьыгя不つзц会下有的加大子ツشءʲшчюж戦щ明קљћ我出生天一家新ʁսհןجі‒公美阿ספ白マルハニチロ社ζ和中法本士相信政治堂版っфچیリ事「」シχψմեայինրւդک《ლさようなら\\n＼🍕\\r🐵😑\\xa0\\ue014≠\\t\\uf818\\uf04a\\xad😢🐶❤️☺\\uf0e0😜😎👊\\u200b\\u200e😁أ😍💖̶💵❥━┣┫Е┗Ｏ►👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏᴇᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ✔\\x96\\x92😋👏😱‼\\x81ジ故障➤\\u2009🚌͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘☕♡◐║▬💩💯⛽🚄🏼ஜ۩۞😖ᴠ🚲✒➥😟😈═ˌ💪🙏🎯◄🌹😇💔😡\\x7f👌ἐὶήὲἀίῃἴ🙄✬ＳＵＰＥＲＨＩＴ😠\\ufeff☻\\u2028😉😤⛺♍🙂\\u3000👮💙😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆✓◾🍻🍽🎶🌺🤔😪\\x08؟🐰🐇🐱🙆😨⬅🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚獄℅ВПАН🐾🐕❣😆🔗🚽舞伎🙈😴🏿🤗🇺🇸♫ѕＣＭ⤵🏆🎃😩█▓▒░\\u200a🌠🐟💫💰💎\\x95🖐🙅⛲🍰⭐🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004❧▰▔ᴼᴷ◞▀\\x13🚬▂▃▄▅▆▇↙🤓\\ue602😵άόέὸ̄😒͝☹➡🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7✋\\uf04c\\x9f\\x10😣⏺̲̅😌🤑́🌏😯😲∙‛Ἰᾶὁ💞🚓◇🔔📚✏🏀👐\\u202d💤🍇\\ue613豆🏡▷❔❓⁉❗\\u202f👠्🇹🇼🌸蔡🌞˚🎲😛˙关系С💋💀🎄💜🤢َِ✨是\\x80\\x9c\\x9d🗑\\u2005💃📣👿༼◕༽😰ḷЗ▱￼🤣卖温哥华议降％你失去所钱拿坏税骗🐝¯🎅\\x85🍺آإ🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴⌠ИОРФДЯМ✘😝🖑ὐύύ特殊作群╪💨圆园▶ℐ☭✭🏈😺♪🌍⏏ệ🍔🐮🍁☔🍆🍑🌮🌯☠🤦\\u200d♂𝓒𝓲𝓿𝓵안영하세요ЖК🍀😫🤤ῦ在了可以说普通话汉语好极🎼🕺☃🍸🥂🗽🎇🎊🆘☎🤠👩✈🖒✌✰❆☙🚪⚲\\u2006⚭⚆⬭⬯⏖○‣⚓∎ℒ▙☏⅛✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ｃϖ\\u2000үａᴦᎥһͺ\\u2007ｓǀ\\u2001ɩ℮ｙｅ൦ｌƽ¸ｗｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋∼ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋℳ𝐀𝐥𝐪❄🚶𝙢Ἱ🤘ͦ💸☼패티Ｗ⋆𝙇ᵻ👂👃ɜ🎫\\uf0a7БУ🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾͡๏̯﴿⚾⚽Φ₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ🎾👹￦⎌🏒⛸寓养宠物吗🏄🐀🚑🤷操𝒑𝒚𝒐𝑴🤙🐒℃欢迎来到拉斯𝙫⏩☮🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ⚠🦄巨收赢得鬼愤怒要买额ẽ🚗✊🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷❌⭕▸𝗢𝟳𝟱𝟬⦁株式⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊☐☑多伦⚡☄ǫ🐽🎻🎹⛓🏹╭╮🍷🦆为友谊祝贺与其想象对如直接问用自己猜传教没积唯认识基督徒曾经让耶稣复活死怪他但当们聊些题时候例战胜因圣把全结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁＞ʕ̣Δ🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용✞🔫👁┈╱╲▏▕┃╰▊▋╯┳┊☒凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿☝💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ✅☛𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨♩🐎🤞☞🐸💟🎰🌝🛳点击查🍭𝑥𝑦𝑧ＡＮＧＪＢ👣\\uf020◔◡🏉💭🎥♀Ξ🐴👨🤳⬆🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲̱ℏ𝑮𝗕𝗴\\x91🍒⠀ꜥⲣⲏ╚🐑⏰↺⇤∏鉄件✾◦♬ї💊\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製虚偽屁理屈｜Г𝑩𝑰𝒀𝑺🌤∵∴𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡῖΛΩ⤏🇳𝒙Ձռձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫☜Βώ💢▲ΜΟΝΑΕ🇱♲𝝈↴↳💒⊘▫Ȼ⬇🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎✧😼🕷ｇｏｖｒｎｍｔｉｄｕ２０８ｆｂ＇ｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦∕🌈🔭🐊🐍\\uf10aˆ⚜☁ڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜🔼'\n    symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@・ω+=”“[]^–>\\\\°<~•™ˈʊɒ∞§{}·ταɡ|¢`―ɪ£♥´¹≈÷′ɔ€†μ½ʻπδηλσερνʃ±µº¾．»ав⋅¿¬β⇒›¡₂₃γ″«φ⅓„：¥сɑ！−²ʌ¼⁴⁄₄‚‖⊂⅔¨×θ？∩，ɐ₀≥↑↓／√－‰≤'\n\n    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n    \n    abbr_mapping = {'ᴀ':'a','ʙ':'b','ᴄ':'c','ᴅ':'d','ᴇ':'e','ꜰ':'f','ɢ':'g','ʜ':'h',\n                      'ɪ':'i','ᴊ':'j','ᴋ':'k','ʟ':'l','ᴍ':'m','ɴ':'n','ᴏ':'o','ᴘ':'p',\n                      'ǫ':'q','ʀ':'r','ꜱ':'s','ᴛ':'t','ᴜ':'u','ᴠ':'v','ᴡ':'w','ʏ':'y','ᴢ':'z', '\\n':' ',\n                      'u.s.a.': 'usa', 'u.s.a': 'usa', 'u.s.': 'usa',  ' u.s ': ' usa ','u s of a': 'usa',\n                      ' u.k. ': 'uk', ' u.k ': ' uk ', ' yr old ': ' years old ',\n                      ' yrs old ': ' years old ',' ph.d ': ' phd ','kim jong-un': 'the president of north korea',\n                      '#metoo': 'metoo', 'trumpster': 'trump supporter','trumper': 'trump supporter',\n                      'trumpian':'trump supporter','trumpism':'trump supporter',\"trump's\" : 'trump',\n                      ' u r ': ' you are ',  'e.g.': 'for example','i.e.': 'in other words',\n                      'et.al': 'elsewhere', 'antisemitic': 'anti-semitic','sb91':'senate bill',                                   \n                      }\n\n\n    contraction_mapping = {\n        \"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n        'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n        'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\n        \"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have', 'can;t': 'cannot','can;t;ve': 'cannot have',\n        'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n        \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\n        \"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n        'couldn;t;ve': 'could not have','couldn´t': 'could not', 'couldn´t´ve': 'could not have','couldn’t': 'could not',\n        'couldn’t’ve': 'could not have', 'could´ve': 'could have',\n        'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n        'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n        'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n        \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n        'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not',\n        'hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not',\n        'hasn’t': 'has not', \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not',\n        'haven’t': 'have not',\"he'd\": 'he would', \"he'd've\": 'he would have',\"he'll\": 'he will','he´ll': 'he will',\n        \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',   \n        'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have',    \n        'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\n        \"how'd\": 'how did',\"how'll\": 'how will',\"how's\": 'how is','how,d': 'how did','how,ll': 'how will',\n        'how,s': 'how is','how;d': 'how did','how;ll': 'how will','how;s': 'how is','how´d': 'how did','how´ll': 'how will',\n        'how´s': 'how is','how’d': 'how did','how’ll': 'how will','how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\n        \"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will','i,m': 'i am','i,ve': 'i have','i;d': 'i would',\n        'i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not','isn,t': 'is not','isn;t': 'is not',\n        'isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is', \"it's\": 'it is',\n        'it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will', 'it;s': 'it is',\n        'it´d': 'it would','it´ll': 'it will','it´s': 'it is','it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n        'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n        'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us', 'let’s': 'let us',\n        \"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not', 'mayn;t': 'may not',\n        'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have',\n        'might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not',\n        'mightn;t': 'might not','mightn´t': 'might not', 'mightn’t': 'might not','might´ve': 'might have',\n        'might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n        \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not',\n        'must´ve': 'must have','must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not',\n        'needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n        'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\n        \"shan't\": 'shall not', 'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not',\n        'sha´n´t': 'shall not','sha’n’t': 'shall not',\"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is',\n        'she,d': 'she would','she,ll': 'she will', 'she,s': 'she is','she;d': 'she would','she;ll': 'she will',\n        'she;s': 'she is','she´d': 'she would','she´ll': 'she will', 'she´s': 'she is','she’d': 'she would',\n        'she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have',\n        'should;ve': 'should have', \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not',\n        'shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have', 'should’ve': 'should have',\n        \"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n        'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\n        \"there'd\": 'there had', \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had',\n        'there;s': 'there is', 'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n        \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have','they,d': 'they would',\n        'they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will',\n        'they;re': 'they are', 'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are',\n        'they´ve': 'they have','they’d': 'they would','they’ll': 'they will','they’re': 'they are','they’ve': 'they have',\n        \"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not','wasn’t': 'was not',\n        \"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n        'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n        \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not',\n        'we´d': 'we would','we´ll': 'we will',    'we´re': 'we are','we´ve': 'we have','we’d': 'we would',\n        'we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\n        \"what's\": 'what is',    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is',\n        'what,ve': 'what have','what;ll': 'what will','what;re': 'what are','what;s': 'what is','what;ve': 'what have',\n        'what´ll': 'what will', 'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will',\n        'what’re': 'what are','what’s': 'what is', 'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is',\n        'where,d': 'where did','where,s': 'where is','where;d': 'where did','where;s': 'where is','where´d': 'where did',\n        'where´s': 'where is','where’d': 'where did','where’s': 'where is', \"who'll\": 'who will',\"who's\": 'who is',\n        'who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is','who´ll': 'who will','who´s': 'who is',\n        'who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n        'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not',\n        'wouldn´t': 'would not','wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are',\n        'you,d': 'you would','you,ll': 'you will', 'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n        'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would',\n        'you’ll': 'you will','you’re': 'you are','´cause': 'because','’cause': 'because',\"you've\": \"you have\",\n        \"could'nt\": 'could not',\"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\n        \"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\"who're\": \"who are\",\"who've\": \"who have\",\n        \"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n        \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",\n        'don\"\"t': \"do not\",\"dosen't\": \"does not\", \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\n        \"there'll\": \"there will\",\"there're\": \"there are\", \"this'll\": \"this all\",\" u're\": \" you are\", \"ya'll\": \"you all\",\n        \"you'r \": \"you are \",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\n        \"dont't\": \"do not\",\"gov't\": \"government\",\"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘i\":'i',  \":)\": ' smile ',\n        \":-)\": ' smile ','…':'...', '😉': ' wink ', '😂': ' joy ', '😀': ' stuck out tongue ',  \n         }\n\n    dirty_dict = {      \n                        re.compile( '[^a-zA-Z][uU] of [oO][^a-zA-Z]'): ' you of all ',\n                        re.compile('[wW][hH][^a-zA-Z ][^a-zA-Z ][eE]'):'whore' ,                  #  wh**e\n                        re.compile('[wW][hH][^a-zA-Z ][rR][eE]'):'whore',                         #  wh*re   \n                        re.compile('[wW][^a-zA-Z ][oO][rR][eE]'):'whore',                         #  w*ore  \n                        '[wW] h o r e':'whore',\n                      #  re.compile('[sS][hH][^a-zA-Z ][tT] '):'shit ',                         #   sh*t_\n                        re.compile(' [sS][hH][^a-zA-Z ][tT]'):' shit',                         #   _sh*t\n                        re.compile('[sS][hH][*_x][tT] '):'shit ',                            #   sh*t\n                        re.compile(' [sS][^a-zA-Z ][^a-zA-Z ][tT]'):' shit',                   #   _s**t\n                      #  re.compile('[sS][^a-zA-Z ][^a-zA-Z ][tT] '):'shit ',                   #   s**t_\n                        re.compile('[sS][-*_x][-*_x][tT] '):'shit ',                      #   s**t\n                      #  re.compile('[sS][hH][^a-zA-Z ][^a-zA-Z ] '):'shit ',                    #   sh**_ \n                        re.compile(' [sS][hH][^a-zA-Z ][^a-zA-Z ]'):' shit',                     #   _sh** \n                      #  re.compile('[sS][^a-zA-Z ][iI][tT] '):'shit ',                         #   s*it_   \n                        re.compile(' [sS][^a-zA-Z ][iI][tT]'):' shit',                         #   _s*it \n                        re.compile('[sS][-*_x][iI][tT] '):'shit ',                            #   shit\n                        '[sS] h i t':'shit','5h1t': 'shit',\n                        re.compile(' [fF][^a-zA-Z ][^a-zA-Z ][kK]'):' fuck',                   #   _f**k\n                        re.compile('[fF][^a-zA-Z ][^a-zA-Z ][kK] '):'fuck ',                   #   f**k_\n                        re.compile('[fF][-*_x][-*_x][kK]'):'fuck',                       #   f**k\n                        re.compile(' [fF][^a-zA-Z ][cC][kK]'):' fuck',                         #   _f*ck\n                        re.compile('[fF][^a-zA-Z ][cC][kK] '):'fuck ',                         #   f*ck_\n                        re.compile('[fF][-*_x][cC][kK]'):'fuck',                            #   f*ck\n                        re.compile(' [fF][uU][^a-zA-Z ][kK]'):' fuck',                         #   _fu*k\n                        re.compile('[fF][uU][^a-zA-Z ][kK] '):'fuck ',                         #   fu*k_\n                        re.compile('[fF][uU][-*_x][kK]'):'fuck',                            #   fu*k\n                        '[pP]huk': 'fuck','[pP]huck': 'fuck','[fF]ukk':'fuck','[fF] u c k':'fuck',\n                        '[fF]cuk': 'fuck',' [fF]uks': ' fucks',              \n                        re.compile(' [dD][^a-zA-Z ][^a-zA-Z ][kK]'):' dick',                   #   _d**k\n                        re.compile('[dD][^a-zA-Z ][^a-zA-Z ][kK] '):'dick ',                   #   d**k_\n                        re.compile('[dD][-*_x][-*_x][kK]'):'dick',                       #   d**k\n                        re.compile(' [dD][^a-zA-Z ][cC][kK]'):' dick',                         #   _d*ck\n                        re.compile('[dD][^a-zA-Z ][cC][kK] '):'dick ',                         #   d*ck_\n                        re.compile('[dD][-*_x][cC][kK]'):'dick',                            #   d*ck\n                        re.compile(' [dD][iI][^a-zA-Z ][kK]'):' dick',                         #   _di*k\n                        re.compile('[dD][iI][^a-zA-Z ][kK] '):'dick ',                         #   di*k_\n                        re.compile('[dD][iI][-*_x][kK]'):'dick',                            #   di*k\n\n                        re.compile(' [sS][^a-zA-Z ][cC][kK]'):' suck',                         #   _s*ck\n                        re.compile('[sS][^a-zA-Z ][cC][kK] '):'suck ',                         #   s*ck_\n                        re.compile('[sS][-*_x][cC][kK]'):'suck',                            #   s*ck\n                        re.compile(' [sS][uU][^a-zA-Z ][kK]'):' suck',                         #   _su*k\n                        re.compile('[sS][uU][^a-zA-Z ][kK] '):'suck ',                         #   su*k_\n                        re.compile('[sS][uU][-*_x][kK]'):'suck',                            #   su*k\n\n                        re.compile(' [cC][^a-zA-Z ][nN][tT]'):' cunt',                         #   _c*nt\n                        re.compile('[cC][^a-zA-Z ][nN][tT] '):'cunt ',                         #   c*nt_\n                        re.compile('[cC][-*_x][nN][tT]'):'cunt',                            #   c*nt\n                        re.compile(' [cC][uU][^a-zA-Z ][tT]'):' cunt',                         #   _cu*t\n                        re.compile('[cC][uU][^a-zA-Z ][tT] '):'cunt ',                         #   cu*t_\n                        re.compile('[cC][uU][-*_x][tT]'):'cunt',                            #   cu*t\n\n                        re.compile(' [bB][^a-zA-Z ][tT][cC][hH]'):' bitch',                       #   _b*tch\n                        re.compile('[bB][^a-zA-Z ][tT][cC][hH] '):'bitch ',                       #   b*tch_\n                        re.compile('[bB][-*_x][tT][cC][hH]'):'bitch',                          #   b*tch\n                        re.compile(' [bB][iI][^a-zA-Z ][cC][hH]'):' bitch',                       #   _bi*ch\n                        re.compile('[bB][iI][^a-zA-Z ][cC][hH] '):'bitch ',                       #   bi*ch_\n                        re.compile('[bB][iI][-*_x][cC][hH]'):'bitch',                          #   bi*ch\n                        re.compile(' [bB][iI][tT][^a-zA-Z ][hH]'):' bitch',                       #   _bit*h\n                        re.compile('[bB][iI][tT][^a-zA-Z ][hH]'):'bitch ',                       #   bit*h_\n                        re.compile('[bB][iI][tT][-*_x][hH]'):'bitch',                          #   bit*h\n                        re.compile('[bB][^a-zA-Z ][tT][^a-zA-Z ][hH]'):'bitch',                   #   b*t*h\n                        'b[-*_x][-*_x][-*_x]h':'bitch',                                          #   b***h\n                        '[bB] i t c h':'bitch',\n                        re.compile('[aA][*_]s'):'ass',                                #   a*s\n                        re.compile('[aA][^a-zA-Z ][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',               #   a**hole\n                        re.compile(' [aA][^a-zA-Z ][^a-zA-Z ][hH]'):' assh',                   #   a**h\n                        re.compile('[aA][^a-zA-Z ][sS][hH][oO][lL][eE]'):'asshole',                     #   a*shole\n                        re.compile('[aA][sS][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',                     #   as*hole\n                        ' [aA]s[*]':' ass','[aA] s s': 'ass ','[aA]sswhole': 'ass hole',\n                        re.compile('[aA]ssh[^a-zA-Z ]le'):'asshole',                     #   assh*le\n                        '[hH] o l e':'hole',\n                        '[bB][*]ll': 'bull', \n                        re.compile('[pP][^a-zA-Z ][sS][sS][yY]'):' pussy',                         #   p*ssy\n                        re.compile('[pP][uU][^a-zA-Z ][sS][yY]'):' pussy',                         #   pu*sy\n                        re.compile('[pP][uU][sS][^a-zA-Z ][yY]'):' pussy',                         #   pus*y\n                        re.compile('[pP][uU][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',                   #   pu**y\n                        re.compile('[pP][^a-zA-Z ][^a-zA-Z ][sS][yY]'):' pussy',                   #   p**sy\n                        re.compile(' [pP][^a-zA-Z ][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',            #   _pussy\n                        '[pP]ussi': 'pussy', '[pP]ussies': 'pussy','[pP]ussys': 'pussy', \n                        '[jJ]ack[-]off': 'jerk off','[mM]asterbat[*]': 'masterbate','[gG]od[-]dam': 'god damm',\n\n              }\n\n\n    new_final_mapping = { 'jackoff': 'jerk off','jerkoff':'jerk off','bestial': 'beastial',\n                         'bestiality': 'beastiality', 'd1ck': 'dick', 'lmfao': 'laughing my fucking ass off',\n                          'masturbate': 'masterbate', 'cashap24':'cash app','nurnie':'pussy',\n                         'n1gger': 'nigger', 'nigga': 'nigger', 'niggas': 'niggers',\n                         'clickbait':'click with bait','yuge':'huge','outsider77':'outsider',\n                         'numbnuts': 'noob nuts', 'orgasms': 'orgasm', 'trudope':'the prime minister of canada',\n                          'daesh':'isis', \"qur'an\":'the central religious text of islam','gofundme':'go fund me',\n                         'finicum':'an american spokesman','trumpkins':'trump with pumpkin',\n                           'trumpcare':'trump health care','obamacare':'obama health care','trumpy':'trump',\n                          'trumpster': 'trump supporter','trumper': 'trump supporter','trumpettes':'trump',\n                         'realdonaldtrump':'real donald trump','trumpeteer[s]?':'trump supporter',\n                          'trumpian':'trump supporter','trumpism':'trump supporter',\"trump[']s\" : 'trump',\n                         'trumplethinskin':'trump','trumpo':'trump','trumpies':'trump',\n                          'kim jong([- ]?un)?': 'the president of north korea','cheetolini':'trump',\n                          'trumpland':'trump land','trumpty':'trump','trumpist[s]?':'trump supporter',\n                          ' brotherin ':' brother ', 'beyak':'canadian politician',\n                          'trudeaus':'prime minister of canada ','shibai':'failure',\n                          'tridentinus':'tridentinum','zupta[s]?':'the south african president',\n                           'putrumpski':'putin and trump supporter','twitler':'twitter user',\n                           'antisemitic': 'anti semitic', 'sb91':'senate bill', \n                            'utmterm':' utm term','fakenews':'fake news',  'thedonald':'the donald',               \n                            'washingtontimes':'washington times','garycrum':'gary crum',\n                            'rangermc':'car','tfws':'tuition fee waiver','sjw?':'social justice warrior',\n                            'koncerned':'concerned','vinis':'vinys','Yᴏᴜ':'you', 'auwe': 'oh no',\n                            'bigly':'big league','drump[f]?':'trump','brexit':'british exit',\n                            'utilitas':'utilities','justiciaries': 'justiciary','doctrne':'doctrine',\n                           'deplorables': 'deplorable','conartist' : 'con-artist','pizzagate':'pizza gate',\n                           'theglobeandmail': 'the globe and mail', 'howhat': 'how that', ' coz ':' because ',\n                           'civilbeat':'civil beat','gubmit':'submit','financialpost':'financial post',               \n                           'theguardian': 'the guardian','shopo':'shop','fentayal': 'fentanyl',\n                         'designation-': 'designation ','mutilitated' : 'mutilated','dood-': 'dood ',\n                         'irakis' : 'iraki', 'supporter[a-z]?':'supporter',' u ':' you ', \n                        }\n\n\n    def pre_clean_abbr_words(x, dic = abbr_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def correct_contraction(x, dic = contraction_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n\n    def clean_dirty_dict(x, dic = dirty_dict):\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n\n\n\n    def handle_punctuation(x):\n        x = x.translate(remove_dict)\n        x = x.translate(isolate_dict)\n        return x\n\n\n    def spacing_punctuation(text): ##clear puncts\n        for punc in new_puncts:\n            if punc in text:\n                text = text.replace(punc, ' ')\n        return text\n\n    '''  \n    def final_contraction(x, dic = final_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n    '''\n\n    def new_final_contraction(x, dic = new_final_mapping):\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n    def preprocess(df_comment):\n\n        # lower\n        # clean misspellings\n        df_comment = df_comment.str.lower()\n        df_comment = df_comment.str.replace('[\\'\\\"\\(\\[\\:]?https?:?//[!-z]+',' ')\n        df_comment = df_comment.str.replace('[\\'\\\"\\(\\[\\:]?www[.][!-z]+',' ')\n        df_comment = df_comment.apply(pre_clean_abbr_words)\n        df_comment = df_comment.apply(correct_contraction) \n        df_comment = df_comment.apply(clean_dirty_dict)\n\n        # clean the text\n    #    df_comment = df_comment.apply(spacing_punctuation)\n        df_comment = df_comment.apply(lambda x:handle_punctuation(x))\n        df_comment = df_comment.apply(new_final_contraction)\n\n        return df_comment\n    \n    \n    \n    ## firstlarge models\n    \n    print('bert_large_uncased_wwm')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/wwm_uncased_l-24_h-1024_a-16/wwm_uncased_L-24_H-1024_A-16' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n    print('build tokenizer uncased done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    \n    model2.load_weights('../input/jul2995365ep2bertlarge/95365ep2bertlarge.h5')\n    print('load ba models new')\n    eval_lines = (preprocess(test_df['comment_text'])).values\n    token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n    seg_input2 = np.zeros((token_input2.shape[0],maxlen))\n    mask_input2 = np.ones((token_input2.shape[0],maxlen))\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertlarge_wwm_uncased',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertlarge.csv', index=False)\n    \n    \n    ##then base uncased models\n    \n    print('bert_based_uncased')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    #checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    #dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    #tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n    print('build tokenizer uncased done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    ##low\n    model2.load_weights('../input/final-model-group2/bertuncasedbase_pre_220_95175_ep2.h5')\n    print('load bert base uncased models low')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_uncased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_low.csv', index=False)\n    \n    ##high\n    model2.load_weights('../input/95282bertbaseuncased/95282bertbaseuncased.h5')\n    print('load bert base uncased models high')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_uncased_high',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_high.csv', index=False)\n    \n    \n    ##last base cased models\n    ##first def preprocessing\n    symbols_to_delete = '→★©®●ː☆¶）иʿ。ﬂﬁ₁♭年▪←ʒ、（月■⇌ɹˤ³の¤‿عدويهصقناخلىبمغرʀɴשלוםביエンᴵאעכח‐ικξتحكسةفزط‑地谷улкноה歌мυтэпрдˢᵒʳʸᴺʷᵗʰᵉᵘοςתמדףנרךצט成都ех小土》करमा英文レクサス外国人бьыгя不つзц会下有的加大子ツشءʲшчюж戦щ明קљћ我出生天一家新ʁսհןجі‒公美阿ספ白マルハニチロ社ζ和中法本士相信政治堂版っфچیリ事「」シχψմեայինրւդک《ლさようならعدويهصقناخلىبمغرʀɴשלוםביエンᴵאעכח‐ικξتحكسةفزط‑地谷улкноה歌мυтэпрдˢᵒʳʸᴺʷᵗʰᵉᵘοςתמדףנרךצט成都ех小土》करमा英文レクサス外国人бьыгя不つзц会下有的加大子ツشءʲшчюж戦щ明קљћ我出生天一家新ʁսհןجі‒公美阿ספ白マルハニチロ社ζ和中法本士相信政治堂版っфچیリ事「」シχψմեայինրւդک《ლさようなら\\n＼🍕\\r🐵😑\\xa0\\ue014≠\\t\\uf818\\uf04a\\xad😢🐶❤️☺\\uf0e0😜😎👊\\u200b\\u200e😁أ😍💖̶💵❥━┣┫Е┗Ｏ►👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏᴇᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ✔\\x96\\x92😋👏😱‼\\x81ジ故障➤\\u2009🚌͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘☕♡◐║▬💩💯⛽🚄🏼ஜ۩۞😖ᴠ🚲✒➥😟😈═ˌ💪🙏🎯◄🌹😇💔😡\\x7f👌ἐὶήὲἀίῃἴ🙄✬ＳＵＰＥＲＨＩＴ😠\\ufeff☻\\u2028😉😤⛺♍🙂\\u3000👮💙😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆✓◾🍻🍽🎶🌺🤔😪\\x08؟🐰🐇🐱🙆😨⬅🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚獄℅ВПАН🐾🐕❣😆🔗🚽舞伎🙈😴🏿🤗🇺🇸♫ѕＣＭ⤵🏆🎃😩█▓▒░\\u200a🌠🐟💫💰💎\\x95🖐🙅⛲🍰⭐🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004❧▰▔ᴼᴷ◞▀\\x13🚬▂▃▄▅▆▇↙🤓\\ue602😵άόέὸ̄😒͝☹➡🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7✋\\uf04c\\x9f\\x10😣⏺̲̅😌🤑́🌏😯😲∙‛Ἰᾶὁ💞🚓◇🔔📚✏🏀👐\\u202d💤🍇\\ue613豆🏡▷❔❓⁉❗\\u202f👠्🇹🇼🌸蔡🌞˚🎲😛˙关系С💋💀🎄💜🤢َِ✨是\\x80\\x9c\\x9d🗑\\u2005💃📣👿༼◕༽😰ḷЗ▱￼🤣卖温哥华议降％你失去所钱拿坏税骗🐝¯🎅\\x85🍺آإ🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴⌠ИОРФДЯМ✘😝🖑ὐύύ特殊作群╪💨圆园▶ℐ☭✭🏈😺♪🌍⏏ệ🍔🐮🍁☔🍆🍑🌮🌯☠🤦\\u200d♂𝓒𝓲𝓿𝓵안영하세요ЖК🍀😫🤤ῦ在了可以说普通话汉语好极🎼🕺☃🍸🥂🗽🎇🎊🆘☎🤠👩✈🖒✌✰❆☙🚪⚲\\u2006⚭⚆⬭⬯⏖○‣⚓∎ℒ▙☏⅛✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ｃϖ\\u2000үａᴦᎥһͺ\\u2007ｓǀ\\u2001ɩ℮ｙｅ൦ｌƽ¸ｗｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋∼ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋℳ𝐀𝐥𝐪❄🚶𝙢Ἱ🤘ͦ💸☼패티Ｗ⋆𝙇ᵻ👂👃ɜ🎫\\uf0a7БУ🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾͡๏̯﴿⚾⚽Φ₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ🎾👹￦⎌🏒⛸寓养宠物吗🏄🐀🚑🤷操𝒑𝒚𝒐𝑴🤙🐒℃欢迎来到拉斯𝙫⏩☮🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ⚠🦄巨收赢得鬼愤怒要买额ẽ🚗✊🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷❌⭕▸𝗢𝟳𝟱𝟬⦁株式⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊☐☑多伦⚡☄ǫ🐽🎻🎹⛓🏹╭╮🍷🦆为友谊祝贺与其想象对如直接问用自己猜传教没积唯认识基督徒曾经让耶稣复活死怪他但当们聊些题时候例战胜因圣把全结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁＞ʕ̣Δ🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용✞🔫👁┈╱╲▏▕┃╰▊▋╯┳┊☒凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿☝💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ✅☛𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨♩🐎🤞☞🐸💟🎰🌝🛳点击查🍭𝑥𝑦𝑧ＡＮＧＪＢ👣\\uf020◔◡🏉💭🎥♀Ξ🐴👨🤳⬆🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲̱ℏ𝑮𝗕𝗴\\x91🍒⠀ꜥⲣⲏ╚🐑⏰↺⇤∏鉄件✾◦♬ї💊\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製虚偽屁理屈｜Г𝑩𝑰𝒀𝑺🌤∵∴𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡῖΛΩ⤏🇳𝒙Ձռձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫☜Βώ💢▲ΜΟΝΑΕ🇱♲𝝈↴↳💒⊘▫Ȼ⬇🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎✧😼🕷ｇｏｖｒｎｍｔｉｄｕ２０８ｆｂ＇ｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦∕🌈🔭🐊🐍\\uf10aˆ⚜☁ڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜🔼'\n    symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@・ω+=”“[]^–>\\\\°<~•™ˈʊɒ∞§{}·ταɡ|¢`―ɪ£♥´¹≈÷′ɔ€†μ½ʻπδηλσερνʃ±µº¾．»ав⋅¿¬β⇒›¡₂₃γ″«φ⅓„：¥сɑ！−²ʌ¼⁴⁄₄‚‖⊂⅔¨×θ？∩，ɐ₀≥↑↓／√－‰≤'\n    \n    abbr_mapping = {      'ᴀ':'a','ʙ':'b','ᴄ':'c','ᴅ':'d','ᴇ':'e','ꜰ':'f','ɢ':'g','ʜ':'h',\n                      'ɪ':'i','ᴊ':'j','ᴋ':'k','ʟ':'l','ᴍ':'m','ɴ':'n','ᴏ':'o','ᴘ':'p',\n                      'ǫ':'q','ʀ':'r','ꜱ':'s','ᴛ':'t','ᴜ':'u','ᴠ':'v','ᴡ':'w','ʏ':'y','ᴢ':'z', '\\n':' ',\n                       ' yr old ': ' years old ',' yrs old ': ' years old ','co₂':'carbon dioxide',\n               }     \n\n    regex_mapping = {\n                         '[Uu][.][Ss][.][Aa][.]': 'USA', '[Uu][.][Ss][.][Aa]': 'USA',\n                          '[Uu][.][Ss][.]': 'USA',  ' [Uu][.][Ss] ': ' USA ','[Uu] [Ss] of [Aa]': 'USA',\n                          ' [Uu][.][Kk][.]? ': ' UK ',' [Pp][Hh][.][Dd] ': ' phd ',' [Uu] [Rr] ': ' you are ',\n                         '[Ee][.][Gg][.]': 'for example','[Ii][.][Ee][.]': 'in other words',\n                          '[Ee][Tt][.][Aa][Ll]': 'elsewhere',\"[Gg]ov[']t\": \"government\",\n                         '[Tt][Rr][Uu][Mm][Pp]':'trump','[Oo][Bb][Aa][Mm][Aa]':'obama',\n                    }\n\n\n    new_final_mapping = {  \n                            'jackoff': 'jerk off','jerkoff':'jerk off','bestial': 'beastial',\n                         'bestiality': 'beastiality', 'd1ck': 'dick', 'lmfao': 'laughing my fucking ass off',\n                          'masturbate': 'masterbate', 'cashap24':'cash app','nurnie':'pussy',\n                         'n1gger': 'nigger', 'nigga': 'nigger', 'niggas': 'niggers',\n                         'clickbait':'click with bait','YUGE':'huge','Outsider77':'outsider',\n                         'numbnuts': 'noob nuts', 'orgasms': 'orgasm', 'Trudope':'The prime minister of Canada',\n                          '[Dd]aesh':'ISIS', \"Qur'an\":'the central religious text of Islam','gofundme':'go fund me',\n                         'Finicum':'an American spokesman','trumpkins':'trump with pumpkin','trumpettes':'trump',\n                           'trump[Cc]are':'trump health care','obama[Cc]are':'obama health care','trumpies':'trump',\n                          'trumpster': 'trump supporter','trumper': 'trump supporter', 'trumpy':'trump',\n                          'trumpian':'trump supporter','trumpism':'trump supporter',\"trump[']s\" : 'trump',\n                          '[Kk]im [Jj]ong([- ][Uu]n)?': 'the president of north korea','Cheetolini':'trump',\n                          'trumpland':'trump land','trumpty':'trump','trumpist[s]?':'trump supporter',\n                          'trumpeteer[s]?':'trump supporter','trumplethinskin':'trump','trumpo':'trump',\n                          'realDonaldtrump':'real Donald trump','[Tt]heDonald':'the Donald',\n                          ' brother[Ii]n ':' brother ', 'Beyak':'Canadian politician',\n                          'Trudeaus':'Prime Minister of Canada ','shibai':'failure',\n                          'Tridentinus':'Tridentinum','[Zz]upta[s]?':'the South African President',\n                           '[Pp]utrumpski':'Putin and trump supporter','Twitler':'twitter user',\n                           'antisemitic': 'anti semitic', '[Ss][Bb]91':'senate bill', \n                            'utmterm':' utm term','[Ff]ake[Nn]ews':'fake news', 'Pizzagate':'Pizza gate',                 \n                            '[Ww]ashingtontimes':'washington times','[Gg]arycrum':'gary crum',\n                            'RangerMC':'car','[Tt][Ff][Ww]s':'tuition fee waiver','[Ss][Jj][Ww][Ss]?':'social justice warrior',\n                            'Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you', '[Aa]uwe': 'oh no',\n                            '[Bb]igly':'big league','Drump[f]?':'Trump','[Bb]rexit':'british exit',\n                            'utilitas':'utilities','justiciaries': 'justiciary','doctrne':'doctrine',\n                           '[Dd]eplorables': 'deplorable','[Cc][Oo][Nn]artist' : 'con-artist',\n                           'theglobeandmail': 'the globe and mail', 'howhat': 'how that', ' coz ':' because ',\n                           'civilbeat':'civil beat','gubmit':'submit','financialpost':'financial post',               \n                           'theguardian': 'the guardian','shopo':'shop','SHOPO':'shop','fentayal': 'fentanyl',\n                         'designation-': 'designation ','[Mm]utilitated' : 'Mutilated','dood-': 'dood ',\n                         '[Ii]rakis' : 'iraki', 'supporter[a-z]+':'supporter',' u ':' you ', \n                        }\n\n\n    contraction_mapping = {\n        \"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n        'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n        'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\n        \"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have', 'can;t': 'cannot','can;t;ve': 'cannot have',\n        'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n        \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\n        \"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n        'couldn;t;ve': 'could not have','couldn´t': 'could not', 'couldn´t´ve': 'could not have','couldn’t': 'could not',\n        'couldn’t’ve': 'could not have', 'could´ve': 'could have',\n        'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n        'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n        'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n        \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n        'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not',\n        'hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not',\n        'hasn’t': 'has not', \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not',\n        'haven’t': 'have not',\"he'd\": 'he would', \"he'd've\": 'he would have',\"he'll\": 'he will','he´ll': 'he will',\n        \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',   \n        'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have',    \n        'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\n        \"how'd\": 'how did',\"how'll\": 'how will',\"how's\": 'how is','how,d': 'how did','how,ll': 'how will',\n        'how,s': 'how is','how;d': 'how did','how;ll': 'how will','how;s': 'how is','how´d': 'how did','how´ll': 'how will',\n        'how´s': 'how is','how’d': 'how did','how’ll': 'how will','how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\n        \"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will','i,m': 'i am','i,ve': 'i have','i;d': 'i would',\n        'i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not','isn,t': 'is not','isn;t': 'is not',\n        'isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is', \"it's\": 'it is',\n        'it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will', 'it;s': 'it is',\n        'it´d': 'it would','it´ll': 'it will','it´s': 'it is','it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n        'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n        'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us', 'let’s': 'let us',\n        \"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not', 'mayn;t': 'may not',\n        'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have',\n        'might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not',\n        'mightn;t': 'might not','mightn´t': 'might not', 'mightn’t': 'might not','might´ve': 'might have',\n        'might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n        \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not',\n        'must´ve': 'must have','must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not',\n        'needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n        'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\n        \"shan't\": 'shall not', 'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not',\n        'sha´n´t': 'shall not','sha’n’t': 'shall not',\"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is',\n        'she,d': 'she would','she,ll': 'she will', 'she,s': 'she is','she;d': 'she would','she;ll': 'she will',\n        'she;s': 'she is','she´d': 'she would','she´ll': 'she will', 'she´s': 'she is','she’d': 'she would',\n        'she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have',\n        'should;ve': 'should have', \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not',\n        'shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have', 'should’ve': 'should have',\n        \"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n        'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\n        \"there'd\": 'there had', \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had',\n        'there;s': 'there is', 'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n        \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have','they,d': 'they would',\n        'they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will',\n        'they;re': 'they are', 'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are',\n        'they´ve': 'they have','they’d': 'they would','they’ll': 'they will','they’re': 'they are','they’ve': 'they have',\n        \"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not','wasn’t': 'was not',\n        \"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n        'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n        \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not',\n        'we´d': 'we would','we´ll': 'we will',    'we´re': 'we are','we´ve': 'we have','we’d': 'we would',\n        'we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\n        \"what's\": 'what is',    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is',\n        'what,ve': 'what have','what;ll': 'what will','what;re': 'what are','what;s': 'what is','what;ve': 'what have',\n        'what´ll': 'what will', 'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will',\n        'what’re': 'what are','what’s': 'what is', 'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is',\n        'where,d': 'where did','where,s': 'where is','where;d': 'where did','where;s': 'where is','where´d': 'where did',\n        'where´s': 'where is','where’d': 'where did','where’s': 'where is', \"who'll\": 'who will',\"who's\": 'who is',\n        'who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is','who´ll': 'who will','who´s': 'who is',\n        'who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n        'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not',\n        'wouldn´t': 'would not','wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are',\n        'you,d': 'you would','you,ll': 'you will', 'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n        'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would',\n        'you’ll': 'you will','you’re': 'you are','´cause': 'because','’cause': 'because',\"you've\": \"you have\",\n        \"could'nt\": 'could not',\"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\n        \"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\"who're\": \"who are\",\"who've\": \"who have\",\n        \"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\n        \"y'know\": \"you know\",\"you.i\": \"you i\",\n        \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",\n        'don\"\"t': \"do not\",\"dosen't\": \"does not\", \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\n        \"there'll\": \"there will\",\"there're\": \"there are\", \"this'll\": \"this all\", \"ya'll\": \"you all\",\n        \"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\n        \"dont't\": \"do not\",\"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘i\":'i',  \":)\": ' smile ',\";)\": ' smile ',\n        \":-)\": ' smile ',\":(\": ' sad ','…':'...', '😉': ' wink ', '😂': ' joy ', '😀': ' stuck out tongue ',  \n         }\n\n    contraction_mapping1 ={\n        \"Agains't\": 'against', \"Ain't\": 'am not', 'Ain,t': 'am not', 'Ain;t': 'am not', 'Ain´t': 'am not',\n     'Ain’t': 'am not', \"Aren't\": 'are not', 'Aren,t': 'are not', 'Aren;t': 'are not', 'Aren´t': 'are not',\n     'Aren’t': 'are not', \"Arn't\": 'are not', \"C'mon\": 'common', \"Can't\": 'cannot', \"Can't've\": 'cannot have',\n     'Can,t': 'cannot', 'Can,t,ve': 'cannot have', 'Can;t': 'cannot', 'Can;t;ve': 'cannot have',\n     'Can´t': 'cannot', 'Can´t´ve': 'cannot have', 'Can’t': 'cannot', 'Can’t’ve': 'cannot have',\n     \"Could'nt\": 'could not', \"Could've\": 'could have', 'Could,ve': 'could have', 'Could;ve': 'could have',\n     \"Couldn't\": 'could not', \"Couldn't've\": 'could not have', 'Couldn,t': 'could not',\n     'Couldn,t,ve': 'could not have', 'Couldn;t': 'could not', 'Couldn;t;ve': 'could not have',\n     'Couldn´t': 'could not', 'Couldn´t´ve': 'could not have', 'Couldn’t': 'could not',\n     'Couldn’t’ve': 'could not have', 'Could´ve': 'could have', 'Could’ve': 'could have',\n     \"D'int\": 'did not', \"Did'nt\": 'did not', \"Didn't\": 'did not', 'Didn,t': 'did not',\n     'Didn;t': 'did not', 'Didn´t': 'did not', 'Didn’t': 'did not', \"Din't\": 'did not',\n     \"Doens't\": 'does not', \"Doesn't\": 'does not', 'Doesn,t': 'does not', 'Doesn;t': 'does not',\n     'Doesn´t': 'does not', 'Doesn’t': 'does not', 'Don\"\"t': 'do not', \"Don't\": 'do not',\n     'Don,t': 'do not', 'Don;t': 'do not', \"Dont't\": 'do not', 'Don´t': 'do not',\n     'Don’t': 'do not', \"Dosen't\": 'does not', \"Dosn't\": 'does not', \"Hadn't\": 'had not',\n     \"Hadn't've\": 'had not have', 'Hadn,t': 'had not', 'Hadn,t,ve': 'had not have', 'Hadn;t': 'had not',\n     'Hadn;t;ve': 'had not have', 'Hadn´t': 'had not', 'Hadn´t´ve': 'had not have', 'Hadn’t': 'had not',\n     'Hadn’t’ve': 'had not have', \"Hasn't\": 'has not', 'Hasn,t': 'has not', 'Hasn;t': 'has not',\n     'Hasn´t': 'has not', 'Hasn’t': 'has not', \"Haven't\": 'have not', 'Haven,t': 'have not',\n     'Haven;t': 'have not', 'Haven´t': 'have not', 'Haven’t': 'have not', \"Havn't\": 'have not',\n     \"He'd\": 'he would', \"He'd've\": 'he would have', \"He'll\": 'he will', \"He's\": 'he is',\n     'He,d': 'he would', 'He,d,ve': 'he would have', 'He,ll': 'he will', 'He,s': 'he is',\n     'He;d': 'he would', 'He;d;ve': 'he would have', 'He;ll': 'he will', 'He;s': 'he is',\n     'Here’s': 'here is', 'He´d': 'he would', 'He´d´ve': 'he would have', 'He´ll': 'he will',\n     'He´s': 'he is', 'He’d': 'he would', 'He’d’ve': 'he would have', 'He’ll': 'he will',\n     'He’s': 'he is', \"How'd\": 'how did', \"How'll\": 'how will', \"How's\": 'how is', 'How,d': 'how did',\n     'How,ll': 'how will', 'How,s': 'how is', 'How;d': 'how did', 'How;ll': 'how will', 'How;s': 'how is',\n     'How´d': 'how did', 'How´ll': 'how will', 'How´s': 'how is', 'How’d': 'how did', 'How’ll': 'how will',\n     'How’s': 'how is', 'I\"\"m': 'i am', \"I'am\": 'i am', \"I'd\": 'i would', \"I'l\": 'i will',\n     \"I'll\": 'i will', \"I'm\": 'i am', \"I'ma\": 'i am', \"I'v\": 'i have', \"I've\": 'i have',\n     'I,d': 'i would', 'I,ll': 'i will', 'I,m': 'i am', 'I,ve': 'i have', 'I;d': 'i would',\n     'I;ll': 'i will', 'I;m': 'i am', 'I;ve': 'i have', \"Is'nt\": 'is not', \"Isn't\": 'is not',\n     'Isn,t': 'is not', 'Isn;t': 'is not', 'Isn´t': 'is not', 'Isn’t': 'is not', \"It'd\": 'it would',\n     \"It'll\": 'it will', \"It's\": 'it is', 'It,d': 'it would', 'It,ll': 'it will', 'It,s': 'it is',\n     'It;d': 'it would', 'It;ll': 'it will', 'It;s': 'it is', 'It´d': 'it would', 'It´ll': 'it will',\n     'It´s': 'it is', 'It’d': 'it would', 'It’ll': 'it will', 'It’s': 'it is', 'I´d': 'i would',\n     'I´ll': 'i will', 'I´m': 'i am', 'I´ve': 'i have', 'I’d': 'i would', 'I’ll': 'i will', 'I’m': 'i am',\n     'I’ve': 'i have', \"Let's\": 'let us', 'Let,s': 'let us', 'Let;s': 'let us', 'Let´s': 'let us',\n     'Let’s': 'let us', \"Ma'am\": 'madam', 'Ma,am': 'madam', 'Ma;am': 'madam', \"Mayn't\": 'may not',\n     'Mayn,t': 'may not', 'Mayn;t': 'may not', 'Mayn´t': 'may not', 'Mayn’t': 'may not',\n     'Ma´am': 'madam', 'Ma’am': 'madam', \"Might've\": 'might have', 'Might,ve': 'might have',\n     'Might;ve': 'might have', \"Mightn't\": 'might not', 'Mightn,t': 'might not', 'Mightn;t': 'might not',\n     'Mightn´t': 'might not', 'Mightn’t': 'might not', 'Might´ve': 'might have', 'Might’ve': 'might have',\n     \"Must've\": 'must have', 'Must,ve': 'must have', 'Must;ve': 'must have', \"Mustn't\": 'must not',\n     'Mustn,t': 'must not', 'Mustn;t': 'must not', 'Mustn´t': 'must not', 'Mustn’t': 'must not',\n     'Must´ve': 'must have', 'Must’ve': 'must have', \"Needn't\": 'need not', 'Needn,t': 'need not',\n     'Needn;t': 'need not', 'Needn´t': 'need not', 'Needn’t': 'need not', \"Oughtn't\": 'ought not',\n     'Oughtn,t': 'ought not', 'Oughtn;t': 'ought not', 'Oughtn´t': 'ought not', 'Oughtn’t': 'ought not',\n     \"Sha'n't\": 'shall not', 'Sha,n,t': 'shall not', 'Sha;n;t': 'shall not', \"Shan't\": 'shall not',\n     'Shan,t': 'shall not', 'Shan;t': 'shall not', 'Shan´t': 'shall not', 'Shan’t': 'shall not',\n     'Sha´n´t': 'shall not', 'Sha’n’t': 'shall not', \"She'd\": 'she would', \"She'll\": 'she will',\n     \"She's\": 'she is', 'She,d': 'she would', 'She,ll': 'she will', 'She,s': 'she is', 'She;d': 'she would',\n     'She;ll': 'she will', 'She;s': 'she is', 'She´d': 'she would', 'She´ll': 'she will', 'She´s': 'she is',\n     'She’d': 'she would', 'She’ll': 'she will', 'She’s': 'she is', \"Shoudn't\": 'should not',\n     \"Should've\": 'should have', 'Should,ve': 'should have', 'Should;ve': 'should have',\n     \"Shouldn't\": 'should not', 'Shouldn,t': 'should not', 'Shouldn;t': 'should not',\n     'Shouldn´t': 'should not', 'Shouldn’t': 'should not', 'Should´ve': 'should have',\n     'Should’ve': 'should have', \"That'd\": 'that would', \"That'll\": 'that will',\n     \"That's\": 'that is', 'That,d': 'that would', 'That,s': 'that is', 'That;d': 'that would',\n     'That;s': 'that is', 'That´d': 'that would', 'That´s': 'that is', 'That’d': 'that would',\n     'That’s': 'that is', \"There'd\": 'there had', \"There'll\": 'there will', \"There're\": 'there are',\n     \"There's\": 'there is', 'There,d': 'there had', 'There,s': 'there is', 'There;d': 'there had',\n     'There;s': 'there is', 'There´d': 'there had', 'There´s': 'there is', 'There’d': 'there had',\n     'There’s': 'there is', \"They'd\": 'they would', \"They'll\": 'they will', \"They're\": 'they are',\n     \"They've\": 'they have', 'They,d': 'they would', 'They,ll': 'they will', 'They,re': 'they are',\n     'They,ve': 'they have', 'They;d': 'they would', 'They;ll': 'they will', 'They;re': 'they are',\n     'They;ve': 'they have', 'They´d': 'they would', 'They´ll': 'they will', 'They´re': 'they are',\n     'They´ve': 'they have', 'They’d': 'they would', 'They’ll': 'they will', 'They’re': 'they are',\n     'They’ve': 'they have', \"This'll\": 'this all', \"Wan't\": 'want', \"Was'nt\": 'was not', \"Wasn't\": 'was not',\n     'Wasn,t': 'was not', 'Wasn;t': 'was not', 'Wasn´t': 'was not', 'Wasn’t': 'was not', \"We'd\": 'we would',\n     \"We'll\": 'we will', \"We're\": 'we are', \"We've\": 'we have', 'We,d': 'we would', 'We,ll': 'we will',\n     'We,re': 'we are', 'We,ve': 'we have', 'We;d': 'we would', 'We;ll': 'we will', 'We;re': 'we are',\n     'We;ve': 'we have', \"Weren't\": 'were not', 'Weren,t': 'were not', 'Weren;t': 'were not',\n     'Weren´t': 'were not', 'Weren’t': 'were not', 'We´d': 'we would', 'We´ll': 'we will',\n     'We´re': 'we are', 'We´ve': 'we have', 'We’d': 'we would', 'We’ll': 'we will', 'We’re': 'we are',\n     'We’ve': 'we have', \"What'll\": 'what will', \"What're\": 'what are', \"What's\": 'what is',\n     \"What've\": 'what have', 'What,ll': 'what will', 'What,re': 'what are', 'What,s': 'what is',\n     'What,ve': 'what have', 'What;ll': 'what will', 'What;re': 'what are', 'What;s': 'what is',\n     'What;ve': 'what have', 'What´ll': 'what will', 'What´re': 'what are', 'What´s': 'what is',\n     'What´ve': 'what have', 'What’ll': 'what will', 'What’re': 'what are', 'What’s': 'what is',\n     'What’ve': 'what have', \"Where'd\": 'where did', \"Where's\": 'where is', 'Where,d': 'where did',\n     'Where,s': 'where is', 'Where;d': 'where did', 'Where;s': 'where is', 'Where´d': 'where did',\n     'Where´s': 'where is', 'Where’d': 'where did', 'Where’s': 'where is', \"Who'd\": 'who would',\n     \"Who'll\": 'who will', \"Who're\": 'who are', \"Who's\": 'who is', \"Who've\": 'who have',\n     'Who,ll': 'who will', 'Who,s': 'who is', 'Who;ll': 'who will', 'Who;s': 'who is',\n     'Who´ll': 'who will', 'Who´s': 'who is', 'Who’ll': 'who will', 'Who’s': 'who is',\n     \"Why'd\": 'why would', \"Won't\": 'will not', 'Won,t': 'will not', 'Won;t': 'will not',\n     'Won´t': 'will not', 'Won’t': 'will not', \"Would've\": 'would have', \"Wouldn't\": 'would not',\n     'Wouldn,t': 'would not', 'Wouldn;t': 'would not', 'Wouldn´t': 'would not', 'Wouldn’t': 'would not',\n     \"Y'all\": 'you all', \"Y'know\": 'you know', \"Ya'll\": 'you all', \"You'd\": 'you would', \"You'll\": 'you will',\n     \"You're\": 'you are', \"You've\": 'you have', 'You,d': 'you would', 'You,ll': 'you will', 'You,re': 'you are',\n     'You.i': 'you i', 'You;d': 'you would', 'You;ll': 'you will', 'You;re': 'you are',\n     \"Your'e\": 'you are', 'You´d': 'you would', 'You´ll': 'you will', 'You´re': 'you are',\n     'You’d': 'you would', 'You’ll': 'you will', 'You’re': 'you are', 'You’ve': 'you have'\n    }\n\n    dirty_dict = {      re.compile( '[^a-zA-Z][uU] of [oO][^a-zA-Z]'): ' you of all ',\n                        re.compile('[wW][hH][^a-zA-Z ][^a-zA-Z ][eE]'):'whore' ,                  #  wh**e\n                        re.compile('[wW][hH][^a-zA-Z ][rR][eE]'):'whore',                         #  wh*re   \n                        re.compile('[wW][^a-zA-Z ][oO][rR][eE]'):'whore',                         #  w*ore  \n                        '[wW] h o r e':'whore',\n                      #  re.compile('[sS][hH][^a-zA-Z ][tT] '):'shit ',                         #   sh*t_\n                        re.compile(' [sS][hH][^a-zA-Z ][tT]'):' shit',                         #   _sh*t\n                        re.compile('[sS][hH][*_x][tT] '):'shit ',                            #   sh*t\n                        re.compile(' [sS][^a-zA-Z ][^a-zA-Z ][tT]'):' shit',                   #   _s**t\n                      #  re.compile('[sS][^a-zA-Z ][^a-zA-Z ][tT] '):'shit ',                   #   s**t_\n                        re.compile('[sS][-*_x][-*_x][tT] '):'shit ',                      #   s**t\n                      #  re.compile('[sS][hH][^a-zA-Z ][^a-zA-Z ] '):'shit ',                    #   sh**_ \n                        re.compile(' [sS][hH][^a-zA-Z ][^a-zA-Z ]'):' shit',                     #   _sh** \n                      #  re.compile('[sS][^a-zA-Z ][iI][tT] '):'shit ',                         #   s*it_   \n                        re.compile(' [sS][^a-zA-Z ][iI][tT]'):' shit',                         #   _s*it \n                        re.compile('[sS][-*_x][iI][tT] '):'shit ',                            #   shit\n                        '[sS] h i t':'shit','5h1t': 'shit',\n                        re.compile(' [fF][^a-zA-Z ][^a-zA-Z ][kK]'):' fuck',                   #   _f**k\n                        re.compile('[fF][^a-zA-Z ][^a-zA-Z ][kK] '):'fuck ',                   #   f**k_\n                        re.compile('[fF][-*_x][-*_x][kK]'):'fuck',                       #   f**k\n                        re.compile(' [fF][^a-zA-Z ][cC][kK]'):' fuck',                         #   _f*ck\n                        re.compile('[fF][^a-zA-Z ][cC][kK] '):'fuck ',                         #   f*ck_\n                        re.compile('[fF][-*_x][cC][kK]'):'fuck',                            #   f*ck\n                        re.compile(' [fF][uU][^a-zA-Z ][kK]'):' fuck',                         #   _fu*k\n                        re.compile('[fF][uU][^a-zA-Z ][kK] '):'fuck ',                         #   fu*k_\n                        re.compile('[fF][uU][-*_x][kK]'):'fuck',                            #   fu*k\n                        '[pP]huk': 'fuck','[pP]huck': 'fuck','[fF]ukk':'fuck','[fF] u c k':'fuck',\n                        '[fF]cuk': 'fuck',' [fF]uks': ' fucks',              \n                        re.compile(' [dD][^a-zA-Z ][^a-zA-Z ][kK]'):' dick',                   #   _d**k\n                        re.compile('[dD][^a-zA-Z ][^a-zA-Z ][kK] '):'dick ',                   #   d**k_\n                        re.compile('[dD][-*_x][-*_x][kK]'):'dick',                       #   d**k\n                        re.compile(' [dD][^a-zA-Z ][cC][kK]'):' dick',                         #   _d*ck\n                        re.compile('[dD][^a-zA-Z ][cC][kK] '):'dick ',                         #   d*ck_\n                        re.compile('[dD][-*_x][cC][kK]'):'dick',                            #   d*ck\n                        re.compile(' [dD][iI][^a-zA-Z ][kK]'):' dick',                         #   _di*k\n                        re.compile('[dD][iI][^a-zA-Z ][kK] '):'dick ',                         #   di*k_\n                        re.compile('[dD][iI][-*_x][kK]'):'dick',                            #   di*k\n\n                        re.compile(' [sS][^a-zA-Z ][cC][kK]'):' suck',                         #   _s*ck\n                        re.compile('[sS][^a-zA-Z ][cC][kK] '):'suck ',                         #   s*ck_\n                        re.compile('[sS][-*_x][cC][kK]'):'suck',                            #   s*ck\n                        re.compile(' [sS][uU][^a-zA-Z ][kK]'):' suck',                         #   _su*k\n                        re.compile('[sS][uU][^a-zA-Z ][kK] '):'suck ',                         #   su*k_\n                        re.compile('[sS][uU][-*_x][kK]'):'suck',                            #   su*k\n\n                        re.compile(' [cC][^a-zA-Z ][nN][tT]'):' cunt',                         #   _c*nt\n                        re.compile('[cC][^a-zA-Z ][nN][tT] '):'cunt ',                         #   c*nt_\n                        re.compile('[cC][-*_x][nN][tT]'):'cunt',                            #   c*nt\n                        re.compile(' [cC][uU][^a-zA-Z ][tT]'):' cunt',                         #   _cu*t\n                        re.compile('[cC][uU][^a-zA-Z ][tT] '):'cunt ',                         #   cu*t_\n                        re.compile('[cC][uU][-*_x][tT]'):'cunt',                            #   cu*t\n\n                        re.compile(' [bB][^a-zA-Z ][tT][cC][hH]'):' bitch',                       #   _b*tch\n                        re.compile('[bB][^a-zA-Z ][tT][cC][hH] '):'bitch ',                       #   b*tch_\n                        re.compile('[bB][-*_x][tT][cC][hH]'):'bitch',                          #   b*tch\n                        re.compile(' [bB][iI][^a-zA-Z ][cC][hH]'):' bitch',                       #   _bi*ch\n                        re.compile('[bB][iI][^a-zA-Z ][cC][hH] '):'bitch ',                       #   bi*ch_\n                        re.compile('[bB][iI][-*_x][cC][hH]'):'bitch',                          #   bi*ch\n                        re.compile(' [bB][iI][tT][^a-zA-Z ][hH]'):' bitch',                       #   _bit*h\n                        re.compile('[bB][iI][tT][^a-zA-Z ][hH]'):'bitch ',                       #   bit*h_\n                        re.compile('[bB][iI][tT][-*_x][hH]'):'bitch',                          #   bit*h\n                        re.compile('[bB][^a-zA-Z ][tT][^a-zA-Z ][hH]'):'bitch',                   #   b*t*h\n                        '[bB] i t c h':'bitch',\n                        re.compile('[aA][*_]s'):'ass',                                #   a*s\n                        re.compile('[aA][^a-zA-Z ][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',               #   a**hole\n                        re.compile(' [aA][^a-zA-Z ][^a-zA-Z ][hH]'):' assh',                   #   a**h\n                        re.compile('[aA][^a-zA-Z ][sS][hH][oO][lL][eE]'):'asshole',                     #   a*shole\n                        re.compile('[aA][sS][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',                     #   as*hole\n                        ' [aA]s[*]':' ass','[aA] s s': 'ass ','[aA]sswhole': 'ass hole',\n                        re.compile('[aA]ssh[^a-zA-Z ]le'):'asshole',                     #   assh*le\n                        '[hH] o l e':'hole',\n                        '[bB][*]ll': 'bull', \n                        re.compile('[pP][^a-zA-Z ][sS][sS][yY]'):' pussy',                         #   p*ssy\n                        re.compile('[pP][uU][^a-zA-Z ][sS][yY]'):' pussy',                         #   pu*sy\n                        re.compile('[pP][uU][sS][^a-zA-Z ][yY]'):' pussy',                         #   pus*y\n                        re.compile('[pP][uU][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',                   #   pu**y\n                        re.compile('[pP][^a-zA-Z ][^a-zA-Z ][sS][yY]'):' pussy',                   #   p**sy\n                        re.compile(' [pP][^a-zA-Z ][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',            #   _pussy\n                        '[pP]ussi': 'pussy', '[pP]ussies': 'pussy','[pP]ussys': 'pussy', \n                        '[jJ]ack[-]off': 'jerk off','[mM]asterbat[*]': 'masterbate','[gG]od[-]dam': 'god damm',\n                }\n    from nltk.tokenize.treebank import TreebankWordTokenizer\n    tokenizer2 = TreebankWordTokenizer()\n\n    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n    def pre_clean_abbr_words(x):\n        dic = abbr_mapping\n        for word in dic.keys():\n            #if word in x:\n            x = x.replace(word, dic[word])\n        return x\n\n    def clean_regex_words(x):\n        dic = regex_mapping\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n    def correct_contraction(x):\n        dic = contraction_mapping\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def correct_contraction1(x):\n        dic = contraction_mapping1\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def clean_dirty_dict(x):\n        dic = dirty_dict\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n\n    def handle_punctuation(x):\n        x = x.translate(remove_dict)\n        x = x.translate(isolate_dict)\n        return x\n\n    def new_final_contraction(x):\n        dic = new_final_mapping\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x \n\n    def handle_contractions(x):\n        x = tokenizer2.tokenize(x)\n        return x\n\n    def fix_quote(x):\n        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n        x = ' '.join(x)\n        return x\n\n    def preprocess(df_comment):\n\n        # lower\n        # clean misspellings\n        #df_comment = df_comment.str.lower()\n        df_comment = df_comment.apply(pre_clean_abbr_words)\n        df_comment = df_comment.apply(clean_regex_words)\n\n        df_comment = df_comment.apply(correct_contraction) \n        df_comment = df_comment.apply(correct_contraction1) \n        df_comment = df_comment.apply(clean_dirty_dict)\n\n        # clean the text\n        df_comment = df_comment.apply(lambda x:handle_punctuation(x))\n        df_comment = df_comment.apply(new_final_contraction)\n\n        df_comment = df_comment.apply(handle_contractions)\n        df_comment = df_comment.apply(fix_quote)\n\n        return df_comment\n    \n    print('bert_based_cased')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/cased_l-12_h-768_a-12/cased_L-12_H-768_A-12/' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=False)\n    print('build tokenizer done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    #low\n    model2.load_weights('../input/final-models-group1/bertcased_pre_220_95089.h5')\n    print('load ba models cased')\n    eval_lines = (preprocess(test_df['comment_text'])).values\n    token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n    print(token_input2[:3])\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_cased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_cased_low.csv', index=False)\n    \n    #high\n    model2.load_weights('../input/final-model-group2/bertcased_pre_220_95108_ep2.h5')\n    print('load ba models cased')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_cased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_cased_high.csv', index=False)\n    \n    \n    K.clear_session()\nbert_get_result()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# gpt2"},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r '../input/keras-gpt-2-latest/keras_gpt_2_latest/' '/kaggle/working'\nprint('gpt2')\nfrom keras_gpt_2_latest.keras_gpt_2.loader import load_trained_model_from_checkpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bsz = 128\nmaxlen=300\n#model_folder = '../input/gpt2-models/'\nconfig_path = '../input/gpt2hparamsjson/hparams.json'#os.path.join(model_folder, 'hparams.json')\ncheckpoint_path = 'anything you like, can be a meme.' #os.path.join(model_folder, 'model.ckpt')#can be anything\nmodel = load_trained_model_from_checkpoint(config_path,\n                                           checkpoint_path,\n                                           seq_len=maxlen,\n                                           fixed_input_shape=True)\nsequence_output  = model.get_layer(index=-2).output\nmaxpool_output = keras.layers.GlobalMaxPooling1D()(sequence_output)\navgpool_output = keras.layers.GlobalAveragePooling1D()(sequence_output)\nconc_output = keras.layers.concatenate([maxpool_output,avgpool_output])\ndropout_output = keras.layers.Dropout(0.4)(conc_output)\nreal_output = keras.layers.Dense(1,activation='sigmoid',name='real_output')(dropout_output)\naux_output = keras.layers.Dense(6,activation='sigmoid',name='aux_output')(dropout_output)\nmodel2  = keras.models.Model(inputs=model.input, outputs=[real_output,aux_output])\n\n##tokenizing\nfrom pytorch_pretrained_bert import BertTokenizer, GPT2Tokenizer\nimport sys\nimport regex as re\ncsv_file = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv' # train or test\ndf = pd.read_csv(csv_file)#[:1024]#.sample(512*2,random_state=112)\ndf['comment_text'] = df['comment_text'].astype(str)\ndf[\"comment_text\"] = df[\"comment_text\"].fillna(\"DUMMY_VALUE\")\n\ndef tokenize(self, text):\n    \"\"\" Tokenize a string. \"\"\"\n    bpe_tokens = []\n    for token in re.findall(self.pat, text):\n        # token = ''.join(self.byte_encoder[ord(b)] for b in token.encode('utf-8'))\n        if sys.version_info[0] == 2:\n            token = ''.join(self.byte_encoder[ord(b)] for b in token)\n        else:\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n        bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(' '))\n    return bpe_tokens\n\ndef convert_lines_gpt2(example, max_seq_length, tokenizer):\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        text = re.sub('[ ]+',' ',text)\n        tokens_a = tokenizer.tokenize(tokenizer, text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:int(max_seq_length/2)] + tokens_a[-int(max_seq_length/2):]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef extract_data_gpt2(\n    model_path,\n    csv_file,\n    dataset,\n    max_sequence_length,\n    output_path,\n):\n    os.makedirs(output_path, exist_ok=True)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_path, cache_dir=None)\n    tokenizer.tokenize = tokenize\n    sequences = convert_lines_gpt2(df[\"comment_text\"].values, max_sequence_length, tokenizer)\n    return sequences\n\nmodel_path = '../input/gpt2-models'\ntoken_input2 = extract_data_gpt2(model_path,csv_file,dataset='gpt2',max_sequence_length=maxlen,output_path=' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.load_weights('../input/gpt2-raw-300-tk0-95217-ep2/gpt2_raw_300_tk0_95217_ep2.h5')\nhehe_model4 = (model2.predict(token_input2,verbose=1,batch_size=bsz))[0]\nprint('gpt2',hehe_model4[:5])\nsubmission = pd.DataFrame.from_dict({\n'id': df['id'],\n'prediction': hehe_model4.flatten()\n})\nsubmission.to_csv('submission_gpt2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nK.clear_session()\ngc.collect()\n%reset -sf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# all"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nresult_bert_df_9 = pd.read_csv('submission_bertlarge.csv')\nresult_bert_9 = result_bert_df_9['prediction'].values.flatten()\nresult_bert_df_6 = pd.read_csv('submission_bertbase_low.csv')\nresult_bert_6 = result_bert_df_6['prediction'].values.flatten()\nresult_bert_df_7 = pd.read_csv('submission_bertbase_high.csv')\nresult_bert_7 = result_bert_df_7['prediction'].values.flatten()\nresult_bert_df_8 = pd.read_csv('submission_bertbase_cased_low.csv')\nresult_bert_8 = result_bert_df_8['prediction'].values.flatten()\nresult_bert_df_5 = pd.read_csv('submission_bertbase_cased_high.csv')\nresult_bert_5 = result_bert_df_5['prediction'].values.flatten()\nresult_gpt2_df = pd.read_csv('submission_gpt2.csv')\nresult_gpt2 = result_gpt2_df['prediction'].values.flatten()\n\nresult_ensemble = (result_bert_9+result_bert_6+result_bert_7+result_bert_8+result_bert_5+result_gpt2)/6.\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': result_ensemble\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}