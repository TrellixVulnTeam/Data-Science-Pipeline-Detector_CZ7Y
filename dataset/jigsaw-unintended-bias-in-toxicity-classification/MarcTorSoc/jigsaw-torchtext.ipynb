{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Features**: (see original kernel at https://www.kaggle.com/leighplt/pytorch-torchtext-glove)\n1. Use cached embedding\n2. Load only selected columns of train\n3. Decrease consumed RAM (use numerized text)\n4. DataFrameDataset for torchtext"},{"metadata":{},"cell_type":"markdown","source":"TODO:\n* ($\\checkmark$) Use logger \n* ($\\checkmark$) Try more epochs (increased to 20 and 2 folds, still NaN accuracy) \n* ($\\checkmark$) Use more data: increase to 10k train, 2 folds, and 50k vocab size. \n  * AUC in train: 93% after 10 epochs\n  * AUC in val: 85% after 10 epochs\n* ($\\checkmark$)Understand why we need all these classes, and explain in code \n* ($\\checkmark$)Use spacy to tokenize --> seems to be a lot slower, maybe not worth it \n* Preprocess using contractions\n* Add patience / early stopping\n* Try a better model\n* Try better embeddings (ELMo, GPT, BERT)\n\nExtras:\n* Separate testing from train_predict function. If any ensembling, play with the seeds, not mixing with K-fold CV\n* Dynamic plot? see notebook in coding_exercises\n* Use spacy for the vocab management --> it seems like spaCy has a pre-loaded vocab, so maybe better to have just a vocab for the words seen. Also that part is not that slow\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Commit history\n\n* v8: took 4h? maybe because batch_size too small\n* v9: trying batch_size=512, vocab_size=500k again, and trying new logs using echo\n* v10: just added a log to start, just to check that Kernel log shows log messages\n* v11: os.system(f\"echo {now} {msg}\")"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\nimport logging\nlog = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nimport numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport os, re, gc, random, tqdm\n \nfrom nltk.tokenize import TweetTokenizer\nfrom collections import Counter\n\n#import spacy\n#nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\"])\n\n#from sklearn.model_selection import StratifiedKFold\n#from sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchtext\nfrom torchtext import vocab, data\nfrom torchtext.data import Field, BucketIterator, Dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Declare logging function\n\nNote: ideally we would use the logging library in Python but it does not show the messages on the log in Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nimport os\ndef log(msg):\n    now = pd.to_datetime(datetime.now()).round(\"s\")\n    print(f\"{now} {msg}\")\n    os.system(f\"echo {now} {msg}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log(\"Start!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9ae58b80e7a871ca069fcefdce83d24c43948e5","trusted":false},"cell_type":"code","source":"vocab.tqdm = tqdm.tqdm_notebook # Replace tqdm to tqdm_notebook in module torchtext","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7c04a817b5ba68498dc9b30638605da891ba6c4","trusted":false},"cell_type":"code","source":"# if True: \n#     path = Path('../../../Documents/Marc/datasets/jigsaw-unintended-bias-in-toxicity-classification/')\n#     emb_path = Path('../../../Documents/Marc/datasets/language_models')\n# else:\n#     path = Path('../../../jigsaw-unintended-bias-in-toxicity-classification/')\n#     emb_path = Path('../../../language_models')\n\npath = Path('../input/jigsaw-unintended-bias-in-toxicity-classification/')\nemb_path = Path('../input/glove840b300dtxt')\n\nemb_cache = './'\nglove_path = emb_path / 'glove.840B.300d.txt'\n\ndevice = 'cuda'\n# preproc hyperparams\nnrows_train = None\nnrows_test = 2 # we can disable test as we are not in competition anymore\nmax_vocab_size = 500000\nmin_vocab_freq = 3 #Â 1\n# model hyperparams\nn_folds = 3\nepochs = 10\nbatch_size = 512\nbidirectional=True\nn_hidden = 64 # try 128?\npos_weight = 2.7\nloss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean', pos_weight=(torch.Tensor([pos_weight])).to(device))\n\ngc.enable();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ddd98901408dbe7c7fb9efdb0ec17cecd511864","trusted":false},"cell_type":"code","source":"# seeds\nseed = 7777\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load csv file with selected columns\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\ncols = ['id', 'comment_text', 'target'] + identity_columns\ndtypes = {'target': np.float16,'comment_text': object,'id': np.int32}\nfor c in identity_columns:\n    dtypes[c] = np.float16\ndf = pd.read_csv(path / 'train.csv', usecols=cols, dtype=dtypes, index_col=[0], nrows=nrows_train)\n\n# Test csv\ntest_df = pd.read_csv(\n    path / 'test.csv', dtype={'comment_text': object,'id': np.int32}, index_col=[0], nrows=nrows_test\n)\n\ntest_df['prediction'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing methods"},{"metadata":{"trusted":false},"cell_type":"code","source":"class RegExCleaner:\n    def __init__(self, expressions=[]):\n        r\"\"\"Create class from compiled expressions: [(re.compile(pattern), repl)]\"\"\"\n        self.expressions = expressions\n\n    @staticmethod\n    def _compile(expressions):\n        regexps = []\n        for pattern, repl in expressions.items():\n            regexps.append((re.compile(pattern), repl))\n        return regexps\n\n    @classmethod\n    def from_dict(cls, custom_dic):\n        r\"\"\"Create class from dictionary with flexible patterns {pattern : replacing}\"\"\"\n        return cls(cls._compile(custom_dic))\n\n    @classmethod\n    def from_vocab(cls, vocab):\n        r\"\"\"Create class from vocabulary with fixed patterns {pattern : replacing}\"\"\"\n        pattern = re.compile(\"|\".join(map(re.escape, vocab.keys())))\n        repl = lambda match: vocab[match.group(0)]\n        return cls([(pattern, repl)])\n\n    def __add__(self, b):\n        return RegExCleaner(self.expressions + b.expressions)\n\n    def __call__(self, s):\n        for regex, repl in self.expressions:\n            s = regex.sub(repl, s)\n        return s\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare text field\n\nCleaning + Tokenisation"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm_pandas\ntqdm_pandas.pandas()\n#from preprocessing import RegExCleaner\n\n# TODO: this can be improved using SpaCy, careful strip_handles removes the user names, so\n# we need to add this to the cleaner before using spaCy\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n# TODO: better cleaning / normalisation using contractions dict\ncleaner = RegExCleaner.from_dict({r'https?:/\\/\\S+':r' ',\n                                  r'[^A-Za-z0-9!.,?$\\'\\\"]+':r' '})\ndef preparation(s): \n    s = cleaner(s)\n    return tknzr.tokenize(s)\n\n#df.comment_text = df.comment_text.progress_apply(preparation)\n#test_df.comment_text = test_df.comment_text.progress_apply(preparation)\ndf.comment_text = df.comment_text.apply(preparation)\ntest_df.comment_text = test_df.comment_text.apply(preparation)\n\n# count unique words\ncounter = Counter()\nfor comment in df.comment_text:\n    counter.update(comment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load embeddings + create vocab"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# this is done once, and after that the vectors are compressed to emb_cache, so the loading will\n# go from 5+ min in my modest laptop, to 10 seconds\nvec = vocab.Vectors(glove_path, cache=emb_cache)\nlog(\"Vectors loaded, creating vocab\")\nvocabulary = vocab.Vocab(counter, max_size=max_vocab_size, min_freq=min_vocab_freq, \n                         vectors=vec, specials=['<pad>', '<unk>'])\ntorch.zero_(vocabulary.vectors[1]); # fill <unk> token as 0\n\ndel vec\ngc.collect();\nlog(f\"Embedding vocab size: {vocabulary.vectors.size(0)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# function for transform sentence to sequence of encoded words\nimport tqdm\nfrom torchtext.data import BucketIterator, Dataset, Example\nimport numpy as np\n\n\ndef sentence2numbers(s, vocabulary, fill_as=1):  # fill <unk> token\n    # TODO: this could be more efficient, maybe np.vectorize or pre-allocating?\n    seq = []\n    for w in s:\n        try:\n            seq.append(vocabulary.stoi[w])\n        except KeyError:\n            seq.append(fill_as)\n    return np.array(seq, dtype=np.int32)\n\n\n# Extension of pytorch Dataset to work with DataFrames\nclass DataFrameDataset(Dataset):\n    def __init__(self, df, fields, **kwargs):\n        keys = dict(fields)\n        for n, f in list(keys.items()):\n            if isinstance(n, tuple):\n                keys.update(zip(n, f))\n                del keys[n]\n        keys = keys.keys()\n        examples = []\n        for i, row in tqdm.tqdm_notebook(df.iterrows(), total=len(df)):\n            examples.append(Example.fromlist([row[k] for k in keys], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n\n# A wrapper to split a DataFrameDataset in subsets. This is just a wrapper where we\n# load the dataset once and then change idx to select the fold we're working with\nclass DataFrameDataSubset:\n    def __init__(self, df_ds, idx):\n        # this should an object of type DataFrameDataset\n        self.df_ds = df_ds\n        self.idx = idx\n        self.len = len(idx)\n        self.fields = self.df_ds.fields\n\n    def update(self, idx):\n        self.idx = idx\n        self.len = len(idx)\n\n    def __getitem__(self, idx):\n        return self.df_ds[self.idx[idx]]\n\n    def __len__(self):\n        return self.len\n\n\n# A wrapper for BucketIterator (the equivalent or DataLoader for torchtext). This\n# wrapper defines how to get the info for every batch for train/val vs test\nclass BatchWrapper:\n    def __init__(self, dl, mode=\"train\"):\n        self.dl, self.mode = dl, mode\n\n    def __iter__(self):\n        if self.mode != \"test\":\n            for batch in self.dl:\n                yield (batch.comment_text, batch.target, batch.id)\n        else:\n            for batch in self.dl:\n                yield (batch.comment_text, batch.id)\n\n    def __len__(self):\n        return len(self.dl)\n\n\ndef get_loader_from_dataset(ds, mode=\"train\", **args):\n    return BatchWrapper(BucketIterator(ds, **args), mode)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create fields"},{"metadata":{"_uuid":"9640b7fef754155e0162b2cd1e70cadf94c6ec6c","trusted":false},"cell_type":"code","source":"#from data_preparation import sentence2numbers\nfrom functools import partial\n# define the columns that we want to process and how to process\npad_token = 0 # '<pad>' position token\n\ntxt_field = Field(sequential=True, preprocessing=partial(sentence2numbers, vocabulary=vocabulary),\n                  pad_token=pad_token, use_vocab=False)\nnum_field = Field(sequential=False, dtype=torch.float, use_vocab=False)\nidx_field = Field(sequential=False, dtype=torch.int64, use_vocab=False)\n\ntrain_fields = [\n    ('id', idx_field), \n    ('target', num_field), \n    ('comment_text', txt_field),\n]\ntest_fields = [\n    ('id', idx_field), \n    ('comment_text', txt_field), \n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Datasets"},{"metadata":{"_uuid":"92ea577a18faedfdccf5198d626c5c27861133ee","trusted":false},"cell_type":"code","source":"# for train/val we need we load all data into the DataFrameDataSubset and for each fold we will\n# select a subset of it, then create a loader\n#from data_preparation import DataFrameDataset, DataFrameDataSubset\nds = DataFrameDataset(df.reset_index(), train_fields)\ntest_ds = DataFrameDataset(test_df.reset_index(), test_fields)\ntrain_dss, valid_dss = DataFrameDataSubset(ds, [0]), DataFrameDataSubset(ds, [0])\n\n# for test all data will be used for all folds, so the loader can be created here\n#from data_preparation import get_loader_from_dataset\nte_loader = get_loader_from_dataset(\n    test_ds, mode='test', batch_size=batch_size, device=device,\n    sort_key=lambda x: len(x.comment_text),\n    sort_within_batch=True, shuffle=False, repeat=False\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Â Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass RecNN(nn.Module):\n    def __init__(\n        self, embs_vocab, hidden_size, layers=1, dropout=0.0, bidirectional=False\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.num_layers = layers\n        self.emb = nn.Embedding.from_pretrained(embs_vocab)\n\n        self.line = nn.Linear(embs_vocab.size(1), embs_vocab.size(1))\n\n        self.lstm = nn.LSTM(\n            embs_vocab.size(1),\n            self.hidden_size,\n            num_layers=layers,\n            bidirectional=bidirectional,\n            dropout=dropout,\n        )\n\n        self.gru = nn.GRU(\n            embs_vocab.size(1),\n            self.hidden_size,\n            num_layers=layers,\n            bidirectional=bidirectional,\n            dropout=dropout,\n        )\n\n        self.out = nn.Linear(self.hidden_size * (bidirectional + 1), 32)\n        self.last = nn.Linear(32, 1)\n\n    def forward(self, x):\n\n        embs = self.emb(x)\n        lstm, (h, c) = self.lstm(embs)\n\n        x = F.relu(self.line(embs), inplace=True)\n        gru, h = self.gru(x, h)\n        lstm = lstm + gru\n\n        lstm, _ = lstm.max(dim=0, keepdim=False)\n        out = self.out(lstm)\n        out = self.last(F.relu(out)).squeeze()\n        return out.squeeze(-1)\n\n    @torch.no_grad()\n    def prediction(self, x):\n        score = self.forward(x)\n        return torch.sigmoid(score)\n\n    @torch.no_grad()\n    def evaluate(self, x, y, func):\n        preds = self.forward(x)\n        loss = func(preds.squeeze(-1), y)\n        return preds, loss\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Thanks sakami\n# https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/90527\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\n\nclass JigsawEvaluator:\n    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n        self.y = (y_true >= 0.5).astype(int)\n        self.y_i = (y_identity >= 0.5).astype(int)\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n\n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n\n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average(\n            [\n                self._power_mean(bias_metrics[0]),\n                self._power_mean(bias_metrics[1]),\n                self._power_mean(bias_metrics[2]),\n            ]\n        )\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        return overall_score + bias_score\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":false},"cell_type":"code","source":"import gc\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim import Adam\n#from data_preparation import wrapper\n#from metrics import JigsawEvaluator\n#from models import RecNN\n#from utils import sigmoid\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_predict(\n    train_ds, valid_ds, df, test_df, te_loader, params, vocabulary, loss_fn\n):\n    bs, device, n_hidden, bidir, epochs, identity_columns, n_folds, seed = params\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    y = df.target.values > 0.5\n\n    for train_idx, valid_idx in skf.split(y, y=y):\n\n        train_ds.update(train_idx)\n        valid_ds.update(valid_idx)\n\n        tr_loader = get_loader_from_dataset(\n            train_ds,\n            batch_size=bs,\n            device=device,\n            sort_key=lambda key: len(key.comment_text),\n            sort_within_batch=True,\n            shuffle=True,\n            repeat=False,\n        )\n\n        v_loader = get_loader_from_dataset(\n            valid_ds,\n            batch_size=bs,\n            device=device,\n            sort_key=lambda key: len(key.comment_text),\n            sort_within_batch=True,\n            shuffle=False,\n            repeat=False,\n        )\n\n        model = RecNN(\n            vocabulary.vectors, n_hidden, layers=2, dropout=0.2, bidirectional=bidir\n        ).to(device)\n\n        opt = Adam(\n            params=filter(lambda p: p.requires_grad, model.parameters()),\n            lr=1e-3,\n            betas=(0.75, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n        )\n\n        for epoch in range(epochs):\n            y_true_train = np.empty(0)\n            y_pred_train = np.empty(0)\n            total_loss_train, total_loss_valid = 0, 0\n            model.train()\n            tcids, vcids = [], []\n            for x, target, ids in tr_loader:\n                tcids.append(ids.detach().cpu().numpy())\n                opt.zero_grad()\n                pred = model(x)\n                loss = loss_fn(pred, target)\n                loss.backward()\n                opt.step()\n\n                y_true_train = np.concatenate(\n                    [y_true_train, target.detach().cpu().numpy()], axis=0\n                )\n                y_pred_train = np.concatenate(\n                    [y_pred_train, pred.detach().cpu().numpy()], axis=0\n                )\n                total_loss_train += loss.item()\n\n            # Get prediction for validation part\n            model.eval()\n            y_true_valid, y_pred_valid = np.empty(0), np.empty(0)\n\n            for x, target, ids in v_loader:\n                vcids.append(ids.detach().cpu().numpy())\n                pred, loss = model.evaluate(x, target, loss_fn)\n                total_loss_valid += loss.item()\n\n                y_true_valid = np.concatenate(\n                    [y_true_valid, target.detach().cpu().data.numpy()], axis=0\n                )\n                y_pred_valid = np.concatenate(\n                    [y_pred_valid, pred.cpu().data.numpy()], axis=0\n                )\n\n            tacc = get_accuracy(df, identity_columns, tcids, y_pred_train, y_true_train)\n            vacc = get_accuracy(df, identity_columns, vcids, y_pred_valid, y_true_valid)\n\n            tloss = total_loss_train / len(tr_loader)\n            vloss = total_loss_valid / len(v_loader)\n            log(\n                f\"Epoch {epoch + 1}: Train loss: {tloss:.4f}, BIAS AUC: {tacc:.4f}, \"\n                f\"Valid loss: {vloss:.4f}, BIAS AUC: {vacc:.4f}\"\n            )\n\n        log(\"\\n\")\n        gc.collect()\n        # Get prediction for test set\n        preds = np.empty(0)\n        cids = []\n        for x, ids in te_loader:\n            cids.append(ids.detach().cpu().numpy())\n            pred = model.prediction(x)\n            preds = np.concatenate([preds, pred.detach().cpu().numpy()], axis=0)\n\n        # Save prediction of test to DataFrame\n        cids = [item for sublist in cids for item in sublist]\n        test_df.at[cids, \"prediction\"] = (\n            test_df.loc[cids][\"prediction\"].values + preds / n_folds\n        )\n\n\ndef get_accuracy(df, identity_columns, cids, y_pred, y_true):\n    cids = [item for sublist in cids for item in sublist]\n    scorer = JigsawEvaluator(y_true, df.loc[cids][identity_columns].values)\n    return scorer.get_final_metric(sigmoid(y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e566ecdb5f44cf32af12127f073021566d6e66dd","trusted":false},"cell_type":"code","source":"#from models import RecNN\n#from training import train_predict\n\nparams = (batch_size, device, n_hidden, bidirectional, epochs, identity_columns, n_folds, seed)  \ntrain_predict(train_dss, valid_dss, df, test_df, te_loader, params, vocabulary, loss_fn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c6a41bd21bf783455fd11b63745ed454e0413f0","trusted":false},"cell_type":"code","source":"test_df.to_csv('submission.csv', columns=['prediction'])","execution_count":null,"outputs":[]}],"metadata":{"jupytext":{"formats":"ipynb,py:light"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}