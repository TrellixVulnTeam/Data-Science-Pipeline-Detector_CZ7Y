{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jigsaw Unintended Bias in Toxicity Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1) Importing libraries and datasets","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\nfrom joblib import dump, load\nimport time\n\nsns.set()\n\nKAGGLE_PATH = '/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/'\n\ntrain = pd.read_csv(KAGGLE_PATH + 'train.csv')\ntest = pd.read_csv(KAGGLE_PATH + 'test_private_expanded.csv')\ntest['target'] = test.toxicity\n\n# Make sure all comment_text values are strings\ntrain['comment_text'] = train['comment_text'].astype(str)\ntest['comment_text'] = test['comment_text'].astype(str)\n\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# Convert taget and identity columns to booleans\nfor col in ['target'] + identity_columns:\n    train[col] = np.where(train[col] >= 0.5, True, False)\n    test[col] = np.where(test[col] >= 0.5, True, False)\n    \n# Creating folds (Stratified Split, K = 5)\ntrain[\"kfold\"] = -1\ntrain = train.sample(frac=1).reset_index(drop=True) # shuffle dataframe\ny = train.target.values\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True)\nfor fold_, (train_idx, test_idx) in enumerate(kf.split(X=train, y=y)):\n    train.loc[test_idx, \"kfold\"] = fold_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JigsawEvaluator:\n\n    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n        self.y = (y_true >= 0.5).astype(int)\n        self.y_i = (y_identity >= 0.5).astype(int)\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n\n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n\n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n\n        return overall_score + bias_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fold_i(fold_i, tfv):\n    # Transform tfv object to train, validation and test sets\n    xtrain_tfv = tfv.transform(train[train.kfold!=fold_i].comment_text.values)\n    xvalid_tfv = tfv.transform(train[train.kfold==fold_i].comment_text.values)\n    xtest_tfv = tfv.transform(test.comment_text.values)\n\n    # Fit model\n    model = LogisticRegression(random_state=0, n_jobs=-1, solver='saga')\n    model.fit(xtrain_tfv,train[train.kfold!=fold_i].target.values)\n\n    # Making predictions\n    preds_train = model.predict_proba(xtrain_tfv)[:,1]\n    preds_valid = model.predict_proba(xvalid_tfv)[:,1]\n    preds_test = model.predict_proba(xtest_tfv)[:,1]\n\n    # Calculate bias auc\n    bias_auc_train = JigsawEvaluator(train[train.kfold!=fold_i].target.values, train[train.kfold!=fold_i][identity_columns].values).get_final_metric(preds_train)\n    bias_auc_valid = JigsawEvaluator(train[train.kfold==fold_i].target.values, train[train.kfold==fold_i][identity_columns].values).get_final_metric(preds_valid)\n    bias_auc_test = JigsawEvaluator(test.target.values, test[identity_columns].values).get_final_metric(preds_test)\n\n    auc_train = roc_auc_score(train[train.kfold!=fold_i].target.values, preds_train)\n    auc_valid = roc_auc_score(train[train.kfold==fold_i].target.values, preds_valid)\n    auc_test = roc_auc_score(test.target.values, preds_test)\n\n    # Print results\n    print('Bias AUC | Train: {}'.format(bias_auc_train))\n    print('Bias AUC | Valid: {}'.format(bias_auc_valid))\n    print('Bias AUC |  Test: {}'.format(bias_auc_test))\n\n    return {\n        'auc_train': auc_train,\n        'auc_valid': auc_valid,\n        'auc_test': auc_test,\n        'bias_auc_train': bias_auc_train,\n        'bias_auc_valid': bias_auc_valid,\n        'bias_auc_test': bias_auc_test,\n        'preds_train': preds_train,\n        'preds_valid': preds_valid,\n        'preds_test': preds_test,\n        'model': model\n    }\n\ndef train_all_folds(saved_filename, n, max_feat):\n    \n    print('\\n------------------ START ------------------\\n')\n    \n    start = time.time()\n    \n    print('Start: fitting tf-idf')\n    #tfv = TfidfVectorizer(min_df=5, token_pattern=r'\\w{1,}', ngram_range=(1, n), sublinear_tf=1, stop_words = 'english')\n    tfv = TfidfVectorizer(max_features=max_feat, token_pattern=r'\\w{1,}', ngram_range=(1, n), sublinear_tf=1, stop_words = 'english')\n    tfv.fit(train.comment_text.values)\n    print('End: fitting tf-idf')\n    \n    dict_results = {}\n    for i in [0,1,2,3,4]:\n        print('\\nStart: fitting fold {}'.format(int(i)))\n        results_i = train_fold_i(i, tfv)\n        dict_results['Fold_{}'.format(i)] = results_i\n        print('End: fitting fold {}'.format(int(i)))\n    \n    dict_results['tfv'] = tfv\n    \n    end = time.time()\n    \n    dict_results['time'] = end - start\n    \n    dump(dict_results, saved_filename + \".joblib\")\n    \n    print('\\n------------------ END ------------------\\n')\n        \n    return dict_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all_folds('model_1_gram_none', n=1, max_feat=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all_folds('model_1_gram_75k', n=1, max_feat=75000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all_folds('model_1_gram_50k', n=1, max_feat=50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all_folds('model_1_gram_25k', n=1, max_feat=25000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}