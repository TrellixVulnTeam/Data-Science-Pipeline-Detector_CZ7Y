{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Version 2 + Bug fix - thanks to @chinhuic\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/nvidiaapex/repository/NVIDIA-apex-39e153a\"))\n#print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n#print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n#print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['csrc', 'setup.py', '.gitignore', 'examples', 'LICENSE', 'apex', 'tests', 'docs', 'README.md', '.nojekyll']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Nvidia Apex\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidiaapex/repository/NVIDIA-apex-39e153a","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py:207: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n  cmdoptions.check_install_build_global(options)\nCreated temporary directory: /tmp/pip-ephem-wheel-cache-7_44_wnt\nCreated temporary directory: /tmp/pip-req-tracker-_cr4zjuu\nCreated requirements tracker '/tmp/pip-req-tracker-_cr4zjuu'\nCreated temporary directory: /tmp/pip-install-thatz119\nProcessing /kaggle/input/nvidiaapex/repository/NVIDIA-apex-39e153a\n  Created temporary directory: /tmp/pip-req-build-h5ml_w9t\n  Added file:///kaggle/input/nvidiaapex/repository/NVIDIA-apex-39e153a to build tracker '/tmp/pip-req-tracker-_cr4zjuu'\n    Running setup.py (path:/tmp/pip-req-build-h5ml_w9t/setup.py) egg_info for package from file:///kaggle/input/nvidiaapex/repository/NVIDIA-apex-39e153a\n    Running command python setup.py egg_info\n    torch.__version__  =  1.0.1.post2\n    running egg_info\n    creating pip-egg-info/apex.egg-info\n    writing pip-egg-info/apex.egg-info/PKG-INFO\n    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\n    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\n    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n    reading manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n  Source in /tmp/pip-req-build-h5ml_w9t has version 0.1, which satisfies requirement apex==0.1 from file:///kaggle/input/nvidiaapex/repository/NVIDIA-apex-39e153a\n  Removed apex==0.1 from file:///kaggle/input/nvidiaapex/repository/NVIDIA-apex-39e153a from build tracker '/tmp/pip-req-tracker-_cr4zjuu'\nInstalling collected packages: apex\n  Created temporary directory: /tmp/pip-record-yqmq5jum\n  Running setup.py install for apex ... \u001b[?25l    Running command /opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-req-build-h5ml_w9t/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" --cpp_ext --cuda_ext install --record /tmp/pip-record-yqmq5jum/install-record.txt --single-version-externally-managed --compile\n    torch.__version__  =  1.0.1.post2\n\n    Compiling cuda extensions with\n    nvcc: NVIDIA (R) Cuda compiler driver\n    Copyright (c) 2005-2018 NVIDIA Corporation\n    Built on Sat_Aug_25_21:08:01_CDT_2018\n    Cuda compilation tools, release 10.0, V10.0.130\n    from /usr/local/cuda/bin\n\n    Pytorch binaries were compiled with Cuda 10.0.130\n\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.6\n    creating build/lib.linux-x86_64-3.6/apex\n    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n    creating build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n    creating build/lib.linux-x86_64-3.6/apex/normalization\n    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n    creating build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n    creating build/lib.linux-x86_64-3.6/apex/RNN\n    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n    creating build/lib.linux-x86_64-3.6/apex/optimizers\n    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n    running build_ext\n    building 'apex_C' extension\n    creating build/temp.linux-x86_64-3.6\n    creating build/temp.linux-x86_64-3.6/csrc\n    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/opt/conda/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n    building 'amp_C' extension\n    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n","name":"stdout"},{"output_type":"stream","text":"    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n    building 'fused_adam_cuda' extension\n    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\n    building 'syncbn' extension\n    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n    building 'fused_layer_norm_cuda' extension\n    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/lib/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n    running install_lib\n    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\n    creating /opt/conda/lib/python3.6/site-packages/apex\n    creating /opt/conda/lib/python3.6/site-packages/apex/amp\n    creating /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\n","name":"stdout"},{"output_type":"stream","text":"    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n    creating /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\r\n    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n","name":"stdout"},{"output_type":"stream","text":"    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n    running install_egg_info\n    running egg_info\n    creating apex.egg-info\n    writing apex.egg-info/PKG-INFO\n    writing dependency_links to apex.egg-info/dependency_links.txt\n    writing top-level names to apex.egg-info/top_level.txt\n    writing manifest file 'apex.egg-info/SOURCES.txt'\n    reading manifest file 'apex.egg-info/SOURCES.txt'\n    writing manifest file 'apex.egg-info/SOURCES.txt'\n    Copying apex.egg-info to /opt/conda/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n    running install_scripts\n    writing list of installed files to '/tmp/pip-record-yqmq5jum/install-record.txt'\ndone\n\u001b[?25h  Removing source in /tmp/pip-req-build-h5ml_w9t\nSuccessfully installed apex-0.1\nCleaning up...\nRemoved build tracker '/tmp/pip-req-tracker-_cr4zjuu'\n1 location(s) to search for versions of pip:\n* https://pypi.org/simple/pip/\nGetting page https://pypi.org/simple/pip/\nStarting new HTTPS connection (1): pypi.org:443\nCould not fetch URL https://pypi.org/simple/pip/: connection error: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f47ffa19780>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)) - skipping\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device=torch.device('cuda')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nEPOCHS = 1\nData_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\"\nnum_to_load=950000                         #Train size to match time limit\nvalid_size= 100000                          #Validation Size\nTOXICITY_COLUMN = 'target'","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the Bart Pytorch repo to the PATH\n# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5201: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translate model from tensorflow to pytorch\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":7,"outputs":[{"output_type":"stream","text":"Building PyTorch model from configuration: {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\nConverting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/bert_model.ckpt\nLoading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\nLoading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\nLoading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\nLoading TF weight bert/embeddings/word_embeddings with shape [30522, 1024]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/pooler/dense/bias with shape [1024]\nLoading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\nLoading TF weight cls/predictions/transform/dense/bias with shape [1024]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../working/pytorch_model.bin\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'../working/bert_config.json'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../working\")","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"['.ipynb_checkpoints',\n 'pytorch_model.bin',\n 'bert_config.json',\n '__notebook_source__.ipynb']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/bert-pretrained-models/multi_cased_l-12_h-768_a-12/multi_cased_L-12_H-768_A-12","execution_count":9,"outputs":[{"output_type":"stream","text":"bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\r\nbert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the Bert configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig('../input/bert-pretrained-models/multi_cased_l-12_h-768_a-12/multi_cased_L-12_H-768_A-12/'+'bert_config.json')\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-pretrained-models/multi_cased_l-12_h-768_a-12/multi_cased_L-12_H-768_A-12'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=False)\ntrain_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(1150000, random_state=1234)\nprint('loaded %d records' % len(train_df))\n\n# Make sure all comment_text values are strings\ntrain_df['comment_text'] = train_df['comment_text'].astype(str) \n\nsequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\ntrain_df=train_df.fillna(0)\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ny_columns=['target','severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntrain_df = train_df.drop(['comment_text'],axis=1)\n# convert target to 0,1\ntrain_df[y_columns]=(train_df[y_columns]>=0.5).astype(float)","execution_count":21,"outputs":[{"output_type":"stream","text":"loaded 100000 records\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060fa34be61042debc35db75cd196df0"}},"metadata":{}},{"output_type":"stream","text":"5248\nCPU times: user 1min 55s, sys: 1.53 s, total: 1min 56s\nWall time: 1min 55s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall\nweights = np.ones((len(train_df),)) / 4\n# Subgroup\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\nloss_weight = 1.0 / weights.mean()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = F.binary_cross_entropy_with_logits(data[:,:1],targets[:,:1],weight=targets[:,1:2])\n    bce_loss_2 = F.binary_cross_entropy_with_logits(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = sequences[:num_to_load]                \ny = train_df['target'].values[:num_to_load]\ny = np.vstack([y.astype(np.int),weights[:num_to_load]]).T\ny_aux_train = train_df[[ 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']][:num_to_load]\ny = np.hstack([y, y_aux_train])\nX_val = sequences[num_to_load:]                \ny_val = train_df[y_columns].values[num_to_load:]\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:, 2:].shape","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"(100000, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=train_df.tail(valid_size).copy()\ntrain_df=train_df.head(num_to_load)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[['id']].to_csv(\"test_ids.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_model_file = \"bert_pytorch.bin\"\n\nlr=2e-5\nbatch_size = 32\naccumulation_steps=2\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nmodel = BertForSequenceClassification.from_pretrained(\"../working\",cache_dir=None,num_labels=len(y_columns))\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()\n\ntq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n    for i,(x_batch, y_batch) in tk0:\n#        optimizer.zero_grad()\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        loss = custom_loss(y_pred,y_batch.to(device))\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n\ntorch.save(model.state_dict(), output_model_file)\n","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"<torch._C.Generator at 0x7fa1d3b2ae50>"},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-e84e188bdf68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../working\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mparam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LayerNorm.bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LayerNorm.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run validation\n# The following 2 lines are not needed but show how to download the model for prediction\nmodel = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\nmodel.load_state_dict(torch.load(output_model_file ))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.eval()\nvalid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_siaze=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From baseline kernel\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nMODEL_NAME = 'model1'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\nbias_metrics_df\nget_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}