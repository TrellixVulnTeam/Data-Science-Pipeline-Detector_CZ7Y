{"cells":[{"metadata":{"colab_type":"text","id":"p7AD0CE2Aptg"},"cell_type":"markdown","source":"<p style=\"font-size:36px;text-align:center\"> <b>Jigsaw Unintended Bias in Toxicity Classification</b> </p>"},{"metadata":{"colab_type":"text","id":"hQBCy93KApth"},"cell_type":"markdown","source":"## 1. Business Problem"},{"metadata":{"colab_type":"text","id":"KAgKxyynApti"},"cell_type":"markdown","source":"### Source : https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n### Problem Statement : \n#### Classify the given comments based on toxicity level. If target > .5 label as 1 (Toxic) else 0 ( Intoxic )"},{"metadata":{"colab_type":"text","id":"wY6wd-heAptj"},"cell_type":"markdown","source":"### 1.1 Background"},{"metadata":{"colab_type":"text","id":"92NMd1QSAptk"},"cell_type":"markdown","source":"\nAt the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nIn the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).\n\nThe data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n\nsevere_toxicity\nobscene\nthreat\ninsult\nidentity_attack\nsexual_explicit\nAdditionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold.\n\nmale\nfemale\ntransgender\nother_gender\nheterosexual\nhomosexual_gay_or_lesbian\nbisexual\nother_sexual_orientation\nchristian\njewish\nmuslim\nhindu\nbuddhist\natheist\nother_religion\nblack\nwhite\nasian\nlatino\nother_race_or_ethnicity\nphysical_disability\nintellectual_or_learning_disability\npsychiatric_or_mental_illness\nother_disability\nNote that the data contains different comments that can have the exact same text. Different comments that have the same text may have been labeled with different targets or subgroups."},{"metadata":{"colab_type":"text","id":"hGE5zheAAptk"},"cell_type":"markdown","source":"### 1.2. Real-world/Business objectives and constraints."},{"metadata":{"colab_type":"text","id":"XCeC91TfAptl"},"cell_type":"markdown","source":"* No low-latency requirement.\n* Interpretability is important.\n* Sentiment Analysis of the comments has to be done.\n* Probability of a data-point belonging to each class is needed."},{"metadata":{"colab_type":"text","id":"-OmT5Z0MAptm"},"cell_type":"markdown","source":"# 2. Machine Learning Problem Formulation"},{"metadata":{"colab_type":"text","id":"ZnyXhjN4Aptn"},"cell_type":"markdown","source":"## 2.1. Data"},{"metadata":{"colab_type":"text","id":"Lf54Xk0tApto"},"cell_type":"markdown","source":"### 2.1.1. Data Overview"},{"metadata":{"colab_type":"text","id":"oZh3kVOfApto"},"cell_type":"markdown","source":"* We have two data files: In the data supplied for problem, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data.\n* Data file's information:\n\n* train.csv : ('id', 'target', 'comment_text', 'severe_toxicity', 'obscene','identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability',      'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count')\n\n* test.csv : ('id', 'comment_text')"},{"metadata":{"colab_type":"text","id":"vR974ry4Aptv"},"cell_type":"markdown","source":"# 3) Exploratory Data Analysis"},{"metadata":{"colab_type":"text","id":"UwWTxOQ5Aptv"},"cell_type":"markdown","source":"### 3.1) Reading train and test dataset"},{"metadata":{"colab":{},"colab_type":"code","id":"VyKRsLb7Aptw","trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"colab_type":"code","executionInfo":{"elapsed":15252,"status":"ok","timestamp":1564771163399,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"bhgoNz64Aptz","outputId":"283afe2e-1fc3-466c-cd54-732c5da964ea","trusted":true},"cell_type":"code","source":"# Loading the train data into pandas dataframe\n# df_train = pd.read_csv('drive/My Drive/Jigsaw-CaseStudy/train.csv')\ndf_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n\n# We have 1.8 millions of data record in train dataset with 45 features given\nprint(\"Train dataframe shape:\", df_train.shape)\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"colab_type":"code","executionInfo":{"elapsed":15219,"status":"ok","timestamp":1564771163400,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"GqHp_BdCApt2","outputId":"bc065555-7b07-46e8-a4e2-edc89f63c3f2","trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"colab_type":"code","executionInfo":{"elapsed":15200,"status":"ok","timestamp":1564771163401,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"vx3sGjBTApt5","outputId":"2b900b94-45cd-479e-a612-6b9f9bb31d83","trusted":true},"cell_type":"code","source":"# Loading the test data into pandas dataframe\n# df_test = pd.read_csv('drive/My Drive/Jigsaw-CaseStudy/test.csv')\ndf_test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\n# We have 10k of data record in test dataset \nprint(\"Test dataframe shape:\", df_test.shape)\ndf_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"v5DHbSlCApt7"},"cell_type":"markdown","source":"### 3.2) Processing required columns from train dataset"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":15500,"status":"ok","timestamp":1564771163720,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"uhxg0kyvApt8","outputId":"981cd4a1-3618-43e8-f3f0-448c1d1cf25c","trusted":true},"cell_type":"code","source":"# For our case study we will focus on the comment_text data and rest of the columns we will ignore\n\ndf_train = df_train[['id','target','comment_text']]\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"RwWSmNiVApt-"},"cell_type":"markdown","source":"### 3.3) Preparing the Y label for train dataset"},{"metadata":{"colab":{},"colab_type":"code","id":"hRarrYVoApt_","trusted":true},"cell_type":"code","source":"# Function to assign the binary class as y label\n\ndef assign_class(target):\n    if target >= .5:\n        return 1\n    else: \n        return 0","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":38059,"status":"ok","timestamp":1564771186325,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"9Khyq5KiApuC","outputId":"60cdb8da-1634-491f-ebb7-50d2751be816","trusted":true},"cell_type":"code","source":"# we will create binary class column which will be our Y label\n\ndf_train['class'] = df_train.apply(lambda x: assign_class(x['target']), axis= 1)\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"colab_type":"code","executionInfo":{"elapsed":38033,"status":"ok","timestamp":1564771186327,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"Jl3yHTjEG5W4","outputId":"bf0ea61c-4b64-485b-d4b8-00dc4840acda","trusted":true},"cell_type":"code","source":"# Total number of points with class 1 = 1,44,334 lakh\n\nprint(\"\\nTotal number of points in both classes:\")\ndf_train['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"1I-y-PrqApuE"},"cell_type":"markdown","source":"### 3.4) Univariate analysis on target column"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"colab_type":"code","executionInfo":{"elapsed":1125,"status":"ok","timestamp":1564773300348,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"BBL3cALmApuF","outputId":"d47c0f87-45ad-4ca9-a516-ef1b47b63dfe","trusted":true},"cell_type":"code","source":"# EDA on target variable\n\nfig = plt.figure(figsize=(10,10))\ndf_train.hist(column='target')\nplt.xlabel(\"Target/Toxicity level\")\nplt.ylabel(\"No of comments\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"66w48JUiApuI"},"cell_type":"markdown","source":"#####  Conclusion : The train dataset given is highly imbalanced. there is less comments with toxicity > .5"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"colab_type":"code","executionInfo":{"elapsed":1218,"status":"ok","timestamp":1564773305255,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"Qv5TJnrwApuI","outputId":"6ca79e37-1c70-468d-dc6b-f3c4bc0bad7c","trusted":true},"cell_type":"code","source":"# EDA on class variable\n\nfig = plt.figure(figsize=(10,10))\ndf_train.hist(column='class')\nplt.xlabel(\"Class\")\nplt.ylabel(\"No of comments\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5) Sampling the Train and Test data for our analysis"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"colab_type":"code","executionInfo":{"elapsed":1426,"status":"ok","timestamp":1564773284911,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"uxboMswAHRZ3","outputId":"adb39706-aa84-48d9-c33a-326263c29f2f","trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Sampling the data such that both classes have equal number of datapoints in train dataset.\n\n# https://stackoverflow.com/questions/56191448/sample-pandas-dataframe-based-on-values-in-column\n\ndf_train_sampled = df_train.groupby('class').apply(lambda x: x.sample(n=70000)).reset_index(drop = True)\n\nprint('\\n Number of datapoints in each class :\\n')\nprint(df_train_sampled['class'].value_counts())\n\nprint(\"\\n The shape of train data is : \",df_train_sampled.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":1250,"status":"ok","timestamp":1564773289315,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"qkkvsOB9bx9t","outputId":"b768e4de-d164-49f2-ca6c-0cc2a587ab68","trusted":true},"cell_type":"code","source":"df_train_sampled.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"RU6Ex1iAApuL"},"cell_type":"markdown","source":"## 4) Pre-processing train and test Commnet text data"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":39217,"status":"ok","timestamp":1564771187650,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"FQ2eqGQyApuM","outputId":"3d07930f-9496-44a3-89c4-a023dbed7c4a","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\n\ndef pre_process_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n    \n    ## Clean the text\n    text = re.sub(\"[^a-zA-Z0-9\\n]\", \" \", text) # removing special characters    \n    text = re.sub(\"what's\", \"what is \", text) # decontracting the phrase    \n    text = re.sub(\"\\'ve\", \" have \", text) \n    text = re.sub(\"n't\", \" not \", text)\n    text = re.sub(\"i'm\", \"i am \", text)\n    text = re.sub(\"\\'re\", \" are \", text)\n    text = re.sub(\"\\'d\", \" would \", text)\n    text = re.sub(\"\\'ll\", \" will \", text)\n    text = re.sub(\"[.!#?]\",\" \", text)        \n    text = re.sub(\"\\s+\",\" \", text) # replace multiple spaces with single space\n    \n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Apply the preprocess text function to the train and test dataset\n\n# Train dataset\n\ndf_train_sampled.comment_text.fillna(\" \",inplace=True)\n\ndf_train_sampled['comment_text'] = df_train_sampled.comment_text.apply(lambda x: pre_process_text(x))\n    \n# Test dataset\n\ndf_test.comment_text.fillna(\" \",inplace=True)\n\ndf_test['comment_text'] = df_test.comment_text.apply(lambda x: pre_process_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sampled.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topic modeling ( Unsupervised Clustering Method )\n#### 1) LDA ( Latent Dirichlet Allocation ) is an unsupervised machine-learning model that automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n#### 2) we will model our comment_text into 5 different topics and then take these topics as features in train and test data for our supervised classification model."},{"metadata":{},"cell_type":"markdown","source":"### 4.1) Tokenize word and clean up text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# References for Topic Modeling : \n# 1) https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n# 2) https://nlpforhackers.io/topic-modeling/\n# 3) https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n\ndata = df_train_sampled.comment_text.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2) Create the Dictionary and Corpus needed for Topic Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora\n\n# Create Dictionary\ndictionary = corpora.Dictionary(data_words)\n\n# Create Corpus\ntexts = data_words\n\n# Term Document Frequency\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Human readable format of corpus (term-frequency)\n[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3) Building topic model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom gensim.models import LdaModel\n\n# Build LDA model\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100, update_every=1,\n                                           chunksize=100, passes=10, alpha='auto', per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import CoherenceModel\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the Keyword in the 5 topics\n\nprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the topics on train\n\nimport pyLDAvis\nimport pyLDAvis.gensim \nimport matplotlib.pyplot as plt\n%matplotlib inline\n \npyLDAvis.enable_notebook()\n\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\nvis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4) Testing model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_test.comment_text.values.tolist()\ntest_words = list(sent_to_words(data)) # corpus\n\nprint(test_words[:1])\n\n# Term Document Frequency\ntest_corpus = [dictionary.doc2bow(text) for text in test_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, test_corpus, dictionary)\nvis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5) Converting Topics to Feature Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For train vectors\n\ntrain_vecs = []\n\nfor i in range(len(df_train_sampled)):\n    top_train_topics = lda_model.get_document_topics(corpus[i], minimum_probability=0.0)\n    topic_train_vec = [top_train_topics[i][1] for i in range(5)]\n    train_vecs.append(topic_train_vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing top five train vectors\ntrain_vecs[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For test vectors\n\ntest_vecs = []\n\nfor i in range(len(df_test)):\n    top_test_topics = lda_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n    topic_test_vec = [top_test_topics[i][1] for i in range(5)]\n    test_vecs.append(topic_test_vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing top five test vectors\ntest_vecs[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6) Merging the topic vectors into train and test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the new df with 5 topics and then merge with the original train and test dataframe\n\ndf1 = pd.DataFrame(train_vecs,columns=['Topic-1','Topic-2','Topic-3','Topic-4','Topic-5'])\ndf_train_sampled = pd.concat([df_train_sampled, df1], axis=1, join='inner')\n\ndf2 = pd.DataFrame(test_vecs,columns=['Topic-1','Topic-2','Topic-3','Topic-4','Topic-5'])\ndf_test = pd.concat([df_test, df2], axis=1, join='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sampled.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Ein1wyteApuZ"},"cell_type":"markdown","source":"## 5) Feature Engineering "},{"metadata":{},"cell_type":"markdown","source":"### 5.1) Calculating sentiment score of comments in train and test set"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"colab_type":"code","executionInfo":{"elapsed":7010,"status":"ok","timestamp":1564774014731,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"WZecmwc9Apua","outputId":"848d021b-d33d-41ab-9cc3-4010cb38bd91","trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom tqdm import tqdm\nimport nltk\nnltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\n# ******************************************* Sentiment analysis for train data\ntext_NegScore = []\ntext_NeuScore = []\ntext_PosScore = []\ntext_compoundScore = []\n\nfor stmt in tqdm(df_train_sampled.comment_text.values):\n    \n    ss = sid.polarity_scores(str(stmt))\n    text_NegScore.append(ss[\"neg\"])\n    text_NeuScore.append(ss[\"neu\"])\n    text_PosScore.append(ss[\"pos\"])\n    text_compoundScore.append(ss[\"compound\"])\n\n# Assigning the score to the train text\ndf_train_sampled['text_NegScore'] = text_NegScore\ndf_train_sampled['text_NeuScore'] = text_NeuScore\ndf_train_sampled['text_PosScore'] = text_PosScore\ndf_train_sampled['text_compoundScore'] = text_compoundScore\n\n# ******************************************** Sentiment analysis for test data\ntext_NegScore = []\ntext_NeuScore = []\ntext_PosScore = []\ntext_compoundScore = []\n    \nfor stmt in tqdm(df_test.comment_text.values):\n    \n    ss = sid.polarity_scores(str(stmt))\n    text_NegScore.append(ss[\"neg\"])\n    text_NeuScore.append(ss[\"neu\"])\n    text_PosScore.append(ss[\"pos\"])\n    text_compoundScore.append(ss[\"compound\"])\n\n# Assigning the score to the test text data\ndf_test['text_NegScore'] = text_NegScore\ndf_test['text_NeuScore'] = text_NeuScore\ndf_test['text_PosScore'] = text_PosScore\ndf_test['text_compoundScore'] = text_compoundScore","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":1250,"status":"ok","timestamp":1564774099613,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"1xtCCpqvApue","outputId":"fcdb2497-0593-41bb-87d5-c0e7e1da9e52","trusted":true},"cell_type":"code","source":"df_train_sampled.head(5)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":916,"status":"ok","timestamp":1564774108412,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"4D-UqmkDiUFf","outputId":"3cf923c1-15a4-49fa-8c1f-0a4ae8a26c08","trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2) Calculating length of the text comment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of the length if comment for train and test dataset\n\nWrdCnt_trn = []; # the avg-w2v for each sentence/review is stored in this list\nWrdCnt_test = [];\n\ndf_train_sampled.comment_text.fillna(\" \",inplace=True)\ndf_test.comment_text.fillna(\" \",inplace=True)\n\n# For train data\nfor sentence in tqdm(df_train_sampled['comment_text'].values): # for each review/sentence\n    cnt_words = 0; # num of words with a valid vector in the sentence/review\n    cnt_words = sentence.split() # for each word in a review/sentence\n    WrdCnt_trn.append(len(cnt_words))\n    \n# For test data\nfor sentence in tqdm(df_test['comment_text'].values): # for each review/sentence\n    cnt_words = 0; # num of words with a valid vector in the sentence/review\n    cnt_words = sentence.split() # for each word in a review/sentence\n    WrdCnt_test.append(len(cnt_words))\n\nprint(\"Length of Comment text in train dataset :\", len(WrdCnt_trn))\nprint(\"Length of Comment text in test dataset :\", len(WrdCnt_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sampled['Comment_Len'] = WrdCnt_trn\ndf_test['Comment_Len'] = WrdCnt_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sampled.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"afjuulzM0BYv"},"cell_type":"markdown","source":"##  6) Machine Learning "},{"metadata":{},"cell_type":"markdown","source":"### 6.1) Train and Cross Validate  split"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"colab_type":"code","executionInfo":{"elapsed":950,"status":"ok","timestamp":1564775035966,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"ZumppmnN0BzI","outputId":"0e0652df-593f-4002-8810-6208eccef376","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Taking class as y label in differnt dataset and then dropping the target and class from train dataset.\nY_train = df_train_sampled['class']\ndf_train_sampled.drop(columns=['target','class'], axis=1, inplace=True)\n\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'Y_train' [stratify=Y_train]\n# Train : cross validate ratio = 70 : 30\n\ntrain_df, cv_df, y_train, y_cv = train_test_split(df_train_sampled, Y_train, stratify=Y_train, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6PY75vfa0CM_","trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])\nprint('Number of data points of Y label in train data:', y_train.shape[0])\nprint('Number of data points of Y label in cross validation data:', y_cv.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"STjrW-pW3Mkv"},"cell_type":"markdown","source":"### 6.2) TFIDF Vectorization on comment text for Train, Test and Cross Validate"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":1829,"status":"ok","timestamp":1564775621414,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"QsvhT_Or0CPv","outputId":"225ee982-6a67-469d-c16e-9c9eaf1f155b","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer(ngram_range=(1,6),max_features=7000)\n\ntfidf_train = tfidf_vect.fit_transform(train_df['comment_text'].values.astype('U'))\ntfidf_cv = tfidf_vect.transform(cv_df['comment_text'].values.astype('U'))\ntfidf_test = tfidf_vect.transform(df_test['comment_text'].values.astype('U'))\n\nprint('shape of train TFIDF vector : ', tfidf_train.shape)\nprint('shape of cross validate TFIDF vector : ', tfidf_cv.shape)\nprint('shape of test TFIDF vector : ', tfidf_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_charvect = TfidfVectorizer(ngram_range=(1,6), analyzer='char', max_features=7000)\n\ntfidf_chartrain = tfidf_charvect.fit_transform(train_df['comment_text'].values.astype('U'))\ntfidf_charcv = tfidf_charvect.transform(cv_df['comment_text'].values.astype('U'))\ntfidf_chartest = tfidf_charvect.transform(df_test['comment_text'].values.astype('U'))\n\nprint('shape of train TFIDF vector : ', tfidf_chartrain.shape)\nprint('shape of cross validate TFIDF vector : ', tfidf_charcv.shape)\nprint('shape of test TFIDF vector : ', tfidf_chartest.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"ixiKrZd05bhP"},"cell_type":"markdown","source":"### 6.3) Vectorization of sentiment analysis scores in train and cross validate"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":1061,"status":"ok","timestamp":1564776461076,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"4mNNbzig0CS7","outputId":"217bcdb1-b231-4acc-a291-9decd619ae5f","trusted":true},"cell_type":"code","source":"# Vectorizing neg score \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nNeg_scaler = MinMaxScaler()\n\nNeg_scaler.fit(train_df['text_NegScore'].values.reshape(-1,1))\n\nneg_train = Neg_scaler.transform(train_df['text_NegScore'].values.reshape(-1,1))\nneg_cv = Neg_scaler.transform(cv_df['text_NegScore'].values.reshape(-1,1))\nneg_test = Neg_scaler.transform(df_test['text_NegScore'].values.reshape(-1,1))\n\nprint('Shape of neg_train :', neg_train.shape)\nprint('Shape of neg_cv :', neg_cv.shape)\nprint('Shape of neg_test :', neg_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":1132,"status":"ok","timestamp":1564776565312,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"8J18gfnF8ksa","outputId":"58c7f07d-7986-497c-8c5b-f37e9367ccc9","trusted":true},"cell_type":"code","source":"# Vectorizing pos score \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nPos_scaler = MinMaxScaler()\n\nPos_scaler.fit(train_df['text_PosScore'].values.reshape(-1,1))\n\npos_train = Pos_scaler.transform(train_df['text_PosScore'].values.reshape(-1,1))\npos_cv = Pos_scaler.transform(cv_df['text_PosScore'].values.reshape(-1,1))\npos_test = Pos_scaler.transform(df_test['text_PosScore'].values.reshape(-1,1))\n\nprint('Shape of pos_train :', pos_train.shape)\nprint('Shape of pos_cv :', pos_cv.shape)\nprint('Shape of pos_test :', pos_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":942,"status":"ok","timestamp":1564776638418,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"-Nmquw4o86Cj","outputId":"7dd3f043-c527-40c8-a73b-7e92c1549abc","trusted":true},"cell_type":"code","source":"# Vectorizing neu score \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nNeu_scaler = MinMaxScaler()\n\nNeu_scaler.fit(train_df['text_NeuScore'].values.reshape(-1,1))\n\nneu_train = Neu_scaler.transform(train_df['text_NeuScore'].values.reshape(-1,1))\nneu_cv = Neu_scaler.transform(cv_df['text_NeuScore'].values.reshape(-1,1))\nneu_test = Neu_scaler.transform(df_test['text_NeuScore'].values.reshape(-1,1))\n\nprint('Shape of neu_train :', neu_train.shape)\nprint('Shape of neu_cv :', neu_cv.shape)\nprint('Shape of neu_test :', neu_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":1018,"status":"ok","timestamp":1564776719425,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"_My5Av579Lrn","outputId":"16e49a4f-7ac2-4c5e-a4e0-e70fee663ca2","trusted":true},"cell_type":"code","source":"# Vectorizing compound score \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nComp_scaler = MinMaxScaler()\n\nComp_scaler.fit(train_df['text_compoundScore'].values.reshape(-1,1))\n\ncomp_train = Comp_scaler.transform(train_df['text_compoundScore'].values.reshape(-1,1))\ncomp_cv = Comp_scaler.transform(cv_df['text_compoundScore'].values.reshape(-1,1))\ncomp_test = Comp_scaler.transform(df_test['text_compoundScore'].values.reshape(-1,1))\n\nprint('Shape of comp_train :', comp_train.shape)\nprint('Shape of comp_cv :', comp_cv.shape)\nprint('Shape of comp_test :', comp_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4) Vectorizing Comment length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing compound score \n\nfrom sklearn.preprocessing import StandardScaler\n\nLen_scaler = StandardScaler()\n\nLen_scaler.fit(train_df['Comment_Len'].values.reshape(-1,1))\nLen_scaler.fit(cv_df['Comment_Len'].values.reshape(-1,1))\nLen_scaler.fit(df_test['Comment_Len'].values.reshape(-1,1))\n\nlen_train = Len_scaler.transform(train_df['Comment_Len'].values.reshape(-1,1))\nlen_cv = Len_scaler.transform(cv_df['Comment_Len'].values.reshape(-1,1))\nlen_test = Len_scaler.transform(df_test['Comment_Len'].values.reshape(-1,1))\n\nprint('Shape of comp_train :', len_train.shape)\nprint('Shape of comp_cv :', len_cv.shape)\nprint('Shape of comp_test :', len_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.5) Vectorizing topic columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing topic-1\n\nfrom sklearn.preprocessing import StandardScaler\n\ntopic_scaler = StandardScaler()\n\ntopic_scaler.fit(train_df['Topic-1'].values.reshape(-1,1))\ntopic_scaler.fit(cv_df['Topic-1'].values.reshape(-1,1))\ntopic_scaler.fit(df_test['Topic-1'].values.reshape(-1,1))\n\ntopic1_train = topic_scaler.transform(train_df['Topic-1'].values.reshape(-1,1))\ntopic1_cv = topic_scaler.transform(cv_df['Topic-1'].values.reshape(-1,1))\ntopic1_test = topic_scaler.transform(df_test['Topic-1'].values.reshape(-1,1))\n\nprint('Shape of topic1_train :', topic1_train.shape)\nprint('Shape of topic1_cv :', topic1_cv.shape)\nprint('Shape of topic1_test :', topic1_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing topic-2\n\ntopic_scaler.fit(train_df['Topic-2'].values.reshape(-1,1))\ntopic_scaler.fit(cv_df['Topic-2'].values.reshape(-1,1))\ntopic_scaler.fit(df_test['Topic-2'].values.reshape(-1,1))\n\ntopic2_train = topic_scaler.transform(train_df['Topic-2'].values.reshape(-1,1))\ntopic2_cv = topic_scaler.transform(cv_df['Topic-2'].values.reshape(-1,1))\ntopic2_test = topic_scaler.transform(df_test['Topic-2'].values.reshape(-1,1))\n\nprint('Shape of topic2_train :', topic2_train.shape)\nprint('Shape of topic2_cv :', topic2_cv.shape)\nprint('Shape of topic2_test :', topic2_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing topic-3\n\ntopic_scaler.fit(train_df['Topic-3'].values.reshape(-1,1))\ntopic_scaler.fit(cv_df['Topic-3'].values.reshape(-1,1))\ntopic_scaler.fit(df_test['Topic-3'].values.reshape(-1,1))\n\ntopic3_train = topic_scaler.transform(train_df['Topic-3'].values.reshape(-1,1))\ntopic3_cv = topic_scaler.transform(cv_df['Topic-3'].values.reshape(-1,1))\ntopic3_test = topic_scaler.transform(df_test['Topic-3'].values.reshape(-1,1))\n\nprint('Shape of topic3_train :', topic3_train.shape)\nprint('Shape of topic3_cv :', topic3_cv.shape)\nprint('Shape of topic3_test :', topic3_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing topic-4\n\ntopic_scaler.fit(train_df['Topic-4'].values.reshape(-1,1))\ntopic_scaler.fit(cv_df['Topic-4'].values.reshape(-1,1))\ntopic_scaler.fit(df_test['Topic-4'].values.reshape(-1,1))\n\ntopic4_train = topic_scaler.transform(train_df['Topic-4'].values.reshape(-1,1))\ntopic4_cv = topic_scaler.transform(cv_df['Topic-4'].values.reshape(-1,1))\ntopic4_test = topic_scaler.transform(df_test['Topic-4'].values.reshape(-1,1))\n\nprint('Shape of topic4_train :', topic4_train.shape)\nprint('Shape of topic4_cv :', topic4_cv.shape)\nprint('Shape of topic4_test :', topic4_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorizing topic-5\n\ntopic_scaler.fit(train_df['Topic-5'].values.reshape(-1,1))\ntopic_scaler.fit(cv_df['Topic-5'].values.reshape(-1,1))\ntopic_scaler.fit(df_test['Topic-5'].values.reshape(-1,1))\n\ntopic5_train = topic_scaler.transform(train_df['Topic-5'].values.reshape(-1,1))\ntopic5_cv = topic_scaler.transform(cv_df['Topic-5'].values.reshape(-1,1))\ntopic5_test = topic_scaler.transform(df_test['Topic-5'].values.reshape(-1,1))\n\nprint('Shape of topic5_train :', topic5_train.shape)\nprint('Shape of topic5_cv :', topic5_cv.shape)\nprint('Shape of topic5_test :', topic5_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"8Uui3rvb9ogG"},"cell_type":"markdown","source":"### 6.5) Merging all the features"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":972,"status":"ok","timestamp":1564777162110,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"JNijo8Xi9vFR","outputId":"bde29758-f4db-4d78-a2b7-d67038b5f0f9","trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\nX_train = hstack((tfidf_train,tfidf_chartrain,neg_train,neu_train,pos_train,comp_train,len_train,topic1_train,topic2_train,topic3_train,topic4_train,topic5_train))\nX_cv = hstack((tfidf_cv,tfidf_charcv,neg_cv,neu_cv,pos_cv,comp_cv,len_cv,topic1_cv,topic2_cv,topic3_cv,topic4_cv,topic5_cv))\nX_test = hstack((tfidf_test,tfidf_chartest,neg_test,neu_test,pos_test,comp_test,len_test,topic1_test,topic2_test,topic3_test,topic4_test,topic5_test))\n\nprint(\"Shape of X train dataset : \", X_train.shape)\nprint(\"Shape of CV dataset : \", X_cv.shape)\nprint(\"Shape of X test dataset : \", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7) Logistic Regression Model"},{"metadata":{"colab_type":"text","id":"U6-67hAz_1JP"},"cell_type":"markdown","source":"### 7.1) Hyperparameter tuning for Logistic Regression"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"colab_type":"code","executionInfo":{"elapsed":17408,"status":"ok","timestamp":1564778719931,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"iEl5kd4o_-j4","outputId":"00a93a9b-2654-42fb-8203-8300b4b17242","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.metrics import roc_auc_score, auc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain_auc = []\ncv_auc = []\n\nalpha = [10 ** x for x in range(-7, -1)]\ncv_log_error_array = []\nfor i in alpha:\n    \n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42, n_jobs=-1)\n    clf.fit(X_train, y_train)    \n\n    y_predictedtrain = clf.predict(X_train)\n    y_predictedCV = clf.predict(X_cv)\n\n    train_auc.append(roc_auc_score(y_train, y_predictedtrain))\n    cv_auc.append(roc_auc_score(y_cv, y_predictedCV))\n    \nfig, ax = plt.subplots()\nax.plot(alpha, train_auc, label=\"train AUC\")\nax.plot(alpha, cv_auc, label=\"CV AUC\")\n\nax.scatter(alpha, train_auc, label=\"train AUC pts\")\nax.scatter(alpha, cv_auc, label=\"CV AUC pts\")\n\nplt.legend()\nplt.xlabel(\"Alpha (Hyper parameter)\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2) Logistic regression with best alpha + Confusion matrix for Cross Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef plot_confusion_matrixas_HeatMap(confusion_matrix):\n    col = [\"Predicted project rejected\",\"Prediction project approved\"]\n    ind = [\"Actual project rejected\",\"Actual project approved\"]\n    df_cm = pd.DataFrame(confusion_matrix, index=ind, columns=col)\n    fig = plt.figure(figsize=(4,4))\n    plt.close()\n    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    return fig","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","executionInfo":{"elapsed":985,"status":"ok","timestamp":1564778807577,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"klXFYQGsFE4D","outputId":"7a8d5081-19c0-466a-aa71-37fdc1684eb4","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# Confusion Matrix for cross validate data\n\n# From Hyperparameter tuning we take that point where train and validate AUC points have high score\nclf = SGDClassifier(class_weight='balanced', alpha=10**-6, penalty='l2', loss='log', random_state=42, n_jobs=-1)\nclf.fit(X_train, y_train)\n\ny_predictedCV = clf.predict(X_cv)\ncm = confusion_matrix(y_cv, y_predictedCV)\nplot_confusion_matrixas_HeatMap(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.3) Receiver operating characteristic (ROC) curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ny_predictedtrain = clf.predict(X_train)\ny_predictedCV = clf.predict(X_cv)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_predictedtrain)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_predictedCV)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"CV AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"ROC Curve for Train and CV data\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8) Linear Support Vector Machine (SVM)"},{"metadata":{},"cell_type":"markdown","source":"### 8.1) Hyperparameter tuning for SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.metrics import roc_auc_score, auc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain_auc = []\ncv_auc = []\n\nalpha = [10 ** x for x in range(-7, -1)]\ncv_log_error_array = []\nfor i in alpha:\n    \n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42, n_jobs=-1)\n    clf.fit(X_train, y_train)    \n\n    y_predictedtrain = clf.predict(X_train)\n    y_predictedCV = clf.predict(X_cv)\n\n    train_auc.append(roc_auc_score(y_train, y_predictedtrain))\n    cv_auc.append(roc_auc_score(y_cv, y_predictedCV))\n    \nfig, ax = plt.subplots()\nax.plot(alpha, train_auc, label=\"train AUC\")\nax.plot(alpha, cv_auc, label=\"CV AUC\")\n\nax.scatter(alpha, train_auc, label=\"train AUC pts\")\nax.scatter(alpha, cv_auc, label=\"CV AUC pts\")\n\nplt.legend()\nplt.xlabel(\"Alpha (Hyper parameter)\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2) SVM with best aplha +  Confusion matrix for Cross Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# Confusion Matrix for cross validate data\n\n# From Hyperparameter tuning we take that point where train and validate AUC points have high score\nclf = SGDClassifier(class_weight='balanced', alpha=10**-5, penalty='l2', loss='hinge', random_state=42, n_jobs=-1)\nclf.fit(X_train, y_train)\n\ny_predictedCV = clf.predict(X_cv)\ncm = confusion_matrix(y_cv, y_predictedCV)\nplot_confusion_matrixas_HeatMap(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.3) Receiver operating characteristic (ROC) curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ny_predictedtrain = clf.predict(X_train)\ny_predictedCV = clf.predict(X_cv)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_predictedtrain)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_predictedCV)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"CV AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"ROC Curve for Train and CV data\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9) Comparing Logistic regression with Linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# http://zetcode.com/python/prettytable/\n\nfrom prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Model\",\"alpha\",\"Train AUC\", \"Validate AUC\"]\n\nx.align[\"Vectorizer\"] = \"l\"\nx.align[\"Hyper parameter\"] = \"r\"\nx.align[\"Train AUC\"] = \"r\"\nx.align[\"Validate AUC\"] = \"r\"\n\nx.add_row([\"logistic regresion\", \"alpha = 0.000001\", \"0.89\", \"0.85\"])\nx.add_row([\"linear SVM\", \"alpha = 0.000001\", \"0.88\", \"0.84\"])\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11) XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nparam_grid = {\n        'silent': [False],\n        'max_depth': [10],\n        'learning_rate': [0.1],\n        'n_estimators': [300]}\n\nclf = xgb.XGBClassifier()\n\nrs_clf = RandomizedSearchCV(clf, param_grid, n_iter=100, n_jobs=-1, verbose=2, cv=5, refit=True, random_state=42)\nrs_clf.fit(X_train,y_train)\n\ny_predictedCV = rs_clf.predict(X_cv)\ncm = confusion_matrix(y_cv, y_predictedCV)\nplot_confusion_matrixas_HeatMap(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Receiver operating characteristic (ROC) curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ny_predictedtrain = rs_clf.predict(X_train)\ny_predictedCV = rs_clf.predict(X_cv)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_predictedtrain)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_predictedCV)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"CV AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"ROC Curve for Train and CV data\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"7FVcKz52FhGI"},"cell_type":"markdown","source":"## 11) Testing the model"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"colab_type":"code","executionInfo":{"elapsed":1044,"status":"ok","timestamp":1564780429495,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"wmEIf8hrFfqJ","outputId":"5424bfa3-4094-4670-c228-85233a6f756f","trusted":true},"cell_type":"code","source":"# As we can see that train AUC for XGBoost classifier is bestest among the logistic regression and linear SVM\n# we will be predicting the test dataset using XGBoost\n\ny_predicted_test = rs_clf.predict(X_test)\ny_predict_proba_test = rs_clf.predict_proba(X_test)\n\nprint(y_predicted_test[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['comment_text'][11] ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":1015,"status":"ok","timestamp":1564780508243,"user":{"displayName":"Sonali","photoUrl":"https://lh6.googleusercontent.com/-29UuJKP2sCU/AAAAAAAAAAI/AAAAAAAAFFE/Z_0wtF2tqiQ/s64/photo.jpg","userId":"16260148040418897175"},"user_tz":-330},"id":"vumESpLRLsjw","outputId":"f4a73765-676b-499f-8f45-1a0de561be12","trusted":true},"cell_type":"code","source":"# Scores for traning and cross validate data\n\nprint(rs_clf.score(X=X_train,y=y_train))\nprint(rs_clf.score(X=X_cv,y=y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_predict_proba_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the predicted Y class and probability to the test dataframe\n\ndf_test['prediction'] = list(y_predicted_test)\ndf_test['Y_predictClass_probability'] = list(y_predict_proba_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df = pd.DataFrame(columns=['id','prediction'])\nsample_submission_df['id'] = df_test['id']\nsample_submission_df['prediction'] = df_test['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Jigsaw_UnIntended_Bias_Toxicity_Classification-Copy1.ipynb","provenance":[],"toc_visible":true,"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}