{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.model_selection import train_test_split\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Model, Input\nfrom keras.layers import Dense, Embedding, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test=pd.read_csv('../input/test.csv')\ntrain=pd.read_csv('../input/train.csv')\n\n#X_train = train[\"comment_text\"].values\n\n\n#X_test = test[\"comment_text\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encode_by_label(dataset, new_col, col_list, threshold_val=0.1):\n    \n    for col in col_list:\n        dataset[new_col] = (\n            dataset[col] > threshold_val\n        )*1.0  \n    \n    return dataset\n\n#\nreligions = [\n    'atheist',\n    'buddhist',\n    'christian',\n    'hindu',\n    'jewish',\n    'muslim',\n    'other_religion'\n]\ntrain = one_hot_encode_by_label(train, 'religion', religions)\n\n#\nethnicity = [\n    'asian',\n    'black',\n    'latino',\n    'white',\n    'other_race_or_ethnicity'\n]\ntrain = one_hot_encode_by_label(train, 'ethnicity', ethnicity)\n\n# \nsexualOrientation = [\n    'bisexual',\n    'heterosexual',\n    'homosexual_gay_or_lesbian',\n    'other_gender',\n    'transgender',\n    'other_sexual_orientation'    \n]\ntrain = one_hot_encode_by_label(train, 'sexualOrientation', sexualOrientation)\n\n# dataset.hist(column=\"religion\")\n# dataset.hist(column=\"ethnicity\")\n# dataset.hist(column=\"sexualOrientation\")\n\ntrain[\n    train['religion'] == 1\n].tail()\n\n# # 1804874\n# # len(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['comment_text']\ny = train[['religion','ethnicity','sexualOrientation']]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[[ 'religion','ethnicity','sexualOrientation']].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"max_features = 20000  # number of words we want to keep\nmaxlen = 100  # max length of the comments in the model\nbatch_size = 64  # batch size for the model\nembedding_dims = 20  # dimension of the hidden variable, i.e. the embedding dimension\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok = Tokenizer(num_words=max_features)\ntok.fit_on_texts(list(X_train) + list(X_test))\nx_train = tok.texts_to_sequences(X_train)\nx_test = tok.texts_to_sequences(X_test)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=y_train[:1209265,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_input = Input((maxlen,))\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\ncomment_emb = Embedding(max_features, embedding_dims, input_length=maxlen, \n                        embeddings_initializer=\"uniform\")(comment_input)\n\n# we add a GlobalMaxPooling1D, which will extract features from the embeddings\n# of all words in the comment\nh = GlobalMaxPooling1D()(comment_emb)\n\n# We project onto a six-unit output layer, and squash it with a sigmoid:\noutput = Dense(3, activation='sigmoid')(h)\n\nmodel = Model(inputs=comment_input, outputs=output)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(0.01),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}