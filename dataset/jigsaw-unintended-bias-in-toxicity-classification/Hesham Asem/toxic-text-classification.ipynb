{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic Text Recognition \nby : Hesham Asem\n\n________________\n\nhere we have a huge database with about 1.8 million sample size , which got a variety comments & texts , some of them are classified as offensive & toxic \n\nwe need to build a model able to train from this database , so he can classify the test data\n\ndatabase  : https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n\n\nso let's import needed libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport collections\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectPercentile , f_classif \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then we'll read the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv' )  \ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv' )  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and since the database is huge , we'll just got 10K rows from it to save time in training , but you can deactivate this line if you need to take the whole data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[:10000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\nnow it's time to define needed functions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')\n\ndef count_nulls() : \n    global data\n    for col in data.columns : \n        if not data[col].isna().sum() == 0 : \n            print(f'Column   {col}    got   {data[col].isna().sum()} nulls  ,  Percentage : {round(100*data[col].isna().sum()/data.shape[0])} %')\n\ndef cplot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n\ndef encoder(feature , new_feature, drop = True) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n    \ndef MakeCloud(text , title = 'Word Clouds' , w = 15 , h = 15):\n    plt.figure(figsize=(w,h))\n    plt.imshow(WordCloud(background_color=\"white\",stopwords=set(stopwords.words('english')))\n               .generate(\" \".join([i for i in text.str.lower()])))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\n\ndef SelectedData(data , feature , value , operation, selected_feature ):\n    if operation==0 : \n        result = data[data[feature]==value][selected_feature]\n    elif operation==1 : \n        result = data[data[feature] > value][selected_feature]\n    elif operation==2 : \n        result = data[data[feature]< value][selected_feature]\n    \n    return result \n\n\n\ndef CommonWords(text ,show = True , kk=10) : \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n    common_words = collections.Counter(all_words).most_common()\n    k=0\n    word_list =[]\n    for word, i in common_words : \n        if not word.lower() in  nlp.Defaults.stop_words :\n            if show : \n                print(f'The word is   {word}   repeated   {i}  times')\n            word_list.append(word)\n            k+=1\n        if k==kk : \n            break\n            \n    return word_list\n\ndef RemoveWords(data , feature , new_feature, words_list ) : \n    new_column = []\n    for i in range(data.shape[0]) : \n        this_phrase = data[feature][i]\n        new_phrase = []\n        for word in this_phrase.split() : \n            if not word.lower() in words_list : \n                new_phrase.append(word)\n        new_column.append(' '.join(new_phrase))\n    \n    data.insert(data.shape[1],new_feature,new_column)\n    \n\n    \ndef CountWords(text) :  \n    \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n\n    print(f'Total words are {len(all_words)} words')   \n    print('')\n    print(f'Total unique words are {len(set(all_words))} words')   \n\ndef SlicedData(feature_list, dropna = False) : \n    global data\n    if dropna :\n        return data.loc[:, feature_list ].dropna()\n    else : \n        return data.loc[:, feature_list ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n____\n\n\n# Data Processing\n\nnow how data looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\nso we need to have a close look to some features"},{"metadata":{"trusted":true},"cell_type":"code","source":"SlicedData(['obscene','identity_attack', 'insult' , 'thread']).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SlicedData(['asian','atheist', 'bisexual' , 'black']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treating Output\n\nsince output 'target' is a numerical value & the model shuld be regression , we'll concert the output into 4 classes , depend on how toxic is the text \n\nso instead have a huge numbers in the output , it will be either 0 , 1, 2 or 3 , & this to make the model easier , specially that we have a huge sample size & will use a big number of features (due to TF Vectorizer)\n\nso let's see max & min value for the output"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].min() , data['target'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , now if we multiply 3 time all numbers then round it into integers , we'll have all outputs either 0 , 1 , 2 or 3 , depend on how toxic is the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target sector'] = round(data['target']*3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's check it"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique('target sector')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how is the distribution of output"},{"metadata":{"trusted":true},"cell_type":"code","source":"cplot('target sector')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"almost 90% of text is non-toxic , now how about the toxic texts , lets temporarly drop all non-toxic text , to see distribution of ther categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data = data[data['target sector'] > 0]['target sector']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now to pie chart it"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.pie(temp_data.value_counts(),labels=list(temp_data.value_counts().index),autopct ='%1.2f%%',labeldistance = 1.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"almost two-third of text are kinda toxic , then moderate & little bit which is very toxic\n\nnow how about nulls in the database "},{"metadata":{"trusted":true},"cell_type":"code","source":"count_nulls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\n# Treating Text\n\nnow it's time to focus in text its self before we build the model \n\nfirst we need to lowercase all words , to avoid any misleading in training"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['comments']  =  data['comment_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how it looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"SlicedData(['comment_text' , 'comments']).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then we need to know the number of total words & unique words in the whole text "},{"metadata":{"trusted":true},"cell_type":"code","source":"CountWords(data['comments'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nalmost a 600K word in only 10K row , which based on 54K unqiue words\n\nnow we need to know most common words in the whole text , & we'll use the feature 'comments' , which is lowered case , not the original feature 'comment_text'"},{"metadata":{"trusted":true},"cell_type":"code","source":"common = CommonWords(data['comments'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it looks like these 10 words are non-leading words , which might appear in toxic or non-toxic words , so it'll be a good idea to remove it from all phrases , to ust leave the most important words\n\nlet's create a new feature called 'filtered comments' , which contain all phrases exclude these common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"RemoveWords(data , 'comments' , 'filtered comments', common)\nSlicedData(['comments' , 'filtered comments']).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , now phrases become more extinctive \n\n_____\n\n# Cloud Words\n\nit's useful to use wordcloud tool , to know most repeated words \n\nlet's see most repeated words now in all filtered comments , after we eliminate those 10 common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"MakeCloud(data['filtered comments'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about to see the could words for each category . . \n\nwe'll make a function now , which will show cloud words for only rows which got offensive value more than 0.1 in each category , of the 24 category we have here \n\n& to make it easier for us to read it , we'll show it 3 by 3 \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def showclouds(n) : \n    this_list = ['asian', 'atheist', 'bisexual','black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n                 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability','jewish', 'latino', 'male', 'muslim',\n                 'other_disability','other_gender', 'other_race_or_ethnicity', 'other_religion','other_sexual_orientation', \n                 'physical_disability','psychiatric_or_mental_illness', 'transgender', 'white' ]\n\n    for item in this_list[n*3:(n*3)+3] : \n        this_data =  SelectedData(data ,item , 0.1 , 1 , 'filtered comments')\n        print(f'for item    {item}')\n        print(f'Number of selected rows {this_data.shape[0]}')\n        print('common words : ')\n        _ = CommonWords(this_data)\n        if this_data.shape[0] >0 : \n            MakeCloud(this_data , str(f'Word Cloud for {item}'), 8 ,8)\n        print('--------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\n\nnow start with :   'asian', 'atheist', 'bisexual'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_______\n\nthen :'black', 'buddhist', 'christian'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nthen :  'female', 'heterosexual', 'hindu'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\nnow : 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability','jewish'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\nnow  :  'latino', 'male', 'muslim'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nthen : 'other_disability','other_gender', 'other_race_or_ethnicity'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\nthen : 'other_religion','other_sexual_orientation'  ,'physical_disability'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\nand last : 'psychiatric_or_mental_illness', 'transgender', 'white'"},{"metadata":{"trusted":true},"cell_type":"code","source":"showclouds(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it was so obvious how distinctive words are appear in each category , & that will help us in the classification\n\n_____\n\n# Data Preparing\n\nnow we are ready to define X & y "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['filtered comments']\ny = data['target sector']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how X looks like ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"& y ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need to be sure there is no nulls in both of them "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum() , y.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then we'll use TF Vectorizer tool , to create sparse matrix for all words"},{"metadata":{"trusted":true},"cell_type":"code","source":"VecModel = TfidfVectorizer()\nX = VecModel.fit_transform(X)\nprint(f'The new shape for X is {X.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok , almost 25K feature , which is so much & will consume a huge amount of time , specially that we have 10K sample size , so we'll use sklearn to only select 1% of those features"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureSelection = SelectPercentile(score_func = f_classif, percentile=1)\nX = FeatureSelection.fit_transform(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how X looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X Shape is ' , X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , then split them "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we are ready for traning\n\n_____\n\n# Build the Model\n\nlet's use GBC since it's more suitable for big amout of data . \n\nwe'll use 500 estimators with max_depth =5\n\nlet's start the train & check accuracy for train & test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBCModel = GradientBoostingClassifier(n_estimators=500,max_depth=5,random_state=33) \nGBCModel.fit(X_train, y_train)\n\n\nprint('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok 84% is not a bad accuracy , might be better if we increase the data more than 100K but it will need more time\n\nnow let's predict the Test Data. . "},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's lower case it as we did in trainging data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comments']  =  test['comment_text'].str.lower()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and put it in the variable X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test['comments']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"what is the shape "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then we have to transform it with the Vectorize Model , which fitted in the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = VecModel.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the data now should have 25K feature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"erfect , now we have to apply again the feature selection model , to only pick 1% of same features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = FeatureSelection.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now it should have only 257 feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now it's ready for predicting using the same GBC Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\nprint('Predicted Value for GBCModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for GBCModel is : ' , y_pred_prob[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , well done !\n\nhope you like it & found it useful . . "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}