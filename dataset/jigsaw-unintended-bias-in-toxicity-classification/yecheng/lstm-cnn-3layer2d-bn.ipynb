{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preface"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nNUM_MODELS = 1\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 224","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_model(model, train,test, loss_fn, output_dim, lr=0.001,\n                batch_size=512, n_epochs=1,\n                enable_checkpoint_ensemble=True):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    \n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 1 ** epoch)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    \n    for epoch in range(n_epochs):\n        start_time = time.time()\n        \n        scheduler.step()\n        \n        model.train()\n        avg_loss = 0.\n        \n        for data in tqdm(train_loader, disable=False):\n            x_batch = data[:-1]\n            y_batch = data[-1]\n\n            y_pred = model(*x_batch) \n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n            \n        model.eval()\n        test_preds = np.zeros((len(test), output_dim))\n    \n        for i, x_batch in enumerate(test_loader):\n            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n            \n        all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        print('Epoch {}/{} \\t triain_loss={:.4f} \\t time={:.2f}s'.format(\n              epoch + 1, n_epochs, avg_loss, elapsed_time))\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.5)\n        \n        self.lstm = nn.LSTM(embed_size,LSTM_UNITS,num_layers=2,dropout=0.2,bidirectional=True,batch_first=True)\n        \n        for name, param in self.lstm.named_parameters():\n            if 'bias' in name:\n                torch.nn.init.constant_(param, 0.0)\n            elif 'weight' in name:\n                torch.nn.init.orthogonal_(param)\n                      \n        self.conv1=nn.Conv2d(1,3, kernel_size=3,stride=1,padding=1)\n        self.maxpool1=nn.MaxPool2d(kernel_size=(56,4))\n        \n        self.batch1=torch.nn.BatchNorm2d(3)\n\n        self.droupout_out=torch.nn.Dropout(0.05)\n        \n        self.droupout_aux_out=torch.nn.Dropout(0.05)\n        \n        self.linear_out = nn.Linear(3*int(MAX_LEN/56)*int(LSTM_UNITS/2), 1)\n        self.linear_aux_out = nn.Linear(3*int(MAX_LEN/56)*int(LSTM_UNITS/2), num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        h_lstm, _ = self.lstm(h_embedding)\n        \n        addaxis_lstm=h_lstm.unsqueeze(1)\n        \n        conv_1=self.conv1(addaxis_lstm)\n        max_pool1=F.relu(self.maxpool1(conv_1))\n        \n        max_pool1_batch=self.batch1(max_pool1)\n        \n        #conv_2=self.conv2(max_pool1_batch)\n        #max_pool2=F.relu(self.maxpool2(conv_2))\n        \n        #deleteaxis_max_pool1=max_pool1.squeeze(1)\n        \n        flatten=torch.flatten(max_pool1_batch, start_dim=1)\n        \n        \n        result = self.linear_out(flatten)\n        aux_result = self.linear_aux_out(flatten)\n\n        result = self.droupout_out(result)\n        aux_result = self.droupout_aux_out(aux_result)\n\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = preprocess(train['comment_text'])\ny_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\nx_test = preprocess(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = max_features or len(tokenizer.word_index) + 1\nmax_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\nx_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\ny_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\ntest_dataset = data.TensorDataset(x_test_torch)\n\nall_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1234 + model_idx)\n    \n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    model.cuda()\n    \n    test_preds = train_model(model, train_dataset,test_dataset, output_dim=y_train_torch.shape[-1],  batch_size=512,\n                              n_epochs=4,loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n    all_test_preds.append(test_preds)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}