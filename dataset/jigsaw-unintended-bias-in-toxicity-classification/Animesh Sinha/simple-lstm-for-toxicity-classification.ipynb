{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple LSTM with GloVe Embeddings (using only Targets)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom matplotlib import pyplot as plt\n\nINPUT_DIR = \"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nGLOVE_DIR = \"../input/glove-global-vectors-for-word-representation\"\n\nprint(os.listdir(INPUT_DIR))\nprint(os.listdir(GLOVE_DIR))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Input and Output"},{"metadata":{},"cell_type":"markdown","source":"### Loading the Training Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding what the Output should look like"},{"metadata":{},"cell_type":"markdown","source":"Here we print a sample output and the test set input. The thing to note is that all we have to submit is the expected value of **TARGET** given the **COMMENT TEXT**. Our Classifier can be trained simply on this data and everything else is to aid rejecting false positives."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(os.path.join(INPUT_DIR, 'sample_submission.csv')) as sample_submission:\n    for x in range(5):\n        print(next(sample_submission), end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(os.path.join(INPUT_DIR, 'test.csv')) as sample_submission:\n    for x in range(10):\n        print(next(sample_submission), end='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some comments from the Problem Statement\n\nHere are the different types of Toxicity labels to help us fine tune our predictions.\n* severe_toxicity\n* obscene\n* threat\n* insult\n* identity_attack\n* sexual_explicit\n\nThere are many more classes storing the severity of attack / count of certain targetted entities. Here are the once that we will be tested on, that have 500 examples or more in the provided Training Set.\n* male\n* female\n* homosexual_gay_or_lesbian\n* christian\n* jewish\n* muslim\n* black\n* white\n* psychiatric_or_mental_illness"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Text with Embeddings"},{"metadata":{},"cell_type":"markdown","source":"### Loading the GloVe Embeddings onto Keras"},{"metadata":{},"cell_type":"markdown","source":"Here we start by reading the GloVe text file. The format here is simple, it's the **token followed by it's 100-D representation, space-separated, in each line**. The token include both words and puncutations, and 's, etc. Next we shall extract data out of this."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as glove_file:\n    for x in range(5):\n        print(next(glove_file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\nprint('Embeddings_index is a map of the words to a', len(embeddings_index['the']), 'dimentional vector.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading through the input and tokenizing the Comments"},{"metadata":{},"cell_type":"markdown","source":"We load the data from the CSV file and print the first few lines to see what the data is like. The **Comment_text** column will be what we start working on first, preprocess it into a form we can use."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Generating the Text corpus in the form of a Numpy list\ncorpus = train['comment_text'].tolist()\nprint(\"Some sample comments we train the Tokenizer on:\\n\", corpus[:3])\n\n# Fitting the tokenizer on the corpus, \ntokenizer = Tokenizer(num_words=1000000)\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# Padding to convert Jagged array into uniform length 2-D time series data\ndata = pad_sequences(sequences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train['target'].as_matrix()\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\n# Split into Train and Validation Sets\nVALIDATION_SPLIT = 0.25 # Percentage of sample going to the Validation set\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating Embedding Matrix and Feeding to Embedding Layer"},{"metadata":{},"cell_type":"markdown","source":"We have the **Data Tensor**, which is a 317-Dimentional representation of every sentence, padded in the front by 0s till it fits the Max-length of 317.\nNow we use the Embedding matrix to freeze the weights in the Embeddings layer. `keras.Embedding` takes in the *Data Tensor* and outputs a *2-D Vectors array representation of the sentence*."},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 100\nMAX_SEQUENCE_LENGTH = len(data[0])\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n                                            EMBEDDING_DIM,\n                                            weights=[embedding_matrix],\n                                            input_length=MAX_SEQUENCE_LENGTH,\n                                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Neural Network Architecture"},{"metadata":{},"cell_type":"markdown","source":"Lets wire up a model. This architecture is derived from this Kernel: https://www.kaggle.com/thousandvoices/simple-lstm\n\nTODO: Understand LSTMs and replace with my own architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    words = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n    x = embedding_layer(words)\n    x = tf.keras.layers.SpatialDropout1D(0.2)(x)\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128, return_sequences=True))(x)\n\n    hidden = tf.keras.layers.concatenate([\n        tf.keras.layers.GlobalMaxPooling1D()(x),\n        tf.keras.layers.GlobalAveragePooling1D()(x),\n    ])\n    hidden = tf.keras.layers.add([hidden, tf.keras.layers.Dense(512, activation='relu')(hidden)])\n    hidden = tf.keras.layers.add([hidden, tf.keras.layers.Dense(512, activation='relu')(hidden)])\n    result = tf.keras.layers.Dense(1, activation='sigmoid')(hidden)\n    \n    model = tf.keras.models.Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the Heavy operation, let's fit the model to the DataSet."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nhistory = model.fit(x = x_train, y = y_train, validation_data=(x_val, y_val), epochs = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a few graphs, see how our model did, and where we can do better."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('The Loss Function')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outputting the Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = test['comment_text'].tolist()\nq_data = pad_sequences(tokenizer.texts_to_sequences(questions), maxlen=MAX_SEQUENCE_LENGTH)\nprint(q_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(q_data)\nids = test['id'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(result) == len(ids)\nwith open('submission.csv', 'w') as file:\n    file.write('id,prediction\\n')\n    for item in range(len(ids)):\n        file.write(str(ids[item]) + ',' + str(result[item][0]) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.csv') as sample_submission:\n    for x in range(5):\n        print(next(sample_submission), end='')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}