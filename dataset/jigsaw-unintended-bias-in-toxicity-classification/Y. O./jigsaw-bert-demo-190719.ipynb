{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">パラメータ設定</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"CONT_MODEL_KERNEL = ''\nTOKENIZED = \"../input/jigsaw-bert-tokenize-x-len-y-yaux-01-01-02/\"\n\nCONTINUE=False\n\nN_FOLD = 1\nFOLD = 5\n\nMAX_SEQUENCE_LENGTH = 300\nSEED = 100+N_FOLD-1\nEPOCHS = 1\nData_dir = \"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\"\nTARGET = 'target'\nTEXT_COL = 'comment_text'\n\nTOTAL_RECORD = 1804874\nfilename = 'checkpoint.pth'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">ライブラリのインポート</font>"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_rows\", 1000)\npd.set_option(\"display.max_colwidth\", 300)\nimport os\nprint(os.listdir(\"../input/nvidiaapex/repository/NVIDIA-apex-39e153a\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NVIDIA Apex のインストール (FP16: 半精度浮動小数点)\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidiaapex/repository/NVIDIA-apex-39e153a","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">pickle用ヘルパーの定義とseed固定</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_obj(obj, name):\n    with open(name, 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\ndef load_obj(name):\n    with open(name, 'rb') as f:\n        return pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 但しkaggle kernelでは起動ごとにインスタンスが変わるのでpytorch seedは固定できない\ndef seed_torch(s):\n    os.environ['PYTHONHASHSEED'] = str(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nseed_torch(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">BERTのライブラリへのパス接続とTF→pytorchの変換</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\n\n# Add the BERT Pytorch repo to the PATH\n# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translate model from tensorflow to pytorch\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH+'bert_config.json', WORK_DIR+'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the BERT configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig('../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'+'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">関数・クラスの定義</font>"},{"metadata":{},"cell_type":"markdown","source":"**AUCをsckit-learnよりも早く計算できる関数の定義（速度改善は未確認。Contributed by Giba）**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numba import jit\n@jit\ndef fast_auc(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc = auc / (nfalse * (n - nfalse))\n    return auc\n\ndef eval_auc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'auc', fast_auc(labels, preds), True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**今回のコンペにおける評価関数を計算するクラスの定義**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_identity = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nclass JigsawEvaluator:\n    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n        self.y = (y_true >= 0.5).astype('int8')\n        self.y_i = (y_identity >= 0.5).astype('int8')\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return fast_auc(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n\n    def _calculate_overall_auc(self, y_pred):\n        return fast_auc(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n\n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        return [\n            overall_score + bias_score, # auc\n            overall_score / self.overall_model_weight, # overall_auc\n            self._power_mean(bias_metrics[0]), self._power_mean(bias_metrics[1]), self._power_mean(bias_metrics[2]) # subgroup_auc, bpsn_auc, bnsp_auc\n            ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validationデータの推論**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_valid(X_val):\n    batch_size = 16\n    valid_preds = np.zeros((len(X_val), len(X_val[0][2])-1))\n    valid_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n    valid_loader = torch.utils.data.DataLoader(X_val, batch_size=batch_size, shuffle=False, collate_fn=valid_collator)\n\n    tk0 = tqdm_notebook(enumerate(valid_loader), total=len(valid_loader), leave=True)\n    avg_val_loss = 0.\n    for i,(x_batch, _) in tk0:\n        pred = model(x_batch[0].to(device), attention_mask=(x_batch[0]>0).to(device), labels=None)\n        valid_preds[i*batch_size: min(len(X_val), (i+1)*batch_size), :]=pred.detach().cpu().squeeze().numpy()\n        \n    avg_val_loss = custom_loss(\n                        torch.tensor(valid_preds, dtype=torch.float32).to(device), \n                        torch.tensor(X_val[:][2], dtype=torch.float32).to(device) # X_val[:][2][:,0] <- delete [:,0] when using multi-label\n                    ).item()\n    return torch.sigmoid(torch.tensor(valid_preds)).numpy(), avg_val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**バッチ内で最長トークン長に合わせてマスキング**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths).cpu()\n        mask = torch.arange(start=0, end=MAX_SEQUENCE_LENGTH, step=1) < length.type(torch.LongTensor)\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i not in (self.length_index, self.label_index)], batch[self.label_index]\n        else:\n            return [x for i, x in enumerate(batch) if i not in (self.length_index, _)], _","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**損失関数の定義**"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_weight = 3.209226860170181\ndef custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">トークナイゼーション</font>"},{"metadata":{},"cell_type":"markdown","source":"**トークン化関数とトークン化されたコメントのチェック関数**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -= 2\n    all_tokens = []\n    longer = 0\n    for text in example:\n        tokens_a = tokenizer.tokenize(text)\n        print('\\n')\n        print('=== トークン化 ===')\n        print(tokens_a)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0]*(max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n#     print(longer)\n    print('\\n')\n    print('=== トークン番号 ===')\n    return np.array(all_tokens)\n\ndef check_tokenizer(i):\n    print('=== オリジナル ===')\n    print(df[TEXT_COL].iloc[i])\n\n    s = pd.Series(df[TEXT_COL].fillna(\"DUMMY_VALUE\").iloc[i])\n    print(convert_lines(s, MAX_SEQUENCE_LENGTH, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**トークン化を実際にチェックする**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\ndf = pd.read_csv(os.path.join(Data_dir, \"train.csv\"), nrows=10).sample(frac=1, random_state=SEED).reset_index(drop=True)\ndf[TEXT_COL] = df[TEXT_COL].astype(str)\ndf[['id', 'target', 'comment_text']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_tokenizer(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_tokenizer(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_tokenizer(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**あらかじめトークン化しておいたものをpickleで読み込む (3分クッキング)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = load_obj(TOKENIZED+'train_dataset')\ntrain_df = load_obj(TOKENIZED+'train_df')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"DarkViolet\">BERTのトレーニング</font>"},{"metadata":{},"cell_type":"markdown","source":"**5-fold分のindex作成 (sklearnのクラスでcallableに持っておくのもよいが、デバッグしやすいのでリストで持っておくのが個人的に好き)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nstratified = True\nstratified_by = TARGET\nif stratified: folds = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=SEED)\nelse: folds = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n\nfold_list = []\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_dataset[:][2][:,0].numpy(), train_dataset[:][2][:,0].numpy())):\n    fold_list.append([n_fold, train_idx, valid_idx])\ndel folds; gc.collect()\nfold_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**速度改善のため、同じバッチ内でトークン長が同じになるようにシャッフルする<font color=\"Red\">8.5時間→3.5時間 (/epoch)</font>**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n\nR_TRAIN = 0.01 # ここで学習対象のデータ量を調整する\nN_VALID = 100000\ntrain_idx, valid_idx = fold_list[N_FOLD-1][1], fold_list[N_FOLD-1][2][:N_VALID]\n\n## Sort train by lengths --------\n# make sure every batch has similar sentences length, and shuffle the batchs\nnp.random.seed(SEED)\ntotal_len = len(train_idx)\ntrain_idx = np.random.choice(train_idx, total_len-total_len%batch_size, replace=False)\nsort_idx = np.argsort(train_dataset[train_idx][1].numpy().reshape(-1))\nsort_idx = sort_idx.reshape(-1, batch_size)\nnp.random.shuffle(sort_idx)\nsort_idx = sort_idx.reshape(-1)\ntrain_idx = train_idx[sort_idx]\n## ------------------------------\n\n## Sort valid by lengths --------\nvalid_idx_df = pd.DataFrame.from_dict({\n    'idx': valid_idx,\n    'length': train_dataset[valid_idx][1].numpy().reshape(-1)\n}).sort_values('length')\nvalid_idx = valid_idx_df['idx'].values\n## ------------------------------\n\ntrain, valid = torch.utils.data.Subset(train_dataset, train_idx), torch.utils.data.Subset(train_dataset, valid_idx)\nvalid_df = train_df.iloc[valid_idx, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_model_file = \"bert_pytorch.bin\"\ny_columns = [TARGET]\n\naccumulation_steps = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**モデル・オプティマイザーの作成とAPEXでFP16化する**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 2e-5\nwarmup = 0.05\nnum_train_optimization_steps = int(TOTAL_RECORD*((FOLD-1)/FOLD)*R_TRAIN/(batch_size*accumulation_steps))\nprint(f'lr: {lr}\\nwarmup: {warmup}\\nnum_train_optimization_steps: {num_train_optimization_steps}')\n\n## Define model & optimizer ----------------------------\nmodel = BertForSequenceClassification.from_pretrained(\"../working\", cache_dir=None, num_labels=train[0:1][2].shape[1]-1)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\nmodel.zero_grad();\nmodel = model.to(device);\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=warmup,\n                     t_total=num_train_optimization_steps)\n## -----------------------------------------------------\n\nif CONTINUE:\n    ## Reload model ----------------------------------------\n    checkpoint = torch.load('../input/'+CONT_MODEL_KERNEL+'/'+filename)\n    model.load_state_dict(checkpoint['state_dict']);\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if isinstance(v, torch.Tensor):\n                state[k] = v.to(device)\n    ## -----------------------------------------------------\nelse:\n    ## Convert digits form fp32 to fp16 --------------------\n    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n    ## -----------------------------------------------------\n\ndel param_optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**学習**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    start_time = time.time()\n    jigsaw_evel_valid = JigsawEvaluator(valid[:][2][:,2].numpy(), valid_df[cols_identity].values)\n    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n    \n    lossf = None\n    tk0 = tqdm_notebook(enumerate(train_loader), total=len(train_loader), leave=True)\n    print(f'Iterations: {len(train_loader)}')\n    optimizer.zero_grad()\n    log_df = pd.DataFrame()\n    for i,(x_batch, y_batch) in tk0:\n        model = model.train()\n        y_pred = model(x_batch[0].to(device), attention_mask=(x_batch[0]>0).to(device), labels=None)\n        loss = custom_loss(y_pred, y_batch.to(device))\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.zero_grad()\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n                        \n        if  (i!=0 and i%20000==0) or (i/len(tk0)>=R_TRAIN):\n            model = model.eval()\n            valid_preds, avg_val_loss = create_valid(valid)\n            valid_score = jigsaw_evel_valid.get_final_metric(valid_preds[:,0])\n            elapsed_time = time.time()-start_time\n            print(f'iter: {i}\\t loss: {lossf:.6f} \\t val_loss: {avg_val_loss:.6f} \\t time: {elapsed_time:.2f}s')\n            print(f'Valid:\\n\\t \\\n                auc:\\t{valid_score[0]:.6f}\\n\\t \\\n                overall:\\t{valid_score[1]:.6f}\\n\\t \\\n                subgroup:\\t{valid_score[2]:.6f}\\n\\t \\\n                bpsn:\\t{valid_score[3]:.6f}\\n\\t \\\n                bnsp:\\t{valid_score[4]:.6f}\\n\\t')\n            scores = 7\n            log_df = pd.concat([log_df, \n                               pd.DataFrame.from_dict(\n                               {'Iteration': [i]*scores,\n                                'Time': [elapsed_time]*scores,\n                                'Score_type': ['loss_train', 'loss_valid', 'auc', 'overall', 'subgroup', 'bpsn', 'bnsp'],\n                                'Score': [lossf, avg_val_loss]+[valid_score[i] for i in range(5)],\n                                'LR': [optimizer.get_lr()[0]]*scores\n                               })], axis=0)\n            \n        if i/len(tk0)>=R_TRAIN:\n            break\n\ncheckpoint = {'state_dict': model.state_dict()}\ntorch.save(checkpoint, filename)\nlog_df[['Iteration', 'Time', 'Score_type', 'Score', 'LR']].to_csv('log.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_dataset, train_df\n\ngc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\n\ngc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1 epoch分(3.5時間)学習させたモデルのインポート (3分クッキング)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(Data_dir, \"train.csv\")).sample(frac=1, random_state=SEED).reset_index(drop=True)\n## 3分クッキングの際にはコメントアウト\n# error_analysis = pd.concat([df.loc[valid_idx, [TARGET, TEXT_COL]+cols_identity].reset_index(drop=True), pd.DataFrame(valid_preds[:,0], columns=['Prediction'])], axis=1)\n# error_analysis = error_analysis[[TARGET, 'Prediction', TEXT_COL]+cols_identity].fillna(0)\n# error_analysis.to_csv('error_analysis.csv')\n\n## 代わりに下記を実行\nerror_analysis = pd.read_csv('../input/jigsaw-bert-demo-prep/error_analysis.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_labels = 8\ncheckpoint = torch.load('../input/jigsaw-bert-demo-prep/checkpoint.pth')\nmodel = BertForSequenceClassification(bert_config, num_labels=num_labels)\nmodel.load_state_dict(checkpoint['state_dict']);\n\nmodel = model.to(device);\nfor param in model.parameters():\n    param.requires_grad = False\nmodel = model.eval()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 推論"},{"metadata":{"trusted":true},"cell_type":"code","source":"def human_vs_ai():\n    i = np.random.randint(len(valid_idx))\n    print('=== Validation通し番号 ===')\n    print(f'{i}/{len(valid_idx)}\\n')\n    check_tokenizer(valid_idx[i])\n    display(pd.DataFrame(error_analysis.iloc[i:i+1,:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"human_vs_ai()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_test(X_test):\n    X_test = pd.Series(X_test)\n    X_test = convert_lines(X_test, MAX_SEQUENCE_LENGTH, tokenizer)\n    print(X_test)\n    lengths = pd.DataFrame(np.array([len([i for i in x if i!=0]) for x in X_test]), columns=['length']).astype('int16').values\n    \n    X_test = torch.utils.data.TensorDataset(\n                        torch.tensor(X_test, dtype=torch.long),\n                        torch.tensor(lengths, dtype=torch.int16)\n                    )\n    \n    batch_size = 16\n    test_preds = np.zeros(len(X_test))\n    test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n    \n#     tk0 = tqdm_notebook(test_loader)\n    tk0 = test_loader\n    for i,(x_batch, _) in enumerate(tk0):\n        pred = model(x_batch[0].to(device), attention_mask=(x_batch[0]>0).to(device), labels=None)\n        test_preds[i*batch_size:(i+1)*batch_size]=pred[:,0].detach().cpu().squeeze().numpy()\n        \n    print('\\n=== Toxic判定結果 ===')\n    print(f'{torch.sigmoid(torch.tensor(test_preds)).numpy()[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_test('Yuta is a great soccer player.')\n# create_test('Yuta is a wonderful soccer player.')\n# create_test('Yuta is a bad soccer player.')\n# create_test('Yuta is a garbage soccer player.')\n# create_test('Yuta took out the garbage in the morning.')\n# create_test('Yuta talked to a garbage man in the morning.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}