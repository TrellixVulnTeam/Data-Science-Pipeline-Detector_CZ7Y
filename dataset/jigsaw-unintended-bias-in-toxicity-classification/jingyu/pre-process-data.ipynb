{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# regression\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport sys\nfrom tqdm import tqdm\nfrom keras.preprocessing.sequence import pad_sequences\nimport time\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n# Any results you write to the current directory are saved as output.\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\ndef config_GPU():\n    GPU_list = get_available_gpus()\n    if len(GPU_list)>0:\n        from keras import backend as K\n        import tensorflow as tf\n        from keras.backend.tensorflow_backend import set_session\n        config = tf.ConfigProto()  \n        config.gpu_options.allow_growth = True  \n        config.gpu_options.per_process_gpu_memory_fraction = 0.99\n        set_session(tf.Session(config=config)) \n        print('GPU configed')\n    else:\n        print('Do not found GPU')\n\n\n\nclass process_description(object):\n    tqdm.pandas(desc=\"\")\n    def __init__(self,vec_dim = 128):\n        self.max_len = 0\n        self.vec_dim = vec_dim\n    def process_description_file(self,file,max_len =  0.99):\n        print('\\nProcessing text data...')\n        file = file.fillna('').str.lower()\n        #file = file.progress_apply(emoji.demojize)  \n        file = file.progress_apply(lambda x: self.process_string(x))\n        text_len = file.apply(len)\n        self.max_len = int(np.ceil(text_len.quantile(0.99)))\n        print('Percentage of quantile of sentence lenghts: 50%: {} 75%: {} 90%: {} 95%: {} 99%: {}'.format(text_len.quantile(0.5),text_len.quantile(0.75),\n                                                                     text_len.quantile(0.90),text_len.quantile(0.95),\n                                                                     text_len.quantile(0.99)))\n        print('The length of sentences: ',self.max_len)\n        print('Data processed sucessfully.')\n        return file\n    def process_string(self,s):\n        s = re.sub(r\"(\\W)\", r\" \\1 \", s)\n        s = re.sub('[0-9][0-9a-z]*', '<number>', s)\n        s = re.sub(r'[\" \"]+', \" \", s)\n        s = s.rstrip().strip()\n        s = s.split(' ')\n        return s\n\n    def padding_file(self,file,MAX_SEQUENCE_LENGTH = None):\n        if MAX_SEQUENCE_LENGTH:\n            max_len = MAX_SEQUENCE_LENGTH\n        else:\n            max_len = self.max_len\n        file = pad_sequences(file,\n            maxlen = max_len,\n            dtype=int,\n            padding = 'post',\n            truncating = 'post',\n            value = 0)\n        return file.astype(np.int32)\n\n    def token2vec(self,token):\n        if token in self.word2index:\n            return self.word2index[token]\n        else:\n            return 1\n    def sent2vec(self,sentence):\n        return np.array(list(map(self.token2vec,sentence)))\n    def text_to_vect(self,file,dictionary):\n        print('\\nConverting sentences to vectors')\n        self.word2index = dictionary\n        res = file.copy()\n        start_time = time.time()\n        for i in tqdm(range(len(file))):\n            res[i] = self.sent2vec(file[i])\n        #sent_vector = np.array(sent_vector)\n        print('Converted sucessfully. Time used:',time.time()-start_time)\n        return res\n\ndef explore_string(unprocess_data,ind,desc_parser):\n    start_time = time.time()\n    sentence_length = []\n    upper_counts =[]\n    upper_pct = []\n    puncts_count = []\n    puncts_pct = []\n    num_exclamation = []\n    num_question = []\n    num_token = []\n    num_unk =[]\n    pct_unk = []\n    #desc_parser = process_description()\n    for s in tqdm(unprocess_data):\n        len_s = len(s)\n        sentence_length.append(len_s)\n        upper_counts.append(len([ c for c in s if c.isupper()]))\n        upper_pct.append(len([ c for c in s if c.isupper()])/len_s)\n        puncts_count.append(len(re.findall(\"\\W\",s)))\n        puncts_pct.append(len(re.findall(\"\\W\",s))/len_s)\n        num_exclamation.append(s.count('!'))\n        num_question.append(s.count('?'))\n        \n        s_tokens = desc_parser.process_string(s)\n        num_token.append(len(s_tokens))\n        num_unk.append(len([t for t in s_tokens if t not in ind]))\n        pct_unk.append(len([t for t in s_tokens if t not in ind])/len(s_tokens))\n    print('mining text using: {} min'.format((time.time() - start_time)/60))\n    numerical_data = pd.DataFrame({'string_length':sentence_length,\n                        'upper_number':upper_counts,\n                        'upper_percentage':upper_pct,\n                        'puncts_number':puncts_count,\n                        'puncts_percentage':puncts_pct,\n                        '\"!\"_number':num_exclamation,\n                        '\"?\"_number':num_question,\n                        'words_number':num_token,\n                        'outside_dictionary_number':num_unk,\n                        'outside_dictionary_percentage':pct_unk})\n    for col in numerical_data:\n        if numerical_data[col].dtypes == np.int64:\n            numerical_data[col] = numerical_data[col].astype(np.int32)\n        elif numerical_data[col].dtypes == np.float64:\n            numerical_data[col] = numerical_data[col].astype(np.float32)\n\n    return numerical_data\n\ndef creating_embedding_weights(glove_embedding_index,ind):\n    #Creating word index\n    print(len(ind))\n    # if the word not in the embedding, using <unk> instead. The vector of <unk> is the mean of all vectors, the <pad> keeps 0\n    word_index = {'<pad>':0,'<unk>':1}\n    for i,word in enumerate(ind.keys()):\n        word_index[word] = i+2\n    print(\"Number of unique tokens: \",len(word_index))\n    \n    #creating matrix\n    EMBEDDING_DIM = glove_embedding_index['i'].shape[0]\n    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = glove_embedding_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n\n    embedding_matrix[1] = embedding_matrix.mean(axis = 0)\n    print(embedding_matrix.shape)\n    return word_index,embedding_matrix.astype(np.float32)\n\ndef extract_useful_glove(tokens,glove_embedding_index):\n    total_tokens = list(itertools.chain.from_iterable(tokens))\n    unique_tokens = set(total_tokens)\n    len_unique_tokens = len(unique_tokens)\n    len_glove = len(glove_embedding_index)\n    len_tokens = len(total_tokens)\n    ood = {}\n    ind = {}\n    within = 0\n    for token in tqdm(total_tokens):\n        if token in  glove_embedding_index:\n            within += 1\n            if token in ind:\n                ind[token] += 1\n            else:\n                ind[token] = 1\n\n        else:\n            if token in ood:\n                ood[token] += 1\n            else:\n                ood[token] =1\n    len_ood = len(ood)\n    len_ind = len(ind)\n    useless_glove = set(glove_embedding_index.keys()).difference(set(ind.keys()))\n    print(\"{} of unique tokens were embedded.\\n{} of text were embeded\\n{} of Glove keys were usesless\".format(\n        len_ind/(len_unique_tokens),  within/len_tokens, len(useless_glove)/len_glove))\n    \n    ood = {k: v for k, v in sorted(ood.items(), key=lambda x: x[1],reverse = True)}\n    return ood,ind\n\ndef orgnize_embedding_weights(file_path,text):\n    print('Loading GloVe...')\n    glove_embedding_index = {}\n    f = open(file_path)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        glove_embedding_index[word] = coefs\n    f.close()\n    print('Found %s word vectors.' % len(glove_embedding_index))\n    \n    ood,ind = extract_useful_glove(text,glove_embedding_index)\n    word_index,embedding_matrix = creating_embedding_weights(glove_embedding_index,ind)\n    joblib.dump(embedding_matrix.astype(np.float32),\"embedding_matrix.joblib\")\n    return word_index,ind\n\n\ndef load_data(train_path,test_path,nrows):\n    usecols = ['id','comment_text','target','male','female','homosexual_gay_or_lesbian','christian','jewish',\n              'muslim','black','white','psychiatric_or_mental_illness']\n    dtype = {'id':np.int32,'target':np.float32,'male':np.float32,'female':np.float32,\n             'homosexual_gay_or_lesbian':np.float32,'christian':np.float32,'jewish':np.float32,\n              'muslim':np.float32,'black':np.float32,'white':np.float32,'psychiatric_or_mental_illness':np.float32}\n    train = pd.read_csv(train_path,usecols = usecols,nrows = nrows,dtype = dtype)\n    test = pd.read_csv(test_path,usecols = ['id','comment_text'],nrows = nrows,dtype = {'id':np.int32})\n    print(\"Train data shape: {}\\tTest data shape: {}\".format(train.shape,test.shape))\n    train_0 = train[train.target == 0]\n    train_non_0 = train[train.target != 0]\n    del train\n    # clean duplicates\n    print('zero train data length: ',len(train_0))\n    train_0 = train_0.drop_duplicates(subset = ['comment_text'])\n    print('zero train data length after drop-duplicates: ',len(train_0))\n    #train_0 = train_0.sample(frac=0.5, random_state=37)\n    print('non-zero train data length: ',len(train_non_0))\n    train_non_0 = train_non_0.drop_duplicates(subset = ['comment_text','target'])\n    print('non-zero train data length after drop-duplicates: ',len(train_non_0))\n    # merge text\n    total_text = pd.concat([train_0,train_non_0,test],\n                           ignore_index = True,\n                          sort = False)\n    print('train & test combine data shape: ',total_text.shape)\n    return total_text\ndef split_data(df):\n    numerical_cols = ['string_length','upper_number','upper_percentage','puncts_number',\n                        'puncts_percentage', '\"!\"_number','\"?\"_number','words_number',\n                        'outside_dictionary_number','outside_dictionary_percentage']\n    train_total = df[~df.target.isna()]\n    print('saving train data...')\n    train_total.to_csv('train_processed.csv',index = False)\n    print('train data saved.')\n    train_total.info()\n    print(train_total.shape)\n    del train_total\n    \n    test_total = df[df.target.isna()][['id','comment_text'] + numerical_cols]\n    print('saving test data...')\n    test_total.to_csv('test_processed.csv',index = False)\n    print('test data saved.')\n    test_total.info()\n    #submission = pd.DataFrame({'id': test_total['id'].values, 'prediction': 0})\n    print(test_total.shape)\n    #X_train_text = np.array(list(train_total['comment_text']))\n    #X_test_text = np.array(list(test_total['comment_text']))\n    \n    #X_train_num = train_total[numerical_cols].values\n    #X_test_num = test_total[numerical_cols].values\n    \n    #print('X_train_text shape: {}\\tX_test_text shape: {}'.format(X_train_text.shape,X_test_text.shape))\n    #print('X_train_num shape: {}\\tX_test_num shape: {}'.format(X_train_num.shape,X_test_num.shape))\n    \n    #regression\n    #y_train = train_total['target']\n        #y_train = (train_total['target'] >= 0.5).astype(int)\n\n    #print(\"y_train.shape: \",y_train.shape)\n        #y_stat = y_train.value_counts()\n        #y_stat = y_stat/y_stat.sum()\n        #print('y percentage: ',dict(y_stat))\n    \n        #class_weight = dict(1-y_stat)\n    \n    #save data\n    #joblib.dump(X_train_num,\"X_train_num.joblib\")\n    #joblib.dump(X_test_num,\"X_test_num.joblib\")\n    \n    #joblib.dump(X_train_text,\"X_train_text.joblib\")\n    #joblib.dump(X_test_text,\"X_test_text.joblib\") \n    \n    #joblib.dump(y_train,\"y_train.joblib\")\n    #return X_train.astype(np.float32),y_train.astype(np.float32),X_test.astype(np.float32),None,submission","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"def main():\n    DEBUG = False\n    if DEBUG:\n        NROWS = 1000\n        EPOCHS = 3\n        MAX_SEQUENCE_LENGTH = 10\n        glove_path = \"../input/glove-twitter/glove.twitter.27B.25d.txt\"\n    else:\n        NROWS = None\n        EPOCHS = 10\n        MAX_SEQUENCE_LENGTH = 100\n        glove_path = \"../input/glove-twitter-100d/glove.twitter.27B.100d.txt\"\n    KFOLD = 5\n    EMBEDDING_DIM = 157\n    time_dict = {}\n    time_dict['Start_time'] = time.time()\n\n\n    #loading data\n    print('\\n**Loading data...**')\n    train_path = \"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\"\n    test_path = \"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\"\n    total_text = load_data(train_path,test_path,NROWS)\n    #print(total_text.tail())\n    time_dict['load_data'] = time.time()\n\n    #creating weights\n    print('\\n**Creating embedding weights...**')\n    desc_parser = process_description()\n    total_text['processed_text'] = list(desc_parser.process_description_file(total_text['comment_text']))\n    word_index,ind = orgnize_embedding_weights(glove_path ,total_text['processed_text'])\n    time_dict['emb_weight'] = time.time()\n    #print('matrix shape',embedding_matrix.shape)\n\n    #numerical_df\n    numerical_df = explore_string(total_text['comment_text'],ind,desc_parser)\n    del ind\n\n\n    #converting data\n    print('\\n**Converting data...**')\n    total_text['processed_text'] = list(desc_parser.text_to_vect(total_text['processed_text'],word_index))\n    del word_index\n    total_text['processed_text'] = list(desc_parser.padding_file(total_text['processed_text'],MAX_SEQUENCE_LENGTH ))\n    del desc_parser\n\n    #concat text and numerical\n    total_text = pd.concat([total_text,numerical_df],axis = 1, sort = False)\n    del numerical_df\n    #print(total_text.head())\n    #X_train,y_train,X_test,class_weight,submission = split_data(total_text)\n    split_data(total_text) \n    time_dict['convert_data'] = time.time()\n    del total_text\n    '''\n    X_train_num = joblib.load(\"X_train_num.joblib\")\n    X_test_num = joblib.load(\"X_test_num.joblib\")\n\n    X_train_text = joblib.load(\"X_train_text.joblib\")\n    X_test_text = joblib.load(\"X_test_text.joblib\") \n    y_train = joblib.load(\"y_train.joblib\")\n\n    train_total = pd.read_csv('train_processed.csv')\n    test_total = pd.read_csv('test_processed.csv')\n    '''\nif __name__ == \"__main__\":\n    main()","execution_count":4,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/1993 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"\n**Loading data...**\nTrain data shape: (1000, 12)\tTest data shape: (1000, 2)\nzero train data length:  795\nzero train data length after drop-duplicates:  788\nnon-zero train data length:  205\nnon-zero train data length after drop-duplicates:  205\ntrain & test combine data shape:  (1993, 12)\n\n**Creating embedding weights...**\n\nProcessing text data...\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1993/1993 [00:00<00:00, 6765.88it/s]\n","name":"stderr"},{"output_type":"stream","text":"Percentage of quantile of sentence lenghts: 50%: 43.0 75%: 88.0 90%: 156.0 95%: 192.0 99%: 217.07999999999993\nThe length of sentences:  218\nData processed sucessfully.\nLoading GloVe...\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 126178/126178 [00:00<00:00, 1037201.38it/s]","name":"stderr"},{"output_type":"stream","text":"Found 1193515 word vectors.\n0.9409407665505226 of unique tokens were embedded.\n0.9773653093249219 of text were embeded\n0.990949422504116 of Glove keys were usesless\n10802","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"},{"output_type":"stream","text":"\nNumber of unique tokens:  10804\n(10805, 25)\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1993/1993 [00:00<00:00, 3810.20it/s]\n100%|██████████| 1993/1993 [00:00<00:00, 14110.75it/s]\n","name":"stderr"},{"output_type":"stream","text":"mining text using: 0.008788331349690755 min\n\n**Converting data...**\n\nConverting sentences to vectors\nConverted sucessfully. Time used: 0.14409399032592773\n(993, 23)\n(1000, 12)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}