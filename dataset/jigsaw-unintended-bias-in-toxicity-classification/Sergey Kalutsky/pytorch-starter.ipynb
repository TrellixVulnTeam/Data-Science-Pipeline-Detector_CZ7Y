{"cells":[{"metadata":{},"cell_type":"markdown","source":"Nothig particularly exciting. Pytorch implementation of a great starting kernel https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n\nNotebook is based on https://www.kaggle.com/hung96ad/pytorch-starter"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\ntqdm.pandas()\n\nprint(os.listdir(\"../input\"))\n","execution_count":6,"outputs":[{"output_type":"stream","text":"['jigsaw-unintended-bias-in-toxicity-classification', 'fasttext-crawl-300d-2m']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  mask |= (ar1 == a)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1,300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embeddings_index[word]\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n    return embedding_matrix","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\ntrain['target'] = train['target'].apply(lambda x: 1 if x > 0.5 else 0)\ny_train = train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\n\ndel tokenizer\ngc.collect()","execution_count":5,"outputs":[{"output_type":"stream","text":"fitting tokenizer\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = load_embeddings()","execution_count":7,"outputs":[{"output_type":"stream","text":"2000001it [02:15, 14725.40it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(word_index, embeddings_index)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_index\ngc.collect()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refactored based on reasonable remarks\n# of @ddanevskyi https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79911\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        hidden_size = 64\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout2d(0.2) \n        self.lstm = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.lstm_attention = Attention(hidden_size*2, maxlen)\n        \n        self.out = nn.Linear(384, 1)\n\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding.transpose(1,2).unsqueeze(-1)).squeeze().transpose(1,2)\n\n        h_lstm, _ = self.lstm(h_embedding)\n        h_lstm_atten = self.lstm_attention(h_lstm)\n\n        avg_pool = torch.mean(h_lstm, 1)\n        max_pool, _ = torch.max(h_lstm, 1)\n        \n        conc = torch.cat((h_lstm_atten, avg_pool, max_pool), 1)\n        out = self.out(conc)\n        \n        return out\n    \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stolen from https://github.com/Bjarten/early-stopping-pytorch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nsplits = list(KFold(n_splits=5).split(X_train, y_train))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 2048\nNUM_EPOCHS = 100\n\ntrain_preds = np.zeros((len(X_train)))\ntest_preds = np.zeros((len(X_test)))\n\nx_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n    \n    model = NeuralNet()\n    model.cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)\n    \n    early_stopping = EarlyStopping(patience=3, verbose=True)\n    \n    print(f'Fold {i + 1}')\n    \n    for epoch in range(NUM_EPOCHS):\n        start_time = time.time()\n        \n        model.train()\n        avg_loss = 0.\n        for x_batch, y_batch in tqdm(train_loader, disable=True):\n            optimizer.zero_grad()\n            y_pred = model(x_batch)\n            loss = loss_fn(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n        \n        model.eval()\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros(len(X_test))\n        avg_val_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n            valid_preds_fold[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, NUM_EPOCHS, avg_loss, avg_val_loss, elapsed_time))\n        \n        early_stopping(avg_val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n        \n    # load the last checkpoint with the best model\n    model.load_state_dict(torch.load('checkpoint.pt'))\n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / len(splits)    ","execution_count":23,"outputs":[{"output_type":"stream","text":"Fold 1\nEpoch 1/100 \t loss=288.8000 \t val_loss=204.8402 \t time=120.99s\nValidation loss decreased (inf --> 204.840202).  Saving model ...\nEpoch 2/100 \t loss=208.0190 \t val_loss=196.7564 \t time=121.54s\nValidation loss decreased (204.840202 --> 196.756409).  Saving model ...\nEpoch 3/100 \t loss=195.5492 \t val_loss=185.1824 \t time=120.59s\nValidation loss decreased (196.756409 --> 185.182407).  Saving model ...\nEpoch 4/100 \t loss=188.8798 \t val_loss=183.0288 \t time=118.08s\nValidation loss decreased (185.182407 --> 183.028750).  Saving model ...\nEpoch 5/100 \t loss=184.8278 \t val_loss=179.7010 \t time=118.06s\nValidation loss decreased (183.028750 --> 179.700984).  Saving model ...\nEpoch 6/100 \t loss=181.3200 \t val_loss=181.3880 \t time=119.95s\nEarlyStopping counter: 1 out of 3\nEpoch 7/100 \t loss=178.8734 \t val_loss=182.0243 \t time=121.55s\nEarlyStopping counter: 2 out of 3\nEpoch 8/100 \t loss=175.9853 \t val_loss=179.4586 \t time=117.45s\nValidation loss decreased (179.700984 --> 179.458582).  Saving model ...\nEpoch 9/100 \t loss=173.6409 \t val_loss=177.3540 \t time=121.34s\nValidation loss decreased (179.458582 --> 177.354008).  Saving model ...\nEpoch 10/100 \t loss=171.6012 \t val_loss=178.0936 \t time=119.30s\nEarlyStopping counter: 1 out of 3\nEpoch 11/100 \t loss=170.4765 \t val_loss=189.5981 \t time=120.83s\nEarlyStopping counter: 2 out of 3\nEpoch 12/100 \t loss=168.4031 \t val_loss=177.9866 \t time=121.41s\nEarlyStopping counter: 3 out of 3\nEarly stopping\nFold 2\nEpoch 1/100 \t loss=278.8018 \t val_loss=201.9863 \t time=121.50s\nValidation loss decreased (inf --> 201.986294).  Saving model ...\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-95edb459927e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5, train_preds)","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"0.9699075479795565"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"        id  prediction\n0  7000000    0.001772\n1  7000001    0.000318\n2  7000002    0.006042\n3  7000003    0.000812\n4  7000004    0.971468","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7000000</td>\n      <td>0.001772</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7000001</td>\n      <td>0.000318</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000002</td>\n      <td>0.006042</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7000003</td>\n      <td>0.000812</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7000004</td>\n      <td>0.971468</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":22,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}