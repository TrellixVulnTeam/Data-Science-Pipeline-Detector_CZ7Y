{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is an attempt to list as many as possible ways to augment or parse more data for the \"jigsaw II competition\".\nWork in progress."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport datetime as dt","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"### K-nearest neuborghs\n_____________________________\nThe idea is, using embedding and k-nn to find \"close\" words to each other.\nWas tried:\n\nhttps://www.kaggle.com/theoviel/using-word-embeddings-for-data-augmentation\n\nhttps://www.kaggle.com/shujian/fake-some-positive-data-data-augmentation\n\nDidn't work for people, who tried, so I've skiped implementation\n"},{"metadata":{},"cell_type":"markdown","source":"### Marcov chains\n_______________________\nWas tried in\nhttps://www.kaggle.com/jpmiller/extending-train-data-with-markov-chains\n\nDidn't work for people, who tried, so I've skiped implementation"},{"metadata":{},"cell_type":"markdown","source":"### Translation\n-------------------------\nProposed in https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038#272289\nby @pavelost\n\nThe code and instruction can be found in my GitHub repo: https://github.com/PavelOstyakov/toxic/tree/master/tools"},{"metadata":{},"cell_type":"markdown","source":"# Third party data sources"},{"metadata":{},"cell_type":"markdown","source":"### First jigsaw competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/jigsaw-toxic-comment-classification-challenge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['toxic'] = data[data.columns[2:]].sum(axis=1)\ntoxic = data[data['toxic'] > 0]\nnot_toxic = data[data['toxic'] == 0]\nprint(f'toxic comments: {toxic.shape[0]}, normal: {not_toxic.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wikipedia Talk Labels: Personal Attacks\n-------------\nhttps://figshare.com/articles/Wikipedia_Detox_Data/4054689"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/wikipedia-talk-labels-personal-attacks/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_commets = pd.read_csv('../input/wikipedia-talk-labels-personal-attacks/attack_annotated_comments.csv')\ndata_attack =  pd.read_csv('../input/wikipedia-talk-labels-personal-attacks/attack_annotations.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_attack.drop(columns=data_attack.columns[1:-1], inplace=True)\nsummery = data_attack.groupby(['rev_id']).sum()\ndata = data_commets.set_index('rev_id').join(summery)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxic = data[data['attack'] > 0]\nnot_toxic = data[data['attack'] == 0]\nprint(f'toxic comments: {toxic.shape[0]}, normal: {not_toxic.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wikipedia Talk Corpus\n-----------\nhttps://figshare.com/articles/Wikipedia_Talk_Corpus/4264973"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/wikipedia-talk-corpus-sample/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The whole corpus is too large to load in kernel, here is a small sample\ndata = pd.read_csv('../input/wikipedia-talk-corpus-sample/chunk_0.tsv', sep='\\t')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reddit\n--------------------------------\nwill use https://github.com/dmarx/psaw \nA minimalist wrapper for searching public reddit comments/submissions via the pushshift.io API."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install psaw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of general use\nfrom psaw import PushshiftAPI\napi = PushshiftAPI()\n\n# The `search_comments` and `search_submissions` methods return generator objects\ngen = api.search_submissions(limit=100)\nresults = list(gen)\n\n# There are 2 main attributes we may be interested in:\n# title - provides the title of a submission\nprint(results[1].title)\n# selftext - provides main text, if text exists\nprint(results[1].selftext)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Toxic Reddit"},{"metadata":{},"cell_type":"markdown","source":"[ShitRedditSays](https://www.reddit.com/r/ShitRedditSays/) is\nself appointed reddit hate speech watchdog.\nAccording to web.archive.org subreddit started in the midst of 2011"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start time\nstart_epoch=int(dt.datetime(2017, 1, 1).timestamp())\n# Found submissions\nshit_reddit_says = list(api.search_submissions(after=start_epoch,\n                                               subreddit='ShitRedditSays',\n                                               filter=['url','author', 'title', 'subreddit'],\n                                               limit=10))\n\n# Some questionable comments\nfor i in range (6):\n    print(shit_reddit_says[i].title, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wholsome reddit"},{"metadata":{},"cell_type":"markdown","source":"#### Male\n* [MensRights](https://www.reddit.com/r/MensRights/)\n* [AskMen](https://www.reddit.com/r/AskMen)\n\n#### Female\n* [AskWomen](https://www.reddit.com/r/AskWomen)\n* [women](https://www.reddit.com/r/women/)\n* [femenism](https://www.reddit.com/r/femenism/)\n\n#### homosexual_gay_or_lesbian\n* [lgbt](https://www.reddit.com/r/lgbt/)\n* [gaybros](https://www.reddit.com/r/gaybros/)\n* [LesbianActually](https://www.reddit.com/r/LesbianActually/)\n\n#### christian\n* [Catholicism](https://www.reddit.com/r/Catholicism/)\n\n#### Jewish \n* [Jewish](https://www.reddit.com/r/Jewish/)\n* [Judaism](https://www.reddit.com/r/Judaism/)\n\n#### muslim\n* [islam](https://www.reddit.com/r/islam/)\n\n#### black\n* [BlackPeopleTwitter](https://www.reddit.com/r/BlackPeopleTwitter/)\n* [Blackfellas](https://www.reddit.com/r/Blackfellas/)\n* [AsABlackMan](https://www.reddit.com/r/AsABlackMan/)\n\n#### white\nWasn't able to find anything particularly positive)))\n\n#### psychiatric_or_mental_illness\n* [mentalhealth](https://www.reddit.com/r/mentalhealth/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start time\nstart_epoch=int(dt.datetime(2018, 1, 1).timestamp())\n# Found submissions\nlgbt = list(api.search_submissions(after=start_epoch,\n                                               subreddit='lgbt',\n                                               filter=['url','author', 'title', 'subreddit'],\n                                               limit=10))\n\n# Some positive comments\nfor i in range (6):\n    print(lgbt[i].title, '\\n')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}