{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import copy\nimport operator\nimport os,glob,re\nimport numpy as np\nfrom sklearn import metrics\nfrom string import punctuation\nfrom collections import defaultdict\nfrom collections import Counter\nfrom collections import OrderedDict\nfrom textblob.classifiers import NaiveBayesClassifier\nimport pandas as pd","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train22neg = open('train_neg.txt')","execution_count":18,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train_neg.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e0ad804db310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain22neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_neg.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_neg.txt'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ajuste (x):\n    return 'ID-' + x\n    ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test['id']=dataset_test['id'].astype('str').apply(ajuste)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train['id']=dataset_train['id'].astype('str').apply(ajuste)\ndataset_train['label_target'] = pd.cut(dataset_train['target'], [-1,0.4999,1], labels=['no_toxic','toxic'])\ndf_train_pos = dataset_train.loc[(dataset_train['label_target'])=='toxic']\ntrain_pos = df_train_pos[['id','comment_text']]\ndf_train_neg = dataset_train.loc[(dataset_train['label_target'])=='no_toxic']\ntrain_neg = df_train_neg[['id','comment_text']]\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Listas\nlist_negative_word = []\nlist_positive_word = []\nUnique_list_positive_word = []\nUnique_list_negative_word = []\ncwd = os.getcwd() \ncount_positive_ID = 0\ncount_negative_ID = 0\nNegative_Word_Dict = defaultdict()\nPositive_Word_Dict = defaultdict()\nSentence_Sentiment = {}\nEvaluation_Dict = OrderedDict()\n\n# Lista de palavras frequentes\nfrequent_word_list = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nID_Val = []\n\n# Limpeza de dados usando expressões regulares. \n# Removemos as palavras frequentemente repetidas como descrito na lista acima. \n# Eliminamos todos os tipos de pontuação - como.,!, -, etc.\nfor line in open(\"../input/bases_v1 /train_pos.txt\", encoding=\"utf8\").readlines():\n    if line.strip():\n        word_split = line.replace(',',' ').replace('.',' ').replace('!',' ').replace('--',' ').split()\n        for each_word in word_split:\n                        if re.match(r'[IDidIdiD].*-[0-9].*',each_word):\n                            count_negative_ID+=1\n                            ID_Val.append(each_word)\n                            continue\n                        elif re.match(r'(\\d+)\\.(\\d+)+',each_word):\n                            continue\n                        elif re.match(r'(\\d+)+',each_word):\n                            continue \n                        elif re.match(r'$(\\d+)+',each_word):\n                            continue  \n                        elif re.match(r'[A-Za-z]*\\.+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\?+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\!+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'-+',each_word):\n                            continue\n                        elif re.match(r'[A-Za-z]*\\)+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'\\(+[A-Za-z]*',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\,+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                            \n                        if each_word in frequent_word_list:\n                            continue\n                        else:\n                            list_negative_word.append(each_word)\n\n# Alimenta as listas\nList_Size_Negative = len(list_negative_word)\nNegative_Word_Dict =  Counter(list_negative_word)\nDenominator_Sum_Neg = sum(Negative_Word_Dict.values())\nUnique_list_negative_word = set(list_negative_word)\nUnique_List_Size_Negative = len(Unique_list_negative_word)\nlist_positive_word = []\n\n# Limpeza de dados usando expressões regulares. \nfor line in open(\"../input/bases_v1 /train_neg.txt\", encoding=\"utf8\").readlines():\n    if line.strip():\n        word_split = line.split()\n        word_split = line.replace(',',' ').replace('.',' ').replace('!',' ').replace('--',' ').split()\n        for each_word in word_split:\n                        if re.match(r'ID-[0-9].*',each_word):\n                            count_positive_ID+=1\n                            ID_Val = each_word\n                            continue\n                        elif re.match(r'(\\d+)\\.(\\d+)+',each_word):\n                            continue\n                        elif re.match(r'(\\d+)+',each_word):\n                            continue \n                        elif re.match(r'$(\\d+)+',each_word):\n                            continue  \n                        elif re.match(r'[A-Za-z]*\\.+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\?+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\!+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'-+',each_word):\n                            continue\n                        elif re.match(r'[A-Za-z]*\\)+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'\\(+[A-Za-z]*',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\,+',each_word):\n                            each_word = re.sub(r'[^\\w\\s]','',each_word)\n                        elif re.match(r'[A-Za-z]*\\/+[A-Za-z]*',each_word):\n                            list_word = each_word.split('/')\n                            if list_word[0] not in frequent_word_list:\n                                list_negative_word.append(list_word [0])\n                            else:\n                                continue\n                            \n                            if list_word[1] not in frequent_word_list:\n                                list_negative_word.append(list_word [1])\n                            else:\n                                continue\n                        if each_word in frequent_word_list:\n                            continue\n                        else:\n                            list_positive_word.append(each_word)","execution_count":14,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/bases_v1 /train_pos.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e79f6ca46da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Removemos as palavras frequentemente repetidas como descrito na lista acima.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Eliminamos todos os tipos de pontuação - como.,!, -, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/bases_v1 /train_pos.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mword_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/bases_v1 /train_pos.txt'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alimenta as listas\nList_Size_Positive = len(list_positive_word)\nPositive_Word_Dict = Counter(list_positive_word)\nDenominator_Sum_Pos = sum(Positive_Word_Dict.values())\nUnique_list_positive_word = set(list_positive_word)\nUnique_List_Size_Positive = len(Unique_list_positive_word)\nTotal_No_of_Docs = count_positive_ID + count_negative_ID\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imprime um resumo até aqui\nprint(\"\\n************ Executando Exemplo1 ************\")\nprint(\"\\nResumo do Dataset: \")\nprint (\"count_positive_ID - \",count_positive_ID)\nprint (\"count_negative_ID - \",count_negative_ID)\nprint (\"Número Total de docs - \",Total_No_of_Docs)\n\nLog_Prior_Positive = np.log10(float(count_positive_ID)/float(Total_No_of_Docs))\nLog_Prior_Negative = np.log10(float(count_negative_ID)/float(Total_No_of_Docs))\n\nNegative_Word_Dict_final = defaultdict()\nPositive_Word_Dict_final = defaultdict()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando valores de estimativa de máxima verossimilhança para cada palavra no conjunto de treinamento positivo\nfor key in Positive_Word_Dict:    \n    val = np.log10 (float(Positive_Word_Dict[key] + 1)/float(Denominator_Sum_Pos + Unique_List_Size_Positive))\n    Positive_Word_Dict_final.update({key:val})\n\n# Criando valores de estimativa de máxima verossimilhança para cada palavra no conjunto de treinamento negativo\nfor key in Negative_Word_Dict:    \n    val =  np.log10(float(Negative_Word_Dict[key] + 1)/float(Denominator_Sum_Neg + Unique_List_Size_Negative))\n    Negative_Word_Dict_final.update({key:val})\n\nNegative_Probability_Sentence = 0.00\nPositive_Probability_Sentence = 0.00\nSentence_Token_List = []\nSentence_List = []\n\n# Abre o dataset de teste\nwith open(\"teste/teste.txt\",'r',encoding=\"utf8\") as f:\n   token =  [line.split() for line in f]\n   for each_word in token:\n        Sentence_Token_List.append(each_word)\n\nTest_Review_Dict = OrderedDict()\nTest_Review_Class = OrderedDict()\ntemp_dict = {}\nList_Tokens_ID = []\ntemp = []\n\n# Processo de limpeza nos dados de teste\nfor i in range (0,len(Sentence_Token_List)):\n    for j in range (0,len(Sentence_Token_List[i])):\n        word = Sentence_Token_List[i][j]\n        if re.match(r'ID-[0-9].*',word):\n            Id_Value = word\n            continue\n        elif re.match(r'(\\d+)\\.(\\d+)+',word):\n            continue\n        elif re.match(r'(\\d+)+',word):\n            continue \n        elif re.match(r'$(\\d+)+',word):\n            continue  \n        elif re.match(r'[A-Za-z]*\\.+',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'[A-Za-z]*\\?+',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'[A-Za-z]*\\!+',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'-+',word):\n            continue\n        elif re.match(r'[A-Za-z]*\\)+',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'\\(+[A-Za-z]*',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'[A-Za-z]*\\,+',word):\n            word = re.sub(r'[^\\w\\s]','',word)\n        elif re.match(r'[A-Za-z]*\\/+[A-Za-z]*',word):\n            list_word = word.split('/')\n            if list_word[0] in frequent_word_list:\n                continue\n            else:\n                List_Tokens_ID.append(list_word[0])\n            if list_word[1] in frequent_word_list:\n                continue\n            else:\n                List_Tokens_ID.append(list_word[0])\n                \n        if (word not in frequent_word_list):\n                List_Tokens_ID.append(word)\n    temp = copy.copy(List_Tokens_ID)\n    Test_Review_Dict.update ({Id_Value:temp})\n    List_Tokens_ID[:] = []\n\nfor each_key,each_value in Test_Review_Dict.items():\n    ID_Value = each_key\n    for val in each_value:\n        if ((val in Negative_Word_Dict_final) and (val in Positive_Word_Dict_final)):\n            Negative_Probability_Sentence += Negative_Word_Dict_final[val]\n            Positive_Probability_Sentence += Positive_Word_Dict_final[val]\n\n    Positive_Probability_Sentence+=Log_Prior_Positive\n    Negative_Probability_Sentence+=Log_Prior_Negative\n    temp_dict.update({'0': Positive_Probability_Sentence})\n    temp_dict.update({'1': Negative_Probability_Sentence})\n    #print (\"ID- Value : {}, Negative : {} , Positive : {}\".format(ID_Value, Negative_Probability_Sentence,Positive_Probability_Sentence))\n    key = [k for k,v in temp_dict.items() if v==max(temp_dict.values())][0] \n    Test_Review_Class.update ({ID_Value:key})\n    temp_dict.clear()\n    Positive_Probability_Sentence = 0.0\n    Negative_Probability_Sentence = 0.0\n    \nwith open(\"output/classificacoes_math.txt\",'w') as ofile:\n        for keys,values in Test_Review_Class.items():\n            ofile.write((str(keys) + '\\t' + values + '\\n'))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}