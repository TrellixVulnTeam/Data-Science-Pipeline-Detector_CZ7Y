{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom tqdm import tqdm\ntqdm.pandas()\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMBEDDING_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = load_embeddings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = train[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntest_sentences = test[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntrain_vocab = build_vocab(train_sentences)\ntest_vocab = build_vocab(test_sentences)\nprint({k: train_vocab[k] for k in list(train_vocab)[:5]})\nprint({k: test_vocab[k] for k in list(test_vocab)[:5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    print(\"Total words common in both vocabulary and in embeddings_index\",len(a))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oov = check_coverage(train_vocab,embeddings_index)\ntest_oov = check_coverage(test_vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_vocab\ndel test_vocab\ndel train_sentences\ndel test_sentences\ndel train_oov\ndel test_oov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"1950's\": \"1950s\", \"1983's\": \"1983\", \"ain't\": \"is not\", \"aren't\": \"are not\", \"Bretzing's\": \"\", \"Bundycon's\": \"Bundycon\", \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"C'mon\": \"Come on\", \"Denzel's\": \"Denzel\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"Don't\": \"Do not\", \"don't\": \"do not\", \"Farmer's\": \"Farmers\", \"FBI's\": \"FBI\", \"Ferguson's\": \"Ferguson\", \"Hammond's\": \"Hammond\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"Haven't\": \"Have not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"Here's\": \"Here is\", \"here's\": \"here is\",\"he'll\": \"he will\", \"he's\": \"he is\", \"He's\": \"He is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"I'd\": \"I had\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"I'm\": \"I am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"It's\": \"it is\", \"Kay's\": \"Kay\", \"let's\": \"let us\", \"Let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"Medford's\": \"Medford\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"Murphy's\": \"Murphys\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"Paula's\": \"Paula\", \"Portland's\": \"Portlands\", \"Portlander's\": \"Portlanders\", \"publication's\": \"publications\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"She's\": \"She is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"Tastebud's\": \"Tastebuds\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"That's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"There's\": \"There is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"They're\": \"They are\", \"they've\": \"they have\", \"to've\": \"to have\", \"Trump's\": \"trump is\", \"U.S.\": \"United state\", \"U.S\": \"United state\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"We'll\": \"We will\", \"we'll've\": \"we will have\", \"Wendy's\": \"Wendy\", \"we're\": \"we are\", \"We're\": \"We are\", \"we've\": \"we have\", \"We've\": \"We have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"What's\": \"What is\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"Who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"Wouldn't\": \"Would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"You'd\": \"You had\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"You're\": \"you are\", \"you've\": \"you have\", \"Zoo's\": \"zoos\", \"zoo's\": \"zoos\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\ntest['comment_text'] = test['comment_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = train[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntest_sentences = test[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntrain_vocab = build_vocab(train_sentences)\ntest_vocab = build_vocab(test_sentences)\nprint({k: train_vocab[k] for k in list(train_vocab)[:5]})\nprint({k: test_vocab[k] for k in list(test_vocab)[:5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oov = check_coverage(train_vocab,embeddings_index)\ntest_oov = check_coverage(test_vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_vocab\ndel test_vocab\ndel train_sentences\ndel test_sentences\ndel train_oov\ndel test_oov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest['comment_text'] = test['comment_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = train[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntest_sentences = test[\"comment_text\"].progress_apply(lambda x: x.split()).values\ntrain_vocab = build_vocab(train_sentences)\ntest_vocab = build_vocab(test_sentences)\nprint({k: train_vocab[k] for k in list(train_vocab)[:5]})\nprint({k: test_vocab[k] for k in list(test_vocab)[:5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oov = check_coverage(train_vocab,embeddings_index)\ntest_oov = check_coverage(test_vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_vocab\ndel test_vocab\ndel train_sentences\ndel test_sentences\ndel train_oov\ndel test_oov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = np.where(train['target'] >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, validate_df = train_test_split(train, test_size=0.1, stratify=train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 120000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_obj = Tokenizer(num_words=max_features)\ntokenizer_obj.fit_on_texts(list(train['comment_text']) +list(test['comment_text']))\nprint(train_df.shape)\nprint(validate_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = max([len(s.split()) for s in list(train['comment_text'])])\nprint(max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 256\nX_train_pad = tokenizer_obj.texts_to_sequences(train_df['comment_text'])\ny_train = train_df['target'].values\nX_test_pad = tokenizer_obj.texts_to_sequences(validate_df['comment_text'])\ny_test = validate_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test['comment_text'].fillna('').values\ntest_sequences = tokenizer_obj.texts_to_sequences(x_test)\nTest_pad = pad_sequences(test_sequences, maxlen=max_length)\nprint(Test_pad.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pad = pad_sequences(X_train_pad, maxlen=max_length)\nX_test_pad = pad_sequences(X_test_pad, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_words = min(max_features, len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of X_train_pad tensor:', X_train_pad.shape)\nprint('shape of y_train tensor:', y_train.shape)\nprint('shape pf X_test_pad tensor:', X_test_pad.shape)\nprint('shape of y_test tensor:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel test\ndel train\ndel tokenizer_obj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU, CuDNNLSTM, SpatialDropout1D\nfrom keras.layers import Bidirectional, BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_embedding_matrix(word_index, embeddings_index, max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM=300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nembedding_layer = Embedding(nb_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix),input_length=max_length, trainable=False)\nmodel.add(embedding_layer)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(EMBEDDING_DIM, return_sequences=True)))\nmodel.add(Conv1D(64, 5, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(32, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j, (train_index, test_index) in enumerate(skfolds.split(X_train_pad, y_train)):\n    print('\\nFold ',j)\n    X_train_folds = X_train_pad[train_index]\n    y_train_folds = y_train[train_index]\n    X_test_fold = X_train_pad[test_index]\n    y_test_fold = y_train[test_index]\n    history = model.fit(X_train_folds, y_train_folds, batch_size=2048, epochs=2, validation_data=(X_test_fold, y_test_fold))\n    val_y = model.predict([X_test_fold], batch_size=1024, verbose=1)\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_test_fold, (val_y>thresh).astype(int))))\n    print('roc_auc_score',metrics.roc_auc_score(y_test_fold, val_y))\n    plt.plot(history.history['loss'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_y = model.predict([X_test_pad], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_test, (val_y>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.roc_auc_score(y_test, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(Test_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['prediction'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}