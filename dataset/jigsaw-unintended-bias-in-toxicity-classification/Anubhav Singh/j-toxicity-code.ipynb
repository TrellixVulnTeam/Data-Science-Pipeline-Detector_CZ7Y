{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Jigsaw Comment Toxicity Dataset\n#In every dataset, most important part of modelling is cleaning of dataset.\n#One simply has to follow some steps along with some intincts of at how much depth you want to clean the data.\n#Talking about this dataset, Cleaning process is divided into 3 functions.\n#1: Clean APPOSTOPHES\n#2: Remove stop words along with lemmatization\n#3: Removes punctuations along along with word 'haha' because it won't contribute in accuracy or loss.\n#4: Last function is just to call the last function which later will call other features.\n#In this cleaning process, i didn't remove rare/most common occuring words since comments tend to have only one word and \n#if that word is rare or most common and got removed then we will have null observation due to which we will have to deal with null \n#observation as well and also, some cursing/Toxic words are rare so model needs to know them.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Load Required Libraries\nimport re\nimport gensim\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom keras import callbacks\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.models import Model,Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom keras.optimizers import Adam,RMSprop\nfrom keras.layers import CuDNNLSTM,CuDNNGRU,Dropout,LeakyReLU,Input,Embedding,Dense,Bidirectional\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Data path\ntrain_path='../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\ntest_path='../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ntrain=pd.read_csv(train_path)\ntest=pd.read_csv(test_path)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nos.listdir('../input/glove840b300dtxt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load Word Embeddings\nembedding_path='../input/glove840b300dtxt/glove.840B.300d.txt'\nembedding_dict={}\nembd_file=open(embedding_path,'r',errors = 'ignore',encoding='utf8')\nfor line in tqdm(embd_file):\n    values=line.split(' ')\n    word=values[0]\n    coef=np.asarray(values[1:],dtype='float32')\n    embedding_dict[word]=coef\nembd_file.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Found %s word vectors.' % len(embedding_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets take a look at train data\ntrain.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns=train.columns\ntrain_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets Drop unnecessary features\nuns_features=['id','created_date','publication_id','parent_id','article_id']\ntrain=train.drop(uns_features,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since target is in Probability form so, we will transform it into class form\n#As described in data description, target>=0.5 is toxic/positive class.\ntrain['target'][train['target']>=0.5]=1\ntrain['target'][train['target']<0.5]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()\ntrain_labels=train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets look at our comment section\ntrain['comment_text'].head()\n#We are going to remove unnecessary stopwords,punctuations and numbers in text if present.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def text_cleaner_1(text): #Correct APPOSTOPHES\n#    words=re.split(r'\\W+',text)\n#    APPOSTOPHES= {\"s\": \"is\", \"re\":\"are\",\"ll\":\"will\"} \n#    tmp=[APPOSTOPHES[word] if word in APPOSTOPHES else word for word in words]\n#    return ' '.join(tmp)\n\ndef text_cleaner_2(text): #Remove StopWords and Lemmatize words\n    tmp=[]\n    #text=text_cleaner_1(text)\n    lm=WordNetLemmatizer()\n    ps=PorterStemmer()\n    sp=stopwords.words('english')\n    for word in text.split():\n        if word not in sp:\n            tmp.append(lm.lemmatize(word.lower()))\n        \n    return ' '.join(tmp)\n\ndef text_cleaner_3(text): #Remove punctuations\n    text=text_cleaner_2(text)\n    text=re.sub(r'[^\\w\\s]','',text)\n    text=re.sub(r'h+a+h+a','',text) #Remove haha/hahaha...\n    text=re.sub(r'f+f+u+u+','fuck',text) #lol :D\n    text =''.join(''.join(s)[:2] for _,s in itertools.groupby(text))\n    return text\n\ndef call_cleaners(data): #Call All cleaning functions\n    filtered_texts=[]\n    print('Cleaning Text Data..\\n')\n    for sent in tqdm(data):       \n        s=text_cleaner_3(sent)\n        filtered_texts.append(s)\n    return filtered_texts\n\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_text=call_cleaners(train['comment_text'])","execution_count":5,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/1804874 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Cleaning Text Data..\n\n","name":"stdout"},{"output_type":"stream","text":"  3%|â–Ž         | 52351/1804874 [00:26<11:40, 2500.88it/s] ","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0dced5b0faec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_cleaners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-e0f082097962>\u001b[0m in \u001b[0;36mcall_cleaners\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaning Text Data..\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_cleaner_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mfiltered_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e0f082097962>\u001b[0m in \u001b[0;36mtext_cleaner_3\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_cleaner_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Remove punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_cleaner_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'h+a+h+a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Remove haha/hahaha...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e0f082097962>\u001b[0m in \u001b[0;36mtext_cleaner_2\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mfilter_forms\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word Tokenization\nprint('Word Tokenization and Transforming them into Sequence..')\ntokenizer=Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(filtered_text)\nsequences=tokenizer.texts_to_sequences(filtered_text)\ntrain_data_prepd=pad_sequences(sequences,maxlen=15)\nword_index=tokenizer.word_index\nprint('Tokenization Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix=np.zeros((30000,300))\nprint('Loading Embedding Matrix..\\n')\nfor word,ix in tqdm(word_index.items()):\n    if ix<50000:\n        embed_vec=embedding_dict.get(word)\n        if embed_vec is not None:\n            embedding_matrix[ix]=embed_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see how many null embedding we have\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Training')\nmodel=Sequential()\nmodel.add(Embedding(30000,300,input_length=15))\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = True\nmodel.add(Bidirectional(CuDNNLSTM(128,kernel_initializer='he_uniform',return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(196,kernel_initializer='he_uniform')))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1024,activation=None))\nmodel.add(LeakyReLU())\nmodel.add(Dense(2048,activation=None))\nmodel.add(LeakyReLU())\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(0.001),metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call Backs\ncb=callbacks.EarlyStopping(monitor='val_loss',patience=4,restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=model.fit(train_data_prepd,train_labels,epochs=20,batch_size=50,validation_split=0.2,callbacks=[cb])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Model Accurcay and Loss\nimport matplotlib.pyplot as plt\nacc = result.history['acc']\nval_acc = result.history['val_acc']\nloss = result.history['loss']\nval_loss = result.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'b',color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b',color='blue', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'b', color='red', label='Training loss')\nplt.plot(epochs, val_loss, 'b',color='blue', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Predictions\ntest_data_filtered=call_cleaners(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Tokenzie test set\nprint('Tokenizing Test Set..')\ntokenizer.fit_on_texts(test_data_filtered)\nsequences_test=tokenizer.texts_to_sequences(test_data_filtered)\ntest_data_prepd=pad_sequences(sequences_test,maxlen=15)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\ntest_predictions=model.predict(test_data_prepd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission file\nsub_file=pd.DataFrame({'id':test['id'],'prediction':test_predictions.reshape(-1)})\nsub_file.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save model\nmodel.save('comment_classifier.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}