{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jigsaw Unintended Bias in Toxicity Classification Kaggle Challenge \nA Problem-Solving Approach by A-Team (Team 5 HWR)"},{"metadata":{},"cell_type":"markdown","source":"## Table of Content\n- [1.Introduction](#1.-Introduction)\n    - [1.1 Understanding the Challenge of the Competition](#1.1-Understanding-the-Challenge-of-the-Competition)\n    - [1.2 Understanding the Meaning of the Data](#1.2-Understanding-the-Meaning-of-the-Data)\n- [2. Data Loading](#2.-Data-Loading)\n    - [2.1 Package Import and Dataset Loading](#2.1-Package-Import-and-Dataset-Loading)\n    - [2.2 Memory Reduction](#2.2-Memory-Reduction)\n- [3. Exploratory Data Analysis of Datasets](#3.-Exploratory-Data-Analysis-of-Datasets)\n    - [3.1 Overview of Train and Test Data](#3.1-Overview-of-Train-and-Test-Data) \n    - [3.2 Examples of Comments](#3.2-Examples-of-Comments) \n    - [3.3 Deepdive into Datasets](#3.3-Deepdive-into-Datasets) \n- [4. Data Preprocessing](#4.-Data-Preprocessing)\n    - [4.1 Loading of Word Embeddings](#4.1-Loading-of-Word-Embeddings)\n    - [4.2 Text Preprocessing](#4.2-Text-Preprocessing)\n- [5. Modeling](#5.-Modeling)\n    - [5.1 Model Definition with Keras](#5.1-Model-Definition-with-Keras)\n    - [5.2 Training](#5.2-Training)\n    - [5.3 Testing](#5.3-Testing)\n    - [5.4 Prediction](#5.4-Prediction)\n- [6. Submission](#6.-Submission)\n- [7. Conclusion](#7.-Conclusion)\n- [References](#References)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"This notebook provides an overview of a possible problem-solving approach to solve the Jigsaw Unintended Bias in Toxicity Classification Challenge on Kaggle. The underlying chapters can be accessed via a click on the the [Table of Content](#Table-of-Content) above. The report commences with an introduction of the challenge of the competition and an overview of the provided datasets for a better understanding. Afterwards, the required libraries and packages for each chapter are loaded as well as function to reduce memory is applied before exploring the datasets in the Exploratory Data Analysis (EDA). After having a look at the datasets, the text will be pre-processed using word embeddings to vectorize comments into embeddings. In chapter 5, `Keras` is used as the underlying Python Deep Learning library to model after pre-processing the text. A `train-test-split` is then performed before prediction the toxicity score and creating a `submission.csv`. The overview of the problem-solving approach can be seen in the image below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overview of problem solving approach to solve the Jigsaw challenge\nfrom IPython.display import Image\nImage(\"../input/problemsolvingapproach/psa.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All kernels which have been used as inspiration or advice throughout the project will be referenced in the [References](#References) section as well as other sources to understand theory, applications and different libraries."},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Understanding the Challenge of the Competition"},{"metadata":{},"cell_type":"markdown","source":"The objective of the \"Jigsaw Unintended Bias in Toxicity Classification\" challenge is to detect potential toxic comments in online conversations and further minimize unintended model bias. The initiator of the challenge have defined \"toxicity\" as \"anything rude, disrespectful or otherwise likely to make someone leave a discussion\" ([Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), 2019). \n\nBased on a previous challenge, the current version aims to further build toxicity models which can be applied across a versatile range of online conversations. The challenge is particularly to define models which can differentiate the usage of  words, such as religion or sex, between a normal and a toxic usage. Hence, solely analyzing the frequency of words is not enough to detect the toxicity level which is why the second challenge has been extended with also minimizing the unintended bias of these comments. "},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Understanding the Meaning of the Data"},{"metadata":{},"cell_type":"markdown","source":"To begin with the challenge we are provided with a dataset which is already labeled for these identity mentions and for which we are asked to optimize a metric which can measure unintended bias. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Package Import to have a first look at the target dataset (sample_submission)\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load submission.csv into a Pandas\nsubmission = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\")\nprint(submission.head())\nprint(submission.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before importing and analyzing the train and test datasets we wanted to take a look at our final goal - the `submission.csv` - and understand what should be predicted. One can see that we need a total of 97.320 rows, each with different IDs for which we predict the toxicity. \n\nImportant to explain at this stage is how the result is being evaluated by the initiators of the challenge. The `sample_submission.csv` is evaluated based on the \"Jigsaw Bias AUC\" which is a newly developed metric by Jigsaw. \n\nBesides the `sample_submission.csv` the challenge provides:\n- `train.csv` - the training set, which includes subgroups\n- `test.csv` - the test set, which does not include subgroups"},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Loading"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Package Import and Dataset Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries and packages for the complete project\n\n# Import standard libraries for data loading, EDA and data preprocessing\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport time\nimport gc\nimport datetime\nimport scipy.stats as stats\nimport operator \nimport re\nfrom wordcloud import WordCloud, STOPWORDS\nsns.set_style('whitegrid')\n\n\n# Import NLP libraries for data preprocessing and modeling\nfrom gensim.models import KeyedVectors\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\nfrom keras.layers import Input\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Bidirectional\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.optimizers import RMSprop\nfrom keras.layers import CuDNNLSTM\nfrom keras.layers import Convolution1D\nfrom keras.models import Model\nfrom keras.models import load_model\nfrom keras.models import Sequential\n\n\n# Import libraries for train and test split and validation\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\n# Print folders in input folder - including additional datasets used in the project\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Load train and test datasets into a Pandas and print its shape\n# Adapt nrows depending on CPU/GPU available\n\n# Load dataset with 200.000 rows\n#train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\", nrows=200000)\n#test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\", nrows=200000)\n\n# Load dataset with 500.000 rows\n# train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\", nrows=500000)\n# test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\", nrows=500000)\n\n# Load dataset with 1.000.000 rows\ntrain = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\", nrows=700000)\ntest = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\", nrows=700000)\n\n# Load entire dataset\n# train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n# test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Memory Reduction"},{"metadata":{},"cell_type":"markdown","source":"Before exploring the different dataset, a function is been applied to reduce memory usage. This has been done in a similar way in the previous Elo Kaggle challenge last semester. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to reduce memory usage to have faster processing later on\ndef reduce_mem_usage(df, verbose=True):\n    \"\"\" This function iterates through columns of a dataframe in order to reduce the memory.        \n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the function to reduce memory usage to the train and test dataset\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reduction of memory function yields a reduction of memory usage of 69.4% for the `train` dataset and 25% for the `test` dataset."},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis of Datasets"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Overview of Train and Test Data"},{"metadata":{},"cell_type":"markdown","source":"Before deepdiving into the different datasets and analyzing its components, it is useful to first get a glance at both, `train` and `test` dataset with the common functions of `.head()`, `.info()` and `.describe()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look at the head of the train dataset\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look at different datatypes in the train dataset with .info()\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe the train dataset\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a first look at the `train` dataset, one can see that there are in total 45 columns. Data types are `float`, `int` and `object`, while the only column with an object type is the `comment_text` column, the key column for the text pre-processing and modeling later on. The other columns mainly illustrate subgroups, such as sex-related (e.g. `female`), religion (e.g. `buddhist` or `christian`) or race (`asian` or `latino`), among others. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look at the head of the test dataset\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look at different datatypes in the test dataset with .info()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe the column \"comment_text\" in the test dataset\ntest[\"comment_text\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `test` dataset includes two columns, `id` and `comment_text`.  "},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Examples of Comments"},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the introduction, the objective of the challenge is not only to identify toxic content, but more importantly to classify its toxicity. This leads to the main challenge of how to differentiate between using a toxic word in a positive context or a negative one. In the following, two examples will be given of comments that are \"obviously\" toxic and a comment that is used in a positive way."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display selected entries for \"comment_text\" to visually inspect their potential for toxicity\npd.options.display.max_colwidth=200\ntrain[\"comment_text\"][:12]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a lot at row 3, for instance, the comment `\"Is this something I'll be able to install on my site? When will you be releasing it?\"` is a very objective comment. It could e.g. be a question a user is posting in a forum - without any emotional impact involved. Therefore the toxicity level has been defined as 0. \nHowever, looking at row 11, `\"This is a great story. Man. I wonder if the person who yelled \"shut the fuck up!\" at him ever heard it.\"`, this comment most certainly is more difficult to classify. Even though there is an insult included in the comment, it is not directly attacking someone else. It is rather meant that someone else who said the insult should learn from a certain situation. "},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Deepdive into Datasets "},{"metadata":{},"cell_type":"markdown","source":"After having a first look on the `train` and `test` datasets and the challenge of classifying toxic comments, a more detailed exploration of the data will follow in the comming cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count missing values\nmval_train = train.isnull().sum(axis=0) / len(train)\n\n# Filter out columns with zero missing values\nmval_train = mval_train[mval_train > 0]\nmval_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The analysis of missing values reveals that there are almost 80% of missing values in several columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check distribution of # of words in the comment_text column\nnr_words = train[\"comment_text\"].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain['nr_words'] = nr_words\nnr_words = train.loc[train['nr_words']<200]['nr_words']\nsns.distplot(nr_words, color='b')\nplt.xlabel('Number of words per comment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of number of words shows a skewed distribution meaning that most of the comments consist of 0-50 words, much less comments contain more than 50 and up to 200 words."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('white')\n\n# Count the values in the target variable to below plot the distribution of toxic comments\nhist_df = pd.cut(train['target'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['percentage'] = (hist_df['target']/hist_df['target'].sum())*100\nhist_df\n\nplt.figure(figsize=(12,4))\n\n# Plot the toxicity distribution of toxic comments (toxicity score)\nplt.subplot(1,2,1)\nsns.barplot(x=hist_df['bins'],y=hist_df['target'])\nplt.xticks(rotation='vertical')\nplt.title('Toxicity Distribution')\nplt.xlabel('Toxicity Score')\nplt.ylabel('Comment Count')\n\n# Plot the toxicity percentage of these comments\nplt.subplot(1,2,2)\nsns.barplot(x=hist_df['bins'],y=hist_df['percentage'])\nplt.xticks(rotation='vertical')\nplt.title('Toxicity Percentage')\nplt.xlabel('Toxicity Score')\nplt.ylabel('Percentage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply a 0.5 threshold to have a deeper look at actual toxic comments\nlen(train[train.target>0.5])/len(train.target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above shows that about 70% of the comments in the `train` dataset are not toxic at all. Considering 0.5 as a threshold for high toxicity, only 5% of the total number of comments are actually toxic. This is an important result which will have an impact on the text-preprocessing and modeling and hence will be considered throughout the kernel to not falsely interpret the result in the end. There are different methods that deal with this issue. For this kernel, the dataset will be downsampled for the modeling and also a train-test-split be conducted before predicting the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('white')\n\n# Filter the train dataset for the column \"created_date\" to investigate the count of toxic comments on a timeline\ntrain['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\ncomment_count = train.groupby(['created_date'])['target'].count().sort_index().reset_index()\ncomment_mean = train.groupby(['created_date'])['target'].mean().sort_index().reset_index()\n\n# Create a first subplot \nfig, ax1 = plt.subplots(figsize=(12,4))\nx_range = comment_count['created_date']\n\n# Create and plot the x-axis with Date and plot the number of comments\ncolor = 'tab:red'\nax1.set_xlabel('Date')\nax1.set_ylabel('# of Comments')\nax1.plot(x_range, comment_count['target'], color=color, label=\"# of comments\")\nplt.xticks(rotation='vertical')\nax1.tick_params(axis='y')\nax1.legend(loc=\"lower right\")\nplt.title(\"Count and Toxicity of Comments\")\n\n# Instantiate a second axes that shares the same x-axis and plot the average toxicity\nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('Avg Toxicity') \nax2.plot(x_range, comment_mean['target'], color=color, label=\"avg toxicity\")\nplt.xticks(rotation='vertical')\nax2.legend(loc=\"upper left\")\n\n# Plot the result\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average toxicity of the comments began to increase steadily since February 2016 but the amount of comments has remained stable during the same period of time. After October 2016, there has been a stronger increase in the number of comments with a decrease in October 2017. Between April 2016 and October 2017, the average toxicity has slightly increased and decreased above and below the value of 10%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking first and last comment on the timeline to validate the previous graph\nprint('Date of first comment in train dataset:', train['created_date'].min())\nprint('Date of last comment in train dataset:', train['created_date'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown in the graph before, the first comment of the dataset was made on 01.09.2015 and the last on 01.11.2017. Therefore the drop on the right side of the graph is not a surprise and has not to be considered as relevant for the further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the columns of severy_toxicity, obscene, threat, insult, identity_attack and sexual_explicit and their count of values\ntox_sub = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\nchart = 0\nfig = plt.figure(figsize=(12,8))\n\nfor col in tox_sub:\n    df_subtox = train.loc[train[col] > 0]\n    hist_subtox = pd.cut(df_subtox[col], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n    chart += 1\n    plt.subplot(3,2,chart)\n    plt.plot(hist_subtox[col])\n    plt.title(str(col))\n    plt.xlabel('Bins')\n    plt.ylabel('Count')\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most common types of subtoxicity are `insults` with about 200.000 and `identity_attack` comments with about 80.0000 among all the datapoints in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the weighted toxicity for different demographics\ndemographics = train.loc[:, ['target']+list(train)[slice(8,32)]].dropna()\nweighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis=\"index\").sum()/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\nplt.figure(figsize=(15,7.5))\nsns.set(font_scale=1)\nax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxicity')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the toxic comments are labeled based on skin color (`black` or `white`), sexual orientation (`homosexual_gay_or_lesbian`) and religion (`muslim`, `jewish`, `atheist`), while the dominant one among these is `homosexual_gay_or_lesbian`."},{"metadata":{},"cell_type":"markdown","source":"Inspired by a Datacamp article of Vu (2018) a deeper look at different wordclouds is taken in the next part in order to visually explore predominat words for different columns. The results are illustrated in the following cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the text to be illustrated in the WordCloud\ntext = train[\"comment_text\"]\n\n# Create and show a wordcloud image with max_font_size=50 and max_words=100\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords=STOPWORDS).generate(str(text))\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first wordcloud shows the predominant comments in `train` dataset. Since there are 1.804.874 rows in the dataset, the result includes common words such as \"great\", \"right\", \"think\" or \"even\". To visualize a wordcloud for different filters, a function is defined in the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rewrite the previous wordcloud image into a function to apply for different texts\ndef show_wordcloud(data, title=None):\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords=STOPWORDS).generate(str(data))\n    plt.figure(figsize=(10,10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and show a wordcloud for the first 50.000 comments\nshow_wordcloud(train[\"comment_text\"][:50000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The wordcloud image above as selected for the first 50.000 comments shows a predominance of \"Trump\", \"people\" and \"Civil\". Considering that the comments in the `train` dataset started in September 2015 which has been illustrated earlier, it seems to be logical that there have been a lot of discussions about Trump, the candidate for presidential elections in 2016."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and show a wordcloud for the first 50.000 comments\nshow_wordcloud(train[\"comment_text\"][50000:100000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filtering the comments between row 50.000 to 100.000 shows a different wordcloud than before. Predominant words are \"people\", \"state\", \"Alaska\" etc. However, clustering just by taking different rows in a chronological order does not yield to meaningful conclusions, as there is no interconnection across the whole dataset. Therefore in a next step, wordclouds out of the comments will be generated for different toxicity levels taking the `target` into account, too."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and show a wordcloud for a threshold of target <0.25 \nshow_wordcloud(train.loc[train[\"target\"] < 0.25][\"comment_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and show a wordcloud for a threshold of target <0.50 \nshow_wordcloud(train.loc[train[\"target\"] > 0.50][\"comment_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and show a wordcloud for a threshold of target <0.75 \nshow_wordcloud(train.loc[train[\"target\"] > 0.75][\"comment_text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering thresholds for the target variable, meaning considering different levels of toxicity to analyze common words in the comments, reveals the word \"comment\" as predominant when the toxicity level is very low (`<0.25`). However, when looking at a threshold `> 0.5` or even `>0.75`, as expected the most common words in the cloud are more and more related towards negative and insulting words, such as \"idiot\", \"stupid\" or \"fool\". Also remarkable for the last wordcloud is the appearance of \"Trump\" in the context of high toxicity, meaning that his name is mostly related to insults in online conversations which - considering his media presence and critics before and after the elections - is an outcome to be expected. "},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"The EDA (Exploratory Data Analysis) provided an overview of the available datasets and helped to gain an insight on important words, phrases and their evaluation. In the next step, the data preprocessing is aimed to utilize the insights gained from the EDA and hence, to prepare a suitable set of data that will be used for the model.  \nPart of the preprocessing is to remove punctuation. This transforms the comments (column: `comment_text`) into sequences of words, which then will be split into lists of tokens. Moreover, a removal of special characters, such as `!\"#$%&()*+,-./:;<=>?` will take place. Thereafter, CBOW or Split Graf will be implemented to characterize words, e.g. classify the words, term frequency. The great challenge in the preprocessing may be seen at this point of the preprocessing. It is to provide a model that is able to get an unbiased context, and thus, to be able to classify more difficult sentences as well. An approach to solve this issue is to use Keras text preprocessing. Keras turns the comments into an integer (representing a reference to a token in a dictionary). Alternatively, Keras vectorizes the comment based on embeddings. This preprocessing technique would combine embeddings and Keras and may improve the `train` data set."},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Loading of Word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load word embeddings from \"fasttext-crawl-300d-2m\" dataset which has been imported to the input workspace before\nfasttext = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(fasttext)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this competition, we focus on using an open-source, free, lightweight fastText library that allows users to learn text representations and text classifiers. In order to easily preprocess the text, this kernel uses the `fasttext-crawl-300d-2m` dataset with word embeddings. This Natural Language Processing (NLP) library provides 300-dimensional pretrained FastText English word vectors which have been released by Facebook. The dataset has been added to the workspace on Kaggle into the `input` folder and can simply be accessed by the loading statement seen above. \n\nAs a structure for mapping between entities and vectors KeyedVectors is used. In this case, the entity corresponds to a word.\n\nTo track the loading time, `%%time` has been applied to the loading function too. Due to the size of the dataset, this step is already quite time-intense. Since the kernel is stored on Kaggle the dataset has to be loaded each time the kernel is committed. "},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"As an important step to preprocess the text, the `train` and `test` dataframes have to be joined and `train` and `test` to be deleted for memory reasons. This can be seen below."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Concatenate train_id and comment_text for further data processing\ndf = pd.concat([train[['id','comment_text']], test], axis=0)\ndel(train, test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To proceed with text preprocessing, first different functions to build the vocabulary and check the amount of words in the embedding are defined and later on applied. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to build the vocabulary\ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to check the amount of words that can be found in the embedding\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Get text to lower caps\ndf['comment_text'] = df[\"comment_text\"].apply(lambda x: x.lower())\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous code has transformed all capital letters to lower cases in order to proceed with the application of the functions defined to build the vocabulary (`build_vocab`) and the function to check the coverage of words that can be found in the embedding (`check_coverage`). "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Build the vocabulary and check the coverage in the FastText embedding\nvocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to not overload the memory of this kernel, `gc.collect()` as seen above is applied after each preprocessing step. This function is a possibility to \"collect the garbage\" and provide free memory for the next calculation steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first 20 out of vocabulary values to check where to procede\noov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that there are a lot of contractions in the `top 20` `oov` results. A mapping can help here so they can be replaced for the long form if they exist in the `FastText` embedding. The contraction mapping is defined in the next cell before deleting and recalculating the `vocab` and `oov` again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a contraction mapping as a dictionary\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete the vocab and oov objecte to recalculate again\ndel(vocab,oov)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After deleting the `vocab` and `oov` objects for recalculation, in the next step a function will be defined to calculate the amount of known contractions in the `FastText` embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to calculate the amount of known contractions in the FastText embedding\ndef known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known\n\nprint(\"The known contractions in the FastText embedding are: \")\nprint(known_contractions(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result shows the known contractions in the `FastText` embedding. In the following step, another function will be defined which can map and replace the known contractions in `comment_text` so they can be properly embedded in the `FastText` embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to map and replace known contractions in comment_text\ndef clean_contractions(text, mapping):\n    spec_characters = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in spec_characters:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a next step, the clean contractions function is applied on `comment_text` and then the `vocab` is rebuilt again to calculate the`out of vocab` percentage plus the `top 20` words in the `oov`."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Apply the clean contractions function on the comment_text fields\ndf['comment_text'] = df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Rebuild the vocab and oov\nvocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first 20 out of vocabulary values again\noov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step in the text pre-processing is the punctuation and special characters handling, but first the `vocab` and `oov` objects has to be deleted for memory reasons as it has been done in the previous text preprocessing steps.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete the vocab and oov objects for memory reasons\ndel(vocab,oov)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, a function is defined that checks which special characters are included in the `FastText` embedding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a variable with special characters\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\n# Define a function to check for special characters\ndef punctuation(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown\n\n# Print the result of applying the punctuation function\nprint(punctuation(embeddings_index, punct))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, a dictionary with the unknown special characters to be replaced as white spaces in the text is created as well as a function to be map and replace every special character and punctuation in the `comment_text` field."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate a dictionary with unknown special characters\nunknown_spec = {\"_\":\" \", \"`\":\" \"}\n\n# Define a function to map and replace every special character\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])    \n    for p in punct:\n        text = text.replace(p, f' {p} ')     \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previously defined `clean_special_chars` function is applied in the coming cell in order to map and replace the characters in the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Apply the clean_special_chars function to comment_text\ndf['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, unknown_spec))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying the `clean_special_chars` function, the `vocab` and the `oov` has to be recalcutad again in the next cells and the `top 20` `out of vocab` words will be printed to see where to proceed further."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Rebuild vocab and oov\nvocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first 20 out of vocabulary values again\noov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no clear ways to proceed besides misspelling of some words, therefore as a next step swearwords and insults have to be mapped. For this purpose, first an array will be created with random insults and its variations in order to figure out if they are included in the `FastText` embedding and which ones are not. These ones have to be replaced with a common insult that is easily found in the embedding.\n\nThe following insult mapping was borrowed from Tom Aindow (@Taindow https://www.kaggle.com/taindow/simple-cudnngru-python-keras) so credits and big thanks go to him:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define insults\ninsults = [' 4r5e ',' 5h1t ',' 5hit ',' a55 ',' anal ',' anus ',' ar5e ',' arrse ',' arse ',' ass ',' ass-fucker ',' asses ',' assfucker ',' assfukka ',' asshole ',' assholes ',' asswhole ',' a_s_s ',' b!tch ',' b00bs ',' b17ch ',' b1tch ',' ballbag ',' balls ',' ballsack ',' bastard ',' beastial ',' beastiality ',' bellend ',' bestial ',' bestiality ',' biatch ',' bitch ',' bitcher ',' bitchers ',' bitches ',' bitchin ',' bitching ',' bloody ',' blow job ',' blowjob ',' blowjobs ',' boiolas ',' bollock ',' bollok ',' boner ',' boob ',' boobs ',' booobs ',' boooobs ',' booooobs ',' booooooobs ',' breasts ',' buceta ',' bugger ',' bum ',' bunny fucker ',' butt ',' butthole ',' buttmuch ',' buttplug ',' c0ck ',' c0cksucker ',' carpet muncher ',' cawk ',' chink ',' cipa ',' cl1t ',' clit ',' clitoris ',' clits ',' cnut ',' cock ',' cock-sucker ',' cockface ',' cockhead ',' cockmunch ',' cockmuncher ',' cocks ',' cocksuck ',' cocksucked ',' cocksucker ',' cocksucking ',' cocksucks ',' cocksuka ',' cocksukka ',' cok ',' cokmuncher ',' coksucka ',' coon ',' cox ',' crap ',' cum ',' cummer ',' cumming ',' cums ',' cumshot ',' cunilingus ',' cunillingus ',' cunnilingus ',' cunt ',' cuntlick ',' cuntlicker ',' cuntlicking ',' cunts ',' cyalis ',' cyberfuc ',' cyberfuck ',' cyberfucked ',' cyberfucker ',' cyberfuckers ',' cyberfucking ',' d1ck ',' damn ',' dick ',' dickhead ',' dildo ',' dildos ',' dink ',' dinks ',' dirsa ',' dlck ',' dog-fucker ',' doggin ',' dogging ',' donkeyribber ',' doosh ',' duche ',' dyke ',' ejaculate ',' ejaculated ',' ejaculates ',' ejaculating ',' ejaculatings ',' ejaculation ',' ejakulate ',' f u c k ',' f u c k e r ',' f4nny ',' fag ',' fagging ',' faggitt ',' faggot ',' faggs ',' fagot ',' fagots ',' fags ',' fanny ',' fannyflaps ',' fannyfucker ',' fanyy ',' fatass ',' fcuk ',' fcuker ',' fcuking ',' feck ',' fecker ',' felching ',' fellate ',' fellatio ',' fingerfuck ',' fingerfucked ',' fingerfucker ',' fingerfuckers ',' fingerfucking ',' fingerfucks ',' fistfuck ',' fistfucked ',' fistfucker ',' fistfuckers ',' fistfucking ',' fistfuckings ',' fistfucks ',' flange ',' fook ',' fooker ',' fuck ',' fucka ',' fucked ',' fucker ',' fuckers ',' fuckhead ',' fuckheads ',' fuckin ',' fucking ',' fuckings ',' fuckingshitmotherfucker ',' fuckme ',' fucks ',' fuckwhit ',' fuckwit ',' fudge packer ',' fudgepacker ',' fuk ',' fuker ',' fukker ',' fukkin ',' fuks ',' fukwhit ',' fukwit ',' fux ',' fux0r ',' f_u_c_k ',' gangbang ',' gangbanged ',' gangbangs ',' gaylord ',' gaysex ',' goatse ',' God ',' god-dam ',' god-damned ',' goddamn ',' goddamned ',' hardcoresex ',' hell ',' heshe ',' hoar ',' hoare ',' hoer ',' homo ',' hore ',' horniest ',' horny ',' hotsex ',' jack-off ',' jackoff ',' jap ',' jerk-off ',' jism ',' jiz ',' jizm ',' jizz ',' kawk ',' knob ',' knobead ',' knobed ',' knobend ',' knobhead ',' knobjocky ',' knobjokey ',' kock ',' kondum ',' kondums ',' kum ',' kummer ',' kumming ',' kums ',' kunilingus ',' l3itch ',' labia ',' lmfao ',' lust ',' lusting ',' m0f0 ',' m0fo ',' m45terbate ',' ma5terb8 ',' ma5terbate ',' masochist ',' master-bate ',' masterb8 ',' masterbat3 ',' masterbate ',' masterbation ',' masterbations ',' masturbate ',' mo-fo ',' mof0 ',' mofo ',' mothafuck ',' mothafucka ',' mothafuckas ',' mothafuckaz ',' mothafucked ',' mothafucker ',' mothafuckers ',' mothafuckin ',' mothafucking ',' mothafuckings ',' mothafucks ',' mother fucker ',' motherfuck ',' motherfucked ',' motherfucker ',' motherfuckers ',' motherfuckin ',' motherfucking ',' motherfuckings ',' motherfuckka ',' motherfucks ',' muff ',' mutha ',' muthafecker ',' muthafuckker ',' muther ',' mutherfucker ',' n1gga ',' n1gger ',' nazi ',' nigg3r ',' nigg4h ',' nigga ',' niggah ',' niggas ',' niggaz ',' nigger ',' niggers ',' nob ',' nob jokey ',' nobhead ',' nobjocky ',' nobjokey ',' numbnuts ',' nutsack ',' orgasim ',' orgasims ',' orgasm ',' orgasms ',' p0rn ',' pawn ',' pecker ',' penis ',' penisfucker ',' phonesex ',' phuck ',' phuk ',' phuked ',' phuking ',' phukked ',' phukking ',' phuks ',' phuq ',' pigfucker ',' pimpis ',' piss ',' pissed ',' pisser ',' pissers ',' pisses ',' pissflaps ',' pissin ',' pissing ',' pissoff ',' poop ',' porn ',' porno ',' pornography ',' pornos ',' prick ',' pricks ',' pron ',' pube ',' pusse ',' pussi ',' pussies ',' pussy ',' pussys ',' rectum ',' retard ',' rimjaw ',' rimming ',' s hit ',' s.o.b. ',' sadist ',' schlong ',' screwing ',' scroat ',' scrote ',' scrotum ',' semen ',' sex ',' sh!t ',' sh1t ',' shag ',' shagger ',' shaggin ',' shagging ',' shemale ',' shit ',' shitdick ',' shite ',' shited ',' shitey ',' shitfuck ',' shitfull ',' shithead ',' shiting ',' shitings ',' shits ',' shitted ',' shitter ',' shitters ',' shitting ',' shittings ',' shitty ',' skank ',' slut ',' sluts ',' smegma ',' smut ',' snatch ',' son-of-a-bitch ',' spac ',' spunk ',' s_h_i_t ',' t1tt1e5 ',' t1tties ',' teets ',' teez ',' testical ',' testicle ',' tit ',' titfuck ',' tits ',' titt ',' tittie5 ',' tittiefucker ',' titties ',' tittyfuck ',' tittywank ',' titwank ',' tosser ',' turd ',' tw4t ',' twat ',' twathead ',' twatty ',' twunt ',' twunter ',' v14gra ',' v1gra ',' vagina ',' viagra ',' vulva ',' w00se ',' wang ',' wank ',' wanker ',' wanky ',' whoar ',' whore ',' willies ',' willy ',' xrated ',' xxx '   ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, a function is defined in order to check how many of the insults are included in the embedding file."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# Initiate insults_notfound\ninsults_notfound = []\n\n# Iterate over insults to check for insults included in the embedding file\nfor insult in insults:\n    if insult[1:(len(insult)-1)] not in embeddings_index:\n        insults_notfound.append(insult)\n        \ninsults_notfound = '|'.join(insults_notfound)\ninsults_notfound","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next cell provides a function to replace the unknown insults in the embedding for an insult that can actually exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# Define a function to replace unknown insults in embedding\ndef handle_insults(text):\n    text = re.sub(insults_notfound, 'fuck', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Apply the handle_insults function previously defined\ndf['comment_text'] = df['comment_text'].apply(lambda x: handle_insults(x))\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete the vocab and oov objects for memory reasons\ndel(vocab,oov)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Rebuild the vocab and oov\nvocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying the `handle_insults` function and deleting and recalculating again the `vocab`, the coming part deals with a check for spacing, numbering, misspelling, rare words and characters.\n\nThe following contraction mapping which is a really wonderful and thorough piece of work is borrowed from Aditya Soni (@Adityaecdrid https://www.kaggle.com/adityaecdrid/public-version-text-cleaning-vocab-65/data) so all credits go to him:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Define a dictionary mispell_dict and rare_words_mapping\nmispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','ᴀ':'a', '😉':'wink','😂':'joy','😀':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists'}\nrare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n                      }\n\n# Define a variable spaces\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n\n# Define a variable extra_punct with extra punctuations not included in the previous analysis\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Define a dictionary with bad case words\nbad_case_words = {'nationalpost':'national post','businessinsider':'business insider','jewprofits': 'jew profits', 'QMAS': 'Quality Migrant Admission Scheme', 'casterating': 'castrating',\n                  'Kashmiristan': 'Kashmir', 'CareOnGo': 'India first and largest Online distributor of medicines',\n                  'Setya Novanto': 'a former Indonesian politician', 'TestoUltra': 'male sexual enhancement supplement',\n                  'rammayana': 'ramayana', 'Badaganadu': 'Brahmin community that mainly reside in Karnataka',\n                  'bitcjes': 'bitches', 'mastubrate': 'masturbate', 'Français': 'France',\n                  'Adsresses': 'address', 'flemmings': 'flemming', 'intermate': 'inter mating', 'feminisam': 'feminism',\n                  'cuckholdry': 'cuckold', 'Niggor': 'black hip-hop and electronic artist', 'narcsissist': 'narcissist',\n                  'Genderfluid': 'Gender fluid', ' Im ': ' I am ', ' dont ': ' do not ', 'Qoura': 'Quora',\n                  'ethethnicitesnicites': 'ethnicity', 'Namit Bathla': 'Content Writer', 'What sApp': 'WhatsApp',\n                  'Führer': 'Fuhrer', 'covfefe': 'coverage', 'accedentitly': 'accidentally', 'Cuckerberg': 'Zuckerberg',\n                  'transtrenders': 'incredibly disrespectful to real transgender people',\n                  'frozen tamod': 'Pornographic website', 'hindians': 'North Indian', 'hindian': 'North Indian',\n                  'celibatess': 'celibates', 'Trimp': 'Trump', 'wanket': 'wanker', 'wouldd': 'would',\n                  'arragent': 'arrogant', 'Ra - apist': 'rapist', 'idoot': 'idiot', 'gangstalkers': 'gangs talkers',\n                  'toastsexual': 'toast sexual', 'inapropriately': 'inappropriately', 'dumbassess': 'dumbass',\n                  'germanized': 'become german', 'helisexual': 'sexual', 'regilious': 'religious',\n                  'timetraveller': 'time traveller', 'darkwebcrawler': 'dark webcrawler', 'routez': 'route',\n                  'trumpians': 'Trump supporters','Trumpster':'trumpeters', 'irreputable': 'reputation', 'serieusly': 'seriously',\n                  'anti cipation': 'anticipation', 'microaggression': 'micro aggression', 'Afircans': 'Africans',\n                  'microapologize': 'micro apologize', 'Vishnus': 'Vishnu', 'excritment': 'excitement',\n                  'disagreemen': 'disagreement', 'gujratis': 'gujarati', 'gujaratis': 'gujarati',\n                  'ugggggggllly': 'ugly',\n                  'Germanity': 'German', 'SoyBoys': 'cuck men lacking masculine characteristics',\n                  'н': 'h', 'м': 'm', 'ѕ': 's', 'т': 't', 'в': 'b', 'υ': 'u', 'ι': 'i',\n                  'genetilia': 'genitalia', 'r - apist': 'rapist', 'Borokabama': 'Barack Obama',\n                  'arectifier': 'rectifier', 'pettypotus': 'petty potus', 'magibabble': 'magi babble',\n                  'nothinking': 'thinking', 'centimiters': 'centimeters', 'saffronized': 'India, politics, derogatory',\n                  'saffronize': 'India, politics, derogatory', ' incect ': ' insect ', 'weenus': 'elbow skin',\n                  'Pakistainies': 'Pakistanis', 'goodspeaks': 'good speaks', 'inpregnated': 'in pregnant',\n                  'rapefilms': 'rape films', 'rapiest': 'rapist', 'hatrednesss': 'hatred',\n                  'heightism': 'height discrimination', 'getmy': 'get my', 'onsocial': 'on social',\n                  'worstplatform': 'worst platform', 'platfrom': 'platform', 'instagate': 'instigate',\n                  'Loy Machedeo': 'person', ' dsire ': ' desire ', 'iservant': 'servant', 'intelliegent': 'intelligent',\n                  'WW 1': ' WW1 ', 'WW 2': ' WW2 ', 'ww 1': ' WW1 ', 'ww 2': ' WW2 ',\n                  'keralapeoples': 'kerala peoples', 'trumpervotes': 'trumper votes', 'fucktrumpet': 'fuck trumpet',\n                  'likebJaish': 'like bJaish', 'likemy': 'like my', 'Howlikely': 'How likely',\n                  'disagreementts': 'disagreements', 'disagreementt': 'disagreement',\n                  'meninist': \"male chauvinism\", 'feminists': 'feminism supporters', 'Ghumendra': 'Bhupendra',\n                  'emellishments': 'embellishments',\n                  'settelemen': 'settlement',\n                  'Richmencupid': 'rich men dating website', 'richmencupid': 'rich men dating website',\n                  'Gaudry - Schost': '', 'ladymen': 'ladyboy', 'hasserment': 'Harassment',\n                  'instrumentalizing': 'instrument', 'darskin': 'dark skin', 'balckwemen': 'balck women',\n                  'recommendor': 'recommender', 'wowmen': 'women', 'expertthink': 'expert think',\n                  'whitesplaining': 'white splaining', 'Inquoraing': 'inquiring', 'whilemany': 'while many',\n                  'manyother': 'many other', 'involvedinthe': 'involved in the', 'slavetrade': 'slave trade',\n                  'aswell': 'as well', 'fewshowanyRemorse': 'few show any Remorse', 'trageting': 'targeting',\n                  'getile': 'gentile', 'Gujjus': 'derogatory Gujarati', 'judisciously': 'judiciously',\n                  'Hue Mungus': 'feminist bait', 'Hugh Mungus': 'feminist bait', 'Hindustanis': '',\n                  'Virushka': 'Great Relationships Couple', 'exclusinary': 'exclusionary', 'himdus': 'hindus',\n                  'Milo Yianopolous': 'a British polemicist', 'hidusim': 'hinduism',\n                  'holocaustable': 'holocaust', 'evangilitacal': 'evangelical', 'Busscas': 'Buscas',\n                  'holocaustal': 'holocaust', 'incestious': 'incestuous', 'Tennesseus': 'Tennessee',\n                  'GusDur': 'Gus Dur',\n                  'RPatah - Tan Eng Hwan': 'Silsilah', 'Reinfectus': 'reinfect', 'pharisaistic': 'pharisaism',\n                  'nuslims': 'Muslims', 'taskus': '', 'musims': 'Muslims',\n                  'Musevi': 'the independence of Mexico', ' racious ': 'discrimination expression of racism',\n                  'Muslimophobia': 'Muslim phobia', 'justyfied': 'justified', 'holocause': 'holocaust',\n                  'musilim': 'Muslim', 'misandrous': 'misandry', 'glrous': 'glorious', 'desemated': 'decimated',\n                  'votebanks': 'vote banks', 'Parkistan': 'Pakistan', 'Eurooe': 'Europe', 'animlaistic': 'animalistic',\n                  'Asiasoid': 'Asian', 'Congoid': 'Congolese', 'inheritantly': 'inherently',\n                  'Asianisation': 'Becoming Asia',\n                  'Russosphere': 'russia sphere of influence', 'exMuslims': 'Ex-Muslims',\n                  'discriminatein': 'discrimination', ' hinus ': ' hindus ', 'Nibirus': 'Nibiru',\n                  'habius - corpus': 'habeas corpus', 'prentious': 'pretentious', 'Sussia': 'ancient Jewish village',\n                  'moustachess': 'moustaches', 'Russions': 'Russians', 'Yuguslavia': 'Yugoslavia',\n                  'atrocitties': 'atrocities', 'Muslimophobe': 'Muslim phobic', 'fallicious': 'fallacious',\n                  'recussed': 'recursed', '@ usafmonitor': '', 'lustfly': 'lustful', 'canMuslims': 'can Muslims',\n                  'journalust': 'journalist', 'digustingly': 'disgustingly', 'harasing': 'harassing',\n                  'greatuncle': 'great uncle', 'Drumpf': 'Trump', 'rejectes': 'rejected', 'polyagamous': 'polygamous',\n                  'Mushlims': 'Muslims', 'accusition': 'accusation', 'geniusses': 'geniuses',\n                  'moustachesomething': 'moustache something', 'heineous': 'heinous',\n                  'Sapiosexuals': 'sapiosexual', 'sapiosexuals': 'sapiosexual', 'Sapiosexual': 'sapiosexual',\n                  'sapiosexual': 'Sexually attracted to intelligence', 'pansexuals': 'pansexual',\n                  'autosexual': 'auto sexual', 'sexualSlutty': 'sexual Slutty', 'hetorosexuality': 'hetoro sexuality',\n                  'chinesese': 'chinese', 'pizza gate': 'debunked conspiracy theory',\n                  'countryless': 'Having no country',\n                  'muslimare': 'Muslim are', 'iPhoneX': 'iPhone', 'lionese': 'lioness', 'marionettist': 'Marionettes',\n                  'demonetize': 'demonetized', 'eneyone': 'anyone', 'Karonese': 'Karo people Indonesia',\n                  'minderheid': 'minder worse', 'mainstreamly': 'mainstream', 'contraproductive': 'contra productive',\n                  'diffenky': 'differently', 'abandined': 'abandoned', 'p0 rnstars': 'pornstars',\n                  'overproud': 'over proud',\n                  'cheekboned': 'cheek boned', 'heriones': 'heroines', 'eventhogh': 'even though',\n                  'americanmedicalassoc': 'american medical assoc', 'feelwhen': 'feel when', 'Hhhow': 'how',\n                  'reallySemites': 'really Semites', 'gamergaye': 'gamersgate', 'manspreading': 'man spreading',\n                  'thammana': 'Tamannaah Bhatia', 'dogmans': 'dogmas', 'managementskills': 'management skills',\n                  'mangoliod': 'mongoloid', 'geerymandered': 'gerrymandered', 'mandateing': 'man dateing',\n                  'Romanium': 'Romanum',\n                  'mailwoman': 'mail woman', 'humancoalition': 'human coalition',\n                  'manipullate': 'manipulate', 'everyo0 ne': 'everyone', 'takeove': 'takeover',\n                  'Nonchristians': 'Non Christians', 'goverenments': 'governments', 'govrment': 'government',\n                  'polygomists': 'polygamists', 'Demogorgan': 'Demogorgon', 'maralago': 'Mar-a-Lago',\n                  'antibigots': 'anti bigots', 'gouing': 'going', 'muzaffarbad': 'muzaffarabad',\n                  'suchvstupid': 'such stupid', 'apartheidisrael': 'apartheid israel', \n                  'personaltiles': 'personal titles', 'lawyergirlfriend': 'lawyer girl friend',\n                  'northestern': 'northwestern', 'yeardold': 'years old', 'masskiller': 'mass killer',\n                  'southeners': 'southerners', 'Unitedstatesian': 'United states',\n\n                  'peoplekind': 'people kind', 'peoplelike': 'people like', 'countrypeople': 'country people',\n                  'shitpeople': 'shit people', 'trumpology': 'trump ology', 'trumpites': 'Trump supporters',\n                  'trumplies': 'trump lies', 'donaldtrumping': 'donald trumping', 'trumpdating': 'trump dating',\n                  'trumpsters': 'trumpeters','Trumpers':'president trump', 'ciswomen': 'cis women', 'womenizer': 'womanizer',\n                  'pregnantwomen': 'pregnant women', 'autoliker': 'auto liker', 'smelllike': 'smell like',\n                  'autolikers': 'auto likers', 'religiouslike': 'religious like', 'likemail': 'like mail',\n                  'fislike': 'dislike', 'sneakerlike': 'sneaker like', 'like⬇': 'like',\n                  'likelovequotes': 'like lovequotes', 'likelogo': 'like logo', 'sexlike': 'sex like',\n                  'Whatwould': 'What would', 'Howwould': 'How would', 'manwould': 'man would',\n                  'exservicemen': 'ex servicemen', 'femenism': 'feminism', 'devopment': 'development',\n                  'doccuments': 'documents', 'supplementplatform': 'supplement platform', 'mendatory': 'mandatory',\n                  'moviments': 'movements', 'Kremenchuh': 'Kremenchug', 'docuements': 'documents',\n                  'determenism': 'determinism', 'envisionment': 'envision ment',\n                  'tricompartmental': 'tri compartmental', 'AddMovement': 'Add Movement',\n                  'mentionong': 'mentioning', 'Whichtreatment': 'Which treatment', 'repyament': 'repayment',\n                  'insemenated': 'inseminated', 'inverstment': 'investment',\n                  'managemental': 'manage mental', 'Inviromental': 'Environmental', 'menstrution': 'menstruation',\n                  'indtrument': 'instrument', 'mentenance': 'maintenance', 'fermentqtion': 'fermentation',\n                  'achivenment': 'achievement', 'mismanagements': 'mis managements', 'requriment': 'requirement',\n                  'denomenator': 'denominator', 'drparment': 'department', 'acumens': 'acumen s',\n                  'celemente': 'Clemente', 'manajement': 'management', 'govermenent': 'government',\n                  'accomplishmments': 'accomplishments', 'rendementry': 'rendement ry',\n                  'repariments': 'departments', 'menstrute': 'menstruate', 'determenistic': 'deterministic',\n                  'resigment': 'resignment', 'selfpayment': 'self payment', 'imrpovement': 'improvement',\n                  'enivironment': 'environment', 'compartmentley': 'compartment',\n                  'augumented': 'augmented', 'parmenent': 'permanent', 'dealignment': 'de alignment',\n                  'develepoments': 'developments', 'menstrated': 'menstruated', 'phnomenon': 'phenomenon',\n                  'Employmment': 'Employment', 'dimensionalise': 'dimensional ise', 'menigioma': 'meningioma',\n                  'recrument': 'recrement', 'Promenient': 'Provenient', 'gonverment': 'government',\n                  'statemment': 'statement', 'recuirement': 'requirement', 'invetsment': 'investment',\n                  'parilment': 'parchment', 'parmently': 'patiently', 'agreementindia': 'agreement india',\n                  'menifesto': 'manifesto', 'accomplsihments': 'accomplishments', 'disangagement': 'disengagement',\n                  'aevelopment': 'development', 'procument': 'procumbent', 'harashment': 'harassment',\n                  'Tiannanmen': 'Tiananmen', 'commensalisms': 'commensal isms', 'devlelpment': 'development',\n                  'dimensons': 'dimensions', 'recruitment2017': 'recruitment 2017', 'polishment': 'pol ishment',\n                  'CommentSafe': 'Comment Safe', 'meausrements': 'measurements', 'geomentrical': 'geometrical',\n                  'undervelopment': 'undevelopment', 'mensurational': 'mensuration al', 'fanmenow': 'fan menow',\n                  'permenganate': 'permanganate', 'bussinessmen': 'businessmen',\n                  'supertournaments': 'super tournaments', 'permanmently': 'permanently',\n                  'lamenectomy': 'lamnectomy', 'assignmentcanyon': 'assignment canyon', 'adgestment': 'adjustment',\n                  'mentalized': 'metalized', 'docyments': 'documents', 'requairment': 'requirement',\n                  'batsmencould': 'batsmen could', 'argumentetc': 'argument etc', 'enjoiment': 'enjoyment',\n                  'invement': 'movement', 'accompliushments': 'accomplishments', 'regements': 'regiments',\n                  'departmentHow': 'department How', 'Aremenian': 'Armenian', 'amenclinics': 'amen clinics',\n                  'nonfermented': 'non fermented', 'Instumentation': 'Instrumentation', 'mentalitiy': 'mentality',\n                  ' govermen ': 'goverment', 'underdevelopement': 'under developement', 'parlimentry': 'parliamentary',\n                  'indemenity': 'indemnity', 'Inatrumentation': 'Instrumentation', 'menedatory': 'mandatory',\n                  'mentiri': 'entire', 'accomploshments': 'accomplishments', 'instrumention': 'instrument ion',\n                  'afvertisements': 'advertisements', 'parlementarian': 'parlement arian',\n                  'entitlments': 'entitlements', 'endrosment': 'endorsement', 'improment': 'impriment',\n                  'archaemenid': 'Achaemenid', 'replecement': 'replacement', 'placdment': 'placement',\n                  'femenise': 'feminise', 'envinment': 'environment', 'AmenityCompany': 'Amenity Company',\n                  'increaments': 'increments', 'accomplihsments': 'accomplishments',\n                  'manygovernment': 'many government', 'panishments': 'punishments', 'elinment': 'eloinment',\n                  'mendalin': 'mend alin', 'farmention': 'farm ention', 'preincrement': 'pre increment',\n                  'postincrement': 'post increment', 'achviements': 'achievements', 'menditory': 'mandatory',\n                  'Emouluments': 'Emoluments', 'Stonemen': 'Stone men', 'menmium': 'medium',\n                  'entaglement': 'entanglement', 'integumen': 'integument', 'harassument': 'harassment',\n                  'retairment': 'retainment', 'enviorement': 'environment', 'tormentous': 'torment ous',\n                  'confiment': 'confident', 'Enchroachment': 'Encroachment', 'prelimenary': 'preliminary',\n                  'fudamental': 'fundamental', 'instrumenot': 'instrument', 'icrement': 'increment',\n                  'prodimently': 'prominently', 'meniss': 'menise', 'Whoimplemented': 'Who implemented',\n                  'Representment': 'Rep resentment', 'StartFragment': 'Start Fragment',\n                  'EndFragment': 'End Fragment', ' documentarie ': ' documentaries ', 'requriments': 'requirements',\n                  'constitutionaldevelopment': 'constitutional development', 'parlamentarians': 'parliamentarians',\n                  'Rumenova': 'Rumen ova', 'argruments': 'arguments', 'findamental': 'fundamental',\n                  'totalinvestment': 'total investment', 'gevernment': 'government', 'recmommend': 'recommend',\n                  'appsmoment': 'apps moment', 'menstruual': 'menstrual', 'immplemented': 'implemented',\n                  'engangement': 'engagement', 'invovement': 'involvement', 'returement': 'retirement',\n                  'simentaneously': 'simultaneously', 'accompishments': 'accomplishments',\n                  'menstraution': 'menstruation', 'experimently': 'experiment', 'abdimen': 'abdomen',\n                  'cemenet': 'cement', 'propelment': 'propel ment', 'unamendable': 'un amendable',\n                  'employmentnews': 'employment news', 'lawforcement': 'law forcement',\n                  'menstuating': 'menstruating', 'fevelopment': 'development', 'reglamented': 'reg lamented',\n                  'imrovment': 'improvement', 'recommening': 'recommending', 'sppliment': 'supplement',\n                  'measument': 'measurement', 'reimbrusement': 'reimbursement', 'Nutrament': 'Nutriment',\n                  'puniahment': 'punishment', 'subligamentous': 'sub ligamentous', 'comlementry': 'complementary',\n                  'reteirement': 'retirement', 'envioronments': 'environments', 'haraasment': 'harassment',\n                  'USAgovernment': 'USA government', 'Apartmentfinder': 'Apartment finder',\n                  'encironment': 'environment', 'metacompartment': 'meta compartment',\n                  'augumentation': 'argumentation', 'dsymenorrhoea': 'dysmenorrhoea',\n                  'nonabandonment': 'non abandonment', 'annoincement': 'announcement',\n                  'menberships': 'memberships', 'Gamenights': 'Game nights', 'enliightenment': 'enlightenment',\n                  'supplymentry': 'supplementary', 'parlamentary': 'parliamentary', 'duramen': 'dura men',\n                  'hotelmanagement': 'hotel management', 'deartment': 'department',\n                  'treatmentshelp': 'treatments help', 'attirements': 'attire ments',\n                  'amendmending': 'amend mending', 'pseudomeningocele': 'pseudo meningocele',\n                  'intrasegmental': 'intra segmental', 'treatmenent': 'treatment', 'infridgement': 'infringement',\n                  'infringiment': 'infringement', 'recrecommend': 'rec recommend', 'entartaiment': 'entertainment',\n                  'inplementing': 'implementing', 'indemendent': 'independent', 'tremendeous': 'tremendous',\n                  'commencial': 'commercial', 'scomplishments': 'accomplishments', 'Emplement': 'Implement',\n                  'dimensiondimensions': 'dimension dimensions', 'depolyment': 'deployment',\n                  'conpartment': 'compartment', 'govnments': 'movements', 'menstrat': 'menstruate',\n                  'accompplishments': 'accomplishments', 'Enchacement': 'Enchancement',\n                  'developmenent': 'development', 'emmenagogues': 'emmenagogue', 'aggeement': 'agreement',\n                  'elementsbond': 'elements bond', 'remenant': 'remnant', 'Manamement': 'Management',\n                  'Augumented': 'Augmented', 'dimensonless': 'dimensionless',\n                  'ointmentsointments': 'ointments ointments', 'achiements': 'achievements',\n                  'recurtment': 'recurrent', 'gouverments': 'governments', 'docoment': 'document',\n                  'programmingassignments': 'programming assignments', 'menifest': 'manifest',\n                  'investmentguru': 'investment guru', 'deployements': 'deployments', 'Invetsment': 'Investment',\n                  'plaement': 'placement', 'Perliament': 'Parliament', 'femenists': 'feminists',\n                  'ecumencial': 'ecumenical', 'advamcements': 'advancements', 'refundment': 'refund ment',\n                  'settlementtake': 'settlement take', 'mensrooms': 'mens rooms',\n                  'productManagement': 'product Management', 'armenains': 'armenians',\n                  'betweenmanagement': 'between management', 'difigurement': 'disfigurement',\n                  'Armenized': 'Armenize', 'hurrasement': 'hurra sement', 'mamgement': 'management',\n                  'momuments': 'monuments', 'eauipments': 'equipments', 'managemenet': 'management',\n                  'treetment': 'treatment', 'webdevelopement': 'web developement', 'supplemenary': 'supplementary',\n                  'Encironmental': 'Environmental', 'Understandment': 'Understand ment',\n                  'enrollnment': 'enrollment', 'thinkstrategic': 'think strategic', 'thinkinh': 'thinking',\n                  'Softthinks': 'Soft thinks', 'underthinking': 'under thinking', 'thinksurvey': 'think survey',\n                  'whitelash': 'white lash', 'whiteheds': 'whiteheads', 'whitetning': 'whitening',\n                  'whitegirls': 'white girls', 'whitewalkers': 'white walkers', 'manycountries': 'many countries',\n                  'accomany': 'accompany', 'fromGermany': 'from Germany', 'manychat': 'many chat',\n                  'Germanyl': 'Germany l', 'manyness': 'many ness', 'many4': 'many', 'exmuslims': 'ex muslims',\n                  'digitizeindia': 'digitize india', 'indiarush': 'india rush', 'indiareads': 'india reads',\n                  'telegraphindia': 'telegraph india', 'Southindia': 'South india', 'Airindia': 'Air india',\n                  'siliconindia': 'silicon india', 'airindia': 'air india', 'indianleaders': 'indian leaders',\n                  'fundsindia': 'funds india', 'indianarmy': 'indian army', 'Technoindia': 'Techno india',\n                  'Betterindia': 'Better india', 'capesindia': 'capes india', 'Rigetti': 'Ligetti',\n                  'vegetablr': 'vegetable', 'get90': 'get', 'Magetta': 'Maretta', 'nagetive': 'native',\n                  'isUnforgettable': 'is Unforgettable', 'get630': 'get 630', 'GadgetPack': 'Gadget Pack',\n                  'Languagetool': 'Language tool', 'bugdget': 'budget', 'africaget': 'africa get',\n                  'ABnegetive': 'Abnegative', 'orangetheory': 'orange theory', 'getsmuggled': 'get smuggled',\n                  'avegeta': 'ave geta', 'gettubg': 'getting', 'gadgetsnow': 'gadgets now',\n                  'surgetank': 'surge tank', 'gadagets': 'gadgets', 'getallparts': 'get allparts',\n                  'messenget': 'messenger', 'vegetarean': 'vegetarian', 'get1000': 'get 1000',\n                  'getfinancing': 'get financing', 'getdrip': 'get drip', 'AdsTargets': 'Ads Targets',\n                  'tgethr': 'together', 'vegetaries': 'vegetables', 'forgetfulnes': 'forgetfulness',\n                  'fisgeting': 'fidgeting', 'BudgetAir': 'Budget Air',\n                  'getDepersonalization': 'get Depersonalization', 'negetively': 'negatively',\n                  'gettibg': 'getting', 'nauget': 'naught', 'Bugetti': 'Bugatti', 'plagetum': 'plage tum',\n                  'vegetabale': 'vegetable', 'changetip': 'change tip', 'blackwashing': 'black washing',\n                  'blackpink': 'black pink', 'blackmoney': 'black money',\n                  'blackmarks': 'black marks', 'blackbeauty': 'black beauty', 'unblacklisted': 'un blacklisted',\n                  'blackdotes': 'black dotes', 'blackboxing': 'black boxing', 'blackpaper': 'black paper',\n                  'blackpower': 'black power', 'Latinamericans': 'Latin americans', 'musigma': 'mu sigma',\n                  'Indominus': 'In dominus', 'usict': 'USSCt', 'indominus': 'in dominus', 'Musigma': 'Mu sigma',\n                  'plus5': 'plus', 'Russiagate': 'Russia gate', 'russophobic': 'Russophobiac',\n                  'Marcusean': 'Marcuse an', 'Radijus': 'Radius', 'cobustion': 'combustion',\n                  'Austrialians': 'Australians', 'mylogenous': 'myogenous', 'Raddus': 'Radius',\n                  'hetrogenous': 'heterogenous', 'greenhouseeffect': 'greenhouse effect', 'aquous': 'aqueous',\n                  'Taharrush': 'Tahar rush', 'Senousa': 'Venous', 'diplococcus': 'diplo coccus',\n                  'CityAirbus': 'City Airbus', 'sponteneously': 'spontaneously', 'trustless': 't rustless',\n                  'Pushkaram': 'Pushkara m', 'Fusanosuke': 'Fu sanosuke', 'isthmuses': 'isthmus es',\n                  'lucideus': 'lucidum', 'overjustification': 'over justification', 'Bindusar': 'Bind usar',\n                  'cousera': 'couler', 'musturbation': 'masturbation', 'infustry': 'industry',\n                  'Huswifery': 'Huswife ry', 'rombous': 'bombous', 'disengenuously': 'disingenuously',\n                  'sllybus': 'syllabus', 'celcious': 'delicious', 'cellsius': 'celsius',\n                  'lethocerus': 'Lethocerus', 'monogmous': 'monogamous', 'Ballyrumpus': 'Bally rumpus',\n                  'Koushika': 'Koushik a', 'vivipoarous': 'viviparous', 'ludiculous': 'ridiculous',\n                  'sychronous': 'synchronous', 'industiry': 'industry', 'scuduse': 'scud use',\n                  'babymust': 'baby must', 'simultqneously': 'simultaneously', 'exust': 'ex ust',\n                  'notmusing': 'not musing', 'Zamusu': 'Amuse', 'tusaki': 'tu saki', 'Marrakush': 'Marrakesh',\n                  'justcheaptickets': 'just cheaptickets', 'Ayahusca': 'Ayahausca', 'samousa': 'samosa',\n                  'Gusenberg': 'Gutenberg', 'illustratuons': 'illustrations', 'extemporeneous': 'extemporaneous',\n                  'Mathusla': 'Mathusala', 'Confundus': 'Con fundus', 'tusts': 'trusts', 'poisenious': 'poisonous',\n                  'Mevius': 'Medius', 'inuslating': 'insulating', 'aroused21000': 'aroused 21000',\n                  'Wenzeslaus': 'Wenceslaus', 'JustinKase': 'Justin Kase', 'purushottampur': 'purushottam pur',\n                  'citruspay': 'citrus pay', 'secutus': 'sects', 'austentic': 'austenitic',\n                  'FacePlusPlus': 'Face PlusPlus', 'aysnchronous': 'asynchronous',\n                  'teamtreehouse': 'team treehouse', 'uncouncious': 'unconscious', 'Priebuss': 'Prie buss',\n                  'consciousuness': 'consciousness', 'susubsoil': 'su subsoil', 'trimegistus': 'Trismegistus',\n                  'protopeterous': 'protopterous', 'trustworhty': 'trustworthy', 'ushually': 'usually',\n                  'industris': 'industries', 'instantneous': 'instantaneous', 'superplus': 'super plus',\n                  'shrusti': 'shruti', 'hindhus': 'hindus', 'outonomous': 'autonomous', 'reliegious': 'religious',\n                  'Kousakis': 'Kou sakis', 'reusult': 'result', 'JanusGraph': 'Janus Graph',\n                  'palusami': 'palus ami', 'mussraff': 'muss raff', 'hukous': 'humous',\n                  'photoacoustics': 'photo acoustics', 'kushanas': 'kusha nas', 'justdile': 'justice',\n                  'Massahusetts': 'Massachusetts', 'uspset': 'upset', 'sustinet': 'sustinent',\n                  'consicious': 'conscious', 'Sadhgurus': 'Sadh gurus', 'hystericus': 'hysteric us',\n                  'visahouse': 'visa house', 'supersynchronous': 'super synchronous', 'posinous': 'rosinous',\n                  'Fernbus': 'Fern bus', 'Tiltbrush': 'Tilt brush', 'glueteus': 'gluteus', 'posionus': 'poisons',\n                  'Freus': 'Frees', 'Zhuchengtyrannus': 'Zhucheng tyrannus', 'savonious': 'sanious',\n                  'CusJo': 'Cusco', 'congusion': 'confusion', 'dejavus': 'dejavu s', 'uncosious': 'uncopious',\n                  'previius': 'previous', 'counciousness': 'conciousness', 'lustorus': 'lustrous',\n                  'sllyabus': 'syllabus', 'mousquitoes': 'mosquitoes', 'Savvius': 'Savvies', 'arceius': 'Arcesius',\n                  'prejusticed': 'prejudiced', 'requsitioned': 'requisitioned',\n                  'deindustralization': 'deindustrialization', 'muscleblaze': 'muscle blaze',\n                  'ConsciousX5': 'conscious', 'nitrogenious': 'nitrogenous', 'mauritious': 'mauritius',\n                  'rigrously': 'rigorously', 'Yutyrannus': 'Yu tyrannus', 'muscualr': 'muscular',\n                  'conscoiusness': 'consciousness', 'Causians': 'Crusians', 'WorkFusion': 'Work Fusion',\n                  'puspak': 'pu spak', 'Inspirus': 'Inspires', 'illiustrations': 'illustrations',\n                  'Nobushi': 'No bushi', 'theuseof': 'thereof', 'suspicius': 'suspicious', 'Intuous': 'Virtuous',\n                  'gaushalas': 'gaus halas', 'campusthrough': 'campus through', 'seriousity': 'seriosity',\n                  'resustence': 'resistence', 'geminatus': 'geminates', 'disquss': 'discuss',\n                  'nicholus': 'nicholas', 'Husnai': 'Hussar', 'diiscuss': 'discuss', 'diffussion': 'diffusion',\n                  'phusicist': 'physicist', 'ernomous': 'enormous', 'Khushali': 'Khushal i', 'heitus': 'Leitus',\n                  'cracksbecause': 'cracks because', 'Nautlius': 'Nautilus', 'trausted': 'trusted',\n                  'Dardandus': 'Dardanus', 'Megatapirus': 'Mega tapirus', 'clusture': 'culture',\n                  'vairamuthus': 'vairamuthu s', 'disclousre': 'disclosure',\n                  'industrilaization': 'industrialization', 'musilms': 'muslims', 'Australia9': 'Australian',\n                  'causinng': 'causing', 'ibdustries': 'industries', 'searious': 'serious',\n                  'Coolmuster': 'Cool muster', 'sissyphus': 'sisyphus', ' justificatio ': 'justification',\n                  'antihindus': 'anti hindus', 'Moduslink': 'Modus link', 'zymogenous': 'zymogen ous',\n                  'prospeorus': 'prosperous', 'Retrocausality': 'Retro causality', 'FusionGPS': 'Fusion GPS',\n                  'Mouseflow': 'Mouse flow', 'bootyplus': 'booty plus', 'Itylus': 'I tylus',\n                  'Olnhausen': 'Olshausen', 'suspeect': 'suspect', 'entusiasta': 'enthusiast',\n                  'fecetious': 'facetious', 'bussiest': 'fussiest', 'Draconius': 'Draconis',\n                  'requsite': 'requisite', 'nauseatic': 'nausea tic', 'Brusssels': 'Brussels',\n                  'repurcussion': 'repercussion', 'Jeisus': 'Jesus', 'philanderous': 'philander ous',\n                  'muslisms': 'muslims', 'august2017': 'august 2017', 'calccalculus': 'calc calculus',\n                  'unanonymously': 'un anonymously', 'Imaprtus': 'Impetus', 'carnivorus': 'carnivorous',\n                  'Corypheus': 'Coryphees', 'austronauts': 'astronauts', 'neucleus': 'nucleus',\n                  'housepoor': 'house poor', 'rescouses': 'responses', 'Tagushi': 'Tagus hi',\n                  'hyperfocusing': 'hyper focusing', 'nutriteous': 'nutritious', 'chylus': 'chylous',\n                  'preussure': 'pressure', 'outfocus': 'out focus', 'Hanfus': 'Hannus', 'Rustyrose': 'Rusty rose',\n                  'vibhushant': 'vibhushan t', 'conciousnes': 'conciousness', 'Venus25': 'Venus',\n                  'Sedataious': 'Seditious', 'promuslim': 'pro muslim', 'statusGuru': 'status Guru',\n                  'yousician': 'musician', 'transgenus': 'trans genus', 'Pushbullet': 'Push bullet',\n                  'jeesyllabus': 'jee syllabus', 'complusary': 'compulsory', 'Holocoust': 'Holocaust',\n                  'careerplus': 'career plus', 'Lllustrate': 'Illustrate', 'Musino': 'Musion',\n                  'Phinneus': 'Phineus', 'usedtoo': 'used too', 'JustBasic': 'Just Basic', 'webmusic': 'web music',\n                  'TrustKit': 'Trust Kit', 'industrZgies': 'industries', 'rubustness': 'robustness',\n                  'Missuses': 'Miss uses', 'Musturbation': 'Masturbation', 'bustees': 'bus tees',\n                  'justyfy': 'justify', 'pegusus': 'pegasus', 'industrybuying': 'industry buying',\n                  'advantegeous': 'advantageous', 'kotatsus': 'kotatsu s', 'justcreated': 'just created',\n                  'simultameously': 'simultaneously', 'husoone': 'huso one', 'twiceusing': 'twice using',\n                  'cetusplay': 'cetus play', 'sqamous': 'squamous', 'claustophobic': 'claustrophobic',\n                  'Kaushika': 'Kaushik a', 'dioestrus': 'di oestrus', 'Degenerous': 'De generous',\n                  'neculeus': 'nucleus', 'cutaneously': 'cu taneously', 'Alamotyrannus': 'Alamo tyrannus',\n                  'Ivanious': 'Avanious', 'arceous': 'araceous', 'Flixbus': 'Flix bus', 'caausing': 'causing',\n                  'publious': 'Publius', 'Juilus': 'Julius', 'Australianism': 'Australian ism',\n                  'vetronus': 'verrons', 'nonspontaneous': 'non spontaneous', 'calcalus': 'calculus',\n                  'commudus': 'Commodus', 'Rheusus': 'Rhesus', 'syallubus': 'syllabus', 'Yousician': 'Musician',\n                  'qurush': 'qu rush', 'athiust': 'athirst', 'conclusionless': 'conclusion less',\n                  'usertesting': 'user testing', 'redius': 'radius', 'Austrolia': 'Australia',\n                  'sllaybus': 'syllabus', 'toponymous': 'top onymous', 'businiss': 'business',\n                  'hyperthalamus': 'hyper thalamus', 'clause55': 'clause', 'cosicous': 'conscious',\n                  'Sushena': 'Saphena', 'Luscinus': 'Luscious', 'Prussophile': 'Russophile', 'jeaslous': 'jealous',\n                  'Austrelia': 'Australia', 'contiguious': 'contiguous',\n                  'subconsciousnesses': 'sub consciousnesses', ' jusification ': 'justification',\n                  'dilusion': 'delusion', 'anticoncussive': 'anti concussive', 'disngush': 'disgust',\n                  'constiously': 'consciously', 'filabustering': 'filibustering', 'GAPbuster': 'GAP buster',\n                  'insectivourous': 'insectivorous', 'glocuse': 'louse', 'Antritrust': 'Antitrust',\n                  'thisAustralian': 'this Australian', 'FusionDrive': 'Fusion Drive', 'nuclus': 'nucleus',\n                  'abussive': 'abusive', 'mustang1': 'mustangs', 'inradius': 'in radius', 'polonious': 'polonius',\n                  'ofKulbhushan': 'of Kulbhushan', 'homosporous': 'homos porous', 'circumradius': 'circum radius',\n                  'atlous': 'atrous', 'insustry': 'industry', 'campuswith': 'campus with', 'beacsuse': 'because',\n                  'concuous': 'conscious', 'nonHindus': 'non Hindus', 'carnivourous': 'carnivorous',\n                  'tradeplus': 'trade plus', 'Jeruselam': 'Jerusalem',\n                  'musuclar': 'muscular', 'deangerous': 'dangerous', 'disscused': 'discussed',\n                  'industdial': 'industrial', 'sallatious': 'fallacious', 'rohmbus': 'rhombus',\n                  'golusu': 'gol usu', 'Minangkabaus': 'Minangkabau s', 'Mustansiriyah': 'Mustansiriya h',\n                  'anomymously': 'anonymously', 'abonymously': 'anonymously', 'indrustry': 'industry',\n                  'Musharrf': 'Musharraf', 'workouses': 'workhouses', 'sponataneously': 'spontaneously',\n                  'anmuslim': 'an muslim', 'syallbus': 'syllabus', 'presumptuousnes': 'presumptuousness',\n                  'Thaedus': 'Thaddus', 'industey': 'industry', 'hkust': 'hust', 'Kousseri': 'Kousser i',\n                  'mousestats': 'mouses tats', 'russiagate': 'russia gate', 'simantaneously': 'simultaneously',\n                  'Austertana': 'Auster tana', 'infussions': 'infusions', 'coclusion': 'conclusion',\n                  'sustainabke': 'sustainable', 'tusami': 'tu sami', 'anonimously': 'anonymously',\n                  'usebase': 'use base', 'balanoglossus': 'Balanoglossus', 'Unglaus': 'Ung laus',\n                  'ignoramouses': 'ignoramuses', 'snuus': 'snugs', 'reusibility': 'reusability',\n                  'Straussianism': 'Straussian ism', 'simoultaneously': 'simultaneously',\n                  'realbonus': 'real bonus', 'nuchakus': 'nunchakus', 'annonimous': 'anonymous',\n                  'Incestious': 'Incestuous', 'Manuscriptology': 'Manuscript ology', 'difusse': 'diffuse',\n                  'Pliosaurus': 'Pliosaur us', 'cushelle': 'cush elle', 'Catallus': 'Catullus',\n                  'MuscleBlaze': 'Muscle Blaze', 'confousing': 'confusing', 'enthusiasmless': 'enthusiasm less',\n                  'Tetherusd': 'Tethered', 'Josephius': 'Josephus', 'jusrlt': 'just',\n                  'simutaneusly': 'simultaneously', 'mountaneous': 'mountainous', 'Badonicus': 'Sardonicus',\n                  'muccus': 'mucous', 'nicus': 'nidus', 'austinlizards': 'austin lizards',\n                  'errounously': 'erroneously', 'Australua': 'Australia', 'sylaabus': 'syllabus',\n                  'dusyant': 'distant', 'javadiscussion': 'java discussion', 'megabuses': 'mega buses',\n                  'danergous': 'dangerous', 'contestious': 'contentious', 'exause': 'excuse',\n                  'muscluar': 'muscular', 'avacous': 'vacuous', 'Ingenhousz': 'Ingenious',\n                  'holocausting': 'holocaust ing', 'Pakustan': 'Pakistan', 'purusharthas': 'purushartha',\n                  'bapus': 'bapu s', 'useul': 'useful', 'pretenious': 'pretentious', 'homogeneus': 'homogeneous',\n                  'bhlushes': 'blushes', 'Saggittarius': 'Sagittarius', 'sportsusa': 'sports usa',\n                  'kerataconus': 'keratoconus', 'infrctuous': 'infectuous', 'Anonoymous': 'Anonymous',\n                  'triphosphorus': 'tri phosphorus', 'ridicjlously': 'ridiculously',\n                  'worldbusiness': 'world business', 'hollcaust': 'holocaust', 'Dusra': 'Dura',\n                  'meritious': 'meritorious', 'Sauskes': 'Causes', 'inudustry': 'industry',\n                  'frustratd': 'frustrate', 'hypotenous': 'hypogenous', 'Dushasana': 'Dush asana',\n                  'saadus': 'status', 'keratokonus': 'keratoconus', 'Jarrus': 'Harrus', 'neuseous': 'nauseous',\n                  'simutanously': 'simultaneously', 'diphosphorus': 'di phosphorus', 'sulprus': 'surplus',\n                  'Hasidus': 'Hasid us', 'suspenive': 'suspensive', 'illlustrator': 'illustrator',\n                  'userflows': 'user flows', 'intrusivethoughts': 'intrusive thoughts', 'countinous': 'continuous',\n                  'gpusli': 'gusli', 'Calculus1': 'Calculus', 'bushiri': 'Bushire',\n                  'torvosaurus': 'Torosaurus', 'chestbusters': 'chest busters', 'Satannus': 'Sat annus',\n                  'falaxious': 'fallacious', 'obnxious': 'obnoxious', 'tranfusions': 'transfusions',\n                  'PlayMagnus': 'Play Magnus', 'Epicodus': 'Episodes', 'Hypercubus': 'Hypercubes',\n                  'Musickers': 'Musick ers', 'programmebecause': 'programme because', 'indiginious': 'indigenous',\n                  'housban': 'Housman', 'iusso': 'kusso', 'annilingus': 'anilingus', 'Nennus': 'Genius',\n                  'pussboy': 'puss boy', 'Photoacoustics': 'Photo acoustics', 'Hindusthanis': 'Hindustanis',\n                  'lndustrial': 'industrial', 'tyrannously': 'tyrannous', 'Susanoomon': 'Susanoo mon',\n                  'colmbus': 'columbus', 'sussessful': 'successful', 'ousmania': 'ous mania',\n                  'ilustrating': 'illustrating', 'famousbirthdays': 'famous birthdays',\n                  'suspectance': 'suspect ance', 'extroneous': 'extraneous', 'teethbrush': 'teeth brush',\n                  'abcmouse': 'abc mouse', 'degenerous': 'de generous', 'doesGauss': 'does Gauss',\n                  'insipudus': 'insipidus', 'movielush': 'movie lush', 'Rustichello': 'Rustic hello',\n                  'Firdausiya': 'Firdausi ya', 'checkusers': 'check users', 'householdware': 'household ware',\n                  'prosporously': 'prosperously', 'SteLouse': 'Ste Louse', 'obfuscaton': 'obfuscation',\n                  'amorphus': 'amorph us', 'trustworhy': 'trustworthy', 'celsious': 'cesious',\n                  'dangorous': 'dangerous', 'anticancerous': 'anti cancerous', 'cousi ': 'cousin ',\n                  'austroloid': 'australoid', 'fergussion': 'percussion', 'andKyokushin': 'and Kyokushin',\n                  'cousan': 'cousin', 'Huskystar': 'Hu skystar', 'retrovisus': 'retrovirus', 'becausr': 'because',\n                  'Jerusalsem': 'Jerusalem', 'motorious': 'notorious', 'industrilised': 'industrialised',\n                  'powerballsusa': 'powerballs usa', 'monoceious': 'monoecious', 'batteriesplus': 'batteries plus',\n                  'nonviscuous': 'nonviscous', 'industion': 'induction', 'bussinss': 'bussings',\n                  'userbags': 'user bags', 'Jlius': 'Julius', 'thausand': 'thousand', 'plustwo': 'plus two',\n                  'defpush': 'def push', 'subconcussive': 'sub concussive', 'muslium': 'muslim',\n                  'industrilization': 'industrialization', 'Maurititus': 'Mauritius', 'uslme': 'some',\n                  'Susgaon': 'Surgeon', 'Pantherous': 'Panther ous', 'antivirius': 'antivirus',\n                  'Trustclix': 'Trust clix', 'silumtaneously': 'simultaneously', 'Icompus': 'Corpus',\n                  'atonomous': 'autonomous', 'Reveuse': 'Reve use', 'legumnous': 'leguminous',\n                  'syllaybus': 'syllabus', 'louspeaker': 'loudspeaker', 'susbtraction': 'substraction',\n                  'virituous': 'virtuous', 'disastrius': 'disastrous', 'jerussalem': 'jerusalem',\n                  'Industrailzed': 'Industrialized', 'recusion': 'recushion',\n                  'simultenously': 'simultaneously',\n                  'Pulphus': 'Pulpous', 'harbaceous': 'herbaceous', 'phlegmonous': 'phlegmon ous', 'use38': 'use',\n                  'jusify': 'justify', 'instatanously': 'instantaneously', 'tetramerous': 'tetramer ous',\n                  'usedvin': 'used vin', 'sagittarious': 'sagittarius', 'mausturbate': 'masturbate',\n                  'subcautaneous': 'subcutaneous', 'dangergrous': 'dangerous', 'sylabbus': 'syllabus',\n                  'hetorozygous': 'heterozygous', 'Ignasius': 'Ignacius', 'businessbor': 'business bor',\n                  'Bhushi': 'Thushi', 'Moussolini': 'Mussolini', 'usucaption': 'usu caption',\n                  'Customzation': 'Customization', 'cretinously': 'cretinous', 'genuiuses': 'geniuses',\n                  'Moushmee': 'Mousmee', 'neigous': 'nervous',\n                  'infrustructre': 'infrastructure', 'Ilusha': 'Ilesha', 'suconciously': 'unconciously',\n                  'stusy': 'study', 'mustectomy': 'mastectomy', 'Farmhousebistro': 'Farmhouse bistro',\n                  'instantanous': 'instantaneous', 'JustForex': 'Just Forex', 'Indusyry': 'Industry',\n                  'mustabating': 'must abating', 'uninstrusive': 'unintrusive', 'customshoes': 'customs hoes',\n                  'homageneous': 'homogeneous', 'Empericus': 'Imperious', 'demisexuality': 'demi sexuality',\n                  'transexualism': 'transsexualism', 'sexualises': 'sexualise', 'demisexuals': 'demisexual',\n                  'sexuly': 'sexily', 'Pornosexuality': 'Porno sexuality', 'sexond': 'second', 'sexxual': 'sexual',\n                  'asexaul': 'asexual', 'sextactic': 'sex tactic', 'sexualityism': 'sexuality ism',\n                  'monosexuality': 'mono sexuality', 'intwrsex': 'intersex', 'hypersexualize': 'hyper sexualize',\n                  'homosexualtiy': 'homosexuality', 'examsexams': 'exams exams', 'sexmates': 'sex mates',\n                  'sexyjobs': 'sexy jobs', 'sexitest': 'sexiest', 'fraysexual': 'fray sexual',\n                  'sexsurrogates': 'sex surrogates', 'sexuallly': 'sexually', 'gamersexual': 'gamer sexual',\n                  'greysexual': 'grey sexual', 'omnisexuality': 'omni sexuality', 'hetereosexual': 'heterosexual',\n                  'productsexamples': 'products examples', 'sexgods': 'sex gods', 'semisexual': 'semi sexual',\n                  'homosexulity': 'homosexuality', 'sexeverytime': 'sex everytime', 'neurosexist': 'neuro sexist',\n                  'worldquant': 'world quant', 'Freshersworld': 'Freshers world', 'smartworld': 'sm artworld',\n                  'Mistworlds': 'Mist worlds', 'boothworld': 'booth world', 'ecoworld': 'eco world',\n                  'Ecoworld': 'Eco world', 'underworldly': 'under worldly', 'worldrank': 'world rank',\n                  'Clearworld': 'Clear world', 'Boothworld': 'Booth world', 'Rimworld': 'Rim world',\n                  'cryptoworld': 'crypto world', 'machineworld': 'machine world', 'worldwideley': 'worldwide ley',\n                  'capuletwant': 'capulet want', 'Bhagwanti': 'Bhagwant i', 'Unwanted72': 'Unwanted 72',\n                  'wantrank': 'want rank',\n                  'willhappen': 'will happen', 'thateasily': 'that easily',\n                  'Whatevidence': 'What evidence', 'metaphosphates': 'meta phosphates',\n                  'exilarchate': 'exilarch ate', 'aulphate': 'sulphate', 'Whateducation': 'What education',\n                  'persulphates': 'per sulphates', 'disulphate': 'di sulphate', 'picosulphate': 'pico sulphate',\n                  'tetraosulphate': 'tetrao sulphate', 'prechinese': 'pre chinese',\n                  'Hellochinese': 'Hello chinese', 'muchdeveloped': 'much developed', 'stomuch': 'stomach',\n                  'Whatmakes': 'What makes', 'Lensmaker': 'Lens maker', 'eyemake': 'eye make',\n                  'Techmakers': 'Tech makers', 'cakemaker': 'cake maker', 'makeup411': 'makeup 411',\n                  'objectmake': 'object make', 'crazymaker': 'crazy maker', 'techmakers': 'tech makers',\n                  'makedonian': 'macedonian', 'makeschool': 'make school', 'anxietymake': 'anxiety make',\n                  'makeshifter': 'make shifter', 'countryball': 'country ball', 'Whichcountry': 'Which country',\n                  'countryHow': 'country How', 'Zenfone': 'Zen fone', 'Electroneum': 'Electro neum',\n                  'electroneum': 'electro neum', 'Demonetisation': 'demonetization', 'zenfone': 'zen fone',\n                  'ZenFone': 'Zen Fone', 'onecoin': 'one coin', 'demonetizing': 'demonetized',\n                  'iphone7': 'iPhone', 'iPhone6': 'iPhone', 'microneedling': 'micro needling', 'iphone6': 'iPhone',\n                  'Monegasques': 'Monegasque s', 'demonetised': 'demonetized',\n                  'EveryoneDiesTM': 'EveryoneDies TM', 'teststerone': 'testosterone', 'DoneDone': 'Done Done',\n                  'papermoney': 'paper money', 'Sasabone': 'Sasa bone', 'Blackphone': 'Black phone',\n                  'Bonechiller': 'Bone chiller', 'Moneyfront': 'Money front', 'workdone': 'work done',\n                  'iphoneX': 'iPhone', 'roxycodone': 'r oxycodone',\n                  'moneycard': 'money card', 'Fantocone': 'Fantocine', 'eletronegativity': 'electronegativity',\n                  'mellophones': 'mellophone s', 'isotones': 'iso tones', 'donesnt': 'doesnt',\n                  'thereanyone': 'there anyone', 'electronegativty': 'electronegativity',\n                  'commissiioned': 'commissioned', 'earvphone': 'earphone', 'condtioners': 'conditioners',\n                  'demonetistaion': 'demonetization', 'ballonets': 'ballo nets', 'DoneClaim': 'Done Claim',\n                  'alimoney': 'alimony', 'iodopovidone': 'iodo povidone', 'bonesetters': 'bone setters',\n                  'componendo': 'compon endo', 'probationees': 'probationers', 'one300': 'one 300',\n                  'nonelectrolyte': 'non electrolyte', 'ozonedepletion': 'ozone depletion',\n                  'Stonehart': 'Stone hart', 'Vodafone2': 'Vodafones', 'chaparone': 'chaperone',\n                  'Noonein': 'Noo nein', 'Frosione': 'Erosion', 'IPhone7': 'Iphone', 'pentanone': 'penta none',\n                  'poneglyphs': 'pone glyphs', 'cyclohexenone': 'cyclohexanone', 'marlstone': 'marls tone',\n                  'androneda': 'andromeda', 'iphone8': 'iPhone', 'acidtone': 'acid tone',\n                  'noneconomically': 'non economically', 'Honeyfund': 'Honey fund', 'germanophone': 'Germanophobe',\n                  'Democratizationed': 'Democratization ed', 'haoneymoon': 'honeymoon', 'iPhone7': 'iPhone 7',\n                  'someonewith': 'some onewith', 'Hexanone': 'Hexa none', 'bonespur': 'bones pur',\n                  'sisterzoned': 'sister zoned', 'HasAnyone': 'Has Anyone',\n                  'stonepelters': 'stone pelters', 'Chronexia': 'Chronaxia', 'brotherzone': 'brother zone',\n                  'brotherzoned': 'brother zoned', 'fonecare': 'f onecare', 'nonexsistence': 'nonexistence',\n                  'conents': 'contents', 'phonecases': 'phone cases', 'Commissionerates': 'Commissioner ates',\n                  'activemoney': 'active money', 'dingtone': 'ding tone', 'wheatestone': 'wheatstone',\n                  'chiropractorone': 'chiropractor one', 'heeadphones': 'headphones', 'Maimonedes': 'Maimonides',\n                  'onepiecedeals': 'onepiece deals', 'oneblade': 'one blade', 'venetioned': 'Venetianed',\n                  'sunnyleone': 'sunny leone', 'prendisone': 'prednisone', 'Anglosaxophone': 'Anglo saxophone',\n                  'Blackphones': 'Black phones', 'jionee': 'jinnee', 'chromonema': 'chromo nema',\n                  'iodoketones': 'iodo ketones', 'demonetizations': 'demonetization', 'aomeone': 'someone',\n                  'trillonere': 'trillones', 'abandonee': 'abandon',\n                  'MasterColonel': 'Master Colonel', 'fronend': 'friend', 'Wildstone': 'Wilds tone',\n                  'patitioned': 'petitioned', 'lonewolfs': 'lone wolfs', 'Spectrastone': 'Spectra stone',\n                  'dishonerable': 'dishonorable', 'poisiones': 'poisons',\n                  'condioner': 'conditioner', 'unpermissioned': 'unper missioned', 'friedzone': 'fried zone',\n                  'umumoney': 'umu money', 'anyonestudied': 'anyone studied', 'dictioneries': 'dictionaries',\n                  'nosebone': 'nose bone', 'ofVodafone': 'of Vodafone',\n                  'Yumstone': 'Yum stone', 'oxandrolonesteroid': 'oxandrolone steroid',\n                  'Mifeprostone': 'Mifepristone', 'pheramones': 'pheromones',\n                  'sinophone': 'Sinophobe', 'peloponesian': 'peloponnesian', 'michrophone': 'microphone',\n                  'commissionets': 'commissioners', 'methedone': 'methadone', 'cobditioners': 'conditioners',\n                  'urotone': 'protone', 'smarthpone': 'smartphone', 'conecTU': 'connect you', 'beloney': 'boloney',\n                  'comfortzone': 'comfort zone', 'testostersone': 'testosterone', 'camponente': 'component',\n                  'Idonesia': 'Indonesia', 'dolostones': 'dolostone', 'psiphone': 'psi phone',\n                  'ceftriazone': 'ceftriaxone', 'feelonely': 'feel onely', 'monetation': 'moderation',\n                  'activationenergy': 'activation energy', 'moneydriven': 'money driven',\n                  'staionery': 'stationery', 'zoneflex': 'zone flex', 'moneycash': 'money cash',\n                  'conectiin': 'connection', 'Wannaone': 'Wanna one',\n                  'Pictones': 'Pict ones', 'demonentization': 'demonetization',\n                  'phenonenon': 'phenomenon', 'evenafter': 'even after', 'Sevenfriday': 'Seven friday',\n                  'Devendale': 'Evendale', 'theeventchronicle': 'the event chronicle',\n                  'seventysomething': 'seventy something', 'sevenpointed': 'seven pointed',\n                  'richfeel': 'rich feel', 'overfeel': 'over feel', 'feelingstupid': 'feeling stupid',\n                  'Photofeeler': 'Photo feeler', 'feelomgs': 'feelings', 'feelinfs': 'feelings',\n                  'PlayerUnknown': 'Player Unknown', 'Playerunknown': 'Player unknown', 'knowlefge': 'knowledge',\n                  'knowledgd': 'knowledge', 'knowledeg': 'knowledge', 'knowble': 'Knowle', 'Howknow': 'Howk now',\n                  'knowledgeWoods': 'knowledge Woods', 'knownprogramming': 'known programming',\n                  'selfknowledge': 'self knowledge', 'knowldage': 'knowledge', 'knowyouve': 'know youve',\n                  'aknowlege': 'knowledge', 'Audetteknown': 'Audette known', 'knowlegdeable': 'knowledgeable',\n                  'trueoutside': 'true outside', 'saynthesize': 'synthesize', 'EssayTyper': 'Essay Typer',\n                  'meesaya': 'mee saya', 'Rasayanam': 'Rasayan am', 'fanessay': 'fan essay', 'momsays': 'moms ays',\n                  'sayying': 'saying', 'saydaw': 'say daw', 'Fanessay': 'Fan essay', 'theyreally': 'they really',\n                  'gayifying': 'gayed up with homosexual love', 'gayke': 'gay Online retailers',\n                  'Lingayatism': 'Lingayat',\n                  'macapugay': 'Macaulay', 'jewsplain': 'jews plain',\n                  'banggood': 'bang good', 'goodfriends': 'good friends',\n                  'goodfirms': 'good firms', 'Banggood': 'Bang good', 'dogooder': 'do gooder',\n                  'stillshots': 'stills hots', 'stillsuits': 'still suits', 'panromantic': 'pan romantic',\n                  'paracommando': 'para commando', 'romantize': 'romanize', 'manupulative': 'manipulative',\n                  'manjha': 'mania', 'mankrit': 'mank rit',\n                  'heteroromantic': 'hetero romantic', 'pulmanery': 'pulmonary', 'manpads': 'man pads',\n                  'supermaneuverable': 'super maneuverable', 'mandatkry': 'mandatory', 'armanents': 'armaments',\n                  'manipative': 'mancipative', 'himanity': 'humanity', 'maneuever': 'maneuver',\n                  'Kumarmangalam': 'Kumar mangalam', 'Brahmanwadi': 'Brahman wadi',\n                  'exserviceman': 'ex serviceman',\n                  'managewp': 'managed', 'manies': 'many', 'recordermans': 'recorder mans',\n                  'Feymann': 'Heymann', 'salemmango': 'salem mango', 'manufraturing': 'manufacturing',\n                  'sreeman': 'freeman', 'tamanaa': 'Tamanac', 'chlamydomanas': 'chlamydomonas',\n                  'comandant': 'commandant', 'huemanity': 'humanity', 'manaagerial': 'managerial',\n                  'lithromantics': 'lith romantics',\n                  'geramans': 'germans', 'Nagamandala': 'Naga mandala', 'humanitariarism': 'humanitarianism',\n                  'wattman': 'watt man', 'salesmanago': 'salesman ago', 'Washwoman': 'Wash woman',\n                  'rammandir': 'ram mandir', 'nomanclature': 'nomenclature', 'Haufman': 'Kaufman',\n                  'prefomance': 'performance', 'ramanunjan': 'Ramanujan', 'Freemansonry': 'Freemasonry',\n                  'supermaneuverability': 'super maneuverability', 'manstruate': 'menstruate',\n                  'Tarumanagara': 'Taruma nagara', 'RomanceTale': 'Romance Tale', 'heteromantic': 'hete romantic',\n                  'terimanals': 'terminals', 'womansplaining': 'wo mansplaining',\n                  'performancelearning': 'performance learning', 'sociomantic': 'sciomantic',\n                  'batmanvoice': 'batman voice', 'PerformanceTesting': 'Performance Testing',\n                  'manorialism': 'manorial ism', 'newscommando': 'news commando',\n                  'Entwicklungsroman': 'Entwicklungs roman',\n                  'Kunstlerroman': 'Kunstler roman', 'bodhidharman': 'Bodhidharma', 'Howmaney': 'How maney',\n                  'manufucturing': 'manufacturing', 'remmaning': 'remaining', 'rangeman': 'range man',\n                  'mythomaniac': 'mythomania', 'katgmandu': 'katmandu',\n                  'Superowoman': 'Superwoman', 'Rahmanland': 'Rahman land', 'Dormmanu': 'Dormant',\n                  'Geftman': 'Gentman', 'manufacturig': 'manufacturing', 'bramanistic': 'Brahmanistic',\n                  'padmanabhanagar': 'padmanabhan agar', 'homoromantic': 'homo romantic', 'femanists': 'feminists',\n                  'demihuman': 'demi human', 'manrega': 'Manresa', 'Pasmanda': 'Pas manda',\n                  'manufacctured': 'manufactured', 'remaninder': 'remainder', 'Marimanga': 'Mari manga',\n                  'Sloatman': 'Sloat man', 'manlet': 'man let', 'perfoemance': 'performance',\n                  'mangolian': 'mongolian', 'mangekyu': 'mange kyu', 'mansatory': 'mandatory',\n                  'managemebt': 'management', 'manufctures': 'manufactures', 'Bramanical': 'Brahmanical',\n                  'manaufacturing': 'manufacturing', 'Lakhsman': 'Lakhs man', 'Sarumans': 'Sarum ans',\n                  'mangalasutra': 'mangalsutra', 'Germanised': 'German ised',\n                  'managersworking': 'managers working', 'cammando': 'commando', 'mandrillaris': 'mandrill aris',\n                  'Emmanvel': 'Emmarvel', 'manupalation': 'manipulation', 'welcomeromanian': 'welcome romanian',\n                  'humanfemale': 'human female', 'mankirt': 'mankind', 'Haffmann': 'Hoffmann',\n                  'Panromantic': 'Pan romantic', 'demantion': 'detention', 'Suparwoman': 'Superwoman',\n                  'parasuramans': 'parasuram ans', 'sulmann': 'Suilmann', 'Shubman': 'Subman',\n                  'manspread': 'man spread', 'mandingan': 'Mandingan', 'mandalikalu': 'mandalika lu',\n                  'manufraturer': 'manufacturer', 'Wedgieman': 'Wedgie man', 'manwues': 'manages',\n                  'humanzees': 'human zees', 'Steymann': 'Stedmann', 'Jobberman': 'Jobber man',\n                  'maniquins': 'mani quins', 'biromantical': 'bi romantical', 'Rovman': 'Roman',\n                  'pyromantic': 'pyro mantic', 'Tastaman': 'Rastaman', 'Spoolman': 'Spool man',\n                  'Subramaniyan': 'Subramani yan', 'abhimana': 'abhiman a', 'manholding': 'man holding',\n                  'seviceman': 'serviceman', 'womansplained': 'womans plained', 'manniya': 'mania',\n                  'Bhraman': 'Braman', 'Laakman': 'Layman', 'mansturbate': 'masturbate',\n                  'Sulamaniya': 'Sulamani ya', 'demanters': 'decanters', 'postmanare': 'postman are',\n                  'mannualy': 'annual', 'rstman': 'Rotman', 'permanentjobs': 'permanent jobs',\n                  'Allmang': 'All mang', 'TradeCommander': 'Trade Commander', 'BasedStickman': 'Based Stickman',\n                  'Deshabhimani': 'Desha bhimani', 'manslamming': 'mans lamming', 'Brahmanwad': 'Brahman wad',\n                  'fundemantally': 'fundamentally', 'supplemantary': 'supplementary', 'egomanias': 'ego manias',\n                  'manvantar': 'Manvantara', 'spymania': 'spy mania', 'mangonada': 'mango nada',\n                  'manthras': 'mantras', 'Humanpark': 'Human park', 'manhuas': 'mahuas',\n                  'manterrupting': 'interrupting', 'dermatillomaniac': 'dermatillomania',\n                  'performancies': 'performances', 'manipulant': 'manipulate',\n                  'painterman': 'painter man', 'mangalik': 'manglik',\n                  'Neurosemantics': 'Neuro semantics', 'discrimantion': 'discrimination',\n                  'Womansplaining': 'feminist', 'mongodump': 'mongo dump', 'roadgods': 'road gods',\n                  'Oligodendraglioma': 'Oligodendroglioma', 'unrightly': 'un rightly', 'Janewright': 'Jane wright',\n                  ' righten ': ' tighten ', 'brightiest': 'brightest',\n                  'frighter': 'fighter', 'righteouness': 'righteousness', 'triangleright': 'triangle right',\n                  'Brightspace': 'Brights pace', 'techinacal': 'technical', 'chinawares': 'china wares',\n                  'Vancouever': 'Vancouver', 'cheverlet': 'cheveret', 'deverstion': 'diversion',\n                  'everbodys': 'everybody', 'Dramafever': 'Drama fever', 'reverificaton': 'reverification',\n                  'canterlever': 'canter lever', 'keywordseverywhere': 'keywords everywhere',\n                  'neverunlearned': 'never unlearned', 'everyfirst': 'every first',\n                  'neverhteless': 'nevertheless', 'clevercoyote': 'clever coyote', 'irrevershible': 'irreversible',\n                  'achievership': 'achievers hip', 'easedeverything': 'eased everything', 'youbever': 'you bever',\n                  'everperson': 'ever person', 'everydsy': 'everyday', 'whemever': 'whenever',\n                  'everyonr': 'everyone', 'severiity': 'severity', 'narracist': 'nar racist',\n                  'racistly': 'racist', 'takesuch': 'take such', 'mystakenly': 'mistakenly',\n                  'shouldntake': 'shouldnt take', 'Kalitake': 'Kali take', 'msitake': 'mistake',\n                  'straitstimes': 'straits times', 'timefram': 'timeframe', 'watchtime': 'watch time',\n                  'timetraveling': 'timet raveling', 'peactime': 'peacetime', 'timetabe': 'timetable',\n                  'cooktime': 'cook time', 'blocktime': 'block time', 'timesjobs': 'times jobs',\n                  'timesence': 'times ence', 'Touchtime': 'Touch time', 'timeloop': 'time loop',\n                  'subcentimeter': 'sub centimeter', 'timejobs': 'time jobs', 'Guardtime': 'Guard time',\n                  'realtimepolitics': 'realtime politics', 'loadingtimes': 'loading times',\n                  'timesnow': '24-hour English news channel in India', 'timesspark': 'times spark',\n                  'timetravelling': 'timet ravelling',\n                  'antimeter': 'anti meter', 'timewaste': 'time waste', 'cryptochristians': 'crypto christians',\n                  'Whatcould': 'What could', 'becomesdouble': 'becomes double', 'deathbecomes': 'death becomes',\n                  'youbecome': 'you become', 'greenseer': 'people who possess the magical ability',\n                  'rseearch': 'research', 'homeseek': 'home seek',\n                  'Greenseer': 'people who possess the magical ability', 'starseeders': 'star seeders',\n                  'seekingmillionaire': 'seeking millionaire', 'see\\u202c': 'see',\n                  'seeies': 'series', 'CodeAgon': 'Code Agon',\n                  'royago': 'royal', 'Dragonkeeper': 'Dragon keeper', 'mcgreggor': 'McGregor',\n                  'catrgory': 'category', 'Dragonknight': 'Dragon knight', 'Antergos': 'Anteros',\n                  'togofogo': 'togo fogo', 'mongorestore': 'mongo restore', 'gorgops': 'gorgons',\n                  'withgoogle': 'with google', 'goundar': 'Gondar', 'algorthmic': 'algorithmic',\n                  'goatnuts': 'goat nuts', 'vitilgo': 'vitiligo', 'polygony': 'poly gony',\n                  'digonals': 'diagonals', 'Luxemgourg': 'Luxembourg', 'UCSanDiego': 'UC SanDiego',\n                  'Ringostat': 'Ringo stat', 'takingoff': 'taking off', 'MongoImport': 'Mongo Import',\n                  'alggorithms': 'algorithms', 'dragonknight': 'dragon knight', 'negotiatior': 'negotiation',\n                  'gomovies': 'go movies', 'Withgott': 'Without',\n                  'categoried': 'categories', 'Stocklogos': 'Stock logos', 'Pedogogical': 'Pedological',\n                  'Wedugo': 'Wedge', 'golddig': 'gold dig', 'goldengroup': 'golden group',\n                  'merrigo': 'merligo', 'googlemapsAPI': 'googlemaps API', 'goldmedal': 'gold medal',\n                  'golemized': 'polemized', 'Caligornia': 'California', 'unergonomic': 'un ergonomic',\n                  'fAegon': 'wagon', 'vertigos': 'vertigo s', 'trigonomatry': 'trigonometry',\n                  'hypogonadic': 'hypogonadia', 'Mogolia': 'Mongolia', 'governmaent': 'government',\n                  'ergotherapy': 'ergo therapy', 'Bogosort': 'Bogo sort', 'goalwise': 'goal wise',\n                  'alogorithms': 'algorithms', 'MercadoPago': 'Mercado Pago', 'rivigo': 'rivi go',\n                  'govshutdown': 'gov shutdown', 'gorlfriend': 'girlfriend',\n                  'stategovt': 'state govt', 'Chickengonia': 'Chicken gonia', 'Yegorovich': 'Yegorov ich',\n                  'regognitions': 'recognitions', 'gorichen': 'Gori Chen Mountain',\n                  'goegraphies': 'geographies', 'gothras': 'goth ras', 'belagola': 'bela gola',\n                  'snapragon': 'snapdragon', 'oogonial': 'oogonia l', 'Amigofoods': 'Amigo foods',\n                  'Sigorn': 'son of Styr', 'algorithimic': 'algorithmic',\n                  'innermongolians': 'inner mongolians', 'ArangoDB': 'Arango DB', 'zigolo': 'gigolo',\n                  'regognized': 'recognized', 'Moongot': 'Moong ot', 'goldquest': 'gold quest',\n                  'catagorey': 'category', 'got7': 'got', 'jetbingo': 'jet bingo', 'Dragonchain': 'Dragon chain',\n                  'catwgorized': 'categorized', 'gogoro': 'gogo ro', 'Tobagoans': 'Tobago ans',\n                  'digonal': 'di gonal', 'algoritmic': 'algorismic', 'dragonflag': 'dragon flag',\n                  'Indigoflight': 'Indigo flight',\n                  'governening': 'governing', 'ergosphere': 'ergo sphere',\n                  'pingo5': 'pingo', 'Montogo': 'montego', 'Rivigo': 'technology-enabled logistics company',\n                  'Jigolo': 'Gigolo', 'phythagoras': 'pythagoras', 'Mangolian': 'Mongolian',\n                  'forgottenfaster': 'forgotten faster', 'stargold': 'a Hindi movie channel',\n                  'googolplexain': 'googolplexian', 'corpgov': 'corp gov',\n                  'govtribe': 'provides real-time federal contracting market intel',\n                  'dragonglass': 'dragon glass', 'gorakpur': 'Gorakhpur', 'MangoPay': 'Mango Pay',\n                  'chigoe': 'sub-tropical climates', 'BingoBox': 'an investment company', '走go': 'go',\n                  'followingorder': 'following order', 'pangolinminer': 'pangolin miner',\n                  'negosiation': 'negotiation', 'lexigographers': 'lexicographers', 'algorithom': 'algorithm',\n                  'unforgottable': 'unforgettable', 'wellsfargoemail': 'wellsfargo email',\n                  'daigonal': 'diagonal', 'Pangoro': 'cantankerous Pokemon', 'negotiotions': 'negotiations',\n                  'Swissgolden': 'Swiss golden', 'google4': 'google', 'Agoraki': 'Ago raki',\n                  'Garthago': 'Carthago', 'Stegosauri': 'stegosaurus', 'ergophobia': 'ergo phobia',\n                  'bigolive': 'big olive', 'bittergoat': 'bitter goat', 'naggots': 'faggots',\n                  'googology': 'online encyclopedia', 'algortihms': 'algorithms', 'bengolis': 'Bengalis',\n                  'fingols': 'Finnish people are supposedly descended from Mongols',\n                  'savethechildren': 'save thechildren',\n                  'stopings': 'stoping', 'stopsits': 'stop sits', 'stopsigns': 'stop signs',\n                  'Galastop': 'Galas top', 'pokestops': 'pokes tops', 'forcestop': 'forces top',\n                  'Hopstop': 'Hops top', 'stoppingexercises': 'stopping exercises', 'coinstop': 'coins top',\n                  'stoppef': 'stopped', 'workaway': 'work away', 'snazzyway': 'snazzy way',\n                  'Rewardingways': 'Rewarding ways', 'cloudways': 'cloud ways', 'Cloudways': 'Cloud ways',\n                  'Brainsway': 'Brains way', 'nesraway': 'nearaway',\n                  'AlwaysHired': 'Always Hired', 'expessway': 'expressway', 'Syncway': 'Sync way',\n                  'LeewayHertz': 'Blockchain Company', 'towayrds': 'towards', 'swayable': 'sway able',\n                  'Telloway': 'Tello way', 'palsmodium': 'plasmodium', 'Gobackmodi': 'Goback modi',\n                  'comodies': 'corodies', 'islamphobic': 'islam phobic', 'islamphobia': 'islam phobia',\n                  'citiesbetter': 'cities better', 'betterv3': 'better', 'betterDtu': 'better Dtu',\n                  'Babadook': 'a horror drama film', 'Ahemadabad': 'Ahmadabad', 'faidabad': 'Faizabad',\n                  'Amedabad': 'Ahmedabad', 'kabadii': 'kabaddi', 'badmothing': 'badmouthing',\n                  'badminaton': 'badminton', 'badtameezdil': 'badtameez dil', 'badeffects': 'bad effects',\n                  '∠bad': 'bad', 'ahemadabad': 'Ahmadabad', 'embaded': 'embased', 'Isdhanbad': 'Is dhanbad',\n                  'badgermoles': 'enormous, blind mammal', 'allhabad': 'Allahabad', 'ghazibad': 'ghazi bad',\n                  'htderabad': 'Hyderabad', 'Auragabad': 'Aurangabad', 'ahmedbad': 'Ahmedabad',\n                  'ahmdabad': 'Ahmadabad', 'alahabad': 'Allahabad',\n                  'Hydeabad': 'Hyderabad', 'Gyroglove': 'wearable technology', 'foodlovee': 'food lovee',\n                  'slovenised': 'slovenia', 'handgloves': 'hand gloves', 'lovestep': 'love step',\n                  'lovejihad': 'love jihad', 'RolloverBox': 'Rollover Box', 'stupidedt': 'stupidest',\n                  'toostupid': 'too stupid',\n                  'pakistanisbeautiful': 'pakistanis beautiful', 'ispakistan': 'is pakistan',\n                  'inpersonations': 'impersonations', 'medicalperson': 'medical person',\n                  'interpersonation': 'inter personation', 'workperson': 'work person',\n                  'personlich': 'person lich', 'persoenlich': 'person lich',\n                  'middleperson': 'middle person', 'personslized': 'personalized',\n                  'personifaction': 'personification', 'welcomemarriage': 'welcome marriage',\n                  'come2': 'come to', 'upcomedians': 'up comedians', 'overvcome': 'overcome',\n                  'talecome': 'tale come', 'cometitive': 'competitive', 'arencome': 'aren come',\n                  'achecomes': 'ache comes', '」come': 'come',\n                  'comepleted': 'completed', 'overcomeanxieties': 'overcome anxieties',\n                  'demigirl': 'demi girl', 'gridgirl': 'female models of the race', 'halfgirlfriend': 'half girlfriend',\n                  'girlriend': 'girlfriend', 'fitgirl': 'fit girl', 'girlfrnd': 'girlfriend', 'awrong': 'aw rong',\n                  'northcap': 'north cap', 'productionsupport': 'production support',\n                  'Designbold': 'Online Photo Editor Design Studio',\n                  'skyhold': 'sky hold', 'shuoldnt': 'shouldnt', 'anarold': 'Android', 'yaerold': 'year old',\n                  'soldiders': 'soldiers', 'indrold': 'Android', 'blindfoldedly': 'blindfolded',\n                  'overcold': 'over cold', 'Goldmont': 'microarchitecture in Intel', 'boldspot': 'bolds pot',\n                  'Rankholders': 'Rank holders', 'cooldrink': 'cool drink', 'beltholders': 'belt holders',\n                  'GoldenDict': 'open-source dictionary program', 'softskill': 'softs kill',\n                  'Cooldige': 'the 30th president of the United States',\n                  'newkiller': 'new killer', 'skillselect': 'skills elect', 'nonskilled': 'non skilled',\n                  'killyou': 'kill you', 'Skillport': 'Army e-Learning Program', 'unkilled': 'un killed',\n                  'killikng': 'killing', 'killograms': 'kilograms',\n                  'Worldkillers': 'World killers', 'reskilled': 'skilled',\n                  'killedshivaji': 'killed shivaji', 'honorkillings': 'honor killings',\n                  'skillclasses': 'skill classes', 'microskills': 'micros kills',\n                  'Skillselect': 'Skills elect', 'ratkill': 'rat kill',\n                  'pleasegive': 'please give', 'flashgive': 'flash give',\n                  'southerntelescope': 'southern telescope', 'westsouth': 'west south',\n                  'southAfricans': 'south Africans', 'Joboutlooks': 'Job outlooks', 'joboutlook': 'job outlook',\n                  'Outlook365': 'Outlook 365', 'Neulife': 'Neu life', 'qualifeid': 'qualified',\n                  'nullifed': 'nullified', 'lifeaffect': 'life affect', 'lifestly': 'lifestyle',\n                  'aristocracylifestyle': 'aristocracy lifestyle', 'antilife': 'anti life',\n                  'afterafterlife': 'after afterlife', 'lifestylye': 'lifestyle', 'prelife': 'pre life',\n                  'lifeute': 'life ute', 'liferature': 'literature',\n                  'securedlife': 'secured life', 'doublelife': 'double life', 'antireligion': 'anti religion',\n                  'coreligionist': 'co religionist', 'petrostates': 'petro states', 'otherstates': 'others tates',\n                  'spacewithout': 'space without', 'withoutyou': 'without you',\n                  'withoutregistered': 'without registered', 'weightwithout': 'weight without',\n                  'withoutcheck': 'without check', 'milkwithout': 'milk without',\n                  'Highschoold': 'High school', 'memoney': 'money', 'moneyof': 'mony of', 'Oneplus': 'OnePlus',\n                  'OnePlus': 'Chinese smartphone manufacturer', 'Beerus': 'the God of Destruction',\n                  'takeoverr': 'takeover', 'demonetizedd': 'demonetized', 'polyhouse': 'Polytunnel',\n                  'Elitmus': 'eLitmus', 'eLitmus': 'Indian company that helps companies in hiring employees',\n                  'becone': 'become', 'nestaway': 'nest away', 'takeoverrs': 'takeovers', 'Istop': 'I stop',\n                  'Austira': 'Australia', 'germeny': 'Germany', 'mansoon': 'man soon',\n                  'worldmax': 'wholesaler of drum parts',\n                  'ammusement': 'amusement', 'manyare': 'many are', 'supplymentary': 'supply mentary',\n                  'timesup': 'times up', 'homologus': 'homologous', 'uimovement': 'ui movement', 'spause': 'spouse',\n                  'aesexual': 'asexual', 'Iovercome': 'I overcome', 'developmeny': 'development',\n                  'hindusm': 'hinduism', 'sexpat': 'sex tourism', 'sunstop': 'sun stop', 'polyhouses': 'Polytunnel',\n                  'usefl': 'useful', 'Fundamantal': 'fundamental', 'environmentai': 'environmental',\n                  'Redmi': 'Xiaomi Mobile', 'Loy Machedo': ' Motivational Speaker ', 'unacademy': 'Unacademy',\n                  'Boruto': 'Naruto Next Generations', 'Upwork': 'Up work',\n                  'Unacademy': 'educational technology company',\n                  'HackerRank': 'Hacker Rank', 'upwork': 'up work', 'Chromecast': 'Chrome cast',\n                  'microservices': 'micro services', 'Undertale': 'video game', 'undergraduation': 'under graduation',\n                  'chapterwise': 'chapter wise', 'twinflame': 'twin flame', 'Hotstar': 'Hot star',\n                  'blockchains': 'blockchain',\n                  'darkweb': 'dark web', 'Microservices': 'Micro services', 'Nearbuy': 'Nearby',\n                  ' Padmaavat ': ' Padmavati ', ' padmavat ': ' Padmavati ', ' Padmaavati ': ' Padmavati ',\n                  ' Padmavat ': ' Padmavati ', ' internshala ': ' internship and online training platform in India ',\n                  'dream11': ' fantasy sports platform in India ', 'conciousnesss': 'consciousnesses',\n                  'Dream11': ' fantasy sports platform in India ', 'cointry': 'country', ' coinvest ': ' invest ',\n                  '23 andme': 'privately held personal genomics and biotechnology company in California',\n                  'Trumpism': 'philosophy and politics espoused by Donald Trump',\n                  'Trumpian': 'viewpoints of President Donald Trump', 'Trumpists': 'admirer of Donald Trump',\n                  'coincidents': 'coincidence', 'coinsized': 'coin sized', 'coincedences': 'coincidences',\n                  'cointries': 'countries', 'coinsidered': 'considered', 'coinfirm': 'confirm',\n                  'humilates':'humiliates', 'vicevice':'vice vice', 'politicak':'political', 'Sumaterans':'Sumatrans',\n                  'Kamikazis':'Kamikazes', 'unmoraled':'unmoral', 'eduacated':'educated', 'moraled':'morale',\n                  'Amharc':'Amarc', 'where Burkhas':'wear Burqas', 'Baloochistan':'Balochistan', 'durgahs':'durgans',\n                  'illigitmate':'illegitimate', 'hillum':'helium','treatens':'threatens','mutiliating':'mutilating',\n                  'speakingly':'speaking', 'pretex':'pretext', 'menstruateion':'menstruation', \n                  'genocidizing':'genociding', 'maratis':'Maratism','Parkistinian':'Pakistani', 'SPEICIAL':'SPECIAL',\n                  'REFERNECE':'REFERENCE', 'provocates':'provokes', 'FAMINAZIS':'FEMINAZIS', 'repugicans':'republicans',\n                  'tonogenesis':'tone', 'winor':'win', 'redicules':'ridiculous', 'Beluchistan':'Balochistan', \n                  'volime':'volume', 'namaj':'namaz', 'CONgressi':'Congress', 'Ashifa':'Asifa', 'queffing':'queefing',\n                  'montheistic':'nontheistic', 'Rajsthan':'Rajasthan', 'Rajsthanis':'Rajasthanis', 'specrum':'spectrum',\n                  'brophytes':'bryophytes', 'adhaar':'Adhara', 'slogun':'slogan', 'harassd':'harassed',\n                  'transness':'trans gender', 'Insdians':'Indians', 'Trampaphobia':'Trump aphobia', 'attrected':'attracted',\n                  'Yahtzees':'Yahtzee', 'thiests':'atheists', 'thrir':'their', 'extraterestrial':'extraterrestrial',\n                  'silghtest':'slightest', 'primarty':'primary','brlieve':'believe', 'fondels':'fondles',\n                  'loundly':'loudly', 'bootythongs':'booty thongs', 'understamding':'understanding', 'degenarate':'degenerate',\n                  'narsistic':'narcistic', 'innerskin':'inner skin','spectulated':'speculated', 'hippocratical':'Hippocratical',\n                  'itstead':'instead', 'parralels':'parallels', 'sloppers':'slippers'\n                  }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# Define functions to remove extra spaces, clean the number of text, rare words and misspellings\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n\ndef clean_number(text):\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    text = re.sub(r'(\\d+)(e)(\\d+)','\\g<1> \\g<3>', text)\n    \n    return text\n\ndef clean_rare_words(text):\n    for rare_word in rare_words_mapping:\n        if rare_word in text:\n            text = text.replace(rare_word, rare_words_mapping[rare_word])\n\n    return text\n\ndef clean_misspell(text):\n    for bad_word in mispell_dict:\n        if bad_word in text:\n            text = text.replace(bad_word, mispell_dict[bad_word])\n    return text\n\n# Create variables with all punctuations, define a function to add space before and after punctuation and a function to clean bad case words\nimport string\nregular_punct = list(string.punctuation)\nall_punct = list(set(regular_punct + extra_punct))\n# Do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text\n\ndef clean_bad_case_words(text):\n    for bad_word in bad_case_words:\n        if bad_word in text:\n            text = text.replace(bad_word, bad_case_words[bad_word])\n    return text\n\n# Apply all above defined functions to preprocess the text\ndef preprocess(text):\n    text = remove_space(text)\n    text = clean_number(text)\n    text = clean_rare_words(text)\n    text = clean_misspell(text)\n    text = spacing_punctuation(text)\n    text = clean_bad_case_words(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuild the vocab and oov\ndel(vocab,oov)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Apply the preprocess function to comment_text\ndf['comment_text'] = df['comment_text'].apply(lambda x: preprocess(x))\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Rebuild the vocab and oov\nvocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Modeling"},{"metadata":{},"cell_type":"markdown","source":"In this chapter the model will be built to make use of the preprocessed text. The underlying model is the Keras library with its different layers and packages. Before initiating a model of `Keras`, the data will be split into the original `train` and `test` data. Different samples can be tried out in the code depending on the CPU or GPU available. Also the text will be tokenized and an embedding matrix be created. Afterwards the `Sequential()` model of Keras will be applied with input, different hidden and output layers. 5.2 and 5.3 deal with the training of the data and testing to validate the result before predicting and creating the `submission.csv`."},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Model Definition with Keras"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"First, the `train` and `test` data is split into the original data and the target variable for the model is added as seen in the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# Select a downsampled dataset of 200.000 rows to commit faster\n#train = df.iloc[:200000,:]\n#test = df.iloc[200000:,:]\n\n# Select a greater sample of 500000 rows if needed\n# train = df.iloc[:500000,:]\n# test = df.iloc[500000:,:]\n\n# Select a greater sample of 700000 rows\ntrain = df.iloc[:700000,:]\ntest = df.iloc[700000:,:]\n\n# Select all of the data\n# train = df.iloc[:1804874,:]\n# test = df.iloc[1804874:,:]\n\n\n# Delete df\ndel(df)\n# Dump memory collection\ngc.collect()\n\n# Get the original data from the train dataset and define the number of rows\n#train_original = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\", nrows=200000)\ntrain_original = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\", nrows=500000)\n\n# Concatenate train and train_original\ntrain = pd.concat([train,train_original[['target']]],axis=1)\n\n# Delete train_original\ndel(train_original)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the .head() of train\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The head of the `train` data shows that there are still different values possible for the `target` variable. For better modeling, this will be converted into binary flags in the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the target to a binary flag\ntrain['target'] = np.where(train['target'] >= 0.5, True, False)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the .trail() of train\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the head of the `train` data again shows that the `target` now takes the values of True or False. In the next cell, the text is tokenized by first defining the variables X (`comment_text`) and Y (`target`) and the creating a tokenizer to fit on X."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Define variables X as comment_text and Y as target from the train dataset\nY = train['target']\nX = train['comment_text']\n\n# Create a text tokenizer.\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\nsequence = tokenizer.texts_to_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizer class has a method called fit_on_texts that creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat\", it will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1, so every word gets a unique integer value. Then, texts_to_sequences transforms each text to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the size of the vocab\nvocab_size = len(sequence) + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tokenizing, all comments need to have the same length which is illustrated in the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# All comments must be truncated or padded to be the same length.\nmax_seq_length = 100\n\n# Apply the function pad_sequences in order to transforms a list of sequences (lists of integers) into a 2D Numpy array of shape `\npad_seq = pad_sequences(sequence, maxlen=max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a last step to prepare for the `Keras` model, an embedding matrix is created in the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Create an embedding matrix\nnum_words_in_embedding = 0\nembedding_matrix = np.zeros((vocab_size,300))\n\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index.vocab:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector        \n        num_words_in_embedding += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following steps the prepared dataset will be utilized to initate a `Sequential()` model of `Keras`, add embeddings and all required layers before compiling the model and having a look at its summary."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Initiate a Keras Sequential() model\nmodel = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Add an embedding layer\nmodel.add(Embedding(vocab_size, 300, input_length = 100, weights = [embedding_matrix],trainable = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we add the CuDNNLSTM layer which is the base of the model use in this kernel. We choose `CuDNNLSTM()` over `LSTM()` since the competition allows the use of GPU processing, making the training of the model substantially faster. \n\nLSTM stands for **Long Short Term Memory** and it's a variant of Recurrent Neural Networks (RNN) that allows important information to be kept within the network for long periods of time. The motivation for using this type of variant in the Jigsaw competition is that in Natural Language Processing, anything larger than a trigram (a group of three consecutive written units, like letters, syllables or in this case, words) is considered to have long-term dependency, and sometimes the gap between the information we want to predict and the location from where we want to predicted is to big to be handled by a simple RNN.\n\nHere is the typical architecture of a LSTM node (borrowed from Nimesh Sinha in an article from Towards Data Science [link](https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47)\n\n![LSTM Architecture](https://cdn-images-1.medium.com/max/800/1*Niu_c_FhGtLuHjrStkB_4Q.png)\n\n\n\n\n\nThe three main components (which are numbered) of a LSTM node are:\n\n1. The forget gate that takes the new input and the information from the previous node to decide which information should be removed or kept, according to the output of the sigmoid function.\n\n2. The next step is to decide which of the new information should be updated in the cell or ignored according to the output of a sigmoid function. At the same time, all the possible values of the new input are created as a vector layer after being evaluated in the tanh function. The product between this two outputs is updated in the old memory.\n\n3. Then, a sigmoid layer decides which parts of the current cell state are going to be carried over as output of the node. After this is done, the current cell state is passed by a tanh function to generate all the possible values, and the product between the sigmoid gate and this tanh vector goes as and output of the node and as 'information memory'. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add layers\nmodel.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\nmodel.add(Convolution1D(64,7,padding='same'))\nmodel.add(GlobalAveragePooling1D())\n\n# Add hidden layers\nmodel.add(Dense(128))\nmodel.add(LeakyReLU())\n\n# Add hidden layers\nmodel.add(Dense(64,activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adding all hidden layers, finally the output layer can be added. Here, a `Dense` layer is chosen with the `sigmoid` activation function as seen in the next cell.\n\nThe sigmoid activation function is chosen to have the output of the neural network between the range of [0,1] which in this case, would be the toxicity score, being 0 the lowest and 1 the most toxic type of comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the output layer\nmodel.add(Dense(1,activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adding all required input, hidden and output layers, the model can be compiled. For this model `adam` is chosen as the optimizer, `binary_crossentropy` as the loss and `accuracy` as the metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last step of the modeling is summarizing the model which is done in the next cell. The summary shows all different layers, their output and parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize the model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Training"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"To train the model, first the data has to be split into `train` and `test` data. Then the model can be fit to the `train` split data and validated with the `test` split data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Create train-test split for validation\nfrom sklearn import model_selection\nx_train,x_test,y_train,y_test = train_test_split(pad_seq, Y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete X and Y\ndel (X,Y)\n# Dump memory collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model to the training split data and validate with the test split data\nhistory = model.fit(x_train,y_train,epochs = 5,batch_size=512,validation_data=(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Testing"},{"metadata":{},"cell_type":"markdown","source":"Now, the model can be evaluated on the test data of the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a variable X with comment_text of the test data\nX = test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Tokenize the test_sequence and apply the pad_sequences function\ntest_sequence = tokenizer.texts_to_sequences(X)\ntest_pad_seq = pad_sequences(test_sequence,maxlen = max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Prediction"},{"metadata":{},"cell_type":"markdown","source":"After evaluating the model in 5.3, the `test` data can finally be predicted to prepare for the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# Predict for test\nprediction = model.predict(test_pad_seq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Submission"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Based on the predicted `test` data, a submission dataframe is created in the next cell, followed by a check for the head of the submission to make sure, this step has worked out. Finally the submission will be read into a `submission.csv` that can then be submitted to the competition to determine the ranking on the leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a submission dataframe with the variables id and prediction\nsubmission = pd.DataFrame([test['id']]).T\nsubmission['prediction'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the head of the submission\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission.csv to be submitted to the competition\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"This notebook has shown a possible approach to solve the Jigsaw Unintended Bias in Toxicity Classification Challenge on Kaggle. Keras has been chosen as the most appropriate model in order to reach the best score on the public leaderboard. To summarize the main challenges during the problem-solving approach, one can say that that particularly the text preprocessing is an important and intense step to lower the problems of unintend bias, while the actual modeling with Keras is quite logic code-wise even though it is very intense computational-wise. "},{"metadata":{},"cell_type":"markdown","source":"## References"},{"metadata":{},"cell_type":"markdown","source":"This notebook was inspired by different kernels which will be named in the following:\n- https://www.kaggle.com/tarunpaparaju/jigsaw-competition-eda-and-modeling\n- https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw\n- https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2\n- https://www.kaggle.com/taindow/simple-cudnngru-python-keras\n- https://www.kaggle.com/gpreda/jigsaw-eda\n- https://www.kaggle.com/samarthsarin/ensemble-network-with-keras-and-embeddings\n\nOther references are:\n- Demidov, V. (2018). FastText crawl 300d 2M | Kaggle. Retrieved June 2, 2019, from https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m\n- Kaggle. (2019). Jigsaw Unintended Bias in Toxicity Classification | Kaggle. Retrieved May 25, 2019, from https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n- Sinha, N. (2018, February 20). Understanding LSTM and its quick Understanding LSTM and its quick implementation in keras for sentiment analysis. Retrieved May 26, 2019, from https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47\n- Vu, D. (2018). Generating WordClouds in Python (article) - DataCamp. Retrieved May 25, 2019, from https://www.datacamp.com/community/tutorials/wordcloud-python"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}