{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning Classifiers\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\n# Machine Learning Resamplers\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n# measure error\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"In this kernel I want to use different machine learning methods to build a model by:\n1. Modeling the training data with identity labels to predict identity labels.\n2. Predict identity labels that are found in the test set on the rest of the training data as well as the testing data.\n3. Train on the training data with its new labels. \n4. Predict the Testing data"},{"metadata":{},"cell_type":"markdown","source":"Before we start, I will import the training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain training and testing dataframes\ntrain_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\nprint(\"number of training rows:%i\" % len(train_df))\nprint(\"number of testing rows:%i \"% len(test_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Modeling the training data with identity labels to predict identity labels."},{"metadata":{},"cell_type":"markdown","source":"I want to subset the training set to train for each of the identity labels that are found in the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the training dataframe to only include rows with identity labels.\nidentityAnn_train_df = train_df.loc[train_df[\"identity_annotator_count\"]>0,:]\nprint(len(identityAnn_train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the identityAnn_train_df to only include the id, target, and comment column\n# as well as the columns that contain identities that are used in the \n# testing data\nidentitiesInTestSet=[\"male\",\"female\",\"homosexual_gay_or_lesbian\",\"christian\",\"jewish\",\"muslim\",\"black\",\"white\",\"psychiatric_or_mental_illness\"]\nidentityAnn_train_df = identityAnn_train_df.loc[:,[\"id\",\"comment_text\", \"target\"]+identitiesInTestSet]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I am going to import TFIDF Vectorizers which is used to first convert the `comments_text` column into a matrix of word counts and then transforms these counts by normalizing them based on the term frequency. This matrix can then be used as by the machine learning algorithm unlike the comments themselves.\n\nFor more information please visit \n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am now going to loop through the identities that will be found in the testing set to see which is the best model to predict these identities.\n1. I will then split the `identityAnn_train_df` into a training and testing set for cross validation using the`train_test_split` module\n2. Transform the comments into a count matrix to a normalized term-frequency representation\n3. Run a classifier on the transformed comments matrix\n4. Obtain the Training Accuract as well as the Testing Accuracy, Precision, and Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n# This dataframe should contain all the info about each model\nidentity_cross_val_df = pd.DataFrame(columns=[\"Classifier\",\"Identity\",\"Train_Acc\",\"Test_Acc\",\"Test_Prec\",\"Test_Recall\"])\nidentity_cross_val_frames=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Multinominal Naive Bayes to Model Identities\nI will test a Naive Bayes classifier for multinomial models with the `MulinomialNB` module"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif os.path.isfile('../input/beginner-modeling/identity_cross_val_df.csv'):\n    identity_cross_val_df = pd.read_csv('../input/beginner-modeling/identity_cross_val_df.csv')\nelse:\n    for identity_x in identitiesInTestSet:\n        print(\"Predicting %s...\" % identity_x)\n        # Split DataFrame for Cross Validation\n        X_identity_train, X_identity_test, y_identity_train, y_identity_test = \\\n            train_test_split(identityAnn_train_df[\"comment_text\"], \\\n                             identityAnn_train_df[identity_x], test_size = .10)\n\n        # In order to convert the coninuous values of the identity value to binary, as\n        #  naive bayes can accept only binary values (0 or 1) as the target values\n        # Here we choose above 0 as a cutoff as we want to classify identities even if\n        # only one of the people thought it matched that identity\n        y_identity_train_binary = np.array(y_identity_train > 0, dtype=np.float)\n        y_identity_test_binary = np.array(y_identity_test > 0, dtype=np.float)\n\n        # Fit the comments into a count matrix \n        #  and then into a normalized term-frequency representation\n        identity_tfvect = TfidfVectorizer().fit(X_identity_train)\n        # Then transform the comments based on the fit\n        X_identity_train_tf = identity_tfvect.transform(X_identity_train)\n        X_identity_test_tf = identity_tfvect.transform(X_identity_test)\n\n        # Fit a Naive Base classifier to the training set\n        identity_clf = MultinomialNB().fit(X_identity_train_tf, y_identity_train_binary)\n\n        # get values\n        train_acc = identity_clf.score(X_identity_train_tf, y_identity_train_binary)\n        test_acc = identity_clf.score(X_identity_test_tf, y_identity_test_binary)\n        identity_predicted = identity_clf.predict(X_identity_test_tf)\n        test_prec = precision_score(y_identity_test_binary, identity_predicted)\n        test_recall = recall_score(y_identity_test_binary, identity_predicted)\n\n        identity_result_df = pd.DataFrame({\"Classifier\":[\"MultinomialNB\"],\n                                           \"Identity\":[identity_x],\n                                           \"Train_Acc\":[train_acc],\n                                           \"Test_Acc\":[test_acc],\n                                           \"Test_Prec\":[test_prec],\n                                           \"Test_Recall\":[test_recall]})\n        identity_cross_val_frames.append(identity_result_df)\n\n\n    identity_cross_val_df = pd.concat(identity_cross_val_frames,ignore_index=False)\n    # Now I am going to output the `cross_val_df` so I never have to run this code again.\n    identity_cross_val_df.to_csv('identity_cross_val_df.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_cross_val_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I want to try logistic regression, naive bayes, and an svm classifier with a linear kernel..."},{"metadata":{},"cell_type":"markdown","source":"# 2. Predict identity labels that are found in the test set on the rest of the training data as well as the testing data."},{"metadata":{},"cell_type":"markdown","source":"First we need to get all the training rows that are not yet labeled for identities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the training dataframe to only include rows with identity labels.\nnotIdentityAnn_train_df = train_df.loc[train_df[\"identity_annotator_count\"]==0,:].copy()\nprint(len(notIdentityAnn_train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also just want to make a copy of the test dataframe for safe measures"},{"metadata":{"trusted":true},"cell_type":"code","source":"MultiNB_test_df = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredicted_allIdent_train_file = \"../input/beginner-modeling/MultiNB_train.csv\"\npredicted_allIdent_test_file = \"../input/beginner-modeling/MultiNB_test.csv\"\n\n\nif os.path.isfile(predicted_allIdent_train_file) and \\\n    os.path.isfile(predicted_allIdent_test_file) and 1==0:\n    MultiNB_train_df = pd.read_csv(predicted_allIdent_train_file)\n    MultiNB_test_df = pd.read_csv(predicted_allIdent_test_file)\n    MultiNB_train_df.loc[:,\"target\"].astype(np.float64)\nelse:\n    for identity_x in identitiesInTestSet:\n        print(\"Predicting %s...\" % identity_x)\n        X_identity_train1 = identityAnn_train_df[\"comment_text\"].copy()\n        X_identity_train2 = notIdentityAnn_train_df[\"comment_text\"].copy()\n        X_identity_test = MultiNB_test_df[\"comment_text\"].copy()\n        y_identity_train = identityAnn_train_df[identity_x].copy()\n\n        # In order to convert the coninuous values of the identity value to binary, as\n        #  naive bayes can accept only binary values (0 or 1) as the target values\n        # Here we choose above 0 as a cutoff as we want to classify identities even if\n        # only one of the people thought it matched that identity\n        y_identity_train_binary = np.array(y_identity_train > 0, dtype=np.float)\n\n        # Fit the comments into a count matrix \n        #  and then into a normalized term-frequency representation\n        identity_tfvect = TfidfVectorizer().fit(X_identity_train1)\n        # Then transform the comments based on the fit\n        X_identity_train_tf1 = identity_tfvect.transform(X_identity_train1)\n        X_identity_train_tf2 = identity_tfvect.transform(X_identity_train2)\n        X_identity_test_tf = identity_tfvect.transform(X_identity_test)\n\n        # Fit a Naive Base classifier to the training set\n        identity_clf = MultinomialNB().fit(X_identity_train_tf1, y_identity_train_binary)\n\n        # get predicted values\n        train2_identity_predicted = identity_clf.predict(X_identity_train_tf2)\n        notIdentityAnn_train_df.loc[:,identity_x] = train2_identity_predicted\n        test_identity_predicted = identity_clf.predict(X_identity_test_tf)\n        MultiNB_test_df.loc[:,identity_x] = test_identity_predicted\n    MultiNB_train_df = pd.concat([identityAnn_train_df, notIdentityAnn_train_df], ignore_index=False)\n    print(\"DONE!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I am going to output these modified testing and training data frames so I never have to run this code again.\nMultiNB_train_df.to_csv('MultiNB_train.csv', index = False)\nMultiNB_test_df.to_csv('MultiNB_test.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets just take a peak at our new dataframes..."},{"metadata":{"trusted":true},"cell_type":"code","source":"MultiNB_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MultiNB_test_df.loc[MultiNB_test_df[\"muslim\"]>0,:].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Train on the training data with its new labels. "},{"metadata":{},"cell_type":"markdown","source":"Okay so now that everything has labels lets train on each label individually. I first need to categorize each of the rows into their respective bins. Note that I am fine with putting the same row into multiple bins. Then I will predict based on the comments of each bin individually. Next, I will average the predictions of the different models to get a final target score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of dataframe where the first dataframe\n# contains all the rows with no labels and then the rest\n# contain rows  with a specific label from the list\n# `identitiesInTestSet`\ndef binByIdentitiesinTestSet(dfWithAllLabels, identitiesInTestSet, verbose = True):\n    # calculate how many labels are given to each row\n    dfWithAllLabels.loc[:,\"numTestSetIdentLabels\"] = \\\n        dfWithAllLabels[identitiesInTestSet].sum(axis=1)\n    \n    # rows with no label\n    noTestSetIdentLabel_df = \\\n        dfWithAllLabels.loc[dfWithAllLabels[\"numTestSetIdentLabels\"]==0, :].copy()\n\n    # rows with labels\n    binnedTrainingDfs=[noTestSetIdentLabel_df]\n    for ident in identitiesInTestSet:\n        identInTestSet = dfWithAllLabels.loc[dfWithAllLabels[ident]>0,:]\n        binnedTrainingDfs.append(identInTestSet)\n\n    if verbose:\n        for i in range(0,len(binnedTrainingDfs)):\n            if i==0:\n                print(\"no label:%i\" % len(binnedTrainingDfs[i]))\n            else:\n                print(\"%s:%i\" % (identitiesInTestSet[i-1],len(binnedTrainingDfs[i])))\n    return(binnedTrainingDfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingRowsWithLabel = MultiNB_train_df[identitiesInTestSet].sum(axis=1) > 0\ntrainingRowsWithNoLabel = MultiNB_train_df[identitiesInTestSet].sum(axis=1) == 0\n\nprint(\"Number of Rows with a Labels Found in Test Df: %i\" % \\\n      sum(trainingRowsWithLabel))\n\nprint(\"Number of Rows with No Labels Found in Test Df: %i\" % \\\n      sum(trainingRowsWithNoLabel))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are a lot more rows that do not have a label found in the test set in them I am going to downsize their rows. This will just help run these tests a bit faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ntrainingRowsWithLabel_df = MultiNB_train_df.loc[trainingRowsWithLabel,:]\ntrainingRowsWithNoLabel_df = MultiNB_train_df.loc[trainingRowsWithNoLabel,:]\n\n\ndownsample_n = 90000\nrandom_rows = random.sample(range(0,len(trainingRowsWithNoLabel_df)), downsample_n)\ntrainingRowsWithNoLabel_df = trainingRowsWithNoLabel_df.iloc[random_rows,:]\n\ntrain_downSampled = pd.concat([trainingRowsWithLabel_df, trainingRowsWithNoLabel_df], \\\n                              ignore_index=False)\nprint(len(train_downSampled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I want to test a model on each of the different training sets now\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This dataframe should contain all the info about each model\ntarget_cross_val_df = pd.DataFrame(columns=[\"Classifier\",\"Identity\",\"Train_Acc\",\"Test_Acc\",\"Test_Prec\",\"Test_Recall\"])\ntarget_cross_val_frames=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultinominalNB on Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"good=False\nx=1\nwhile good==False:\n    print(\"Round \"+ str(x))\n    # Split DataFrame for Cross Validation\n    X_fullcv_train, X_fullcv_test, y_fullcv_train, y_fullcv_test = \\\n            train_test_split(train_downSampled.loc[:,['id',\"comment_text\",\"target\"]+identitiesInTestSet], \\\n                             train_downSampled[\"target\"], test_size = .10)\n    # Bin rows by labels\n    binnedTrainingCV_list = binByIdentitiesinTestSet(X_fullcv_train, identitiesInTestSet, False)\n    binnedTestingCV_list = binByIdentitiesinTestSet(X_fullcv_test, identitiesInTestSet, False)\n    \n    good=True\n    for train_df_x in binnedTrainingCV_list:\n        y_cv_train = train_df_x.loc[:,\"target\"].copy()\n        y_cv_train_binary = np.array(y_cv_train > 0, dtype=np.float)\n        toxicTrainCount = sum(y_cv_train_binary==1)\n        if toxicTrainCount < 6:\n            print(\"Count too low: \"+ str(toxicTrainCount))\n            good=False\n    x+=1\n# Bin rows by labels\nprint(\"test dataframe sizes:\")\nbinnedTrainingCV_list = binByIdentitiesinTestSet(X_fullcv_train, identitiesInTestSet)\nprint(\"\\ntraining dataframe sizes:\")\nbinnedTestingCV_list = binByIdentitiesinTestSet(X_fullcv_test, identitiesInTestSet)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(binnedTrainingCV_list)):\n    identity_x = \"No\"\n    if i > 0:\n        identity_x = identitiesInTestSet[i-1]\n    print(\"Predicting Rows with %s Identity Labels...\" % identity_x)\n    cur_train_df = binnedTrainingCV_list[i]\n    cur_test_df = binnedTestingCV_list[i]\n    X_cv_train = binnedTrainingCV_list[i].loc[:,\"comment_text\"].copy()\n    X_cv_test = binnedTestingCV_list[i].loc[:,\"comment_text\"].copy()\n    y_cv_train = binnedTrainingCV_list[i].loc[:,\"target\"].copy()\n    y_cv_test = binnedTestingCV_list[i].loc[:,\"target\"].copy()\n\n    # In order to convert the coninuous values of the identity value to binary, as\n    #  naive bayes can accept only binary values (0 or 1) as the target values\n    # Here we choose above 0 as a cutoff as we want to classify identities even if\n    # only one of the people thought it matched that identity\n    y_cv_train_binary = np.array(y_cv_train > 0, dtype=np.float)\n    y_cv_test_binary = np.array(y_cv_test > 0, dtype=np.float)\n    print(\"targets in training set:\")\n    print(Counter(y_cv_train_binary))\n    print(\"targets in testing set:\")\n    print(Counter(y_cv_test_binary))\n\n    \n    # Fit the comments into a count matrix \n    #  and then into a normalized term-frequency representation\n    identity_tfvect = TfidfVectorizer().fit(X_cv_train)\n    # Then transform the comments based on the fit\n    X_cv_train_tf = identity_tfvect.transform(X_cv_train)\n    X_cv_test_tf = identity_tfvect.transform(X_cv_test)\n\n    # over-sample the toxic comments using SMOTE\n    sm = SMOTE(random_state=46)\n    X_cv_train_tf_sm, y_cv_train_binary_sm = sm.fit_resample(X_cv_train_tf, y_cv_train_binary)\n    \n    print(\"targets in training set after SMOTE:\")\n    print(Counter(y_cv_train_binary_sm))\n    \n    # Fit a Naive Base classifier to the training set\n    identity_clf = MultinomialNB().fit(X_cv_train_tf_sm, y_cv_train_binary_sm)\n\n    # get values\n    train_acc = identity_clf.score(X_cv_train_tf_sm, y_cv_train_binary_sm)\n    test_acc = identity_clf.score(X_cv_test_tf, y_cv_test_binary)\n    identity_predicted = identity_clf.predict(X_cv_test_tf)\n    print(\"prediction counter\")\n    print(Counter(identity_predicted))\n    \n    test_prec = precision_score(y_cv_test_binary, identity_predicted)\n    test_recall = recall_score(y_cv_test_binary, identity_predicted)\n    prediction_row = (\"%s_Ident_Pred\" % identity_x)\n    predicted_df =  cur_test_df.copy()\n    predicted_df.loc[:,prediction_row] = identity_predicted\n    predicted_df = predicted_df.loc[:,[\"id\",prediction_row]]\n    X_fullcv_test = X_fullcv_test.merge(predicted_df, on=\"id\", how=\"outer\")\n\n    cv_result_df = pd.DataFrame({\"Classifier\":[\"MultinomialNB\"],\n                                       \"Identity\":[identity_x],\n                                       \"Train_Acc\":[train_acc],\n                                       \"Test_Acc\":[test_acc],\n                                       \"Test_Prec\":[test_prec],\n                                       \"Test_Recall\":[test_recall]})\n    target_cross_val_frames.append(cv_result_df)\n\n\ntarget_cross_val_df = pd.concat(target_cross_val_frames,ignore_index=False)\n# Now I am going to output the `target_cross_val_df` so I never have to run this code again.\ntarget_cross_val_df.to_csv('target_cross_val_df.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I want to merge any rows that fell into multiple categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=len(train_downSampled.loc[train_downSampled[\"target\"].notnull(),:])\ny=len(train_downSampled)\nprint(x/y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\npredCols = [x + \"_Ident_Pred\" for x in [\"No\"] + identitiesInTestSet]\nprint(predCols)\ncv_predict = X_fullcv_test[predCols].mean(axis=1,skipna=True)\nprint(\"prediction\")\nprint(Counter(cv_predict))\nprint(\"truth\")\nprint(Counter(y_fullcv_test))\ntest_prec = mean_squared_error(y_fullcv_test, cv_predict)\ntest_recall = recall_score(y_fullcv_test, cv_predict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Predict the Testing data"},{"metadata":{},"cell_type":"markdown","source":"I first need to categorize each of the rows into their respective bins. Note that I am fine with putting the same row into multiple bins."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df = MultiNB_train_df.copy()\nfinal_test_df = MultiNB_test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Rows in Each Training Bin\")\nbinnedTraining_list = binByIdentitiesinTestSet(final_train_df,identitiesInTestSet)\nprint(\"\\nNumber of Rows in Each Testing Bin\")\nbinnedTesting_list = binByIdentitiesinTestSet(final_test_df,identitiesInTestSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(binnedTraining_list)):\n    identity_x = \"No\"\n    if i > 0:\n        identity_x = identitiesInTestSet[i-1]\n    print(\"Predicting Rows with %s Identity Labels...\" % identity_x)\n    cur_train_df = binnedTraining_list[i]\n    cur_test_df = binnedTesting_list[i]\n    X_train = binnedTraining_list[i].loc[:,\"comment_text\"]\n    X_test = binnedTesting_list[i].loc[:,\"comment_text\"]\n    y_train = binnedTraining_list[i].loc[:,\"target\"]\n    print(\"Training Set:\")\n    print(Counter(y_train))\n    # In order to convert the coninuous values of the identity value to binary, as\n    #  naive bayes can accept only binary values (0 or 1) as the target values\n    # Here we choose above 0 as a cutoff as we want to classify identities even if\n    # only one of the people thought it matched that identity\n    y_train_binary = np.array(y_train > 0, dtype=np.float)\n\n    # Fit the comments into a count matrix \n    #  and then into a normalized term-frequency representation\n    identity_tfvect = TfidfVectorizer().fit(X_train)\n    # Then transform the comments based on the fit\n    X_train_tf = identity_tfvect.transform(X_train)\n    X_test_tf = identity_tfvect.transform(X_test)\n    \n    # over-sample the toxic comments using SMOTE\n    sm = SMOTE(random_state=42)\n    X_train_tf_sm, y_train_binary_sm = sm.fit_resample(X_train_tf, y_train_binary)\n    \n    print(\"targets in training set after SMOTE:\")\n    print(Counter(y_train_binary_sm))\n    \n    # Fit a Naive Base classifier to the training set\n    target_clf = MultinomialNB().fit(X_train_tf_sm, y_train_binary_sm)\n\n    # get values\n    target_prediction = target_clf.predict(X_test_tf)\n    print(\"Prediction Set:\")\n    print(Counter(target_prediction))\n    prediction_row = (\"%s_Ident_Pred\" % identity_x)\n    cur_test_df.loc[:,prediction_row] = target_prediction\n    cur_test_df = cur_test_df.loc[:,[\"id\",prediction_row]]\n    final_test_df = final_test_df.merge(cur_test_df, on=\"id\", how=\"outer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predCols = [x + \"_Ident_Pred\" for x in [\"No\"] + identitiesInTestSet]\nprint(predCols)\nfinal_test_df[\"prediction\"] = final_test_df[predCols].mean(axis=1)\n\nsubmission_df = final_test_df.loc[:,[\"id\",\"prediction\"]].copy()\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}