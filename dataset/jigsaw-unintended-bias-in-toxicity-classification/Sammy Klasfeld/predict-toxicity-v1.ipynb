{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Machine Learning Classifiers\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\n# Machine Learning Resamplers\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n# measure error\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set random state\nrand_state=42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"In this kernel I want to use different machine learning methods to build a model by:\n\n1. Modeling the training data with identity labels to predict identity labels and Predict identity labels that are found in the test set on the rest of the training data as well as the testing data.\n2. Train on the training data with its new labels and Predict the Testing data"},{"metadata":{},"cell_type":"markdown","source":"Before we start, I will import the training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain training and testing dataframes\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"number of training rows:%i\" % len(train_df))\nprint(\"number of testing rows:%i \"% len(test_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Train and Predict Identity Labels"},{"metadata":{},"cell_type":"markdown","source":"I want to subset the training set to train for each of the identity labels that are found in the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the training dataframe to only include rows with identity labels.\nidentityAnn_train_df = train_df.loc[train_df[\"identity_annotator_count\"]>0,:]\nprint(len(identityAnn_train_df))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the identityAnn_train_df to only include the id, target, and comment column\n# as well as the columns that contain identities that are used in the \n# testing data\nidentitiesInTestSet=[\"male\",\"female\",\"homosexual_gay_or_lesbian\",\"christian\",\"jewish\",\"muslim\",\"black\",\"white\",\"psychiatric_or_mental_illness\"]\nidentityAnn_train_df = identityAnn_train_df.loc[:,[\"id\",\"comment_text\", \"target\"]+identitiesInTestSet]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I am going to import TFIDF Vectorizers which is used to first convert the comments_text column into a matrix of word counts and then transforms these counts by normalizing them based on the term frequency. This matrix can then be used as by the machine learning algorithm unlike the comments themselves.\n\nFor more information please visit\n\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am now going to loop through the identities that will be found in the testing set, train them on rows that have identities and predict them on rows that do not as well as predict them on the test set."},{"metadata":{},"cell_type":"markdown","source":"First we need to get all the training rows that are not yet labeled for identities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset the training dataframe to only include rows with identity labels.\nnotIdentityAnn_train_df = train_df.loc[train_df[\"identity_annotator_count\"]==0,:].copy()\nprint(len(notIdentityAnn_train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also just want to make a copy of the test dataframe for safe measures"},{"metadata":{"trusted":true},"cell_type":"code","source":"MultiNB_test_df = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor identity_x in identitiesInTestSet:\n  print(\"Predicting %s...\" % identity_x)\n  X_identity_train1 = identityAnn_train_df[\"comment_text\"].copy()\n  X_identity_train2 = notIdentityAnn_train_df[\"comment_text\"].copy()\n  X_identity_test = MultiNB_test_df[\"comment_text\"].copy()\n  y_identity_train = identityAnn_train_df[identity_x].copy()\n\n  # In order to convert the coninuous values of the identity value to binary, as\n  #  naive bayes can accept only binary values (0 or 1) as the target values\n  # Here we choose above 0 as a cutoff as we want to classify identities even if\n  # only one of the people thought it matched that identity\n  y_identity_train_binary = np.array(y_identity_train > 0, dtype=np.float)\n\n  # Fit the comments into a count matrix \n  #  and then into a normalized term-frequency representation\n  identity_tfvect = TfidfVectorizer().fit(X_identity_train1)\n  # Then transform the comments based on the fit\n  X_identity_train_tf1 = identity_tfvect.transform(X_identity_train1)\n  X_identity_train_tf2 = identity_tfvect.transform(X_identity_train2)\n  X_identity_test_tf = identity_tfvect.transform(X_identity_test)\n  \n  # over-sample the toxic comments using SMOTE\n  sm = SMOTE(random_state=rand_state)\n  X_identity_train_tf1_sm, y_identity_train_binary_sm = \\\n      sm.fit_resample(X_identity_train_tf1, y_identity_train_binary)\n  \n  # Fit a Naive Base classifier to the training set\n  identity_clf = MultinomialNB().fit(X_identity_train_tf1_sm, y_identity_train_binary_sm)\n\n  # get predicted values\n  train2_identity_predicted = identity_clf.predict(X_identity_train_tf2)\n  notIdentityAnn_train_df.loc[:,identity_x] = train2_identity_predicted\n  test_identity_predicted = identity_clf.predict(X_identity_test_tf)\n  MultiNB_test_df.loc[:,identity_x] = test_identity_predicted\nMultiNB_train_df = pd.concat([identityAnn_train_df, notIdentityAnn_train_df], ignore_index=False)\nprint(\"DONE!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Train and Predict Toxic Labels"},{"metadata":{},"cell_type":"markdown","source":"Okay so now that everything has labels lets train on each label individually. I first need to categorize each of the rows into their respective bins. Note that I am fine with putting the same row into multiple bins. Then I will predict based on the comments of each bin individually. Next, I will average the predictions of the different models to get a final target score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of dataframe where the first dataframe\n# contains all the rows with no labels and then the rest\n# contain rows  with a specific label from the list\n# `identitiesInTestSet`\ndef binByIdentitiesinTestSet(dfWithAllLabels, identitiesInTestSet, verbose = True):\n    # calculate how many labels are given to each row\n    dfWithAllLabels.loc[:,\"numTestSetIdentLabels\"] = \\\n        dfWithAllLabels[identitiesInTestSet].sum(axis=1)\n    \n    # rows with no label\n    noTestSetIdentLabel_df = \\\n        dfWithAllLabels.loc[dfWithAllLabels[\"numTestSetIdentLabels\"]==0, :].copy()\n\n    # rows with labels\n    binnedTrainingDfs=[noTestSetIdentLabel_df]\n    for ident in identitiesInTestSet:\n        identInTestSet = dfWithAllLabels.loc[dfWithAllLabels[ident]>0,:]\n        binnedTrainingDfs.append(identInTestSet)\n\n    if verbose:\n        for i in range(0,len(binnedTrainingDfs)):\n            if i==0:\n                print(\"no label:%i\" % len(binnedTrainingDfs[i]))\n            else:\n                print(\"%s:%i\" % (identitiesInTestSet[i-1],len(binnedTrainingDfs[i])))\n    return(binnedTrainingDfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingRowsWithLabel = MultiNB_train_df[identitiesInTestSet].sum(axis=1) > 0\ntrainingRowsWithNoLabel = MultiNB_train_df[identitiesInTestSet].sum(axis=1) == 0\n\nprint(\"Number of Rows with a Labels Found in Test Df: %i\" % \\\n      sum(trainingRowsWithLabel))\n\nprint(\"Number of Rows with No Labels Found in Test Df: %i\" % \\\n      sum(trainingRowsWithNoLabel))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df = MultiNB_train_df.copy()\nfinal_test_df = MultiNB_test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Rows in Each Training Bin\")\nbinnedTraining_list = binByIdentitiesinTestSet(final_train_df,identitiesInTestSet)\nprint(\"\\nNumber of Rows in Each Testing Bin\")\nbinnedTesting_list = binByIdentitiesinTestSet(final_test_df,identitiesInTestSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(binnedTraining_list)):\n    identity_x = \"No\"\n    if i > 0:\n        identity_x = identitiesInTestSet[i-1]\n    print(\"Predicting Rows with %s Identity Labels...\" % identity_x)\n    cur_train_df = binnedTraining_list[i]\n    cur_test_df = binnedTesting_list[i]\n    X_train = binnedTraining_list[i].loc[:,\"comment_text\"]\n    X_test = binnedTesting_list[i].loc[:,\"comment_text\"]\n    y_train = binnedTraining_list[i].loc[:,\"target\"]\n    print(\"Training Set:\")\n    print(Counter(y_train))\n    # In order to convert the coninuous values of the identity value to binary, as\n    #  naive bayes can accept only binary values (0 or 1) as the target values\n    # Here we choose above 0 as a cutoff as we want to classify identities even if\n    # only one of the people thought it matched that identity\n    y_train_binary = np.array(y_train > 0, dtype=np.float)\n\n    # Fit the comments into a count matrix \n    #  and then into a normalized term-frequency representation\n    identity_tfvect = TfidfVectorizer().fit(X_train)\n    # Then transform the comments based on the fit\n    X_train_tf = identity_tfvect.transform(X_train)\n    X_test_tf = identity_tfvect.transform(X_test)\n    \n    # over-sample the toxic comments using SMOTE\n    sm = SMOTE(random_state=42)\n    X_train_tf_sm, y_train_binary_sm = sm.fit_resample(X_train_tf, y_train_binary)\n    \n    print(\"targets in training set after SMOTE:\")\n    print(Counter(y_train_binary_sm))\n    \n    # Fit a Naive Base classifier to the training set\n    target_clf = MultinomialNB().fit(X_train_tf_sm, y_train_binary_sm)\n\n    # get values\n    target_prediction = target_clf.predict(X_test_tf)\n    print(\"Prediction Set:\")\n    print(Counter(target_prediction))\n    prediction_row = (\"%s_Ident_Pred\" % identity_x)\n    cur_test_df.loc[:,prediction_row] = target_prediction\n    cur_test_df = cur_test_df.loc[:,[\"id\",prediction_row]]\n    final_test_df = final_test_df.merge(cur_test_df, on=\"id\", how=\"outer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predCols = [x + \"_Ident_Pred\" for x in [\"No\"] + identitiesInTestSet]\nprint(predCols)\nfinal_test_df[\"prediction\"] = final_test_df[predCols].mean(axis=1)\n\nsubmission_df = final_test_df.loc[:,[\"id\",\"prediction\"]].copy()\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}