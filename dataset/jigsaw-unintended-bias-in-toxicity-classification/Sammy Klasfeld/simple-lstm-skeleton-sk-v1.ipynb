{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n# Not Needed since pandas has np already loaded. just use pd.np.whatever you need numpy for\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.sparse import coo_matrix\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import spacy libraries\nfrom spacy.vocab import Vocab\nfrom spacy.tokenizer import Tokenizer\n\n# import gensim libraries\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n# import pytorch\nimport torch\nfrom torch.utils import data\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# sklearn metrics\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# other libraries\nimport time\n#from tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"Here we are building an Long Short Term Memory (LSTM) network, a type of recurrent neural networks that is well explained in the following websites: \n* https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n* https://adventuresinmachinelearning.com/keras-lstm-tutorial/\n\nI've been seeing LSTM pop up a lot in kaggle competitions so its good to become familiar with them."},{"metadata":{},"cell_type":"markdown","source":"# Universal Parameters\nFirst we set up the parameters to identify basic parts of the input data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# universal parameter settings\n\n# identity columns that are featured in the testing data\n# according to the data description of the competition\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# columns that describe the comment\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\n# column with text data that will need to be converted for processing\nTEXT_COLUMN = 'comment_text'\n\n# column we eventually need to predict\nTARGET_COLUMN = 'target'\n\nRANDOM_STATE=100\n\n# 0 = fastText Word Vector Model\n# 1 = GloVe Word Vector Model\nWORD_VEC = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"parameters for text processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# characters that we can ignore when tokenizating the TEXT_COLUMN\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data\n## training dataset\nOpen the training datasets into a pandas data frame. When we open it we will also add another column `bin_target` which contains the target in binary format (1 if >0.5 and 0 if not)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = (\n    pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n    .assign(bin_target=lambda x: x.target.apply(lambda x: 1 if x > 0.5 else 0))\n)\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the top 5 rows of our training dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"counts how many 1's and 0's are in the `bin_target` column"},{"metadata":{"trusted":true},"cell_type":"code","source":"(\n    train_df\n    .bin_target\n    .value_counts()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the training set it so large we want to shrink it to test our code without having to wait around forever. Therefore, we make a smaller version of `train_df` and named it `X_dev`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dev_pos = (\n    train_df\n    .query(\"bin_target == 1\")\n    .sample(30219, random_state=RANDOM_STATE, replace=False) #53219\n    .sort_values(\"id\")\n)\n\nX_dev_neg = (\n    train_df\n    .query(\"bin_target == 0\")\n    .sample(38243, random_state=RANDOM_STATE, replace=False) #488243\n    .sort_values(\"id\")\n)\n\nX_dev = X_dev_pos.append(X_dev_neg)\nX_dev.shape # 30219+38243 = 68462","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Types of items not in X_dev\nnumRowsInTrainDFnotXdev = train_df.shape[0] -68462\nnumNegRowsInTrainDFnotXdev = 1698436 - 38243\nnumPosRowsInTrainDFnotXdev = 106438 - 30219\nprint(\"total rows not in X_dev: %i\" % numRowsInTrainDFnotXdev)\nprint(\"total positive rows not in X_dev: %i\" % numPosRowsInTrainDFnotXdev)\nprint(\"total negative rows not in X_dev: %i\" % numNegRowsInTrainDFnotXdev)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get rows from `train_df` that are not found in `X_dev`\nX = train_df[~train_df.id.isin(X_dev.id.values.tolist())]\nprint(X.shape)\nX = (\n    X\n    .query(\"bin_target==1\")\n    .sample(76219,random_state=RANDOM_STATE, replace=False)\n    .append(\n        X\n        .query(\"bin_target==0\")\n        .sample(76219,random_state=RANDOM_STATE, replace=False)\n    )\n)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## testing dataframe\nOpen the testing datasets into a pandas data frame. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word matricies"},{"metadata":{},"cell_type":"markdown","source":"Next, we will load a word matrix which contain vectors for specific words. We could have custom made one but instead we downloaded two different ones. Since they are computationally expensive we can only use one at a time so let's just look at one."},{"metadata":{},"cell_type":"markdown","source":"* Our first word matrix (which I call the fastText model) is in word2vec format. This means that the first line contains the number of vectors and the dimension of the matrix. \n* The second model is in GloVe vector format so we can convert it to word2vec format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set $foo to 0 to use the fastText model\n# set $foo to 1 to use the GloVe word matrix\ndef get_word_model(foo):\n    if foo == 0:\n        fastText_model = KeyedVectors.load_word2vec_format(\n            \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n        binary=False\n        )\n        return(fastText_model)\n    elif foo==1:\n        # convert the glove twitter word vectors into \n        # word2vec format\n        glove2word2vec(\n            glove_input_file=\"../input/glove-global-vectors-for-word-representation/glove.twitter.27B.50d.txt\",\n            word2vec_output_file=\"../input/glove.twitter.27B.50d.vec\"\n        )\n        \n        glove_model = KeyedVectors.load_word2vec_format(\n            \"../input/glove.twitter.27B.50d.vec\",\n            binary=False\n        )\n        return(glove_model)\n    else:\n        print(\"ERROR!!!!! YOU MUST SET TO 0 OR 1!!!\")\n        return(None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"set the word model and then index each word starting at 2. The index of each word is not quantitative."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_model = get_word_model(WORD_VEC)\n\n# Make a dictionary with [word]->index \nglobal_word_dict = {key.lower():index for index, key in enumerate(word_model.vocab.keys(), start=2)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Processing"},{"metadata":{},"cell_type":"markdown","source":"## Get unique sequences for each comment"},{"metadata":{},"cell_type":"markdown","source":"Once the comments have been fit to the tokenizer we could get:\n* word_counts: A dictionary of words and their counts across all the comments.\n* word_docs: A dictionary of words and how many comments each appeared in.\n* word_index: A dictionary of words and their uniquely assigned integers.\n* document_count:An integer count of the total number of documents that were used to fit the Tokenizer."},{"metadata":{},"cell_type":"markdown","source":"Using the tokenizer, we translate the comments in the training and testing set respectively to lists of each word's word_index in each comment. For example if the comment was \"Hello World\" and the word index for \"Hello\" is 5 and \"World\" is 202 then we would translate the comment to [5,202]. In other words, we can now identify each comment by the order of the unique indexes."},{"metadata":{},"cell_type":"markdown","source":"Let us just note that the word_index that matches a word is arbitrary. If two words have word_index values that are relatively close that does NOT mean the words are closely related it just means they are different from each other. However, none of the words have the index `0` nor `1` since we started at `2`. A word_index of `1` means the word is outside the vocabulary. A word index of `0` can be used to pad comments to be longer if we need comments to be of equal length."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(vocab=Vocab(strings=list(word_model.vocab.keys())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return a (pytorch LongTensor) list\n# of word indexes corresponding to\n# each sentence. If the word is\n# not in the vocabulary (global_word_dict)\n# then it's index is 1.\ndef index_sentences(sentence_list, global_word_dict):\n    indexed_sentences =list(\n        map(\n            lambda x: torch.LongTensor([\n                global_word_dict[token.text] \n                if token.text in global_word_dict else 1 \n                for token in tokenizer(x.lower())\n            ]),\n            tqdm.tqdm(sentence_list)\n        )\n    )\n    return indexed_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch_X = index_sentences(X[TEXT_COLUMN].values, global_word_dict)\ntorch_Y = torch.FloatTensor(X.bin_target.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch_X_dev = index_sentences(X_dev[TEXT_COLUMN].values, global_word_dict)\ntorch_Y_dev = torch.FloatTensor(X_dev.bin_target.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM Model Pytorch"},{"metadata":{},"cell_type":"markdown","source":"## Set Up the Model Classes and DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the just the layer that does LSTM\n# LSTM is awesome and does magic and turns\n# numbers into more numbers\nclass CustomLSTMLayer(nn.Module):\n    def __init__(\n        self, \n        input_size=200, hidden_size=200,\n        num_layers=2, batch_size=256, \n        bidirectional=False, inner_dropout=0.25,\n        outer_droput = [0.25, 0.25]\n    ):\n        super(CustomLSTMLayer, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.bidirectional = bidirectional\n\n        self.lstm = nn.LSTM(\n            self.input_size, self.hidden_size, \n            self.num_layers, batch_first=True,\n            bidirectional=self.bidirectional, \n            dropout=inner_dropout\n        )\n        \n    def forward(self, input):\n        #seq_lengths = torch.zeros(input.shape(0), dtype=torch.long)\n        \n        #for i in range(batch_size):\n        #    for j in range(max_seq - 1, -1, -1):\n        #        if not torch.all(X[i, j] == 0):\n        #            seq_lengths[i] = j + 1\n        #           break\n        _, (ht,_) = self.lstm(input)\n        return ht[-1, :]\n    \n    def init_hidden_size(self):\n        cell_state = torch.zeros(\n            self.num_layers * (2 if self.bidirectional else 1),\n            self.batch_size,\n            self.hidden_size\n        )\n        \n        hidden_state = torch.zeros(\n            self.num_layers * (2 if self.bidirectional else 1),\n            self.batch_size,\n            self.hidden_size\n        )\n        \n        return (hidden_state, cell_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this layer is used to convert the words into numbers\nclass CustomEmbeddingLayer(nn.Module):\n    def __init__(\n        self, \n        vocab_size, embedding_size, \n        pretrained_embeddings=None, freeze=False\n    ):\n        super(CustomEmbeddingLayer, self).__init__()\n        \n        if pretrained_embeddings is None:\n            self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        else:\n            rows, cols = pretrained_embeddings.shape\n            self.embed = nn.Embedding(num_embeddings=rows, embedding_dim=cols, padding_idx=0)\n            self.embed.weight.data.copy_(pretrained_embeddings)\n    \n        self.embed.weight.requries_grad = not freeze\n        \n    def forward(self, input):\n        return self.embed(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is your final layer that converts the numbers to probailities\nclass CustomFullyConnected(nn.Module):\n    def __init__(self, hidden_size=200):\n        super(CustomFullyConnected, self).__init__()\n        \n        self.fc1 = nn.Linear(hidden_size, 20)\n        self.fc2 = nn.Linear(20, 10)\n        self.fc3 = nn.Linear(10, 2)\n        \n    def forward(self, input):\n        output = self.fc1(input)\n        ouput = F.relu(output)\n        output = self.fc2(output)\n        ouput = F.relu(output)\n        output = self.fc3(output)\n        ouput = F.relu(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the dataloader\nclass SentenceDataLoader(data.Dataset):\n    def __init__(self, train_data, train_labels):\n        super(SentenceDataLoader, self).__init__()\n        \n        self.X = train_data\n        self.Y = train_labels\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        return tuple([self.X[index], self.Y[index]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_sentences(batch):\n    max_batch_length = max(list(map(lambda x: x[0].size(0), batch)))\n    padded_sentences = torch.LongTensor(\n        list(\n            map(\n                lambda x: pd.np.pad(x[0].numpy(), (0, max_batch_length-x[0].size(0)), 'constant', constant_values=0),\n                batch\n            )\n        )\n    )\n    sentence_labels = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n    return (padded_sentences, sentence_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train The Model"},{"metadata":{},"cell_type":"markdown","source":"## Set the Global Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rate at which comments are dropped for training\n# too high can underfit\n# too low can overfit\nDROPOUT_RATE = 0.25\n\n# NUMBER OF EPOCHS\n# One Epoch is when an entire dataset is passed forward and backward\n# through the neural network once.\nEPOCHS = 30\n\n# dimensions of the output vectors of each LSTM cell.\n# Too high can overfit\n# Too low can underfit\n# The length of this vector reflects the number of\n# Bidirectional CuDNNLSTM layers there will be\nLSTM_HIDDEN_UNITS = 25\n\n\n# dimensions of the densely-connected NN layer cells.\n# The length of this vector reflects the number of\n# Dense layers there will be\nDENSE_HIDDEN_UNITS = 4 * LSTM_HIDDEN_UNITS\n\n# The size of the vocab the LSTM uses\nVOCAB_SIZE = len(global_word_dict)\n\n# The side of the word vectors\nEMBEDDING_SIZE = 50\n\n#How big the batch size should be\nBATCH_SIZE = 128\n\n# The learning Rate\nLEARNING_RATE = 0.01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run the LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.Sequential(\n    CustomEmbeddingLayer(\n        vocab_size=VOCAB_SIZE, \n        embedding_size=EMBEDDING_SIZE, \n        pretrained_embeddings=torch.FloatTensor(word_model.vectors) #find the correct code here\n    ),\n    CustomLSTMLayer(\n        input_size=EMBEDDING_SIZE, hidden_size=LSTM_HIDDEN_UNITS,\n        batch_size=BATCH_SIZE\n    ),\n    CustomFullyConnected(LSTM_HIDDEN_UNITS),\n)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = SentenceDataLoader(torch_X, torch_Y)\ntrain_data_loader = data.DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=pad_sentences,\n    shuffle=True\n)\n\nval_dataset = SentenceDataLoader(torch_X_dev, torch_Y_dev)\nval_data_loader = data.DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=pad_sentences,\n    shuffle=True\n)\n\n# Set up the optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Set up the loss\nce_loss = nn.BCEWithLogitsLoss()\n\nfor epoch in range(EPOCHS):\n    # Set the progress bar up\n    progress_bar = tqdm.tqdm(\n        enumerate(train_data_loader),\n        total=len(train_data_loader),\n    )\n    \n    #throw the model on the gpu\n    model = model.cuda()\n    \n    avg_epoch_loss = []\n    model.train()\n\n    for index, batch in progress_bar:\n        data_batch = batch[0]\n\n        data_labels = torch.zeros(batch[0].size(0), 2)\n        data_labels[range(batch[0].size(0)), batch[1].long()] = 1\n        \n        #Throw it on the gpu\n        data_batch = data_batch.cuda()\n        data_labels = data_labels.cuda()\n        \n        # Zero out the optimizer\n        optimizer.zero_grad()\n        \n        #predict batch\n        predicted = F.softmax(model(data_batch), dim=1)\n        \n        #Calculate the loss\n        loss = ce_loss(predicted, data_labels)\n        avg_epoch_loss.append(loss.item())\n        loss.backward()\n        \n        # Update the weights\n        optimizer.step()\n        \n        progress_bar.set_postfix(avg_loss=avg_epoch_loss[-1])\n    \n    model.eval()\n    predicted_proba = []\n    dev_targets = []\n    \n    for val_batch in val_data_loader:\n        val_data_batch = val_batch[0]\n        val_data_batch = val_data_batch.cuda()\n        \n        predicted = F.softmax(model(val_data_batch), dim=1)\n        predicted_proba.append(predicted[:,1])\n        dev_targets.append(val_batch[1])\n    \n    predicted_proba = torch.cat(predicted_proba, dim=0)\n    dev_targets = torch.cat(dev_targets)\n    predicted_labels = list(\n        map(\n            lambda x: 1 if x > 0.5 else 0,\n            predicted_proba\n            .cpu()\n            .float()\n            .detach()\n            .numpy()\n        )\n    )\n    \n    msg = f\"E[{epoch+1}] Train Loss: {pd.np.mean(avg_epoch_loss):.3f} \"\n    msg += f\"Dev Accuracy: {accuracy_score(dev_targets.long().numpy(), predicted_labels):.3f} \"\n    msg += f\"Dev F1: {f1_score(dev_targets.long().numpy(), predicted_labels):.3f}\"\n    print(msg)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}