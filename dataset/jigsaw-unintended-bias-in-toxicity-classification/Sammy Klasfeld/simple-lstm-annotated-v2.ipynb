{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import keras libraries\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n# import pytorch\nimport torch\nfrom torch.utils import data\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# other libraries\nimport time\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom fastai.basics import *\nfrom fastai.basic_train import Learner\nfrom fastai.callbacks.general_sched import *\nimport gc\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"This code is based on the scripts from the following kernels: \n* [https://www.kaggle.com/thousandvoices/simple-lstm](https://www.kaggle.com/thousandvoices/simple-lstm)\n * Author: @thousandvoices\n * Version 7\n* [https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing](https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing)\n * Author: Benjamin Minixhofer\n * Version 6\n\nI just added my own notes and changes some of the code to make it more clear."},{"metadata":{},"cell_type":"markdown","source":"Here we are building an Long Short Term Memory (LSTM) network, a type of recurrent neural networks that is well explained in the following websites: \n* https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n* https://adventuresinmachinelearning.com/keras-lstm-tutorial/\n\nI've been seeing LSTM pop up a lot in kaggle competitions so its good to become familiar with them."},{"metadata":{},"cell_type":"markdown","source":"# Universal Parameters\nFirst we set up the parameters to identify basic parts of the input data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# universal parameter settings\n\n# identity columns that are featured in the testing data\n# according to the data description of the competition\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# columns that describe the comment\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\n# column with text data that will need to be converted for processing\nTEXT_COLUMN = 'comment_text'\n\n# column we eventually need to predict\nTARGET_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"parameters for text processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# characters that we can ignore when tokenizating the TEXT_COLUMN\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"parameters for building neuronal network model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rate at which comments are dropped for training\n# too high can underfit\n# too low can overfit\nDROPOUT_RATE = 0.2\n\n# NUMBER OF EPOCHS\n# One Epoch is when an entire dataset is passed forward and backward\n# through the neural network once.\nEPOCHS = 2\n\n# dimensions of the output vectors of each LSTM cell.\n# Too high can overfit\n# Too low can underfit\n# The length of this vector reflects the number of\n# Bidirectional CuDNNLSTM layers there will be\nLSTM_UNITS = 128\n\n\n# dimensions of the densely-connected NN layer cells.\n# The length of this vector reflects the number of\n# Dense layers there will be\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data\nOpen the testing and training datasets into data frames"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Organize competition data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_weights = torch.from_numpy(train_df[TARGET_COLUMN].values[:,np.newaxis])\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    #train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n    train_df[column] = np.where(train_df[column] >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values[:,np.newaxis]\n#y_aux_train = train_df[AUX_COLUMNS].values\n#y_aux_train[:,1:] = np.where(y_aux_train[:,1:] >= .5,1,0)\nx_test = test_df[TEXT_COLUMN].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"make the target binary rather than continuous"},{"metadata":{},"cell_type":"markdown","source":"# Text Processing"},{"metadata":{},"cell_type":"markdown","source":"## Get unique sequences for each comment"},{"metadata":{},"cell_type":"markdown","source":"We base the keras token vocabulary on the comments in both the training and testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the comments have been fit to the tokenizer we could get:\n* word_counts: A dictionary of words and their counts across all the comments.\n* word_docs: A dictionary of words and how many comments each appeared in.\n* word_index: A dictionary of words and their uniquely assigned integers.\n* document_count:An integer count of the total number of documents that were used to fit the Tokenizer."},{"metadata":{},"cell_type":"markdown","source":"Let us just note that the word_index that matches a word is arbitrary. If two words have word_index values that are relatively close that does NOT mean the words are closely related it just means they are different from each other. However, none of the words have the index \"0\". A word_index of 0 means the word is outside the vocabulary and therefore can be used to pad comments to be longer if we need comments to be of equal length."},{"metadata":{},"cell_type":"markdown","source":"Using the tokenizer, we translate the comments in the training and testing set respectively to lists of each word's word_index in each comment. For example if the comment was \"Hello World\" and the word index for \"Hello\" is 5 and \"World\" is 202 then we would translate the comment to [5,202]. In other words, we can now identify each comment by the order of the unique indexes."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create matrix containing the integer vector for each comment with sequence bucketing"},{"metadata":{},"cell_type":"markdown","source":"In the original [Simple LSTM](https://www.kaggle.com/thousandvoices/simple-lstm) kernel, they created a matrix by making each comment a uniform length. if the comment was shorter than MAX_LEN than they would add 0's to the end of it. If the comment was longer than MAX_LEN, then they trimmed it. They set MAX_LEN to 220 as in the code below."},{"metadata":{},"cell_type":"markdown","source":"However, in the [Speed up your RNN with Sequence Bucketing](https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing) kernel, they point out that this is \"suboptimal because when iterating over the dataset in batches, there will be some batches where the length of all samples is smaller than `MAX_LEN`. So there will be tokens which are zero everywhere in the batch but are still processed by the RNN. Therefore they optimize a process they call \"sequence bucketing\"."},{"metadata":{},"cell_type":"markdown","source":"Two ideas came from this analysis.\n1. Independent of the batch size, the 95th percentile of sequence lengths is about 163. We could use this number as a static pad, but we would still be losing information from the comments that are longer than 163 words.\n2. They found method 3 to be more elegant and more or equal to the other three methods of sequence bucketing that was tried. Therefore, I am also going to do method 3 which they titled \"Default Tensor Dataset with custom collate_fn\""},{"metadata":{},"cell_type":"markdown","source":"### Method 3: Default TensorDataset with custom collate_fn"},{"metadata":{},"cell_type":"markdown","source":"Based on @Benjamin_Minixhofer's analysis he found that when he used bin size of about 512 the maximum lengths of the batches were mostly the around ~190 words (only a little bit higher than the 95th percentile of sequence lengths which makes sense) and there were relatively few outliers. Therefore, he uses this batch size."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we put the target values in pytorch format"},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train_torch = torch.from_numpy(y_train).float()\n#y_aux_train_torch = torch.from_numpy(y_aux_train).float()\n#y_train_torch = torch.cat([y_train_torch.unsqueeze(1),y_aux_train_torch],1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_torch = torch.from_numpy(y_train)\ny_train_torch = y_train_torch.float()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we get a list of the lengths of all the comments and put them into pytorch format"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"collect the length of the longest comment.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = train_lengths.max() # length of longest comment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" pad the sequences so that they are all as long as the longest comments (add 0's to the end of comments that are shorter than the longest one)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\nprint(\"x_train_padded size:\")\nprint(x_train_padded.shape)\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))\nprint(\"x_test_padded size:\")\nprint(x_test_padded.shape)\n# save the space within RAM\ndel x_train, x_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we create a python object called `SequenceBucketCollator` which is used to create a matrix for each batch with the sequences padded based on the longest comment in each of the batches."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following object calls a `batch` which is a \n# TensorDataset that contains two or three items:\n# 1. Mandatory - a torch object with a matrix that has rows containing \n#    word incides for each comment (eg. x_train_padded)\n# 2. Mandatory - a torch object that contains a list of the lengths of \n#    each of these comments in the same order as the matrix\n#   (eg. train_lengths)\n# 3. Optional- a torch object that contins a list of the \n# target values for each comment (eg. y_train_torch)\nclass SequenceBucketCollator():\n    # initalizing features\n    # choose_length - function to choose uniform length of each comment\n    # sequence_index - index in Tensor Dataset where a torch object with \n    # a matrix that has rows containing word incides for each comment \n    # is located.\n    # length_index - index in Tensor Dataset where a list of the lengths of \n    # each of these comments in the same order as the matrix\n    # is located.\n    # label_index - index in Tensor Dataset where a torch object that contins\n    # a list of the target values for each comment is located (Optional)\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None, weight_index = None):\n        self.choose_length = choose_length \n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.weight_index = weight_index\n        self.label_index = label_index\n    \n    # An example of batch is:\n    # data.TensorDataset(x_train_padded, train_lengths, y_train_torch)\n    def __call__(self, batch):\n        # make a list \n        # eg. [x_train_padded, train_lengths, y_train_torch]\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        # put the padded comment matrix in a variable `sequences`\n        sequences = batch[self.sequence_index]\n        \n        # put list of lengths of the comments in a variable `lengths`\n        lengths = batch[self.length_index]\n        \n        # set uniform length to set all the comments to\n        length = self.choose_length(lengths) \n        \n        # add 0's to the comments that are shorter than `length`\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        # reset the batch sequences\n        batch[self.sequence_index] = padded_sequences\n        \n        # if present, add target labels to the batch\n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i not in [self.label_index,self.weight_index]],[batch[self.label_index],batch[self.weight_index]]\n\n        return batch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we make a training TensorDataset which contains\n1. x_train_padded - a torch object with a matrix where each row represents a comment in the form of a sequence of numbers where each number represents a specific word. Each comment is as long as the longest comment (if was smaller than the longest comment it was passed with 0's to lengthen it)\n2. train_lengths -  a torch object that contains a list of the lengths of each of these comments in the same order as the matrix\n3. y_train_torch - a torch object that contins a list of the target values for each comment\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[TARGET_COLUMN] = train_df[TARGET_COLUMN].astype(np.bool8)\ntrain_df[IDENTITY_COLUMNS] = train_df[IDENTITY_COLUMNS].astype(np.bool8)\n# first we make all comments equal weights of 1\n# this makes the math easier later\nsample_weights = np.ones(len(x_train_padded), dtype=np.float32)\n# then we add weight to columns with identity labels\n# if a comment has more identity labels it gets more\n# weight\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1).values\n# if the comment is labeled a toxic, then we add\n# the amount of identity columns that are not\n# labeled\nsample_weights += np.abs(train_df[TARGET_COLUMN] * \\\n    (~train_df[IDENTITY_COLUMNS]).sum(axis=1).values)\nsample_weights += np.abs(train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1).values)\n\n# if the comment is NOT labeled as toxic then we add\n# 5 times the amount of identity columns\nsample_weights += np.abs(~train_df[TARGET_COLUMN]) * \\\n    train_df[IDENTITY_COLUMNS].sum(axis=1).values * 5\n# then we normalize the weights by dividing them all by the mean\nsample_weights /= sample_weights.mean()\nsample_weights = torch.from_numpy(sample_weights.values[:,np.newaxis]).float()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = data.TensorDataset(x_train_padded, train_lengths, y_train_torch, sample_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and a testing TensorDataset which contains\n1. x_test_padded - a torch object with a matrix where each row represents a comment in the form of a sequence of numbers where each number represents a specific word. Each comment is as long as the longest comment (if was smaller than the longest comment it was passed with 0's to lengthen it)\n2. test+lengths -  a torch object that contains a list of the lengths of each of these comments in the same order as the matrix\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = data.TensorDataset(x_test_padded, test_lengths)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note we also make a validation Tensor Dataset which is just 2 rows of the train_dataset. The validation dataset is only added so that the fast.ai DataBunch works as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset = data.Subset(train_dataset, indices=[0,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we load the datasets into data.DataLoader which splits them batches of size `batch_size` and resets each batch to no longer have each of the comments as long as the longest comment, but instead resets each batch to be as long as the longest comment in the batch. I am not sure why shuffle is true for the `train_loader` and not the rest, but comment below if you do."},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the SequenceBucketCollator objects\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2,\n                                        weight_index = 3)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\n# run the SequenceBucketCollator method to uniformly change \n# each of the comments in each batch to be the size of the \n# longest comment in each batch\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Embeddings Dictionaries"},{"metadata":{},"cell_type":"markdown","source":"We added two external word embeddings (also known as word vectors) files that were not given in the original data. According to the rules of the competition we are allowed \"External data, freely & publicly available, is allowed, including pre-trained models\". Word embeddings are tools for representing words that have similar representation. Each word is a vector and words that are different should have a greater distance between them and visa versa."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fastText_wordEmbedder_f = '../input/fasttextsubword/crawl-300d-2m-subword/crawl-300d-2M-subword.vec'\n#fastText_wordEmbedder_f='../input/fasttext-toxic/crawl-300d-2M.vec'\nfastText_wordEmbedder_f='../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nglove_wordEmbedder_f='../input/glove840b300dtxt/glove.840B.300d.txt'\nglove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a dictionary of words with their respective vectors (AKA the embedding_index) for each of the word Embedding files."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path,dim = 300):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, dim))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef create_embedding_index(file,header=False):\n    f = open(file,'r')\n    lines = []\n    if header:\n        line_vec = f.readlines()[1:]\n    else:\n        line_vec = f.readlines()\n    return dict(get_coefs(*line.strip().split(' ')) for line in line_vec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to kaggle, the fastText word embedder is a \n\n\"300-dimensional pretrained FastText English word vectors released by Facebook.\n\nThe ** first line** of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.\"\n\nTherefore, to build our embedding_index we need to skip the first line, keep the first word as the dictionary key, and the rest of the numbers as the values for the key."},{"metadata":{"trusted":true},"cell_type":"code","source":"fastText_embedding_index,unknown_words_ft = \\\n      build_matrix(tokenizer.word_index, fastText_wordEmbedder_f)\n    #create_embedding_index(fastText_wordEmbedder_f,True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the other hand the glove word embedder is also 300-dimensional but it does NOT have the first header line so we use the same code but don't skip that first line."},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_index,_ = build_matrix(tokenizer.word_index,glove_wordEmbedder_f,300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_twitter,_ = build_matrix(tokenizer.word_index,glove_twitter,200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"instead of writing 300 over and over to signify that my vectors are 300 dimensions I am going to make it a variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([fastText_embedding_index, glove_embedding_index,glove_twitter], axis=-1)\nembedding_matrix.shape\n\ndel glove_embedding_index\ndel fastText_embedding_index\ndel glove_twitter\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vec_size=300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Word Embedding Matricies Based on Unique Word Indecies"},{"metadata":{},"cell_type":"markdown","source":"As stated before, the vector of each word is 300-dimensional. Therefore we can create a matrix where each row represents a word in the vocabulary (word_index) and the values are based on these word embedding vectors. For example if the word_index of hello is \"3\" then index \"3\" would have the \"hello\" vector values given in the enbedding_index."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embedding_matrix(word_index, embedding_index, word_vec_size):\n    # toxinizer_vocab = all word indexes plus index 0\n    toxinizer_vocab = len(word_index) +1\n    embedding_matrix = np.zeros((toxinizer_vocab, word_vec_size))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfastText_embedding_matrix = get_embedding_matrix(tokenizer.word_index,fastText_embedding_index,word_vec_size)\n\nglove_embedding_matrix = get_embedding_matrix(tokenizer.word_index,glove_embedding_index,word_vec_size)\n#embedding_matrix = np.concatenate([fastText_embedding_matrix, glove_embedding_matrix], axis=-1)\n\n#del fastText_embedding_matrix\n#del glove_embedding_matrix\n#gc.collect()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To review, we now have a matrix which contains the comments in each row AND two embedding matricies which contain row vectors for each word in the training and testing sets. Next, we need to combine these two data structures in Keras Embedding Layer."},{"metadata":{},"cell_type":"markdown","source":"# LSTM Model"},{"metadata":{},"cell_type":"markdown","source":"## Weights for Model"},{"metadata":{},"cell_type":"markdown","source":"To avoid bias of comments that contain an identity as toxic we attempt to use weights to push toxicity classificaiton to not be based on identity labels"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Build Model"},{"metadata":{},"cell_type":"markdown","source":"make sure to seed everything to remove a reproducibility issue"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function to dropout random comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.rand(10)\nfor s in range(3):\n    if s != 1:\n        a = a.unsqueeze(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_avg(tensor,dim):\n    weight_array = np.arange(1,tensor.shape[dim] + 1)\n    \n    weight_array = weight_array / np.sum(weight_array)\n    weight_array = torch.from_numpy(weight_array).float().cuda()\n    for a in range(len(tensor.shape)):\n        if a != dim:\n            weight_array = weight_array.unsqueeze(a)\n    weighted_tensor = tensor * weight_array\n    return torch.sum(weighted_tensor,dim)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    # initializing parameters:\n    ## embedding matrix - 2D matrix containing a unique vectors in each row \n    ## that corresponds to words based on each word indexes in a specific \n    ## vocabulary\n    ## num_aux_targets - number of AUX columns in training set\n    ## drouput_rate - rate at which input layer drops out comments\n    ## lstm_units - dimension of the lstm outputs\n    ## dense_hidden_units  - dimension of the dense-layer outputs\n    def __init__(self, embedding_matrix, dropout_rate,\n                lstm_units, dense_hidden_units):\n        super(NeuralNet, self).__init__()\n        \n        vocab_size = embedding_matrix.shape[0]\n        embed_size = embedding_matrix.shape[1]\n        \n        # Create a table using nn.Embedding shaped by the size of the \n        ## vocabulary (vocab_size) by the size of the word vectors \n        ## (embed_size)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # set the embedding.weight based on the embedding matrix that\n        # was created using the word em\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,\n                                                          dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(dropout_rate)\n        \n        \n        #self.lstm1 = nn.LSTM(embed_size, lstm_units, bidirectional=True, batch_first=True)\n        #self.lstm1 = BNLSTM(embed_size, lstm_units, bidirectional=True, batch_first=True)\n        #self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, bidirectional=True, batch_first=True,dropout=.5)\n        #self.lstm2 = BNLSTM(lstm_units * 2, lstm_units, bidirectional=True, batch_first=True)\n        self.lstm1 = nn.LSTM(embed_size,lstm_units,bidirectional=True,batch_first=True,num_layers=2)\n        self.linear1 = nn.Linear(dense_hidden_units, dense_hidden_units)\n        self.linear2 = nn.Linear(dense_hidden_units, dense_hidden_units)\n        #self.linear2 = nn.Sequential(nn.Linear(dense_hidden_units, int(dense_hidden_units / 2)),nn.BatchNorm1d(int(dense_hidden_units / 2)),nn.ReLU(),nn.Linear(int(dense_hidden_units / 2), dense_hidden_units))\n        \n        self.dropout = nn.Dropout(.2)\n        self.linear_out = nn.Linear(dense_hidden_units, 1)\n        self.bn = nn.BatchNorm1d(dense_hidden_units)\n        #self.linear_aux_out = nn.Linear(dense_hidden_units, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm2, _ = self.lstm1(h_embedding)\n        #h_lstm2, _ = self.lstm2(h_lstm1)\n        # see what happens if we add skip connection here\n        #h_lstm2 = h_lstm2 + h_lstm1\n        # global average pooling\n        avg_pool_1 = torch.mean(h_lstm2, 1)\n        #avg_pool_2 = weighted_avg(h_lstm2,1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool_1), 1)\n        h_conc_linear1  = F.relu(self.bn(self.linear1(h_conc)))\n        h_conc_linear2  = self.dropout(F.relu(self.bn(self.linear2(h_conc))))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        #aux_result = self.linear_aux_out(hidden)\n        #out = torch.cat([result, aux_result], 1)\n        #print(out.dtype)\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets, weights):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=weights.float())(data[:,:1].float(),targets[:,:1].float())\n    #bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,2:].float(),targets[:,1:].float())\n    #return (bce_loss_1 ) + bce_loss_2\n    return bce_loss_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(learn,test,output_dim,lr=0.002,\n                batch_size=512, n_epochs=5,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))).schedule_hp('wd',1e-2) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(3)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('fastText Model')\nseed_everything(1234 + 0)\nfastText_model = NeuralNet(embedding_matrix,dropout_rate=DROPOUT_RATE,lstm_units=LSTM_UNITS,dense_hidden_units=DENSE_HIDDEN_UNITS)\nfastText_learn = Learner(databunch, fastText_model, loss_func=custom_loss)\n\nfastText_test_preds = train_model(fastText_learn,test_dataset,output_dim=1,batch_size = batch_size,n_epochs = EPOCHS)    \n#all_test_preds.append(fastText_test_preds)\n\nfastText_model_submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': np.squeeze(fastText_test_preds[:,0])\n})\nfastText_model_submission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n'''\nall_test_preds = []\n\nprint('fastText Model')\nseed_everything(1234 + 0)\nfastText_model = NeuralNet(fastText_embedding_matrix,dropout_rate=DROPOUT_RATE,lstm_units=LSTM_UNITS,dense_hidden_units=DENSE_HIDDEN_UNITS)\nfastText_learn = Learner(databunch, fastText_model, loss_func=custom_loss)\n\nfastText_test_preds = train_model(fastText_learn,test_dataset,output_dim=1,batch_size = batch_size,n_epochs = EPOCHS)    \nall_test_preds.append(fastText_test_preds)\n\nfastText_model_submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': np.squeeze(fastText_test_preds[:,0])\n})\nfastText_model_submission.to_csv('fastText_submission.csv', index=False)\nfastText_model_submission.to_csv('submission.csv', index=False)\n\nprint('glove Model')\nseed_everything(1234 + 0)\nglove_model = NeuralNet(glove_embedding_matrix,dropout_rate=DROPOUT_RATE,lstm_units=LSTM_UNITS,dense_hidden_units=DENSE_HIDDEN_UNITS)\nglove_learn = Learner(databunch, glove_model, loss_func=custom_loss)\nglove_test_preds = train_model(glove_learn,test_dataset,output_dim=1,batch_size = batch_size,n_epochs = EPOCHS)    \nall_test_preds.append(glove_test_preds)\n\n\nglove_model_submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': glove_test_preds\n})\nglove_model_submission.to_csv('glove_submission.csv', index=False)\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsubmission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model (Static Model: Deprecated)"},{"metadata":{},"cell_type":"markdown","source":"The function below was used to model using static padding rather than binned padding..."},{"metadata":{},"cell_type":"markdown","source":"get weighted average predictions using both the models"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}