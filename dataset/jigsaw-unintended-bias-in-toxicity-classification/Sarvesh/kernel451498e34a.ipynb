{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport tensorflow as tf\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\nx_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n    \ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            #print(\"NOT found\")\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\nMAX_LEN = 220\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nembedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 100\nlstm_size = 256\nnum_input = 600\nnum_hidden = 1024\n\nseq_max_len = 220\ninput_dim = 600\n\nout_dim = 1             # output dimension\n\n# Parameters\nlearning_rate = 0.01    # The optimization initial learning rate\ntraining_steps = 1  # Total number of training steps\nbatch_size = 10         # batch size\ndisplay_freq = 1     # Frequency of displaying the training results\n#num_hidden_units = 10   # number of hidden units","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weight and bais wrappers\ndef weight_variable(shape):\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable('W',dtype=tf.float32,shape=shape,initializer=initer)\n\ndef bias_variable(shape):\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable('b',dtype=tf.float32,initializer=initial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RNN(x, weights, biases, n_hidden, seq_max_len, seq_len):\n\n    cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, sequence_length=seq_len, dtype=tf.float32)\n\n    # Hack to build the indexing and retrieve the right output.\n    batch_size = tf.shape(outputs)[0]\n    # Start indices for each sample\n    index = tf.range(0, batch_size) * seq_max_len + (seq_len - 1)\n    # Indexing\n    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n    out = tf.matmul(outputs, weights) + biases\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Placeholders for inputs(x), input sequence lengths (seqLen) and outputs(y)\nx = tf.placeholder(tf.float32, [None, seq_max_len, input_dim])\nseqLen = tf.placeholder(tf.int32, [None])\ny = tf.placeholder(tf.float32, [None, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create weight matrix initialized randomly from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden, out_dim])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[out_dim])\n\n# Network predictions\npred_out = RNN(x, W, b, num_hidden, seq_max_len, seqLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the loss function (i.e. mean-squared error loss) and optimizer\ncost = tf.reduce_mean(tf.square(pred_out - y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\ny_train = y_train.reshape((y_train.size,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(init)\n    print('----------Training---------')\n    seq_len_batch = np.full((100, ), 220)\n    for i in range(training_steps):\n        sum_mse = 0\n        if(i==1):\n            tem = 18048\n        else:\n            tem = 9000\n        for j in range(tem):\n        #for j in range(2):\n            x_batch = embedding_matrix[x_train[j*100:j*100+100],:]\n            y_batch = y_train[j*100:j*100+100]         \n            _, mse = sess.run([train_op, cost], feed_dict={x: x_batch,\n                                                           y: y_batch,\n                                                           seqLen: seq_len_batch})\n            sum_mse = mse + sum_mse\n        if i % display_freq == 0:\n            print('Step {0:<6}, MSE={1:.4f}'.format(i, sum_mse))\n            \n\n    y_test = np.full((97320,),0.0)\n    for k in range(973):\n        x_batch = embedding_matrix[x_test[k*100:k*100+100],:]\n        temp1 = sess.run([pred_out], feed_dict={x: x_batch,seqLen: seq_len_batch})\n        #print(temp.shape)\n        #print(temp1[0][1])\n        y_test[k*100:k*100+100] = temp1[0][:].ravel()\n        #print(y_test[k*100:k*100+100])\n    submission = pd.DataFrame.from_dict({'id': test_df.id,'prediction': y_test})    \n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}