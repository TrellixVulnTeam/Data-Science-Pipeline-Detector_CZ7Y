{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\ntqdm_notebook().pandas()\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nimport pickle as pkl\nimport re","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23faab71df25471bb8333c6bf34ba4c2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NUM_WORDS = 10000\nTOXICITY_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'\nEMBEDDINGS_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt',\n    '../input/paragram-300-sl999/paragram_300_sl999.txt'\n]\nEMBEDDINGS_DIMENSION = 300\n# All comments must be truncated or padded to be the same length.\nMAX_SEQUENCE_LENGTH = 250","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure all comment_text values are strings\ntrain[TEXT_COLUMN] = train[TEXT_COLUMN].astype(str) ","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stop_words(text, stopword_list):\n    return ' '.join([word for word in text.split() if word not in stopword_list])","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english_stopwords = set(stopwords.words('english'))\ntrain[TEXT_COLUMN] = train[TEXT_COLUMN].progress_apply(lambda x: remove_stop_words(x, english_stopwords))","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa49c600fce840bba3b4ae4af07fca25"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TEXT_COLUMN] = train[TEXT_COLUMN].progress_apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe1a7264f7845aeba3abad3405df1f5"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fixing_with_regex(text) -> str:\n    \"\"\"\n    Additional fixing of words.\n\n    :param text: text to clean\n    :return: cleaned text\n    \"\"\"\n\n    mis_connect_list = ['\\b(W|w)hat\\b', '\\b(W|w)hy\\b', '(H|h)ow\\b', '(W|w)hich\\b', '(W|w)here\\b', '(W|w)ill\\b']\n    mis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n\n    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n    text = mis_connect_re.sub(r\" \\1 \", text)\n    text = text.replace(\"What sApp\", ' WhatsApp ')\n\n    # Clean repeated letters.\n    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n    text = re.sub(r\"(-+|\\.+)\", \" \", text)\n\n    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f\\xad]', '', text)\n    text = re.sub(r'(\\d+)(e)(\\d+)', r'\\g<1> \\g<3>', text)  # is a dup from above cell...\n    text = re.sub(r\"(-+|\\.+)\\s?\", \"  \", text)\n    text = re.sub(\"\\s\\s+\", \" \", text)\n    text = re.sub(r'ᴵ+', '', text)\n\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n\n    text = re.sub(\n        r'(by|been|and|are|for|it|TV|already|justhow|some|had|is|will|would|should|shall|must|can|his|here|there|them|these|their|has|have|the|be|that|not|was|he|just|they|who)(how)',\n        '\\g<1> \\g<2>', text)\n\n    return text","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TEXT_COLUMN] = train[TEXT_COLUMN].progress_apply(lambda x: fixing_with_regex(x))","execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf6a9ade97f4cd4998d70a1c6f95fcd"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_number(text: str) -> str:\n    \"\"\"\n    Cleans numbers.\n\n    :param text: text to clean\n    :return: cleaned text\n    \"\"\"\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    text = re.sub(r'(\\d+),', '\\g<1>', text)\n    text = re.sub(r'(\\d+)(e)(\\d+)', '\\g<1> \\g<3>', text)\n\n    return text","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TEXT_COLUMN] = train[TEXT_COLUMN].progress_apply(lambda x: clean_number(x))","execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f2cd2829db430cbe9d1b8be37e0362"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Create a text tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_filter = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\n# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters=tokenizer_filter)\ntokenizer.fit_on_texts(train[TEXT_COLUMN])","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    embeddings_dict = {}\n    with open(path) as f:\n        for line in f:\n            values = line.strip().split(' ')\n            word = values[0]\n            coef = np.asarray(values[1:], dtype='float32')\n            if len(coef) == 300:\n                embeddings_dict[word] = coef\n    return embeddings_dict","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembeddings_dict = {\n    **load_embeddings(EMBEDDINGS_FILES[0]),\n    **load_embeddings(EMBEDDINGS_FILES[1]),\n    **load_embeddings(EMBEDDINGS_FILES[2])\n}","execution_count":44,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-42-05b053ac6e90>\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, embeddings_dict):\n    embeddings_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDINGS_DIMENSION))\n\n    for word, i in word_index.items():\n        embedding_vector = embeddings_dict.get(word)\n        if embedding_vector is not None:\n            embeddings_matrix[i] = embedding_vector\n    \n    return embeddings_matrix    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembeddings_matrix = build_matrix(tokenizer.word_index, embeddings_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(),  key=lambda kv: kv[1])[::-1]\n\n    return unknown_words","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_coverage(tokenizer.word_counts, embeddings_dict)","execution_count":46,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embeddings_dict' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-d74a184fab07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'embeddings_dict' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('embedding_matrix.pickle', 'wb') as f:\n    pkl.dump(embeddings_matrix, f)","execution_count":47,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embeddings_matrix' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-7bf4c88258d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding_matrix.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'embeddings_matrix' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('tokenizer.pickle', 'wb') as f:\n    pkl.dump(tokenizer, f)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('preprocessed_train_data.pickle', 'wb') as f:\n    pkl.dump(train, f)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}