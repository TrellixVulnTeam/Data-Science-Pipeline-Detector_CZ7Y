{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nset_seed(2411)\nSEED = 42\nimport psutil\nfrom multiprocessing import Pool\nimport multiprocessing\n\nnum_partitions = 10  # number of partitions to split dataframe\nnum_cores = psutil.cpu_count()  # number of cores on your machine\n\nprint('number of cores:', num_cores)\n\ndef df_parallelize_run(df, func):\n    \n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nglove_path = '../input/glove840b300dtxt/glove.840B.300d.txt'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# From Quora kaggle Comp's (latest one)\nimport re\n# remove space\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n\n# replace strange punctuations and raplace diacritics\nfrom unicodedata import category, name, normalize\n\ndef remove_diacritics(s):\n    return ''.join(c for c in normalize('NFKD', s.replace('√∏', 'o').replace('√ò', 'O').replace('‚Åª', '-').replace('‚Çã', '-'))\n                  if category(c) != 'Mn')\n\nspecial_punc_mappings = {\"‚Äî\": \"-\", \"‚Äì\": \"-\", \"_\": \"-\", '‚Äù': '\"', \"‚Ä≥\": '\"', '‚Äú': '\"', '‚Ä¢': '.', '‚àí': '-',\n                         \"‚Äô\": \"'\", \"‚Äò\": \"'\", \"¬¥\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ÿå':'','‚Äû':'',\n                         '‚Ä¶': ' ... ', '\\ufeff': ''}\ndef clean_special_punctuations(text):\n    for punc in special_punc_mappings:\n        if punc in text:\n            text = text.replace(punc, special_punc_mappings[punc])\n    text = remove_diacritics(text)\n    return text\n\n# clean numbers\ndef clean_number(text):\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text) # digits followed by a single alphabet...\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text) #1st, 2nd, 3rd, 4th...\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    return text\n\nimport string\nregular_punct = list(string.punctuation)\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '‚Ä¢',  '~', '@', '¬£',\n    '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',\n    '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù',\n    '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ',\n    '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº',\n    '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤',\n    '√®', '¬∏', '¬æ', '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª',\n    'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò',\n    '¬π', '‚â§', '‚Ä°', '‚àö', '¬´', '¬ª', '¬¥', '¬∫', '¬æ', '¬°', '¬ß', '¬£', '‚Ç§',\n    ':)', ': )', ':-)', '(:', '( :', '(-:', ':\\')',\n    ':D', ': D', ':-D', 'xD', 'x-D', 'XD', 'X-D',\n    '<3', ':*',\n    ';-)', ';)', ';-D', ';D', '(;',  '(-;',\n    ':-(', ': (', ':(', '\\'):', ')-:',\n    '-- :','(', ':\\'(', ':\"(\\'',]\n\ndef handle_emojis(text):\n    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', text)\n    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', text)\n    # Love -- <3, :*\n    text = re.sub(r'(<3|:\\*)', ' EMO_POS ', text)\n    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', text)\n    # Sad -- :-(, : (, :(, ):, )-:\n    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', text)\n    # Cry -- :,(, :'(, :\"(\n    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', text)\n    return text\n\ndef stop(text):\n    \n    from nltk.corpus import stopwords\n    \n    text = \" \".join([w.lower() for w in text.split()])\n    stop_words = stopwords.words('english')\n    \n    words = [w for w in text.split() if not w in stop_words]\n    return \" \".join(words)\n\nall_punct = list(set(regular_punct + extra_punct))\n# do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n\n# clean repeated letters\ndef clean_repeat_words(text):\n    \n    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n    text = re.sub(r\"plzz+\", \"please\", text)\n    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n    text = re.sub(r\"(-+|\\.+)\", \" \", text) #new haha #this adds a space token so we need to remove xtra spaces\n    return text\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text\n\ndef preprocess(text):\n    \"\"\"\n    preprocess text main steps\n    \"\"\"\n    text = remove_space(text)\n    text = clean_special_punctuations(text)\n    text = handle_emojis(text)\n    text = clean_number(text)\n    text = spacing_punctuation(text)\n    text = clean_repeat_words(text)\n    text = remove_space(text)\n    #text = stop(text)# if changing this, then chnage the dims \n    #(not to be done yet as its effecting the embeddings..,we might be\n    #loosing words)...\n    return text\n\nmispell_dict = {'üòâ':'wink','üòÇ':'joy','üòÄ':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndef correct_contraction(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\n\ndef text_clean_wrapper(df):\n    \n    df[\"comment_text\"] = df[\"comment_text\"].astype('str').transform(preprocess)\n    df['comment_text'] = df['comment_text'].transform(lambda x: correct_spelling(x, mispell_dict))\n    df['comment_text'] = df['comment_text'].transform(lambda x: correct_contraction(x, contraction_mapping))\n    \n    return df\n\n#fast!\ntrain = df_parallelize_run(train, text_clean_wrapper)\ntest  = df_parallelize_run(test, text_clean_wrapper)\n\nimport gc\ngc.enable()\ndel mispell_dict, all_punct, special_punc_mappings, regular_punct, extra_punct\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1,300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embeddings_index[word]\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gc\n\nmaxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\ny_train = train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\ndel tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index1 = load_embeddings()\nembedding_matrix1 = build_matrix(word_index, embeddings_index1)\ndel embeddings_index1\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index2 = load_embeddings(glove_path)\nembedding_matrix2 = build_matrix(word_index, embeddings_index2)\ndel embeddings_index2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = embedding_matrix1*0.6 + embedding_matrix2*0.4","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"attention"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.layers as L\nfrom keras.models import Model\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM+Attention+BN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LSTM_ATT_BN(verbose = False, compile = True):\n    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n    embedding_layer = L.Embedding(len(word_index) + 1,\n                                300,\n                                weights=[embedding_matrix],\n                                input_length=maxlen,\n                                trainable=False)\n    x = embedding_layer(sequence_input)\n    x = L.SpatialDropout1D(0.2)(x)\n    #x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n    x = L.Bidirectional(L.CuDNNGRU(64, return_sequences=True))(x)\n\n    #CuDNNGRU\n    att = Attention(maxlen)(x)\n    avg_pool1 = L.GlobalAveragePooling1D()(x)\n    max_pool1 = L.GlobalMaxPooling1D()(x)\n   \n    x = L.concatenate([att,avg_pool1, max_pool1])\n    x = L.Dense(128,activation='relu')(x)\n    x = L.BatchNormalization()(x)\n    preds = L.Dense(1, activation='sigmoid')(x)\n    \n    model = Model(sequence_input, preds)\n    if verbose:\n        model.summary()\n    if compile:\n        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # https://www.kaggle.com/yekenot/2dcnn-textclassifier\n# def model_cnn():\n#     filter_sizes = [1,2,3,5]\n#     num_filters = 36\n\n#     inp = L.Input(shape=(maxlen,))\n#     x = L.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix])(inp)\n#     x = L.Reshape((maxlen, 300, 1))(x)\n\n#     maxpool_pool = []\n#     for i in range(len(filter_sizes)):\n#         conv = L.Conv2D(num_filters, kernel_size=(filter_sizes[i], 300),\n#                                      kernel_initializer='he_normal', activation='elu')(x)\n#         maxpool_pool.append(L.MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\n#     z = L.Concatenate(axis=1)(maxpool_pool)   \n#     z = L.Flatten()(z)\n#     z = L.Dropout(0.1)(z)\n\n#     outp = L.Dense(1, activation=\"sigmoid\")(z)\n\n#     model = Model(inputs=inp, outputs=outp)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n#     return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTMs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def model_lstm_atten():\n#     inp = L.Input(shape=(maxlen,))\n#     x = L.Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n#     x = L.Bidirectional(L.CuDNNLSTM(128, return_sequences=True))(x)\n#     x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n#     x = Attention(maxlen)(x)\n#     x = L.Dense(64, activation=\"relu\")(x)\n#     x = L.Dense(1, activation=\"sigmoid\")(x)\n#     model = Model(inputs=inp, outputs=x)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def model_gru_srk_atten():\n#     inp = L.Input(shape=(maxlen,))\n#     x = L.Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix])(inp)\n#     x = L.Bidirectional(L.CuDNNGRU(64, return_sequences=True))(x)\n#     x = Attention(maxlen)(x) # New\n#     x = L.Dense(16, activation=\"relu\")(x)\n#     x = L.Dropout(0.1)(x)\n#     x = L.Dense(1, activation=\"sigmoid\")(x)\n#     model = Model(inputs=inp, outputs=x)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n#     return model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def model_lstm_du():\n#     inp = L.Input(shape=(maxlen,))\n#     x = L.Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix])(inp)\n#     x = L.Bidirectional(L.CuDNNGRU(64, return_sequences=True))(x)\n#     avg_pool = L.GlobalAveragePooling1D()(x)\n#     max_pool = L.GlobalMaxPooling1D()(x)\n#     conc = L.concatenate([avg_pool, max_pool])\n#     conc = L.Dense(64, activation=\"relu\")(conc)\n#     conc = L.Dropout(0.1)(conc)\n#     outp = L.Dense(1, activation=\"sigmoid\")(conc)\n    \n#     model = Model(inputs=inp, outputs=outp)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_gru_atten_3():\n    inp = L.Input(shape=(maxlen,))\n    x = L.Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = L.Bidirectional(L.CuDNNGRU(128, return_sequences=True))(x)\n    x = L.Bidirectional(L.CuDNNGRU(100, return_sequences=True))(x)\n    x = L.Bidirectional(L.CuDNNGRU(64, return_sequences=True))(x)\n    x = Attention(maxlen)(x)\n    x = L.Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLR"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# # https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n\n# from keras.callbacks import *\n# class CyclicLR(Callback):\n#     \"\"\"This callback implements a cyclical learning rate policy (CLR).\n#     The method cycles the learning rate between two boundaries with\n#     some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n#     The amplitude of the cycle can be scaled on a per-iteration or \n#     per-cycle basis.\n#     This class has three built-in policies, as put forth in the paper.\n#     \"triangular\":\n#         A basic triangular cycle w/ no amplitude scaling.\n#     \"triangular2\":\n#         A basic triangular cycle that scales initial amplitude by half each cycle.\n#     \"exp_range\":\n#         A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n#         cycle iteration.\n#     For more detail, please see paper.\n    \n#     # Example\n#         ```python\n#             clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n#                                 step_size=2000., mode='triangular')\n#             model.fit(X_train, Y_train, callbacks=[clr])\n#         ```\n    \n#     Class also supports custom scaling functions:\n#         ```python\n#             clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n#             clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n#                                 step_size=2000., scale_fn=clr_fn,\n#                                 scale_mode='cycle')\n#             model.fit(X_train, Y_train, callbacks=[clr])\n#         ```    \n#     # Arguments\n#         base_lr: initial learning rate which is the\n#             lower boundary in the cycle.\n#         max_lr: upper boundary in the cycle. Functionally,\n#             it defines the cycle amplitude (max_lr - base_lr).\n#             The lr at any cycle is the sum of base_lr\n#             and some scaling of the amplitude; therefore \n#             max_lr may not actually be reached depending on\n#             scaling function.\n#         step_size: number of training iterations per\n#             half cycle. Authors suggest setting step_size\n#             2-8 x training iterations in epoch.\n#         mode: one of {triangular, triangular2, exp_range}.\n#             Default 'triangular'.\n#             Values correspond to policies detailed above.\n#             If scale_fn is not None, this argument is ignored.\n#         gamma: constant in 'exp_range' scaling function:\n#             gamma**(cycle iterations)\n#         scale_fn: Custom scaling policy defined by a single\n#             argument lambda function, where \n#             0 <= scale_fn(x) <= 1 for all x >= 0.\n#             mode paramater is ignored \n#         scale_mode: {'cycle', 'iterations'}.\n#             Defines whether scale_fn is evaluated on \n#             cycle number or cycle iterations (training\n#             iterations since start of cycle). Default is 'cycle'.\n#     \"\"\"\n\n#     def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n#                  gamma=1., scale_fn=None, scale_mode='cycle'):\n#         super(CyclicLR, self).__init__()\n\n#         self.base_lr = base_lr\n#         self.max_lr = max_lr\n#         self.step_size = step_size\n#         self.mode = mode\n#         self.gamma = gamma\n#         if scale_fn == None:\n#             if self.mode == 'triangular':\n#                 self.scale_fn = lambda x: 1.\n#                 self.scale_mode = 'cycle'\n#             elif self.mode == 'triangular2':\n#                 self.scale_fn = lambda x: 1/(2.**(x-1))\n#                 self.scale_mode = 'cycle'\n#             elif self.mode == 'exp_range':\n#                 self.scale_fn = lambda x: gamma**(x)\n#                 self.scale_mode = 'iterations'\n#         else:\n#             self.scale_fn = scale_fn\n#             self.scale_mode = scale_mode\n#         self.clr_iterations = 0.\n#         self.trn_iterations = 0.\n#         self.history = {}\n\n#         self._reset()\n\n#     def _reset(self, new_base_lr=None, new_max_lr=None,\n#                new_step_size=None):\n#         \"\"\"Resets cycle iterations.\n#         Optional boundary/step size adjustment.\n#         \"\"\"\n#         if new_base_lr != None:\n#             self.base_lr = new_base_lr\n#         if new_max_lr != None:\n#             self.max_lr = new_max_lr\n#         if new_step_size != None:\n#             self.step_size = new_step_size\n#         self.clr_iterations = 0.\n        \n#     def clr(self):\n#         cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n#         x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n#         if self.scale_mode == 'cycle':\n#             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n#         else:\n#             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n#     def on_train_begin(self, logs={}):\n#         logs = logs or {}\n\n#         if self.clr_iterations == 0:\n#             K.set_value(self.model.optimizer.lr, self.base_lr)\n#         else:\n#             K.set_value(self.model.optimizer.lr, self.clr())        \n            \n#     def on_batch_end(self, epoch, logs=None):\n        \n#         logs = logs or {}\n#         self.trn_iterations += 1\n#         self.clr_iterations += 1\n\n#         self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n#         self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n#         for k, v in logs.items():\n#             self.history.setdefault(k, []).append(v)\n        \n#         K.set_value(self.model.optimizer.lr, self.clr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n#                step_size=300., mode='exp_range',\n#                gamma=0.99994)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.model_selection import KFold\nsplits = list(KFold(n_splits=5).split(X_train,y_train))\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\nimport numpy as np\nBATCH_SIZE = 2048\nNUM_EPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_preds = np.zeros((X_train.shape[0]))\n# test_preds = np.zeros((X_test.shape[0]))\n# for fold in [0,1]:\n#     K.clear_session()\n#     tr_ind, val_ind = splits[fold]\n# #     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n#     model = LSTM_ATT_BN()\n#     model.fit(X_train[tr_ind],\n#         y_train[tr_ind]>0.5,\n#         batch_size=BATCH_SIZE,\n#         epochs=NUM_EPOCHS,\n#         validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n#         callbacks = [es])\n\n#     oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n#     test_preds += model.predict(X_test)[:,0]\n# test_preds /= 2\n# outputs.append([test_preds, 'LSTM_ATT_BN'])\n# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros((X_train.shape[0]))\ntest_preds = np.zeros((X_test.shape[0]))\nfor fold in [0,1,2,3,4]:\n    K.clear_session()\n    tr_ind, val_ind = splits[fold]\n#     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    model = model_gru_atten_3()\n    model.fit(X_train[tr_ind],\n        y_train[tr_ind]>0.5,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n        callbacks = [es])\n\n    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n    test_preds += model.predict(X_test)[:,0]\ntest_preds /= 5\noutputs.append([test_preds, '3 GRU w/ atten'])\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_preds = np.zeros((X_train.shape[0]))\n# test_preds = np.zeros((X_test.shape[0]))\n# for fold in [0,1]:\n#     K.clear_session()\n#     tr_ind, val_ind = splits[fold]\n# #     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n#     model = model_cnn()\n#     model.fit(X_train[tr_ind],\n#         y_train[tr_ind]>0.5,\n#         batch_size=BATCH_SIZE,\n#         epochs=NUM_EPOCHS,\n#         validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n#         callbacks = [es])\n\n#     oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n#     test_preds += model.predict(X_test)[:,0]\n# test_preds /= 2\n# outputs.append([test_preds, 'model_cnn'])\n# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_preds = np.zeros((X_train.shape[0]))\n# test_preds = np.zeros((X_test.shape[0]))\n# for fold in [0,1]:\n#     K.clear_session()\n#     tr_ind, val_ind = splits[fold]\n# #     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n#     model = model_lstm_atten()\n#     model.fit(X_train[tr_ind],\n#         y_train[tr_ind]>0.5,\n#         batch_size=BATCH_SIZE,\n#         epochs=NUM_EPOCHS,\n#         validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n#         callbacks = [es])\n\n#     oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n#     test_preds += model.predict(X_test)[:,0]\n# test_preds /= 2\n# outputs.append([test_preds, 'model_lstm_atten'])\n# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_preds = np.zeros((X_train.shape[0]))\n# test_preds = np.zeros((X_test.shape[0]))\n# for fold in [0,1]:\n#     K.clear_session()\n#     tr_ind, val_ind = splits[fold]\n#     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n#     model = model_gru_srk_atten()\n#     model.fit(X_train[tr_ind],\n#         y_train[tr_ind]>0.5,\n#         batch_size=BATCH_SIZE,\n#         epochs=NUM_EPOCHS,\n#         validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n#         callbacks = [es,ckpt])\n\n#     oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n#     test_preds += model.predict(X_test)[:,0]\n# test_preds /= 2\n# outputs.append([test_preds, 'model_gru_srk_atten'])\n# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_preds = np.zeros((X_train.shape[0]))\n# test_preds = np.zeros((X_test.shape[0]))\n# for fold in [0,1]:\n#     K.clear_session()\n#     tr_ind, val_ind = splits[fold]\n#     ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n#     model = model_lstm_du()\n#     model.fit(X_train[tr_ind],\n#         y_train[tr_ind]>0.5,\n#         batch_size=BATCH_SIZE,\n#         epochs=NUM_EPOCHS,\n#         validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n#         callbacks = [es,ckpt])\n\n#     oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n#     test_preds += model.predict(X_test)[:,0]\n# test_preds /= 2\n# outputs.append([test_preds, 'model_lstm_du'])\n# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test= np.mean([outputs[i][0] for i in range(len(outputs))], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = pred_test\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()\n#%%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}