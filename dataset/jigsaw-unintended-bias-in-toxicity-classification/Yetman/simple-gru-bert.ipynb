{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json, string, re, random, pickle, gc, operator, time, sys\nfrom contextlib import contextmanager\nfrom collections import Counter\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader, Sampler\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nfrom keras_preprocessing.text import text_to_word_sequence\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def manual_seed(seed=420, cuda=False):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nmanual_seed(cuda=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(msg):\n    start = time.time()\n    print(f'[{msg}] start...')\n    yield\n    elapsed = time.time() - start\n    hours, rem = divmod(elapsed, 3600)\n    minutes, seconds = divmod(rem, 60)\n    elapsed_str = \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n    print(f'[{msg}] done in {elapsed_str}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_VOCAB_SIZE = 100000\nMAX_LEN = 256\nTARGET_COLUMNS = ['target'] #, 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = ['asian', 'atheist',\n       'bisexual', 'black', 'buddhist', 'christian', 'female',\n       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n       'muslim', 'other_disability', 'other_gender',\n       'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special_characters = {\n    \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", \"…\": \"...\", \"&\": \" and \", \"“\": '\"', \"”\": '\"',\n    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\", \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",\n    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \n    \"ᴀ\": \"a\", \"ʙ\": \"b\", \"ᴄ\": \"c\", \"ᴅ\": \"d\", \"ᴇ\": \"e\", \"ғ\": \"f\", \"ɢ\": \"g\", \"ʜ\": \"h\", \"ɪ\": \"i\", \n    \"ᴊ\": \"j\", \"ᴋ\": \"k\", \"ʟ\": \"l\", \"ᴍ\": \"m\", \"ɴ\": \"n\", \"ᴏ\": \"o\", \"ᴘ\": \"p\", \"ǫ\": \"q\", \"ʀ\": \"r\", \n    \"s\": \"s\", \"ᴛ\": \"t\", \"ᴜ\": \"u\", \"ᴠ\": \"v\", \"ᴡ\": \"w\", \"x\": \"x\", \"ʏ\": \"y\", \"ᴢ\": \"z\"\n}\ncontractions = json.load(open('../input/english-contractions/contractions.json', 'r'))\ncontractions = {key.lower():value.lower() for key, value in contractions.items()}\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\nspecial_characters_re = re.compile('({})'.format('|'.join(special_characters.keys())))\nspecial_characters_map = lambda match: special_characters[match.group(0)]\ncontractions_re = re.compile('({})'.format('|'.join(contractions.keys())))\ncontractions_map = lambda match: contractions[match.group(0)]\npunct_table = str.maketrans(punct, ' '*len(punct))\n\ndef tokenize(text):\n    text = text.lower()\n    text = special_characters_re.sub(special_characters_map, text)\n    text = contractions_re.sub(contractions_map, text)\n    text = text.translate(punct_table)\n    tokens = text_to_word_sequence(text, lower=False)\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('Process train.csv'):\n    print(\"Loading File...\")\n    train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n    print(\"Filling NaNs with 0s\")\n    train_df.fillna(0.0, inplace=True)\n    print(f'Training data size: {len(train_df)}')\n    print(\"Extracting Data...\")\n    train_text, train_target = train_df['comment_text'].values, torch.from_numpy(train_df[TARGET_COLUMNS].astype(np.float32).values)\n    #train_target = torch.where(train_target >= 0.5, torch.ones_like(train_target), torch.zeros_like(train_target))\n    train_has_identities = (train_df[identity_columns].values >= 0.5).any(axis=1)\n    train_is_toxic = (train_df['target'].values >= 0.5)\n    train_weights = (1 + ~train_is_toxic) * train_has_identities + train_is_toxic * ~train_has_identities + 1\n    train_weights = train_weights.astype(np.float32) / train_weights.mean()\n    train_weights = torch.from_numpy(train_weights)\n    del train_df, train_has_identities, train_is_toxic\n    gc.collect()\n    print(\"Tokenizing...\")\n    for index, comment in enumerate(tqdm(train_text)):\n        train_text[index] = tokenize(comment)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('Process test.csv'):\n    print(\"Loading File...\")\n    test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    print(f'Testing data size: {len(test_df)}')\n    print(\"Extracting Data...\")\n    test_ids, test_text = torch.from_numpy(test_df['id'].astype(np.int32).values), test_df['comment_text'].values\n    del test_df\n    gc.collect()\n    print(\"Tokenizing...\")\n    for index, comment in enumerate(tqdm(test_text)):\n        test_text[index] = tokenize(comment)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('Calculating Lengths'):\n    oversized = 0\n    print('Trimming')\n    for index, comment in enumerate(train_text):\n        if len(comment) > MAX_LEN:\n            train_text[index] = comment[:MAX_LEN]\n            oversized += 1\n    train_lengths = torch.tensor([len(comment) for comment in train_text], dtype=torch.int16)\n    for index, comment in enumerate(test_text):\n        if len(comment) > MAX_LEN:\n            test_text[index] = comment[:MAX_LEN]\n            oversized += 1\n    test_lengths = torch.tensor([len(comment) for comment in test_text], dtype=torch.int16)\n    print(f'{oversized} comment(s) ({oversized*100/(len(train_text)+len(test_text))}%) are longers than {MAX_LEN}')\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('Indexing tokens'):\n    vocab = Counter()\n    print('Counting training tokens...')\n    vocab.update(token for comment in tqdm(train_text) for token in comment)\n    print('Counting testing tokens...')\n    vocab.update(token for comment in tqdm(test_text) for token in comment)\n    print(f'Full vocabulary size is {len(vocab)} covering {sum(vocab.values())} tokens.')\n    print('Top 20 words:', vocab.most_common(20))\n    top_words, top_freq = zip(*vocab.most_common(min(len(vocab),MAX_VOCAB_SIZE)))\n    print(f'Top-{len(top_words)} covers {sum(top_freq)*100/sum(vocab.values())}%.')\n    del vocab\n    del top_freq\n    print(\"Building token index...\")\n    token2index = {token:index for index, token in enumerate(['<PAD>', '<UNK>'] + list(top_words))}\n    del top_words\n    gc.collect()\n    print(\"Indexing training data...\")\n    train_input = torch.zeros(len(train_text), MAX_LEN, dtype=torch.int32)\n    for index, comment in enumerate(tqdm(train_text)):\n        train_input[index,:len(comment)] = torch.tensor([token2index.get(token,1) for token in comment], dtype=torch.int32) \n    del train_text\n    gc.collect()\n    print(\"Indexing testing data...\")\n    test_input = torch.zeros(len(test_text), MAX_LEN, dtype=torch.int32)\n    for index, comment in enumerate(tqdm(test_text)):\n        test_input[index,:len(comment)] = torch.tensor([token2index.get(token,1) for token in comment], dtype=torch.int32) \n    del test_text\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('Resolving zero length elements'):\n    zero_mask = (train_lengths == 0)\n    print(f'found {zero_mask.sum().item()} zero length elements in training data')\n    train_lengths[zero_mask] = 1\n    train_input[zero_mask, 0] = 1\n    zero_mask = (test_lengths == 0)\n    print(f'found {zero_mask.sum().item()} zero length elements in testing data')\n    test_lengths[zero_mask] = 1\n    test_input[zero_mask, 0] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_files = [\n    {'file': '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', 'size': 300},\n    {'file': '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'size': 300}\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_size = sum(file['size'] for file in embedding_files)\nword_embeddings = torch.empty(len(token2index), vector_size, dtype=torch.float32)\nword_embeddings[0] = 0\nstart = 0\nfor file in embedding_files:\n    with timer(f'Loading {file[\"file\"]}'):\n        w2v = pickle.load(open(file[\"file\"], 'rb'))\n    size = file['size']\n    end = start + size\n    unk_gen = lambda: F.normalize(torch.randn(size), p=2, dim=0)\n    word_embeddings[1, start:end] = unk_gen()\n    not_found = []\n    for token, index in tqdm(token2index.items()):\n        if index < 2: continue\n        try:\n            word_embeddings[index, start:end] = torch.from_numpy(w2v[token])\n        except KeyError:\n            word_embeddings[index, start:end] = unk_gen()\n            not_found.append((index, token))\n    print(f'Could not find vectors for {len(not_found)} token(s) ({len(not_found)*100/(len(token2index)-2)}%)')\n    not_found.sort()\n    print('Top-10 not found words:\\n'+'\\n'.join(f'{str(index+1)}- {token}' for index, token in not_found[:10]))\n    del w2v\n    del not_found\n    gc.collect()\n    start = end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LengthSortedBatchSampler(Sampler):\n    def __init__(self, lengths, batch_size):\n        self.lengths = lengths\n        self.batch_size = batch_size\n    \n    def __iter__(self):\n        shuffled_indices = torch.randperm(self.lengths.size(0))\n        shuffled_lengths = self.lengths[shuffled_indices]\n        shuffled_lengths, sorted_indices = shuffled_lengths.sort()\n        shuffled_indices = shuffled_indices[sorted_indices]\n        batches = self.batch_size * torch.randperm(len(self))\n        for batch_start in batches:\n            yield shuffled_indices[batch_start:batch_start+self.batch_size]\n\n    def __len__(self):\n        return (len(self.lengths) + self.batch_size - 1) // self.batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\ntrain_dataset = TensorDataset(train_lengths, train_input, train_target, train_weights)\ntrain_dataloader = DataLoader(train_dataset, batch_sampler=LengthSortedBatchSampler(train_lengths, BATCH_SIZE))\ntest_dataset = TensorDataset(test_lengths, test_input)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, embeddings, hidden_size=128, layer_count=2, linear_layer_count=2, num_outputs=len(TARGET_COLUMNS)):\n        super(Model, self).__init__()\n        vocab_size, emb_dim = embeddings.size()\n        self.embed = nn.Embedding.from_pretrained(embeddings)\n        self.embed_dropout = SpatialDropout(0.2)\n        self.gru = nn.GRU(emb_dim, hidden_size, num_layers=layer_count, batch_first=True, bidirectional=True)\n        feats_size = 2*2*hidden_size # bidirectional * (mean+max) * hidden_size\n        self.linear_layers = nn.ModuleList([nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feats_size, feats_size),\n            nn.PReLU()\n        ) for _ in range(linear_layer_count)])\n        self.head = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feats_size, num_outputs)\n        )\n\n    def forward(self, sentences, lengths):\n        embeddings = self.embed(sentences)\n        embeddings = self.embed_dropout(embeddings)\n        packed_input = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, h = self.gru(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n        output_max, _ = output.max(dim=1)\n        output_mean = output.sum(dim=1)/(lengths.unsqueeze(1).float()+(1e-8))\n        feats = torch.cat([output_max, output_mean], dim=1)\n        for layer in self.linear_layers:\n            feats = feats + layer(feats)\n        return self.head(feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make 1st target more important\ntarget_weights = torch.ones(len(TARGET_COLUMNS), dtype=torch.float32)\ntarget_weights[0] = len(TARGET_COLUMNS)+1\ntarget_weights /= 2\ntarget_weights = target_weights.cuda()\n\ndef loss_fn(predicted, target, weights):\n    return ((F.binary_cross_entropy_with_logits(predicted, target, reduction='none') * weights.unsqueeze(1)) * target_weights).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_one_epoch(model, optimizer, epoch):\n    model.train()\n    interactive = is_interactive()\n    epoch_loss = 0\n    batch_it = tqdm(train_dataloader, desc=\"Epoch {}/{}\".format(epoch, num_epochs))\n    for batch_lengths, batch_sentences, batch_target, batch_weights in batch_it:\n        optimizer.zero_grad()\n        predicted = model(batch_sentences.cuda().long(), batch_lengths.cuda())\n        loss = loss_fn(predicted, batch_target.cuda(), batch_weights.cuda())\n        loss.backward()\n        optimizer.step()\n        batch_loss = loss.item()\n        if interactive:\n            batch_it.set_postfix({'batch loss': batch_loss})\n        epoch_loss += batch_loss * batch_lengths.size(0)\n    if interactive:\n        batch_it.close()\n    epoch_loss /= train_lengths.size(0)\n    return epoch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions(model):\n    model.eval()\n    test_size = test_ids.size(0)\n    predictions = torch.empty(test_size, dtype=torch.float)\n    index = 0\n    for batch_lengths, batch_sentences in tqdm(test_dataloader, desc=\"Test\"):\n        predicted = torch.sigmoid(model(batch_sentences.cuda().long(), batch_lengths.cuda()))\n        index_to = index + predicted.size(0)\n        predictions[index:index_to] = predicted[:,0].cpu().detach()\n        index = index_to\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract(history, key, default=None):\n        return [elem.get(key, default) for elem in history]\n\ndef view_history(history):\n    plt.plot(extract(history, 'epoch'), extract(history, 'loss'))\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()\n    print(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_models = 7\npredictions = torch.zeros(test_ids.size(0), dtype=torch.float)\ntotal_weight = 0\nfor index in range(1,num_models+1):\n    print(f'Start training model#{index}')\n    model = Model(embeddings=word_embeddings, hidden_size=128, layer_count=2, linear_layer_count=2).cuda()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.6**epoch)\n    num_epochs = 4\n    history = []\n    for epoch in range(1,num_epochs+1):\n        epoch_loss = fit_one_epoch(model, optimizer, epoch)\n        print(\"Epoch {}/{}: Average Loss={}\".format(epoch, num_epochs, epoch_loss), flush=True)\n        history.append({'epoch': epoch, 'loss': epoch_loss})\n        epoch_predictions = get_predictions(model)\n        epoch_weight = 2**epoch\n        predictions += epoch_weight * epoch_predictions\n        total_weight += epoch_weight\n        lr_scheduler.step()\n    view_history(history)\npredictions /= total_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del identity_columns, train_target, train_weights, test_ids, train_lengths, test_lengths, token2index, train_input, test_input, zero_mask\ndel word_embeddings, train_dataset, train_dataloader, test_dataset, test_dataloader, target_weights, model, optimizer, lr_scheduler\ndel history, epoch_predictions\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BERT**"},{"metadata":{"trusted":true},"cell_type":"code","source":"package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport warnings\nimport torch.utils.data\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig('../input/finetuned-bert-for-jigsaw-toxicity-classification/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"../input/finetuned-bert-for-jigsaw-toxicity-classification/bert_pytorch.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = 0.5*test_pred + 0.5*predictions.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': test_df['id'], 'prediction': final_predictions})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}