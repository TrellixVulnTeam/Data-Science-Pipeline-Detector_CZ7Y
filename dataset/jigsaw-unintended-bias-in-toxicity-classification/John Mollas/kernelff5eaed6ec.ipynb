{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Aristotaliens: Jigsaw Toxicity Competition**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"","execution_count":1,"outputs":[{"output_type":"stream","text":"--2019-05-28 07:34:33--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n--2019-05-28 07:34:33--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n--2019-05-28 07:34:34--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1520408563 (1.4G) [application/zip]\nSaving to: ‘glove.twitter.27B.zip’\n\nglove.twitter.27B.z 100%[===================>]   1.42G  12.8MB/s    in 90s     \n\n2019-05-28 07:36:04 (16.2 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\nimport os\nwith zipfile.ZipFile(\"./glove.twitter.27B.zip\",\"r\") as zip_ref:\n    zip_ref.extract(\"glove.twitter.27B.200d.txt\")\n    print(zip_ref.filelist)\nii = ['glove.twitter.27B.zip']\nfor i in ii:\n    os.remove(i)\nprint(os.listdir(\"./\"))\ndel zip_ref","execution_count":2,"outputs":[{"output_type":"stream","text":"[<ZipInfo filename='glove.twitter.27B.25d.txt' compress_type=deflate filemode='-r--r--r--' external_attr=0x1 file_size=257699726 compress_size=109884966>, <ZipInfo filename='glove.twitter.27B.50d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=510887943 compress_size=209216884>, <ZipInfo filename='glove.twitter.27B.100d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=1021669379 compress_size=405932911>, <ZipInfo filename='glove.twitter.27B.200d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=2057590469 compress_size=795373064>]\n['.ipynb_checkpoints', 'glove.twitter.27B.200d.txt', '__notebook_source__.ipynb']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_comments = train_df['comment_text'].values","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_ori = train_df['target'].values","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_ori2 = []\nfor i in y_train_ori:\n    if (i>=0.5):\n        y_train_ori2.append(1)\n    else:\n        y_train_ori2.append(0)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler \nprint('Original dataset shape %s' % Counter(y_train_ori2))\n","execution_count":7,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Original dataset shape Counter({0: 1660540, 1: 144334})\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X_train_comments.reshape(-1,1), y_train_ori2)\nprint('Resampled dataset shape %s' % Counter(y_res))","execution_count":8,"outputs":[{"output_type":"stream","text":"Resampled dataset shape Counter({0: 144334, 1: 144334})\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_comments= test_df[\"comment_text\"].values","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\nimport string\n\nfrom bs4 import BeautifulSoup\nfrom nltk import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\ndef clean(text):\n    tok = WordPunctTokenizer()\n    pat1 = '@[\\w\\-]+'  # for @\n    pat2 = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')  # for url\n    pat3 = '#[\\w\\-]+'  # for hashtag\n    pat4 = 'ï»¿'\n    pat5 = '[' + string.punctuation + ']'  # for punctuation\n    pat6 = '[^\\x00-\\x7f]'\n    soup = BeautifulSoup(text, 'html.parser')  # html decoding (\"@amp\")\n    souped = soup.get_text()\n    souped = re.sub(pat1, '', souped)  # remove @\n    souped = re.sub(pat2, '', souped)  # remove url\n    souped = re.sub(pat4, '', souped)  # remove strange symbols\n    souped = re.sub(pat5, '', souped)  # remove punctuation\n    souped = re.sub(pat3, '', souped)  # remove \"#\" symbol and keeps the words\n    clean = re.sub(pat6, '', souped)  # remove non-ascii characters\n    lower_case = clean.lower()  # convert to lowercase\n    words = tok.tokenize(lower_case)\n    return (\" \".join(words)).strip()\ndef my_clean(text,stops = False,stemming = False,minLength = 2):\n    text = str(text)\n    text = text.lower().split()\n    text = [w for w in text if len(w) >= minLength]\n\n    text = \" \".join(text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"don't\", \"do not \", text)\n    text = re.sub(r\"aren't\", \"are not \", text)\n    text = re.sub(r\"isn't\", \"is not \", text)\n    text = re.sub(r\"%\", \" percent \", text)\n    text = re.sub(r\"that's\", \"that is \", text)\n    text = re.sub(r\"doesn't\", \"dos not \", text)\n    text = re.sub(r\"he's\", \"he is \", text)\n    text = re.sub(r\"she's\", \"she is \", text)\n    text = re.sub(r\"it's\", \"it is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = text.lower().split()\n    text = [w for w in text if len(w) >= minLength]\n    if stemming and stops:\n        text = [word for word in text if word not in stopwords.words('english')]\n        wordnet_lemmatizer = WordNetLemmatizer()\n        englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n        text = [englishStemmer.stem(word) for word in text]\n        text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n        # text = [lancaster.stem(word) for word in text]\n        text = [word for word in text if word not in stopwords.words('english')]\n    elif stops:\n        text = [word for word in text if word not in stopwords.words('english')]\n    elif stemming:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n        text = [englishStemmer.stem(word) for word in text]\n        text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n    text = \" \".join(text)\n    return text","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_comments_pre = []\ncount = 0\nmax_length = -5\nimport time\nstart = time.time()\nfor t in X_res:\n    te = my_clean(t,False,True,2)\n    X_train_comments_pre.append(te)#You can add one more clean()\n    length = len(te.split(' '))\n    if length > max_length:\n        max_length = length\n    \n    if count % 10000 == 0:\n        print(count)\n        final = time.time()\n        total = final - start\n        print(total)\n    \n    count = count + 1","execution_count":11,"outputs":[{"output_type":"stream","text":"0\n1.9630110263824463\n10000\n12.00268268585205\n20000\n22.053771018981934\n30000\n31.983318090438843\n40000\n41.95389652252197\n50000\n52.111347913742065\n60000\n62.029842138290405\n70000\n72.91719913482666\n80000\n83.40393662452698\n90000\n93.25682139396667\n100000\n103.36679768562317\n110000\n113.40086722373962\n120000\n123.37895035743713\n130000\n133.3569257259369\n140000\n143.36048197746277\n150000\n154.31364560127258\n160000\n164.56612753868103\n170000\n174.62856316566467\n180000\n184.0469627380371\n190000\n193.61445140838623\n200000\n203.39085340499878\n210000\n212.8374080657959\n220000\n222.36709809303284\n230000\n232.5022006034851\n240000\n242.62908697128296\n250000\n252.00121355056763\n260000\n262.202219247818\n270000\n273.2985918521881\n280000\n283.0172529220581\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_res[1])\nprint(X_train_comments_pre[1])\nprint(max_length)","execution_count":12,"outputs":[{"output_type":"stream","text":"[\"I don't trust Nanos Polling.\"]\ndo not trust nano poll\n469\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test ,y_train ,y_test = train_test_split(X_train_comments_pre,y_res, random_state=826, test_size=0.33)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nprint(\"Opening Glove\")\nembeddings_index = dict()\nf = open('./glove.twitter.27B.200d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nvocabulary_size = 50000\ntokenizer = Tokenizer(num_words=vocabulary_size)\ntokenizer.fit_on_texts(X_train)\nembedding_matrix = np.zeros((50000, 200))\nfor word, index in tokenizer.word_index.items():\n    if index > 50000 - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","execution_count":14,"outputs":[{"output_type":"stream","text":"Opening Glove\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom keras.preprocessing.sequence import pad_sequences\n\nclass MyPadder(BaseEstimator,TransformerMixin):\n    def __init__(self,maxlen=5000):\n        self.maxlen = maxlen\n        self.max_index = None\n\n    def fit(self,X,y=None):\n        self.max_index = pad_sequences(X,maxlen=self.maxlen).max()\n        return self\n\n    def transform(self,X,y=None):\n        X = pad_sequences(X,maxlen=self.maxlen)\n        X[X>self.max_index] = 0\n        return X\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nclass MyTextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def fit(self,texts,y=None):\n        self.fit_on_texts(texts)\n        return self\n\n    def transform(self,texts,y=None):\n        return np.array(self.texts_to_sequences(texts))\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length=250\nsequencer = MyTextsToSequences(num_words=50000)\npadder = MyPadder(max_length)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequencer.fit(X_train)\nX_train_copy = sequencer.transform(X_train)\nX_test_copy = sequencer.transform(X_test)\npadder.fit(X_train_copy)\nX_train_copy = padder.transform(X_train_copy)\nX_test_copy = padder.transform(X_test_copy)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Input, Model\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom lime.lime_text import LimeTextExplainer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Dropout, concatenate\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nfrom collections import OrderedDict\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features=500)\nvec.fit(X_train)\nX_train_copy2 = vec.transform(X_train)\nX_test_copy2 = vec.transform(X_test)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_input = Input(shape=(max_length,), dtype='int32', name='main_input')\nglove_Embed = (Embedding(50000, 200, input_length=max_length, weights=[embedding_matrix], trainable=False))(main_input)\n\nx = Conv1D(64, 5, activation='relu')(glove_Embed)\nx = Conv1D(32, 5, activation='relu')(x)\nx = Dropout(rate=0.05)(x)\nx = MaxPooling1D(pool_size=4)(x)\nx = Dropout(rate=0.35)(x)\nx = LSTM(50)(x)\n\ny = Dense(300,activation='relu')(glove_Embed)\ny = Dropout(rate=0.05)(y)\ny = LSTM(300)(y)\ny = Dropout(rate=0.35)(y)\ny = Dense(100,activation='relu')(y)\ny = Dense(50,activation='relu')(y)\n\nmain_input2 = Input(shape=(len(vec.get_feature_names()),), dtype='float32', name='main_input2')\ne = Dense(300,activation='relu')(main_input2)\ne = Dense(1000,activation='relu')(e)\ne = Dropout(rate=0.35)(e)\ne = Dense(200,activation='relu')(e)\ne = Dropout(rate=0.05)(e)\ne = Dense(50,activation='relu')(e)\n\nz = concatenate([x, y, e])\n\nz = Dense(128,activation='relu')(z)\nz = Dropout(0.05)(z)\nz = Dense(64,activation='relu')(z)\nz = Dropout(0.1)(z)\nz = Dense(32,activation='relu')(z)\noutput_lay = Dense(1, activation='sigmoid')(z)\nmodel = Model(inputs=[main_input,main_input2], outputs=[output_lay])\nmodel.compile(optimizer=Adam(),loss='binary_crossentropy',metrics=['accuracy'])\nprint(model.summary())\nmodel.fit([X_train_copy,X_train_copy2], [y_train],validation_data=([X_test_copy,X_test_copy2],y_test),epochs=4, batch_size=128)  # starts training\ny_predicted = model.predict([X_test_copy,X_test_copy2])\n\ny_pred = []\nfor i in y_predicted:\n    if (i>=0.5):\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\nmodel_name = \"dn\"\n# We want both weighted and macro, because the dataset is imbalanced!\nprint(model_name, 'f1 weighted', metrics.f1_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'f1 macro', metrics.f1_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'precision weighted', metrics.precision_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'precision macro', metrics.precision_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'recall weighted', metrics.recall_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'recall macro', metrics.recall_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'acc', metrics.accuracy_score(y_pred, y_test))\nprint()","execution_count":20,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nmain_input (InputLayer)         (None, 250)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 250, 200)     10000000    main_input[0][0]                 \n__________________________________________________________________________________________________\nmain_input2 (InputLayer)        (None, 500)          0                                            \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 246, 64)      64064       embedding_1[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 250, 300)     60300       embedding_1[0][0]                \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 300)          150300      main_input2[0][0]                \n__________________________________________________________________________________________________\nconv1d_2 (Conv1D)               (None, 242, 32)      10272       conv1d_1[0][0]                   \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 250, 300)     0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 1000)         301000      dense_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 242, 32)      0           conv1d_2[0][0]                   \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 300)          721200      dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 1000)         0           dense_5[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling1d_1 (MaxPooling1D)  (None, 60, 32)       0           dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 300)          0           lstm_2[0][0]                     \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 200)          200200      dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 60, 32)       0           max_pooling1d_1[0][0]            \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 100)          30100       dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 200)          0           dense_6[0][0]                    \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 50)           16600       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 50)           5050        dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 50)           10050       dropout_6[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 150)          0           lstm_1[0][0]                     \n                                                                 dense_3[0][0]                    \n                                                                 dense_7[0][0]                    \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 128)          19328       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 128)          0           dense_8[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 64)           8256        dropout_7[0][0]                  \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 64)           0           dense_9[0][0]                    \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 32)           2080        dropout_8[0][0]                  \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 1)            33          dense_10[0][0]                   \n==================================================================================================\nTotal params: 11,598,833\nTrainable params: 1,598,833\nNon-trainable params: 10,000,000\n__________________________________________________________________________________________________\nNone\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 193407 samples, validate on 95261 samples\nEpoch 1/4\n193407/193407 [==============================] - 760s 4ms/step - loss: 0.3784 - acc: 0.8306 - val_loss: 0.3267 - val_acc: 0.8601\nEpoch 2/4\n193407/193407 [==============================] - 761s 4ms/step - loss: 0.3178 - acc: 0.8654 - val_loss: 0.3290 - val_acc: 0.8586\nEpoch 3/4\n193407/193407 [==============================] - 759s 4ms/step - loss: 0.2925 - acc: 0.8767 - val_loss: 0.3197 - val_acc: 0.8631\nEpoch 4/4\n193407/193407 [==============================] - 747s 4ms/step - loss: 0.2575 - acc: 0.8932 - val_loss: 0.3445 - val_acc: 0.8572\ndn f1 weighted 0.857262488358588\ndn f1 macro 0.8571429821901252\ndn precision weighted 0.8580353129106122\ndn precision macro 0.8572013431560179\ndn recall weighted 0.8572028427163267\ndn recall macro 0.8577988957405321\ndn acc 0.8572028427163267\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train_copy2,X_test_copy2, train_df, X_train_copy, X_test_copy, embeddings_index,embedding_matrix, f, X_train_comments_pre, X_train, X_test ,y_train ,y_test, X_res,y_res, y_train_ori2, X_train_comments, y_train_ori","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_test_comments[0]) ##Do preproccessing! \nprint(len(x_test_comments))\n#test_df.head","execution_count":22,"outputs":[{"output_type":"stream","text":"Jeff Sessions is another one of Trump's Orwellian choices. He believes and has believed his entire career the exact opposite of what the position requires.\n97320\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_comments_pre = []\ncount = 0\nmax_length = -5\nimport time\nstart = time.time()\nfor t in x_test_comments:\n    te = my_clean(t,False,True,2)\n    x_test_comments_pre.append(te)#You can add one more clean()\n    length = len(te.split(' '))\n    if length > max_length:\n        max_length = length\n    \n    if count % 10000 == 0:\n        print(count)\n        final = time.time()\n        total = final - start\n        print(total)\n    \n    count = count + 1","execution_count":23,"outputs":[{"output_type":"stream","text":"0\n0.11119818687438965\n10000\n9.589458465576172\n20000\n18.97055721282959\n30000\n28.398686170578003\n40000\n37.69258975982666\n50000\n47.245259046554565\n60000\n56.73210549354553\n70000\n66.89442610740662\n80000\n77.48118424415588\n90000\n86.93920016288757\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_comments_pre_copy = sequencer.transform(x_test_comments_pre)\nx_test_comments_pre_copy = padder.transform(x_test_comments_pre_copy)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_comments_pre_copy2 = vec.transform(x_test_comments_pre)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y_preds = model.predict([x_test_comments_pre_copy,x_test_comments_pre_copy2])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': np.mean(new_y_preds, axis=1)\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"            id  prediction\n0      7000000    0.003475\n1      7000001    0.002232\n2      7000002    0.357811\n3      7000003    0.002441\n4      7000004    0.998287\n5      7000005    0.010201\n6      7000006    0.028388\n7      7000007    0.051032\n8      7000008    0.147784\n9      7000009    0.099364\n10     7000010    0.019668\n11     7000011    0.404527\n12     7000012    0.120279\n13     7000013    0.002451\n14     7000014    0.089006\n15     7000015    0.003769\n16     7000016    0.275674\n17     7000017    0.001517\n18     7000018    0.313439\n19     7000019    0.187698\n20     7000020    0.065140\n21     7000021    0.012946\n22     7000022    0.004067\n23     7000023    0.983853\n24     7000024    0.966621\n25     7000025    0.546546\n26     7000026    0.895506\n27     7000027    0.014612\n28     7000028    0.119438\n29     7000029    0.014798\n...        ...         ...\n97290  7097290    0.013118\n97291  7097291    0.873745\n97292  7097292    0.757020\n97293  7097293    0.000677\n97294  7097294    0.016343\n97295  7097295    0.005378\n97296  7097296    0.000933\n97297  7097297    0.001345\n97298  7097298    0.033827\n97299  7097299    0.071863\n97300  7097300    0.002052\n97301  7097301    0.012683\n97302  7097302    0.001191\n97303  7097303    0.002447\n97304  7097304    0.817809\n97305  7097305    0.208589\n97306  7097306    0.004990\n97307  7097307    0.000439\n97308  7097308    0.071220\n97309  7097309    0.335021\n97310  7097310    0.081137\n97311  7097311    0.351123\n97312  7097312    0.441631\n97313  7097313    0.919028\n97314  7097314    0.001334\n97315  7097315    0.009218\n97316  7097316    0.008202\n97317  7097317    0.101303\n97318  7097318    0.814063\n97319  7097319    0.000590\n\n[97320 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7000000</td>\n      <td>0.003475</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7000001</td>\n      <td>0.002232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000002</td>\n      <td>0.357811</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7000003</td>\n      <td>0.002441</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7000004</td>\n      <td>0.998287</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7000005</td>\n      <td>0.010201</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7000006</td>\n      <td>0.028388</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7000007</td>\n      <td>0.051032</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7000008</td>\n      <td>0.147784</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7000009</td>\n      <td>0.099364</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7000010</td>\n      <td>0.019668</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7000011</td>\n      <td>0.404527</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7000012</td>\n      <td>0.120279</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7000013</td>\n      <td>0.002451</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7000014</td>\n      <td>0.089006</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7000015</td>\n      <td>0.003769</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7000016</td>\n      <td>0.275674</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7000017</td>\n      <td>0.001517</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7000018</td>\n      <td>0.313439</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7000019</td>\n      <td>0.187698</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7000020</td>\n      <td>0.065140</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7000021</td>\n      <td>0.012946</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7000022</td>\n      <td>0.004067</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7000023</td>\n      <td>0.983853</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>7000024</td>\n      <td>0.966621</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>7000025</td>\n      <td>0.546546</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7000026</td>\n      <td>0.895506</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7000027</td>\n      <td>0.014612</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7000028</td>\n      <td>0.119438</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7000029</td>\n      <td>0.014798</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97290</th>\n      <td>7097290</td>\n      <td>0.013118</td>\n    </tr>\n    <tr>\n      <th>97291</th>\n      <td>7097291</td>\n      <td>0.873745</td>\n    </tr>\n    <tr>\n      <th>97292</th>\n      <td>7097292</td>\n      <td>0.757020</td>\n    </tr>\n    <tr>\n      <th>97293</th>\n      <td>7097293</td>\n      <td>0.000677</td>\n    </tr>\n    <tr>\n      <th>97294</th>\n      <td>7097294</td>\n      <td>0.016343</td>\n    </tr>\n    <tr>\n      <th>97295</th>\n      <td>7097295</td>\n      <td>0.005378</td>\n    </tr>\n    <tr>\n      <th>97296</th>\n      <td>7097296</td>\n      <td>0.000933</td>\n    </tr>\n    <tr>\n      <th>97297</th>\n      <td>7097297</td>\n      <td>0.001345</td>\n    </tr>\n    <tr>\n      <th>97298</th>\n      <td>7097298</td>\n      <td>0.033827</td>\n    </tr>\n    <tr>\n      <th>97299</th>\n      <td>7097299</td>\n      <td>0.071863</td>\n    </tr>\n    <tr>\n      <th>97300</th>\n      <td>7097300</td>\n      <td>0.002052</td>\n    </tr>\n    <tr>\n      <th>97301</th>\n      <td>7097301</td>\n      <td>0.012683</td>\n    </tr>\n    <tr>\n      <th>97302</th>\n      <td>7097302</td>\n      <td>0.001191</td>\n    </tr>\n    <tr>\n      <th>97303</th>\n      <td>7097303</td>\n      <td>0.002447</td>\n    </tr>\n    <tr>\n      <th>97304</th>\n      <td>7097304</td>\n      <td>0.817809</td>\n    </tr>\n    <tr>\n      <th>97305</th>\n      <td>7097305</td>\n      <td>0.208589</td>\n    </tr>\n    <tr>\n      <th>97306</th>\n      <td>7097306</td>\n      <td>0.004990</td>\n    </tr>\n    <tr>\n      <th>97307</th>\n      <td>7097307</td>\n      <td>0.000439</td>\n    </tr>\n    <tr>\n      <th>97308</th>\n      <td>7097308</td>\n      <td>0.071220</td>\n    </tr>\n    <tr>\n      <th>97309</th>\n      <td>7097309</td>\n      <td>0.335021</td>\n    </tr>\n    <tr>\n      <th>97310</th>\n      <td>7097310</td>\n      <td>0.081137</td>\n    </tr>\n    <tr>\n      <th>97311</th>\n      <td>7097311</td>\n      <td>0.351123</td>\n    </tr>\n    <tr>\n      <th>97312</th>\n      <td>7097312</td>\n      <td>0.441631</td>\n    </tr>\n    <tr>\n      <th>97313</th>\n      <td>7097313</td>\n      <td>0.919028</td>\n    </tr>\n    <tr>\n      <th>97314</th>\n      <td>7097314</td>\n      <td>0.001334</td>\n    </tr>\n    <tr>\n      <th>97315</th>\n      <td>7097315</td>\n      <td>0.009218</td>\n    </tr>\n    <tr>\n      <th>97316</th>\n      <td>7097316</td>\n      <td>0.008202</td>\n    </tr>\n    <tr>\n      <th>97317</th>\n      <td>7097317</td>\n      <td>0.101303</td>\n    </tr>\n    <tr>\n      <th>97318</th>\n      <td>7097318</td>\n      <td>0.814063</td>\n    </tr>\n    <tr>\n      <th>97319</th>\n      <td>7097319</td>\n      <td>0.000590</td>\n    </tr>\n  </tbody>\n</table>\n<p>97320 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}