{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load up the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv') # nrows=100000\n# test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntrain_df, val_df = model_selection.train_test_split(train_df, test_size=0.2)\nprint('%d train comments, %d validate comments' % (len(train_df), len(val_df)))\n\ntrain_x = train_df['comment_text'].astype(str)\ntrain_y = np.where(train_df['target'] >= 0.5, 1, 0)\n\nval_x = val_df['comment_text'].astype(str)\nval_y = np.where(val_df['target'] >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 220\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_x) + list(val_x))\n\ntrain_x = tokenizer.texts_to_sequences(train_x)\nval_x = tokenizer.texts_to_sequences(val_x)\ntrain_x = sequence.pad_sequences(train_x, maxlen=MAX_LEN)\nval_x = sequence.pad_sequences(val_x, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load embeddings"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(tokenizer.word_index, '../input/glove840b300dtxt/glove.840B.300d.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 64\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\n\ndef build_model(embedding_matrix): #, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = CuDNNLSTM(LSTM_UNITS, return_sequences=True)(x)\n    # x = CuDNNLSTM(LSTM_UNITS, return_sequences=True)(x)\n    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\n    # hidden = GlobalMaxPooling1D()(x)\n    hidden = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n    # hidden = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n    # hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    # hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    # aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    # model = Model(inputs=words, outputs=[result, aux_result])\n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run model"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nmodel = build_model(embedding_matrix)\nfor global_epoch in range(EPOCHS):\n    model.fit(\n        train_x,\n        train_y,\n        batch_size=BATCH_SIZE,\n        epochs=1,\n        verbose=2,\n        callbacks=[\n            LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n        ]\n    )\n    checkpoint_predictions.append(model.predict(val_x, batch_size=2048)[0].flatten())\n    # weights.append(2 ** global_epoch)\n\n# predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_yhat = model.predict(val_x, batch_size=2048)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subgroup analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df['prediction'] = val_yhat\nval_df['target'] = val_y\ndf = val_df\n\ngroups = ['black', 'white', 'male', 'female','christian', 'jewish', 'muslim','psychiatric_or_mental_illness','homosexual_gay_or_lesbian']\ncategories = pd.DataFrame(columns = ['SUB', 'BPSN', 'BNSP'], index = groups)\n\ndef auc(df):\n    y = df['target']\n    pred = df['prediction']\n    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n    return metrics.auc(fpr, tpr)\n\ndef Mp(data, p=-5.0):\n    return np.average(data ** p) ** (1/p)\n\nfor group in groups:\n    df[group] = df[group] >= 0.5\n    categories.loc[group,'SUB'] = auc(df[df[group]])\n    bpsn = ((~df[group] & df['target'])    #background positive\n            | (df[group] & ~df['target'])) #subgroup negative\n    categories.loc[group,'BPSN'] = auc(df[bpsn])\n    bnsp = ((~df[group] & ~df['target'])   #background negative\n            | (df[group] & df['target']))  #subgrooup positive\n    categories.loc[group,'BNSP'] = auc(df[bnsp])\n\ncategories.loc['Mp',:] = categories.apply(Mp, axis= 0)\n\nprint(\"Overal AUC: \" + str(auc(df)))\n\ncategories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}