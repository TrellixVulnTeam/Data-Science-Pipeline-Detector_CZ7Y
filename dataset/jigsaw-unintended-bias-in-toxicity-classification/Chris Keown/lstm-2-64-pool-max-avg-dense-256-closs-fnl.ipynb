{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn import metrics\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Load up the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv') # , nrows=100000)\n# test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntrain_df, val_df = model_selection.train_test_split(train_df, test_size=0.2)\nprint('%d train comments, %d validate comments' % (len(train_df), len(val_df)))\n\ntrain_x = train_df['comment_text'].astype(str)\n# train_y = np.where(train_df['target'] >= 0.5, 1, 0)\n\nval_x = val_df['comment_text'].astype(str)\nval_y = np.where(val_df['target'] >= 0.5, 1, 0)","execution_count":2,"outputs":[{"output_type":"stream","text":"80000 train comments, 20000 validate comments\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 220\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_x) + list(val_x))\n\ntrain_x = tokenizer.texts_to_sequences(train_x)\nval_x = tokenizer.texts_to_sequences(val_x)\ntrain_x = sequence.pad_sequences(train_x, maxlen=MAX_LEN)\nval_x = sequence.pad_sequences(val_x, maxlen=MAX_LEN)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss function weight calculations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create custom loss function and setup corresponding weights.\n\nidentity_columns = ['asian', 'atheist','bisexual', 'black', 'buddhist', 'christian', 'female',\n                    'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', \n                    'jewish', 'latino', 'male','muslim', 'other_disability', 'other_gender','other_race_or_ethnicity', \n                    'other_religion','other_sexual_orientation', 'physical_disability','psychiatric_or_mental_illness', \n                    'transgender', 'white']\n\n# Overall\nweights = np.ones((len(train_df),)) / 4\n# Subgroup\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ntrain_y = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\nnum_aux_targets = y_aux_train.shape[-1]\n\ndef custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load embeddings"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(tokenizer.word_index, '../input/glove840b300dtxt/glove.840B.300d.txt')","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 64\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\n\ndef build_model(embedding_matrix): #, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = CuDNNLSTM(LSTM_UNITS, return_sequences=True)(x)\n    x = CuDNNLSTM(LSTM_UNITS, return_sequences=True)(x)\n    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\n    # hidden = GlobalMaxPooling1D()(x)\n    hidden = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n    # hidden = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n    # hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    # hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    # aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n\n    # model = Model(inputs=words, outputs=result)\n    # model.compile(loss='binary_crossentropy', optimizer='adam')\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n    \n    return model\n","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run model"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nmodel = build_model(embedding_matrix)\nfor global_epoch in range(EPOCHS):\n    model.fit(\n        train_x,\n        [train_y, y_aux_train],\n        batch_size=BATCH_SIZE,\n        epochs=1,\n        verbose=2,\n        callbacks=[\n            LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n        ]\n    )\n    checkpoint_predictions.append(model.predict(val_x, batch_size=2048)[0].flatten())\n    # weights.append(2 ** global_epoch)\n\n# predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","execution_count":8,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nEpoch 1/1\n - 9s - loss: 0.5524 - dense_2_loss: 0.1149 - dense_3_loss: 0.1675\nEpoch 1/1\n - 7s - loss: 0.4152 - dense_2_loss: 0.0879 - dense_3_loss: 0.1210\nEpoch 1/1\n - 7s - loss: 0.3470 - dense_2_loss: 0.0707 - dense_3_loss: 0.1103\nEpoch 1/1\n - 7s - loss: 0.3287 - dense_2_loss: 0.0661 - dense_3_loss: 0.1075\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_yhat = model.predict(val_x, batch_size=2048)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subgroup analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df['prediction'] = val_yhat[0]\nval_df['target'] = val_y\ndf = val_df\n\ngroups = ['black', 'white', 'male', 'female','christian', 'jewish', 'muslim','psychiatric_or_mental_illness','homosexual_gay_or_lesbian']\ncategories = pd.DataFrame(columns = ['SUB', 'BPSN', 'BNSP'], index = groups)\n\ndef auc(df):\n    y = df['target']\n    pred = df['prediction']\n    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n    return metrics.auc(fpr, tpr)\n\ndef Mp(data, p=-5.0):\n    return np.average(data ** p) ** (1/p)\n\nfor group in groups:\n    df[group] = df[group] >= 0.5\n    categories.loc[group,'SUB'] = auc(df[df[group]])\n    bpsn = ((~df[group] & df['target'])    #background positive\n            | (df[group] & ~df['target'])) #subgroup negative\n    categories.loc[group,'BPSN'] = auc(df[bpsn])\n    bnsp = ((~df[group] & ~df['target'])   #background negative\n            | (df[group] & df['target']))  #subgrooup positive\n    categories.loc[group,'BNSP'] = auc(df[bnsp])\n\ncategories.loc['Mp',:] = categories.apply(Mp, axis= 0)\n\nprint(\"Overal AUC: \" + str(auc(df)))\n\ncategories","execution_count":10,"outputs":[{"output_type":"stream","text":"Overal AUC: 0.9326149377316283\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"                                    SUB      BPSN      BNSP\nblack                          0.779707  0.845359  0.913695\nwhite                          0.749533  0.828208  0.907125\nmale                           0.848403  0.881144  0.917569\nfemale                         0.872068  0.890706  0.926127\nchristian                      0.894953  0.871332   0.94685\njewish                                1  0.877688  0.960431\nmuslim                         0.786966  0.821914  0.929306\npsychiatric_or_mental_illness  0.797727  0.903853  0.867427\nhomosexual_gay_or_lesbian      0.765699   0.80489  0.932481\nMp                             0.815817  0.854598  0.920224","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUB</th>\n      <th>BPSN</th>\n      <th>BNSP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>black</th>\n      <td>0.779707</td>\n      <td>0.845359</td>\n      <td>0.913695</td>\n    </tr>\n    <tr>\n      <th>white</th>\n      <td>0.749533</td>\n      <td>0.828208</td>\n      <td>0.907125</td>\n    </tr>\n    <tr>\n      <th>male</th>\n      <td>0.848403</td>\n      <td>0.881144</td>\n      <td>0.917569</td>\n    </tr>\n    <tr>\n      <th>female</th>\n      <td>0.872068</td>\n      <td>0.890706</td>\n      <td>0.926127</td>\n    </tr>\n    <tr>\n      <th>christian</th>\n      <td>0.894953</td>\n      <td>0.871332</td>\n      <td>0.94685</td>\n    </tr>\n    <tr>\n      <th>jewish</th>\n      <td>1</td>\n      <td>0.877688</td>\n      <td>0.960431</td>\n    </tr>\n    <tr>\n      <th>muslim</th>\n      <td>0.786966</td>\n      <td>0.821914</td>\n      <td>0.929306</td>\n    </tr>\n    <tr>\n      <th>psychiatric_or_mental_illness</th>\n      <td>0.797727</td>\n      <td>0.903853</td>\n      <td>0.867427</td>\n    </tr>\n    <tr>\n      <th>homosexual_gay_or_lesbian</th>\n      <td>0.765699</td>\n      <td>0.80489</td>\n      <td>0.932481</td>\n    </tr>\n    <tr>\n      <th>Mp</th>\n      <td>0.815817</td>\n      <td>0.854598</td>\n      <td>0.920224</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}