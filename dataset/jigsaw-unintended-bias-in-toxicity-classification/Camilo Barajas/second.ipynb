{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importar las librerias"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#!pip install livelossplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#from livelossplot import PlotLossesKeras\n#%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n#from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\nNUM_MODELS = 4\nBATCH_SIZE = 330\nLSTM_UNITS = 120\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 3\nMAX_LEN = 210\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE =  '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path,'r', encoding='UTF-8') as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import nltk\nnltk.download('stopwords')\n# nltk library\nsw = stopwords.words('english')  \nnp.array(sw)\nprint(\"Number of stopwords: \", len(sw))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def stopwords(text):\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train = x_train.apply(stopwords)\nx_test = x_test.apply(stopwords)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.55, True, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights /= sample_weights.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_t, y, y_t , ya, ya_t, sw, sw_t = train_test_split(x_train, y_train , y_aux_train ,sample_weights, test_size=0.18, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(embedding_matrix, ya.shape[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for global_model in range(NUM_MODELS):\n    model.fit(X, [y, ya], validation_data=[X_t, [y_t,ya_t]], \n              batch_size=BATCH_SIZE, epochs= 2, verbose=2,sample_weight=[sw.values, np.ones_like(sw)],\n              callbacks=[LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_model))])\n    checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}