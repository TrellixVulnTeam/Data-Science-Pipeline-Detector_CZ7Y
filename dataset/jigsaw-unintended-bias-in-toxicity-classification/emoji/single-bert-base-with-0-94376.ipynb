{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport random\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator\nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nimport time\n\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\n\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil\n\npackage_dir_a = \"./pytorch-pretrained-BERT/\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom pytorch_pretrained_bert import BertTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nbert_config = BertConfig('./data/uncased_L-12_H-768_A-12/' + 'bert_config.json')\ndevice = torch.device('cuda')\nData_dir = \"./data\"\nWORK_DIR = \"./data/working/\"\nBERT_MODEL_PATH = './data/uncased_L-12_H-768_A-12/'\n\nMAX_SEQUENCE_LENGTH = 220\nSEED = 42\n\ndef seed_everything(SEED=SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)\n    \n\n# num_to_load = 1700000  # Train size to match time limit\n# valid_size = 100000  # Validation Size\nTOXICITY_COLUMN = 'target'\n\n\noutput_model_file = \"./data/output/bert_base_epoch2.bin\"\n# BIAS_FILE = \"./data/output/bias/bias_loss.csv\"\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# has sexual_explicit\nAUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat','sexual_explicit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            # tokens_a = tokens_a[:max_seq_length]\n            half_seq_length = (int)(max_seq_length/2)\n            tokens_a = tokens_a[:half_seq_length] + tokens_a[len(tokens_a)-half_seq_length:]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n    def __iter__(self):\n\n        buckets = [[]] * 100\n        yielded = 0\n\n        for idx in self.sampler:\n            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n            count_zeros = int(count_zeros / 64) \n            if len(buckets[count_zeros]) == 0:  buckets[count_zeros] = []\n\n            buckets[count_zeros].append(idx)\n\n            if len(buckets[count_zeros]) == self.batch_size:\n                batch = list(buckets[count_zeros])\n                yield batch\n                yielded += 1\n                buckets[count_zeros] = []\n\n        batch = []\n        leftover = [idx for bucket in buckets for idx in bucket]\n\n        for idx in leftover:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yielded += 1\n                yield batch\n                batch = []\n\n        if len(batch) > 0 and not self.drop_last:\n            yielded += 1\n            yield batch\n\n        assert len(self) == yielded, \"produced an inccorect number of batches. expected %i, but yielded %i\" %(len(self), yielded)\n\ndef trim_tensors(tsrs):\n    max_len = torch.max(torch.sum( (tsrs[0] != 0  ), 1))\n    if max_len > 2: \n        tsrs = [tsr[:, :max_len] for tsr in tsrs]\n    return tsrs\n\nbatch_size = 64\n\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\n\n# train_df = pd.read_csv(os.path.join(Data_dir, \"train.csv\")).sample(num_to_load + valid_size, random_state=SEED)\ntrain_df = pd.read_csv(os.path.join(Data_dir, \"train.csv\"))\ntrain_df = train_df.sample(len(train_df),random_state=SEED)\nprint('loaded %d records' % len(train_df))\n\ntrain_df['comment_text'] = train_df['comment_text'].astype(str)\nsequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\ntrain_df = train_df.fillna(0)\n\nX = sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOSS\n# Overall\ncoll = ['black','white','homosexual_gay_or_lesbian','muslim']\n\nweights = np.ones((len(X),)) / 4\n# Subgroup  identity_columns  > 0.5\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[coll].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) /8\n\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[coll].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 8\n\nloss_weight = 1.0 / weights.mean()\nweights =weights.reshape(-1,1)\n\n# y_columns = ['target']\ntrain_df = train_df.drop(['comment_text'], axis=1)\n# convert target to 0,1\n# train_df['target']=(train_df['target']>=0.5).astype(float)\n\ny = train_df[AUX_COLUMNS].values\ny = np.hstack([ weights,y])\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.float))\nran_sampler = torch.utils.data.RandomSampler(train_dataset)\nlen_sampler = LenMatchBatchSampler(ran_sampler, batch_size = batch_size, drop_last = False)\n\ndef custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,0:1])(data[:,0:1],targets[:,1:2])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 2e-5\n#batch_size = 64\naccumulation_steps = 2\nseed_everything(SEED)\n\nmodel = BertForSequenceClassification.from_pretrained(\"./data/working\", cache_dir=None, num_labels=len(AUX_COLUMNS))\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\n\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\n\nEPOCHS = 2\ntrain = train_dataset\nnum_train_optimization_steps = int(EPOCHS * len(train) / batch_size / accumulation_steps)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\nmodel = model.train()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_sampler=len_sampler)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf = None\n    tk0 = tqdm_notebook(enumerate(train_loader), total=len(train_loader), leave=False)\n    optimizer.zero_grad()\n    if epoch == 1 :\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 1e-5\n            param_group['warmup'] = 0\n#     elif epoch == 2:\n#         for param_group in optimizer.param_groups:\n#             param_group['lr'] = 1e-5\n#             param_group['warmup'] = 0\n    for i, batch in tk0:\n        tsrs = trim_tensors(batch)\n        x_batch, y_batch = tuple(t.to(device) for t in tsrs)\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        loss = custom_loss(y_pred,y_batch.to(device))\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if (i + 1) % accumulation_steps == 0:  # Wait for several backward steps\n            optimizer.step()  # Now we can do an optimizer step\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98 * lossf + 0.02 * loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss=lossf)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(\n            ((torch.sigmoid(y_pred[:, 0]) > 0.5) == (y_batch[:, 0] > 0.5).to(device)).to(torch.float)).item() / len(\n            train_loader)\n    tq.set_postfix(avg_loss=avg_loss, avg_accuracy=avg_accuracy)\n\ntorch.save(model.state_dict(), output_model_file)\nprint('costing:%.4f S' % (time.time() - start_time))\ndel model","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}