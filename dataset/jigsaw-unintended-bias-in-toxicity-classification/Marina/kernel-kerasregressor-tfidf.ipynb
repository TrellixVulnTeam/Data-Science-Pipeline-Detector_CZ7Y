{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom keras.optimizers import SGD\nfrom keras.optimizers import rmsprop\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Flatten, Dense,Conv1D,MaxPooling1D,AveragePooling1D,BatchNormalization\nfrom sklearn.metrics import confusion_matrix,classification_report,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['comment_text'][1804800])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef obr_text(text):\n    text = text.lower().replace(\"ё\", \"е\")\n    text = re.sub(r\"\\d+\", \"\", text, flags=re.UNICODE)\n    return text.strip()\ntrain['comment_text'] = [obr_text(t) for t in train['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"englishStemmer=SnowballStemmer(\"english\")\nrussianStemmer=SnowballStemmer(\"russian\")\ndef lemmatize_stemming(text):\n    return englishStemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n            result.append(lemmatize_stemming(token))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = [preprocess(t) for t in train['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['comment_text'][1804800])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = [' '.join(t) for t in train['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['comment_text'][1804800])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = set()\nmy_df = train['comment_text']\nmy_df.apply(results.update)\nmax_word = len(results)\nprint (len(results))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nls =[]\nfor i in train['comment_text']:\n     ls.append(len(str(i).split()))   \nc = Counter(ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nplt.hist(ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = round(train['target'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\ndf_majority = train[train.target==0]\ndf_minority1 = train.loc[(train['target'] >0) & (train['target'] <= 0.1)]\ndf_minority2 = train.loc[(train['target'] >0.1) & (train['target'] <= 0.2)]\ndf_minority3 = train.loc[(train['target'] >0.2) & (train['target'] <= 0.3)]\ndf_minority4 = train.loc[(train['target'] >0.3) & (train['target'] <= 0.4)]\ndf_minority5 = train.loc[(train['target'] >0.4) & (train['target'] <= 0.5)]\ndf_minority6 = train.loc[(train['target'] >0.5) & (train['target'] <= 0.6)]\ndf_minority7 = train.loc[(train['target']>0.6) & (train['target'] <= 0.7)]\ndf_minority8 = train.loc[(train['target'] >0.7) & (train['target'] <= 0.8)]\ndf_minority9 = train.loc[(train['target'] >0.8) & (train['target'] <= 0.9)]\ndf_minority10 = train.loc[(train['target'] >0.9) & (train['target'] <= 1.0)]\n \n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=len(df_majority)//3,     # to match minority class\n                                 random_state=100) # reproducible results\ndf_minority_1 = resample(df_minority1, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_2 = resample(df_minority2, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_3 = resample(df_minority3, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_4 = resample(df_minority4, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_5 = resample(df_minority5, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_6 = resample(df_minority6, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_7 = resample(df_minority7, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_8 = resample(df_minority8, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_9 = resample(df_minority9, replace=True, n_samples=len(df_majority)//4, random_state=100)\ndf_minority_10 = resample(df_minority10, replace=True, n_samples=len(df_majority)//4, random_state=100)\n \n# Combine minority class with downsampled majority class\n#df_downsampled = pd.concat([df_majority_downsampled, df_minority])\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority_1, df_minority_2,df_minority_3,df_minority_4,\n                            df_minority_5,df_minority_6,df_minority_7,df_minority_8,df_minority_9,df_minority_10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display new class counts\nprint(len(df_downsampled))\nbins = 50\nplt.hist(df_downsampled['target'], bins, alpha=0.5, color='b',label='Сбалансированный датасет')\nplt.hist(train['target'], bins, alpha=0.5, color='r',label='Заданный датасет')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_raw = df_downsampled['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(max_word)\ntokenizer.fit_on_texts(x_raw) \nvocab_size = len(tokenizer.word_index) + 1\nprint (vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_raw.shape, x_raw[1804800])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntv = TfidfVectorizer(max_features=1000,ngram_range=(1,2),analyzer= 'word',\n                        token_pattern=r\"\\b\\w[\\w']+\\b\", min_df=0.01, max_df=0.8, \n                        stop_words = ('english', 'russian'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tv.fit_transform(x_raw)\n#vocab = tv.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (x_train[1804800].toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.array(df_downsampled['target']).astype(\"float32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection  import  train_test_split \nx_train1, x_train2,y_train1,y_train2 = train_test_split(x_train,y_train, test_size=0.3, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train1.shape, len(y_train1))\nprint(x_train2.shape, len(y_train2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor='acc', patience=2, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasRegressor\ndef create_model(optimizer='adam',\n                 kernel_initializer='glorot_uniform', \n                 dropout=0.2):\n    model = Sequential()\n    model.add(Dense(512,activation='relu',kernel_initializer=kernel_initializer, input_shape=(x_train.shape[1],)))\n    model.add(Dropout(dropout))\n    model.add(Dense(256,activation='relu',kernel_initializer=kernel_initializer))\n    model.add(Dropout(dropout))\n    model.add(Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer))\n\n    model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n\n    return model\n\n# wrap the model using the function you created\nclf = KerasRegressor(build_fn=create_model,epochs=55, batch_size=300,callbacks=[early_stopping_callback],verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(x_train1, y_train1)\nprint(\"Обучение остановлено на эпохе\", early_stopping_callback.stopped_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=clf.predict(x_train2)\nfrom sklearn.metrics import r2_score\nr2_score(y_train2, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'] = [obr_text(t) for t in test['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'] = [preprocess(t) for t in test['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'] = [' '.join(t) for t in test['comment_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_raw_t = test['comment_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(x_raw_t) \nx_test = tv.transform(x_raw_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test=clf.predict(x_test)\nprint(y_test[:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission= pd.read_csv('../input/sample_submission.csv')\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['prediction']=y_test\nsample_submission.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', sep=',',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}