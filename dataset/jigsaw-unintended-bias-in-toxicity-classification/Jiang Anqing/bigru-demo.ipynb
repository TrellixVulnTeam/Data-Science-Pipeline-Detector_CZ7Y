{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":34,"outputs":[{"output_type":"stream","text":"['fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom gensim.models import KeyedVectors\n","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 数据读取\n\n利用pd.read_csv()可以直接读取很多表格类的数据，同样如果数据是压缩格式的话可以利用 compression这个参数直接读取压缩格式下的数据。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntrain = train.fillna(0)\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n#test = test.fillna(0)\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n#sub = sub.fillna(0)\n\nprint('Train shape : ', train.shape)\nprint('Test shape : ', test.shape)\nprint('Sub shape:', sub.shape)","execution_count":36,"outputs":[{"output_type":"stream","text":"Train shape :  (1804874, 45)\nTest shape :  (97320, 2)\nSub shape: (97320, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":37,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1804874 entries, 0 to 1804873\nData columns (total 45 columns):\nid                                     int64\ntarget                                 float64\ncomment_text                           object\nsevere_toxicity                        float64\nobscene                                float64\nidentity_attack                        float64\ninsult                                 float64\nthreat                                 float64\nasian                                  float64\natheist                                float64\nbisexual                               float64\nblack                                  float64\nbuddhist                               float64\nchristian                              float64\nfemale                                 float64\nheterosexual                           float64\nhindu                                  float64\nhomosexual_gay_or_lesbian              float64\nintellectual_or_learning_disability    float64\njewish                                 float64\nlatino                                 float64\nmale                                   float64\nmuslim                                 float64\nother_disability                       float64\nother_gender                           float64\nother_race_or_ethnicity                float64\nother_religion                         float64\nother_sexual_orientation               float64\nphysical_disability                    float64\npsychiatric_or_mental_illness          float64\ntransgender                            float64\nwhite                                  float64\ncreated_date                           object\npublication_id                         int64\nparent_id                              float64\narticle_id                             int64\nrating                                 object\nfunny                                  int64\nwow                                    int64\nsad                                    int64\nlikes                                  int64\ndisagree                               int64\nsexual_explicit                        float64\nidentity_annotator_count               int64\ntoxicity_annotator_count               int64\ndtypes: float64(32), int64(10), object(3)\nmemory usage: 619.7+ MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 减少内存使用：\n\n大概观测了一下数据，发现很多数据远远不在int64或者float64的内存使用范围内，所以这里进行内存的reduce。\n\n这里我们把float64的数据占用变为了float32,这样大约释放了一般左右的内存。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n\n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            \n\n            else:\n                df[col] = df[col].astype(np.float32)\n\n\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return df","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)","execution_count":39,"outputs":[{"output_type":"stream","text":"Memory usage of properties dataframe is : 619.6543884277344  MB\n___MEMORY USAGE AFTER COMPLETION:___\nMemory usage is:  294.3358745574951  MB\nThis is  47.50000646397767 % of the initial size\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Embeddings\n\nWe'll use FastText Common Crawl embeddings to a simple start."},{"metadata":{"trusted":true},"cell_type":"code","source":"fast_text_common_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(fast_text_common_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\n\n#train_df, val = model_selection.train_test_split(train, test_size = 0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.engine import InputSpec, Layer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,  Callback, EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(train['comment_text'])\n\nx_train = train['comment_text']\n#x_test = val['comment_text']\n\ntrain_labels = train['target']\n#text_labels = val['target']\n\nx_train = tokenizer.texts_to_sequences(x_train)\n#x_test = tokenizer.texts_to_sequences(x_test)\n\nx_train = pad_sequences(x_train, maxlen=256)\n#x_test = pad_sequences(x_test, maxlen=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n\nnum_words_in_embedding = 0\n\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index.vocab:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector        \n        num_words_in_embedding += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_binary_crossentropy(y_true, y_pred) :\n\n    logloss = -(y_true * K.log(y_pred) * weights[0] + (1 - y_true) * K.log(1 - y_pred) * weights[1])\n\n    return K.mean(logloss, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model using keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(lr=0.0, lr_d=0.0, units=64, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=0, dr=0.1, conv_size=32, epochs=20):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    sequence_input = Input(shape=(256,), dtype='int32')\n    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                            300,\n                            weights=[embedding_matrix],\n                            input_length=256,\n                            trainable=False)\n    x = embedding_layer(sequence_input)\n    x = SpatialDropout1D(spatial_dr)(x)\n    x = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)   \n    x = Conv1D(conv_size, kernel_size2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    x = Conv1D(int(conv_size/2), kernel_size1, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n\n    x = Dropout(dr)(x)\n\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)     \n    \n    x = concatenate([avg_pool1, max_pool1])\n    x = BatchNormalization()(x)#1\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))#2\n    \n    preds = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs = sequence_input, outputs = preds)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\",\"binary_crossentropy\"])\n    model.summary()\n    history = model.fit(x_train, train_labels, batch_size = 512, epochs = epochs, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n   \n    model = load_model(file_path)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.2, kernel_size1=4, kernel_size2=2, dense_units=32, dr=0.1, conv_size=64, epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.2, kernel_size1=4, kernel_size2=2, dense_units=64, dr=0.1, conv_size=64, epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n\ntest = test['comment_text']\ntest = tokenizer.texts_to_sequences(test)\ntest = pad_sequences(test, maxlen=256)\n\nsub['prediction'] = Model1.predict(test)\nsub.reset_index(drop=False, inplace=True)\n\nsub.to_csv('demosubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"kaggle","language":"python","name":"kaggle"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}