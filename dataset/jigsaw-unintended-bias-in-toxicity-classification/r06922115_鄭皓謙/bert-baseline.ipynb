{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"100k sample version\n\n\nnote version 18 is 10k version\n\n\nv21 = 100k version, 1 pos_w, barely training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\npos_w = torch.FloatTensor([1.]).cuda()\na = torch.FloatTensor([1, 1]).cuda()\nb = torch.FloatTensor([0.3, 0.5]).cuda()\ntorch.nn.functional.binary_cross_entropy_with_logits(b, a, pos_weight=pos_w)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. # install apex bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn import metrics\n!cd ../input/apex-master/apex-master/apex-master/ && pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\nos.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')\n\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom apex import amp\n\nfrom pytorch_pretrained_bert import BertModel, BertTokenizer, BertForSequenceClassification,BertAdam\nBERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'\n\nbert = BertModel.from_pretrained(BERT_FP).cuda()\ntokenizer = BertTokenizer(vocab_file='../input/torch-bert-weights/bert-base-uncased-vocab.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_words = set([\n    'homosexual', 'gay', 'lesbian',\n    'blacks', 'black',\n    'whites', 'white',\n    'Muslims', 'Muslim', 'islamic', 'islam', \n    'christian', 'jesus', 'bible', 'jewish',\n    'psychiatric', 'mental_illness',\n    'russian', 'palestinian',\n    'mexico', 'india', 'canada',\n    'muslim', 'black', 'white',\n    \n])\nidentity_words = set([w.lower() for w in identity_words] + [w.lower()+'s' for w in identity_words])\ndef mask_identity(identity_words, sample):\n    return ' '.join(['[MASK]' if w.lower() in identity_words else w for w in sample.split(' ')])\nMAX_SEQUENCE_LENGTH = 220\nSEED = 1234\n\nData_dir=\"../input/data/jigsaw-unintended-bias-in-toxicity-classification\"\nTOXICITY_COLUMN = 'target'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice=torch.device('cuda')\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        text = mask_identity(identity_words, text)\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n\ntokenizer = BertTokenizer(vocab_file='../input/torch-bert-weights/bert-base-uncased-vocab.txt', do_lower_case=True)\n\n# total_df = pd.read_csv(\"./data/jigsaw-unintended-bias-in-toxicity-classification/train.csv\").sample(1000,random_state=SEED)\ntotal_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\").sample(1000000, random_state=SEED)\nN = len(total_df)\n\nnum_to_load= (N // 10)*9                 #Train size to match time limit\nvalid_size= N//10 \nprint('loaded %d records' % len(total_df))\n# Make sure all comment_text values are strings\ntotal_df['comment_text'] = total_df['comment_text'].astype(str) \n# Make sure all comment_text values are strings\nsequences = convert_lines(total_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\ntotal_df=total_df.fillna(0)\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ny_columns=['target']\n# convert target to 0,1\ntotal_df['target']=(total_df['target']>=0.5).astype(float)\n\n\nX = sequences[:num_to_load]                \ny = total_df[y_columns].values[:num_to_load]\nX_val = sequences[num_to_load:]                \ny_val = total_df[y_columns].values[num_to_load:]\ntrain_df = total_df.head(num_to_load)\nval_df = total_df.tail(valid_size)\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\nval_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long), torch.tensor(y_val,dtype=torch.float))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=2e-5\nbatch_size = 32\naccumulation_steps=2\nEPOCHS = 1\nmodel = BertForSequenceClassification.from_pretrained(BERT_FP, num_labels=1)\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel = model.train()\ntq = tqdm_notebook(range(EPOCHS))\niters = 1\nauc = 0\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n    for i,(x_batch, y_batch) in tk0:\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device), pos_weight=pos_w)\n        accuracy = torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float))\n        \n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step\n            optimizer.zero_grad()\n        \n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf, accuracy=accuracy.item(), iters=iters)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n        \n                \n        iters += 1\n        if iters % 2000 == 0:\n            torch.save(model.state_dict(), './iters_%d.mdl' % (iters))\n        if iters > 35000:\n            break\n        if (time.time() - start_time ) // 60 > 100:\n            break\nprint('iter : %d'  % iters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('iter : %d'  % iters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\nprint('loaded %d records' % len(test_df))\n# Make sure all comment_text values are strings\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n# Make sure all comment_text values are strings\nsequences = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\nX = sequences\ny = np.zeros([len(X), 1])\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\ntk = tqdm_notebook(enumerate(test_loader),total=len(test_loader),leave=False)\nidx = 7000000\nmodel.to(device)\nwith open('./submission.csv', 'w') as f:\n    f.write('id,prediction\\n')\n    with torch.no_grad():\n        for i,(x_batch, _) in tk:\n            out = torch.sigmoid(model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)).cpu().detach().numpy()\n            for o in out.flatten():\n                f.write('%d,%.1f\\n' % (idx, o))\n                idx += 1\nprint('done')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}