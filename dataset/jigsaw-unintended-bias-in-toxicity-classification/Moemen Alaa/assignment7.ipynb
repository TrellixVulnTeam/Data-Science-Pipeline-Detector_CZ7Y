{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.plots import plot_convergence\nimport json\nimport string\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes,linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.layers import Embedding, Flatten, Dense, GRU\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nimport unicodedata\nimport html\nstop_words = stopwords.words('english')\n\n\nimport warnings\nimport numpy as np \nimport pandas as pd \nimport os\nimport re\nimport nltk\nfrom nltk.corpus import abc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import *\nfrom nltk.util import ngrams\nimport string\nimport spacy\nfrom spacy import displacy\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom wordcloud import WordCloud, STOPWORDS\nimport gensim\nfrom gensim.models.word2vec import Text8Corpus\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n#from spacytextblob.spacytextblob import SpacyTextBlob\nfrom IPython.core.display import HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport html\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nrand_seed = 13579\nnp.random.seed(rand_seed)\n\nsns.set(style=\"darkgrid\", context=\"notebook\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample of the data\nidx_1 = train_df.query('target==0').sample(n=10000, random_state=42).index\nidx_0 = train_df.query('target!=0').sample(n=8000, random_state=42).index\ntrain_df = train_df.loc[list(idx_1) + list(idx_0), :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_df['comment_text'].fillna('').values\n\n#if true, y_train[i] =1, if false, it is = 0\ny_train = np.where(train_df['target'] >= 0.5, 1, 0)\n\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\nx_train = train_df['comment_text'].fillna('').values\n\nx_test = test_df['comment_text'].fillna('').values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_sz = 10000\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok.fit_on_texts(list(x_train) + list(x_test))\nx_train = tok.texts_to_sequences(x_train)\nx_test = tok.texts_to_sequences(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = pad_sequences(x_train, maxlen=220)\nx_test = pad_sequences(x_test, maxlen=220)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(os.path.join('./glove.6B.200d.txt'), encoding='utf8')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tok.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 220\nmax_words = 10000\nembedding_dim = 200\nembedding_matrix = np.zeros((max_words, embedding_dim))\n\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_text, x_val_text, y_train_text, y_val_text = train_test_split(x_train, y_train, test_size=0.4, random_state=41)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(GRU(64, return_sequences=True))\nmodel.add(GRU(64))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.optimizers import Adam\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\nmodel.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(clipnorm=0.1),\n        metrics=['accuracy'])\n\n\nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val_text, y_val_text))\nhistory_dict = history.history\nhistory_dict.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\n\n\nwords = Input(shape=(maxlen,))\nx = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words) #Finds word embeddings for each word\nx = SpatialDropout1D(0.3)(x) #This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements\nx = Bidirectional(LSTM(128, return_sequences=True))(x)\nx = Bidirectional(LSTM(128, return_sequences=True))(x)\nhidden = concatenate([\n    GlobalMaxPooling1D()(x), \n    GlobalAveragePooling1D()(x),#layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input \n    #of variable length in the simplest way possible.\n])\nhidden = add([hidden, Dense(4*128, activation='relu')(hidden)]) #This fixed-length output vector is piped through a fully-connected (Dense) layer with x hidden units.\nhidden = add([hidden, Dense(4*128, activation='relu')(hidden)])\nresult = Dense(1, activation='sigmoid')(hidden)\naux_result = Dense(6, activation='sigmoid')(hidden)\nmodel = Model(inputs=words, outputs=[result, aux_result])\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for global_epoch in range(4):\n    model.fit(\n        x_train,\n        [y_train, y_aux_train],\n        batch_size=512,\n        epochs=1,\n        verbose=1,\n        callbacks=[\n            LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}