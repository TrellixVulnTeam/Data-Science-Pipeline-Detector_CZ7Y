{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# standard imports\nimport time\nimport random\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\n# pytorch imports\nfrom torch.optim import lr_scheduler\nimport torch\nimport shutil\nimport torch.nn as nn\nimport torch.utils.data\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer as keras_tokenizer\n# cross validation and metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n# progress bars\nimport nltk\nimport re\nimport collections\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertModel\nfrom pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertConfig","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\npercent_true = train_df.target.mean()\nprint(\"Percent true\", percent_true)\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\nprint('Train data dimension: ', train_df.shape)\nprint('Test data dimension: ', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df['target'].values\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\ny_train_aux = train_df[['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n\ndef convert_to_bool(df, col_name):\n    train_df[col_name] = np.where(train_df[col_name] >= 0.5, True, False)\n    \nfor col in ['target'] + identity_columns:\n        convert_to_bool(train_df, col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicity_ann = np.sqrt(train_df.toxicity_annotator_count)\nidentity_ann = np.sqrt(train_df.identity_annotator_count)\n\nidentity_ann = identity_ann.clip_upper(np.percentile(identity_ann, 95))\ntoxicity_ann = toxicity_ann.clip_upper(np.percentile(toxicity_ann, 95))\n\nweights = np.ones((len(train_df),)) / 4 #* toxicity_ann\n# Subgroup\nweights += train_df[identity_columns].sum(axis=1).astype(np.int) / 4 #* identity_ann\n# Background Positive, Subgroup Negative\nweights += ((train_df['target'].astype(np.int) +\n   (~train_df[identity_columns]).sum(axis=1).astype(np.int) ) > 1 ).astype(np.int) / 4 #* (identity_ann + toxicity_ann) / 2\n# Background Negative, Subgroup Positive\nweights += ((~train_df['target']).astype(np.int) +\n   (train_df[identity_columns].sum(axis=1).astype(np.int) ) > 1 ).astype(np.int) / 4 #* (identity_ann + toxicity_ann) / 2\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([y_train, weights, toxicity_ann]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAXLEN = 220\nBATCH_SIZE = 16\nN_EPOCHS = 2\nFOLDS = 4\nMODEL = \"bert_uncased\"\n\nif not os.path.exists(MODEL):\n    os.makedirs(MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# for df in [train_df, test_df]:\n#     df[\"comment_text\"] = df[\"comment_text\"].progress_apply(lambda x: tokenizer.tokenize(x.lower()))\n# for df in [train_df, test_df]:\n#     df[\"tokens\"] = df[\"comment_text\"].progress_apply(lambda x:\\\n#                                         tokenizer.convert_tokens_to_ids(x[:MAXLEN]))\n# x_train = train_df.logits.values   \n\nx_train = np.load(\"../input/bert-train/train2.npy\",  allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    MODEL + '/pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', MODEL + '/bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_final_auc(indexes, preds):\n    SUBGROUP_AUC = 'subgroup_auc'\n    BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n    BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\n    def compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true >= 0.5, y_pred)\n        except ValueError:\n            return np.nan\n\n    def compute_subgroup_auc(df, subgroup, label, model_name):\n        subgroup_examples = df[df[subgroup]]\n        return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\n    def compute_bpsn_auc(df, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n        subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n        non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n        examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n        return compute_auc(examples[label], examples[model_name])\n\n    def compute_bnsp_auc(df, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n        subgroup_positive_examples = df[df[subgroup] & df[label]]\n        non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n        examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n        return compute_auc(examples[label], examples[model_name])\n\n    def compute_bias_metrics_for_model(dataset,\n                                       subgroups,\n                                       model,\n                                       label_col,\n                                       include_asegs=False):\n        \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n        records = []\n        for subgroup in subgroups:\n            record = {\n                'subgroup': subgroup,\n                'subgroup_size': len(dataset[dataset[subgroup] >= 0.5])\n            }\n            record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n            record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n            record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n            records.append(record)\n        return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n\n    def convert_to_bool(df, col_name):\n        df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n\n    for ic in ['target'] + identity_columns:\n        convert_to_bool(train_df, ic)\n\n    def calculate_overall_auc(df, model_name):\n        true_labels = df[\"target\"]\n        predicted_labels = df[model_name]\n        return roc_auc_score(true_labels, predicted_labels)\n\n    def power_mean(series, p):\n        total = sum(np.power(series, p))\n        return np.power(total / len(series), 1 / p)\n\n    def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n        bias_score = np.average([\n            power_mean(bias_df[SUBGROUP_AUC], POWER),\n            power_mean(bias_df[BPSN_AUC], POWER),\n            power_mean(bias_df[BNSP_AUC], POWER)\n        ])\n        return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \n    train_df.loc[indexes, \"predictions\"] = preds\n    bias_metrics_df = compute_bias_metrics_for_model(train_df.loc[indexes], identity_columns, \"predictions\", 'target')\n    final_auc = get_final_metric(bias_metrics_df, calculate_overall_auc(train_df.loc[indexes], \"predictions\"))\n    train_df.drop(\"predictions\", axis=1, inplace=True)\n    \n    return final_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(BertPreTrainedModel):\n    def __init__(self, config, num_labels):\n        super(NeuralNet, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n    \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits\n\ndef pad_batch(batch):\n    sequences, targets, targets_aux = zip(*batch)\n    max_length = max([len(s) for s in sequences])\n\n    return pad_sequences(sequences, maxlen=min(max_length, MAXLEN)), targets, targets_aux\n            \nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, sequences, targets, targets_aux, is_test = False):\n        self.sequences = sequences\n        self.lengths = [len(x) for x in sequences]\n        self.targets = targets\n        self.targets_aux = targets_aux\n        self.is_test = is_test\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, index):\n        if self.is_test:\n            return (self.sequences[index], None, None)\n        else:\n            return self.sequences[index], self.targets[index], self.targets_aux[index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(model, valid_loader):\n    model.eval()\n    valid_preds_fold = np.zeros((len(valid_idx)))\n\n    for i, (x_batch, y_batch, _) in enumerate(valid_loader):\n        x_batch = torch.tensor(x_batch, dtype=torch.long).cuda()        \n        y_batch = torch.tensor(y_batch, dtype=torch.float32).cuda()\n\n        y_pred = model(x_batch, attention_mask=(x_batch > 0).cuda()).detach()\n        \n        valid_preds_fold[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = y_pred.sigmoid().cpu().numpy()[:, 0]\n    \n    valid_preds_fold = valid_preds_fold[val_sampler.get_reverse_indexes()]\n    elapsed_time = time.time() - start_time \n    roc_val = roc_auc_score(y_train[valid_idx, 0] > 0.5, valid_preds_fold)\n    b_roc_val = get_final_auc(valid_idx, valid_preds_fold)\n    \n    print('Epoch {}/{} \\t roc_auc={:.4f} \\t b_roc_auc={:.4f} \\t time={:.2f}s'.format(\n        epoch + 1, N_EPOCHS, roc_val, b_roc_val, elapsed_time))\n    \n    return valid_preds_fold, b_roc_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_checkpoint(checkpoint_path, model, optimizer):\n    checkpoint_path = os.path.join(MODEL, checkpoint_path)\n    state = {'state_dict': model.state_dict()}\n    if optimizer:\n        state['optimizer'] = optimizer.state_dict()\n    torch.save(state, checkpoint_path)\n    print('model saved to %s' % checkpoint_path)\n    \ndef load_checkpoint(checkpoint_path, model, optimizer):\n    checkpoint_path = os.path.join(MODEL, checkpoint_path)\n    state = torch.load(checkpoint_path)\n    model.load_state_dict(state['state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(state['optimizer'])\n    print('model loaded from %s' % checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  bucket iterator\ndef divide_chunks(l, n):\n    if n == len(l):\n        yield np.arange(len(l), dtype=np.int32), l\n    else:\n        # looping till length l\n        for i in range(0, len(l), n):\n            data = l[i:i + n]\n            yield np.arange(i, i + len(data), dtype=np.int32), data\n\n\ndef prepare_buckets(lens, bucket_size, batch_size, shuffle_data=True, indices=None):\n    lens = -lens\n    assert bucket_size % batch_size == 0 or bucket_size == len(lens)\n    if indices is None:\n        if shuffle_data:\n            indices = shuffle(np.arange(len(lens), dtype=np.int32))\n            lens = lens[indices]\n        else:\n            indices = np.arange(len(lens), dtype=np.int32)\n    new_indices = []\n    extra_batch = None\n    for chunk_index, chunk in (divide_chunks(lens, bucket_size)):\n        # sort indices in bucket by descending order of length\n        indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n        batches = []\n        for _, batch in divide_chunks(indices_sorted, batch_size):\n            if len(batch) == batch_size:\n                batches.append(batch.tolist())\n            else:\n                assert extra_batch is None\n                assert batch is not None\n                extra_batch = batch\n        # shuffling batches within buckets\n        if shuffle_data:\n            batches = shuffle(batches)\n        for batch in batches:\n            new_indices.extend(batch)\n\n    if extra_batch is not None:\n        new_indices.extend(extra_batch)\n    return indices[new_indices]\n\n\nclass BucketSampler(torch.utils.data.Sampler):\n\n    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1536, shuffle_data=True):\n        super().__init__(data_source)\n        self.shuffle = shuffle_data\n        self.batch_size = batch_size\n        self.sort_keys = sort_keys\n        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n        if not shuffle_data:\n            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n                                         shuffle_data=self.shuffle)\n        else:\n            self.index = None\n        self.weights = None\n\n    def set_weights(self, w):\n        assert w >= 0\n        total = np.sum(w)\n        if total != 1:\n            w = w / total\n        self.weights = w\n\n    def __iter__(self):\n        indices = None\n        if self.weights is not None:\n            total = len(self.sort_keys)\n            \n            indices = np.random.choice(total, (total,), p=self.weights)\n        if self.shuffle:\n            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n                                         shuffle_data=self.shuffle, indices=indices)\n\n        return iter(self.index)\n\n    def get_reverse_indexes(self):\n        indexes = np.zeros((len(self.index),), dtype=np.int32)\n        for i, j in enumerate(self.index):\n            indexes[j] = i\n        return indexes\n\n    def __len__(self):\n        return len(self.sort_keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = np.zeros((len(train_df)))\nsplits = list(MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42).split(x_train, train_df[identity_columns + [\"target\"]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything()\nFOLD = 0\nfor fold, (train_idx, valid_idx) in enumerate(splits):   \n    if fold != FOLD:\n        continue\n    train = Dataset([x_train[i] for i in train_idx], \n                    y_train[train_idx],\n                    y_train_aux[train_idx])\n    valid = Dataset([x_train[i] for i in  valid_idx], \n                    y_train[valid_idx],\n                    y_train_aux[valid_idx])\n    \n    model = NeuralNet.from_pretrained(MODEL,\n            num_labels=6)\n    model.cuda()\n    \n    loss_fn = torch.nn.functional.binary_cross_entropy_with_logits\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-5)\n    scheduler = lr_scheduler.StepLR(optimizer, gamma=1/4, step_size=1)\n\n    train_sampler = BucketSampler(train, \n                     np.array(train.lengths), \n                     bucket_size=BATCH_SIZE*1000,\n                     batch_size=BATCH_SIZE,\n                     shuffle_data=True)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, \n                sampler=train_sampler, collate_fn=pad_batch)\n    \n    val_sampler = BucketSampler(valid, \n                     np.array(valid.lengths), \n                     batch_size=BATCH_SIZE,\n                     shuffle_data=False)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, \n                sampler=val_sampler, collate_fn=pad_batch)\n    \n    \n    print('Fold ', fold)\n    f1_best_best = 0.91\n    \n    for epoch in range(N_EPOCHS):\n        start_time = time.time()\n        model.train()\n        \n        for itr, (x_batch, y_batch, y_batch_aux) in tqdm(enumerate(train_loader), disable=True):\n            x_batch = torch.tensor(x_batch, dtype=torch.long).cuda()\n            y_batch = torch.tensor(y_batch, dtype=torch.float32).cuda()\n            y_batch_aux = torch.tensor(y_batch_aux, dtype=torch.float32).cuda()\n\n            y_pred = model(x_batch, attention_mask=(x_batch > 0).cuda())\n \n            loss = loss_weight * loss_fn(y_pred[:, 0], y_batch[:,0], y_batch[:,1])\n            loss += loss_fn(y_pred[:, 1:], y_batch_aux)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n           \n        valid_preds_fold, f1_best = validate(model, valid_loader)\n        if f1_best > f1_best_best:\n            f1_best_best = f1_best\n            train_preds[valid_idx] = valid_preds_fold\n            save_checkpoint(\"f{}.pth\".format(fold), model, None)\n\n        scheduler.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}