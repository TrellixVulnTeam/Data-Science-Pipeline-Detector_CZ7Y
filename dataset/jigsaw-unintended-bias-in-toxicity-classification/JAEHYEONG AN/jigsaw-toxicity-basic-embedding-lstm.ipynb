{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\n# nltk.download('stopwords')\n# nltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) \nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score\nfrom keras import models, layers, Model\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Function"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## Clean Punctuation & Stopwords\nclass clean_text:\n\tdef __init__(self, text):\n\t\tself.text = text\n\t\n\t# Remove Punctuation\n\tdef rm_punct(text):\n\t\tpunct = set([p for p in \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'])\n\t\ttext = [t for t in text if t not in punct]\n\t\t\t\n\t\treturn \"\".join(text)\n\n\t# Remove Stopwords\n\tdef rm_stopwords(text):\n\t\tword_tokens = word_tokenize(text)   \n\t\tresult = [w for w in word_tokens if w not in stop_words]\n\t\t\t\t\n\t\treturn \" \".join(result)\n\ndef Embedding_CuDNNLSTM_model(max_words, max_len):\n    sequence_input = layers.Input(shape=(None, ))\n    x = layers.Embedding(max_words, 128, input_length=max_len)(sequence_input)\n    x = layers.SpatialDropout1D(0.3)(x)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x)\n    \n    avg_pool1d = layers.GlobalAveragePooling1D()(x)\n    max_pool1d = layers.GlobalMaxPool1D()(x)\n    \n    x = layers.concatenate([avg_pool1d, max_pool1d])\n    x = layers.Dense(32, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = models.Model(sequence_input, output)\n    \n    return model\n    \n\ndef auroc(y_true, y_pred):\n\treturn tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## load data\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Set index & target label"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_data[['id','comment_text','target']]\ntest_df = test_data.copy()\n\n# set index\ntrain_df.set_index('id', inplace=True)\ntest_df.set_index('id', inplace=True)\n\n# y_label\ntrain_y_label = np.where(train_df['target'] >= 0.5, 1, 0) # Label 1 >= 0.5 / Label 0 < 0.5\ntrain_df.drop(['target'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio by Class\nCounter(train_y_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. View text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['comment_text'].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Remove Punctuation & Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove punctuation \ntrain_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))\n# remove stopwords\nX_train = train_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))\nX_test = test_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Tokenize"},{"metadata":{"trusted":true},"cell_type":"code","source":"## tokenize\nmax_words = 100000\ntokenizer = text.Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\n# texts_to_sequences\nsequences_text_train = tokenizer.texts_to_sequences(X_train)\nsequences_text_test = tokenizer.texts_to_sequences(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_text_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add padding\nmax_len = max(len(l) for l in sequences_text_train)\npad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)\npad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Embedding + LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## embedding_lstm models \nmodel = Embedding_CuDNNLSTM_model(max_words, max_len)\n\n# model compile\nmodel.compile(optimizer='adam',\n\t\t\t loss='binary_crossentropy', metrics=['acc', auroc])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras.callbacks\ncallbacks_list = [\n\t\tReduceLROnPlateau(\n\t\t\tmonitor='val_auroc', patience=2, factor=0.1, mode='max'),\t# val_loss가 patience동안 향상되지 않으면 학습률을 0.1만큼 감소 (new_lr = lr * factor)\n\t\tEarlyStopping(\n\t\t\tpatience=5, monitor='val_auroc', mode='max', restore_best_weights=True)\n]\n\nhistory = model.fit(pad_train, train_y_label,\n\t\t\t\t\tepochs=7, batch_size=1024,\n\t\t\t\t\tcallbacks=callbacks_list, \n\t\t\t\t\tvalidation_split=0.3, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot score by epochs\nauroc = history.history['auroc']\nval_auroc = history.history['val_auroc']\nepochs = range(1, len(auroc)+1)\n\nplt.figure(figsize=(7,3))\nplt.plot(epochs, auroc, 'b', label='auroc')\nplt.plot(epochs, val_auroc, 'r', label='validation auroc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"## predict test_set\ntest_pred = model.predict(pad_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. submit submission.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_result = pd.DataFrame()\nsample_result['id'] = test_df.index\nsample_result['prediction'] = test_pred\n\n## submit sample_submission.csv\nsample_result.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}