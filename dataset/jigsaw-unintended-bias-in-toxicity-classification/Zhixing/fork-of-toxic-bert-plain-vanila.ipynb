{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/nvidiaapex/repository/NVIDIA-apex-39e153a\"))\n#print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n#print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n#print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Installing Nvidia Apex\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidiaapex/repository/NVIDIA-apex-39e153a","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device=torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nEPOCHS = 1\nData_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\"\n# full_length=1804874\nnum_to_load=1804800\nvalid_size= 0                         #Validation Size\n# num_to_load=full_length-valid_size                         #Train size to match time limit\n# num_to_load=20000                         #Train size to match time limit\n# valid_size= 5000                         #Validation Size\nTOXICITY_COLUMN = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the Bart Pytorch repo to the PATH\n# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam,OpenAIAdam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Translate model from tensorflow to pytorch\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the Bert configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig('../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'+'bert_config.json')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def convert_line(tl, max_seq_length,tokenizer):\n#     max_seq_length -=2\n#     tokens_a = tokenizer.tokenize(tl)\n#     if len(tokens_a)>max_seq_length:\n#         tokens_a = tokens_a[:max_seq_length]\n#     one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n#     return one_token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"%%time\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\nnp.random.seed(SEED)\nchosen_idx = np.random.choice(num_to_load+valid_size,size = num_to_load+valid_size,replace=False) \ntrain_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).iloc[chosen_idx] \n# train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(num_to_load+valid_size,random_state=SEED)\nprint('loaded %d records' % len(train_df))\n\n# Make sure all comment_text values are strings\ntrain_df['comment_text'] = train_df['comment_text'].astype(str)\n# train_df['comment_text'] = train_df['comment_text'].progress_apply(lambda x:preprocess(x))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\nsequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# from joblib import Parallel, delayed\n# train_lines = train_df['comment_text'].fillna(\"DUMMY_VALUE\").values.tolist()\n# sequences1 = Parallel(n_jobs=2, backend='multiprocessing')(delayed(convert_line)(i, MAX_SEQUENCE_LENGTH, tokenizer) for i in train_lines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import multiprocessing\n# from functools import partial\n# cpunum = multiprocessing.cpu_count()\n# pool = multiprocessing.Pool(cpunum)\n# # pool.map(partial(func, b=second_arg), a_args)\n# sequence = pool.map(partial(convert_line, max_seq_length=MAX_SEQUENCE_LENGTH,tokenizer=tokenizer),\n#                         (i for i in train_df['comment_text'].fillna(\"DUMMY_VALUE\").values.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train_df.fillna(0)\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nx_train = train_df['comment_text']\ntrain_df = train_df.drop(['comment_text'],axis=1)\n# convert target to 0,1\ntrain_df['target']=(train_df['target']>=0.5).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = sequences[:num_to_load]                \n# y = train_df[y_columns].values[:num_to_load]\nX_val = sequences[num_to_load:]                \n# y_val = train_df[y_columns].values[num_to_load:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall\nweights = np.ones((len(train_df),)) / 4\n# Subgroup\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n    (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n    (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_columns=['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat','sexual_explicit']\n# y_train=np.vstack([train_df['target'],weights]).T\n# y_train=np.concatenate((y_train,train_df[y_columns]),axis=1)\ny_train=np.array(train_df[y_columns])\ny_train = np.hstack((y_train, np.reshape(weights, (-1, 1))))\n# y_train = np.hstack((y_train, train_df[y_columns]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val = y_train[num_to_load:]\ny = y_train[:num_to_load]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=train_df.tail(valid_size).copy()\ntrain_df=train_df.head(num_to_load)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n#         print(length)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long),torch.tensor(y,dtype=torch.float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,-1:])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,1:-1])\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def custom_loss(data, targets):\n#     ''' Define custom loss function for weighted BCE on 'target' column '''\n#     bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n#     bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n#     return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, y_train, sequences, X\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_model_file = \"bert_pytorch.bin\"\n\nlr=2e-5\nbatch_size = 32\naccumulation_steps= 1\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nmodel = BertForSequenceClassification.from_pretrained(\"../working\",cache_dir=None,num_labels=7)\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n\noptimizer = OpenAIAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils import data\nfrom tqdm import tqdm_notebook as tqdm\n\nclass LenMatchBatchSampler(data.BatchSampler):\n    def __iter__(self):\n\n        buckets = [[]] * 100\n        yielded = 0\n\n        for idx in self.sampler:\n            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n            count_zeros = int(count_zeros / 64) \n            if len(buckets[count_zeros]) == 0:  buckets[count_zeros] = []\n\n            buckets[count_zeros].append(idx)\n\n            if len(buckets[count_zeros]) == self.batch_size:\n                batch = list(buckets[count_zeros])\n                yield batch\n                yielded += 1\n                buckets[count_zeros] = []\n\n        batch = []\n        leftover = [idx for bucket in buckets for idx in bucket]\n\n        for idx in leftover:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yielded += 1\n                yield batch\n                batch = []\n\n        if len(batch) > 0 and not self.drop_last:\n            yielded += 1\n            yield batch\n\n        assert len(self) == yielded, \"produced an inccorect number of batches. expected %i, but yielded %i\" %(len(self), yielded)\n\ndef trim_tensors(tsrs):\n    max_len = torch.max(torch.sum( (tsrs != 0  ), 1))\n    if max_len > 2: \n        tsrs = tsrs[:,:max_len]\n    return tsrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n#     train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    ran_sampler = data.RandomSampler(train_dataset)\n    len_sampler = LenMatchBatchSampler(ran_sampler, batch_size = 32, drop_last = False)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler = len_sampler)\n    \n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()\n    for i,(x_batch, y_batch) in tk0:\n#         x_batch=x_batch[0]\n#         optimizer.zero_grad()\n        x_batch = trim_tensors(x_batch)\n#         print(x_batch.shape)\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n#         loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n#         print(y_pred,y_batch)\n        loss = custom_loss(y_pred,y_batch.to(device))\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n# torch.save(model.state_dict(), output_model_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Run validation\n# The following 2 lines are not needed but show how to download the model for prediction\n# model = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\n# model.load_state_dict(torch.load(output_model_file ))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.eval()\nvalid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From baseline kernel\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>=0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>=0.5]\n    return compute_auc((subgroup_examples[label]>=0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>=0.5) & (df[label]<0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<0.5) & (df[label]>=0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>=0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>=0.5) & (df[label]>=0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<0.5) & (df[label]<0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>=0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>=0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'model1'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\nbias_metrics_df\nget_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = {'state_dict': model.state_dict()}\n# ,'optimizer': optimizer.state_dict()}\ntorch.save(state, 'bert_checkpoint_part1.bin')\n\n#model, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer, losslogger)\n#model = model.to(device)\n## now individually transfer the optimizer parts...\n#for state in optimizer.state.values():\n#    for k, v in state.items():\n#        if isinstance(v, torch.Tensor):\n#            state[k] = v.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}