{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.keras.layers import LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_MODELS = 1\nBATCH_SIZE = 512\nLSTM_UNITS = 64\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nEPOCHS = 1\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv',low_memory=True,nrows = 5000) #Use full data here\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv',low_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = preprocess(train['comment_text'])\n#y_train = np.where(train['target'] >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = preprocess(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words) #Finds word embeddings for each word\n    x = SpatialDropout1D(0.3)(x) #This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements\n    x = LSTM(LSTM_UNITS, return_sequences=True)(x)\n    x = LSTM(LSTM_UNITS, return_sequences=True)(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x), \n        GlobalAveragePooling1D()(x),#layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input \n        #of variable length in the simplest way possible.\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS)(hidden)]) #This fixed-length output vector is piped through a fully-connected (Dense) layer with x hidden units.\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS)(hidden)])\n    #result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    model = Model(inputs=words, outputs=aux_result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(embedding_matrix, y_aux_train.shape[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_model = model.fit(\n            x_train,\n            y_aux_train,\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_aux_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_train[1].reshape(1,220))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}