{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\n\nfrom string import punctuation\nfrom keras.models import Sequential,Model\nfrom keras.layers import Embedding,Input,Activation,Flatten,CuDNNLSTM,Dense,Dropout,Bidirectional,LSTM,MaxPool1D\nfrom keras.layers import Convolution1D,GlobalAveragePooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LeakyReLU\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport re\nimport gc\nimport seaborn as sns\n%matplotlib inline\ntqdm.pandas()","execution_count":1,"outputs":[{"output_type":"stream","text":"['crawlembedding', 'gloveembeddings', 'jigsaw-unintended-bias-in-toxicity-classification']\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Reading the CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Converting Target Probabilities to 0 or 1 for making it categorical. All the values in target column that are above 0.5 are considered as 1 and rest are considered as 0.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def target(value):\n    if value>=0.5:\n        return 1\n    else:\n        return 0","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'] = df['target'].apply(target)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Getting out X and y variable. We will be training on the X variable and predicting the Y variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df['comment_text']\ny = df['target']","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using seaborn library to check the number of observations in each class that is 0 and 1. Countplot helps in visualization of the values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fd8827542e8>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Performing some cleaning in the commnet text using regular expression. This code with help us extract only the characters from the expression.\n**\n**Tutorial for regular expressions can be found** [here](https://www.w3schools.com/python/python_regex.asp)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaning(text):\n    text = text.lower()\n    text = re.sub(r'\\W+',' ',text)\n    return text","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.progress_apply(cleaning)","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 1804874/1804874 [00:39<00:00, 45222.56it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**Using word embeddings so that words with similar words have similar representation in vector space. It represents every word as a vector. The words which have similar meaning are place close to each other. Quick understanding can be done from [this](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) article.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('../input/gloveembeddings/glove.6B.100d.txt')\nembedding_values = {}\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_values[word]=coef","execution_count":10,"outputs":[{"output_type":"stream","text":"400000it [00:10, 36930.83it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**These are the steps that needs to be performed so that we can convert each word of our vocabulary into a unique integer. Tokenizer is initalized in first step. Then fitting on the text will help us create a vocabulary so that each word is assigned with a unique integer. Then we convert in the whole sentence of the comment into a sequence of numbers which are assigned by the tokenizer.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"token = Tokenizer()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token.fit_on_texts(x)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = token.texts_to_sequences(x)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sequence)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"1804874"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(sequence)+1","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Padding the sequence helps in making all the sentence of same length. maxlen is the parameter which decides the length we want to assign to all the sentences. Padding is done by adding 0 on either the end of sentence or prior the sentence if the sentence is having length less than max length. This is also a parameter which user can change, by defaults its prefix. If the length of the sentence is more than 100 then it is pruned which brings down the length to 100 (maxlen)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_seq = pad_sequences(sequence,maxlen = 100)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will be converting each word in our vocabulary into word embeddings. This embedding is vector of 1x100 dimension which represents each word as a vector and placing them into a vector space. Embedding matrix is created in which the number assigned to the word by tokenizer is assigned with the corresponding vector which we get from the glove embeddings.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_emb = np.stack(embedding_values.values())\nall_mean,all_std = all_emb.mean(),all_emb.std()\nall_mean,all_std","execution_count":18,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"(0.004451992, 0.4081574)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.random.normal(all_mean,all_std,(vocab_size,100))\nfor word,i in tqdm(token.word_index.items()):\n    values = embedding_values.get(word)\n    if values is not None:\n        embedding_matrix[i] = values","execution_count":19,"outputs":[{"output_type":"stream","text":"100%|██████████| 309011/309011 [00:00<00:00, 696869.92it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Now we start building the model with Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here this embedding layer is important as this will help us in training of the sentences with their respective embeddings whihc we have assigned above. The first parameter is the size of our vocabulary. Second parameter is the output embeddings length which is 100 in this case as we used the 100 glove embeddings of each word. The length of each observation which is expected by the network is given by input_length parameter. We have padded all the observations to 100 hence we set input_length = 10. Weights parameter shows that the embeddings which we want to use is embeddings_matrix and it should not be altered hence trainable is kept false.\nIf we want to train our own embeddings we can simply remove the weights and trainable parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Embedding(vocab_size,100,input_length = 100,weights = [embedding_matrix],trainable = False))","execution_count":21,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Building a LSTM model. LSTM networks are useful in sequence data as they are capable of remembering the past words which help them in understanding the meaning of the sentence which helps in text classification. Bidirectional Layer is helpful as it helps in understanding thesentence from start to end and also from end to start. It works in both the direction. This is useful as the reverse order LSTM layer is capable of learning patterns which are not possible for the normal LSTM layers which goes from start to end of the sentence in the normal order. Hence Bidirectional layers are useful in text classification problems as different patterns can be captured from 2 directions.\nCuDNNLSTM is same as LSTM. If you are using GPU then CuDNNLSTM will be faster but if you are using CPU please use LSTM."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\nmodel1.add(Convolution1D(64,7,padding='same'))\nmodel1.add(GlobalAveragePooling1D())","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Dense(128))\nmodel1.add(LeakyReLU())","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Dense(64,activation = 'relu'))","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Dense(1,activation = 'sigmoid'))","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(pad_seq,y,test_size = 0.15,random_state = 42)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del (x,y)\ngc.collect()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"10"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(x_train,y_train,epochs = 5,batch_size=128,validation_data=(x_test,y_test))","execution_count":null,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 1534142 samples, validate on 270732 samples\nEpoch 1/5\n 983040/1534142 [==================>...........] - ETA: 1:14 - loss: 0.1592 - acc: 0.9425","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a graph between Training and Testing Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a graph between Training and Testing Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing of the test data so that model can easily make its prediction as it should be in the same format as that of our training data.\nNote that we are using the same tokenizer in our testing and we are not fitting it again becuase this might change the numbers assigned to words which are there in the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequence = token.texts_to_sequences(X)\ntest_pad_seq = pad_sequences(test_sequence,maxlen = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction1 = model1.predict(test_pad_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1 = pd.DataFrame([test['id']]).T\nsubmission1['prediction'] = prediction1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple LSTM layers are added without any Bidirectional Layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model2 = Sequential()\n# model2.add(Embedding(vocab_size,100,input_length=100,weights = [embedding_matrix],trainable = False))\n# model2.add(CuDNNLSTM(75,return_sequences=True))\n# model2.add(CuDNNLSTM(75))\n# model2.add(Dense(128,activation='relu'))\n# model2.add(Dropout(0.3))\n# model2.add(Dense(1,activation='sigmoid'))\n# model2.compile(optimizer= 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n# history = model2.fit(x_train,y_train,batch_size = 128,epochs = 5,validation_data = (x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction2 = model2.predict(test_pad_seq)\nsubmission2 = pd.DataFrame([test['id']]).T\nsubmission2['prediction'] = prediction2\nsubmission2.to_csv('submission_model2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will try to build a model only with convolution layers and not including any LSTM layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model3 = Sequential()\n# model3.add(Embedding(vocab_size,100,input_length=100,weights = [embedding_matrix],trainable = False))\n# model3.add(Convolution1D(32,5,activation='relu'))\n# model3.add(MaxPool1D(2,2))\n# model3.add(Convolution1D(64,5,activation='relu'))\n# model3.add(MaxPool1D(2,2))\n# model3.add(Flatten())\n# model3.add(Dense(128,activation='relu'))\n# model3.add(Dropout(0.2))\n# model3.add(Dense(1,activation='sigmoid'))\n# model3.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n# history = model3.fit(x_train,y_train,batch_size = 128,epochs = 5,validation_data = (x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction3 = model3.predict(test_pad_seq)\nsubmission3 = pd.DataFrame([test['id']]).T\nsubmission3['prediction'] = prediction3\nsubmission3.to_csv('submission_model3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining output of all models with 1/3 ratio and making a new prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction4 = 0.33*submission1['prediction']+0.33*submission2['prediction']+0.33*submission3['prediction']\nsubmission4 = pd.DataFrame([test['id']]).T\nsubmission4['prediction'] = prediction4\nsubmission4.to_csv('submission4.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}