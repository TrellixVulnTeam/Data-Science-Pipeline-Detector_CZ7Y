{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Analysis of subgroup AUCs from a BERT Baseline\n\nThe results used for this analysis are based on a straightforward fine-tuning of BERT, using `run_classifier.py`, available [here](https://github.com/google-research/bert). I used a sequence length of 128 tokens and trained for 1 epoch.\n\nThe analysis clearly shows that I need more civil (non-toxic) examples mentioning from the groups `Muslim` and `homosexual_gay_or_lesbian` to improve training.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A random sample of 100k examples was taken from the training set provided for this competition. This was split into a dev set of 50k and `test.csv` in the code below. The `predictions.csv` file here is based predictions from the BERT model described above."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.read_csv(\"../input/bert-baseline/predictions.csv\")\ndf = pd.read_csv(\"../input/bert-baseline/test.csv\")\ndf['prediction'] = pred[' Toxic']\ndf['target'] = df['target'] >= 0.5\ndf['bool_pred'] = df['prediction'] >= 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, a simple AUC function taking a dataframe as input and using sklearn for the calculation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc(df):\n    y = df['target']\n    pred = df['prediction']\n    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n    return metrics.auc(fpr, tpr)\n\noverall = auc(df)\noverall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 9 subgroups have enough examples in the competition test set to be included in the bias calculation. Other subgroups are ignored as part of the background for this competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"groups = ['black', 'white', 'male', 'female',\n          'christian', 'jewish', 'muslim',\n          'psychiatric_or_mental_illness',\n          'homosexual_gay_or_lesbian']\n\ncategories = pd.DataFrame(columns = ['SUB', 'BPSN', 'BNSP'], index = groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe Mp function below is based on the [evaluation formula](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation) this competition. This formula is then applied to the 9 subgroups and their BPSN and BNSP datasets.\n\n* **SUB**\n: Claculates an AUC using only examples from the sub-group.\n* **BPSN**\n: Background Positive Subgroup Negative. Claculates an AUC using a subset of toxic comments outside the sub-group (BP) and non-toxic comments in the sub-group (SN).\n* **BNSP**\n: Background Negative Subgroup Positive. Claculates an AUC using a subset of non-toxic comments outside the sub-group (BN) and toxic comments in the sub-group (SP)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef Mp(data, p=-5.0):\n    return np.average(data ** p) ** (1/p)\n\nfor group in groups:\n    df[group] = df[group] >= 0.5\n    categories.loc[group,'SUB'] = auc(df[df[group]])\n    bpsn = ((~df[group] & df['target'])    #background positive\n            | (df[group] & ~df['target'])) #subgroup negative\n    categories.loc[group,'BPSN'] = auc(df[bpsn])\n    bnsp = ((~df[group] & ~df['target'])   #background negative\n            | (df[group] & df['target']))  #subgrooup positive\n    categories.loc[group,'BNSP'] = auc(df[bnsp])\n\ncategories.loc['Mp',:] = categories.apply(Mp, axis= 0)\ncategories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting results. Clearly I need to modify my input pipeline to include more subgroup examples, especially `homosexual` and `Muslim` examples from the BPSN subset. In other words, I need more civil (non-toxic) examples mentioning Muslims and homosexuals to improve the leader board score."},{"metadata":{},"cell_type":"markdown","source":"Finally, the final leader-board score."},{"metadata":{"trusted":true},"cell_type":"code","source":"leaderboard = (np.sum(categories.loc['Mp',:]) + overall) / 4\nleaderboard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" This same vanilla BERT model gave me an actual score of `0.9223` on the competition leader board. Sy my randomly sampled 50k test set must have easier examples in the sub-groups than the official test set of 97k examples."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}},"nbformat":4,"nbformat_minor":1}