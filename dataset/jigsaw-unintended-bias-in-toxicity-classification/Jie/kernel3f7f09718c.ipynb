{"cells":[{"metadata":{},"cell_type":"markdown","source":"featrue engineering"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nMAX_FEATURE = 100000\nMAX_LEN = 100\nEMBEDDING_DIM = 300\nBATCH_SIZE = 512\nEPOCHS = 2\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain_comments = train[\"comment_text\"].astype(str)\ntrain_targets = train[\"target\"].astype(\"float32\")\ntest_comments = test[\"comment_text\"].astype(str)\ntest_ids = test[\"id\"].astype(str)\n\ntokenizer = Tokenizer(MAX_LEN)\ntokenizer.fit_on_texts(train_comments)\nsequences = tokenizer.texts_to_sequences(train_comments)\ntrain_x = pad_sequences(sequences, MAX_LEN)\n\ntokenizer.fit_on_texts(test_comments)\nsequences = tokenizer.texts_to_sequences(test_comments)\ntest_x = pad_sequences(sequences, MAX_LEN)\ntrain_y = np.where(train_targets >= 0.5, 1, 0)\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create model"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\nfrom keras import Input\n\n\ninput_tensor = Input(shape=(MAX_LEN,))\nx = layers.Embedding(MAX_FEATURE, EMBEDDING_DIM, input_length=MAX_LEN)(input_tensor)\nx = layers.Bidirectional(layers.LSTM(32 ,return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n#attention = Attention.Attention(max_len)(x)\n#attention = SeqSelfAttention(attention_activation='sigmoid')(x)\n#attention = layers.GlobalMaxPooling1D()(attention)\nx = layers.GlobalMaxPooling1D()(x)\n#x = layers.concatenate([attention, x])\n#x = layers.Dense(32, activation=\"relu\")(x)\noutput_tensor = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = models.Model(input_tensor, output_tensor)\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\nmodel.summary()","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'MAX_LEN' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4a4f64c53757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_FEATURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MAX_LEN' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"start to train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_x, train_y, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_x, batch_size=BATCH_SIZE, verbose=1)\npredictions = predictions.ravel()\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test_ids,\n    'prediction': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}