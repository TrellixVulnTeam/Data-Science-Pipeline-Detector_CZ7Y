{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This is my solution to [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview).**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple, Dict\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# a list of paths of embedding matrix files\n# Two embedding matrixes will be combined together and used in the embedding layer of the RNN.\n# They are CRAWL and GLOVE. Each of them is a collection of 300-dimension vector.\n# Each vector represents a word.\n# The coverage of words of CRAWL is different from that of GLOVE.\nEMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\n# <NUM_MODELS> represents the amount of times the same model should be trained\n# Although each training is using the same RNN model, the predictions will be slightly different\n# from each other due to different initialization (He Initialization).\nNUM_MODELS = 2\n\n# amount of epoch during training, this number is mainly limited by the GPU quota during committing.\nEPOCHS = 4\n\n# batch size\nBATCH_SIZE = 256\n\n# amount of LSTM units in each LSTM layer\nLSTM_UNITS = 128\n\n# amount of unit in the dense layer\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n\n# maximum length of one comment (one sample)\nMAX_LEN = 220\n\n# column names related to identity in the training set\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# a list of all the label names (Each sample/comment corresponds to multiple labels.)\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\n# column name of the comment column\nTEXT_COLUMN = 'comment_text'\n\n# target column\nTARGET_COLUMN = 'target'\n\n# chars to remove in the comment\n# These chars are not covered by the embedding matrix. \nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word: str, *arr: str) -> (str, np.ndarray):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path: str) -> Dict[str, np.ndarray]:\n    \"\"\"Return a dict by analyzing the embedding matrix file under the path <path>.\"\"\"\n    \n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index: Dict[str, int], path: str, indexesOfWordsContainTrump: List[int]) -> np.ndarray:\n    \"\"\"Return an embedding matrix, which is ready to put into the RNN's embedding-matrix layer.\n    \n    <word_index>: Each word corresponds to a unique index. A word's vector can be found in the embedding\n        matrix using the word's index.\n    <path>: The path where the embedding matrix file is located at.\n    <indexesOfWordsContainTrump>: A list of indexes of words that contain substring \"Trump\" or \"trump\".\n    \"\"\"\n    \n    # get a word-to-vector Dict by analyzing the embedding matrix file under the path <path>\n    embedding_dict = load_embeddings(path)\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    # fill the <embedding_matrix> according to <embedding_dict>\n    # If a tocken/word/string contains substring \"Trump\" or \"trump\", set the tocken/word/string's\n    # vector to be the same as Trump's.\n    # If a tocken/word cannot be found in <embedding_dict>, the tocken/word's vector is set to be zeros.\n    # Otherwise, copy a tocken/word's vector from <embedding_dict> to <embedding_matrix>.\n    for word, i in word_index.items():\n        if(i in indexesOfWordsContainTrump):\n            embedding_matrix[i] = embedding_dict['Trump']\n        else:\n            try:\n                embedding_matrix[i] = embedding_dict[word]\n            except KeyError:\n                pass\n            \n    return embedding_matrix\n\ndef build_model(embedding_matrix: np.ndarray) -> Model:\n    \"\"\"Return a RNN model, which uses bidirectional LSTM.\"\"\"\n    \n    # input layer\n    words = Input(shape=(None,))\n    \n    # embedding matrix layer-this layer should be set to be not trainable.\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    \n    # The dropout operation is used to prevent overfitting.\n    x = SpatialDropout1D(0.2)(x)\n    \n    # two bidirectional LSTM layer\n    # Since it is bidirectional, the output's size is twice the input's. \n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    # flatten the tensor by max pooling and average pooling\n    # Since it is a concatenation of two pooling layer, the output's size is twice the input's.\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    \n    # two dense layers, skip conections trick is used here to prevent gradient's vanishing.\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    \n    # two different output layers\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(len(AUX_COLUMNS), activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the training set and test set offered in this competition\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\n# seperate the targets and the features\nx_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)\n\n# change to continuous target values into discrete target values\n# There are multiple targets. Each of them contains two different classes. They are True and False.\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n\n# One drawback of using Tokenizer is that it will change all characters to lower case.\n# But the words in both CRAW and GLOVE are case sensitive.\n# For example, \"Trump\" and \"trump\" are represented by different vectors in CRAWL or GLOVE.\n# But Tokenizer will change \"Trump\" into \"trump\".\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n\n# A word-to-index Dict will be generated internally after analyzing the train set and the test set.\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n# Replace all the words/tokens in train/test set by the corresponding index according to the\n# internal word-to-index Dict.\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\n# make the length of all the sequencess the same\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign different weights to different samples according to their labels\n# This is because different groups have different effect on the evaluation metric.\n# Another reason is that the evaluation metric is too complicated to be directly used during optimization.\n# The following specific weight assignment is decided after many tries.\nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] & (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) & train_df[IDENTITY_COLUMNS].sum(axis=1) \nsample_weights += (~train_df[TARGET_COLUMN] & train_df['homosexual_gay_or_lesbian'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['black'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['white'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['muslim'] + 0) * 1\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['jewish'] + 0) * 1\nsample_weights /= sample_weights.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexesOfWordsContainTrump = []\n\n# find out all the indexes of the words that contain substring \"Trump\" or \"trump\"\nfor word, index in tokenizer.word_index.items():\n    if(('trump' in word) or ('Trump' in word)):\n        indexesOfWordsContainTrump.append(index)\n\n# The final embedding matrix is a concatenation of CRAWL embedding matrix and GLOVE embedding matrix.\n# So each word is represented by a 600-d vector.\n# In the final matrix, the words that contain substring \"Trump\" or \"trump\" are replaced by \"Trump\".\n# This is found to be able to enhance the model performance by EDA(exploratory data analysis).\n# The reason behind this is that strings like \"Trump\" and \"trumpist\" are related to toxicity,\n# but they are covered neither in CRAWL nor GLOVE.\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, filePath, indexesOfWordsContainTrump) for filePath in EMBEDDING_FILES], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# release memory space by deleting variables that are no longer useful\ndel train_df\ndel tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# <checkpoint_predictions> is a list of predictions generated after each epoch.\ncheckpoint_predictions = []\n# <weights> is a list of weights corresponding to <checkpoint_predictions>.\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        \n        # record predictions after each epoch\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        # Since the predictions tend to be more accurate after more epochs,\n        # the weights is set to grow exponetially.\n        weights.append(2 ** global_epoch)\n\n# get the weighted average of the predictions. The average operation can help prevent overfitting.\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output the averaged predictions to a file for submission\nsubmission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}