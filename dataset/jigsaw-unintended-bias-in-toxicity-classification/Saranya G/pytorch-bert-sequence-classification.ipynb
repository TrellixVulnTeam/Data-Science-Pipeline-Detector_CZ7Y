{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-BERT/pytorch_pretrained_bert/\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to convert sentence into tokenized format\n\ndef convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -=2\n    all_tokens =[]\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"]) +[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED  = 7 \nBATCH_SIZE = 32\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir = None, do_lower_case = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading test dataset\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype('str')\nX_test = convert_lines(example = test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), max_seq_length = MAX_SEQUENCE_LENGTH, tokenizer = tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config , num_labels = 1)\nmodel.load_state_dict(torch.load('../input/bert-inference/bert/bert_pytorch.bin'))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros(len(X_test))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype = torch.long))\ntest_loader =  torch.utils.data.DataLoader(test,batch_size = 32, shuffle = False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask = (x_batch > 0).to(device), labels = None)\n    test_preds[i*32:(i+1)*32] = pred[:,0].detach().cpu().squeeze().numpy()\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id':test_df['id'],\n    'prediction':test_pred\n})\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference:\nhttps://www.kaggle.com/abhishek/pytorch-bert-inference/notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}