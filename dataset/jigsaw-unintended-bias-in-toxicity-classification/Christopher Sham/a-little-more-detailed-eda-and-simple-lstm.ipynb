{"cells":[{"metadata":{},"cell_type":"markdown","source":"*Disclaimer: The Visualizations and dataframes in this kernel may be considered profane, vulgar, or offensive.*"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# change default plot config\nplt.rc('figure', figsize=(14.4, 8.1), dpi=72)\nplt.rc('font', size=13)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['created_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.info()\ndisplay(train_df.head())\ndisplay(train_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Subgroups\ntoxicity_subtypes = [\n    'severe_toxicity', 'obscene', 'identity_attack',\n    'insult', 'threat', 'sexual_explicit'\n]\n\nidentities = [\n    'asian', 'atheist', 'bisexual',\n    'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n    'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n    'jewish', 'latino', 'male', 'muslim', 'other_disability',\n    'other_gender', 'other_race_or_ethnicity', 'other_religion',\n    'other_sexual_orientation', 'physical_disability',\n    'psychiatric_or_mental_illness', 'transgender', 'white'\n]\n\nmetadata = [\n    'created_date', 'publication_id', 'parent_id', 'article_id',\n    'rating', 'funny', 'wow', 'sad', 'likes', 'disagree'\n]\n\nannotation = ['identity_annotator_count', 'toxicity_annotator_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Feture Engineering for visualization\ntrain_df['rating'] = train_df['rating'].map({\"approved\": 1, \"rejected\": 0})\ntrain_df[\"is_toxic\"] = np.where(train_df[\"target\"].values >= 0.5, 1, 0).astype(\"int32\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distributions"},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in train_df.columns:\n    if col in [\"rating\", \"is_toxic\"]:\n        sns.countplot(train_df[col])\n    elif train_df[col].dtype.name in [\"float64\", \"int64\"] and col not in [\"id\", \"comment_text\", \"article_id\", \"parent_id\", \"publication_id\"]:\n        sns.distplot(train_df.loc[train_df[col].notna() & train_df[\"is_toxic\"].eq(0), col], label=\"is_toxic=0\")\n        sns.distplot(train_df.loc[train_df[col].notna() & train_df[\"is_toxic\"].eq(1), col], label=\"is_toxic=1\")\n        plt.legend()\n    else:\n        continue\n    plt.title(f\"Distribution of `{col}` in train\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore frequent words"},{"metadata":{"trusted":false},"cell_type":"code","source":"import unicodedata\nimport sys\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\npuncts_trans = {i: ' ' for i in range(sys.maxunicode)\n                if unicodedata.category(chr(i)).startswith('P')}\n\ndel puncts_trans[ord(\"'\")]\n\npuncts = [chr(i) for i in puncts_trans.keys()]\n# print(\"Puncts:\", puncts)\n\nstop_words = stopwords.words('english')\nother_frequent_words = [\"people\", \"don\", \"doesn\", \"didn\", \"can\",\n                        \"could\", \"like\", \"would\", \"one\", \"get\",\n                        't', 's', 'i', 'you']\n\n\ndef freqs_plot_incond(df, cond=''):\n    new_comment_text = df[\"comment_text\"].str.translate(puncts_trans).str.lower()\n    tokenized = ' '.join(new_comment_text.values.tolist()).split()\n    tokenized = [word for word in tokenized if word not in stop_words + other_frequent_words]\n    s = pd.Series(tokenized)\n    del tokenized\n    gc.collect()\n    freq = s.value_counts().to_dict()\n    del s\n    gc.collect()\n    if len(freq) == 0:\n        print(f\"The {cond} has no words\")\n        return\n    wc = WordCloud(width=800, height=450)\n    pic_mat = wc.fit_words(freq).to_array()\n    plt.imshow(pic_mat)\n    plt.title(f\"Frequent words in condition of {cond}\")\n    plt.show()\n    del pic_mat, wc\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequent words in identities mentioned"},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in identities:\n    freqs_plot_incond(train_df.loc[train_df[col].gt(0.2)], f\"mentioned identity `{col}` > 0.2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequent words in toxicity subtypes"},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in toxicity_subtypes:\n    freqs_plot_incond(train_df.loc[train_df[col].gt(0.2)], f\"subtype `{col}` > 0.2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare datasets for training"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", usecols=identities+toxicity_subtypes+[\"id\", \"target\", \"comment_text\"])\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df.info()\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df[\"is_toxic\"] = np.where(train_df[\"target\"].values >= 0.5, 1, 0).astype(\"int32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nSIZEOF_VOCAB = 49999  # size of vocabulary\nINPUT_LENGTH = 192  # max length for each sequence\n\n# fit on text in all the datasets\n# text_to_fit = pd.concat([train_df[\"comment_text\"], test_df[\"comment_text\"]])\n\n# fit on text in condition of:\ntext_to_fit = train_df.loc[((train_df[\"target\"]>0.3)\n                             &(train_df[toxicity_subtypes].gt(0.2).any(axis=1)))\n                           |((train_df[\"target\"]>0.3)\n                             &(train_df[identities].gt(0.2).any(axis=1))), \"comment_text\"]\n\ntokenizer = Tokenizer(num_words=SIZEOF_VOCAB,\n                      filters=''.join(puncts) + '\\n\\t\\r',\n                     )\ntokenizer.fit_on_texts(text_to_fit)\nprint(len(tokenizer.word_index))\n\nword_counts = pd.Series(dict(tokenizer.word_counts.items())).sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(14.4, 10.8))\nsns.barplot(x=word_counts[:50], y=word_counts[:50].index, ax=ax)\nax.set_title(\"Top frequent words in tokenizer\")\nplt.show()\n\ntrain_text_seq = tokenizer.texts_to_sequences(train_df[\"comment_text\"])\ntest_text_seq = tokenizer.texts_to_sequences(test_df[\"comment_text\"])\n\n# Find out the lengths of words in each sequence\nfor seq, title in zip([train_text_seq, test_text_seq], [\"Train\", \"Test\"]):\n    s = pd.Series([len(x) for x in seq])\n    sns.boxplot(s)\n    plt.title(f\"Distribution of number of words in each comment in {title}\")\n    plt.show()\n\ntrain_features = pad_sequences(train_text_seq, maxlen=INPUT_LENGTH).astype(\"int32\")\ntest_features = pad_sequences(test_text_seq, maxlen=INPUT_LENGTH).astype(\"int32\")\n\ntrn_istoxic = train_df[\"is_toxic\"].values\ntrn_aux_target = train_df[\"target\"].values\ntrn_subtypes = train_df[toxicity_subtypes].values\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and train models"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras import Model\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.regularizers import l1_l2\n\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\n\nEMBEDDING_SIZE = 512\nBATCH_SIZE = 512\nRECURRENT_UNITS = 128\nLR = 0.005\nreg_l1 = 0.3\nreg_l2 = 0.5\n\n\ndef model_fn():\n    \n    inp = L.Input(shape=(INPUT_LENGTH,))\n    emb = L.Embedding(input_dim=SIZEOF_VOCAB+1, output_dim=EMBEDDING_SIZE,\n                      input_length=INPUT_LENGTH,\n                      trainable=True)(inp)\n    \n    drop_0 = L.SpatialDropout1D(rate=0.15)(emb)\n    bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS,\n                                            recurrent_regularizer=l1_l2(l1=reg_l1, l2=reg_l2),\n                                            return_sequences=True))(drop_0)\n    bi_lstm_1 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS,\n                                            recurrent_regularizer=l1_l2(l1=reg_l1, l2=reg_l2),\n                                            return_sequences=False))(bi_lstm_0)\n\n    out_main = L.Dense(1, activation='sigmoid', name=\"main_proba\")(bi_lstm_1)\n    out_aux = L.Dense(1, activation='sigmoid', name=\"aux_proba\")(bi_lstm_1)\n    out_subtypes_probas = L.Dense(6, activation='sigmoid', name=\"subtypes_proba\")(bi_lstm_1)\n    \n    model = Model(inputs=inp, outputs=[out_main, out_aux, out_subtypes_probas])\n    model.compile(Adam(lr=LR), loss=binary_crossentropy, metrics=['acc'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\n\nmodel = model_fn()\nmodel.summary()\n\nhist = model.fit(\n    train_features,\n    [trn_istoxic, trn_aux_target, trn_subtypes],\n    batch_size=BATCH_SIZE,\n    epochs=3,\n    callbacks=[\n     LearningRateScheduler(lambda epoch: max([LR-0.3*LR*epoch, 0.001]), verbose=1),\n    ],\n    validation_split=0.1, shuffle=True,\n)\n\ntrn_pred, trn_aux, _ = model.predict(train_features)\n\ntest_pred, test_aux, sub_pred = model.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation on trainset"},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()\n\n# AUC\nprint(\"Train's istoxic,main AUC: {}\".format(roc_auc_score(trn_istoxic, trn_pred)))\nprint(\"Train's istoxic,aux AUC: {}\".format(roc_auc_score(trn_istoxic, trn_aux)))\n\n# Plot Confusion Matrices\nmain_cm = confusion_matrix(trn_istoxic, np.where(trn_pred>=0.5, 1, 0))\nsns.heatmap(main_cm, annot=True)\nplt.xlabel(\"Actual classes\")\nplt.ylabel(\"Predicted classes\")\nplt.title(\"Confusion Matrix in Train\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"prediction\": test_pred.flatten(),\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration on predicted testset"},{"metadata":{"trusted":false},"cell_type":"code","source":"subtypes = pd.DataFrame(data=sub_pred, columns=toxicity_subtypes)\nsubmission[toxicity_subtypes] = subtypes[toxicity_subtypes]\nsubmission[\"target\"] = test_aux.flatten()\nsubmission[\"comment_text\"] = test_df[\"comment_text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find out the distribution of subtypes\nfor col in toxicity_subtypes+[\"target\"]:\n    sns.distplot(submission[col])\n    plt.title(f\"Distribution of `{col}` in test\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in toxicity_subtypes+[\"target\"]:\n    freqs_plot_incond(submission.loc[submission[col].gt(0.2)], f\"`{col}` > 0.2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next step:\n\n* Explore on extra datasets containing preset embedding weights."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}