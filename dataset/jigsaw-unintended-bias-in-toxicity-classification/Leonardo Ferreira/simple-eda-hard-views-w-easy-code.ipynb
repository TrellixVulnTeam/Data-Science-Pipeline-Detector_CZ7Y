{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NOTE: This Kernel is not finished. I'm working on it yet. \nVotes up the kernel and stay tuned.\n\n\n# Introduction\n\nIn this notebook I will create a baseline to understand the Jigsaw data and after it create my model to predict Toxic Comments.\n\n## Competition Description:\nCan you help detect toxic comments ― and minimize unintended model bias? That's your challenge in this competition.\n\nThe Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\n\nLast year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.\n\nHere’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n\nIn this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Some questions that I will try to answer: <br>\n- What is the distribuition of Toxicity?<br>\n- What is the most attacked group?<br>\n- We can see some difference between comments of toxic and non-toxic comments?<br>\n- Are the date of the year correlated with the toxicity comments?<br>\n- The toxicity ratio is equal to all articles?<br>\n- What's the groups with the highest toxicity, likes, disagrees.  \n- And much more."},{"metadata":{},"cell_type":"markdown","source":"\nEnglish is not my first language, so sorry for any mistake. "},{"metadata":{},"cell_type":"markdown","source":"# Let's start the work"},{"metadata":{},"cell_type":"markdown","source":"## Import librarys"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.offline as py \nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring some information about the data\n- nulls\n- data types\n- shape\n- First rows of dataset"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Missing values\ndf_train.isnull().sum() / len(df_train) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data info"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shape of our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Head - knowning the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, Now that we have some idea of how the dataset is, we can start exploring and understanding the dataset"},{"metadata":{},"cell_type":"markdown","source":"## Ploting Target (Toxicity) distribuition"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"count_target_zero = round(df_train[df_train['target'] == 0]['target'].count() / len(df_train['target']) * 100,2)\nprint(f'Total of zero values in Toxic rate: {count_target_zero}%')\n\nplt.figure(figsize=(13,6))\n\ng = sns.distplot(df_train[df_train['target'] > 0]['target'])\nplt.title('Toxic Distribuition', fontsize=22)\nplt.xlabel(\"Toxic Rate\", fontsize=18)\nplt.ylabel(\"Distribuition\", fontsize=18) \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting distribuition. <br>\nFor default, in the competition description we will consider toxic when the target has values above 0.5"},{"metadata":{},"cell_type":"markdown","source":"## Toxicity Subtype attributes Distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comment_adj = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\nplt.figure(figsize=(14,6))\n\nfor col in comment_adj[1:]:\n    g = sns.distplot(df_train[df_train[col] > 0][col], label=col, hist=False)\n    #plt.legend(f'{col} Distribuition', fontsize=22)\n    plt.xlabel(\"Rate\", fontsize=18)\n    plt.ylabel(\"Distribuition\", fontsize=18)\n    plt.legend(loc=1, prop={'size': 14})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a flag to separe Toxic and Non-Toxic comments (Target >= 0.5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a flag to toxic and non-toxic comments\ndf_train['toxic'] = np.where(df_train['target'] >= .5, 'Toxic', 'Non-Toxic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting the Distribuition of Categorical Toxic and Non-Toxic"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train['toxic'].value_counts().iplot(kind='bar', xTitle='Toxic or Non-Toxic', yTitle=\"Count\", \n                                       title='Distribuition of Toxicity of comments')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining some categories of comment and counting"},{"metadata":{"trusted":true},"cell_type":"code","source":"etnics = ['asian' , 'latino' , 'black', 'white', 'other_race_or_ethnicity']\n\nreligions = ['atheist', 'buddhist', 'hindu', 'jewish', 'muslim', 'christian', 'other_religion']\n\nsexual = ['female', 'male', 'other_gender'] \n\nsexual_orientation = ['heterosexual', 'bisexual', 'transgender', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\n\ndisabilities = ['intellectual_or_learning_disability', 'physical_disability', 'psychiatric_or_mental_illness', 'other_disability']\n\nreactions = ['funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Grouping by toxic by each demographic "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## The inspiration to this kernel is https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw\n\netnics_dem = df_train.loc[:, ['target']+list(etnics + ['toxic'])].dropna()\ncount_etnics = etnics_dem.iloc[:, 1:][etnics_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nreligion_dem = df_train.loc[:, ['target']+list(religions  + ['toxic'])].dropna()\ncount_religions = religion_dem.iloc[:, 1:][religion_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nsexual_dem = df_train.loc[:, ['target']+list(sexual  + ['toxic'])].dropna()\ncount_sexual = sexual_dem.iloc[:, 1:][sexual_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nsexual_orient = df_train.loc[:, ['target']+list(sexual_orientation  + ['toxic'])].dropna()\ncount_orient_sexual = sexual_orient.iloc[:, 1:][sexual_orient.iloc[:, 1:] > 0].groupby('toxic').count()\n\ndisabilities_dem = df_train.loc[:, ['target']+list(disabilities  + ['toxic'])].dropna()\ncount_desabilities = disabilities_dem.iloc[:, 1:][disabilities_dem.iloc[:, 1:] > 0].groupby('toxic').count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a list to plot"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"list_groupbys = [count_etnics, count_religions, count_sexual, count_orient_sexual, count_desabilities]\nlist_names = ['Ethnics Comments by Toxic and Non-Toxic Classification',\n              'Religions Comments by Toxic and Non-Toxic Classification',\n              'Sexual Comments by Toxic and Non-Toxic Classification',\n              'Sexual Orientation Comments by Toxic and Non-Toxic Classification',\n              'Disabilities Comments by Toxic and Non-Toxic Classification', ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting the distribuitions of the data modelling"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for plot, text in zip(list_groupbys, list_names):\n    plot.T.iplot(kind='bar', xTitle='Demographic categories', yTitle='Count',\n                 title=text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see and have insight about some of the toxicy comments. Further, I will explore the comments;"},{"metadata":{},"cell_type":"markdown","source":"## Rating distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train['rating'].value_counts().iplot(kind='bar', title='Rating of Comment', \n                                        xTitle='Rating', yTitle='Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rating by Toxic and Non-Toxic comments"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"round((pd.crosstab(df_train['rating'], df_train['toxic'], \n            normalize='index') * 100),2).iplot(kind='bar', barmode='stack', \n                                               title= \"Rating Ratio by Toxic and Non-Toxic\",\n                                               xTitle=\"Rating Status\", yTitle='% of Toxic and Non-Toxic')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's interesting. <br>\n5% of approved comments are Toxic; <br>\nAs we can see 66% of rejected comments aren't toxic.\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Understanding Toxic and Non-Toxic Comments by Dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming to pandas \ndf_train['created_date'] = pd.to_datetime(df_train['created_date'], format='%Y-%m-%d %H:%M:%S')\n\ndf_train['month'] = df_train['created_date'].dt.month\ndf_train['weekday'] =  df_train['created_date'].dt.weekday_name\ndf_train['hour'] =  df_train['created_date'].dt.hour\n\n# df_train['created_date'] = pd.to_datetime(df_train['created_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the first and last date\nprint(f'The first date is {df_train[\"created_date\"].dt.date.min()} and the last date is {df_train[\"created_date\"].dt.date.max()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will filter by comments date higher than 2016-01-01\ntoxic_comment_dates = df_train[df_train['created_date'] >= '2016-01-01'].groupby([df_train['created_date'].dt.date,'toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_dates.iplot(kind='bar', barmode='stack', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that altough the number comments has increased a lot, the toxic comments are seemgly a constant value"},{"metadata":{},"cell_type":"markdown","source":"## Looking the distribution by Months;\nWe will consider the dates higher than 2016-01-01 so I have a small sample of months. <br>\nIt's just to explore and may find a interesting pattern."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dates higher than 2016-01-01\ntoxic_comment_months = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['month','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_months.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking the distribution by week days;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dates higher than 2016-01-01\ntoxic_comment_week = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['weekday','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0).sort_index()\n\ntoxic_comment_week.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Weekdays', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Interesting that Saturday and Sunday are the day with smallest number of comments but \"respecting\" the Toxicity Ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dates higher than 2016-01-01\ntoxic_comment_hour = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['hour','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_hour.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Hours', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very interesting distribuitions. <br>\nalso, we can see that the Toxicity ratio is very constant."},{"metadata":{},"cell_type":"markdown","source":"## Creating some features from comments "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Setting the stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\n#Word count in each comment:\ndf_train['count_word']= df_train[\"comment_text\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count\ndf_train['count_unique_word']=df_train[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count\ndf_train['count_letters']=df_train[\"comment_text\"].apply(lambda x: len(str(x)))\n\n#punctuation count\ndf_train[\"count_punctuations\"] = df_train[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count\ndf_train[\"count_words_upper\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count\ndf_train[\"count_words_title\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords\ndf_train[\"count_stopwords\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n#Average length of the words\ndf_train[\"mean_word_len\"] = df_train[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribuitions of the new features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comments_counts = ['count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n                   'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len']\ndef quantiles(columns):\n    # To append the quantile outputs\n    quantile_data = []\n\n    # Looping for the created columns\n    for counts in columns:\n        # Quantiles from desired columns\n        quantiles = df_train[counts].quantile([.01,.25,.5,.75,.99])\n        # Store quantile DataFrame in list\n        quantile_data.append(quantiles)\n\n    # Now concat the data that you got the quantiles\n    return pd.concat(quantile_data, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantiles(comments_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting distribuition of text metrics"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace0 = go.Box(\n    x=df_train['toxic'].sample(20000),\n    y=df_train['count_word'].sample(20000),\n    name='Toxic', showlegend=False, jitter=0.2, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace2 = go.Box(\n    x=df_train[(df_train['count_unique_word'] <= 128)]['toxic'].sample(20000),\n    y=df_train[(df_train['count_unique_word'] <= 128)]['count_unique_word'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace4 = go.Box(\n    x=df_train[ (df_train['count_letters'] <= 999)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_letters'] <= 999)]['count_letters'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace6 = go.Box(\n    x=df_train[ (df_train['count_punctuations'] <= 45)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_punctuations'] <= 45)]['count_punctuations'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace8 = go.Box(\n    x=df_train[ (df_train['count_words_upper'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_upper'] <= 9)]['count_words_upper'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace10 = go.Box(\n    x=df_train[ (df_train['count_words_title'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_title'] <= 9)]['count_words_title'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace12 = go.Box(\n    x=df_train[ (df_train['count_stopwords'] <= 88)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_stopwords'] <= 88)]['count_stopwords'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace14 = go.Box(\n    x=df_train[  (df_train['mean_word_len'] <= 9.129411)]['toxic'].sample(20000),\n    y=df_train[ (df_train['mean_word_len'] <= 9.129411)]['mean_word_len'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n    \n)\n\n\ndata = [trace0, trace2, trace4, trace6,trace8, \n        trace10,trace12, trace14]\n\nfig = tls.make_subplots(rows=4, cols=2, specs=[[{}, {}], \n                                               [{}, {}], \n                                               [{}, {}], \n                                               [{}, {}]],\n                          subplot_titles=('Word Counts','Unique Words Count', 'Letters Count', 'Punctuation Count', \n                                          'Upper Case Count','Words Title Count', 'Stopwords Count', 'Mean Words Len'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace6, 2, 2)\nfig.append_trace(trace8, 3, 1)\nfig.append_trace(trace10, 3, 2)\nfig.append_trace(trace12, 4, 1)\nfig.append_trace(trace14, 4, 2)\n\nfig['layout'].update(title='Comment Metrics by Toxic and Non-Toxic', autosize=True, boxmode='group')\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HELP: How can I set manually the size of box?! \nWhy they're not not aligned?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing a violin plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the highest values for each demographic group"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['attacked_group'] = df_train[etnics+ sexual + sexual_orientation + religions + disabilities].replace(0, np.nan).idxmax(axis=1)\ndf_train['attacked_group'] = df_train['attacked_group'].fillna(\"No demo group detected\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"att_count = df_train['attacked_group'].value_counts()\nprint(f\"Total of No Demographic Group Detected {att_count[0]}\")\n\ndf_train[df_train['attacked_group'] != 'No demo group detected']['attacked_group'].value_counts().iplot(kind='bar', title='Count of Highest values in Attacked Groups',\n                                                xTitle='Demographic Group Name', yTitle='Count of highest \"Citations\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We have almost 1.6M comments that are not related to demographic groups that are in the categories"},{"metadata":{},"cell_type":"markdown","source":"## Looking the % ratio of Toxic and Non-Toxic for each demo group"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"attacked_group = pd.crosstab(df_train['attacked_group'],df_train['toxic'], aggfunc='count', values=df_train['target']).apply(lambda r: r/r.sum(), axis=1)\nattacked_group.iplot(kind='bar',barmode='stack',\n                     title='Percent of Toxic and Non-Toxic Comments for Attacked Groups',\n                     xTitle='Demographic Group Name', yTitle='Percent ratio of each Group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's a very informative visualization. <br>We can see that some categories, as blacks, hindu (indians?), and hetero(??) orientation  has a highest number of toxic comments against this group. "},{"metadata":{},"cell_type":"markdown","source":"## Understanding the \"Reaction\" metrics"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## Again, calling the quantile function that we created before\nquantiles(reactions)\n\n\naggs = {\n    'sexual_explicit': ['sum', 'size'],\n    'likes': ['sum'],\n}\n\n# Previous applications categorical features\n\nprev_agg = df_train[df_train['attacked_group'] != 'No demo group detected'].groupby(['attacked_group','toxic']).agg({**aggs})\n\nprev_agg.columns = pd.Index(['Agg_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\nprev_agg.rename(columns={'Agg_sexual_explicit_SUM':'Sexual bias sum',\n                         'Agg_likes_SUM':' Likes sum',\n                         'Agg_sexual_explicit_SIZE':'Total Comments'}, inplace=True)\nprev_agg.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"prev_agg.sort_index().unstack(\"toxic\").fillna(0).iplot(kind='bar', showlegend=False, \n                                                       title ='Demographic Groups by Sum of sexual explicit and Sum of Likes',\n                                                       xTitle='Demographic Groups', yTitle='Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- I will try a way to best apresent this chart above\n\nIt's another interesting information about the data. Below, I will try to investigate it further;"},{"metadata":{},"cell_type":"markdown","source":"## Let's invetigate some \"stealth\" columns by many metrics\n- Some tests using PieChart"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"color_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ndef PieChart(df_cat, df_value, title, limit=15):\n    \"\"\"\n    This function helps to investigate the proportion of metrics of toxicity and other values\n    \"\"\"\n\n    # count_trace = df_train[df_cat].value_counts()[:limit].to_frame().reset_index()\n    rev_trace = df_train[df_train['toxic'] == \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n    rev_trace_non = df_train[df_train['toxic'] != \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n\n    trace1 = go.Pie(labels=rev_trace_non[df_cat], \n                    values=rev_trace_non[df_value], name= \"Non-Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=True,\n                    domain= {'x': [0, .48]})\n\n    trace2 = go.Pie(labels=rev_trace[df_cat], \n                    values=rev_trace[df_value], name=\"Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=False, \n                    domain= {'x': [.52, 1]})\n\n    layout = dict(title= title, height=450, font=dict(size=15),\n                  annotations = [\n                      dict(\n                          x=.20, y=.5,\n                          text='Non-Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sexual Explicit by Attacked Groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"attacked_group\", 'sexual_explicit', \"Mean of sexual Explicit by categories\", limit=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow. It's very interesting and curious. <br>\nIn Non-Toxic:<br>\nAs we can see, comments about Transgender has a value 3 times highest than the third position. <br>\n  \nIn Toxic:<br>\nIt's interesting that atheists has a very high mean in sexual explicit. I will investigate this category further in comments."},{"metadata":{},"cell_type":"markdown","source":"## Likes by Attacked Group"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"attacked_group\", 'likes', \"Mean of sexual Explicit by categories\", limit=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Likes distribution by Articles Id"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"article_id\", 'likes', \"Mean Likes in Toxic and Non-Toxic comments by TOP 10 Articles\", limit = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sexual Explicit distribution by Articles Id"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"article_id\", 'sexual_explicit', \"Mean Sexual Explicit in Comments by TOP 10 Articles\", limit = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Toxicity mean by Publication"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"publication_id\", 'target', \"Mean Target (Toxicity) in Comments by each Publication\", limit = 10) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sexual Explicit by Publication Id"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"publication_id\", 'sexual_explicit', \"Mean Sexual Explicit in comments by Publication\", limit = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Likes distribution by Publication Id"},{"metadata":{"trusted":true},"cell_type":"code","source":"PieChart(\"publication_id\", 'likes', \"Mean Likes in Comments by each Publication\", limit = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"As we can see, the unique valid values are likes and sexual explicit. <br>\nLet's investigate it further"},{"metadata":{},"cell_type":"markdown","source":"## Knowing identity_annotator_count"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"name = df_train['identity_annotator_count'].value_counts()[:8]\n\nfig = pd.crosstab(df_train[df_train['identity_annotator_count'].isin(name.index)]['identity_annotator_count'], \n                  df_train[df_train['identity_annotator_count'].isin(name.index)]['toxic'], \n                  normalize='index').iplot(kind='bar', barmode='stack', bargap=.2, asFigure=True,\n                                           title= \"TOP 8 Identity Annotator by Toxic and Non-Toxic\",\n                                           xTitle=\"Identity Annotator Count\", yTitle='Count')\nfig.layout.xaxis.type = 'category'\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"99% of all data are in this top 8 Identitity annotators. <br>\nThe value of toxic on the value 0 is lowest in proportionality than Identity Annotator 4;<br>\nWhat it means? "},{"metadata":{},"cell_type":"markdown","source":"## WordCloud of Non-Toxic Comments"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\ntext_to_analize= df_train[df_train['toxic'] == \"Non-Toxic\"]['comment_text']\n\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1 \n\nplt.figure(figsize = (15,15))\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1000,\n                          max_font_size=120, \n                          random_state=42\n                         ).generate(str(text_to_analize))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - NON-TOXIC\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud of Toxic Comments"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\ntext_to_analize= df_train[df_train['toxic'] == \"Toxic\"]['comment_text']\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1 \n\nplt.figure(figsize = (15,15))\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1000,\n                          max_font_size=120, \n                          random_state=42\n                         ).generate(str(text_to_analize))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - TOXICS\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I will keep improving and Working on this Kernel. \n# If you liked this Kernel, please votes up the kernel \n\nSome fonts that I used to inspire and learn to build this kernel: <br>\nhttps://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw <br>\nhttps://www.kaggle.com/gpreda/jigsaw-eda <br>\nand a lot of other that I forget to save the link"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}