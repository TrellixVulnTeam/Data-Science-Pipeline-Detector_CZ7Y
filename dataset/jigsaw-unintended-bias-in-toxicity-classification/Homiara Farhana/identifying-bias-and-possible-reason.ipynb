{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run the next code cell without changes to use the data to train a simple model.  The output shows the accuracy of the model on some test data.","metadata":{"papermill":{"duration":0.012727,"end_time":"2020-10-30T19:06:45.774334","exception":false,"start_time":"2020-10-30T19:06:45.761607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set up feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ethics.ex3 import *\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Get the same results each time\nnp.random.seed(0)\n\n# Load the (full) training data\nfull_data = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n\n# Work with a small subset of the data: if target > 0.7, toxic.  If target < 0.3, non-toxic\nfull_toxic = full_data[full_data[\"target\"]>0.7]\nfull_nontoxic = full_data[full_data[\"target\"]<0.3].sample(len(full_toxic))\ndata = pd.concat([full_toxic, full_nontoxic], ignore_index=True)\ncomments = data[\"comment_text\"]\ntarget = (data[\"target\"]>0.7).astype(int)\n\n# Break into training and test sets\ncomments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n\n# Get vocabulary from training data\nvectorizer = CountVectorizer()\nvectorizer.fit(comments_train)\n\n# Get word counts for training and test sets\nX_train = vectorizer.transform(comments_train)\nX_test = vectorizer.transform(comments_test)\n\n# Preview the dataset\nprint(\"Data successfully loaded!\\n\")\nprint(\"Sample toxic comment:\", comments_train.iloc[18])\nprint(\"Sample not-toxic comment:\", comments_train.iloc[3])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:29:10.682818Z","iopub.execute_input":"2021-06-25T16:29:10.683196Z","iopub.status.idle":"2021-06-25T16:29:40.628726Z","shell.execute_reply.started":"2021-06-25T16:29:10.683115Z","shell.execute_reply":"2021-06-25T16:29:40.62687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Train a model and evaluate performance on test dataset\nclassifier = LogisticRegression(max_iter=2000)\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\nprint(\"Accuracy:\", score)\n\n# Function to classify any string\ndef classify_string(string, investigate=False):\n    prediction = classifier.predict(vectorizer.transform([string]))[0]\n    if prediction == 0:\n        print(\"NOT TOXIC:\", string)\n    else:\n        print(\"TOXIC:\", string)","metadata":{"papermill":{"duration":11.047454,"end_time":"2020-10-30T19:06:56.836704","exception":false,"start_time":"2020-10-30T19:06:45.78925","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:29:40.630217Z","iopub.execute_input":"2021-06-25T16:29:40.630458Z","iopub.status.idle":"2021-06-25T16:30:08.379824Z","shell.execute_reply.started":"2021-06-25T16:29:40.630434Z","shell.execute_reply":"2021-06-25T16:30:08.378554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Roughly 93% of the comments in the test data are classified correctly!\n\n","metadata":{"papermill":{"duration":0.014849,"end_time":"2020-10-30T19:06:56.868884","exception":false,"start_time":"2020-10-30T19:06:56.854035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Comment to pass through the model\nmy_comment = \"i hate orange\"\n\n# Do not change the code below\nclassify_string(my_comment)\nq_1.check()","metadata":{"papermill":{"duration":0.026189,"end_time":"2020-10-30T19:06:56.90901","exception":false,"start_time":"2020-10-30T19:06:56.882821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:30:08.38208Z","iopub.execute_input":"2021-06-25T16:30:08.384471Z","iopub.status.idle":"2021-06-25T16:30:08.406108Z","shell.execute_reply.started":"2021-06-25T16:30:08.384415Z","shell.execute_reply":"2021-06-25T16:30:08.405033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic.  The code cell outputs the ten words that are considered most toxic, along with their coefficients.  ","metadata":{"papermill":{"duration":0.018539,"end_time":"2020-10-30T19:06:56.942426","exception":false,"start_time":"2020-10-30T19:06:56.923887","status":"completed"},"tags":[]}},{"cell_type":"code","source":"coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\ncoefficients.sort_values(by=['coeff']).tail(10)","metadata":{"papermill":{"duration":0.115908,"end_time":"2020-10-30T19:06:57.07637","exception":false,"start_time":"2020-10-30T19:06:56.960462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:30:39.128239Z","iopub.execute_input":"2021-06-25T16:30:39.128595Z","iopub.status.idle":"2021-06-25T16:30:39.191739Z","shell.execute_reply.started":"2021-06-25T16:30:39.128564Z","shell.execute_reply":"2021-06-25T16:30:39.191063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"None of the words are surprising. They are all clearly toxic.","metadata":{}},{"cell_type":"markdown","source":"# A closer investigation\n\nWe'll take a closer look at how the model classifies comments.\n","metadata":{"papermill":{"duration":0.015333,"end_time":"2020-10-30T19:06:57.175887","exception":false,"start_time":"2020-10-30T19:06:57.160554","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment = \"I have a christian friend\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\n","metadata":{"papermill":{"duration":0.044861,"end_time":"2020-10-30T19:06:57.236559","exception":false,"start_time":"2020-10-30T19:06:57.191698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:32:56.198567Z","iopub.execute_input":"2021-06-25T16:32:56.198913Z","iopub.status.idle":"2021-06-25T16:32:56.217665Z","shell.execute_reply.started":"2021-06-25T16:32:56.198883Z","shell.execute_reply":"2021-06-25T16:32:56.216416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify bias\n\nLet's run the comment \"I have a muslim friend\" and see the prediction of the model","metadata":{"papermill":{"duration":0.016152,"end_time":"2020-10-30T19:06:57.26916","exception":false,"start_time":"2020-10-30T19:06:57.253008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment = \"I have a muslim friend\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\n","metadata":{"papermill":{"duration":0.024534,"end_time":"2020-10-30T19:06:57.310158","exception":false,"start_time":"2020-10-30T19:06:57.285624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:34:42.389849Z","iopub.execute_input":"2021-06-25T16:34:42.390373Z","iopub.status.idle":"2021-06-25T16:34:42.409187Z","shell.execute_reply.started":"2021-06-25T16:34:42.390325Z","shell.execute_reply":"2021-06-25T16:34:42.408017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_comment = \"My friend is black\"\n\n# Do not change the code below\nclassify_string(new_comment)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:48:14.377038Z","iopub.execute_input":"2021-06-25T16:48:14.377368Z","iopub.status.idle":"2021-06-25T16:48:14.382287Z","shell.execute_reply.started":"2021-06-25T16:48:14.377341Z","shell.execute_reply":"2021-06-25T16:48:14.381506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_comment = \"I'm gay\"\n\n# Do not change the code below\nclassify_string(new_comment)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:50:51.041298Z","iopub.execute_input":"2021-06-25T16:50:51.041646Z","iopub.status.idle":"2021-06-25T16:50:51.046632Z","shell.execute_reply.started":"2021-06-25T16:50:51.041617Z","shell.execute_reply":"2021-06-25T16:50:51.045891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we can see how biased the model is.","metadata":{}},{"cell_type":"markdown","source":"So,Comments that refer to Islam are more likely to be classified as toxic, because of a flawed state of the online community where the data was collected. This can introduce historical bias.\n\nBeside, if we hypothesize that a model is being trained to classify online comments as toxic. So, any comments that are not in english,so trasnslated in English with a seperate tool. This can introduce since non-English comments will often not be translated perfectly.\n\nAdditionally,If the model is evaluated based on comments from users in the United Kingdom and deployed to users in Australia, this will lead to evaluation bias and deployment bias. The model will also have representation bias, because it was built to serve users in Australia, but was trained with data from users based in the United Kingdom.","metadata":{}}]}