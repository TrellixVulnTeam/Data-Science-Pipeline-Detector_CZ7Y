{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\nhttps://nicgian.github.io/text-generation-vae/"},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.advanced_activations import ELU\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.models import Model\nfrom scipy import spatial\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport csv\nimport os\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Presets "},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DIR = '/kaggle/input/'\nTRAIN_DATA_FILE = BASE_DIR + 'jigsaw-unintended-bias-in-toxicity-classification/train.csv'\nGLOVE_EMBEDDING = BASE_DIR + 'glove6b50dtxt/glove.6B.50d.txt'\nVALIDATION_SPLIT = 0.2\nMAX_SEQUENCE_LENGTH = 220\nMAX_NB_WORDS = 10000\nEMBEDDING_DIM = 50\n\ntexts = [] \nwith codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        texts.append(values[2])\nprint('Found %s texts in train.csv' % len(texts))\ngc.collect()\nprint(\"Sample text:\")\nprint(texts[100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Token, Padding, & Train/Val Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\nindex2word = {v: k for k, v in word_index.items()}\nprint('Found %s unique tokens' % len(word_index))\nsequences = tokenizer.texts_to_sequences(texts)\ndata_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data_1.shape)\nNB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\nprint(\"NB_WORDS: \", NB_WORDS)\ndata_1_val = data_1[200000:200500] #select last 500 sentences as validation data\nprint(\"Validation Data Shape: \", data_1_val.shape)\ndel data_1, texts, sequences\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Gen"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_generator(TRAIN_DATA_FILE, chunksize):\n    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n    for df in reader:\n        texts = df.iloc[:,2]\n        sequences = tokenizer.texts_to_sequences(texts)\n        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n        yield [data_train, data_train]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open(GLOVE_EMBEDDING, encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nglove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i < NB_WORDS:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be the word embedding of 'unk'.\n            glove_embedding_matrix[i] = embedding_vector\n        else:\n            glove_embedding_matrix[i] = embeddings_index.get('unk')\nprint('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VAE Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\nmax_len = MAX_SEQUENCE_LENGTH\nemb_dim = EMBEDDING_DIM\nlatent_dim = 32\nintermediate_dim = 96\nepsilon_std = 1.0\nnum_sampled=500\nact = ELU()\n\n#y = Input(batch_shape=(None, max_len, NB_WORDS))\nx = Input(batch_shape=(None, max_len))\nx_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n                            input_length=max_len, trainable=False)(x)\nh = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)\nh = Dropout(0.2)(h)\nh = Dense(intermediate_dim, activation='linear')(h)\nh = act(h)\nh = Dropout(0.2)(h)\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n                              stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var / 2) * epsilon\n\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\nrepeated_context = RepeatVector(max_len)\ndecoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\ndecoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))#softmax is applied in the seq2seqloss by tf\nh_decoded = decoder_h(repeated_context(z))\nx_decoded_mean = decoder_mean(h_decoded)\n\n\n# placeholder loss\ndef zero_loss(y_true, y_pred):\n    return K.zeros_like(y_pred)\n\n#=========================== Necessary only if you want to use Sampled Softmax =======================#\n#Sampled softmax\nlogits = tf.constant(np.random.randn(batch_size, max_len, NB_WORDS), tf.float32)\ntargets = tf.constant(np.random.randint(NB_WORDS, size=(batch_size, max_len)), tf.int32)\nproj_w = tf.constant(np.random.randn(NB_WORDS, NB_WORDS), tf.float32)\nproj_b = tf.constant(np.zeros(NB_WORDS), tf.float32)\n\ndef _sampled_loss(labels, logits):\n    labels = tf.cast(labels, tf.int64)\n    labels = tf.reshape(labels, [-1, 1])\n    logits = tf.cast(logits, tf.float32)\n    return tf.cast(\n                    tf.nn.sampled_softmax_loss(\n                        proj_w,\n                        proj_b,\n                        labels,\n                        logits,\n                        num_sampled=num_sampled,\n                        num_classes=NB_WORDS),\n                    tf.float32)\nsoftmax_loss_f = _sampled_loss\n#====================================================================================================#\n\n# Custom VAE loss layer\nclass CustomVariationalLayer(Layer):\n    def __init__(self, **kwargs):\n        self.is_placeholder = True\n        super(CustomVariationalLayer, self).__init__(**kwargs)\n        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n\n    def vae_loss(self, x, x_decoded_mean):\n        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n        labels = tf.cast(x, tf.int32)\n        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n                                                     weights=self.target_weights,\n                                                     average_across_timesteps=False,\n                                                     average_across_batch=False), axis=-1)\n                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#, uncomment for sampled doftmax\n        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n        return K.mean(xent_loss + kl_loss)\n\n    def call(self, inputs):\n        x = inputs[0]\n        x_decoded_mean = inputs[1]\n        print(x.shape, x_decoded_mean.shape)\n        loss = self.vae_loss(x, x_decoded_mean)\n        self.add_loss(loss, inputs=inputs)\n        # we don't use this output, but it has to have the correct shape:\n        return K.ones_like(x)\n\nloss_layer = CustomVariationalLayer()([x, x_decoded_mean])\nvae = Model(x, [loss_layer])\nopt = Adam(lr=0.01) #SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\nvae.compile(optimizer='adam', loss=[zero_loss])\nvae.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_checkpoint(dir, model_name):\n    filepath = dir + '/' + model_name + \".h5\" #-{epoch:02d}-{decoded_mean:.2f}\n    directory = os.path.dirname(filepath)\n    try:\n        os.stat(directory)\n    except:\n        os.mkdir(directory)\n    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n    return checkpointer\n\ncheckpointer = create_model_checkpoint('models', 'vae_seq2seq')\n\nnb_epoch = 1\nn_steps = (200000/2) // batch_size \nfor counter in range(nb_epoch):\n    print('-------epoch: ',counter,'--------')\n    vae.fit_generator(sent_generator(TRAIN_DATA_FILE, batch_size),\n                          steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n                          validation_data=(data_1_val, data_1_val))\n    \nvae.save('models/vae_lstm800k32dim96hid2.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract Encoder & Decoder Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build a model to project sentences on the latent space\nencoder = Model(x, z_mean)\n\n# build a generator that can sample sentences from the learned distribution\ndecoder_input = Input(shape=(latent_dim,))\n_h_decoded = decoder_h(repeated_context(decoder_input))\n_x_decoded_mean = decoder_mean(_h_decoded)\n_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\ngenerator = Model(decoder_input, _x_decoded_mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"index2word = {v: k for k, v in word_index.items()}\nsent_encoded = encoder.predict(data_1_val, batch_size = 16)\nx_test_reconstructed = generator.predict(sent_encoded)\n                                         \nsent_idx = 272\nreconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n#np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\nword_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\nprint(word_list)\noriginal_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\nprint(original_sent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions for Sentence Processing and Interpolation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to parse a sentence\ndef sent_parse(sentence, mat_shape):\n    sequence = tokenizer.texts_to_sequences(sentence)\n    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n    return padded_sent#[padded_sent, sent_one_hot]\n\n# input: encoded sentence vector\n# output: encoded sentence vector in dataset with highest cosine similarity\ndef find_similar_encoding(sent_vect):\n    all_cosine = []\n    for sent in sent_encoded:\n        result = 1 - spatial.distance.cosine(sent_vect, sent)\n        all_cosine.append(result)\n    data_array = np.array(all_cosine)\n    maximum = data_array.argsort()[-3:][::-1][1]\n    new_vec = sent_encoded[maximum]\n    return new_vec\n\n# input: two points, integer n\n# output: n equidistant points on the line between the input points (inclusive)\ndef shortest_homology(point_one, point_two, num):\n    dist_vec = point_two - point_one\n    sample = np.linspace(0, 1, num, endpoint = True)\n    hom_sample = []\n    for s in sample:\n        hom_sample.append(point_one + s * dist_vec)\n    return hom_sample\n\n# input: original dimension sentence vector\n# output: sentence text\ndef print_latent_sentence(sent_vect):\n    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n    sent_reconstructed = generator.predict(sent_vect)\n    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS])\n    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n    np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n    np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n    w_list = [w for w in word_list if w]\n    print(' '.join(w_list))\n    #print(word_list)\n        \ndef new_sents_interp(sent1, sent2, n):\n    tok_sent1 = sent_parse(sent1, [15])\n    tok_sent2 = sent_parse(sent2, [15])\n    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n    for point in test_hom:\n        print_latent_sentence(point)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model QA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1=['where can i find a book on machine learning']\nmysent = sent_parse(sentence1, [15])\nmysent_encoded = encoder.predict(mysent, batch_size = 16)\nprint(mysent_encoded)\nprint('-----------------')\nsentence2=['how can i become a successful entrepreneur']\nmysent2 = sent_parse(sentence2, [15])\nmysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\nprint(mysent_encoded2)\nprint('-----------------')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}