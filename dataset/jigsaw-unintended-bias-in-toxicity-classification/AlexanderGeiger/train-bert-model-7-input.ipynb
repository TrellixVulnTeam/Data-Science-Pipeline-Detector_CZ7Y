{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup Directories\nsetup directories to the models and data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup target & Name\nMODEL_NAME   = 'model1'\nTARGET       = 'target'\n\n# Create Input Paths\nWORK_DIR          = \"../working/\"\nInput_dir         = \"../input/\"\noutput_model_file = \"bert_pytorch.bin\"\nsave_model_file   = \"bert_pytorch.bin\"\n\n# Generate Directories & Paths\nBERT_MODEL_PATH       = Input_dir + 'bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\npackage_dir_a         = Input_dir + \"ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nData_dir              = Input_dir + \"jigsaw-unintended-bias-in-toxicity-classification\"\nBERT_MODEL_PATH_SETUP = Input_dir + \"kaggle-model-ii/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Parameters\nSetup the model training parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing Parameters\nnum_to_load  = 1100000 # Train size to match time limit\nvalid_size   = 10000   # Validation Size\nSEED         = 5432\nEPOCHS       = 1\nNUM_OUTPUTS  = 7\nacc_steps    = 1\n\n# Model Parameters\nMAX_SEQUENCE_LENGTH = 200\nlr                  = 1e-5\nbatch_size          = 64\nWARM_UP             = 0.01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup Variables & Packages\nsetup variables and packages for the model"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Setup Bert Path\nBERT_LOAD    = BERT_MODEL_PATH_SETUP + output_model_file\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC     = 'bpsn_auc'     # stands for background positive, subgroup negative\nBNSP_AUC     = 'bnsp_auc'     # stands for background negative, subgroup positive\nno_decay      = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # FOR MODEL\nidentity_cols = ['male','female','homosexual_gay_or_lesbian','christian','jewish','muslim','black','white','psychiatric_or_mental_illness']\ny_columns     = [TARGET]\n\n# Import Packages\nimport os, numpy as np,  pandas as pd\nprint(os.listdir(\"../input/nvidiaapex/repository/NVIDIA-apex-39e153a\"))\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidiaapex/repository/NVIDIA-apex-39e153a\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport torch.utils.data\nimport torch.nn.functional as F\nimport scipy.stats as stats\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nfrom keras.preprocessing import text, sequence\nimport datetime, pkg_resources, time, gc, re, operator, sys\nimport shutil, pickle, warnings\nfrom nltk.stem import PorterStemmer\nfrom tqdm import tqdm, tqdm_notebook\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom apex import amp\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings(action='once')\ndevice=torch.device('cuda')\nsys.path.insert(0, package_dir_a)\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch, BertConfig\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\nshutil.copyfile(BERT_MODEL_PATH_SETUP + 'bert_config.json', WORK_DIR + 'bert_config.json') # copy file\nbert_config = BertConfig(BERT_MODEL_PATH_SETUP + \"bert_config.json\")                       # Translate model from tensorflow to pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    the_score = (bce_loss_1 * loss_weight) + bce_loss_2\n    return the_score\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\n# ==========================\n# ==== Result Analysis =====\n# ==========================\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TARGET]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset, subgroups, model, label_col, include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup Training Data\nSetup the sequence training data for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# setup tokenizer\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\n# setup records\ntrain_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n\n# sample training dataset\ntrain_df = train_df.sample(num_to_load+valid_size,random_state=SEED)\n\n# Make sure all comment_text values are strings\ntrain_df['comment_text'] = train_df['comment_text'].astype(str) \n\n# create sequence data\nseq_data = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n\n# drop comment text\ntrain_df = train_df.drop(['comment_text'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Targets\nGenerate weighted targets for main prediction and underlying subgroups "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['valid_target'] = (train_df[y_columns]>=0.5).astype(float)\n\n# Overall\nweights = np.ones((len(seq_data),)) / 4\n\n# Subgroup\nweights += (train_df[identity_cols].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n\n# Background Positive, Subgroup Negative\nweights += (((train_df['target'].values>=0.5).astype(bool).astype(np.int) +  (train_df[identity_cols].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\n# Background Negative, Subgroup Positive\nweights += (((train_df['target'].values<0.5).astype(bool).astype(np.int) +  (train_df[identity_cols].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\n# loss weights\nloss_weight = 1.0 / weights.mean()\n\n# set y train vector\ny_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n\n# obtain aux \ny_aux_train = train_df[identity_cols].fillna(0).values\n\n# setup y aux train\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\n# y train mix\ny_train = np.hstack([y_train, y_aux_train])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup Training Dataset\nsetup a torch dataset for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup Training Data\nX = seq_data[:num_to_load]                \ny = y_train[:num_to_load]\n\n# Setup Validation Data\nX_val = seq_data[num_to_load:]                \ny_val = train_df['valid_target'].values[num_to_load:]\n\n# setup train and test dataframe? maybe iffy.... bc of head and tail\ntest_df  = train_df.tail(valid_size).copy()\ntrain_df = train_df.head(num_to_load)\n\n# setup torch dataset\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n\n# Seed Everything\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n# Set model to being deterministic\ntorch.backends.cudnn.deterministic = False\n\n# setup default model\nmodel = BertForSequenceClassification(bert_config,num_labels = NUM_OUTPUTS)\n\n# load trained model into the default model\nmodel.load_state_dict(torch.load(BERT_LOAD))\n\n# set the device to cuda GPU computing\nmodel.to(device)\n\n# zero the model gradiant\nmodel.zero_grad()\n\n# setup model to cuda cores\nmodel = model.to(device)\n\n# setup parameter optimizer\nparam_optimizer = list(model.named_parameters())\n\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\n# create new train table (May need to add .copy())\ntrain = train_dataset\n\n# obtain number of train optimization steps\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/acc_steps)\n\n# setup the optimizer\noptimizer = BertAdam(optimizer_grouped_parameters, lr = lr, warmup = WARM_UP, t_total = num_train_optimization_steps)\n\n# initialize model\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n\n# setup model for training\nmodel=model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# garbage collect\ngc.collect()\n\n# setup notebook display load bar for loop.\ntq = tqdm_notebook(range(EPOCHS))\n\nfor epoch in tq:\n    \n    # perform train collector\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    \n    # set performance parameters\n    avg_loss     = 0.\n    avg_accuracy = 0.\n    lossf        = None\n    \n    # Setup Display\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    \n    # zero optimizer gradiant\n    optimizer.zero_grad()\n    \n    # empty cache\n    torch.cuda.empty_cache()\n    \n    for i,(x_batch, y_batch) in tk0:\n        \n        # predict y results\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        \n        # calculate loss\n        y_batch_device = y_batch.to(device);\n        loss = custom_loss(y_pred,y_batch_device)\n       \n        # amp scale loss optimizer\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            \n            # scaled loss backward\n            scaled_loss.backward()\n            \n        # Wait for several backward steps    \n        if (i+1) % acc_steps == 0:       \n            \n            # Perform an additional optimiztion step\n            optimizer.step()  \n            \n            # zero gradiant for the optimizer\n            optimizer.zero_grad()\n            \n        # drill into loss function     \n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n            \n        tk0.set_postfix(loss = lossf)\n        avg_loss     += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float)).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n\n# save models\ntorch.save(model.state_dict(), save_model_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup default model\nmodel = BertForSequenceClassification(bert_config,num_labels = NUM_OUTPUTS)\n\n# load trained model into the default model\nmodel.load_state_dict(torch.load(output_model_file))\n\n# set the device to cuda GPU computing\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model parameters\nfor param in model.parameters():\n    param.requires_grad=False\n\n# set model to evaluation    \nmodel.eval()\n\n# setup torch dataset\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n\n# perform train loader\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup validation prediction array\nvalid_preds = np.zeros((len(X_val)))\n\n# setup tqdm load bar\ntk0 = tqdm_notebook(valid_loader)\n\nfor i,(x_batch)  in enumerate(tk0):\n    \n    # extract and transform x inputs\n    x_batch = x_batch[0].long()\n    \n    # generate predictions\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    \n    # validate\n    valid_preds[i*batch_size:(i+1)*batch_size] = sigmoid(pred[:,0].detach().cpu().squeeze().numpy())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[MODEL_NAME] = valid_preds \nbias_metrics_df     = compute_bias_metrics_for_model(test_df, identity_cols, MODEL_NAME, 'target')\nwrite_df            = get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))\nprint(write_df)\nbias_metrics_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('validation_results.csv', index=False)\nbias_metrics_df.to_csv('bias_metrics_results.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}