{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# refrence: https://www.kaggle.com/dborkan/benchmark-kernel\nclass SubmetricsAUC(object):\n    def __init__(self, valid_df, pred_y):\n        self.identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian',\n                                 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n        self.SUBGROUP_AUC = 'subgroup_auc'\n        self.BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n        self.BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\n        self.TOXICITY_COLUMN = 'target'\n\n        self.valid_df = valid_df\n        self.pred_y = pred_y\n        self.model_name = 'pred'\n        self.valid_df[self.model_name] = self.pred_y\n        self.valid_df = self.convert_dataframe_to_bool(self.valid_df)\n\n    def compute_auc(self):\n\n        bias_metrics_df = self.compute_bias_metrics_for_model(self.identity_columns,\n                                                              self.model_name,\n                                                              self.TOXICITY_COLUMN).fillna(0)\n\n        final_score = self.get_final_metric(bias_metrics_df, self.calculate_overall_auc())\n\n        return final_score\n\n    @staticmethod\n    def power_mean(series, p):\n        total = sum(np.power(series, p))\n        return np.power(total / len(series), 1 / p)\n\n    @staticmethod\n    def calculate_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    @staticmethod\n    def convert_to_bool(df, col_name):\n        df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n\n    def convert_dataframe_to_bool(self, df):\n        bool_df = df.copy()\n        for col in ['target'] + self.identity_columns:\n            self.convert_to_bool(bool_df, col)\n        return bool_df\n\n    def compute_subgroup_auc(self, subgroup, label, model_name):\n        subgroup_examples = self.valid_df[self.valid_df[subgroup]]\n        return self.calculate_auc(subgroup_examples[label], subgroup_examples[model_name])\n\n    def compute_bpsn_auc(self, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n        subgroup_negative_examples = self.valid_df[self.valid_df[subgroup] & ~self.valid_df[label]]\n        non_subgroup_positive_examples = self.valid_df[~self.valid_df[subgroup] & self.valid_df[label]]\n        examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n        return self.calculate_auc(examples[label], examples[model_name])\n\n    def compute_bnsp_auc(self, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n        subgroup_positive_examples = self.valid_df[self.valid_df[subgroup] & self.valid_df[label]]\n        non_subgroup_negative_examples = self.valid_df[~self.valid_df[subgroup] & ~self.valid_df[label]]\n        examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n        return self.calculate_auc(examples[label], examples[model_name])\n\n    def compute_bias_metrics_for_model(self,\n                                       subgroups,\n                                       model,\n                                       label_col,\n                                       include_asegs=False):\n        \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n        records = []\n        for subgroup in subgroups:\n            record = {'subgroup': subgroup,\n                      'subgroup_size': len(self.valid_df[self.valid_df[subgroup]])\n                      }\n\n            record[self.SUBGROUP_AUC] = self.compute_subgroup_auc(subgroup, label_col, model)\n            record[self.BPSN_AUC] = self.compute_bpsn_auc(subgroup, label_col, model)\n            record[self.BNSP_AUC] = self.compute_bnsp_auc(subgroup, label_col, model)\n            records.append(record)\n        return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n    def calculate_overall_auc(self):\n        true_labels = self.valid_df[self.TOXICITY_COLUMN]\n        predicted_labels = self.valid_df[self.model_name]\n        return roc_auc_score(true_labels, predicted_labels)\n\n    def get_final_metric(self, bias_df, overall_auc, power=-5, weight=0.25):\n        bias_score = np.average([\n            self.power_mean(bias_df[self.SUBGROUP_AUC], power),\n            self.power_mean(bias_df[self.BPSN_AUC], power),\n            self.power_mean(bias_df[self.BNSP_AUC], power)\n        ])\n        return (weight * overall_auc) + ((1 - weight) * bias_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## how to use"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# # just like this\n# score = SubmetricsAUC(train_df, train_preds).compute_auc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # seems like this \n# auc = roc_auc_score(train_y, train_preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}