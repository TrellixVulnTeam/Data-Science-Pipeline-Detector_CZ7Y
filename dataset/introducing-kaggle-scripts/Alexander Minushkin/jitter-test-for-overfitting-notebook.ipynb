{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":"%matplotlib inline"},{"cell_type":"markdown","metadata":{},"source":"#Jitter Test for Overfitting"},{"cell_type":"markdown","metadata":{},"source":"In this notebook I describe some intuition about possible testing of predicion models for overfitting.\n\nNotebook contain description of the idea, its application to classification and regression models with continuous features, comparison with cross validation and performance considerations."},{"cell_type":"markdown","metadata":{},"source":"###Overfitting in Classification Models\n\nAssume that we have models **M_0** (overfitted) and  **M_1** (non-overfitted) that trained on data **[X, y ]**. \n\nFor overfitted model boundary between predicted categories is bigger than for non-overfitted model. \nSo if we jitter *X* and look at the accuracy of prediction on train data **M_i(jitter(X, \\sigma)) -> y** it will decrease faster with growing **\\sigma** for overfitted model, since more points will cross boundary between predictions.\n\nTherefore accuracy decrease should look like this:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nx = np.linspace(0, 0.5, 100)\n\nplt.plot( x, 0.7 - 0.5*x + 0.3*np.exp(-x*20), label = \"Overfitted model\")\nplt.plot( x, 0.9 - 0.5*x, label = \"Non-Overfitted model\")\nplt.plot( x, 0.6 - 0.5*x, label = \"Just Bad model\")\n\naxes = plt.gca()\naxes.set_ylim([0, 1.1])\n\nplt.legend(loc=3)\nplt.suptitle(\"Expected decrease of accuracy in jitter test\")\n\naxes.set_xlabel('$\\sigma$')\naxes.set_ylabel('Accuracy')\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"Probably area under the plot of **Accuracy(M(jitter(X,\\sigma)))** can be better metric to analyze models than accuracy itself \nsince it also can incorporate information about overfitting.\n\nBelow you can see examples of different prediction models from \"Supervised learning superstitions cheat sheet\" \nnotebook https://github.com/rcompton/ml_cheat_sheet and plots of accuracy in jitter test for these models."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Jitter test code\n\nfrom sklearn.metrics import accuracy_score\n\ndef jitter(X, scale):\n    #out = X.copy()\n    if scale > 0:        \n        return X + np.random.normal(0, scale, X.shape)\n    return X\n\ndef jitter_test(classifier, X, y, metric_FUNC = accuracy_score, sigmas = np.linspace(0, 0.5, 30), averaging_N = 5):\n    out = []\n    \n    for s in sigmas:\n        averageAccuracy = 0.0\n        for x in range(averaging_N):\n            averageAccuracy += metric_FUNC( y, classifier.predict(jitter(X, s)))\n\n        out.append( averageAccuracy/averaging_N)\n\n    return (out, sigmas, np.trapz(out, sigmas))\n\nallJT = {}"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\"\"\"\nBuild some datasets that I'll demo the models on\n\"\"\"\n\nimport sklearn\nimport sklearn.datasets\n#sklearn two moons generator makes lots of these...\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nXs = []\nys = []\n\n#low noise, plenty of samples, should be easy\nX0, y0 = sklearn.datasets.make_moons(n_samples=1000, noise=.05)\nXs.append(X0)\nys.append(y0)\n\n#more noise, plenty of samples\nX1, y1 = sklearn.datasets.make_moons(n_samples=1000, noise=.3)\nXs.append(X1)\nys.append(y1)\n\n#less noise, few samples\nX2, y2 = sklearn.datasets.make_moons(n_samples=200, noise=.05)\nXs.append(X2)\nys.append(y2)\n\n#more noise, less samples, should be hard\nX3, y3 = sklearn.datasets.make_moons(n_samples=200, noise=.3)\nXs.append(X3)\nys.append(y3)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def plotter(model, X, Y, ax, npts=5000):\n    \"\"\"\n    Simple way to get a visualization of the decision boundary \n    by applying the model to randomly-chosen points\n    could alternately use sklearn's \"decision_function\"\n    at some point it made sense to bring pandas into this\n    \"\"\"\n    xs = []\n    ys = []\n    cs = []\n    for _ in range(npts):\n        x0spr = max(X[:,0])-min(X[:,0])\n        x1spr = max(X[:,1])-min(X[:,1])\n        x = np.random.rand()*x0spr + min(X[:,0])\n        y = np.random.rand()*x1spr + min(X[:,1])\n        xs.append(x)\n        ys.append(y)\n        cs.append(model.predict([x,y]))\n    ax.scatter(xs,ys,c=list(map(lambda x:'lightgrey' if x==0 else 'black', cs)), alpha=.35)\n    ax.hold(True)\n    ax.scatter(X[:,0],X[:,1],\n                 c=list(map(lambda x:'r' if x else 'lime',Y)), \n                 linewidth=0,s=25,alpha=1)\n    ax.set_xlim([min(X[:,0]), max(X[:,0])])\n    ax.set_ylim([min(X[:,1]), max(X[:,1])])\n    return"},{"cell_type":"markdown","metadata":{},"source":"####LogisticRegression"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import sklearn.linear_model\nclassifier = sklearn.linear_model.LogisticRegression()\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\n\nallJT[str(classifier)] = list()\n\ni=0\nfor X,y in zip(Xs,ys): \n    classifier.fit(X,y)\n    plotter(classifier,X,y,ax=axes[i//2,i%2])\n    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n    i += 1\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"####DecisionTreeClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import sklearn.tree\nclassifier = sklearn.tree.DecisionTreeClassifier()\n\nallJT[str(classifier)] = list()\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\ni=0\nfor X,y in zip(Xs,ys): \n    classifier.fit(X,y)\n    plotter(classifier,X,y,ax=axes[i//2,i%2])\n    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n    i += 1\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"####Support Vector Machines"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import sklearn.svm\nclassifier = sklearn.svm.SVC()\n\nallJT[str(classifier)] = list()\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\ni=0\nfor X,y in zip(Xs,ys): \n    classifier.fit(X,y)\n    plotter(classifier,X,y,ax=axes[i//2,i%2])\n    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n    i += 1\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"####Accuracy in jitter test"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,10))\n\n\n\nhandlers =[]\nfor c_name in allJT:\n    for i in range(4): \n\n        ax=axes[i//2,i%2]\n        \n        ax.set_xlim([0, 0.5])\n        ax.set_ylim([0.7, 1.1])\n\n        accuracy, sigmas, area = allJT[c_name][i]\n        ax.plot( sigmas, accuracy, label = \"Area: {:.2} \".format(area) + c_name.split(\"(\")[0])\n        ax.legend()\n    \nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"####Conclusion\n\nDecisionTreeClassifier demonstrate overfitting behavior on high-noise data (rightmost pictures). That can be seen from plot itself and computed area under accuracy curve."},{"cell_type":"markdown","metadata":{},"source":"###Overfitting in Regression Models\n\nSame approach can be applied to regression."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Generate sample data for regression\nnp.random.seed(2)\nN_points = 10\nX = np.linspace(0, 1, N_points)\ny = np.random.rand(N_points) + X\nX = X[:, np.newaxis]\n\nallJT = {}\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11,5))\n\nax=axes[0]\nax.scatter(X, y)\n\n#for i in [1, 5, 9]:\nfor i in [1, 4, 7]:\n    model = Pipeline([('poly', PolynomialFeatures(degree=i)),\n                   ('linear', LinearRegression())])\n\n    model = model.fit(X, y)\n    \n    allJT[i] = jitter_test(model, X, y, lambda x,y: np.exp(-mean_squared_error(x,y)))\n    \n    ax.plot(X, model.predict(X), label=\"Degree {}\".format(i))\n\nplt.suptitle(\"Polynomial regression\", fontsize=16)\nax.legend(loc=2)\nax.set_xlabel('$X$')\nax.set_ylabel('$y$')\n\n\n\nax=axes[1]\n#ax.set_xlim([0, .4])\n\nfor i in allJT:\n    accuracy, sigmas, area = allJT[i]\n    ax.plot( sigmas, np.exp(accuracy), label = \"{:.3} \".format(area) + \"Degree {}\".format(i) )\n\nax.legend(loc=10)\nax.set_xlabel('$\\sigma$')\nax.set_ylabel('$exp(-mean_squared_error)$')\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"##Comparison with Cross Validation\n\nLet's try to find best model using cross validation and compare results with JTO"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn import cross_validation\n#more noise, plenty of samples\n#X1, y1\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X1, y1, test_size=0.4, random_state=120)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def CV_test( X_train, y_train, depth, averaging, random_state_ = 1):\n    out =[]\n    for i in depth:\n        classifier = sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state = random_state_)\n        scores = cross_validation.cross_val_score(classifier, X_train, y_train, cv=averaging)\n        out.append(scores.mean())\n        \n    return out\n\ndef JOT_test(X_train, y_train, depth, averaging, random_state_ = 1):\n    out =[]\n    \n\n    for i in depth:\n        classifier = sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state = random_state_)\n        classifier.fit(X_train, y_train)\n        jitter_errors, sigmas, score = jitter_test(classifier, X_train, y_train, averaging_N=averaging)\n        \n        #plt.plot(sigmas, jitter_errors, alpha=float(i+1)/(max(depth)+1), color=\"blue\")\n        out.append( score)\n\n    #plt.show()\n    return out\n\ntest_range = range(1,30)\naveraging = 50\n\ncv_accuracy = CV_test(X_train, y_train, test_range, averaging)\njot_score = JOT_test(X_train, y_train, test_range, averaging)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def plot_scoring(test_range, score, title):\n    plt.scatter(test_range, score)\n    plt.title(title)\n    \n    best_id = np.argmax(score)\n    plt.scatter(test_range[best_id], score[best_id], color=\"red\", label = \"Best depth = {}\".format(test_range[best_id]))    \n    plt.legend()\n    plt.show()\n    \n    return best_id\n\nscore_cv_id = plot_scoring(test_range, cv_accuracy, \"Cross validation\")\n\nscore_JTO_id = plot_scoring(test_range, jot_score, \"Jitter test for overfitting\")\n\n# Compute scoring for test set\ntest_score = []\nfor i in test_range:\n    test_score.append(sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state=1).fit(X_train, y_train).score(X_test, y_test))\n\nscore_test = plot_scoring(test_range, test_score, \"Test set scoring\")\n\nprint(\"Scores on test set\")\nprint(\"CV   = {}\\nJTO  = {}\\nTest = {}\".format(test_score[score_cv_id], test_score[score_JTO_id], test_score[score_test]))"},{"cell_type":"markdown","metadata":{},"source":"Let's see how averaged results for JOT and CV looks like."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"repeat_N = 10\ntest_best_score = 0.\njot_best_score = 0.\ncv_best_score = 0.\n\nfor i in range(repeat_N):\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X1, y1, test_size=0.4)\n    \n    cv_accuracy = CV_test(X_train, y_train, test_range, averaging, None)    \n    \n    jot_score = JOT_test(X_train, y_train, test_range, averaging, None)    \n    \n    test_score = []\n    for i in test_range:\n        test_score.append(sklearn.tree.DecisionTreeClassifier(max_depth=i).fit(X_train, y_train).score(X_test, y_test))\n        \n    test_best_score += test_score[np.argmax(test_score)]\n    jot_best_score += test_score[np.argmax(jot_score)]\n    cv_best_score += test_score[np.argmax(cv_accuracy)]\n\nprint(\"Average score:\")\nprint(\"Test = {}\\nJOT  = {}\\nCV   = {}\".format(test_best_score/repeat_N, jot_best_score/repeat_N, cv_best_score/repeat_N))"},{"cell_type":"markdown","metadata":{},"source":"###Conlusion\nAt least in some cases JOT shows small improvemnet over CV in average."},{"cell_type":"markdown","metadata":{},"source":"##Performance testing"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import timeit\n\nrepeat_N = 10\ncv_time = timeit.Timer('CV_test(X_train, y_train, test_range, averaging)', \"from __main__ import CV_test, X_train, y_train, test_range, averaging\").timeit(repeat_N)\njot_time = timeit.Timer('JOT_test(X_train, y_train, test_range, averaging)', \"from __main__ import JOT_test, X_train, y_train, test_range, averaging\").timeit(repeat_N)\n\nprint(\"Timing\")\nprint(\"CV  = {} sec.\\nJOT = {} sec.\".format(cv_time, jot_time))"},{"cell_type":"markdown","metadata":{},"source":"Jitter test is much slower than cross validation. This could be explained by non-optimal implementation. \nBut in any case, I would expect that it will be faster when model training is very slow. \nSince in CV case we have to train model on every data subset, then make prediction and find out a score. \nWhile for JOT we train model only once and than make multiple predictions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}