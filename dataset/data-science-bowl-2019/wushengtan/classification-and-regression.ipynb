{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Acknowledgment\n* https://www.kaggle.com/braquino/convert-to-regression\n* https://www.kaggle.com/challenge1a3/convert-to-regression"},{"metadata":{},"cell_type":"markdown","source":"# My upgrade:\n* Dropout = 0.2 (everywhere)\n* learning_rate = 0.02 (everywhere)\n* weights = {'lbg': 0.6, 'xgb': 0.25, 'nn': 0.15}"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nimport random\nimport shap\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport json\npd.set_option('display.max_columns', 1000)\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>div.output_scroll { height: 88em; }</style>\"))  \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    #dist = Counter(reduce_train['accuracy_group'])\n    dist = {3.0: 9936, 0.0: 4978, 2.0: 2447, 1.0: 2676}\n    s = sum(list(dist.values()))\n    for k in dist:\n        #dist[k] /= len(reduce_train)\n        dist[k] /= s\n    #reduce_train['accuracy_group'].hist()\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n        \n    print('bound =', bound)\n    \n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cohenkappa(ypred, y):\n    y = y.get_label().astype(\"int\")\n    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n    return \"cappa\", loss, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n    train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\") \n    del keep_id\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n\n    train['hour'] = train['timestamp'].dt.hour\n    test['hour'] = test['timestamp'].dt.hour\n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clip_time = {'Welcome to Lost Lagoon!':19,'Tree Top City - Level 1':17,'Ordering Spheres':61, 'Costume Box':61,\n        '12 Monkeys':109,'Tree Top City - Level 2':25, 'Pirate\\'s Tale':80, 'Treasure Map':156,'Tree Top City - Level 3':26,\n        'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18,\n        'Balancing Act':72, 'Lifting Heavy Things':118,'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19,\n        'Heavy, Heavier, Heaviest':61}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_event_features():\n    event_feats = {}\n    event_feats['event_code'] = event_code_count.copy() # key: event_code\n    event_feats['event_id'] = event_id_count.copy() #key: event_id\n    event_feats['title'] = title_count.copy() #key:title\n    event_feats['title_event_code'] = title_event_code_count.copy() # key:title_event_code\n    #event_feats['last_accuracy_title'] = {'acc_' + title: -1 for title in [title_labels[i] for i in assess_title_set]}\n    return event_feats\n\ndef init_user_activities_features():\n    user_act_feats = {}\n    user_act_feats['career_count'] = {'CareerClip':0, 'CareerActivity': 0, 'CareerAssessment': 0, 'CareerGame':0}\n    user_act_feats['career_time'] = {'CareerClipTime':0, 'CareerActivityTime':0, 'CareerAssessmentTime':0, 'CareerGameTime':0}\n    user_act_feats['career_avg_time'] = {'CareerAVGClipTime':0, 'CareerAVGActivityTime':0,'CareerAVGAssessmentTime':0, 'CareerAVGGameTime':0}\n    return user_act_feats\n\ndef update_counters(session, event_feats, col):\n    counter = event_feats[col]\n    num_of_session_count = Counter(session[col])\n    for k in num_of_session_count.keys():\n        x = k\n        if col == 'title':\n            x = activities_labels[k]\n        counter[x] += num_of_session_count[k]\n    return counter\n\ndef get_group(accuracy):\n    if accuracy <= 0:\n        return 0\n    elif accuracy ==1:\n        return 3\n    elif accuracy == 0.5:\n        return 2\n    else:\n        return 1\n    \n# collect basic features from original data:\ndef get_context(user_sample, test_set = False):\n\n    # generate raw data chart in train:uid1: session1, session2 ...\n    \n    # in test: uid: session1, session2 (if is the last session append to final_test df)\n    # normal test chart used to get user info and assessment info, and then merge to final test for prediction.\n\n    user_act_feats = init_user_activities_features()\n    all_assessments = []\n    accumulated_true_attempts = 0\n    accumulated_false_attempts = 0\n    init = 1\n    \n    \n    # iterate by session, n sessions + assessment is a cycle\n    for i, session in user_sample.groupby('game_session', sort = False):\n        if init == 1:\n            # Cycle restart\n            features = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features.update({'ClipTime':0, 'ActivityTime':0, 'AssessmentTime':0, 'GameTime':0})\n            event_feats = init_event_features()\n            accuracy_group = {0:0, 1:0, 2:0, 3:0}\n            true_attempts, false_attempts = 0, 0\n            init = 0\n            \n        # session info \n        session_type, session_title  = session['type'].iloc[0], session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        session_time = (session.iloc[-1,2] - session.iloc[0,2]).seconds\n        \n        \n        features[session_type] +=1\n        features[session_type + 'Time'] += session_time\n        \n        if session_type != 'Assessment':\n            user_act_feats['career_time']['Career'+ session_type +'Time'] += session_time\n            user_act_feats['career_count']['Career'+ session_type] += 1\n        # assessment\n        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # if no attempts, skip.\n            if (true_attempts + false_attempts):\n                accuracy = true_attempts/(true_attempts + false_attempts)\n            elif test_set:\n                accuracy = 0\n            else:\n                init = 1\n                continue\n            # constructing features\n            features.update(event_feats['event_code'])\n            features.update(event_feats['event_id'])\n            features.update(event_feats['title'])\n            features.update(event_feats['title_event_code'])\n            features.update(user_act_feats['career_count'])\n            features.update(user_act_feats['career_time'])\n            features.update(user_act_feats['career_avg_time'])\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            features['session_title'] = session_title\n            features['true_attempts'] = true_attempts\n            features['false_attempts'] = false_attempts\n            features['accuracy'] = accuracy\n            features['accuracy_group'] = get_group(accuracy)\n            features['accumulated_true_attempts'] = accumulated_true_attempts\n            features['accumulated_false_attempts'] = accumulated_false_attempts\n            num = accumulated_true_attempts + accumulated_false_attempts\n            features['accumulated_accuracy'] = accumulated_true_attempts/(num-1) if num > 1 else 0\n\n            all_assessments.append(features)\n            init = 1\n            # update features afterwards to prevent data leakage.\n            accumulated_true_attempts += true_attempts\n            accumulated_false_attempts += false_attempts\n            user_act_feats['career_time']['Career'+ session_type +'Time'] += session_time\n            user_act_feats['career_count']['Career'+ session_type] += 1\n            user_act_feats['career_avg_time']['CareerAVG'+session_type + 'Time'] = user_act_feats['career_time']['Career'+ session_type +'Time']/user_act_feats['career_count']['Career'+ session_type]\n            \n        event_feats['event_code'] = update_counters(session, event_feats, 'event_code')    \n        event_feats['event_id'] = update_counters(session, event_feats, 'event_id')\n        event_feats['title'] = update_counters(session, event_feats, 'title')\n        event_feats['title_event_code'] = update_counters(session, event_feats, 'title_event_code')\n        \n    return all_assessments\n    \n#def fix_json_colname(df):\n    #df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n    \ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test_history = []\n    compiled_test = []\n    \n    for _,(ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = train['installation_id'].nunique()):\n        compiled_train.extend(get_context(user_sample, test_set = False))\n        \n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = test['installation_id'].nunique()):\n        test_data = get_context(user_sample, test_set = True)\n        compiled_test_history.extend(test_data[:-1])\n        compiled_test.append(test_data[-1])\n    \n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    reduce_test_history = pd.DataFrame(compiled_test_history)\n    \n    #fix_json_colname(reduce_train)\n    #fix_json_colname(reduce_test)\n    #fix_json_colname(reduce_test_history)\n    for df in [reduce_train, reduce_test, reduce_test_history]:\n        for col in df.columns:\n            if df[col].dtypes == np.float64 or df[col].dtypes == np.int64:\n                df[col] = df[col].astype(np.float32)\n                \n    print('reduce_train shape:' + str(reduce_train.shape) + ', reduce_test shape: ' + str(reduce_test.shape) +'.' + 'reduce_test_history shape: ' + str(reduce_test_history.shape))\n    print('Totally ' + str(reduce_train['installation_id'].nunique()) + ' users.')\n    gc.collect()\n    reduce_train_merge = pd.concat([reduce_train, reduce_test_history], axis = 0).reset_index().drop(columns = ['index'])\n    categorical = ['session_title']\n    # merge reduce_test_history to reduce_train\n    return reduce_train_merge, reduce_test, categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    \n    #The user_sample is a DataFrame from train or test where the only one \n    #installation_id is filtered\n    #And the test_set parameter is related with the labels processing, that is only requered\n    #if test_set=False\n    \n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    clip_durations = []\n    Activity_durations = []\n    Game_durations = []\n    \n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # last features\n    sessions_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n        if session_type == 'Clip':\n            clip_durations.append((clip_time[activities_labels[session_title]]))\n        \n        if session_type == 'Activity':\n            Activity_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n        \n        if session_type == 'Game':\n            Game_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features['installation_session_count'] = sessions_count\n#             features['hour'] = session['hour'].iloc[-1]\n            \n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                \n            if clip_durations == []:\n                features['Clip_duration_mean'] = 0\n                features['Clip_duration_std'] = 0\n            else:\n                features['Clip_duration_mean'] = np.mean(clip_durations)\n                features['Clip_duration_std'] = np.std(clip_durations)\n                \n            if Activity_durations == []:\n                features['Activity_duration_mean'] = 0\n                features['Activity_duration_std'] = 0\n            else:\n                features['Activity_duration_mean'] = np.mean(Activity_durations)\n                features['Activity_duration_std'] = np.std(Activity_durations)\n                \n            if Game_durations == []:\n                features['Game_duration_mean'] = 0\n                features['Game_duration_std'] = 0\n            else:\n                features['Game_duration_mean'] = np.mean(Game_durations)\n                features['Game_duration_std'] = np.std(Game_durations)\n                \n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n   \n    return all_assessments\nimport random\n\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        t = get_data(user_sample)\n        for xt in range(len(t)):\n            if xt != len(t) - 1:\n                if random.random() >= 0.25:\n                    compiled_train.append(t[xt])\n            else:\n                compiled_train.append(t[xt])\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_sequence_kfolds_split2(df, k, random_state):\n    random.seed(random_state)\n    one_shot_ids = []\n    train_index = []\n    test_index = []\n    for ins_id, uid in df.groupby('installation_id', sort = False):\n        shape = uid.shape\n        if shape[0] > k: # if number of records is larger than k\n            for n in range(k): \n                train_index.extend(uid.iloc[:-1].index.values.tolist())\n                test_index.extend(uid.iloc[-1:].index.values.tolist())\n        elif shape[0] == 1:\n            one_shot_ids.extend(uid.index.values.tolist())\n    for i in range(k):\n        random_train = random.sample(train_index, int(.8*len(train_index)))\n        random_test = random.sample(test_index, int(.8*len(test_index)))\n        yield random_train, random_test     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq\nclass Base_Model(object):\n    def __init__(self, train_df, test_df, features, categoricals=[], n_best = 4, n_splits = 5, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.n_best = n_best\n        print('train data shape: (%d, %d)'%(train_df.shape[0], train_df.shape[1]))\n        print('test data shape: (%d, %d)'%(test_df.shape[0], test_df.shape[1]))\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = 'accuracy_group'\n        self.cv = self.get_cv()\n        self.verbose = verbose\n        self.params = self.get_params()\n        self.y_pred, self.score, self.model = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        #cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return time_sequence_kfolds_split2(self.train_df, k = self.n_splits, random_state = int(round(random.random())))\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        oof_pred = np.zeros((len(reduce_train), ))\n        y_pred = np.zeros((len(reduce_test), ))\n        y_pred_list = []\n        score_square_sum = 0\n        score_sum = 0\n        \n              \n              \n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            conv_x_val = self.convert_x(x_val)\n            \n            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n            x_test = self.convert_x(self.test_df[self.features])\n            partial_score = eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]\n            y_pred_list.append((partial_score, model.predict(x_test).reshape(y_pred.shape)))\n            score_sum += partial_score/self.n_splits\n            score_square_sum += (partial_score)**2\n            print('Partial score of fold {} is: {}'.format(fold, partial_score))\n            \n        score_sum_std = (score_square_sum - score_sum**2)/((self.n_splits - 1)*self.n_splits)\n        \n        best_score = 0\n        best_score_square = 0\n        for i, (score, y_pred_ele) in enumerate(heapq.nlargest(self.n_best, y_pred_list)):\n            best_score+= score/self.n_best\n            best_score_square += score**2\n            y_pred += y_pred_ele/self.n_best\n        \n        best_score_std = (best_score_square - best_score**2)/((self.n_best -1)*self.n_best)\n        \n        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n        if self.verbose:\n            print('Average partial score is: ' +str(score_sum) +' +/- ' +str(score_sum_std) + 'std')\n            print('Best n partial score is: ' + str(best_score) + '+/- ' + str(best_score_std) + 'std')\n    \n        return y_pred, best_score, model\n    \nclass Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        objective = 'tweedie' #poisson, tweebie, regression(rmse/mae)\n        params = {'n_estimators':5000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'metric': 'tweedie',\n                    'subsample': 0.66,\n                    'num_leaves': 236,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.02,\n                    'feature_fraction': 0.9,\n                    'max_depth': 16,\n                    'lambda_l1': .02,  \n                    'lambda_l2': .8,\n                    'early_stopping_rounds': 100,\n                    'verbose': 0\n                    }\n        return params\n    \nclass Xgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return xgb.train(self.params, train_set, \n                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=verbosity, early_stopping_rounds=100)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        return train_set, val_set\n    \n    def convert_x(self, x):\n        return xgb.DMatrix(x)\n        \n    def get_params(self):\n        params = {'colsample_bytree': 0.8,                 \n            'learning_rate': 0.02,\n            'max_depth': 10,\n            'subsample': 1,\n            'objective':'reg:squarederror',\n            #'eval_metric':'rmse',\n            'min_child_weight':3,\n            'gamma':0.25,\n            'n_estimators':5000}\n        return params\n    \nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nclass Nn_Model(Base_Model):\n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=10, verbose=True):\n        features = features.copy()\n        if len(categoricals) > 0:\n            for cat in categoricals:\n                enc = OneHotEncoder()\n                train_cats = enc.fit_transform(train_df[[cat]])\n                test_cats = enc.transform(test_df[[cat]])\n                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n                features += cat_cols\n                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n                train_df = pd.concat([train_df, train_cats], axis=1)\n                test_df = pd.concat([test_df, test_cats], axis=1)\n        scalar = MinMaxScaler()\n        train_df[features] = scalar.fit_transform(train_df[features])\n        test_df[features] = scalar.transform(test_df[features])\n        print(train_df[features].shape)\n        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n        \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n            tf.keras.layers.Dense(200, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(100, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(25, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1, activation='relu')\n        ])\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n        print(model.summary())\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                 callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n        return model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nfrom random import choice\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer,Dense\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow import metrics\nfrom sklearn.decomposition import PCA\nK.clear_session()\n\nclass FactorizationMachine(Layer):\n    def __init__(self, params):\n        super (FactorizationMachine, self).__init__()\n        self.params = params\n        self.v_shape, self.v2_shape = self.params['v_shape'], self.params['v2_shape']\n        self.first_order_dense = Dense(1, activation = 'relu')\n        #self.output_dense = Dense(1, activatinon = 'relu')\n        \n    def build(self, input_shape):\n        shape1, shape2 = input_shape\n        self.V = self.add_variable('V', shape = [int(shape2[-1]), self.v_shape])\n\n    def call(self, input):\n        '''\n        input: stacked embeddings with shape (batch,fields,embdim)\n        '''\n        x_train, embedding = input\n        squared_of_sum = tf.reduce_sum(tf.square(tf.matmul(embedding, self.V)), axis = -1) # (b,f,e) * (e,k) ->(b,f,k) ->b,f\n        #print(self.squared_of_sum.shape)\n        sum_of_squared = tf.reduce_sum(tf.matmul(tf.square(embedding), tf.square(self.V)), axis = -1) # (b,f,e) * (e,k) ->(b,f,k) ->b,f\n        second_order = tf.reduce_sum(0.5 * tf.subtract(squared_of_sum, sum_of_squared), axis = -1)\n        #devider_order = tf.reduce_sum(0.5 * tf.subtract(squared_of_sum, sum_of_squared), axis = -1)\n        first_order = tf.reduce_sum(self.first_order_dense(x_train), axis = -1)\n        return second_order + first_order, tf.concat([tf.reshape(second_order, (-1,1)),  tf.reshape(first_order, (-1,1))], axis = 1)\n    \nclass DFM_Model(Base_Model):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        features = features.copy()\n        if categoricals:\n            for cat in categoricals:\n                enc = OneHotEncoder()\n                train_cats = enc.fit_transform(train_df[[cat]])\n                test_cats = enc.transform(test_df[[cat]])\n                self.cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n                features += self.cat_cols\n                train_cats = pd.DataFrame(train_cats.toarray(), columns = self.cat_cols)\n                test_cats = pd.DataFrame(test_cats.toarray(), columns = self.cat_cols)\n                train_df = pd.concat([train_df, train_cats], axis=1)\n                test_df = pd.concat([test_df, test_cats], axis=1)\n        scaler = MinMaxScaler()\n        train_df[features] = scaler.fit_transform(train_df[features])\n        test_df[features] = scaler.transform(test_df[features])\n        self.look_up = self.get_lookup(train_df[features].reset_index().drop(columns = 'index'))\n        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n        \n    def get_lookup(self, df):\n        fields = {} # embedding fields in dnnfm\n        fields['event_code'] = list(event_code_count.keys()) # key: event_code\n        fields['event_id'] = list(event_id_count.keys()) #key: event_id\n        fields['title'] = list(title_count.keys()) #key:title\n        fields['title_event_code'] = list(title_event_code_count.keys()) # key:title_event_code\n        fields['session_title'] = self.cat_cols\n        fields_list = []\n        for k, v in fields.items():\n            if len(v) < 4:\n                continue\n            idx = [df.columns.get_loc(c) for c in v if c in df.columns]\n            fields[k] = idx\n            fields_list.extend(v)\n        #raw_list = self.reduce_train.columns.tolist()\n        #fields['linear'] = [self.reduce_train.columns.get_loc(c) for c in raw_list if c not in fields_list]\n        return fields\n    \n    def get_params(self):\n        params = {'fmlr': 4e-4,\n                    'dfmlr': 4e-4,\n                    'v_shape': 8,\n                    'v2_shape': 8,\n                    #'batch_size': 128,\n                    'emb_dim': 65,\n                    'dnn_dim': 89,\n                    'l1': .41,\n                    'l2': .27,\n                    'fm_l1': .14,\n                    'fm_l2': .95,\n                    'epoch':1000,\n                    'objective':'mse',\n                    'metrics':['mse'],\n                    }\n        return params\n    \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train.values, 'y': y_train.values}\n        val_set = {'X': x_val.values, 'y': y_val.values}\n        return train_set, val_set\n    \n    def train_model(self, train_set, val_set):\n        x_in = tf.keras.layers.Input(shape = (train_set['X'].shape[1],))\n        #print(train_set['X'].shape)\n        embedding_list = []\n        fm_list = []\n        b_l = tf.shape(x_in)[0]\n        for k,v in self.look_up.items():\n            if len(set(v)) < 4:\n                continue\n            begin = min(v)\n            length = max(v) - min(v) +1\n            field = tf.slice(x_in, [0, begin], [b_l, length]) # (b, n)\n            embedding = Dense(self.params['emb_dim'], activation = 'relu')(field)\n            #if k != 'linear':\n            fm_list.append(embedding)\n            embedding_list.append(embedding)\n            \n        fm_in = tf.stack(fm_list, axis = 1) #(b,f,e)\n        dnn_in = tf.concat(embedding_list, axis = 1) #(b, f*e)\n        fm_layer = FactorizationMachine(self.params)\n        fm_out, fm_out_2 = fm_layer((x_in, fm_in))\n        \n        dnn1 = Dense(self.params['dnn_dim'], 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1 = self.params['l1'], l2 = self.params['l2']))(dnn_in)\n        dnn1 = tf.keras.layers.Dropout(0.2)(dnn1)\n        dnn2 = Dense(self.params['dnn_dim'], 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1 = self.params['l1'], l2 = self.params['l2']))(dnn1)\n        dnn2 = tf.keras.layers.Dropout(0.2)(dnn2)\n        dnn3 = Dense(self.params['dnn_dim'], 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1 = self.params['l1'], l2 = self.params['l2']))(dnn2)\n        dnn3 = tf.keras.layers.Dropout(0.2)(dnn3)\n        dnn_out = Dense(self.params['dnn_dim'], 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1 = self.params['l1'], l2 = self.params['l2']))(dnn3)\n        final_out = Dense(1, kernel_regularizer=keras.regularizers.l1_l2(l1 = self.params['l1'], l2 = self.params['l2']))(tf.concat([dnn_out, fm_out_2], axis = 1))\n        \n        self.fm_model = Model(inputs = x_in, outputs = fm_out)\n        self.dfm_model = Model(inputs = x_in, outputs = final_out) \n        opt = Adam(lr = self.params['fmlr'], amsgrad=True)\n        self.fm_model.compile(optimizer = opt, loss = self.params['objective'], metrics = self.params['metrics'])\n        opt2 = Adam(lr = self.params['dfmlr'], amsgrad=True)\n        self.dfm_model.compile(optimizer = opt2, loss = self.params['objective'], metrics = self.params['metrics'])\n        \n        save_best1 = tf.keras.callbacks.ModelCheckpoint('fm_model.w8', save_weights_only=True, save_best_only=True, verbose = 1)\n        save_best2 = tf.keras.callbacks.ModelCheckpoint('dfm_model.w8', save_weights_only=True, save_best_only=True, verbose = 1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        print(self.fm_model.summary())\n        self.fm_model.fit(train_set['X'],\n                     train_set['y'],\n                     validation_data = (val_set['X'], val_set['y']),\n                     epochs = self.params['epoch'],\n                     callbacks = [save_best1, early_stop],\n                     verbose = 1)\n        self.fm_model.load_weights('fm_model.w8')\n        print(self.dfm_model.summary())\n        self.dfm_model.fit(train_set['X'], \n                      train_set['y'], \n                     validation_data=(val_set['X'], val_set['y']),\n                     epochs = self.params['epoch'],\n                     callbacks = [save_best2, early_stop],\n                     verbose = 1)\n        self.dfm_model.load_weights('dfm_model.w8')\n        \n        return self.dfm_model\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n# tranform function to get the train and test set\n# event features dict:\nevent_code_count = {ev:0 for ev in list_of_event_code}\nevent_id_count = {ev:0 for ev in list_of_event_id}\ntitle_count = {ev:0 for ev in activities_labels.values()}\ntitle_event_code_count = {t_ev:0 for t_ev in all_title_event_code}\nreduce_train, reduce_test, categoricals = get_train_and_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    msre = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(msre)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n        plt.show()\n    return msre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call feature engineering function\nfeatures = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\nfeatures = [x for x in features if x not in ['accuracy_group', 'installation_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_exclude = [] \najusted_test = reduce_test.copy()\nfor feature in ajusted_test.columns:\n    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']:\n        data = reduce_train[feature]\n        train_mean = data.mean()\n        data = ajusted_test[feature] \n        test_mean = data.mean()\n        try:\n            error = stract_hists(feature, adjust=True)\n            ajust_factor = train_mean / test_mean\n            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n                to_exclude.append(feature)\n                print(feature, train_mean, test_mean, error)\n            else:\n                ajusted_test[feature] *= ajust_factor\n        except:\n            to_exclude.append(feature)\n            print(feature, train_mean, test_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [x for x in features if x not in to_exclude]\nreduce_train[features].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef plot_feature_importance(features, importances):\n    d = {}\n    d['features'] = features\n    d['importance'] = importances\n    important_features = pd.DataFrame(d)\n    plt.figure(figsize = (12,15))\n    important_features = important_features.groupby('features')['importance'].mean().reset_index().sort_values('importance')\n    if len(features) < 80:\n        n = len(features)-1\n    else:\n        n = 80\n    sns.barplot(important_features['importance'][-n:], important_features['features'][-n:])\n    return important_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nOriginal Data:\npoisson\nAverage partial score is: 0.5623904605653245 +/- 0.03164414073670059std\nBest n partial score is: 0.5728333816362448+/- 0.06563117983605811std\ntweedie/tweedie:\nAverage partial score is: 0.5657056330290424 +/- 0.0320165459609263std\nBest n partial score is: 0.5742538311494599+/- 0.06597108256274661std\nAverage partial score is: 0.6030431598354409 +/- 0.03639031569695544std\nBest n partial score is: 0.6138975396315688+/- 0.0753835155343371std\n\nMyData\nregression+rmse\nAverage partial score is: 0.5533183263386379\nBest n partial score is: 0.5598111624907085\npoisson + rmse:\nAverage partial score is: 0.5664034221359218\nBest n partial score is: 0.5716126280759017 \npoisson + poisson:\nAverage partial score is: 0.5627788516987987 +/- 0.031678317312866584std\nBest n partial score is: 0.5689541833964443+/- 0.06474670116987956std\ntweedie + tweedie:\nAverage partial score is: 0.5660214276251782 +/- 0.032042277869784405std\nBest n partial score is: 0.5712556169932934+/- 0.0652691109376978std\n'''\nlgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\nplot_feature_importance(features, lgb_model.model.feature_importance())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\noriginal dataset\nAverage partial score is: 0.5478419481588803 +/- 0.030029940362635992std\nBest n partial score is: 0.5576519402978031+/- 0.0622026002160719std\nmy dataset:\n\n'''\nxgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dfm_model = DFM_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {'lbg': .6, 'xgb': 0.25, 'nn': 0.15}\n#weights = {'lbg': 1, 'xgb': 0, 'nn': 0}\nfinal_pred = (lgb_model.y_pred * weights['lbg'])\n             + (xgb_model.y_pred * weights['xgb'])\n             + (nn_model.y_pred * weights['nn'])\nprint(final_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = Counter(reduce_train['accuracy_group'])\nfor k in dist:\n    dist[k] /= len(reduce_train)\nreduce_train['accuracy_group'].hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(final_pred, acum * 100)\n    \nprint('bound =', bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, final_pred)))\n\nsample_submission['accuracy_group'] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}