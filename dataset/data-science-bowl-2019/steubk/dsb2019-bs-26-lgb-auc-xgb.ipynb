{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import seed\nfrom random import randint\n# seed random number generator\nseed(44)\n\nfrom tqdm import tqdm\nimport pickle\nfrom numba import jit \nfrom scipy import stats\nimport json\nfrom collections import Counter\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import roc_auc_score\n\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gc\nimport os\nimport psutil\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef rmse (y_true, y_pred):\n    return np.sqrt ( mean_squared_error (y_true, y_pred) )\n\ndef eval_qwk_lgb_regr(y_pred, accuracy_groups):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(accuracy_groups)\n    for k in dist:\n        dist[k] /= len(accuracy_groups)\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred)))\n    \n    return y_pred\n\n\n@jit\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\ndef round_prediction (x,a,b,c):\n    x = np.where(x < a, 0, x)  \n    x = np.where((a < x) & (x <= a + b), 1, x)  \n    x = np.where((a + b < x) & (x <= a + b + c), 2, x)  \n    x = np.where((a + b + c < x) , 3, x)  \n    return x\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef encode_title(train, test, train_labels):\n    #start = time.time()\n\n    print(\"Start encoding data\")\n    # encode title\n    train['title_event_code'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code'])))\n    test['title_event_code'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code'])))\n    all_title_event_code = sorted(list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())))\n\n    train['type_world'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), train['type'], train['world'])))\n    test['type_world'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), test['type'], test['world'])))\n    all_type_world = sorted(list(set(train[\"type_world\"].unique()).union(test[\"type_world\"].unique())))\n\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n    \n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n    list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n    \n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n    \n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(\n        set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n    \n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    #print(\"End encoding data, time - \", time.time() - start)\n\n\n    event_data = {}\n    event_data[\"train_labels\"] = train_labels\n    event_data[\"win_code\"] = win_code\n    event_data[\"list_of_user_activities\"] = list_of_user_activities\n    event_data[\"list_of_event_code\"] = list_of_event_code\n    event_data[\"activities_labels\"] = activities_labels\n    event_data[\"assess_titles\"] = assess_titles\n    event_data[\"list_of_event_id\"] = list_of_event_id\n    event_data[\"all_title_event_code\"] = all_title_event_code\n    event_data[\"activities_map\"] = activities_map\n    event_data[\"all_type_world\"] = all_type_world\n\n    return train, test, event_data\n\ndef get_all_features(feature_dict, ac_data):\n    if len(ac_data['durations']) > 0:\n        feature_dict['installation_duration_mean'] = np.mean(ac_data['durations'])\n        feature_dict['installation_duration_sum'] = np.sum(ac_data['durations'])\n    else:\n        feature_dict['installation_duration_mean'] = 0\n        feature_dict['installation_duration_sum'] = 0\n\n    return feature_dict\n\ndef get_data(user_sample, event_data, test_set):\n    '''\n    The user_sample is a DataFrame from train or test where the only one\n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_assesment = {}\n\n    last_activity = 0\n\n    user_activities_count = {'Clip': 0, 'Activity': 0, 'Assessment': 0, 'Game': 0}\n\n    assess_4020_acc_dict = {'Cauldron Filler (Assessment)_4020_accuracy': 0,\n                            'Mushroom Sorter (Assessment)_4020_accuracy': 0,\n                            'Bird Measurer (Assessment)_4020_accuracy': 0,\n                            'Chest Sorter (Assessment)_4020_accuracy': 0}\n\n    game_time_dict = {'Clip_gametime': 0, 'Game_gametime': 0,\n                      'Activity_gametime': 0, 'Assessment_gametime': 0}\n\n    last_session_time_sec = 0\n    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0\n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n\n    # Newly added features\n    accumulated_game_miss = 0\n    Cauldron_Filler_4025 = 0\n    mean_game_round = 0\n    mean_game_duration = 0\n    mean_game_level = 0\n    Assessment_mean_event_count = 0\n    Game_mean_event_count = 0\n    Activity_mean_event_count = 0\n    chest_assessment_uncorrect_sum = 0\n\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    durations_game = []\n    durations_activity = []\n    last_accuracy_title = {'acc_' + title: -1 for title in event_data[\"assess_titles\"]}\n    last_game_time_title = {'lgt_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_game_time_title = {'agt_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_true_attempts_title = {'ata_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_false_attempts_title = {'afa_' + title: 0 for title in event_data[\"assess_titles\"]}\n    event_code_count: dict[str, int] = {ev: 0 for ev in event_data[\"list_of_event_code\"]}\n    event_code_proc_count = {str(ev) + \"_proc\" : 0. for ev in event_data[\"list_of_event_code\"]}\n    event_id_count: dict[str, int] = {eve: 0 for eve in event_data[\"list_of_event_id\"]}\n    title_count: dict[str, int] = {eve: 0 for eve in event_data[\"activities_labels\"].values()}\n    title_event_code_count: dict[str, int] = {t_eve: 0 for t_eve in event_data[\"all_title_event_code\"]}\n    type_world_count: dict[str, int] = {w_eve: 0 for w_eve in event_data[\"all_type_world\"]}\n    session_count = 0\n    \n    Activity_game_durations = [] \n\n    last_Game_Features = {}\n    last_game_session_correct_true = np.nan\n    last_game_session_correct_false = np.nan\n\n    last_2_game_session_correct_true = np.nan\n    last_2_game_session_correct_false = np.nan\n\n    acc_game_session_correct_true = 0\n    acc_game_session_correct_false = 0\n\n    last_Assessment_Features = {}\n\n    \n    last_Activity_Features = {}\n    \n    session_type_story = []\n    \n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        \n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = event_data[\"activities_labels\"][session_title]\n        game_session = session['game_session'].iloc[0]\n        \n        session_world = session['world'].iloc[0]\n        \n        timestamp = session['timestamp'].iloc[0]\n        \n        if session_type == \"Activity\":\n            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n            \n            last_Activity_Features = {\n                \"last_activity_event_count\":session[\"event_count\"].values[-1],\n                \"last_activity_event_count_nunique\" : session[\"event_count\"].nunique(),\n                \"last_activity_timestamp\": timestamp,\n                \"last_activity_world\": session_world, \n                \"last_activity_session_title\": session_title\n            }\n            \n        if session_type == \"Game\":\n            \n            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n\n            game_s = session[session.event_code == 2030]\n            misses_cnt = cnt_miss(game_s)\n            accumulated_game_miss += misses_cnt\n\n            try:\n                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n                mean_game_round = (mean_game_round + game_round) / 2.0\n            except:\n                pass\n\n            try:\n                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n                mean_game_duration = (mean_game_duration + game_duration) / 2.0\n            except:\n                pass\n\n            try:\n                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n                mean_game_level = (mean_game_level + game_level) / 2.0\n            except:\n                pass\n\n            \n            game_session_correct =session [ session[\"event_data\"].map(lambda x: '\"correct\"' in x ) ][\"event_data\"].map(lambda x: json.loads(x)[\"correct\"])\n            last_2_game_session_correct_true = last_game_session_correct_true\n            last_2_game_session_correct_false = last_game_session_correct_false\n            \n            last_game_session_correct_true = game_session_correct.sum()\n            last_game_session_correct_false = game_session_correct.shape[0] - last_game_session_correct_true\n            acc_game_session_correct_true += last_game_session_correct_true\n            acc_game_session_correct_false += last_game_session_correct_false\n        \n            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n            \n            last_Game_Features = {\n                \"last_game_event_count\":session[\"event_count\"].values[-1],\n                \"last_game_event_count_nunique\" : session[\"event_count\"].nunique(),\n                \"last_game_timestamp\": timestamp,\n                \"last_game_world\": session_world,\n                \"last_game_session_title\": session_title\n            }\n\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n            \n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {event_data[\"win_code\"][session_title]}')\n            \n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            # copy a dict to use as feature template, it's initialized with some itens:\n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(title_count.copy())\n            features.update(game_time_dict.copy())\n            features.update(event_id_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(assess_4020_acc_dict.copy())\n            features.update(type_world_count.copy())\n            features.update(last_game_time_title.copy())\n            features.update(ac_game_time_title.copy())\n            features.update(ac_true_attempts_title.copy())\n            features.update(ac_false_attempts_title.copy())\n\n            features.update(event_code_proc_count.copy())\n            features['installation_session_count'] = session_count\n            features['game_session'] =  game_session\n            features['timestamp'] =  timestamp\n            features['accumulated_game_miss'] = accumulated_game_miss\n            features['mean_game_round'] = mean_game_round\n            features['mean_game_duration'] = mean_game_duration\n            features['mean_game_level'] = mean_game_level\n            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n            features['Game_mean_event_count'] = Game_mean_event_count\n            features['Activity_mean_event_count'] = Activity_mean_event_count\n            features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n\n            variety_features = [('var_event_code', event_code_count),\n                                ('var_event_id', event_id_count),\n                                ('var_title', title_count),\n                                ('var_title_event_code', title_event_code_count),\n                                ('var_type_world', type_world_count)]\n\n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n\n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n\n            # ----------------------------------------------\n            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n\n            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n            # ----------------------------------------------\n            \n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = np.nan\n                features['duration_std'] = np.nan\n                features['last_duration'] = np.nan\n                features['last-2_duration'] = np.nan\n                features['duration_max'] = np.nan\n                #features['duration_min'] = np.nan\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                features['last_duration'] = durations[-1]\n                if len(durations)>1:\n                    features['last-2_duration'] = durations[-2]\n                else:\n                    features['last-2_duration'] = np.nan\n                features['duration_max'] = np.max(durations)\n                #features['duration_min'] = np.min(durations)\n            \n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n\n            if durations_game == []:\n                features['duration_game_mean'] = np.nan\n                features['duration_game_std'] = np.nan\n                features['game_last_duration'] = np.nan\n                features['game_last-2_duration'] = np.nan\n                features['game_max_duration'] = np.nan\n                #features['game_min_duration'] = np.nan\n            else:\n                features['duration_game_mean'] = np.mean(durations_game)\n                features['duration_game_std'] = np.std(durations_game)\n                features['game_last_duration'] = durations_game[-1]\n                if len(durations_game)>1:\n                    features['game_last-2_duration'] = durations_game[-2]\n                else:\n                    features['game_last-2_duration'] = np.nan\n                features['game_max_duration'] = np.max(durations_game)\n                #features['game_min_duration'] = np.min(durations_game)\n\n            if last_Game_Features  == {}:\n                features[\"last_game_event_count\"] = np.nan\n                features[\"last_game_event_count_nunique\"] = np.nan\n                features[\"last_game_timestamp\"] = np.nan\n                features[\"last_game_world_is_the_same\"] = np.nan\n                features[\"last_game_session_title\"] = np.nan\n            else:\n                features[\"last_game_event_count\"] =  last_Game_Features[\"last_game_event_count\"]\n                features[\"last_game_event_count_nunique\"] =  last_Game_Features[\"last_game_event_count_nunique\"]\n                features[\"last_game_timestamp\"] = last_Game_Features[\"last_game_timestamp\"]\n                features[\"last_game_world_is_the_same\"] = int(last_Game_Features[\"last_game_world\"] == session_world)\n                features[\"last_game_session_title\"] = last_Game_Features[\"last_game_session_title\"]\n\n            if last_Assessment_Features  == {}:\n                features[\"last_assessment_event_count\"] = np.nan\n                features[\"last_assessment_event_count_nunique\"] = np.nan\n                features[\"last_assessment_timestamp\"] = np.nan\n                features[\"last_assessment_world_is_the_same\"] = np.nan\n                features[\"last_assessment_title_is_the_same\"] = np.nan\n                features[\"last_assessment_accuracy_group\"] = np.nan\n                features[\"last_assessment_session_title\"] = np.nan\n                \n            else:\n                features[\"last_assessment_event_count\"] =  last_Assessment_Features[\"last_assessment_event_count\"]\n                features[\"last_assessment_event_count_nunique\"] =  last_Assessment_Features[\"last_assessment_event_count_nunique\"]\n                features[\"last_assessment_timestamp\"] = last_Assessment_Features[\"last_assessment_timestamp\"]\n                features[\"last_assessment_world_is_the_same\"] = int(last_Assessment_Features[\"last_assessment_world\"] == session_world)\n                features[\"last_assessment_title_is_the_same\"] = int(last_Assessment_Features[\"last_assessment_title\"] == session_title)\n                features[\"last_assessment_accuracy_group\"] = last_Assessment_Features[\"last_assessment_accuracy_group\"]\n                features[\"last_assessment_session_title\"] = last_Assessment_Features[\"last_assessment_title\"]\n\n                \n            if last_Activity_Features  == {}:\n                features[\"last_activity_event_count\"] = np.nan\n                features[\"last_activity_event_count_nunique\"] = np.nan\n                features[\"last_activity_timestamp\"] = np.nan\n                features[\"last_activity_world_is_the_same\"] = np.nan\n                features[\"last_activity_session_title\"] = np.nan\n            else:\n                features[\"last_activity_event_count\"] =  last_Activity_Features[\"last_activity_event_count\"]\n                features[\"last_activity_event_count_nunique\"] =  last_Activity_Features[\"last_activity_event_count_nunique\"]\n                features[\"last_activity_timestamp\"] = last_Activity_Features[\"last_activity_timestamp\"]\n                features[\"last_activity_world_is_the_same\"] = int(last_Activity_Features[\"last_activity_world\"] == session_world)\n                features[\"last_activity_session_title\"] = last_Activity_Features[\"last_activity_session_title\"]\n                \n                \n            if durations_activity == []:\n                #features['duration_activity_mean'] = np.nan\n                #features['duration_activity_std'] = np.nan\n                #features['activity_last_duration'] = np.nan\n                #features['activity_last-2_duration'] = np.nan\n                #features['activity_max_duration'] = np.nan\n                features['activity_min_duration'] = np.nan\n            else:\n                #features['duration_activity_mean'] = np.mean(durations_activity)\n                #features['duration_activity_std'] = np.std(durations_activity)\n                #features['activity_last_duration'] = durations_activity[-1]\n                #if len(durations_activity)>1:\n                #    features['activity_last-2_duration'] = durations_activity[-2]\n                #else:\n                #    features['activity_last-2_duration'] = np.nan\n                #features['activity_max_duration'] = np.max(durations_activity)\n                features['activity_min_duration'] = np.min(durations_activity)\n\n            # the accuracy is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy / counter if counter > 0 else 0\n            # --------------------------\n            features['Cauldron_Filler_4025'] = Cauldron_Filler_4025 / counter if counter > 0 else 0\n\n            Assess_4025 = session[(session.event_code == 4025) & (session.title == 'Cauldron Filler (Assessment)')]\n            true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n            false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n\n            cau_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (true_attempts_ + false_attempts_) != 0 else 0\n            Cauldron_Filler_4025 += cau_assess_accuracy_\n\n            chest_assessment_uncorrect_sum += len(session[session.event_id == \"df4fe8b6\"])\n\n            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n            # ----------------------------\n            accuracy = true_attempts / (true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group / counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n\n            features[\"acc_current_assessment\"] = features[\"acc_\"+session_title_text]\n            features[\"lgt_current_assessment\"] = features[\"lgt_\"+session_title_text]\n            features[\"agt_current_assessment\"] = features[\"agt_\"+session_title_text]\n            features[\"ata_current_assessment\"] = features[\"ata_\"+session_title_text]\n            features[\"afa_current_assessment\"] = features[\"afa_\"+session_title_text]\n            \n            features[\"current_4020_accuracy\"] = features.get(session_title_text + \"_4020_accuracy\",-1)\n  \n            #for _event in ['2000','2010','2020','2030','3010','3020','3021','3110','3120',\n            #              '3121','4020','4030','4035','4040','4070','4080','4090','4100']:\n            #    features[\"current_assessment_\" + _event] = features.get(session_title_text + \"_\" + _event,0)\n\n            \n            features[\"last_game_session_correct_true\"] = last_game_session_correct_true\n            features[\"last_game_session_correct_false\"] = last_game_session_correct_false\n            features[\"last_2_game_session_correct_true\"] = last_2_game_session_correct_true\n            features[\"last_2_game_session_correct_false\"] = last_2_game_session_correct_false\n            \n            features[\"acc_game_session_correct_true\"] = acc_game_session_correct_true\n            features[\"acc_game_session_correct_false\"] = acc_game_session_correct_false\n            \n            session_type_story_length = len(session_type_story)\n            features[\"session_type_story_length\"] = session_type_story_length\n            features[\"session_type_story_count_clip\"] =session_type_story.count('Clip')\n            features[\"session_type_story_count_game\"] =session_type_story.count('Game')\n            features[\"session_type_story_count_activity\"] =session_type_story.count('Activity')\n            features[\"session_type_story_count_assessment\"] =session_type_story.count('Assessment')\n            \n            session_type_story = []\n            \n            last_Assessment_Features = {\n                \"last_assessment_event_count\":session[\"event_count\"].values[-1],\n                \"last_assessment_event_count_nunique\" : session[\"event_count\"].nunique(),\n                \"last_assessment_timestamp\": timestamp,\n                \"last_assessment_world\": session_world,\n                \"last_assessment_title\": session_title,\n                \"last_assessment_accuracy_group\": features['accuracy_group'] \n            }\n\n\n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                last_assesment = features.copy()\n\n            if true_attempts + false_attempts > 0:\n                all_assessments.append(features)\n\n                \n            counter += 1\n\n\n        session_count += 1\n\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(session[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if col == 'title':\n                    x = event_data[\"activities_labels\"][k]\n                counter[x] += num_of_session_count[k]\n            return counter\n\n        def update_proc(count: dict):\n            res = {}\n            for k, val in count.items():\n                res[str(k) + \"_proc\"] = (float(val) * 100.0) / accumulated_actions\n            return res\n\n        event_code_count = update_counters(event_code_count, \"event_code\")\n\n\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        type_world_count = update_counters(type_world_count, 'type_world')\n\n        assess_4020_acc_dict = get_4020_acc(session, assess_4020_acc_dict, event_data)\n        game_time_dict[session_type + '_gametime'] = (game_time_dict[session_type + '_gametime'] + (\n                    session['game_time'].iloc[-1] / 1000.0)) / 2.0\n\n        \n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        event_code_proc_count = update_proc(event_code_count)\n\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n       \n        session_type_story.append ( session_type ) \n\n    # if it't the test_set, only the last assessment must be predicted, the previous goes to the dataset\n    if test_set:\n        return last_assesment, all_assessments\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\n\ndef cnt_miss(df):\n    cnt = 0\n    for e in range(len(df)):\n        x = df['event_data'].iloc[e]\n        y = json.loads(x)['misses']\n        cnt += y\n    return cnt\n\ndef get_4020_acc(df, counter_dict, event_data):\n    for e in ['Cauldron Filler (Assessment)', 'Bird Measurer (Assessment)',\n              'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)']:\n        Assess_4020 = df[(df.event_code == 4020) & (df.title == event_data[\"activities_map\"][e])]\n        true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n        false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n\n        measure_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n                                                                                                      true_attempts_ + false_attempts_) != 0 else 0\n        counter_dict[e + \"_4020_accuracy\"] += (counter_dict[e + \"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n\n    return counter_dict\n\ndef get_users_data(users_list, return_dict,  event_data, test_set):\n    if test_set:\n        for user in users_list:\n            return_dict.append(get_data(user, event_data, test_set))\n    else:\n        answer = []\n        for user in users_list:\n            answer += get_data(user, event_data, test_set)\n        return_dict += answer\n\n        \ndef get_train_and_test_single_proc(train, test, event_data, load_train = False):\n\n    if load_train :\n        reduce_train = pd.read_pickle (PROCESSED_TRAIN_PATH)\n    else:    \n        compiled_train = []\n        for ins_id, user_sample in tqdm(train.groupby('installation_id', sort=False), total=17000):\n            compiled_train += get_data(user_sample, event_data, False)\n        reduce_train = pd.DataFrame(compiled_train)\n    \n    compiled_test = []\n    compiled_test_to_train = []\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n        test_data = get_data(user_sample, event_data, True)\n        compiled_test.append(test_data[0])\n        compiled_test_to_train += test_data[1]\n\n\n    reduce_test = pd.DataFrame(compiled_test)\n    reduce_test_to_train = pd.DataFrame(compiled_test_to_train)\n\n    return reduce_train, reduce_test, reduce_test_to_train        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_correlated_features(reduce_train,features ):\n    counter = 0\n    to_remove = []\n    for feat_a in features:\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n                if c > 0.9999:\n                    counter += 1\n                    to_remove.append(feat_b)\n                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n    return to_remove\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process (df):\n    df[\"hour\"]=df[\"timestamp\"].dt.hour\n    df[\"dayofyear\"]=df[\"timestamp\"].dt.dayofyear\n    df[\"dayofweek\"]=df[\"timestamp\"].dt.dayofweek\n    df['sin_hour'] = np.sin(2*np.pi*df[\"hour\"]/24)\n    df['cos_hour'] = np.cos(2*np.pi*df[\"hour\"]/24)\n    df = df.drop ([\"hour\"], axis = 1)  \n    \n    df[\"timestamp\"]=df[\"timestamp\"].astype(int)\n    df[\"last_game_timestamp\"]=(df[\"timestamp\"] - df[\"last_game_timestamp\"].astype(int)) // 1e6\n    df[\"last_assessment_timestamp\"]=(df[\"timestamp\"] - df[\"last_assessment_timestamp\"].astype(int)) // 1e6\n    df[\"last_activity_timestamp\"]=(df[\"timestamp\"] - df[\"last_activity_timestamp\"].astype(int)) // 1e6\n    \n    df[\"last_game_session_correct_accuracy\"] = df[\"last_game_session_correct_true\"]/(df[\"last_game_session_correct_true\"]+df[\"last_game_session_correct_false\"])\n    df[\"last_2_game_session_correct_accuracy\"] = df[\"last_2_game_session_correct_true\"]/(df[\"last_2_game_session_correct_true\"]+df[\"last_2_game_session_correct_false\"])\n    df[\"last_game_session_correct_accuracy*last_2_game_session_correct_accuracy\"] = df[\"last_game_session_correct_accuracy\"]*df[\"last_2_game_session_correct_accuracy\"]\n    \n    df[\"acc_game_session_correct_accuracy\"] = df[\"acc_game_session_correct_true\"]/(df[\"acc_game_session_correct_true\"]+df[\"acc_game_session_correct_false\"])\n    \n    \n    \n    \n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def truncate_generator (X, y, groups):\n    X = X.copy()\n    X[\"__y__\"] = y\n    X[\"__groups__\"] = groups\n    state = 44\n    while True:\n        X = X.sample (frac=1.0, random_state=state)\n        state +=1\n        _X = X.drop_duplicates([\"__groups__\"], keep=\"first\")\n        yield _X.drop([\"__y__\",\"__groups__\"],axis=1), _X[\"__y__\"].values, _X[\"__groups__\"].values  \n\n\ndef trunc_rmse_qwk_score(y,pred,groups, nsamples=5000):\n    rmse_scores = np.zeros ( (nsamples, ))\n    oof_cohen_scores = np.zeros ( (nsamples, ))\n    \n    X=pd.DataFrame()\n    X[\"pred\"] = pred\n    truncate_gen = truncate_generator (X, y, groups)\n    for i in range (nsamples):\n        X, y, groups = truncate_gen.__next__()\n        pred = X[\"pred\"]\n        rmse_scores [i]= rmse(y, pred)\n        oof_cohen_scores [i]= cohen_kappa_score(y, eval_qwk_lgb_regr(pred, y), weights = 'quadratic')    \n    \n    return rmse_scores, oof_cohen_scores\n\ndef trunc_qwk_score(y,pred,groups, nsamples=5000):\n    oof_cohen_scores = np.zeros ( (nsamples, ))\n    \n    X=pd.DataFrame()\n    X[\"pred\"] = pred\n    truncate_gen = truncate_generator (X, y, groups)\n    for i in range (nsamples):\n        X, y, groups = truncate_gen.__next__()\n        pred = X[\"pred\"]\n        oof_cohen_scores [i]= qwk3(pred, y)    \n    \n    return oof_cohen_scores\n\ndef trunc_auc_score(y,pred,groups, nsamples=5000):\n    scores = np.zeros ( (nsamples, ))\n    \n    X=pd.DataFrame()\n    X[\"pred\"] = pred\n    truncate_gen = truncate_generator (X, y, groups)\n    for i in range (nsamples):\n        X, y, groups = truncate_gen.__next__()\n        pred = X[\"pred\"]\n        scores [i]= roc_auc_score(y,pred)    \n    \n    return scores\n\n\ndef oof_lgb (X, ids, y, groups, num_boost_round, early_stopping_rounds, params, categoricals, splits, verbose_eval):\n\n    columns = [c for c in X.columns]\n    \n    oof = pd.DataFrame({\"id\":ids})\n    y_oof = np.zeros((X.shape[0], ))\n    \n    models= []\n    \n\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = columns \n\n    \n    \n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        #group_train = groups.iloc [train_index] \n        #train_truncate_gen = truncate_generator ( X_train, y_train, group_train)\n\n        \n        group_valid = groups.iloc [valid_index] \n        \n        truncate_gen = truncate_generator ( X_valid, y_valid, group_valid) \n        \n        X_valid_trunc, y_valid_trunc, _ = truncate_gen.__next__()\n        \n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dvalid = lgb.Dataset(X_valid_trunc, label=y_valid_trunc)\n\n        model = lgb.train(params, dtrain, num_boost_round, \n                        valid_sets = [dtrain, dvalid],\n                        categorical_feature = categoricals,\n                        verbose_eval=verbose_eval, early_stopping_rounds=early_stopping_rounds)\n\n        y_pred = model.predict(X_valid)\n        \n        \n        #if verbose_eval > 0:\n        #   rmse_scores, oof_cohen_scores = trunc_rmse_qwk_score (y_valid.values, y_pred, group_valid.values,nsamples=5000)\n        #    print (f'fold: {fold_n}, rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}' )\n        \n        models.append(model)\n \n        feature_importances[f'fold_{fold_n + 1}'] = model.feature_importance()\n\n        y_oof[valid_index] = y_pred\n\n        del X_train, X_valid, y_train, y_valid\n        gc.collect()\n\n    #rmse_scores, oof_cohen_scores = trunc_rmse_qwk_score (y.values, y_oof, groups.values, nsamples=5000)\n    #if verbose_eval > -1:\n    #    print (f'oof rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n        \n    \n    feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(len(splits))]].mean(axis=1)\n    feature_importances = feature_importances.sort_values (by=\"average\", ascending = False)\n    feature_importances.to_csv(\"feature_importance.csv\", index=False)\n    \n    return models, y_oof\n\ndef run_lgb (df_train, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds, verbose_eval  ):\n\n    if verbose_eval > 0:\n        print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n    \n    X = df_train[model_feats]\n    y = df_train[\"accuracy_group\"]\n    ids = df_train[\"sample\"]\n    groups = df_train[\"installation_id\"]\n\n    folds = GroupKFold(n_splits=nfolds) \n\n    splits = []\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n        splits.append((train_index, valid_index)) \n\n    lgb_model, y_oof = oof_lgb (X, ids, y, groups,  num_boost_round, early_stopping_rounds, params, categoricals=categorical_features, splits=splits, verbose_eval = verbose_eval)\n    \n    return lgb_model, y_oof  #, rmse_scores, oof_cohen_scores\n\ndef run_lgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n    y_oof = np.zeros ( (df_train.shape[0],) )\n    for i in range(nmodels):\n        lgb_i_models, y_i_oof  = run_lgb(df_train, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n        y_oof += y_i_oof \n        for n,model in enumerate(lgb_i_models):\n            model.save_model(f'lgb_{i}_model_fold_{n}.txt')\n        params['seed'] +=1\n        params['feature_fraction_seed'] +=1\n        params['bagging_seed'] +=1\n        params['drop_seed'] +=1\n        params['data_random_seed'] +=1            \n    \n    y_oof = y_oof / nmodels\n    \n    rmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n    print (f'lgb {nmodels} models rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n    \n    return y_oof, rmse_scores,  oof_cohen_scores\n\ndef run_lgb_auc (df_train, y, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds, verbose_eval  ):\n\n    if verbose_eval > 0:\n        print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n    \n    X = df_train[model_feats]\n\n    ids = df_train[\"sample\"]\n    groups = df_train[\"installation_id\"]\n\n    folds = GroupKFold(n_splits=nfolds) \n\n    splits = []\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n        splits.append((train_index, valid_index)) \n\n    lgb_model, y_oof = oof_lgb (X, ids, y, groups,  num_boost_round, early_stopping_rounds, params, categoricals=categorical_features, splits=splits, verbose_eval = verbose_eval)\n    \n    return lgb_model, y_oof  #, rmse_scores, oof_cohen_scores\n\ndef run_lgb_auc_ensemble (df_train, y, name,  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n    y_oof = np.zeros ( (df_train.shape[0],) )\n    for i in range(nmodels):\n        lgb_i_models, y_i_oof  = run_lgb_auc(df_train, y, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n        y_oof += y_i_oof \n        for n,model in enumerate(lgb_i_models):\n            model.save_model(f'lgb_{name}_{i}_model_fold_{n}.txt')\n        params['seed'] +=1\n        params['feature_fraction_seed'] +=1\n        params['bagging_seed'] +=1\n        params['drop_seed'] +=1\n        params['data_random_seed'] +=1            \n    \n    y_oof = y_oof / nmodels\n    \n    auc_scores  = trunc_auc_score ( y.values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n    print (f'lgb {nmodels} models auc: {np.median(auc_scores)} std:{auc_scores.std()}')\n    \n    return y_oof, auc_scores\n\n\ndef oof_xgb (X, ids, y, groups,num_boost_round, early_stopping_rounds, params, splits, verbose_eval):\n    columns = [c for c in X.columns]\n    \n    oof = pd.DataFrame({\"id\":ids})\n    y_oof = np.zeros((X.shape[0], ))\n    \n    models= []\n    score = 0\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        group_valid = groups.iloc [valid_index] \n        \n        truncate_gen = truncate_generator ( X_valid, y_valid, group_valid) \n        \n        X_valid_trunc, y_valid_trunc, _ = truncate_gen.__next__()\n      \n    \n        dtrain = xgb.DMatrix(X_train,label=y_train)\n        dvalid = xgb.DMatrix(X_valid_trunc,label=y_valid_trunc)\n       \n        model = xgb.train(params, dtrain=dtrain, num_boost_round=num_boost_round, evals =  [(dtrain, 'train'),(dvalid, 'valid')],\n                          early_stopping_rounds=early_stopping_rounds, maximize=False, verbose_eval=verbose_eval)\n\n\n        dvalid = xgb.DMatrix(X_valid,label=y_valid)\n        y_pred = model.predict(dvalid)\n        \n           \n            \n        models.append(model)\n\n        y_oof[valid_index] = y_pred\n\n        del X_train, X_valid, y_train, y_valid\n        gc.collect()\n\n    #rmse_scores, oof_cohen_scores = trunc_rmse_qwk_score (y, y_oof, groups)\n    #avg_score = score/len(splits)    \n    #if verbose_eval > 0:\n    #    print (f'oof rmse: {rmse_scores.median()}, mean rmse: {avg_score}, cohen kappa score : {oof_cohen_scores.median()}')\n    \n    return models, y_oof\n\n\n\ndef run_xgb (df_train, num_boost_round, early_stopping_rounds,  params, model_feats,categorical_features, nfolds  , verbose_eval   ):\n\n    if verbose_eval > 0:\n        print(f'nfolds:{nfolds}, features:{len(model_feats)}, categorical:{len(categorical_features)}')\n    \n    X = df_train[model_feats].copy()\n    \n    X = pd.get_dummies(X, dummy_na=True, columns=categorical_features)\n    \n    y = df_train[\"accuracy_group\"]\n    ids = df_train[\"sample\"]\n    groups = df_train[\"installation_id\"]\n\n    folds = GroupKFold(n_splits=nfolds) \n\n    splits = []\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X,groups=groups)):\n        splits.append((train_index, valid_index)) \n\n    xgb_model, y_oof = oof_xgb (X, ids, y, groups, num_boost_round, early_stopping_rounds, params,  splits=splits, verbose_eval = verbose_eval)\n    \n    return xgb_model, y_oof\n\n\ndef run_xgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  params, nmodel,nfolds, verbose_eval ):\n    y_oof = np.zeros ( (df_train.shape[0],) )\n    for i in range(nmodels):\n        xgb_i_models, y_i_oof  = run_xgb(df_train, num_boost_round, early_stopping_rounds, params, model_feats, categorical_features,  nfolds, verbose_eval )\n        y_oof += y_i_oof \n        for n,model in enumerate(xgb_i_models):\n            model.save_model(f'xgb_{i}_model_fold_{n}.txt')\n        params['seed'] +=1       \n    \n    y_oof = y_oof / nmodels\n    \n    rmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y_oof, df_train[\"installation_id\"].values, nsamples=5000)\n    print (f'xgb {nmodels} models rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n    \n    return y_oof, rmse_scores,  oof_cohen_scores\n\n\ndef predict_lgb (nmodel, nfolds):\n    y_blend = np.zeros((test.shape[0], ))     \n    \n    for i in range(nmodels):\n        y_test = np.zeros((test.shape[0], ))     \n\n        for n in range(nfolds):\n            model_file = f'./lgb_{i}_model_fold_{n}.txt'\n            model = lgb.Booster(model_file=model_file)\n            pred = model.predict( test[model_feats], num_iteration=model.best_iteration )\n            y_test += pred\n        \n        y_blend += y_test / nfolds\n    \n    y_blend = y_blend / nmodels\n    \n    return y_blend\n\ndef predict_auc_lgb (name, nmodel, nfolds):\n    y_blend = np.zeros((test.shape[0], ))     \n    \n    for i in range(nmodels):\n        y_test = np.zeros((test.shape[0], ))     \n\n        for n in range(nfolds):\n            model_file = f'./lgb_{name}_{i}_model_fold_{n}.txt'\n            model = lgb.Booster(model_file=model_file)\n            pred = model.predict( test[model_feats], num_iteration=model.best_iteration )\n            y_test += pred\n        \n        y_blend += y_test / nfolds\n    \n    y_blend = y_blend / nmodels\n    \n    return y_blend\n\n\n\ndef predict_xgb (nmodel, nfolds):\n    y_blend = np.zeros((test.shape[0], ))     \n\n    test_xgb = test[model_feats].copy()\n    test_xgb = pd.get_dummies(test_xgb,  dummy_na=True, columns=categorical_features)\n    \n    for i in range(nmodels):\n        y_test = np.zeros((test.shape[0], ))     \n\n        for n in range(nfolds):\n            model_file = f'./xgb_{i}_model_fold_{n}.txt'\n            model = xgb.Booster() #init model\n            model.load_model(model_file) # load data\n            dtest =  xgb.DMatrix (test_xgb)\n            pred = model.predict(dtest)           \n            y_test += pred\n        \n        y_blend += y_test / nfolds\n    \n    y_blend = y_blend / nmodels\n    \n    return y_blend\n\ndef predict(sample_submission, y_pred):\n    sample_submission['accuracy_group'] = y_pred\n    sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n    sample_submission.to_csv('submission.csv', index = False)\n    print(sample_submission['accuracy_group'].value_counts(normalize = True))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_bound ( y_oof, groups ):\n\n    dist = Counter(groups)\n    for k in dist:\n        dist[k] /= len(y_oof)\n\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_oof, acum * 100)\n\n    return bound[0], bound[1] - bound[0],  bound[2] - bound[1]  \n\ndef qwk3_optimizer ( y, y_oof, init_points = 20, n_iter = 50  ):\n    pbounds = {'a': (0.0, 1.5), 'b': (0.0, 1.5), 'c': (0.0, 1.5)}\n\n    def qwk3_opt ( x, y, a,b,c ):\n\n        x = round_prediction ( x, a,b,c )\n\n        return qwk3 ( x, y, max_rat=3 )\n\n\n    def q (a,b,c):\n        return qwk3_opt  ( y_oof, y, a,b,c )\n\n    optimizer = BayesianOptimization(\n        f=q,\n        pbounds=pbounds,\n        random_state=44,\n    )\n\n\n    optimizer.maximize(\n        init_points=init_points,\n        n_iter=n_iter,\n    )\n\n    a = optimizer.max[\"params\"][\"a\"]\n    b = optimizer.max[\"params\"][\"b\"]\n    c = optimizer.max[\"params\"][\"c\"]\n    t = optimizer.max[\"target\"]\n    print ( f'qwk3:{t}, a:{a}, b:{b}, c:{c}' )\n    \n    return a,b,c ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\nNMODELS = 5\n\nevent_data = {}\n\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, event_data_update = encode_title(train, test, train_labels)\nevent_data.update(event_data_update)\n\nreduce_train, reduce_test, reduce_train_from_test = get_train_and_test_single_proc(train, test, event_data, load_train=False)\n\nreduce_train = post_process (reduce_train)\nreduce_test = post_process (reduce_test)\nreduce_train_from_test = post_process (reduce_train_from_test)\nreduce_train.shape,reduce_test.shape,reduce_train_from_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete train and test to release memory\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train = reduce_train.append (reduce_train_from_test, sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = reduce_test\ndf_train = reduce_train\n\ndf_train[\"sample\"] = df_train[\"installation_id\"] + \"_\" + df_train[\"game_session\"]\ntest[\"sample\"]=test[\"installation_id\"]\n\n\nmodel_feats = [\n'last_game_session_title', 'session_title', '4070_proc', 'acc_game_session_correct_accuracy', 'last_game_session_correct_accuracy',\n'4020_proc', 'activity_min_duration', '2030_proc', 'last_game_timestamp', 'acc_current_assessment', '3021_proc', 'ata_current_assessment',\n'2000_proc', 'last_2_game_session_correct_accuracy', '2010_proc', 'last_assessment_timestamp', 'last_game_event_count', 'Activity_gametime',\n'4025_proc', 'last_activity_event_count', 'accumulated_accuracy', 'duration_mean', 'last_duration', '4090_proc', 'Clip', 'last_assessment_event_count',\n'game_last_duration', 'game_last-2_duration', 'accumulated_accuracy_group', '3020_proc', '4040_proc', '4035_proc', 'afa_current_assessment',\n'mean_game_duration', 'Activity_mean_event_count', '4030_proc', '4010_proc', '4100_proc', 'duration_std', 'duration_game_mean',\n'last_assessment_session_title', '2035_proc', 'sin_hour', '3120_proc', '3110_proc', 'lgt_Cauldron Filler (Assessment)', '2025_proc',\n'2080_proc', '3010_proc', '4021_proc', 'session_type_story_count_clip', 'agt_Cart Balancer (Assessment)', '4095_proc', 'Game_mean_event_count',\n'2075_proc', 'game_max_duration', '2060_proc', '7372e1a5', 'duration_game_std', '56817e2b', 2000, 'Assessment_gametime', 'mean_game_round',\n'15a43e5b', '2083_proc', 'var_title', '3ee399c3', 'cos_hour', 'last_2_game_session_correct_true', 'duration_max', 4035, '4031_proc', '5000_proc',\n'4045_proc', 'agt_Cauldron Filler (Assessment)', 'dayofweek', 'bbfe0445', '4220_proc', 'b120f2ac', 'e694a35b', 'lgt_Chest Sorter (Assessment)',\n'587b5989', '84538528', 'last_game_session_correct_true', 'current_4020_accuracy', '6bf9e3e1', 'Sandcastle Builder (Activity)', '3afde5dd',\n'last_assessment_event_count_nunique', 'last_activity_event_count_nunique', '2081_proc', 3010, 3021, 3020, 'Mushroom Sorter (Assessment)_4020_accuracy',\n'51102b85', '1bb5fbdb', '4110_proc', 'accumulated_uncorrect_attempts', 'Bird Measurer (Assessment)_4020_accuracy', '3bf1cf26', 'session_type_story_count_game',\n'agt_Chest Sorter (Assessment)', 'agt_current_assessment', '562cec5f', '499edb7c', 'last_assessment_accuracy_group', 'ca11f653', '2040_proc', 4100, 4020,\n'0db6d71d', '37ee8496', 'acc_Bird Measurer (Assessment)', 'Game_CRYSTALCAVES', 'acc_game_session_correct_true', 'lgt_Bird Measurer (Assessment)',\n'ata_Chest Sorter (Assessment)', '907a054b', '3babcb9b', 'acc_Mushroom Sorter (Assessment)', 'acc_Chest Sorter (Assessment)', 'Bottle Filler (Activity)_4020',\n'Game_TREETOPCITY', 'session_type_story_count_activity', 'last_activity_world_is_the_same', 'last_assessment_world_is_the_same', 'last_assessment_title_is_the_same',\n]\n\ncategorical_features =  [f for f in model_feats if f in ['session_title',\n                                                         'last_game_session_title',\n                                                         'last_assessment_session_title',\n                                                         'last_activity_session_title',                                                         \n                                                         'last_assessment_accuracy_group']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnmodels = NMODELS\nnfolds = NFOLDS\nnum_boost_round = 1600 \nearly_stopping_rounds = None \nverbose_eval = 2000\nlgb_params = {\n          'num_leaves': 19, \n          'min_data_in_leaf': 160,\n          'min_child_weight': 0.03,\n          'bagging_fraction' : 0.7,\n          'feature_fraction' : 0.8,\n          'learning_rate' : 0.01,\n          'max_depth': -1,\n          'reg_alpha': 0.02,\n          'reg_lambda': 0.12,\n          'objective': 'regression',\n          'seed': 1337,\n          'feature_fraction_seed': 1337,\n          'bagging_seed': 1337,\n          'drop_seed': 1337,\n          'data_random_seed': 1337,\n          'boosting_type': 'gbdt',\n          'verbose': 100,\n          'boost_from_average': False,\n          'metric':'rmse'\n}        \n\ny_oof_lgb, rmse_scores,  oof_cohen_scores = run_lgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )\n\nfeature_importances = pd.read_csv(\"./feature_importance.csv\")\n\nfig, ax = plt.subplots(figsize=(16, 12))\nplt.subplot(1, 2, 1)\nsns.barplot(data=feature_importances[:50], x='average', y='feature', orient='h')\nplt.title('Feature importances (LGB)')\n\nplt.subplot(1, 2, 2)\nplt.hist(df_train[\"accuracy_group\"].values.reshape(-1, 1) - y_oof_lgb.reshape(-1, 1))\nplt.title('Distribution of errors (LGB)')\nplt.show()    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.title('RMSE LGB')\nplt.hist(rmse_scores, bins=100)\n\nplt.subplot(1, 2, 2)\nplt.title('QWK LGB')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'LGB {nmodels} rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,c = calc_bound (y_oof_lgb, df_train[\"accuracy_group\"])\n\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_lgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title('LGB QWK (percentile opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y_oof_lgb)\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_lgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title('LGB QWK (bayesian opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nnmodels = NMODELS\nnfolds = NFOLDS\nnum_boost_round = 1600 \nearly_stopping_rounds = None \nverbose_eval = 2000\nlgb_params = {\n          'num_leaves': 19, \n          'min_data_in_leaf': 160,\n          'min_child_weight': 0.03,\n          'bagging_fraction' : 0.7,\n          'feature_fraction' : 0.8,\n          'learning_rate' : 0.01,\n          'max_depth': -1,\n          'reg_alpha': 0.02,\n          'reg_lambda': 0.12,\n          'objective': 'binary',\n          'seed': 1337,\n          'feature_fraction_seed': 1337,\n          'bagging_seed': 1337,\n          'drop_seed': 1337,\n          'data_random_seed': 1337,\n          'boosting_type': 'gbdt',\n          'verbose': 100,\n          'boost_from_average': False,\n          'metric':'auc'\n}        \n\ncategorical_features =  [f for f in model_feats if f in ['session_title',\n                                                         'last_game_session_title',\n                                                         'last_assessment_session_title',\n                                                         'last_activity_session_title',                                                         \n                                                         'last_assessment_accuracy_group']]\n    \ny_lgb_solved_oof, auc_score = run_lgb_auc_ensemble (df_train, df_train[\"accuracy_group\"].map(lambda x: 0 if x ==0 else 1) , \"solved\",  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )\ny_lgb_first_oof, auc_score = run_lgb_auc_ensemble (df_train, df_train[\"accuracy_group\"].map(lambda x: 1 if x ==3 else 0) , \"first\",  model_feats, categorical_features, num_boost_round, early_stopping_rounds,  lgb_params, nmodels, nfolds, verbose_eval )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nmodels = NMODELS\nnfolds = NFOLDS\nnum_boost_round = 600 \nearly_stopping_rounds = None \nverbose_eval = 1000\n\nxgb_params = {\n            'objective':'reg:squarederror',\n            'eval_metric':'rmse',\n            'seed': 1337,\n            'colsample_bytree': 0.8,                 \n            'learning_rate': 0.01,\n            'max_depth': 9,\n            'subsample': 0.7,\n            'min_child_weight':3,\n            'gamma':0.25,\n            }    \n\ncategorical_features =  [f for f in model_feats if f in ['session_title',\n                                                         'last_game_session_title',\n                                                         'last_assessment_session_title',\n                                                         'last_activity_session_title',                                                         \n                                                         'last_assessment_accuracy_group']]\n\ny_oof_xgb, rmse_scores,  oof_cohen_scores = run_xgb_ensemble (df_train, model_feats, categorical_features, num_boost_round, early_stopping_rounds,  xgb_params, nmodels, nfolds, verbose_eval )\n\nplt.subplot(1, 1, 1)\nplt.hist(df_train[\"accuracy_group\"].values.reshape(-1, 1) - y_oof_xgb.reshape(-1, 1))\nplt.title('Distribution of errors (XGB)')\nplt.show()        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.title('RMSE XGB')\nplt.hist(rmse_scores, bins=100)\n\nplt.subplot(1, 2, 2)\nplt.title('QWK XGB')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'XGB {nmodels} rmse: {np.median(rmse_scores)} std:{rmse_scores.std()} , oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,c = calc_bound (y_oof_xgb, df_train[\"accuracy_group\"])\n\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_xgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title('XGB QWK (percentile opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y_oof_xgb)\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y_oof_xgb,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title('XGB QWK (bayesian opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"accuracy_group\"] = df_train[\"accuracy_group\"].values\ndf[\"sample\"] = df_train[\"sample\"].values\ndf[\"installation_id\"] = df_train[\"installation_id\"].values\ndf[\"y\"] = y_oof_lgb\ndf[\"y_xgb\"] = y_oof_xgb\ndf[\"solved\"] = y_lgb_solved_oof\ndf[\"first\"] = y_lgb_first_oof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (2*0.7*df[\"y\"] + 2*0.3*df[\"y_xgb\"] + 2*3*df[\"solved\"]  + 3*df[\"first\"])/5\nrmse_scores,  oof_cohen_scores  = trunc_rmse_qwk_score ( df_train[\"accuracy_group\"].values, y, df_train[\"installation_id\"].values, nsamples=5000)\nnp.median(rmse_scores),  np.median(oof_cohen_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a,b,c = calc_bound (y, df_train[\"accuracy_group\"])\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title('QWK (percentile opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')\n\na,b,c = qwk3_optimizer ( df_train[\"accuracy_group\"], y)\n\noof_cohen_scores  = trunc_qwk_score ( df_train[\"accuracy_group\"].values, round_prediction (y,a,b,c), df_train[\"installation_id\"].values, nsamples=5000)\n\nfig, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1, 1, 1)\nplt.title(' QWK (bayesian opt)')\nplt.hist(oof_cohen_scores, bins=100)\nplt.show()\n\nprint (f'a:{a} b:{b} c:{c} oof_cohen: {np.median(oof_cohen_scores)} std:{oof_cohen_scores.std()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train\ngc.collect()\npsutil.virtual_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ny_test_lgb = predict_lgb (NMODELS, NFOLDS) \ny_test_xgb = predict_xgb (NMODELS, NFOLDS) \ny_test_auc_solved = predict_auc_lgb ( \"solved\", NMODELS, NFOLDS)\ny_test_auc_first = predict_auc_lgb ( \"first\", NMODELS, NFOLDS)\n\n\ny_test = (2*0.7*y_test_lgb + 2*0.3*y_test_xgb + 2*3*y_test_auc_solved  + 3*y_test_auc_first)/5\ny_test = round_prediction (y_test,a,b,c)\n\nsubmission = pd.DataFrame()\nsubmission[\"installation_id\"] = test[\"installation_id\"]\nsubmission[\"accuracy_group\"] = y_test.astype(int)\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}