{"cells":[{"metadata":{},"cell_type":"markdown","source":"** Import Libraries **"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 100)\n\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\n\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import tqdm_notebook\n\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n%matplotlib inline\nplt.rc('figure', figsize=(15.0, 8.0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/data-science-bowl-2019\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nroot = '../input/data-science-bowl-2019/'\n\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\ntrain = pd.read_csv(root + 'train.csv',usecols=keep_cols)\ntest = pd.read_csv(root + 'test.csv', usecols=keep_cols)\n\ntrain_labels = pd.read_csv(root + 'train_labels.csv')\nspecs = pd.read_csv(root + 'specs.csv')\n##sample_submission = pd.read_csv(root + 'sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of train data', train.shape)\nprint('Size of train_labels data', train_labels.shape)\nprint('Size of specs data', specs.shape)\nprint('Size of test data', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reduce Memory of DF **"},{"metadata":{},"cell_type":"markdown","source":"Explore data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique classes in each column"},{"metadata":{},"cell_type":"markdown","source":"specs column types"},{"metadata":{"trusted":true},"cell_type":"code","source":"specs.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** CHECK FOR MISSING Values **"},{"metadata":{},"cell_type":"markdown","source":"** Check variable correlation **\n** credit to below notebook **"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/tanreinama/ds-bowl-2019-simple-lgbm-aggregated-data-with-cv\nhttps://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction\n\n"},{"metadata":{},"cell_type":"markdown","source":"** Combine all the summary metrics from above **"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_df(df):\n    \n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    \n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n\n    group3.reset_index(inplace = True) \n    \n    return group3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n##train_small = group_and_reduce(train)\n##test_small = group_and_reduce(test)\n\ntrain_small = summarize_df(train)\ntest_small = summarize_df(test)\n\n\n\nprint(train_small.shape)\ntrain_small.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"small_labels = train_labels[['installation_id', 'accuracy_group']].set_index('installation_id')\n##small_labels = train_labels[['installation_id', 'accuracy_group']].reset_index()\n##train_joined = train_small.join(small_labels).dropna()\ntrain_joined = train_small.merge(small_labels,on='installation_id', how='left').dropna()\n##train_joined.head()\n##train_joined.columns\n##print(type(train_joined))\n##aa = pd.DataFrame(train_joined.ravel())\n##aa.columns\n##print(small_labels.iloc['installation_id'].head())\n\n#train_joined.head()\ntrain_joined.drop(columns=['accuracy_group','installation_id'])"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n##small_labels = train_labels[['installation_id', 'accuracy_group']].set_index('installation_id')\nsmall_labels = train_labels[['installation_id', 'accuracy_group']]\n##train_joined = train_small.join(small_labels).dropna()\ntrain_joined = train_small.merge(small_labels,on='installation_id', how='left').dropna()\n##train_joined.set_index('installation_id')\nkf = KFold(n_splits=10, random_state=2019)\nX = train_joined.drop(columns=['accuracy_group','installation_id']).values\ny = train_joined['accuracy_group'].values.astype(np.int32)\ny_pred = np.zeros((len(test_small), 4))\nfor train, test in kf.split(X):\n    x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n    train_set = lgb.Dataset(x_train, y_train)\n    val_set = lgb.Dataset(x_val, y_val)\n\n    params = {\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.9,\n        'feature_fraction': 0.9,\n        'num_leaves': 14,\n        'lambda_l1': 0.1,\n        'lambda_l2': 1,\n        'metric': 'multiclass',\n        'objective': 'multiclass',\n        'num_classes': 4,\n        'random_state': 2019\n    }\n\n    model = lgb.train(params, train_set, \n                      num_boost_round=100, \n                      early_stopping_rounds=20, \n                      valid_sets=[train_set, val_set], \n                      verbose_eval=100)\n    \n    test_df = test_small.drop(columns='installation_id')\n    y_pred += model.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_max = y_pred.argmax(axis=1)\n##y_pred_max\n##(pd.DataFrame(y_pred_max)).describe()\n\n##np.histogram(y_pred_max)\n\ntest_small['accuracy_group'] = pd.DataFrame(y_pred_max)\n\n##print(test_small[['accuracy_group']])\n\ntest_small[['accuracy_group']].groupby(['accuracy_group']).sum()\n\n##type(test_small)\n##test_small.columns\n##print(test_small[\"installation_id\"])\n\n##y_pred\n\n##test_small.shape\n##y_pred.argmax(axis=1)\n\nsubmission = pd.concat([test_small['installation_id'],\n                                     pd.DataFrame(y_pred).idxmax(1)], axis=1)\nsubmission.columns = ['installation_id','accuracy_group']\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n##submission.to_csv('submission.csv', index= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}