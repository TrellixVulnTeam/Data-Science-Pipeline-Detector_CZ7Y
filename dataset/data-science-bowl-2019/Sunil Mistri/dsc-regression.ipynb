{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport json\nfrom pandas.io.json import json_normalize\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\nfrom collections import defaultdict\nfrom collections import Counter\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 4\nimport lightgbm as lgb\nimport time\nimport datetime\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nfrom numba import jit\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import deque","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_group(accuracy):\n    acc_group = np.nan\n\n    if accuracy == 0:\n        acc_group = 0\n    elif accuracy == 1:\n        acc_group = 3\n    elif accuracy == 0.5:\n        acc_group = 2\n    else:\n        acc_group = 1\n\n    return acc_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_raw_data(n1,n2):\n    \n    print('Start reading train data')\n    train101 = pd.read_csv('/kaggle/input/data-science-bowl-2019//train.csv', nrows=n1)\n   \n    print('Start reading test data')\n    test101 = pd.read_csv('/kaggle/input/data-science-bowl-2019//test.csv', nrows=n2)\n   \n    print('Start reading train lables data')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    \n    \n    print('Start reading specs data')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    \n    print('Start reading sample_submission data')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    \n    print( \"Raw size...\")\n    print(train101.shape)\n    print(test101.shape)\n    \n    train_ids_with_subms = train101[train101['type'] == \"Assessment\"]['installation_id'].drop_duplicates().tolist()\n    train101= train101[train101['installation_id'].isin(train_ids_with_subms)]\n    \n    \n    test_ids_with_subms = test101[test101['type'] == \"Assessment\"]['installation_id'].drop_duplicates().tolist()\n    test101= test101[test101['installation_id'].isin(test_ids_with_subms)]\n   \n    print( \"keep ..only with assessments..\")\n    \n    print(train101.shape)\n    print(test101.shape)\n \n    # convert text into datetime\n    train101['timestamp'] = pd.to_datetime(train101['timestamp'])\n    test101['timestamp'] = pd.to_datetime(test101['timestamp'])  \n    \n    \n    train101.rename(columns={'type': 'sesion_type'}, inplace=True)\n    test101.rename(columns={'type': 'sesion_type'}, inplace=True)\n    \n    \n    train101['title'] = train101['title'].str.slice(0,7).str.replace(' ','_') + \"_\" + train101['sesion_type'].str.slice(0,2)\n    test101['title'] = test101['title'].str.slice(0,7).str.replace(' ','_') + \"_\" + test101['sesion_type'].str.slice(0,2)\n    \n   \n    print(\"raw files reading completed...\")\n    return train101, test101, train_labels, specs, sample_submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_last_row(test):\n    test.sort_values(['installation_id','timestamp'], inplace=True)\n    test.loc[test.groupby('installation_id')['event_code'].tail(1).index, 'event_code'] = 4100\n    \n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process(input_df):\n    \n    input_df.loc[((input_df['event_code']==4100) &  (input_df['title']=='Bird_Me_As')),'event_code']=1001\n    x= input_df.copy()\n    \n    x.loc[((x['event_code']==4110) &  (x['title']=='Bird_Me_As')),'event_code']=4100\n   \n    del input_df\n    gc.collect()\n    \n    #x = x[x.installation_id=='051794c4']\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_json(input_df):\n    \n    variables_array = []\n    event_data_dict = {}\n    row_counter = 0\n    for r in input_df.itertuples(name='Row', index=False):\n        \n        correct_cnt = np.nan\n        wrong_cnt =np.nan\n        #print(r.sesion_type)\n\n        json_string = json.loads(r.event_data)\n\n        if json_string.get('correct') is True:\n            correct_cnt =1\n            wrong_cnt = 0\n        if json_string.get('correct') is False:\n            wrong_cnt = 1\n            correct_cnt = 0\n\n        variables_array.append([r.installation_id, r.game_session, r.title, r.sesion_type ,\n                                r.event_code , r.event_id,  r.game_time ,  r.timestamp, json_string.get('duration') , \n                                correct_cnt, wrong_cnt,\n                                json_string.get('dwell_time')]) \n        row_counter +=1\n    \n    del input_df\n    gc.collect()\n    \n    print(row_counter)\n    print(len(variables_array))\n    return pd.DataFrame(data=variables_array, columns=['installation_id','game_session','title', 'sesion_type',\\\n                                                       'event_code','event_id', 'game_time' ,  'timestamp', 'duration','correct_cnt', 'wrong_cnt','dwell_time' ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cal_accuracy(input_df, session_type):\n    \n    input_df['timestamp'] = pd.to_datetime(input_df['timestamp'])\n    \n    if session_type=='Assessment':\n        \n        act_assm_gm =input_df.loc[ ((input_df['event_code']==4100) & (input_df['sesion_type']==session_type)) ,['installation_id','game_session','event_code','sesion_type', 'title','correct_cnt','wrong_cnt']]\\\n        .groupby(['installation_id','game_session','sesion_type','title']).agg({'correct_cnt':'sum', 'wrong_cnt':'sum'}).reset_index()\n    else:\n        act_assm_gm =input_df.loc[ (input_df['sesion_type']==session_type) ,['installation_id','game_session','event_code','sesion_type', 'title','correct_cnt','wrong_cnt']]\\\n        .groupby(['installation_id','game_session','sesion_type','title']).agg({'correct_cnt':'sum', 'wrong_cnt':'sum'}).reset_index()\n        \n    act_assm_gm['accuracy'] = act_assm_gm['correct_cnt']/(act_assm_gm['correct_cnt']+ act_assm_gm['wrong_cnt']) \n    act_assm_gm.head()\n\n    act_assm_tm = input_df[['installation_id','game_session','event_code','sesion_type', 'title','duration']]\\\n    .groupby(['installation_id','game_session','sesion_type','title']).agg({'duration':'sum'}).reset_index()\n    \n    act_assm_tm.head()\n    output_df = input_df[['installation_id','game_session','title','sesion_type','timestamp','game_time']]\\\n        .groupby(['installation_id','game_session','title','sesion_type']).agg({'game_time':'max', 'timestamp':'min'}).reset_index()\n\n    output_df = pd.merge(output_df,act_assm_tm[['installation_id','game_session', 'duration']], on=['installation_id','game_session'], how='inner')\n    \n    #output_df['tm']=output_df['duration'].mask(pd.isnull, output_df['game_time'])\n    output_df.loc[(output_df['game_time'] >= 5000000) ,'game_time'] = 5000000\n\n\n    # bring in accuracy count\n\n    output_df = pd.merge(output_df,act_assm_gm[['installation_id','game_session', 'correct_cnt','wrong_cnt','accuracy']],\n                         on=['installation_id','game_session'], how='inner')\n    output_df['acc_group'] = output_df['accuracy'].map(get_group)\n    #output_df['tm_std'] = output_df.groupby('title')['tm'].transform(lambda x: minmax_scale(x.astype(float)))\n    output_df['tm'] = output_df.groupby('title')['game_time'].transform(lambda x: minmax_scale(x.astype(float)))\n    output_df['total_attemps'] = output_df['correct_cnt']+ output_df['wrong_cnt']\n    output_df['mod_accuracy']=output_df.apply(lambda x: x['accuracy'] + (x['accuracy']*(1- x['tm'])) , axis=1)\n    \n    del input_df\n    gc.collect()\n    \n    print(output_df.shape)\n    \n    return output_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_pivot(pivot_df):\n    \n    \n    # select all assessments except the 4100 of Bird_Measu_Assessment\n    acc2 = pivot_df.pivot_table( index=['installation_id','game_session'],\\\n                                 columns='title', values=['correct_cnt','wrong_cnt','accuracy','acc_group','tm','mod_accuracy'],\\\n                                   aggfunc={ 'tm': np.sum, \n                                             'correct_cnt':np.sum,\n                                            'wrong_cnt': np.sum , \n                                           'accuracy':np.sum ,\n                                           'acc_group':np.sum,\n                                            'mod_accuracy':np.sum\n                                           }, fill_value=np.nan)\n\n\n    acc2.columns.tolist()\n    acc2.columns = ['_'.join(map(str,i)) for i in acc2.columns.tolist()]\n    acc2 = acc2.reset_index()\n    print(acc2.shape)\n\n    # bring timestamp for correct accumulation of all numbers\n    print(acc2.shape)\n   \n    acc_cumul = pd.merge(pivot_df[['installation_id','game_session','title','sesion_type']], \n                         acc2, on= ['installation_id','game_session'], how= 'inner')\n    \n    # this should be same as training lables file..\n    acc_cumul.shape\n    \n    del pivot_df\n    gc.collect()\n    \n    return acc_cumul","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_event_pivot(input_df, pivot_column):\n    \n    if pivot_column=='event_id':\n        code_list= [3110,4070,3010,4020,2020,2030,3020,3021,3120,3121]\n    else:\n        code_list= [4010,2030,4025,4030,3020,3010,4100,3110,4020,4070,4220,2020,4110,\n                    4090,4031,3121,2010,3120,3021,2060,4035,2050,4045,4040,4022,2083,\n                    2070,2075,2080,5010,2035,4235,4095,4021,5000,4230,2025,2081,2040]\n\n    \n    \n    x= input_df.loc[input_df['event_code'].isin(code_list), ['installation_id','game_session','event_code','event_id']]\n    t1 = x.pivot_table( index=['installation_id','game_session'],\n                                                        columns=[pivot_column],\n                                                        values=[pivot_column], \n                                                        aggfunc= { pivot_column: 'size' }, \n                                                        fill_value=np.nan)\n    t1.columns.tolist()\n    t1.columns = ['_cnt_'.join(map(str,i)) for i in t1.columns.tolist()]\n    t1 = t1.reset_index()\n    print(t1.shape)\n    \n    del input_df\n    gc.collect()\n\n    return t1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_other_kpi(input_df, n_sessions):\n\n\n    input_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in input_df.columns]\n    input_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n    variables_array = []\n    fixed_cols =  ['installation_id', 'timestamp', 'game_session', 'title', 'sesion_type']\n    drop_cols =  ['game_time', 'duration']\n    num_col_names = (set(input_df.columns.tolist())- set(fixed_cols) -set(drop_cols) )\n\n\n    \n\n    for inst_id, g_session in input_df.groupby('installation_id'):\n        kpi_deque = {name:deque([np.nan],n_sessions) for name in num_col_names}\n        \n        for r in g_session.itertuples(name='Row', index=False):\n            #print(inst_id)\n            if r.sesion_type == 'Assessment':\n\n                accuracy_dict = {name:0 for name in num_col_names}\n                fixed_cols_dict = {name:'' for name in fixed_cols } \n                # update accuracy_dict \n                for i in num_col_names:\n                    accuracy_dict[i]= np.nanmean (kpi_deque[i])\n                    kpi_deque[i].append(getattr(r, i))\n                for i in fixed_cols:\n                    fixed_cols_dict[i]=getattr(r, i) \n\n                variables_array.insert(len(variables_array), {**fixed_cols_dict,**accuracy_dict})      \n            else:\n                for i in num_col_names:\n                    kpi_deque[i].append(getattr(r, i))\n    del input_df\n    gc.collect()\n                    \n    return pd.DataFrame(variables_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_agg(input_df, lable_df,df_type):\n\n    tr_cols = input_df.columns.tolist()\n    \n    fixed_cols= [ 'installation_id', 'game_session','title','timestamp','sesion_type']\n    event_code_columns = [s for s in tr_cols if \"event_code\" in s ]\n    event_id_columns = [s for s in tr_cols if \"event_id\" in s ]\n    accu_columns =  [s for s in tr_cols if \"_As\" in s ]\n    game_columns =  [s for s in tr_cols if \"_Ga\" in s ]\n    first_df_col = fixed_cols + accu_columns\n    second_df_col = fixed_cols + event_id_columns\n    third_df_col =  fixed_cols + event_code_columns\n    fourth_df_col = fixed_cols + game_columns\n\n    processed_df1= get_other_kpi(input_df[first_df_col],40)\n    processed_df2= get_other_kpi(input_df[second_df_col],40)\n    processed_df3= get_other_kpi(input_df[third_df_col],40)\n    processed_df4= get_other_kpi(input_df[fourth_df_col],40)\n\n    \n    \n    processed_df = pd.merge(processed_df1,processed_df2, on =['installation_id','timestamp','game_session','title','sesion_type'], how='inner')\n    processed_df = pd.merge(processed_df,processed_df3, on =['installation_id','timestamp','game_session','title','sesion_type'], how='inner')\n    processed_df = pd.merge(processed_df,processed_df4, on =['installation_id','timestamp','game_session','title','sesion_type'], how='inner')\n    \n    if  df_type=='train':\n        processed_df = pd.merge(processed_df,lable_df, on =['installation_id','game_session'], how='inner')\n        \n   \n    del input_df\n    del processed_df1\n    del processed_df2\n    del processed_df3\n    del processed_df4\n    \n    gc.collect()\n    \n    processed_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in processed_df.columns]\n    \n    title_dict = {'Cart_Ba_As':0, 'Cauldro_As':1, 'Chest_S_As':2, 'Mushroo_As':3, 'Bird_Me_As':4}\n    processed_df['title'] = processed_df['title'].map(title_dict)\n    \n\n    all_columns  = processed_df.columns.to_list()\n    acc_columns= ['accuracy_Bird_Me_As', 'accuracy_Mushroo_As', 'accuracy_Cauldro_As', 'accuracy_Chest_S_As', 'accuracy_Cart_Ba_As']\n    acc_grp_columns =  ['acc_group_Cauldro_As', 'acc_group_Bird_Me_As','acc_group_Chest_S_As','acc_group_Cart_Ba_As', 'acc_group_Mushroo_As']\n    acc_mod_columns = ['mod_accuracy_Cauldro_As','mod_accuracy_Bird_Me_As','mod_accuracy_Chest_S_As','mod_accuracy_Mushroo_As','mod_accuracy_Cart_Ba_As']\n\n    acc_cls = [elem for elem in acc_columns if  any(re.search('(^|\\s){}(\\s|$)'.format(c), elem) for c in all_columns)]\n    acc_grp_cls = [elem for elem in acc_grp_columns if  any(re.search('(^|\\s){}(\\s|$)'.format(c), elem) for c in all_columns)]\n    acc_mod_cls = [elem for elem in acc_mod_columns if  any(re.search('(^|\\s){}(\\s|$)'.format(c), elem) for c in all_columns)]\n    \n    \n      \n    gm_columns= ['accuracy_Crystal_Ga','accuracy_Pan_Bal_Ga','accuracy_Leaf_Le_Ga','accuracy_Chow_Ti_Ga',\n                 'accuracy_Dino_Di_Ga','accuracy_Dino_Dr_Ga','accuracy_Happy_C_Ga','accuracy_Bubble__Ga',\n                 'accuracy_Air_Sho_Ga','accuracy_Scrub_A_Ga','accuracy_All_Sta_Ga']\n    \n    gm_grp_columns= ['acc_group_Crystal_Ga','acc_group_Pan_Bal_Ga','acc_group_Leaf_Le_Ga','acc_group_Chow_Ti_Ga',\n                 'acc_group_Dino_Di_Ga','acc_group_Dino_Dr_Ga','acc_group_Happy_C_Ga','acc_group_Bubble__Ga',\n                 'acc_group_Air_Sho_Ga','acc_group_Scrub_A_Ga','acc_group_All_Sta_Ga']\n    \n    \n        \n    gm_cls =  [elem for elem in gm_columns if  any(re.search('(^|\\s){}(\\s|$)'.format(c), elem) for c in all_columns)]\n    gm_grp_cls =  [elem for elem in gm_grp_columns if  any(re.search('(^|\\s){}(\\s|$)'.format(c), elem) for c in all_columns)]\n    \n    processed_df['gm_group_col_ax1']=  processed_df[gm_cls].sum(axis=1)\n    processed_df['gm_accuracy_col_ax1']=  processed_df[gm_grp_cls].sum(axis=1)\n\n    processed_df['gm_group_avg'] = processed_df.groupby(['installation_id'])['gm_group_col_ax1'].transform('mean')\n    processed_df['gm_accuracy_avg'] = processed_df.groupby(['installation_id'])['gm_accuracy_col_ax1'].transform('mean')\n\n   \n    processed_df['acc_group_col_ax1']=  processed_df[acc_cls].sum(axis=1)\n    processed_df['accuracy_col_ax1']=  processed_df[acc_grp_cls].sum(axis=1)\n    processed_df['mod_acc_group_col_ax1']=  processed_df[acc_mod_cls].sum(axis=1)\n\n\n\n    processed_df['acc_group_avg'] = processed_df.groupby(['installation_id'])['acc_group_col_ax1'].transform('mean')\n    processed_df['accuracy_avg'] = processed_df.groupby(['installation_id'])['accuracy_col_ax1'].transform('mean')\n    processed_df['mod_acc_group_avg'] = processed_df.groupby(['installation_id'])['mod_acc_group_col_ax1'].transform('mean')\n    \n    \n\n    return processed_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_labels, specs, sample_submission = process_raw_data(n1=3000000000,n2=1000000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pre_process(train)\nprint(train.shape)\ntest = pre_process(test)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_last_row(test)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_df_train = parse_json(train)\nprint(json_df_train.shape)\ngc.collect()\njson_df_test = parse_json(test)\nprint(json_df_test.shape)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_df_train = cal_accuracy(json_df_train,'Assessment')\nprint(acc_df_train.shape)\nacc_df_test =  cal_accuracy(json_df_test,'Assessment')\nprint(acc_df_test.shape)\n\n#only for traindataset\nlable_df = acc_df_train.loc[( acc_df_train.sesion_type=='Assessment'),['installation_id','game_session','mod_accuracy', 'acc_group'] ] \n#lable_df[lable_df.installation_id=='051794c4']\nlable_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_df_train[['acc_group', 'mod_accuracy']].groupby('acc_group').describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gm_df_train = cal_accuracy(json_df_train,'Game' )\nprint(gm_df_train.shape)\ngm_df_test =  cal_accuracy(json_df_test,'Game')\nprint(gm_df_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_dpivot_df_train = accuracy_pivot(acc_df_train)\nprint(acc_dpivot_df_train.shape)\nacc_dpivot_df_test = accuracy_pivot(acc_df_test)\nprint(acc_dpivot_df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gm_pivot_df_train = accuracy_pivot(gm_df_train)\nprint(gm_pivot_df_train.shape)\ngm_pivot_df_test = accuracy_pivot(gm_df_test)\nprint(gm_pivot_df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_df_train = json_df_train[['installation_id','game_session','title', 'sesion_type','timestamp']]\\\n        .groupby(['installation_id','game_session','title','sesion_type']).agg({'timestamp':'min'}).reset_index()\nmaster_df_train.shape\nmaster_df_test = json_df_test[['installation_id','game_session','title', 'sesion_type','timestamp']]\\\n        .groupby(['installation_id','game_session','title','sesion_type']).agg({'timestamp':'min'}).reset_index()\nprint(master_df_train.shape)\nprint(master_df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_count_df_train = get_event_pivot(json_df_train, 'event_code')\nevent_count_df_test = get_event_pivot(json_df_test, 'event_code')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_id_count_df_train = get_event_pivot(json_df_train, 'event_id')\nevent_id_count_df_test = get_event_pivot(json_df_test, 'event_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del json_df_train\ndel json_df_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_11 = pd.merge(master_df_train , acc_dpivot_df_train.drop(columns=['sesion_type']) , on =['installation_id','game_session'], how='left', suffixes=(\"\", \"_y\"))\ntrain_df_11 = pd.merge(train_df_11 , gm_pivot_df_train.drop(columns=['sesion_type']) , on =['installation_id','game_session'], how='left', suffixes=(\"\", \"_y\"))\ntrain_df_11 = pd.merge(train_df_11 , event_count_df_train , on =['installation_id','game_session'], how='left')\ntrain_df_11 = pd.merge(train_df_11, event_id_count_df_train , on =['installation_id','game_session'], how='left')\ntrain_df_11.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_11 = pd.merge(master_df_test , acc_dpivot_df_test.drop(columns=['sesion_type']) ,  on =['installation_id','game_session'], how='left', suffixes=(\"\", \"_y\"))\ntest_df_11 = pd.merge(test_df_11 , gm_pivot_df_test.drop(columns=['sesion_type']) , on =['installation_id','game_session'], how='left', suffixes=(\"\", \"_y\"))\ntest_df_11 = pd.merge(test_df_11 , event_count_df_test , on =['installation_id','game_session'], how='left')\ntest_df_11 = pd.merge(test_df_11, event_id_count_df_test , on =['installation_id','game_session'], how='left')\ntest_df_11.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all = get_agg(train_df_11,lable_df,'train')\nprint( train_all.shape)\ntest_all = get_agg(test_df_11, 'NA','test')\nprint( test_all.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df_11\ndel test_df_11\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all[['mod_accuracy']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_all = test_all.sort_values(['installation_id','timestamp']).groupby('installation_id', sort=False).tail(1)\ntest_all[test_all['installation_id'].isin(['048e7427'])] # sholud give last row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_all.shape)\nprint(test_all.shape)\ntrain_all[['mod_accuracy']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [ 'installation_id', 'game_session','timestamp','sesion_type', \n                'tm_Bird_Me_As', 'tm_Cart_Ba_As', 'tm_Mushroo_As', 'tm_Cauldro_As', \n                'tm_Chest_S_As', 'tm_Pan_Bal_Ga', 'tm_Air_Sho_Ga']\n\n\ntrain = train_all.drop(columns= cols_to_drop + ['mod_accuracy'] + ['acc_group'])\ntest = test_all.drop(columns=cols_to_drop)\nacc = train_all['mod_accuracy'] # store training labels\nfeature_names = list(train.columns) # store feature names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nk_fold = KFold(n_splits = n_folds, shuffle=True,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reg_model(train,n_folds):\n    \n    validation_scores = [] \n    training_scrores = [] \n    imp_features = np.zeros(len(feature_names)) \n    oof = np.zeros(train.shape[0]) \n     \n   \n    \n\n    for tr_idx, vld_idx in k_fold.split(train):\n        print(k_fold.n_splits)\n        x_train, x_train_acc = train.iloc[tr_idx], acc.iloc[tr_idx] \n        x_valid, x_valid_acc = train.iloc[vld_idx], acc.iloc[vld_idx] \n\n        model = lgb.LGBMRegressor( n_estimators=2000, \n                                    objective='regression',\n                                    boosting_type='gbdt',\n                                    metric= 'rmse',\n                                    subsample= 0.75,\n                                    subsample_freq= 1,\n                                    learning_rate= 0.04,\n                                    feature_fraction= 0.8,\n                                    max_depth= 10,\n                                    lambda_l1= 0.5,  \n                                    lambda_l2= 0.5,\n                                    verbose=100,\n                                    early_stopping_rounds=100,\n                                    random_state=50)\n\n        model.fit(x_train, x_train_acc, \n                   eval_metric='rmse', \n                   eval_set = [(x_valid, x_valid_acc), (x_train, x_train_acc)],\n                   eval_names = ['x_valid','x_train'], \n                   early_stopping_rounds = 100, \n                   verbose = 100,\n                   categorical_feature = ['title'])\n \n        best_iter = model.best_iteration_ \n        imp_features += model.feature_importances_/k_fold.n_splits \n        oof[vld_idx] = model.predict(x_valid, num_iteration = best_iter).reshape(-1,)/k_fold.n_splits\n\n \n        valid_score = model.best_score_['x_valid']['rmse']\n        train_score = model.best_score_['x_train']['rmse']\n\n        validation_scores.append(valid_score)\n        training_scrores.append(train_score)\n        \n    del x_train, x_valid\n    gc.collect()\n        \n    return model, imp_features , oof, validation_scores, training_scrores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, feat_imp_vals , oof, valid_scores, train_scores =  reg_model(train,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(test)\ny_pred.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[y_pred <= 0.7] = 0\ny_pred[np.where(np.logical_and(y_pred > 0.7, y_pred <= 1.3))] = 1\ny_pred[np.where(np.logical_and(y_pred > 1.3, y_pred <=1.85))] = 2\ny_pred[y_pred > 1.85] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x11= test_all.copy()\nx11['accuracy_group']= y_pred\nsample_submission =pd.merge(sample_submission[['installation_id']], x11[['installation_id','accuracy_group']], \n                            on='installation_id' , how='inner')\nsample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\nprint(sample_submission.shape)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}