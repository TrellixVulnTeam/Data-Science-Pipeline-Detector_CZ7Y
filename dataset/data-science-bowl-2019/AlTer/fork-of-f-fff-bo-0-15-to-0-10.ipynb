{"cells":[{"metadata":{},"cell_type":"markdown","source":"Base on https://www.kaggle.com/artgor/quick-and-dirty-regression @artgor\n\nPlease upvote the original kernel, thanks. üëã"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport random\nrandom.seed(1029)\nnp.random.seed(1029)\n\nimport os\nimport copy\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nimport time\nfrom collections import Counter\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\nfrom category_encoders.ordinal import OrdinalEncoder\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom typing import List\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nfrom functools import partial\nimport scipy as sp\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom typing import Any\nfrom itertools import product\npd.set_option('max_rows', 500)\nimport re\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions and classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyGroupShuffleSplit:\n    def __init__(self,n_splits=5,n_repeat=5,seed=22):\n        self.n_splits = n_splits\n        self.n_repeat = n_repeat\n        self.seed = seed\n        \n    def split(self, X, y = None, groups = None):\n        \n        nuniq = groups.unique()\n\n        group_to_ix = groups.reset_index().set_index(groups.name)\n\n        for rnd in range(self.n_repeat):\n            folds = KFold(n_splits=self.n_splits, random_state = self.seed+rnd*7, shuffle=True)\n\n            for ix_group_train, ix_group_valid in folds.split(nuniq):\n                gr_train = nuniq[ix_group_train]\n                gr_valid = nuniq[ix_group_valid]\n\n                ix_train = group_to_ix.loc[gr_train]['index'].values\n                ix_valid = group_to_ix.loc[gr_valid]['index'].values\n                \n                yield ix_train, ix_valid\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef norm_samples(df, n = 15, added=False, seed=22):\n    df_list = []\n    if added:\n        df_list.append(df) # –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Ç–µ–∫—É—â–µ–º—É\n\n    df0 = df\n    df = df0[['installation_id','Assessment']]\n    df['Assessment_count'] = df.groupby('installation_id').cumcount()\n    #df['max_Assessment'] = df.groupby('installation_id')['Assessment'].transform('max') #.max()\n    xx = df.groupby('installation_id')['Assessment_count'].max()\n    local_state = np.random.RandomState(seed)\n    for i in range(n):\n        sr = xx.apply(lambda x: local_state.randint(0,x+1))\n        #display(sr['0006a69f'])\n        df['rnd_Assessment'] = df.installation_id.map(sr)\n        dfnew = df0.loc[df.query(\"rnd_Assessment==Assessment_count\").index]\n        #display(dfnew[dfnew['installation_id']=='0006a69f'])\n        df_list.append(dfnew)\n    \n    df = pd.concat(df_list, axis=0, sort=False, ignore_index=True)  \n    return df\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    y_pred[y_pred <= 1.12232214] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n    y_pred[y_pred > 2.22506454] = 3\n\n    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n\n    return 'cappa', qwk(y_true, y_pred), True\n\n\nclass LGBWrapper_regr(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb_regr\n        else:\n            eval_metric = 'auc'\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n\n    \ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n\n\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n#                          or 'attempt' in col]\n        \n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n    \n    \nclass RegressorModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper_fun=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper_fun = model_wrapper_fun\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='rmse',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True,\n            weight: str = None,\n            norm_valid: bool = True\n           ):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n        \n        if weight is not None:\n            cols_to_drop = cols_to_drop + [weight]\n            #X_weight = X[[weight]]\n        \n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.oof_cnt = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n\n        \n                \n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n            \n\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n\n            ## addv31  \n            if norm_valid:\n                #np.random.seed(2019)\n                #cum_list_x = []\n                #cum_list_y = []\n                #xv = X_valid.reset_index(drop=True)\n                \n                X_valid = norm_samples(X_valid,30)\n                \n                \n#                 for i in range(15):\n#                     sr = X.groupby('installation_id')['Assessment'].max().apply(lambda x: np.random.randint(0,x+1)).reset_index()\n#                     nx = pd.merge(X_valid,sr,on=['installation_id', 'Assessment'],how='inner')\n#                     cum_list_x.append(nx)\n#                     #cum_list_y.append(y_valid.iloc[nx.index])\n#                     #cum_list_y.append(X_valid['accuracy_group'])\n#                 X_valid = pd.concat(cum_list_x, axis=0, sort=False, ignore_index=True)            \n                y_valid = X_valid['accuracy_group']\n                ## end addv31            \n            \n            X_train_w = None\n            X_valid_w = None\n            if weight is not None:\n                print('set weight',weight)\n                #X_train_w = X_weight.iloc[train_index][weight].ravel()\n                #X_valid_w = X_weight.iloc[valid_index][weight].ravel()\n                X_train_w = X_train[weight].ravel()\n                X_valid_w = X_valid[weight].ravel()            \n            \n            \n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n            \n\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            #model = copy.deepcopy(self.model_wrapper)\n            model = self.model_wrapper_fun()\n\n            #model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params, X_train_w=X_train_w, X_valid_w=X_valid_w)\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n                self.oof_cnt = np.zeros((X.shape[0], self.oof.shape[1])) ## addv24\n\n            self.oof[valid_index]     = self.oof[valid_index]     + model.predict(X.iloc[valid_index][X_train.columns]).reshape(-1, n_target)\n            self.oof_cnt[valid_index] = self.oof_cnt[valid_index] + 1 ### addv24\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n        \n        ## addv24\n        self.oof[self.oof_cnt>0] = self.oof[self.oof_cnt>0]/ self.oof_cnt[self.oof_cnt>0]\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            # print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 16))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=30)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions');\n            plt.show()\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(sorted(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(sorted(set(train['title'].unique()).union(set(test['title'].unique()))))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(sorted(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n    list_of_event_id = list(sorted(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(sorted(set(train['world'].unique()).union(set(test['world'].unique()))))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(sorted(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n\ndef get_data(user_sample, test_set=False, test_as_train=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # add v4\n    act_corr = dict(zip(assess_titles,[0]*len(assess_titles)))  # acc answers by titles\n    act_uncorr = dict(zip(assess_titles,[0]*len(assess_titles)))\n        \n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n#              # add v4 rool_back_v5\n#             for kk in act_corr.keys():\n#                 features['accumulated_correct_attempts_'+kk] = act_corr[kk]\n#                 features['accumulated_uncorrect_attempts_'+kk] = act_uncorr[kk]\n#             if session_title_text in act_corr.keys():\n#                 act_corr[session_title_text] += true_attempts\n#                 act_uncorr[session_title_text] += false_attempts\n\n            \n            \n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # add v3\n    if test_as_train:            \n        return all_assessments[:-1]\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n# add v3\nfrom multiprocessing import Pool\n\ndef get_data_test(df):\n    return get_data(df,test_set = True)\n\ndef get_data_test_as_train(df):\n    return get_data(df,test_as_train = True)\n\n\ndef get_train_and_test2(train, test):\n    compiled_train = []\n    compiled_test = []\n\n    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort = False)), total = 1000):\n        compiled_train += get_data(user_sample,test_as_train = True)        \n    \n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n\ndef get_train_and_test_mp(train, test):\n    compiled_train = []\n    compiled_test = []\n    sample = []\n    \n    # add train data\n    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort = False), total = 17000):\n        sample.append(user_sample)\n\n    with Pool(4) as pool:\n        for samp in tqdm(pool.imap(get_data, sample),total = len(sample)):\n            compiled_train += samp\n\n    # add test as train data\n    sample = []\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        sample.append(user_sample)\n\n    with Pool(4) as pool:\n        for samp in tqdm(pool.imap(get_data_test_as_train, sample),total = len(sample)):\n            compiled_train += samp\n            \n            \n    sample = []\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        sample.append(user_sample)\n    with Pool(4) as pool:\n        for samp in tqdm(pool.imap(get_data_test, sample),total = len(sample)):\n            compiled_test.append(samp)\n\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n# tranform function to get the train and test set\nreduce_train0, reduce_test0, categoricals = get_train_and_test2(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        #df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean_sum'] = df.groupby(['installation_id'])['duration_mean'].transform('cumsum')\n        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        #df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['accumulated_actions/duration_mean'] = df['accumulated_actions']/df['duration_mean']\n        df['installation_duration_mean_sum/accumulated_actions'] = df['installation_duration_mean_sum']/(df['accumulated_actions']+1e-6)\n        df['Assessment/installation_duration_mean_sum'] = df['Assessment']/(df['installation_duration_mean_sum']+1)\n        \n        \n        \n        df['Activity/accumulated_actions'] = df['Activity']/df['accumulated_actions']\n        #df['Activity/accumulated_actions'] = df['Activity']/df['accumulated_actions']\n        df['Assessment/accumulated_actions'] = df['Assessment']/df['accumulated_actions']\n        \n        \n        #df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, features\n# call feature engineering function\nreduce_train, reduce_test, features = preprocess(reduce_train0, reduce_test0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess2(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, features\n\n# call feature engineering function\n#reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = reduce_train['accuracy_group']\n\ncols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate']\n\n#n_fold = 5\n#folds = GroupKFold(n_splits=n_fold)\n\nfolds = MyGroupShuffleSplit(5,3,22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef LGB_bayesian(max_depth,\n                 lambda_l1,\n                 lambda_l2,\n                 bagging_fraction,\n                 bagging_freq,\n                 colsample_bytree,\n                 learning_rate):\n    \n    params = {\n        'boosting_type': 'gbdt',\n        #'metric': 'rmse',\n        'metric': 'cappa',\n        'objective': 'regression',\n        'eval_metric': 'cappa',\n        'n_jobs': -1,\n        'seed': 42,\n        'early_stopping_rounds': 100,\n        'n_estimators': 2000,\n        'learning_rate': learning_rate,\n        'max_depth': int(max_depth),\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'bagging_fraction': bagging_fraction,\n        'bagging_freq': int(bagging_freq),\n        'colsample_bytree': colsample_bytree,\n        'verbose': 0\n    }\n    \n    mt = MainTransformer()\n    ft = FeatureTransformer()\n    transformers = {'ft': ft}\n    model = RegressorModel(model_wrapper_fun=LGBWrapper_regr)\n    model.fit(X=reduce_train, \n              y=y, \n              folds=folds, \n              params=params, \n              preprocesser=mt, \n              transformers=transformers,\n              eval_metric='cappa', \n              cols_to_drop=cols_to_drop,\n              plot=False)\n    \n    MODELS.append((model.scores['valid'], model, model.oof))\n    \n    return model.scores['valid']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# st = []\n# for cola in tqdm(corr.columns):\n#     if cola in st:\n#         continue\n#     for colb in corr.columns:\n        \n#         if cola==colb:\n#             continue\n#         if colb in st:\n#             continue\n        \n#         c = corr[cola][colb]\n#         if c > 0.95:\n#             ca = abs(corr[cola]['accuracy_group'])\n#             cb = abs(corr[colb]['accuracy_group'])\n#             #print(cola,colb,c,ca,cb)\n#             if ca>cb:\n#                 st.append(colb)\n#             else:\n#                 st.append(cola)\n#                 break\n\n# st = list(set(st))\n# st\n            \n# cols_to_drop = list(set(cols_to_drop+st))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_points = 16\nn_iter = 16\nMODELS = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bounds_LGB = {\n    'max_depth': (8, 11),\n    'lambda_l1': (0, 5),\n    'lambda_l2': (0, 5),\n    'bagging_fraction': (0.4, 0.6),\n    'bagging_freq': (1, 10),\n    'colsample_bytree': (0.4, 0.6),\n    'learning_rate': (0.05, 0.1)\n}\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1029)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS1 = MODELS.copy()\nMODELS = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    #'metric': 'rmse',\n    'metric': 'cappa',\n    'objective': 'regression',\n    'eval_metric': 'cappa',\n    'n_jobs': -1,\n    'seed': 42,\n    'early_stopping_rounds': 100,\n    'n_estimators': 2000,\n    'learning_rate': LGB_BO.max['params']['learning_rate'],\n    'max_depth': int(LGB_BO.max['params']['max_depth']),\n    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n    'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n    'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n    'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n    'verbose': 100\n}\n\nmt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}\nregressor_model = RegressorModel(model_wrapper_fun=LGBWrapper_regr)\nregressor_model.fit(X=reduce_train, \n                    y=y, \n                    folds=folds, \n                    params=params, \n                    preprocesser=mt, \n                    transformers=transformers,\n                    eval_metric='cappa', \n                    cols_to_drop=cols_to_drop, norm_valid=True)\n\npreds_1 = regressor_model.predict(reduce_test)\nw_1 = LGB_BO.max['target']\n\n# CV mean score on train: 0.7438 +/- 0.0161 std.\n# CV mean score on valid: 0.5603 +/- 0.0075 std.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bounds_LGB = {\n    'max_depth': (11, 14),\n    'lambda_l1': (0, 10),\n    'lambda_l2': (0, 10),\n    'bagging_fraction': (0.7, 1),\n    'bagging_freq': (1, 10),\n    'colsample_bytree': (0.7, 1),\n    'learning_rate': (0.08, 0.2)\n}\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1030)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n    \n#  19       |  0.5916","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS2 = MODELS.copy()\nMODELS = []\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n#    'metric': 'rmse',\n    'metric': 'cappa',\n\n    'objective': 'regression',\n    'eval_metric': 'cappa',\n    'n_jobs': -1,\n    'seed': 42,\n    'early_stopping_rounds': 100,\n    'n_estimators': 2000,\n    'learning_rate': LGB_BO.max['params']['learning_rate'],\n    'max_depth': int(LGB_BO.max['params']['max_depth']),\n    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n    'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n    'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n    'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n    'verbose': 100\n}\n\nmt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}\nregressor_model = RegressorModel(model_wrapper_fun=LGBWrapper_regr)\nregressor_model.fit(X=reduce_train, \n                    y=y, \n                    folds=folds, \n                    params=params, \n                    preprocesser=mt, \n                    transformers=transformers,\n                    eval_metric='cappa',\n                    cols_to_drop=cols_to_drop, norm_valid=True)\n\npreds_2 = regressor_model.predict(reduce_test)\nw_2 = LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blend and sumbit"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = 0\npreds = []\noofs = []\nN = 16\nsumscore = 0\nfor m in list(reversed(sorted(MODELS1)))[0:N]:\n    score, model, oof = m\n    scor = max(score - max(MODELS1)[0]+0.005, 0)\n    print(score, scor)\n    pred = model.predict(reduce_test)\n    preds.append(pd.Series(np.reshape(pred,-1))*scor)\n    oofs.append(pd.Series(np.reshape(oof,-1))*scor)\n    sumscore += scor\nfor m in list(reversed(sorted(MODELS2)))[0:N]:\n    score, model, oof = m\n    scor = max(score - max(MODELS2)[0]+0.005, 0)\n                          # 5965241075926819)[0]+0.002, 0)\n    print(score, scor)\n    pred = model.predict(reduce_test)\n    preds.append(pd.Series(np.reshape(pred,-1))*scor)\n    oofs.append(pd.Series(np.reshape(oof,-1))*scor)\n    sumscore += scor\n    \npr = pd.concat(preds,axis=1,sort=False)\n#print(pr)\npreds = pr.sum(axis=1)/sumscore\n#print(pr)\noofs = pd.concat(oofs,axis=1,sort=False).sum(axis=1)/sumscore\n\neval_qwk_lgb_regr(y,oofs.values)\n\n# 5902003584617939\n# 5914440609450073\n# 0.5917970087623345 5\n# 5927417394023831\n# 5934720869104393\n# 5943307075605857 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = (w_1/(w_1+w_2)) * preds_1 + (w_2/(w_1+w_2)) * preds_2\n\n# del preds_1, preds_2\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients = [1.12232214, 1.73925866, 2.22506454]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[preds <= coefficients[0]] = 0\npreds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\npreds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\npreds[preds > coefficients[2]] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = preds.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}