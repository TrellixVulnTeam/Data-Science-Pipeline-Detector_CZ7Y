{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.options.display.max_seq_items = 2000\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"keep_cols = ['game_session', 'installation_id','timestamp', 'event_data', 'event_code', 'title', 'game_time', 'type']\n\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', usecols=keep_cols)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n#specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train.head(1000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing installation ids not found in train_labels as these will not be useful in training\nstart_mem = train.memory_usage().sum() / 1024**2\n#print(\"Memory usage at start is {:.2f} MB\".format(start_mem))\nnot_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))\ntrain_new=~train['installation_id'].isin(not_req)\ntrain.where(train_new,inplace=True)\ntrain.dropna(inplace=True)\ntrain['event_code']=train.event_code.astype(int)\nend_mem = train.memory_usage().sum() / 1024**2\n#print(\"Memory usage at end is {:.2f} MB\".format(end_mem))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activityTypes= train[['title','type']].copy().drop_duplicates()\nlist_of_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\nlist_of_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\nactivity_correct_event_code = dict(zip(list_of_activities, (4100*np.ones(len(list_of_activities))).astype('int')))\nactivity_correct_event_code['Bird Measurer (Assessment)'] = 4110","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFeatures(userData, isTest):\n    counter = 0\n    #test_set=False\n    session_List=[]\n\n    for i, (ins_id, user_sample) in enumerate(userData.groupby('installation_id', sort=False)):\n        session_type_counter = dict({'Clip':0, 'Activity': 0, 'Game': 0, 'Assessment': 0})\n        time_spent = 0\n        clipTime=0\n        activityTime=0\n        gameTime=0\n        assessmentTime=0\n\n        activityLog = dict()\n        for i, row in activityTypes.iterrows():\n            activityLog[row['title']]=0\n            activityLog[row[\"title\"]+\"Time\"]=0\n            if (\"Assessment\" in row['title']):\n                activityLog[row[\"title\"]+\"Solved\"]=0\n\n\n        for i, session in user_sample.groupby('game_session', sort=False):\n            session_type = session['type'].iloc[0]\n            session_title = session['title'].iloc[0]\n            session_id = session['game_session'].iloc[0]\n            processAssessment=False\n            if (isTest) & (len(session)==1) & (session_type == 'Assessment'):\n                processAssessment = True\n                #print('checkpoint1 ', session_type, '  ', len(session), ' ', session_id)\n            elif (~isTest) & (len(session) > 1):\n                processAssessment = True\n            else:\n                processAssessment= False\n            \n\n            if (session_type == 'Assessment'):\n                all_attempts = session.query(f'event_code == {activity_correct_event_code[session_title]}')\n                true_attempts = all_attempts['event_data'].str.contains('true').sum()\n                false_attempts = all_attempts['event_data'].str.contains('false').sum()\n\n            if (session_type == 'Assessment') & (processAssessment):\n                Dict1 =dict({'installation_id': session['installation_id'].iloc[0]})\n                Dict1['game_session']=session['game_session'].iloc[0]\n\n                # Dict1['type']=session['type'].iloc[0]\n                Dict1['title']=session['title'].iloc[0]\n                Dict1['priorClips']=session_type_counter['Clip']\n                Dict1['priorActivity']=session_type_counter['Activity']\n                Dict1['priorGames']=session_type_counter['Game']\n                Dict1['priorAssessments']=session_type_counter['Assessment']\n                #Dict1['session_time']= session['game_time'].max()\n                #Dict1['total_prior_time']=time_spent\n    #            Dict1['avgclipTime']=clipTime/session_type_counter['Clip']\n    #            Dict1['avgactivityTime']=activityTime/session_type_counter['Activity']\n    #            Dict1['avggameTime']= gameTime/session_type_counter['Game']\n    #            Dict1['avgassessmentTime']=assessmentTime/session_type_counter['Assessment']\n                Dict1.update(activityLog)\n                if (isTest):\n                    session_List.append(Dict1)\n                else:\n                    if true_attempts+false_attempts > 0:\n                        session_List.append(Dict1)\n                              \n            if len(session)>1:\n                    session_type_counter[session_type]+=1\n                    time_spent+=session['game_time'].max()\n                    activityLog[session_title]+=1\n                    activityLog[session_title+\"Time\"]+=session['game_time'].max()\n\n                    if(session_type==\"Clip\"):\n                        clipTime+=session['game_time'].max()\n                    elif(session_type==\"Game\"):\n                        gameTime+=session['game_time'].max()\n                    elif(session_type==\"Activity\"):\n                        activityTime+=session['game_time'].max()\n                    elif(session_type==\"Assessment\"):\n                        assessmentTime+=session['game_time'].max()\n                        if true_attempts > 0:\n                            activityLog[session_title+\"Solved\"]+=1\n    processedDf=pd.DataFrame(session_List)\n    return processedDf\n# featuresDf.head(20)\n# print(len(session_List))\n# featuresDf=pd.DataFrame(session_List)\n# featuresDf.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresDf_xgb = getFeatures(train, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDf_xgb = getFeatures(test, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge with accuracy group data\nfeaturesDf_xgb=pd.merge(train_labels, featuresDf_xgb, on=['game_session', 'installation_id', 'title'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columns\nid_columns=['game_session', 'installation_id']\ndrop_columns=['num_correct', 'num_incorrect', 'accuracy']\ncat_columns=['title']\n#numeric_columns= ['priorClips',\n       #'priorActivity', 'priorGames', 'priorAssessments', 'total_prior_time', 'clipTime', 'activityTime', 'gameTime', 'assessmentTime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresDf_xgb.drop(id_columns, axis=1, inplace=True)\nfeaturesDf_xgb.drop(drop_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDf_xgb.drop(id_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = [x for x in featuresDf_xgb.columns if x not in ['accuracy_group']]\nX, y = featuresDf_xgb[all_features], featuresDf_xgb['accuracy_group']\n#del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_classifier():\n    clf = CatBoostClassifier(\n                               loss_function='MultiClass',\n    #                            eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n#                                depth=8,\n                               early_stopping_rounds=500,\n    #                            l2_leaf_reg=1,\n    #                            border_count=96,\n                               random_seed=2020\n                              )\n        \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = make_classifier()\nclf.fit(X, y, verbose=500, cat_features=cat_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = clf.predict(testDf_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'] = np.round(preds).astype('int')\nsubmission.to_csv('submission.csv', index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lb = LabelEncoder()\n# lb.fit(featuresDf_xgb['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# featuresDf_xgb['title']=lb.transform(featuresDf_xgb['title'])\n# testDf_xgb['title']=lb.transform(testDf_xgb['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train=featuresDf_xgb.drop('accuracy_group',axis=1)\n# y_train=featuresDf_xgb['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pars = {\n#     'colsample_bytree': 0.8,                 \n#     'learning_rate': 0.08,\n#     'max_depth': 10,\n#     'subsample': 1,\n#     'objective':'multi:softprob',\n#     'num_class':4,\n#     'eval_metric':'mlogloss',\n#     'min_child_weight':3,\n#     'gamma':0.25,\n#     'n_estimators':500\n# }\n\n# y_pre=np.zeros(len(testDf_xgb),dtype=float)\n#testDf_xgb=xgb.DMatrix(testDf_xgb)\n\n#kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n# y_pre=np.zeros((l,dtype=float)\n# testDf_xgb=xgb.DMatrix(testDf_xgb)\n\n# for train_index, val_index in kf.split(X_train, y_train):\n#     print('learning')\n#     train_X = X_train.iloc[train_index]\n#     val_X = X_train.iloc[val_index]\n#     train_y = y_train[train_index]\n#     val_y = y_train[val_index]\n#     xgb_train = xgb.DMatrix(train_X, train_y)\n#     xgb_eval = xgb.DMatrix(val_X, val_y)\n\n#     xgb_model = xgb.train(pars,\n#                   xgb_train,\n#                   num_boost_round=1000,\n#                   evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n#                   verbose_eval=False,\n#                   early_stopping_rounds=20\n#                  )\n\n#     pred=xgb_model.predict(testDf_xgb)\n#     y_pre+=pred\n\n#     #val_X=xgb.DMatrix(val_X)\n#     #pred_val=[np.argmax(x) for x in xgb_model.predict(val_X)]\n# 3\n# # fit model no training data\n# model = XGBClassifier()\n# model.fit(X_train.values, y_train.values)\n\n#y_pre=model.predict(testDf_xgb.values)\n\n# pred = np.asarray(y_pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pre=xgb_model.predict(testDf_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb_model, pred=model(X_train,y_train,testDf_xgb, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig, ax = plt.subplots(figsize=(10,10))\n#xgb.plot_importance(xgb_model, max_num_features=50, height=0.5, ax=ax,importance_type='gain')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n# submission['accuracy_group'] = np.round(pred).astype('int')\n# submission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}