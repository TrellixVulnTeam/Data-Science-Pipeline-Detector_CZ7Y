{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description\n\n\nThis kernel is based on the result of [Catboost - Some more features](https://www.kaggle.com/braquino/catboost-some-more-features).  \nThe largest difference between this kernel and the aforementioned reference kernel is the use of [GroupKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html). It prevents us from overfitting, and eventually bump up the score."},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import abc\nimport codecs\nimport inspect\nimport json\nimport logging\nimport gc\nimport pickle\nimport sys\nimport time\nimport warnings\n\nimport catboost as cat\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport yaml\n\nfrom abc import abstractmethod\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom numba import jit\nfrom typing import List, Optional, Union, Tuple, Dict\nfrom collections import Counter\nfrom functools import partial\nimport scipy as sp\n\nfrom sklearn.model_selection import train_test_split, GroupKFold\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\n\nfrom sklearn.utils import shuffle\nfrom numpy.random import RandomState\nimport sys\nimport pdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config\n\nIt seems a bit strange but I give configuration for this model with 'yaml like' string, since I usually work on data pipeline which takes yaml config file as input. I got this data pipeline idea from the repository [pudae/kaggle-hpa](https://github.com/pudae/kaggle-hpa)."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_string = '''\ndataset:\n  dir: \"../input/data-science-bowl-2019/\"\n  feature_dir: \"features\"\n  params:\n\nfeatures:\n  - Basic\n\nav:\n  split_params:\n    test_size: 0.33\n    random_state: 42\n\n  model_params:\n    objective: \"binary\"\n    metric: \"auc\"\n    boosting: \"gbdt\"\n    max_depth: 7\n    num_leaves: 75\n    learning_rate: 0.01\n    colsample_bytree: 0.7\n    subsample: 0.1\n    subsample_freq: 1\n    seed: 111\n    feature_fraction_seed: 111\n    drop_seed: 111\n    verbose: -1\n    first_metric_only: True\n\n  train_params:\n    num_boost_round: 1000\n    early_stopping_rounds: 100\n    verbose_eval: 100\n\ncat_model:\n  name: \"catboost\"\n  model_params:\n    loss_function: \"RMSE\"\n    task_type: \"CPU\"\n    iterations: 6000\n    colsample_bylevel: 0.5\n    early_stopping_rounds: 400\n    l2_leaf_reg: 18\n    random_seed: 2019\n    use_best_model: True\n    \nlgbm_model:\n  name: \"lgbm\"\n  model_params:\n    n_estimators: 5000\n    boosting_type: 'gbdt'\n    metric: 'rmse'\n    subsample: 0.75\n    subsample_freq: 1\n    learning_rate: 0.01\n    feature_fraction: 0.9\n    max_depth: 15\n    lambda_l1: 1\n    lambda_l2: 1\n    early_stopping_rounds: 100\n    \nxgb_model:\n   name: \"xgb\"\n   model_params:\n    colsample_bytree: 0.8\n    learning_rate: 0.01\n    max_depth: 10\n    subsample: 1\n    objective: 'reg:squarederror'\n    min_child_weight: 3\n    gamma: 0.25\n    num_boost_round: 5000\n    early_stopping_rounds: 100\n\nnn_model:\n    name: \"nn\"\n    \ntrain_params:\n    mode: \"regression\"\n\nval:\n  name: \"group_kfold\"\n  params:\n    n_splits: 5\n\noutput_dir: \"output\"\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = dict(yaml.load(conf_string, Loader=yaml.SafeLoader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions and Classes"},{"metadata":{},"cell_type":"markdown","source":"### utils"},{"metadata":{},"cell_type":"markdown","source":"#### checker"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def feature_existence_checker(feature_path: Path,\n                              feature_names: List[str]) -> bool:\n    features = [f.name for f in feature_path.glob(\"*.ftr\")]\n    for f in feature_names:\n        if f + \"_train.ftr\" not in features:\n            return False\n        if f + \"_test.ftr\" not in features:\n            return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### jsonutil"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n\ndef save_json(config: dict, save_path: Union[str, Path]):\n    f = codecs.open(str(save_path), mode=\"w\", encoding=\"utf-8\")\n    json.dump(config, f, indent=4, cls=MyEncoder, ensure_ascii=False)\n    f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### logger"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def configure_logger(config_name: str, log_dir: Union[Path, str], debug: bool):\n    if isinstance(log_dir, str):\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n    else:\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n    log_filename = config_name.split(\"/\")[-1].replace(\".yml\", \".log\")\n    log_filepath = log_dir / log_filename \\\n        if isinstance(log_dir, Path) else Path(log_dir) / log_filename\n\n    # delete the old log\n    if log_filepath.exists():\n        with open(log_filepath, mode=\"w\"):\n            pass\n\n    level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        filename=str(log_filepath),\n        level=level,\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n        datefmt=\"%m/%d/%Y %I:%M:%S %p\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### timer"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@contextmanager\ndef timer(name: str, log: bool = False):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation\n\nThis kernel uses GroupKFold as validation strategy.  \nIn this kernel, I grouped up the training sample with `installation_id` so that samples with certain `installation_id` do not exist in both train and val set in the same fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def RandGroupKfold(groups, n_splits, random_state=None, shuffle_groups=False):\n\n    ix = np.array(range(len(groups)))\n    unique_groups = np.unique(groups)\n    if shuffle_groups:\n        prng = RandomState(random_state)\n        prng.shuffle(unique_groups)\n    splits = np.array_split(unique_groups, n_splits)\n    train_test_indices = []\n\n    for split in splits:\n        mask = [el in split for el in groups]\n        train = ix[np.invert(mask)]\n        test = ix[mask]\n        train_test_indices.append((train, test))\n    return train_test_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_kfold(df: pd.DataFrame, groups: pd.Series,random_state,\n                config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    params = config[\"val\"][\"params\"]\n    #kf = GroupKFold(n_splits=params[\"n_splits\"])\n    splits = RandGroupKfold(groups, n_splits=params[\"n_splits\"], random_state=random_state, shuffle_groups=True)\n    #split = list(kf.split(df, groups=groups))\n    return list(splits)\n\n\ndef get_validation(df: pd.DataFrame,random_state,\n                   config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    name: str = config[\"val\"][\"name\"]\n\n    func = globals().get(name)\n    if func is None:\n        raise NotImplementedError\n\n    if \"group\" in name:\n        cols = df.columns.tolist()\n        cols.remove(\"group\")\n        groups = df[\"group\"]\n        return func(df[cols], groups,random_state, config)\n    else:\n        return func(df, config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### evaluation\n\nCode from [Ultra Fast QWK Calc Method](https://www.kaggle.com/cpmpml/ultra-fast-qwk-calc-method)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n\n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [1.10, 1.72, 2.25]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead', options={\n            'maxiter': 5000})\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n\n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit\ndef qwk(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        max_rat: int = 3) -> float:\n    y_true_ = np.asarray(y_true, dtype=int)\n    y_pred_ = np.asarray(y_pred, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    numerator = 0\n    for k in range(y_true_.shape[0]):\n        i, j = y_true_[k], y_pred_[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        numerator += (i - j) * (i - j)\n\n    denominator = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            denominator += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    denominator /= y_true_.shape[0]\n    return 1 - numerator / denominator\n\n\ndef calc_metric(y_true: Union[np.ndarray, list],\n                y_pred: Union[np.ndarray, list]) -> float:\n    return qwk(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### models"},{"metadata":{},"cell_type":"markdown","source":"#### base\n\n\nCode taken from [hakubishin/kaggle_ieee](https://github.com/hakubishin3/kaggle_ieee)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(y_train)\n    for k in dist:\n        dist[k] /= len(y_train)\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n\ndef cohenkappa(ypred, y):\n    y = y.get_label().astype(\"int\")\n    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n    return \"cappa\", loss, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier, CatBoostRegressor\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom lightgbm import LGBMClassifier, LGBMRegressor\n\n# type alias\nAoD = Union[np.ndarray, pd.DataFrame]\nAoS = Union[np.ndarray, pd.Series]\nCatModel = Union[cat.CatBoostClassifier, cat.CatBoostRegressor]\nLGBModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\nXGBModel = Union[xgb.XGBClassifier, xgb.XGBRegressor]\nModel = Union[CatModel, LGBModel, XGBModel]\n\n\nclass BaseModel(object):\n    @abstractmethod\n    def fit(self, x_train: AoD, y_train: AoS, x_valid: AoD, y_valid: AoS,\n            config: dict) -> Tuple[Model, dict]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_best_iteration(self, model: Model) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model: Model, features: AoD) -> np.ndarray:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self, model: Model) -> np.ndarray:\n        raise NotImplementedError\n\n    def cv(self,\n           y_train: AoS,\n           train_features: AoD,\n           test_features: AoD,\n           feature_name: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]],\n           config: dict,\n           log: bool = True\n           ) -> Tuple[List[Model], np.ndarray, np.ndarray, np.ndarray, pd.DataFrame, dict]:\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        oof_true = np.zeros(len(train_features))\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0.0\n        cv_score_list: List[dict] = []\n        models: List[Model] = []\n        X = train_features.values if isinstance(train_features, pd.DataFrame) \\\n            else train_features\n        y = y_train.values if isinstance(y_train, pd.Series) \\\n            else y_train\n        \n        #test_features = self.convert_x(test_features.values)\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            # get train data and valid data\n            x_trn = X[trn_idx]\n            y_trn = y[trn_idx]\n            x_val = X[val_idx]\n            y_val = y[val_idx]\n\n            # train model\n            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n\n            # predict oof and test\n            \n            ## Here something strange\n            x_val = self.convert_x(x_val)\n            oof_preds[val_idx] = self.predict(model, x_val).reshape(-1)\n            oof_true[val_idx] = y_val\n            \n            _, oof_score, _ = eval_qwk_lgb_regr(y_val, oof_preds[val_idx])\n            print(f\"fold : {i_fold}| oof score: {oof_score:.5f}\")\n            \n            test_preds += self.predict(\n                model, test_features).reshape(-1) / len(folds_ids)\n            \n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f\"gain_{i_fold+1}\"],\n                index=feature_name)\n            importances = importances.join(importances_tmp, how=\"inner\")\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        # print oof score\n        _, oof_score, _ = eval_qwk_lgb_regr(oof_true, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n\n        if log:\n            logging.info(f\"oof score: {oof_score:.5f}\")\n\n        evals_results = {\n            \"evals_result\": {\n                \"oof_score\":\n                oof_score,\n                \"cv_score\": {\n                    f\"cv{i + 1}\": cv_score\n                    for i, cv_score in enumerate(cv_score_list)\n                },\n                \"n_data\":\n                len(train_features),\n                \"best_iteration\":\n                best_iteration,\n                \"n_features\":\n                len(train_features.columns),\n                \"feature_importance\":\n                feature_importance.sort_values(ascending=False).to_dict()\n            }\n        }\n\n        return models, oof_preds,oof_true, test_preds, feature_importance, evals_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NN(BaseModel):\n    \n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n                x_valid: np.ndarray, y_valid:np.ndarray, config: dict) -> Tuple[Model,dict]:\n        train_set = {'X': x_train, 'y': y_train}\n        valid_set = {'X': x_valid, 'y': y_valid}\n         \n        scaler = MinMaxScaler()\n        train_set['X'] = scaler.fit_transform(train_set['X'])\n        valid_set['X'] = scaler.fit_transform(valid_set['X'])\n        \n        print(x_train.shape)\n        verbosity = 100\n        size = train_set['X'].shape[1]\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Input(shape=(size,)),\n            tf.keras.layers.Dense(100, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.4),\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.4),\n            tf.keras.layers.Dense(25, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.4),\n            tf.keras.layers.Dense(1, activation='relu')\n        ])\n            \n        model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8',\n                                                        save_weights_only=True, save_best_only=True,\n                                                        verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n            \n        model.fit(train_set['X'],train_set['y'],\n                    validation_data = (valid_set['X'],valid_set['y']),\n                    epochs = 100,\n                    callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n            \n        return model, 0\n    def convert_x(self,x):\n        scaler = MinMaxScaler()\n        return scaler.fit_transform(x)\n    \n    def predict(self,model\n                ,features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        return model.predict(np.array(features,dtype=float))\n        \n    def get_best_iteration(self, model):\n        return 0\n        \n    def get_feature_importance(self, model):\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cat"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"CatModel = Union[CatBoostClassifier, CatBoostRegressor]\n\nclass CatBoost(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            config: dict) -> Tuple[CatModel, dict]:\n        model_params = config[\"cat_model\"][\"model_params\"]\n        mode = config[\"train_params\"][\"mode\"]\n        if mode == \"regression\":\n            model = CatBoostRegressor(**model_params)\n        else:\n            model = CatBoostClassifier(**model_params)\n        \n        train_set = {'X': x_train, 'y': y_train}\n        valid_set = {'X': x_valid, 'y': y_valid}\n        \n        model.fit(\n            train_set['X'],\n            train_set['y'],\n            eval_set=(valid_set['X'], valid_set['y']),\n            use_best_model=True,\n            verbose=model_params[\"early_stopping_rounds\"])\n        \n        best_score = model.best_score_\n        return model, best_score\n    \n    def convert_x(self,x):\n        return x\n    \n    def get_best_iteration(self, model: CatModel):\n        return model.best_iteration_\n\n    def predict(self, model: CatModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        return model.predict(features)\n\n    def get_feature_importance(self, model: CatModel) -> np.ndarray:\n        return model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"LgbmModel = Union[LGBMClassifier, LGBMRegressor]\nclass Lgbm(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n           x_valid: np.ndarray, y_valid: np.ndarray,\n           config: dict) -> Tuple[LgbmModel,dict]:\n        \n        model_params = config[\"lgbm_model\"][\"model_params\"]\n            \n        train_set = lgb.Dataset(x_train, y_train)\n        valid_set = lgb.Dataset(x_valid, y_valid)\n        \n        model = lgb.train(\n            model_params,\n            train_set,\n            valid_sets=[train_set, valid_set],\n            valid_names=[\"train\", \"valid\"],\n            verbose_eval=model_params[\"early_stopping_rounds\"])\n        \n        best_score = model.best_score\n        \n        return model, best_score\n    \n    def convert_x(self,x):\n        return x\n    \n    def get_best_iteration(self, model: LgbmModel):\n        return model.best_iteration\n    \n    def predict(self, model:LgbmModel,\n               features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        return model.predict(features)\n    \n    def get_feature_importance(self,model: LgbmModel) -> np.ndarray:\n        return model.feature_importance(importance_type=\"gain\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"xbg"},{"metadata":{"trusted":true},"cell_type":"code","source":"XgbModel = Union[XGBRegressor,XGBClassifier]\nclass Xgb(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n           x_valid: np.ndarray, y_valid: np.ndarray,\n           config: dict) -> Tuple[XgbModel,dict]:\n        model_params = config[\"xgb_model\"][\"model_params\"]\n        import xgboost as xgb\n        train_set = xgb.DMatrix(x_train, y_train)\n        valid_set = xgb.DMatrix(x_valid, y_valid)\n        \n        model = xgb.train(model_params, train_set,\n                  evals=[(train_set,'train'),(valid_set,'valid')],\n                  verbose_eval = model_params[\"early_stopping_rounds\"]\n                 )\n        \n        best_score = 0\n        \n        return model, best_score\n    \n    def convert_x(self,x):\n        import xgboost as xgb\n        return xgb.DMatrix(x)\n    def get_best_iteration(self, model: LgbmModel):\n        return model.best_iteration\n    \n    def predict(self, model:XgbModel,\n               features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        return model.predict(features)\n    \n    def get_feature_importance(self,model: LgbmModel) -> np.ndarray:\n        return model.get_fscore()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NN"},{"metadata":{},"cell_type":"markdown","source":"#### factory"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def catboost() -> CatBoost:\n    return CatBoost()\n\ndef lgbm() -> Lgbm:\n    return Lgbm()\n\ndef xgb() -> Xgb:\n    return Xgb()\n\ndef nn() -> NN:\n    return NN()\n\ndef get_model(config: dict, model:str):\n    model_name = config[model][\"name\"]\n    func = globals().get(model_name)\n    if func is None:\n        raise NotImplementedError\n    return func()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### features"},{"metadata":{},"cell_type":"markdown","source":"#### base"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Feature(metaclass=abc.ABCMeta):\n    prefix = \"\"\n    suffix = \"\"\n    save_dir = \"features\"\n    is_feature = True\n\n    def __init__(self):\n        self.name = self.__class__.__name__\n        self.train = pd.DataFrame()\n        self.test = pd.DataFrame()\n        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n        self.train_path = Path(self.save_dir) / f\"{self.name}_train.ftr\"\n        self.test_path = Path(self.save_dir) / f\"{self.name}_test.ftr\"\n\n    def run(self,\n            train_df: pd.DataFrame,\n            test_df: Optional[pd.DataFrame] = None,\n            log: bool = False):\n        with timer(self.name, log=log):\n            self.create_features(train_df, test_df)\n            prefix = self.prefix + \"_\" if self.prefix else \"\"\n            suffix = self.suffix + \"_\" if self.suffix else \"\"\n            self.train.columns = [str(c) for c in self.train.columns]\n            self.test.columns = [str(c) for c in self.test.columns]\n            self.train.columns = prefix + self.train.columns + suffix\n            self.test.columns = prefix + self.test.columns + suffix\n        return self\n\n    @abc.abstractmethod\n    def create_features(self, train_df: pd.DataFrame,\n                        test_df: Optional[pd.DataFrame]):\n        raise NotImplementedError\n\n    def save(self):\n        self.train.to_feather(str(self.train_path))\n        self.test.to_feather(str(self.test_path))\n\n\nclass PartialFeature(metaclass=abc.ABCMeta):\n    def __init__(self):\n        self.df = pd.DataFrame\n\n    @abc.abstractmethod\n    def create_features(self, df: pd.DataFrame, test: bool = False):\n        raise NotImplementedError\n\n\ndef is_feature(klass) -> bool:\n    return \"is_feature\" in set(dir(klass))\n\n\ndef get_features(namespace: dict):\n    for v in namespace.values():\n        if inspect.isclass(v) and is_feature(v) and not inspect.isabstract(v):\n            yield v()\n\n\ndef generate_features(train_df: pd.DataFrame,\n                      test_df: pd.DataFrame,\n                      namespace: dict,\n                      overwrite: bool,\n                      log: bool = False):\n    for f in get_features(namespace):\n        if f.train_path.exists() and f.test_path.exists() and not overwrite:\n            if not log:\n                print(f.name, \"was skipped\")\n            else:\n                logging.info(f\"{f.name} was skipped\")\n        else:\n            f.run(train_df, test_df, log).save()\n\n\ndef load_features(config: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    feather_path = config[\"dataset\"][\"feature_dir\"]\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_train.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_train.ftr\").exists()\n    ]\n    x_train = pd.concat(dfs, axis=1)\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_test.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_test.ftr\").exists()\n    ]\n    x_test = pd.concat(dfs, axis=1)\n    return x_train, x_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### basic\n\n\nThe features used in this kernel is all the same as those used in [Catboost - Some more features](https://www.kaggle.com/braquino/catboost-some-more-features)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"IoF = Union[int, float]\nIoS = Union[int, str]\n\n\nclass Basic(Feature):\n    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n        \n        #Title Event\n        train_df['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train_df['title'], train_df['event_code']))\n        test_df['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test_df['title'], test_df['event_code']))\n        all_title_event_codes = set(train_df['title_event_code'].unique()).union(\n            set(test_df['title_event_code'].unique()))\n        \n        title_event_code_map = dict(zip(all_title_event_codes,np.arange(len(all_title_event_codes))))\n        \n        #Type World\n        train_df['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), train_df['type'], train_df['world']))\n        test_df['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), test_df['type'], test_df['world']))\n        all_type_worlds = set(train_df['type_world'].unique()).union(\n            set(test_df['type_world'].unique()))\n        type_world_map = dict(zip(all_type_worlds,np.arange(len(all_type_worlds))))\n        \n        #Assess Title\n        all_assess_titles = set(train_df[train_df['type']=='Assessment']['title'].unique()).union(\n            set(test_df[test_df['type']=='Assessment']['title'].unique()))\n        assess_title_map = dict(zip(all_assess_titles,np.arange(len(all_assess_titles))))\n        \n        #Activities\n        all_activities = set(train_df[\"title\"].unique()).union(set(test_df[\"title\"].unique()))\n        activities_map = dict(zip(all_activities, np.arange(len(all_activities))))\n        inverse_activities_map = dict(zip(np.arange(len(all_activities)), all_activities))\n        \n        train_df[\"title\"] = train_df[\"title\"].map(activities_map)\n        test_df[\"title\"] = test_df[\"title\"].map(activities_map)\n\n        \n        #Worlds\n        all_worlds = set(train_df['world'].unique()).union(\n        set(test_df['world'].unique()))\n        worlds_map = dict(zip(np.arange(len(all_worlds)),all_worlds))\n        \n        #Event Codes\n        all_event_codes = set(train_df[\"event_code\"].unique()).union(\n            test_df[\"event_code\"].unique())\n        \n        event_codes_map = dict(zip(all_event_codes,np.arange(len(all_event_codes))))\n        \n        #Event Ids\n        all_event_ids = set(train_df['event_id'].unique()).union(\n            set(test_df['event_id'].unique()))\n        \n        event_ids_map = dict(zip(all_event_ids,np.arange(len(all_event_ids))))\n        \n        \n        compiled_data_train: List[List[IoF]] = []\n        compiled_data_test: List[List[IoF]] = []\n\n        installation_ids_train = []\n        installation_ids_test = []\n\n\n        train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n        test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\n        \n        feats = KernelFeatures(all_activities, all_event_codes, all_worlds,\n                                   all_event_ids,all_title_event_codes,all_type_worlds,\n                                   all_assess_titles,\n                                   activities_map, inverse_activities_map, worlds_map,\n                                   event_codes_map,\n                                   event_ids_map,\n                                   title_event_code_map,\n                                   type_world_map,\n                                   assess_title_map)\n        \n        for ins_id, user_sample in tqdm_notebook(\n                train_df.groupby(\"installation_id\", sort=False),\n                total=train_df[\"installation_id\"].nunique(),\n                desc=\"train features\"):\n            '''\n            if \"Assessment\" not in user_sample[\"type\"].unique():\n                continue\n            '''\n            feat_df = feats.create_features(user_sample, test=False)\n            installation_ids_train.extend([ins_id] * len(feat_df))\n            compiled_data_train.append(feat_df)\n        self.train = pd.concat(compiled_data_train, axis=0, sort=False)\n        self.train[\"installation_id\"] = installation_ids_train\n        self.train.reset_index(drop=True, inplace=True)\n\n        for ins_id, user_sample in tqdm_notebook(\n                test_df.groupby(\"installation_id\", sort=False),\n                total=test_df[\"installation_id\"].nunique(),\n                desc=\"test features\"):\n            \n            feat_df = feats.create_features(user_sample, test=True)\n            installation_ids_test.extend([ins_id] * len(feat_df))\n            compiled_data_test.append(feat_df)\n        self.test = pd.concat(compiled_data_test, axis=0, sort=False)\n        self.test[\"installation_id\"] = installation_ids_test\n        self.test.reset_index(drop=True, inplace=True)\n\n\nclass KernelFeatures(PartialFeature):\n    def __init__(self, all_activities: set, all_event_codes: set,all_worlds: set,\n                all_event_ids: set,all_title_event_codes: set, all_type_worlds:set,\n                all_assess_titles: set,\n                activities_map: Dict[str, float],\n                inverse_activities_map: Dict[float, str],\n                event_codes_map: Dict[str, float],\n                worlds_map: Dict[str, float],\n                event_ids_map: Dict[str, float],\n                title_event_code_map: Dict[str,float],\n                type_world_map: Dict[str,float],\n                assess_title_map: Dict[str,float]):\n        \n        self.all_activities = all_activities\n        self.all_event_codes = all_event_codes\n        self.all_worlds = all_worlds\n        self.all_event_ids = all_event_ids\n        self.all_title_event_codes = all_title_event_codes\n        self.all_type_worlds = all_type_worlds\n        self.all_assess_titles = all_assess_titles\n        \n        self.activities_map = activities_map\n        self.inverse_activities_map = inverse_activities_map\n        self.worlds_map = worlds_map\n        self.event_codes_map = event_codes_map\n        self.event_ids_map = event_ids_map\n        self.title_event_code_map = title_event_code_map\n        self.type_world_map = type_world_map\n        self.assess_title_map = assess_title_map\n\n        win_code = dict(\n            zip(activities_map.values(),\n                (4100 * np.ones(len(activities_map))).astype(int)))\n        win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n        self.win_code = win_code\n\n        super().__init__()\n        \n    def cnt_miss(self,df):\n        cnt = 0\n        for e in range(len(df)):\n            x = df['event_data'].iloc[e]\n            y = json.loads(x)['misses']\n            cnt += y\n        return cnt\n    \n    def get_4020_acc(self,df, counter_dict):\n        for e in ['Cauldron Filler (Assessment)', 'Bird Measurer (Assessment)',\n                  'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)']:\n            Assess_4020 = df[(df.event_code == 4020) & (df.title == self.activities_map[e])]\n            true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n            false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n\n            measure_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n                                                                                                      true_attempts_ + false_attempts_) != 0 else 0\n            counter_dict[e + \"_4020_accuracy\"] += (counter_dict[e + \"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n\n        return counter_dict\n\n    def create_features(self, df: pd.DataFrame, test: bool = False):\n        \n        time_spent_each_act: Dict[str, int] = {act: 0 for act in self.all_activities}\n        event_code_count: Dict[int, int] = {ev: 0 for ev in self.all_event_codes}\n        world_count: Dict[str, int] = {wrd: 0 for wrd in self.all_worlds}\n        title_count: Dict[str, int] = {tit: 0 for tit in self.activities_map.keys()}\n        event_id_count: Dict[str, int] = {ids: 0 for ids in self.all_event_ids}\n        title_event_count: Dict[str, int] = {te: 0 for te in self.all_title_event_codes}\n        type_world_count: Dict[str, int] = {tw: 0 for tw in self.all_type_worlds}\n        assess_title_count: Dict[str, int] = {at: 0 for at in self.all_assess_titles}\n        last_acc_title_count: Dict[str,int] = {'acc_' + at: -1 for at in self.all_assess_titles}\n            \n        user_activities_count: Dict[IoS, IoF] = {\n            \"Clip\": 0,\n            \"Activity\": 0,\n            \"Assessment\": 0,\n            \"Game\": 0\n        }\n        assess_4020_acc_dict: Dict[IoS, IoF] = {\n            'Cauldron Filler (Assessment)_4020_accuracy': 0,\n                            'Mushroom Sorter (Assessment)_4020_accuracy': 0,\n                            'Bird Measurer (Assessment)_4020_accuracy': 0,\n                            'Chest Sorter (Assessment)_4020_accuracy': 0}\n\n        all_assesments = []\n\n        accumulated_acc_groups = 0\n        accumulated_acc = 0\n        accumulated_correct_attempts = 0\n        accumulated_failed_attempts = 0\n        accumulated_actions = 0\n        \n        #New\n        accumulated_game_miss = 0\n        Cauldron_Filler_4025 = 0\n        mean_game_round = 0\n        mean_game_duration = 0\n        mean_game_level = 0\n        Assessment_mean_event_count = 0\n        Game_mean_event_count = 0\n        Activity_mean_event_count = 0\n        chest_assessment_uncorrect_sum = 0\n\n        counter = 0\n\n        accuracy_group: Dict[int, int] = {0: 0, 1: 0, 2: 0, 3: 0}\n\n        durations: List[float] = []\n        durations_game: List[float] = []\n        durations_activity: List[float] = []\n        last_activity = \"\"\n\n        for i, sess in df.groupby(\"game_session\", sort=False):\n            sess_type = sess[\"type\"].iloc[0]\n            sess_title = sess[\"title\"].iloc[0]\n            sess_title_text = self.inverse_activities_map[sess_title]\n            \n            if sess_type == \"Actiity\":\n                Activity_mean_event_count = (Activity_mean_event_count + sess['event_count'].iloc[-1])/2.0\n            if sess_type == \"Game\":\n                Game_mean_event_count = (Game_mean_event_count + sess['event_count'].iloc[-1]) / 2.0\n                game_s = sess[sess.event_code == 2030]\n                misses_cnt = self.cnt_miss(game_s)\n                accumulated_game_miss += misses_cnt\n                \n                try:\n                    game_round = json.loads(sess['event_data'].iloc[-1])['round']\n                    mean_game_round = (mean_game_round + game_round)/2.0\n                except:\n                    pass\n                try:\n                    game_duration = json.loads(sess['event_data'].iloc[-1])['duration']\n                    mean_game_duration = (mean_game_duration + game_duration)/2.0\n                except:\n                    pass\n                try:\n                    game_level = json.loads(sess['event_data'].iloc[-1])['level']\n                    mean_game_level = (mean_game_level + game_level) /2.0\n                except:\n                    pass\n                \n            \n            if sess_type == 'Game':\n                durations_game.append((sess.iloc[-1,2] - sess.iloc[0,2]).seconds)\n            if sess_type == 'Activity':\n                durations_activity.append((sess.iloc[-1,2] - sess.iloc[0,2]).seconds)\n\n            if sess_type != \"Assessment\":\n                time_spent = int(sess[\"game_time\"].iloc[-1] / 1000)\n                time_spent_each_act[\n                    self.inverse_activities_map[sess_title]] += time_spent\n                \n            if sess_type == \"Assessment\" and (test or len(sess) > 1):\n\n                all_attempts: pd.DataFrame = sess.query(\n                    f\"event_code == {self.win_code[sess_title]}\")\n                true_attempt = all_attempts[\"event_data\"].str.contains(\n                    \"true\").sum()\n                false_attempt = all_attempts[\"event_data\"].str.contains(\n                    \"false\").sum()\n\n                features = user_activities_count.copy()\n                features.update(time_spent_each_act.copy())\n                features.update(event_code_count.copy())\n                features.update(world_count.copy())\n                features.update(title_count.copy())\n                features.update(title_event_count.copy())\n                features.update(type_world_count.copy())\n                features.update(event_id_count.copy())\n                features.update(last_acc_title_count.copy())\n                features.update(assess_4020_acc_dict.copy())\n                \n                features['session_id'] = i\n                features['accumulated_game_miss'] = accumulated_game_miss\n                features['mean_game_round'] = mean_game_round\n                features['mean_game_duration'] = mean_game_duration\n                features['mean_game_level'] = mean_game_level\n                features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n                features['accumulated_game_miss'] = accumulated_game_miss\n                features['Assessment_mean_event_count'] = Assessment_mean_event_count\n                features['Activity_mean_event_count'] = Activity_mean_event_count\n                features['Game_mean_event_count'] = Game_mean_event_count\n                '''\n                variety_features = [('var_event_code', event_code_count),\n                                ('var_event_id', event_id_count),\n                                ('var_title', title_count),\n                                ('var_title_event_code', title_event_count),\n                                ('var_type_world', type_world_count)]\n                \n                for name, dict_counts in variety_features:\n                    arr = np.array(list(dict_counts.values()))\n                    features[name] = np.count_nonzero(arr)\n                '''\n                \n                \n                features[\"session_title\"] = sess_title\n\n                features[\"accumulated_correct_attempts\"] = \\\n                    accumulated_correct_attempts\n                features[\"accumulated_failed_attempts\"] = \\\n                    accumulated_failed_attempts\n\n                accumulated_correct_attempts += true_attempt\n                accumulated_failed_attempts += false_attempt\n                \n    \n                if durations == []:\n                    features[\"duration_mean\"] = 0\n                    #features[\"duration_medain\"] = 0\n                    features[\"duration_max\"] = 0\n                    features[\"duration_min\"] = 0\n                    #features[\"duration_std\"] = 0\n                    features['last_duration'] = 0\n                    #features['first_duration'] = 0\n                    \n                else:\n                    features[\"duration_mean\"] = np.mean(durations)\n                    #features[\"duration_medain\"] = np.median(durations)\n                    features[\"duration_max\"] = np.max(durations)\n                    features[\"duration_min\"] = np.min(durations)\n                    #features[\"duration_std\"] = np.std(durations)\n                    features['last_duration'] = durations[-1]\n                    #features['first_duration'] = durations[0]\n                    \n                durations.append((sess.iloc[-1, 2] - sess.iloc[0, 2]).seconds)\n                \n                if durations_game == []:\n                    \n                    features[\"duration_game_mean\"] = 0\n                    #features[\"duration_game_medain\"] = 0\n                    features[\"duration_game_max\"] = 0\n                    features[\"duration_game_min\"] = 0\n                    #features[\"duration_game_std\"] = 0\n                    features['last_game_duration'] = 0\n                    #features['first_game_duration'] = 0\n                else:\n                    features[\"duration_game_mean\"] = np.mean(durations_game)\n                    #features[\"duration_game_medain\"] = np.median(durations_game)\n                    features[\"duration_game_max\"] = np.max(durations_game)\n                    features[\"duration_game_min\"] = np.min(durations_game)\n                    #features[\"duration_game_std\"] = np.std(durations_game)\n                    features['last_game_duration'] = durations_game[-1]\n                    #features['first_game_duration'] = durations_game[0]\n                    \n                if durations_activity == []:\n                    features[\"duration_activity_mean\"] = 0\n                    #features[\"duration_activity_medain\"] = 0\n                    #features[\"duration_activity_max\"] = 0\n                    features[\"duration_activity_min\"] = 0\n                    features[\"duration_activity_std\"] = 0\n                    features['last_activity_duration'] = 0\n                    #features['first_activity_duration'] = 0\n                    \n                else:\n                    features[\"duration_activity_mean\"] = np.mean(durations_activity)\n                    #features[\"duration_activity_medain\"] = np.median(durations_activity)\n                    features[\"duration_activity_max\"] = np.max(durations_activity)\n                    features[\"duration_activity_min\"] = np.min(durations_activity)\n                    #features[\"duration_activity_std\"] = np.std(durations_activity)\n                    features['last_activity_duration'] = durations_activity[-1]\n                    #features['first_activity_duration'] = durations_activity[0]\n\n                features[\"accumulated_acc\"] = \\\n                    accumulated_acc / counter if counter > 0 else 0\n                \n                acc = true_attempt / (true_attempt + false_attempt) \\\n                    if (true_attempt + false_attempt) != 0 else 0\n                accumulated_acc += acc\n                \n                last_acc_title_count['acc_' + sess_title_text] = acc\n                \n                #features['Couldron_Filler_4025'] = \\\n                    #Cauldron_Filler_4025 / counter if counter > 0 else 0\n                \n                #Assess_4025 = sess[(sess.event_code == 4025) & (sess.title == 'Cauldron Filler (Assessment)')]\n                #true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n                #false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n                \n                #cau_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n                                                                                            #true_attempts_ + false_attempts_) != 0 else 0\n                #Cauldron_Filler_4025 += cau_assess_accuracy_\n                \n                #chest_assessment_uncorrect_sum += len(sess[sess.event_id == \"df4fe8b6\"])\n                \n                #Assessment_mean_event_count = (Assessment_mean_event_count + sess['event_count'].iloc[-1]) / 2.0\n                \n\n                if acc == 0:\n                    features[\"accuracy_group\"] = 0\n                elif acc == 1:\n                    features[\"accuracy_group\"] = 3\n                elif acc == 0.5:\n                    features[\"accuracy_group\"] = 2\n                else:\n                    features[\"accuracy_group\"] = 1\n\n                features.update(accuracy_group.copy())\n                accuracy_group[features[\"accuracy_group\"]] += 1\n\n                features[\"accumulated_accuracy_group\"] = \\\n                    accumulated_acc_groups / counter if counter > 0 else 0\n                accumulated_acc_groups += features[\"accuracy_group\"]\n\n                features[\"accumulated_actions\"] = accumulated_actions\n                if test:\n\n                    all_assesments.append(features)\n                elif true_attempt + false_attempt > 0:\n                    all_assesments.append(features)\n\n                counter += 1\n            \n            def update_counters(counter: dict, col: str):\n                num_of_count = Counter(sess[col])\n                for k in num_of_count.keys():\n                    if col == 'title':\n                        counter[self.inverse_activities_map[k]] += num_of_count[k]\n                    else:\n                        counter[k] += num_of_count[k]\n                        \n                return counter\n                        \n            event_code_count = update_counters(event_code_count,'event_code')\n            world_count = update_counters(world_count,'world')\n            title_count = update_counters(title_count,'title')\n            event_ids = update_counters(event_id_count, 'event_id')\n            title_event_count = update_counters(title_event_count,'title_event_code')\n            type_world_count = update_counters(type_world_count, 'type_world')\n            #assess_title_count = update_counters(assess_title_count,'type')\n            \n            #assess_4020_acc_dict = self.get_4020_acc(sess, assess_4020_acc_dict)\n            \n\n            accumulated_actions += len(sess)\n            if last_activity != sess_type:\n                user_activities_count[sess_type] +=1\n                last_activity = sess_type\n\n        if test:\n            self.df = pd.DataFrame([all_assesments[-1]])\n        else:\n            self.df = pd.DataFrame(all_assesments)\n\n        return self.df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## main"},{"metadata":{},"cell_type":"markdown","source":"### Settings"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\ndebug = True\nconfig_path = \"../config/cat_0.yml\"\nlog_dir = \"../log/\"\n\nconfigure_logger(config_path, log_dir, debug)\n\nlogging.info(f\"config: {config_path}\")\nlogging.info(f\"debug: {debug}\")\n\nconfig[\"args\"] = dict()\nconfig[\"args\"][\"config\"] = config_path\n\n# make output dir\noutput_root_dir = Path(config[\"output_dir\"])\nfeature_dir = Path(config[\"dataset\"][\"feature_dir\"])\n\nconfig_name: str = config_path.split(\"/\")[-1].replace(\".yml\", \"\")\noutput_dir = output_root_dir / config_name\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nlogging.info(f\"model output dir: {str(output_dir)}\")\n\nconfig[\"model_output_dir\"] = str(output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data/Feature Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = Path(config[\"dataset\"][\"dir\"])\ntrain = reduce_mem_usage(pd.read_csv(input_dir / \"train.csv\"))\ninst_id = train.loc[train.type=='Assessment'].installation_id.unique()\ntrain = train.loc[train.installation_id.isin(inst_id)]\ntest = reduce_mem_usage(pd.read_csv(input_dir / \"test.csv\"))\ninst_testid = test.loc[test.type=='Assessment'].installation_id.unique()\ntest = test.loc[test.installation_id.isin(inst_testid)]\nspecs = pd.read_csv(input_dir / \"specs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = Path(config[\"dataset\"][\"dir\"])\nif not feature_existence_checker(feature_dir, config[\"features\"]):\n    with timer(name=\"load data\", log=True):\n        \n        generate_features(\n            train, test, namespace=globals(), overwrite=False, log=True)\n\n        del train, test\n        gc.collect()\n\n\nwith timer(\"feature laoding\", log=True):\n    x_train = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_train.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ],\n                        axis=1,\n                        sort=False)\n    x_test = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_test.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ])\n\ncols: List[str] = x_train.columns.tolist()\n\nx_train, x_test = x_train[cols], x_test[cols]\n\ngroups = x_train[\"installation_id\"].values\ny_train = x_train[\"accuracy_group\"].values.reshape(-1)\ncols.remove(\"installation_id\")\ncols.remove(\"accuracy_group\")\ncols.remove('session_id')\n\nassert len(x_train) == len(y_train)\nlogging.debug(f\"number of features: {len(cols)}\")\nlogging.debug(f\"number of train samples: {len(x_train)}\")\nlogging.debug(f\"numbber of test samples: {len(x_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train, test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    msre = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(msre)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n        plt.show()\n    return msre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_correlated_features(reduce_train, features):\n    counter = 0\n    to_remove = []\n    for feat_a in features:\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n                if c > 0.99:\n                    counter += 1\n                    to_remove.append(feat_b)\n                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n    return to_remove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_remove = remove_correlated_features(x_train, cols)\nfeatures = [col for col in x_train.columns if col not in to_remove]\nfeatures = [col for col in features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_exclude = [] \najusted_x_test = x_test.copy()\nfor feature in features:\n    if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n        data = x_train[feature]\n        train_mean = data.mean()\n        data = ajusted_x_test[feature] \n        test_mean = data.mean()\n        try:\n            error = stract_hists(feature,x_train,ajusted_x_test, adjust=True)\n            ajust_factor = train_mean / test_mean\n            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n                to_exclude.append(feature)\n                print(feature, train_mean, test_mean, error)\n            else:\n                ajusted_x_test[feature] *= ajust_factor\n        except:\n            to_exclude.append(feature)\n            print(feature, train_mean, test_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_exclude = list(set(to_exclude).union(['installation_id','accuracy_group','session_id']))\nfeatures = [x for x in features if x not in (to_exclude)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_uncorrelated_features(reduce_train, features):\n    counter = 0\n    to_remove1 = []\n    to_remove2 = []\n    lists= []\n    for feat_a in features:\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in lists and feat_b not in lists:\n                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n                if c <= 0.6:\n                    counter += 1\n                    to_remove1.append(feat_a)\n                    to_remove2.append(feat_b)\n                    lists.append(feat_a)\n                    lists.append(feat_b)\n                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n                    break\n    return to_remove1,to_remove2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_seperate1,to_seperate2 = select_uncorrelated_features(x_train, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_uncorrelated_features(reduce_train, features):\n    counter = 0\n    to_remove1 = []\n    to_remove2 = []\n    lists= []\n    for feat_a in features:\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in lists and feat_b not in lists:\n                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n                if c <= 0.6:\n                    counter += 1\n                    to_remove1.append(feat_a)\n                    to_remove2.append(feat_b)\n                    lists.append(feat_a)\n                    lists.append(feat_b)\n                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n                    break\n    return to_remove1,to_remove2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_seperate11,to_seperate12 = select_uncorrelated_features(x_train, to_seperate1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_seperate21,to_seperate22 = select_uncorrelated_features(x_train, to_seperate2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1 = [feat for feat in features if feat in to_seperate11]\nfeatures2 = [feat for feat in features if feat in to_seperate12]\nfeatures3 = [feat for feat in features if feat in to_seperate21]\nfeatures4 = [feat for feat in features if feat in to_seperate22]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1 = list(set(features1).union(['session_title']))\nfeatures2 = list(set(features2).union(['session_title']))\nfeatures3 = list(set(features3).union(['session_title']))\nfeatures4 = list(set(features4).union(['session_title']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adversarial Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Adversarial Validation\")\ntrain_adv = x_train.copy()\ntest_adv = x_test.copy()\n\ntrain_adv[\"target\"] = 0\ntest_adv[\"target\"] = 1\ntrain_test_adv = pd.concat([train_adv, test_adv], axis=0,\n                           sort=False).reset_index(drop=True)\n\nsplit_params: dict = config[\"av\"][\"split_params\"]\ntrain_set, val_set = train_test_split(\n    train_test_adv,\n    random_state=split_params[\"random_state\"],\n    test_size=split_params[\"test_size\"])\nx_train_adv = train_set[features1]\ny_train_adv = train_set[\"target\"]\nx_val_adv = val_set[features1]\ny_val_adv = val_set[\"target\"]\n\nlogging.debug(f\"The number of train set: {len(x_train_adv)}\")\nlogging.debug(f\"The number of valid set: {len(x_val_adv)}\")\n\ntrain_lgb = lgb.Dataset(x_train_adv, label=y_train_adv)\nvalid_lgb = lgb.Dataset(x_val_adv, label=y_val_adv)\n\nmodel_params = config[\"av\"][\"model_params\"]\ntrain_params = config[\"av\"][\"train_params\"]\nclf = lgb.train(\n    model_params,\n    train_lgb,\n    valid_sets=[train_lgb, valid_lgb],\n    valid_names=[\"train\", \"valid\"],\n    **train_params)\n\n# Check the feature importance\nfeature_imp = pd.DataFrame(\n    sorted(zip(clf.feature_importance(importance_type=\"gain\"), features1)),\n    columns=[\"value\", \"feature\"])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"LightGBM Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_adv.png\")\n\nconfig[\"av_result\"] = dict()\nconfig[\"av_result\"][\"score\"] = clf.best_score\nconfig[\"av_result\"][\"feature_importances\"] = \\\n    feature_imp.set_index(\"feature\").sort_values(\n        by=\"value\",\n        ascending=False\n    ).head(100).to_dict()[\"value\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Adversarial Validation\")\ntrain_adv = x_train.copy()\ntest_adv = x_test.copy()\n\ntrain_adv[\"target\"] = 0\ntest_adv[\"target\"] = 1\ntrain_test_adv = pd.concat([train_adv, test_adv], axis=0,\n                           sort=False).reset_index(drop=True)\n\nsplit_params: dict = config[\"av\"][\"split_params\"]\ntrain_set, val_set = train_test_split(\n    train_test_adv,\n    random_state=split_params[\"random_state\"],\n    test_size=split_params[\"test_size\"])\nx_train_adv = train_set[features2]\ny_train_adv = train_set[\"target\"]\nx_val_adv = val_set[features2]\ny_val_adv = val_set[\"target\"]\n\nlogging.debug(f\"The number of train set: {len(x_train_adv)}\")\nlogging.debug(f\"The number of valid set: {len(x_val_adv)}\")\n\ntrain_lgb = lgb.Dataset(x_train_adv, label=y_train_adv)\nvalid_lgb = lgb.Dataset(x_val_adv, label=y_val_adv)\n\nmodel_params = config[\"av\"][\"model_params\"]\ntrain_params = config[\"av\"][\"train_params\"]\nclf = lgb.train(\n    model_params,\n    train_lgb,\n    valid_sets=[train_lgb, valid_lgb],\n    valid_names=[\"train\", \"valid\"],\n    **train_params)\n\n# Check the feature importance\nfeature_imp = pd.DataFrame(\n    sorted(zip(clf.feature_importance(importance_type=\"gain\"), features2)),\n    columns=[\"value\", \"feature\"])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"LightGBM Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_adv.png\")\n\nconfig[\"av_result\"] = dict()\nconfig[\"av_result\"][\"score\"] = clf.best_score\nconfig[\"av_result\"][\"feature_importances\"] = \\\n    feature_imp.set_index(\"feature\").sort_values(\n        by=\"value\",\n        ascending=False\n    ).head(100).to_dict()[\"value\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Adversarial Validation\")\ntrain_adv = x_train.copy()\ntest_adv = x_test.copy()\n\ntrain_adv[\"target\"] = 0\ntest_adv[\"target\"] = 1\ntrain_test_adv = pd.concat([train_adv, test_adv], axis=0,\n                           sort=False).reset_index(drop=True)\n\nsplit_params: dict = config[\"av\"][\"split_params\"]\ntrain_set, val_set = train_test_split(\n    train_test_adv,\n    random_state=split_params[\"random_state\"],\n    test_size=split_params[\"test_size\"])\nx_train_adv = train_set[features3]\ny_train_adv = train_set[\"target\"]\nx_val_adv = val_set[features3]\ny_val_adv = val_set[\"target\"]\n\nlogging.debug(f\"The number of train set: {len(x_train_adv)}\")\nlogging.debug(f\"The number of valid set: {len(x_val_adv)}\")\n\ntrain_lgb = lgb.Dataset(x_train_adv, label=y_train_adv)\nvalid_lgb = lgb.Dataset(x_val_adv, label=y_val_adv)\n\nmodel_params = config[\"av\"][\"model_params\"]\ntrain_params = config[\"av\"][\"train_params\"]\nclf = lgb.train(\n    model_params,\n    train_lgb,\n    valid_sets=[train_lgb, valid_lgb],\n    valid_names=[\"train\", \"valid\"],\n    **train_params)\n\n# Check the feature importance\nfeature_imp = pd.DataFrame(\n    sorted(zip(clf.feature_importance(importance_type=\"gain\"), features3)),\n    columns=[\"value\", \"feature\"])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"LightGBM Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_adv.png\")\n\nconfig[\"av_result\"] = dict()\nconfig[\"av_result\"][\"score\"] = clf.best_score\nconfig[\"av_result\"][\"feature_importances\"] = \\\n    feature_imp.set_index(\"feature\").sort_values(\n        by=\"value\",\n        ascending=False\n    ).head(100).to_dict()[\"value\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Adversarial Validation\")\ntrain_adv = x_train.copy()\ntest_adv = x_test.copy()\n\ntrain_adv[\"target\"] = 0\ntest_adv[\"target\"] = 1\ntrain_test_adv = pd.concat([train_adv, test_adv], axis=0,\n                           sort=False).reset_index(drop=True)\n\nsplit_params: dict = config[\"av\"][\"split_params\"]\ntrain_set, val_set = train_test_split(\n    train_test_adv,\n    random_state=split_params[\"random_state\"],\n    test_size=split_params[\"test_size\"])\nx_train_adv = train_set[features4]\ny_train_adv = train_set[\"target\"]\nx_val_adv = val_set[features4]\ny_val_adv = val_set[\"target\"]\n\nlogging.debug(f\"The number of train set: {len(x_train_adv)}\")\nlogging.debug(f\"The number of valid set: {len(x_val_adv)}\")\n\ntrain_lgb = lgb.Dataset(x_train_adv, label=y_train_adv)\nvalid_lgb = lgb.Dataset(x_val_adv, label=y_val_adv)\n\nmodel_params = config[\"av\"][\"model_params\"]\ntrain_params = config[\"av\"][\"train_params\"]\nclf = lgb.train(\n    model_params,\n    train_lgb,\n    valid_sets=[train_lgb, valid_lgb],\n    valid_names=[\"train\", \"valid\"],\n    **train_params)\n\n# Check the feature importance\nfeature_imp = pd.DataFrame(\n    sorted(zip(clf.feature_importance(importance_type=\"gain\"), features4)),\n    columns=[\"value\", \"feature\"])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"LightGBM Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_adv.png\")\n\nconfig[\"av_result\"] = dict()\nconfig[\"av_result\"][\"score\"] = clf.best_score\nconfig[\"av_result\"][\"feature_importances\"] = \\\n    feature_imp.set_index(\"feature\").sort_values(\n        by=\"value\",\n        ascending=False\n    ).head(100).to_dict()[\"value\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlogging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"installation_id\"].values\nsplits = get_validation(x_train,7, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'cat_model')\nmodels, oof_preds1,oof_true1, cat_preds1, feature_importance, eval_results = model.cv(\n    y_train, x_train[features1], x_test[features1], features1, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlogging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"installation_id\"].values\nsplits = get_validation(x_train,27, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'cat_model')\nmodels, oof_preds2,oof_true2, cat_preds2, feature_importance, eval_results = model.cv(\n    y_train, x_train[features2], x_test[features2], features2, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlogging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"installation_id\"].values\nsplits = get_validation(x_train,1989, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'cat_model')\nmodels, oof_preds3,oof_true3, cat_preds3, feature_importance, eval_results = model.cv(\n    y_train, x_train[features3], x_test[features3], features3, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlogging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"installation_id\"].values\nsplits = get_validation(x_train,2019, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'cat_model')\nmodels, oof_preds4,oof_true4, cat_preds4, feature_importance, eval_results = model.cv(\n    y_train, x_train[features2], x_test[features2], features2, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train['session_title'] = x_train['session_title'].astype('category')\nx_test['session_title'] = x_test['session_title'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"session_id\"].values\nsplits = get_validation(x_train,4567, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'lgbm_model')\nmodels, oof_preds5,oof_true5, lgbm_preds1, feature_importance, eval_results = model.cv(\n    y_train, x_train[features1], x_test[features1], features1, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"session_id\"].values\nsplits = get_validation(x_train,8262, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'lgbm_model')\nmodels, oof_preds6,oof_true6, lgbm_preds2, feature_importance, eval_results = model.cv(\n    y_train, x_train[features2], x_test[features2], features2, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"session_id\"].values\nsplits = get_validation(x_train,287, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'lgbm_model')\nmodels, oof_preds7,oof_true7, lgbm_preds3, feature_importance, eval_results = model.cv(\n    y_train, x_train[features3], x_test[features3], features3, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = x_train[\"session_id\"].values\nsplits = get_validation(x_train,2530, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'lgbm_model')\nmodels, oof_preds8,oof_true8, lgbm_preds4, feature_importance, eval_results = model.cv(\n    y_train, x_train[features4], x_test[features4], features4, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.corrcoef([cat_preds1,cat_preds2,cat_preds3,cat_preds4,lgbm_preds1,lgbm_preds2,lgbm_preds3,lgbm_preds4])\nnp.corrcoef([lgbm_preds1,lgbm_preds2,lgbm_preds3,lgbm_preds4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.corrcoef([oof_preds1,oof_preds2,oof_preds3,oof_preds4,oof_preds5,oof_preds6,oof_preds7,oof_preds8])\nnp.corrcoef([oof_preds5,oof_preds6,oof_preds7,oof_preds8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.corrcoef([oof_true1,oof_true2,oof_true3,oof_true4,oof_true5,oof_true6,oof_true7,oof_true8])\nnp.corrcoef([oof_true5,oof_true6,oof_true7,oof_true8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlogging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = groups\nsplits = get_validation(x_train, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config,'xgb_model')\nmodels, oof_preds, test_preds, feature_importance, eval_results = model.cv(\n    y_train, x_train, x_test, cols, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_path = output_dir / \"output.json\"\nsave_json(config, save_path)\nnp.save(output_dir / \"oof_preds.npy\", oof_preds)\n\nwith open(output_dir / \"model.pkl\", \"wb\") as m:\n    pickle.dump(models, m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef keras_model(X):\n    model = Sequential([\n            Dense(units=1, input_shape=(X.shape[1],))\n        ])\n    return model\ndef train_keras(X, y, run_lr_finder=False, epochs=5):\n    print(\"train_keras\")\n    # Parameters\n    VAL_SPLIT = 0.3\n\n    def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred-y_true)))\n\n    model = keras_model(X)\n\n    model.compile(optimizer=Adam(lr=0.001), # Default of adam is 0.001. Check large and small values, use a value slighly lower than a diverging lr\n                 loss=root_mean_squared_error)\n    clr = CyclicLR(base_lr=1e-3, \n                   max_lr=1e-1,\n                   step_size=2*int(len(y)/BS), # 2 times the number of iterations\n                   mode='exp_range',\n                   gamma=0.99994\n                  )\n    checkpointer=ModelCheckpoint('best_val.hdf5', monitor='val_loss', verbose=1, save_best_only=True,mode='min', period=1)\n    \n    if run_lr_finder:\n        model.fit(X, y, epochs=1, validation_split=VAL_SPLIT, callbacks=[LRFinder(min_lr=1e-4, max_lr=10)])\n    else:\n        print(\"Fitting Keras Model\")\n        model.fit(X, y, epochs=epochs, validation_split=0.2, callbacks=[checkpointer], verbose=0)\n        model.load_weights('best_val.hdf5')\n        return model\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_lists = ['cat1','cat2','cat3','cat4','lgbm1','lgbm2','lgbm3','lgbm4']\nmodel_lists = ['lgbm1','lgbm2','lgbm3','lgbm4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef ridgecv_predict():\n    PRINT_CORR_HEATMAP=True\n    PRINT_RIDGE_WEIGHTS = True\n    RIDGE_ALPHAS = (0.1, 1.0, 10.0)\n    #X = np.array([oof_preds1,oof_preds2,oof_preds3,oof_preds4,oof_preds5,oof_preds6,oof_preds7,oof_preds8]).T\n    X = np.array([oof_preds5,oof_preds6,oof_preds7,oof_preds8]).T\n    y = oof_true5\n    #if PRINT_CORR_HEATMAP:\n        #sns_plot = sns.heatmap(pd.concat([X, y], axis=1).corr(), annot=True)\n        #sns_plot.savefig(\"corr_w_gt.png\")\n\n    reg = RidgeCV(alphas = RIDGE_ALPHAS, normalize=True).fit(X, y)\n    if PRINT_RIDGE_WEIGHTS:\n        print(\"## Ridge Coefficients\")\n        print(f'Sum of coefficients: {sum(reg.coef_)}')\n        for ww, ss in zip(reg.coef_, model_lists):\n            print(f'{ss} has weight {ww:.2f}')\n    #X = subs.iloc[:, :len(submission_paths)]\n    #X = prepare_X(X)\n    #y_pred = reg.predict(X)\n    #y_pred = y_pred.T[0]\n    #y_pred = np.clip(y_pred, 0, None)\n    #y_pred = np.expm1(y_pred)\n    return reg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\ncoeff = ridgecv_predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_oof_pred =(coeff[0]*oof_preds1+coeff[1]*oof_preds2+coeff[2]*oof_preds3+coeff[3]*oof_preds4+\n                #coeff[4]*oof_preds5+coeff[5]*oof_preds6+coeff[6]*oof_preds7+coeff[7]*oof_preds8)\nfinal_oof_pred =(coeff[0]*oof_preds5+coeff[1]*oof_preds6+coeff[2]*oof_preds7+coeff[3]*oof_preds8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_pred = (coeff[0]*cat_preds1+coeff[1]*cat_preds2+coeff[2]*cat_preds3+coeff[3]*cat_preds4+\n              #coeff[4]*lgbm_preds1+coeff[5]*lgbm_preds2+coeff[6]*lgbm_preds3+coeff[7]*lgbm_preds4)\nfinal_pred = (coeff[0]*lgbm_preds1+coeff[1]*lgbm_preds2+coeff[2]*lgbm_preds3+coeff[3]*lgbm_preds4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.corrcoef([oof_preds1,oof_preds2,oof_preds3,oof_preds4,oof_preds5,oof_preds6,oof_preds7,oof_preds8,final_oof_pred])\nnp.corrcoef([oof_preds5,oof_preds6,oof_preds7,oof_preds8,final_oof_pred])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.corrcoef([cat_preds1,cat_preds2,cat_preds3,cat_preds4,lgbm_preds1,lgbm_preds2,lgbm_preds3,lgbm_preds4,final_pred])\nnp.corrcoef([lgbm_preds1,lgbm_preds2,lgbm_preds3,lgbm_preds4,final_pred])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\noptR = OptimizedRounder()\noptR.fit(final_oof_pred, oof_true5)\ncoefficients = optR.coefficients()\nprint(\"New coefs = \", coefficients)\nopt_preds = optR.predict(final_oof_pred, coefficients)\nprint(\"New train cappa rounding= \", qwk(oof_true5, opt_preds))\n_,train_rounding_origin,_ = eval_qwk_lgb_regr(oof_true5, final_oof_pred)\nprint(\"Train cappa origin \", train_rounding_origin)\nfinal_pred = optR.predict(final_pred, coefficients)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = Counter(y_train)\nfor k in dist:\n    dist[k] /= len(y_train)\npd.DataFrame(y_train).hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(final_pred, acum * 100)\nprint(bound)\n\n\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, final_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(final_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\n    input_dir / \"sample_submission.csv\")\nsample_submission[\"accuracy_group\"] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}