{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport datetime\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# %load_ext line_profiler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is my submission to the Kaggle, data science bowl. All code here is under the open source license referred to in the Kaggle competition rules.\n\n\n## Inspect the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv') #df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv') #df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Training.csv file has {} rows and {} columns'.format(train.shape[0], train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Test.csv file has {} rows and {} columns'.format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\nprint('specs.csv file has {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nprint('train_lablels.csv file has {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n#train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\nprint('sample submission file has {} rows and {} columns'.format(sample.shape[0], sample.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\nlet's see how many installation_ids there are"},{"metadata":{"trusted":false},"cell_type":"code","source":"users = train.installation_id.unique()\nlen(users) # number of distinct users","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 17000 unique installation_id's in the training set. Let's examine the breakdown of event types for an example `installation_id` (from the first row in the dataset)"},{"metadata":{"trusted":false},"cell_type":"code","source":"types = train.loc[train['installation_id']=='0001e90f'].groupby(['type'],as_index=False).size().reset_index(name='counts')\ntypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below i calculate the total time spent on each different types of events in minutes for each installation_id. When I first tried to do this, I called train.loc without narrowing to the user data... a serious mistake from a code design point of view"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''N=10\nuser_times = np.zeros(N)\nfor j in tqdm(range(N)):\n    user = users[j]\n    event_types = [0,0,0,0]\n    sessions_typed = [0,0,0,0]\n    user_dat = train.loc[(train['installation_id']==user)]\n    event_types[0] = user_dat.loc[user_dat['type'] == 'Activity']\n    sessions_typed[0] = event_types[0]['game_session'].unique()\n    event_types[1] = user_dat.loc[user_dat['type'] == 'Clip']\n    sessions_typed[1] = event_types[1]['game_session'].unique()\n    event_types[2] = user_dat.loc[user_dat['type'] == 'Game']\n    sessions_typed[2] = event_types[2]['game_session'].unique()\n    event_types[3]   = user_dat.loc[user_dat['type'] == 'Assessment']\n    sessions_typed[3] = event_types[3]['game_session'].unique()\n    \n    times_per_type = []\n    type_time = 0.0\n    for i in range(4):\n        for session in sessions_typed[i]:\n            sesh_start_ts = event_types[i].loc[event_types[i]['game_session']==session]['timestamp'].min()\n            sesh_start_dt = datetime.datetime.strptime(sesh_start_ts,'%Y-%m-%dT%H:%M:%S.%fZ')\n            sesh_end_ts = event_types[i].loc[event_types[i]['game_session']==session]['timestamp'].max()\n            sesh_end_dt = datetime.datetime.strptime(sesh_end_ts,'%Y-%m-%dT%H:%M:%S.%fZ')\n            #print( session, sesh_start_dt.hour, \" hours \", sesh_start_dt.minute, \" minutes \", sesh_start_dt.second,\" seconds\")\n            #print( session, (sesh_end_dt - sesh_start_dt).total_seconds()/60.0)\n            type_time = type_time + (sesh_end_dt - sesh_start_dt).total_seconds()/60.0\n        times_per_type.append(type_time)\n        type_time = 0.0\n    total_time = sum(times_per_type)\n    user_times[j] = total_time'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## to do list\n\n1. calculate ground-truth\n\n2. feature definitions - wrangling\n\n3. Models and prediction\n\n4. Calculate quadratic weighted kappa (qwk)\n\n5. Submissions, debugging, optimization\n\n6. Post this to your github at some point as an example of your work"},{"metadata":{},"cell_type":"markdown","source":"## Ground truth\n\nNote that the Bird Measurer assessment has two parts however we consider the it to be passed correctly if just the first part is passed, that is the 4110 code. Here, I duplicate the train_labels that were provided by parsing the train data set and calculating the accuracy. "},{"metadata":{"trusted":false},"cell_type":"code","source":"assessments = train.loc[(train['type'] == 'Assessment') & (((train['event_code'] == 4100)&(train['title'] != 'Bird Measurer (Assessment)')) | (train['event_code'] == 4110))].copy()\nassessments['num_correct'] = True\n#assessments\ntest_assessments = test.loc[(test['type'] == 'Assessment') & (((test['event_code'] == 4100)&(test['title'] != 'Bird Measurer (Assessment)')) | (test['event_code'] == 4110))].copy()\ntest_assessments_for_union = test_assessments.copy()\ntest_assessments['num_correct'] = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_assessments.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assessments.loc[assessments['event_data'].str.contains(\":false,\"),'num_correct'] = False\n\ntest_assessments.loc[test_assessments['event_data'].str.contains(\":false,\"),'num_correct'] = False\n\nassessments['num_incorrect'] = np.where(assessments.num_correct > 0,0,1)\n\ntest_assessments['num_incorrect'] = np.where(test_assessments.num_correct > 0,0,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The way I calculate the number of incorrect assessments is using a groupby(). However, we need to know both number incorrect as well as if there was a correct answer in separate columns. I chose two separately create these two columns and then merge them using an inner join, however this is probably not best practice, there must be a way to create two aggregate columns in a single step without creating two dataframes (stackoverflow this) **possible solution** using agg() allows for multiple aggregations - might do the trick"},{"metadata":{"trusted":false},"cell_type":"code","source":"g = assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_correct'].sum().sort_values(by=['installation_id'])\nh = assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_incorrect'].sum().sort_values(by=['installation_id'])\n\nggg = test_assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_correct'].sum().sort_values(by=['installation_id'])\nhhh = test_assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_incorrect'].sum().sort_values(by=['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"g['num_correct'] = g['num_correct'].astype(int)      #g.shape\nh['num_incorrect'] = h['num_incorrect'].astype(int)   #h.shape\n\nggg['num_correct'] = ggg['num_correct'].astype(int)      #g.shape\nhhh['num_incorrect'] = hhh['num_incorrect'].astype(int)   #h.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"merged_inner = pd.merge(left=g,right=h, left_on='game_session', right_on='game_session')\n\ntest_merged_inner = pd.merge(left=ggg,right=hhh, left_on='game_session', right_on='game_session')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1 = merged_inner[['game_session','installation_id_x','title_x','num_correct','num_incorrect']]\ndf2 = test_merged_inner[['game_session','installation_id_x','title_x','num_correct','num_incorrect']] \npd.options.mode.chained_assignment = None\ndf1.rename(columns = {'installation_id_x':'installation_id'}, inplace = True)\ndf1.rename(columns = {'title_x':'title'}, inplace = True)\ndf2.rename(columns = {'installation_id_x':'installation_id'}, inplace = True)\ndf2.rename(columns = {'title_x':'title'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1['accuracy'] = df1.apply(lambda row: row.num_correct/(row.num_correct + row.num_incorrect), axis=1)\ndf2['accuracy'] = df2.apply(lambda row: row.num_correct/(row.num_correct + row.num_incorrect), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define accuracy groups\n\ndef groupacc(row):\n    if row['num_correct'] == 1 and row['num_incorrect'] == 0:\n        return 3\n    if row['num_correct'] == 1 and row['num_incorrect'] == 1:\n        return 2\n    if row['num_correct'] == 1 and row['num_incorrect'] > 1:\n        return 1\n    if row['num_correct'] == 0: \n        return 0\n    return 'Other'\n\ndf1['accuracy_group'] = df1.apply(groupacc, axis=1)\ndf2['accuracy_group'] = df2.apply(groupacc, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point I want to compare to see if the ground truths are the same as the one provided. I check the values below and they are all pretty much same except for a few unimportant trailing float differences."},{"metadata":{"trusted":false},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"c = df1.sort_values(by=['game_session'])\nk = train_labels.sort_values(by=['game_session'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"comparison_array = c.values == k.values\n\nif False in comparison_array:\n    print (\"Not the same\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(np.where(comparison_array==False)[0])  # 46 differences due to floating point stuff","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features \n\n$Y$ is the accuracy group for an assessment, e.g. this is what you are trying to predict.\n\n$X$ will be a feature matrix for each such session collected by looking up data for that user, you only want to consider data \nup to the timestamp of the assessment you are training or predicting on.\n\n\nThere will also be features like \"title\" which tells you which one it is, this is not historical.\n\nLet's pair down the train set to only the ids that actually took (e.g. started 2000) an assessment and also made an attempt (4100/4110) and do some more inspection, with comments detailing below.\n\none issue is features which have NA for some data - (accumulated previous accuracy group) -> our approach will be to substitue the mean value for the type.\n\n**NOTE: YOU HAVE TO BUILD FEATURES FOR THE TEST SET AS WELL** \n\nbelow i do some encodings of some text titles to integers"},{"metadata":{"trusted":false},"cell_type":"code","source":"# some encodings\nlist_of_assessment_titles = list(set(assessments['title'].unique()))\nlist_of_assessment_titles.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assessment_titles_map = dict(zip(list_of_assessment_titles, np.arange(len(list_of_assessment_titles))))\nassessment_titles_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assessments['title'] = assessments['title'].map(assessment_titles_map)    # note if you run this twice it will break\ntrain_labels['title'] = train_labels['title'].map(assessment_titles_map)  # note if you run this twice it will break","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(assessments['game_session'].unique()))  # unique sessions of type assesment in train set\ntrain_reduced_users = assessments['installation_id'].unique()\nprint(len(train_reduced_users))  # unique users who made at least one attempt on an assessment in train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train.loc[train['installation_id'].isin(train_reduced_users)]  # drop some data\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_users = test.installation_id.unique()    # installation_ids in test set\nprint(len(test_users))\nlen(set(test_users).intersection(set(users)))  # there is no intersection in installation_ids between train and test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point after inspection, it becomes clear that we need to predict the score for the last assessment in the test set. Also, despite some unclear language in the instructions, after some effort it became clear to me from inspection that **the last assessment (by timestamp) has been truncated for each user in the test set**. Some of the test set users have no previous assessments to learn from. \n\nTo build features let's build some **accumulations**. We want accumulated accuracy groups on previous assessments. Possibly broken down for each assessment. Let's start there. Later we also want counts of time spent previously on various stuff and counts of events for previous stuff."},{"metadata":{"trusted":false},"cell_type":"code","source":"mean_group_accs = c.groupby('title',as_index=False).mean()['accuracy_group']\nc.groupby('title',as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**remember to improve these means by adding rows from the test set or external data would help also!!**"},{"metadata":{"trusted":false},"cell_type":"code","source":"list(mean_group_accs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You need to combine the train data with the test data (except for last line)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# code to combine train and test datas\n\ntest_predict = test.loc[test['type'] == 'Assessment'].groupby('installation_id',as_index=False).last()\n\n# you need to combine test_assessments so that it has those last rows basically do a union at this stage\n\ntest_final_assessments = pd.concat([test_assessments_for_union,test_predict],sort=False)\ntest_final_assessments = test_final_assessments.sort_values(by=['installation_id','timestamp'])\n#test_final_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_features(num,test_or_train):\n\n    N=num   # to go to production set N = len(train_reduced_users)\n\n    features = []   # this will by the full data for each assessment   X  & Y\n\n    build_test = test_or_train   # False\n\n    for j in tqdm(range(N)):\n\n        if build_test == False:\n            user = train_reduced_users[j]   \n            prev_hist = train.loc[train['installation_id']==user]   # for previous history counts, narrowed to user    \n            user_assessments = assessments.loc[assessments['installation_id']==user]  # for previous cum_accuracy_grps    \n            user_sessions = user_assessments['game_session'].unique()  # time ordered\n        else:\n            user = test_users[j]\n            prev_hist = test.loc[test['installation_id']==user]   # for previous history counts, narrowed to user \n            user_sessions = test_final_assessments.loc[test_final_assessments['installation_id']==user]['game_session'].unique()\n\n        counters = np.array([0.0,0.0,0.0,0.0,0.0])  # number of each type of assessment initialization\n\n        cumalitive_acc_groups = np.array(list(mean_group_accs))   # initialization\n\n        #cumalitive_acc_groups = np.array([0.0,0.0,0.0,0.0,0.0])   # alternative zero initialization\n\n        magmapeak_counts = 0\n        treetopcity_counts = 0\n        crystalcaves_counts = 0\n\n        game_counts = 0\n        clip_counts = 0\n        activity_counts = 0\n        assess_counts = 0\n\n        ii = 1\n\n        for session in user_sessions:     # these are already time-ordered \n\n            # get the time at the start of the game_session\n\n            # code to convert timestamp data to datetime data\n\n            prev_hist['timestamp'] = pd.to_datetime(prev_hist['timestamp'])\n\n            the_time = prev_hist.loc[prev_hist['game_session']==session]['timestamp'].iloc[0]\n\n            the_past = prev_hist.loc[prev_hist['timestamp'] < the_time]  # this could be empty!! \n\n            num_events = the_past.groupby('world',as_index=False).size().reset_index(name='counts')\n\n            num_sessions_by_type = the_past.groupby('game_session',as_index=False).last().groupby('type').size().reset_index(name='tcounts')\n\n            for index, row in num_sessions_by_type.iterrows():\n                if row['type'] == 'Game':\n                    game_counts = row['tcounts']\n                if row['type'] == 'Activity':\n                    activity_counts = row['tcounts']\n                if row['type'] == 'Assessment':\n                    assess_counts = row['tcounts'] \n                if row['type'] == 'Clip':\n                    clip_counts = row['tcounts']\n\n            # now you want to count number of previous events for this user before this timestamp\n\n            user_features = []\n\n            user_features.append(clip_counts)\n            user_features.append(activity_counts)\n            user_features.append(assess_counts)\n            user_features.append(game_counts)  \n\n            if build_test == False:\n                sesh_dat = train_labels.loc[train_labels['game_session'] == session]\n                score = sesh_dat['accuracy_group'].iloc[0]\n            else:\n                if ii < len(user_sessions):\n                    sesh_dat = df2.loc[df2['game_session'] == session]\n                    score = sesh_dat['accuracy_group'].iloc[0]\n                else:\n                    score = 0         \n\n            #title = sesh_dat['title'].iloc[0]\n\n            title = prev_hist.loc[prev_hist['game_session']==session]['title'].iloc[0]\n            \n            title = assessment_titles_map[title]\n\n            #user_features.append(user)\n\n            user_features.append(title)\n\n            total_accum = np.sum(cumalitive_acc_groups)/(np.sum(counters)+5.0)\n            user_features.append(cumalitive_acc_groups[0]/(counters[0]+1.0))\n            user_features.append(cumalitive_acc_groups[1]/(counters[1]+1.0))\n            user_features.append(cumalitive_acc_groups[2]/(counters[2]+1.0))\n            user_features.append(cumalitive_acc_groups[3]/(counters[3]+1.0))\n            user_features.append(cumalitive_acc_groups[4]/(counters[4]+1.0))\n\n            if title == 0:\n                counters[0] += 1\n                cumalitive_acc_groups[0] = cumalitive_acc_groups[0] + score\n            elif title == 1:\n                counters[1] += 1\n                cumalitive_acc_groups[1] = cumalitive_acc_groups[1] + score\n            elif title == 2:\n                counters[2] += 1\n                cumalitive_acc_groups[2] = cumalitive_acc_groups[2] + score\n            elif title == 3:\n                counters[3] += 1\n                cumalitive_acc_groups[3] = cumalitive_acc_groups[3] + score\n            elif title == 4:\n                counters[4] += 1\n                cumalitive_acc_groups[4] = cumalitive_acc_groups[4] + score\n\n            user_features.append(total_accum)\n\n            user_features.append(score)\n\n            for index, row in num_events.iterrows():\n                if row['world'] == 'MAGMAPEAK':\n                    magmapeak_counts = row['counts']\n                if row['world'] == 'TREETOPCITY':\n                    treetopcity_counts = row['counts']\n                if row['world'] == 'CRYSTALCAVES':\n                    crystalcaves_counts = row['counts']  \n\n            user_features.append(magmapeak_counts)\n            user_features.append(treetopcity_counts)\n            user_features.append(crystalcaves_counts)\n            user_features.append(sum(num_events['counts']) - (magmapeak_counts + treetopcity_counts+crystalcaves_counts))\n            user_features.append(sum(num_events['counts']))\n            \n            if build_test == False:\n                features.append(user_features)\n            else:\n                if ii == len(user_sessions):\n                    features.append(user_features)\n            ii += 1\n            \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = pd.DataFrame(build_features(len(test_users),True))\ntest = test.round(2)\n#test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.DataFrame(build_features(3614,False))\ntrain = train.round(2)\n#train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point there is alot of **optimization, remove inefficiencies** use Counter() etc. that needs to be done to speed up the `build_features` function - you can search through fewer events.\n\nAlso, alot more features could be included such as some **durations**... alot more could be done here but for now we just want to go ahead and get it to work and then we can come back to feature engineering later if we have time\n\n**another idea is to include the historical test data into train**"},{"metadata":{},"cell_type":"markdown","source":"\n## Approach to modelling\n\n\nOur approach to modelling is to work from simple models to complex models and to learn about how to implement each model correctly while building cross-validation into the pipeline as well. The idea for this competition is to learn how to build models, not neccessarily to win this competition... though that would be nice...\n\n1. Simple regression (or logistic regression)\n\n2. Single decision tree\n\n3. KNN classifier \n\n4. Naive Bayes\n\n5. Neural Network (probably will not score great but you should still implement)\n\n6. \n\nThe idea is to start with a **simple model** and **submit** and see how you improve as you choose different models!!\nFor this purpose a common framework, wrapper, for models does make sense.\n**categoricals** the title - column 4 is a categorical, as is the target variable column 11\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Model eval - quadratic weighted kappa (qwk)"},{"metadata":{"trusted":false},"cell_type":"code","source":"def qwk(a1,a2):\n    N = 4\n    o = confusion_matrix(a1,a2)\n    w = np.zeros(shape=(N,N))\n    for i in range(N):\n        for j in range(N):\n            w[i,j] = (i-j)**2\n    w = w/((N-1)*(N-1)) \n    w = w.round(3)\n    \n    e = np.outer(np.histogram(a1, [i for i in range(N+1)])[0],np.histogram(a2,[i for i in range(N+1)])[0])\n\n    e = e/np.sum(e)\n    o = o/np.sum(o)\n\n    return 1 - np.sum(np.multiply(o,w))/np.sum(np.multiply(e,w))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# preprocessing \n\ny_train = train[11]\ny_test = test[11]\nX_train = train.drop(11, axis=1)\nX_test = test.drop(11, axis=1)\n\ncategorical = 4   # categoricals\n\ndumb = pd.get_dummies(X_train[categorical],drop_first=True,prefix='g')\ndumby = pd.get_dummies(X_test[categorical],drop_first=True,prefix='j')\n\nX_train = X_train.drop([categorical],axis=1)\nX_test = X_test.drop([categorical],axis=1)\n\nX_train = pd.concat([X_train,dumb],axis=1)\nX_test = pd.concat([X_test,dumby],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN model\n\nThis is probably the simplest model to implement, unfortunately, it maxes out at a qwk of about 0.5 which is not competitive, but it is a good starting point. It's also very important to do **feature transformation (scaling, demean etc.)** for this type of model"},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# with feature scaling\n\n# feature scaling is VERY important for KNN\n\nX_scaled = preprocessing.scale(X_train)\n\nmean_scores = []\nnns = []\nstds = []\n\nqwks_mean = []\n\nfor nn in range(2,50,5):\n    cv = KFold(n_splits=5)\n\n    neigh = KNeighborsClassifier(n_neighbors = nn)\n\n    scores = []\n    qwks = []\n\n    # if you are dealing with dataframes, you have to index with X.iloc[test_index] etc. !!1\n\n    for train_index, test_index in cv.split(X_scaled,y_train):\n        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train_cv, X_test_cv = X_scaled[train_index], X_scaled[test_index]\n        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n        neigh.fit(X_train_cv, y_train_cv)\n        scores.append(neigh.score(X_test_cv, y_test_cv))\n        y_pred = neigh.predict(X_test_cv)\n        qwk_res = qwk(y_pred,y_test_cv)\n        qwks.append(qwk_res)\n    qwks_mean.append(np.mean(qwks))\n    scores\n    nns.append(nn)\n    stds.append(np.std(scores))\n    mean_scores.append(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(nns,mean_scores);\nplt.xlabel(\"k\");plt.ylabel(\"cv-mean-accuracy\");\nfig1, ax1 = plt.subplots();\nax1.plot(nns,qwks_mean);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's interesting to note that qwk actually has a maxima earlier and then decreases whereas accuracy keeps increasing with kk.\nThis suggests that we can a 0.48 with this solution, so let's just submit and see if that's the case, keep working, we want to do a submit just to get the practice"},{"metadata":{"trusted":false},"cell_type":"code","source":"neigh = KNeighborsClassifier(n_neighbors = 20)\n\n# don't forget to scale the test data!!\n\nneigh.fit(X_scaled, y_train)\n\nX_test_scaled = preprocessing.scale(X_test)\n\ny_pred = neigh.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#y_pred\nd = {'installation_id': test_users, 'accuracy_group': y_pred}\nsample_submission = pd.DataFrame(d)\nsample_submission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(test_users)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}