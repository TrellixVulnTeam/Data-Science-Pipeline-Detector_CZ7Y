{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n%matplotlib inline\nimport matplotlib.pyplot as plt  \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n#Importing most common alogorithms \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC#we will not be using SVM due tot he huge training time required on our dataset.\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n\nfrom sklearn import model_selection #Cross-validation multiple scoring function\nimport warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', usecols=keep_cols)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group and Reduce"},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = group_and_reduce(train)\ntest = group_and_reduce(test)\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training models"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_labels[['installation_id', 'accuracy_group']]\ntrain = train.merge(labels, how='left', on='installation_id').dropna()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train  = train.drop(['installation_id', 'accuracy_group'], axis=1),train['accuracy_group']\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\nnp.random.seed(seed)\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(multi_class='auto',n_jobs=-1)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA',QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(7, n_jobs=-1)))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=10)))\nmodels.append(('NB', GaussianNB()))\n#models.append(('Gaussian Process Classifier', GaussianProcessClassifier()))\n#models.append(('SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('SVM_',SVC(gamma=2, C=1)))\nmodels.append(('RandomForest',RandomForestClassifier( n_estimators=100, n_jobs=-1)))\nmodels.append(('MLP',MLPClassifier(alpha=0.0001)))\nmodels.append(('ADABoost',AdaBoostClassifier()))\nmodels.append(('One-Vs-Rest LR', OneVsRestClassifier(LogisticRegression(multi_class='auto',n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest LDA', OneVsRestClassifier(OneVsRestClassifier(LinearDiscriminantAnalysis(),n_jobs=-1))))\nmodels.append(('One-Vs-Rest QDA',OneVsRestClassifier(QuadraticDiscriminantAnalysis())))\nmodels.append(('One-Vs-Rest KNN', OneVsRestClassifier(KNeighborsClassifier(7, n_jobs=-1))))\nmodels.append(('One-Vs-Rest CART', OneVsRestClassifier(DecisionTreeClassifier(max_depth=10),n_jobs=-1)))\nmodels.append(('One-Vs-Rest NB', OneVsRestClassifier(GaussianNB(),n_jobs=-1)))\n#models.append(('One-Vs-Rest Gaussian Process Classifier', OneVsRestClassifier(GaussianProcessClassifier(),n_jobs=-1)))\n#models.append(('One-Vs-Rest SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('One-Vs-Rest SVM_',SVC(gamma=2, C=1)))\nmodels.append(('One-Vs-Rest RandomForest',OneVsRestClassifier(RandomForestClassifier( n_estimators=100, n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest MLP',OneVsRestClassifier(MLPClassifier(alpha=0.0001),n_jobs=-1)))\nmodels.append(('One-Vs-Rest ADABoost',OneVsRestClassifier(AdaBoostClassifier(),n_jobs=-1)))\n\n# evaluate each model in turn\n\nresults = []\nscoring = ['accuracy',\n          'precision_weighted',\n          'recall_weighted',\n          'f1_weighted']\nnames = []\n\nsk = model_selection.StratifiedKFold(n_splits=10, random_state=42)\nfor name, model in models:\n    cv_results = model_selection.cross_validate(model, x_train, y_train, cv=sk, scoring=scoring) \n    results.append(cv_results)\n    names.append(name)\n    msg ='-------------------------------------------------------------------------------------------------------------\\n'\n    msg = \"Model : %s \\n\" % (name)\n    msg = msg +'\\n'\n    msg =  msg + \"Accuracy :  %f (%f)\\n\" % (cv_results['test_accuracy'].mean(),cv_results['test_accuracy'].std())\n    msg =  msg + \"Precision score :  %f (%f)\\n\" % (cv_results['test_precision_weighted'].mean(),cv_results['test_precision_weighted'].std())\n    msg =  msg + \"Recall score :  %f (%f)\\n\" % (cv_results['test_recall_weighted'].mean(),cv_results['test_recall_weighted'].std())\n    msg =  msg + \"F1 score :  %f (%f)\\n\" % (cv_results['test_f1_weighted'].mean(),cv_results['test_f1_weighted'].std())\n    msg = msg + '------------------------------------------------------------------------------------------------------------\\n'\n    print(msg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy = []\nPrecision = []\nRecall = []\nF1 = []\nfor idx,scores in enumerate(results):\n    Accuracy.append(scores['test_accuracy'])\n    Precision.append(scores['test_precision_weighted'])\n    Recall.append(scores['test_recall_weighted'])\n    F1.append(scores['test_f1_weighted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(30,30))\nfig.suptitle('Algorithms Comparison')\nax = fig.add_subplot(221)\nplt.boxplot(Accuracy)\nplt.title('Accuracy score')\nax.set_xticklabels(names)\nax = fig.add_subplot(222)\nplt.boxplot(Precision)\nplt.title('Precision Score')\nax.set_xticklabels(names)\nax = fig.add_subplot(223)\nplt.boxplot(Recall)\nax.set_xticklabels(names)\nplt.title('Recall score')\nax = fig.add_subplot(224)\nplt.title('F1 score')\nplt.boxplot(F1)\nax.set_xticklabels(names)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the 'One vs the Rest' Classifier approach give a little boost on performance to all the classification aproach on diferent folds of the validation set, but we have a clear winner wich is AdaBoost algorthms. Let's see in performance on the test set trough the leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\n\nmodel = OneVsRestClassifier(AdaBoostClassifier(),n_jobs=-1)\n\ntic = time()\nmodel.fit(x_train,y_train)\ntoc = time()\n\nprint(\"Classifier : One-Vs-Rest AdaBoostClassifier ===> Training duration : {} sec\".format( toc - tic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(test.drop(['installation_id'], axis=1).fillna(0))#.argmax(axis=1)\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['accuracy_group'] = y_test\ntest[['installation_id', 'accuracy_group']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}