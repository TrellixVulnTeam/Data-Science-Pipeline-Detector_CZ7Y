{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"2019 Data Science Bowl\n===\nDamien Park  \n2019.11.14  \n\nversion\n---\n* ver 35. event_code aggregates by type  \n* ver 36. fix null event_code in test data set\n* ver 37. take log and standardscaling  \n* ver 38. counting event_id-0.488  \n* ver 39. improving code efficiency(rolling, memory management)\n* ver 40. modeling\n* ver 47. fix minor error(columns)\n* ver 54. category, true, 20, true, minmax\n* ver 55. category, true, 20, true, standard\n* ver 55. category, true, 1, true, minmax"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.metrics import confusion_matrix\n# from sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n# from sklearn.svm import SVC\n# from catboost import CatBoostClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import AdaBoostClassifier\n\nimport keras\nimport tensorflow as tf\n\n# import pprint\nimport gc\nimport os\nimport tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pandas display option\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_row', 1500)\npd.set_option('max_colwidth', 150)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# data load option\ndtypes = {\"event_id\":\"object\", \"game_session\":\"object\", \"timestamp\":\"object\", \n          \"event_data\":\"object\", \"installation_id\":\"object\", \"event_count\":\"int16\", \n          \"event_code\":\"int16\", \"game_time\":\"int32\", \"title\":\"category\", \n          \"type\":\"category\", \"world\":\"category\"}\nlabel = {\"game_session\":\"object\", \"installation_id\":\"object\", \"title\":\"category\", \n         \"num_correct\":\"int8\", \"num_incorrect\":\"int8\", \n         \"accuracy\":\"float16\", \"accuracy_group\":\"int8\"}\n\n# hyper parameter\nloss_type = \"category\" # mse/category\ndp_log = True\n# window = 70\nbatch_sizes = 1\nvalidation = True\nscale_type = \"minmax\" # minmax/robust/standard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Prepareing"},{"metadata":{},"cell_type":"markdown","source":"### Split data by ID"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\", dtype=dtypes)\ntest = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\", dtype=dtypes)\nlabel_ = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\", dtype=label)\n# sample = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/sample_submission.csv\")\n# specs = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/specs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# calculating accuracy\nclass accuracy:\n    def __init__(self, df):\n        self.df = df\n\n        \n    # Assessment evaluation-Cart Balancer (Assessment)\n    def cart_assessment(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='d122731b'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cart_assessment_2(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='b74258a0'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Chest Sorter (Assessment)\n    def chest_assessment(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='93b353f2'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def chest_assessment_2(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='38074c54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Cauldron Filler (Assessment)\n    def cauldron_assessment(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='392e14df'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cauldron_assessment_2(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='28520915'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Mushroom Sorter (Assessment)\n    def mushroom_assessment(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='25fa8af4'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def mushroom_assessment_2(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='6c930e6e'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Bird Measurer (Assessment)\n    def bird_assessment(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='17113b36'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def bird_assessment_2(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='f6947f54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n\n# quadratic kappa\ndef quadratic_kappa(actuals, preds, N=4):\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E/E.sum();\n    O = O/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num/den))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"timestamp\"] = pd.to_datetime(test.timestamp)\ntest.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\n\n_ = accuracy(test).cart_assessment()\n_ = _.append(accuracy(test).chest_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).cauldron_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).mushroom_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).bird_assessment(), ignore_index=True)\n\ntest = test[test.installation_id.isin(pd.unique(_.installation_id))]\ntest = test.merge(_, how=\"left\", on=[\"installation_id\", \"game_session\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = []\nidx = 0\nfor _, val in tqdm.tqdm_notebook(test.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(test.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    _ = val.query(\"type=='Assessment'\")\n    _ = _[~_.accuracy_group.isnull()]\n    session = _.reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        sample = val[:j+1]\n        sample[\"ID\"] = idx\n        idx += 1\n        df_test.append(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = pd.DataFrame(columns=[\"ID\", \"accuracy_group\"])\nfor i in tqdm.tqdm_notebook(df_test):\n# for i in tqdm.notebook.tqdm(df_test):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train[train.installation_id.isin(pd.unique(label_.installation_id))]\ndel train\ndf = df.merge(label_.loc[:, [\"installation_id\", \"game_session\", \"title\", \"accuracy_group\"]], \n              on=[\"installation_id\", \"game_session\", \"title\"], how=\"left\")\ndf[\"timestamp\"] = pd.to_datetime(df.timestamp)\ndf.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = []\nidx = max(label.ID)+1\nfor _, val in tqdm.tqdm_notebook(df.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(df.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    session = val.query(\"type=='Assessment'\").reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        if ~np.isnan(val.iat[j, -1]):\n            sample = val[:j+1]\n            sample[\"ID\"] = idx\n            idx += 1\n            df_train.append(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm_notebook(df_train):\n# for i in tqdm.notebook.tqdm(df_train):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")\nlabel = label.merge(pd.get_dummies(label.accuracy_group, prefix=\"y\"), left_on=[\"ID\"], right_index=True)\n\ndf_test.extend(df_train)\ndf_train = df_test\ndel df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train[0].head()), display(label.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## World"},{"metadata":{},"cell_type":"markdown","source":"### world_log\nHow many log in each world"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\nworld = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_log\n    _ = i.groupby([\"ID\", \"world\"]).size().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    size.extend(_[0])\n\nworld_log = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"size\":size})\nworld_log = world_log.pivot_table(index=\"ID\", columns=\"world\", values=\"size\")\nworld_log = world_log.fillna(0)\nworld_log.columns.name = None\nworld_log.reset_index(inplace=True)\nworld_log = world_log.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_log.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_log.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_log = world_log.add_suffix(\"_l\")\nworld_log.rename(columns={\"ID_l\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_log.iloc[:, 1:] = np.log(world_log.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### world_time\nHow long did play in each world"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\nworld = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_time\n    _ = i.groupby([\"ID\", \"world\", \"game_session\"]).game_time.max().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    game_time.extend(_.game_time)\n\nworld_time = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"game_time\":game_time})\nworld_time = world_time.groupby([\"ID\", \"world\"]).sum().reset_index()\nworld_time = world_time.pivot_table(index=\"ID\", columns=\"world\", values=\"game_time\")\nworld_time = world_time.fillna(-1)\nworld_time.columns.name = None\nworld_time[\"ID\"] = world_time.index\nworld_time.reset_index(drop=True, inplace=True)\nworld_time = world_time.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_time.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_time.merge(label).query(\"accuracy_group==@i\")[val]+2), label=i)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    sns.distplot(world_time[val])\n#     plt.title(val)\n    plt.subplot(2, 4, idx+5)\n    sns.distplot(np.log(world_time[val]+2))\n#     plt.title(val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_time.drop(columns=[\"NONE\"], inplace=True)\nworld_time = world_time.add_suffix(\"_t\")\nworld_time.rename(columns={\"ID_t\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_time.iloc[:, 1:] = np.log(world_time.iloc[:, 1:]+2)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_time.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### world_session\nHow many session is opend by world"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\nworld = []\ngame_session = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # world_session\n    _ = i.groupby([\"ID\", \"world\"]).game_session.nunique().reset_index()\n    ID.extend(_.ID)\n    world.extend(_.world)\n    game_session.extend(_.game_session)\n\nworld_session = pd.DataFrame(data={\"ID\":ID, \"world\":world, \"game_session\":game_session})\nworld_session = world_session.pivot_table(index=\"ID\", columns=\"world\", values=\"game_session\")\nworld_session = world_session.fillna(0)\nworld_session.columns.name = None\nworld_session[\"ID\"] = world_session.index\nworld_session.reset_index(drop=True, inplace=True)\nworld_session = world_session.loc[:, [\"ID\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(world_session.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(world_session.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]):\n    plt.subplot(2, 4, idx+1)\n    sns.distplot(world_session[val])\n#     plt.title(val)\n    plt.subplot(2, 4, idx+5)\n    sns.distplot(np.log(world_session[val]+1))\n#     plt.title(val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_session = world_session.add_suffix(\"_s\")\nworld_session.rename(columns={\"ID_s\":\"ID\"}, inplace=True)\nif dp_log==True:\n    world_session.iloc[:, 1:] = np.log(world_session.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_session.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Event_id\nHow many times call event_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\nevent_id = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # event_id\n    _ = i.groupby([\"ID\", \"event_id\"]).size().reset_index()\n    ID.extend(_.ID)\n    event_id.extend(_.event_id)\n    size.extend(_[0])\n\nevent_id = pd.DataFrame(data={\"ID\":ID, \"event_id\":event_id, \"size\":size})\nevent_id = event_id.pivot_table(index=\"ID\", columns=\"event_id\", values=\"size\")\nevent_id = event_id.fillna(0)\nevent_id.columns.name = None\nevent_id.index.name = None\nevent_id[\"ID\"] = event_id.index\nevent_id.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    event_id.iloc[:, :-1] = np.log(event_id.iloc[:, :-1]+1)\n#     event_id.iloc[:, 1:] = np.log(event_id.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_id.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Game time"},{"metadata":{},"cell_type":"markdown","source":"### play_time\nHow long play game"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # play_time\n    _ = i.groupby([\"ID\", \"game_session\"]).game_time.max().reset_index()\n    ID.extend(_.ID)\n    game_time.extend(_.game_time)\n\nplay_time = pd.DataFrame(data={\"ID\":ID, \"game_time\":game_time})\nplay_time = play_time.groupby([\"ID\"]).sum().reset_index()\nplay_time.reset_index(drop=True, inplace=True)\nplay_time = play_time.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(play_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"], label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(play_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"]+1), label=i)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    play_time.iloc[:, 1:] = np.log(play_time.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_time.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### gap_time\nThe gap between start and end"},{"metadata":{"trusted":true},"cell_type":"code","source":"gap_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # gap_time\n    _ = i.groupby([\"ID\"]).timestamp.agg([\"min\", \"max\"])\n    _.columns.name = None\n    gap_time = pd.concat([gap_time, _], sort=True)\n\ngap_time.reset_index(inplace=True)\ngap_time[\"gap\"] = gap_time[\"max\"]-gap_time[\"min\"]\ngap_time[\"gap\"] = gap_time[\"gap\"].astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(gap_time.merge(label).query(\"accuracy_group==@i\")[\"gap\"], label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(gap_time.merge(label).query(\"accuracy_group==@i\")[\"gap\"]+1), label=i)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gap_time.drop(columns=[\"max\", \"min\"], inplace=True)\nif dp_log==True:\n    gap_time.iloc[:, 1:] = np.log(gap_time.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gap_time.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Session"},{"metadata":{},"cell_type":"markdown","source":"### Session_count\nHow many session is opend?"},{"metadata":{"trusted":true},"cell_type":"code","source":"session_count = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_count\n    _ = i.groupby([\"ID\"]).game_session.nunique().reset_index()\n    _.columns.name = None\n    session_count = pd.concat([session_count, _], sort=True)\n\nsession_count.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_count.merge(label).query(\"accuracy_group==@i\")[\"game_session\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_count.merge(label).query(\"accuracy_group==@i\")[\"game_session\"]), bins=50, label=i)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    session_count.iloc[:, 1:] = np.log(session_count.iloc[:, 1:])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_count.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Session length\nHow long did you play in each session on average? (mean, log)"},{"metadata":{"trusted":true},"cell_type":"code","source":"session_length = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_length\n#     _ = i.query(\"type!='Clip'\").groupby([\"ID\", \"game_session\"]).size().groupby([\"ID\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _ = i.groupby([\"ID\", \"game_session\"]).size().groupby([\"ID\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _.columns.name = None\n    session_length = pd.concat([session_length, _], sort=True)\n\nsession_length.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_length.merge(label).query(\"accuracy_group==@i\")[\"session_length\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_length.merge(label).query(\"accuracy_group==@i\")[\"session_length\"]), bins=50, label=i)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    session_length.iloc[:, 1:] = np.log(session_length.iloc[:, 1:])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_length.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Session time\nHow long did you play in each session on average? (mean, time)"},{"metadata":{"trusted":true},"cell_type":"code","source":"session_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_train):\n    # session_time\n    _ = i.groupby([\"ID\", \"game_session\"]).game_time.max().groupby([\"ID\"]).mean().reset_index()\n    _.columns.name = None\n    session_time = pd.concat([session_time, _], sort=True)\n\nsession_time.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(session_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"], bins=50, label=i)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nfor i in [0, 1, 2, 3]:\n    sns.distplot(np.log(session_time.merge(label).query(\"accuracy_group==@i\")[\"game_time\"]+1), bins=50, label=i)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    session_time.iloc[:, 1:] = np.log(session_time.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_time.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\ntypes = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # types\n    _ = i.groupby([\"ID\", \"type\"]).size().reset_index()\n    ID.extend(_.ID)\n    types.extend(_.type)\n    size.extend(_[0])\n\ntypes = pd.DataFrame(data={\"ID\":ID, \"type\":types, \"size\":size})\ntypes = types.pivot_table(index=\"ID\", columns=\"type\", values=\"size\")\ntypes.columns.name = None\ntypes.index.name = None\ntypes = types.fillna(0)\ntypes[\"ID\"] = types.index\ntypes = types.loc[:, [\"ID\", \"Activity\", \"Assessment\", \"Clip\", \"Game\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nfor idx, val in enumerate([\"Activity\", \"Assessment\", \"Clip\", \"Game\"]):\n    plt.subplot(2, 4, idx+1)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(types.merge(label).query(\"accuracy_group==@i\")[val], label=i)\n    plt.legend()\n    \n    plt.subplot(2, 4, idx+5)\n    for i in [0, 1, 2, 3]:\n        sns.distplot(np.log(types.merge(label).query(\"accuracy_group==@i\")[val]+1), label=i)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    types.iloc[:, 1:] = np.log(types.iloc[:, 1:]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Title\nWhat title is played?"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\ntitle = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    # title\n    _ = i.groupby([\"ID\", \"title\"]).size().reset_index()\n    ID.extend(_.ID)\n    title.extend(_.title)\n    size.extend(_[0])\n\ntitle = pd.DataFrame(data={\"ID\":ID, \"title\":title, \"size\":size})\ntitle = title.pivot_table(index=\"ID\", columns=\"title\", values=\"size\")\ntitle.columns.name = None\ntitle.index.name = None\ntitle = title.fillna(0)\ntitle[\"ID\"] = title.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    title.iloc[:, :-1] = np.log(title.iloc[:, :-1]+1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last Assessment type\ntarget Assessment type"},{"metadata":{"trusted":true},"cell_type":"code","source":"assessment = pd.DataFrame(columns=[\"ID\", \"title\"])\nfor i in tqdm.tqdm_notebook(df_train):\n    # assessment\n    _ = i.tail(1).loc[:, [\"ID\", \"title\"]].reset_index(drop=True)\n    assessment = pd.concat([assessment, _], sort=False)\n\nassessment['Assessment_1'] = 0\nassessment['Assessment_2'] = 0\nassessment['Assessment_3'] = 0\nassessment['Assessment_4'] = 0\nassessment['Assessment_5'] = 0\n\nassessment.loc[assessment.title=='Mushroom Sorter (Assessment)', 'Assessment_1'] = 1\nassessment.loc[assessment.title=='Cauldron Filler (Assessment)', 'Assessment_2'] = 1\nassessment.loc[assessment.title=='Chest Sorter (Assessment)', 'Assessment_3'] = 1\nassessment.loc[assessment.title=='Cart Balancer (Assessment)', 'Assessment_4'] = 1\nassessment.loc[assessment.title=='Bird Measurer (Assessment)', 'Assessment_5'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = assessment.merge(label).groupby([\"title\", \"accuracy_group\"]).size().reset_index()\n_.accuracy_group = _.accuracy_group.astype(\"object\")\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"title\", y=0, hue=\"accuracy_group\", data=_, dodge=True, alpha=.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.bar(\"title\", height=\"count\", \n        data=assessment.groupby(\"title\").size().reset_index().rename(columns={0:\"count\"}))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del assessment[\"title\"]\n# assessment = assessment.loc[:, [\"ID\", \"Assessment_1\", \"Assessment_2\", \"Assessment_3\", \"Assessment_4\", \"Assessment_5\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessment.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Assessment time\nWhen did player submit assessment?"},{"metadata":{"trusted":true},"cell_type":"code","source":"time = pd.DataFrame(columns=[\"ID\", \"timestamp\"])\nfor i in tqdm.tqdm_notebook(df_train):\n    # time\n    _ = i.tail(1).loc[:, [\"ID\", \"timestamp\"]]\n    time = pd.concat([time, _], sort=False)\n\ntime.reset_index(drop=True, inplace=True)\ntime[\"hour\"] = time.timestamp.dt.hour\ntime[\"hour\"] = time.hour.astype(\"object\")\ntime = time.merge(pd.get_dummies(time.hour, prefix=\"hour\"), how=\"left\", \n                  left_index=True, right_index=True)\ntime.drop(columns=[\"timestamp\", \"hour\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAME\nIn Type Game, we can find round feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = []\ngame_title = []\ngame_round = []\n\nfor i in tqdm.tqdm_notebook(df_train):\n    if \"Game\" in i.type.unique():\n        _ = i.query(\"type=='Game'\").loc[:, [\"ID\", \"title\", \"event_data\"]].set_index([\"ID\", \"title\"]).event_data.apply(lambda x: json.loads(x)[\"round\"]).groupby([\"ID\", \"title\"]).max().reset_index()\n        ID.extend(list(_.ID))\n        game_title.extend(_.title)\n        game_round.extend(_.event_data)\n        \ngame = pd.DataFrame(data={\"ID\":ID, \"game_title\":game_title, \"round\":game_round})\ngame = game.pivot_table(index=\"ID\", columns=\"game_title\", values=\"round\")\ngame.reset_index(inplace=True)\ngame.columns.name = None\ngame = game.fillna(-1)\n\nID = pd.DataFrame(data={\"ID\":range(0, len(df_train))})\ngame = ID.merge(game, how=\"left\")\ngame = game.fillna(-1)\n\ngame = game.add_suffix(\"_r\")\ngame.rename(columns={\"ID_r\":\"ID\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"game.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Merge all data set\nworld_log, world_time, world_session  \nevent_id  \nplay_time, gap_time  \nsession_count, session_length, session_time  \ntypes  \ntitle  \nassessment  \ntime  \ngame  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_set = [world_log, world_time, world_session, event_id, play_time, gap_time, session_count, session_length, session_time, types, title, assessment, time, game]\n# _ = pd.concat(data_set, axis=1, keys=[\"ID\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = world_log.merge(world_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(world_session, how=\"left\", on=[\"ID\"])\n_ = _.merge(event_id, how=\"left\", on=[\"ID\"])\n_ = _.merge(play_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(gap_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_count, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_length, how=\"left\", on=[\"ID\"])\n_ = _.merge(session_time, how=\"left\", on=[\"ID\"])\n_ = _.merge(types, how=\"left\", on=[\"ID\"])\n_ = _.merge(title, how=\"left\", on=[\"ID\"])\n_ = _.merge(assessment, how=\"left\", on=[\"ID\"])\n_ = _.merge(time, how=\"left\", on=[\"ID\"])\n_ = _.merge(game, how=\"left\", on=[\"ID\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x_col = list(_.columns)\ntrain_y_col = [\"accuracy_group\", \"y_0\", \"y_1\", \"y_2\", \"y_3\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_.to_csv(\"train.csv\", index=False)\nlabel.to_csv(\"label.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Scaling / Data Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"if loss_type==\"mse\":\n    if scale_type==\"minmax\":\n        scaler = MinMaxScaler()\n    elif scale_type==\"robust\":\n        scaler = RobustScaler()\n    elif scale_type==\"standard\":\n        scaler = StandardScaler()\n    scaler_y = MinMaxScaler()\n    train_x = scaler.fit_transform(_.loc[:, train_x_col[1:]])\n#     train_y = scaler_y.fit_transform([_.loc[:, \"accuracy_group\"]])\n    train_y = label.loc[:, train_y_col]\n    print(train_x[0])\n    print(train_y.iloc[0, :])\nelif loss_type==\"category\":\n    if scale_type==\"minmax\":\n        scaler = MinMaxScaler()\n    elif scale_type==\"robust\":\n        scaler = RobustScaler()\n    elif scale_type==\"standard\":\n        scaler = StandardScaler()\n    train_x = scaler.fit_transform(_.loc[:, train_x_col[1:]])\n    train_y = label.loc[:, train_y_col]\n    print(train_x[0])\n    print(train_y.iloc[0, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = class_weight.compute_class_weight('balanced', np.unique(label.accuracy_group),\n                                                  label.accuracy_group)\nnp.unique(label.accuracy_group), class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y[\"class_weight\"] = 0\ntrain_y.loc[train_y.accuracy_group==0, \"class_weight\"] = class_weights[0]\ntrain_y.loc[train_y.accuracy_group==1, \"class_weight\"] = class_weights[1]\ntrain_y.loc[train_y.accuracy_group==2, \"class_weight\"] = class_weights[2]\ntrain_y.loc[train_y.accuracy_group==3, \"class_weight\"] = class_weights[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, random_state=1228)\n    display(train_x.shape, train_y.shape)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(train_x[0])):\n    print(i, min(train_x[:, i]), max(train_x[:, i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# leakyrelu = keras.layers.LeakyReLU(alpha=0.3)\nleakyrelu = tf.nn.leaky_relu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(128, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(256, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(256, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(128, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(64, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(32, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nmodel.add(keras.layers.Dense(16, activation=leakyrelu, kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dropout(.3))\n\nif loss_type==\"mse\":\n    model.add(keras.layers.Dense(1, activation=\"linear\"))\n    model.compile(loss=\"mse\", optimizer=\"Adam\")\nelif loss_type==\"category\":\n    model.add(keras.layers.Dense(4, activation=\"softmax\"))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# keras.backend.reset_uids()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(\"model\"):\n    os.mkdir(\"model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    if loss_type==\"mse\":\n        model.fit(x=train_x, y=train_y.loc[:, [\"accuracy_group\"]], \n                  validation_data=[val_x, val_y.loc[:, [\"accuracy_group\"]]], \n                  epochs=50, batch_size=batch_sizes, shuffle=True, class_weight=class_weight)\n    elif loss_type==\"category\":\n        model.fit(x=train_x, y=train_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]].values, \n                  validation_data=[val_x, val_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]].values], \n                  epochs=1000, batch_size=batch_sizes, shuffle=True, \n                  sample_weight=train_y.loc[:, [\"class_weight\"]].values.flatten(), \n                  callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", \n                                                           patience=100, mode=\"auto\"), \n                             keras.callbacks.ModelCheckpoint(\"model/weights.{epoch:02d}-{val_categorical_accuracy:.3f}.hdf5\", \n                                                             monitor='val_categorical_accuracy', \n                                                             verbose=0, save_best_only=True, save_weights_only=False, \n                                                             mode=\"auto\", period=1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation==False:\n    if loss_type==\"mse\":\n        model.fit(train_x, train_y.values, epochs=150, batch_size=batch_sizes, verbose=1, validation_split=.1, shuffle=True)\n    elif loss_type==\"category\":\n        model.fit(train_x, train_y.values, epochs=100, batch_size=batch_sizes, verbose=1, validation_split=.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(train_x, _.accuracy_group.values, epochs=20, batch_size=10, verbose=1, validation_split=.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if loss_type==\"mse\":\n    plt.figure(figsize=(40, 20))\n    plt.subplot(2, 1, 1)\n    plt.plot(model.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.2, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.subplot(2, 1, 2)\n    plt.plot(model.history.history[\"loss\"][3:], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"][3:], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.1, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.show()\n\nelif loss_type==\"category\":\n    plt.figure(figsize=(40, 20))\n    plt.subplot(2, 1, 1)\n    plt.plot(model.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n    plt.plot(model.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n    plt.axhline(1.05, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.subplot(2, 1, 2)\n    plt.plot(model.history.history[\"categorical_accuracy\"], \"o-\", alpha=.4, label=\"categorical_accuracy\")\n    plt.plot(model.history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.4, label=\"val_categorical_accuracy\")\n    plt.axhline(.65, linestyle=\"--\", c=\"C2\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sort(os.listdir(\"model\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.load_model(\"model/\"+np.sort(os.listdir(\"model\"))[-1], custom_objects={'leaky_relu': tf.nn.leaky_relu})\nnp.sort(os.listdir(\"model\"))[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    if loss_type==\"mse\":\n        result = model.predict(val_x)\n        result[result <= 1.12232214] = 0\n        result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n        result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n        result[result > 2.22506454] = 3\n        result = result.astype(\"int\")\n        print(quadratic_kappa(val_y.accuracy_group, result))\n    elif loss_type==\"category\":\n        result = model.predict(val_x)\n        print(quadratic_kappa(val_y.accuracy_group, result.argmax(axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation==False:\n    if loss_type==\"mse\":\n        result = model.predict(train_x)\n        result[result <= 1.12232214] = 0\n        result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n        result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n        result[result > 2.22506454] = 3\n        result = result.astype(\"int\")\n        print(quadratic_kappa(train_y.accuracy_group, result))\n    elif loss_type==\"category\":\n        result = model.predict(train_x)\n        print(quadratic_kappa(train_y.accuracy_group, result.argmax(axis=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\", dtype=dtypes)\ntest[\"timestamp\"] = pd.to_datetime(test.timestamp)\n\nlabel = []\ndf_test = []\nfor idx, val in tqdm.tqdm_notebook(test.groupby([\"installation_id\"])):\n    label.append(idx)\n    df_test.append(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = {}\nfor i in [\"world_log\", \"world_time\", \"world_session\", \"event_id\", \"play_time\", \"gap_time\", \"session_count\", \"session_length\", \"session_time\", \"types\", \"title\", \"assessment\", \"time\", \"game\"]:\n    vars()[i].rename(columns={\"ID\":\"installation_id\"}, inplace=True)\n    col[i] = list(vars()[i].columns)\n\n# world_log\ninstallation_id = []\nworld = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_log\n    _ = i.groupby([\"installation_id\", \"world\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    size.extend(_[0])\n\nworld_log = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"size\":size})\nworld_log = world_log.pivot_table(index=\"installation_id\", columns=\"world\", values=\"size\")\nworld_log = world_log.fillna(0)\nworld_log.columns.name = None\nworld_log.reset_index(inplace=True)\nworld_log = world_log.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_log = world_log.add_suffix(\"_l\")\nworld_log.rename(columns={\"installation_id_l\":\"installation_id\"}, inplace=True)\nworld_log = world_log.loc[:, col[\"world_log\"]]\nworld_log = world_log.fillna(0)\n\n# world_time\ninstallation_id = []\nworld = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_time\n    _ = i.groupby([\"installation_id\", \"world\", \"game_session\"]).game_time.max().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    game_time.extend(_.game_time)\n\nworld_time = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"game_time\":game_time})\nworld_time = world_time.groupby([\"installation_id\", \"world\"]).sum().reset_index()\nworld_time = world_time.pivot_table(index=\"installation_id\", columns=\"world\", values=\"game_time\")\nworld_time = world_time.fillna(-1)\nworld_time.columns.name = None\nworld_time[\"installation_id\"] = world_time.index\nworld_time.reset_index(drop=True, inplace=True)\nworld_time = world_time.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_time = world_time.add_suffix(\"_t\")\nworld_time.rename(columns={\"installation_id_t\":\"installation_id\"}, inplace=True)\nworld_time = world_time.loc[:, col[\"world_time\"]]\nworld_time = world_time.fillna(-1)\n\n# world_session\ninstallation_id = []\nworld = []\ngame_session = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # world_session\n    _ = i.groupby([\"installation_id\", \"world\"]).game_session.nunique().reset_index()\n    installation_id.extend(_.installation_id)\n    world.extend(_.world)\n    game_session.extend(_.game_session)\n\nworld_session = pd.DataFrame(data={\"installation_id\":installation_id, \"world\":world, \"game_session\":game_session})\nworld_session = world_session.pivot_table(index=\"installation_id\", columns=\"world\", values=\"game_session\")\nworld_session = world_session.fillna(0)\nworld_session.columns.name = None\nworld_session[\"installation_id\"] = world_session.index\nworld_session.reset_index(drop=True, inplace=True)\nworld_session = world_session.loc[:, [\"installation_id\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\", \"NONE\"]]\nworld_session = world_session.add_suffix(\"_s\")\nworld_session.rename(columns={\"installation_id_s\":\"installation_id\"}, inplace=True)\nworld_session = world_session.loc[:, col[\"world_session\"]]\nworld_session = world_session.fillna(0)\n\n# event_id\ninstallation_id = []\nevent_id = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # event_id\n    _ = i.groupby([\"installation_id\", \"event_id\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    event_id.extend(_.event_id)\n    size.extend(_[0])\n\nevent_id = pd.DataFrame(data={\"installation_id\":installation_id, \"event_id\":event_id, \"size\":size})\nevent_id = event_id.pivot_table(index=\"installation_id\", columns=\"event_id\", values=\"size\")\nevent_id = event_id.fillna(0)\nevent_id.columns.name = None\nevent_id.index.name = None\nevent_id[\"installation_id\"] = event_id.index\nevent_id.reset_index(drop=True, inplace=True)\nevent_id = event_id.loc[:, col[\"event_id\"]]\nevent_id = event_id.fillna(0)\n\n# play_time\ninstallation_id = []\ngame_time = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # play_time\n    _ = i.groupby([\"installation_id\", \"game_session\"]).game_time.max().reset_index()\n    installation_id.extend(_.installation_id)\n    game_time.extend(_.game_time)\n\nplay_time = pd.DataFrame(data={\"installation_id\":installation_id, \"game_time\":game_time})\nplay_time = play_time.groupby([\"installation_id\"]).sum().reset_index()\nplay_time.reset_index(drop=True, inplace=True)\nplay_time = play_time.fillna(0)\nplay_time = play_time.loc[:, col[\"play_time\"]]\nplay_time = play_time.fillna(0)\n\n# gap_time\ngap_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # gap_time\n    _ = i.groupby([\"installation_id\"]).timestamp.agg([\"min\", \"max\"])\n    _.columns.name = None\n    gap_time = pd.concat([gap_time, _], sort=True)\n\ngap_time.reset_index(inplace=True)\ngap_time[\"gap\"] = gap_time[\"max\"]-gap_time[\"min\"]\ngap_time[\"gap\"] = gap_time[\"gap\"].astype(\"int\")\ngap_time = gap_time.loc[:, col[\"gap_time\"]]\ngap_time = gap_time.fillna(0)\n\n# session_count\nsession_count = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_count\n    _ = i.groupby([\"installation_id\"]).game_session.nunique().reset_index()\n    _.columns.name = None\n    session_count = pd.concat([session_count, _], sort=False)\n\nsession_count.reset_index(drop=True, inplace=True)\nsession_count = session_count.loc[:, col[\"session_count\"]]\nsession_count = session_count.fillna(0)\n\n# session_length\nsession_length = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_length\n#     _ = i.query(\"type!='Clip'\").groupby([\"installation_id\", \"game_session\"]).size().groupby([\"installation_id\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _ = i.groupby([\"installation_id\", \"game_session\"]).size().groupby([\"installation_id\"]).mean().reset_index().rename(columns={0:\"session_length\"})\n    _.columns.name = None\n    session_length = pd.concat([session_length, _], sort=False)\n\nsession_length.reset_index(drop=True, inplace=True)\nsession_length = session_length.loc[:, col[\"session_length\"]]\nsession_length = session_length.fillna(0)\n\n# session_time\nsession_time = pd.DataFrame()\nfor i in tqdm.tqdm_notebook(df_test):\n    # session_time\n    _ = i.groupby([\"installation_id\", \"game_session\"]).game_time.max().groupby([\"installation_id\"]).mean().reset_index()\n    _.columns.name = None\n    session_time = pd.concat([session_time, _], sort=False)\n\nsession_time.reset_index(drop=True, inplace=True)\nsession_time = session_time.loc[:, col[\"session_time\"]]\nsession_time = session_time.fillna(0)\n\n# types\ninstallation_id = []\ntypes = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # types\n    _ = i.groupby([\"installation_id\", \"type\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    types.extend(_.type)\n    size.extend(_[0])\n\ntypes = pd.DataFrame(data={\"installation_id\":installation_id, \"type\":types, \"size\":size})\ntypes = types.pivot_table(index=\"installation_id\", columns=\"type\", values=\"size\")\ntypes.columns.name = None\ntypes.index.name = None\ntypes = types.fillna(0)\ntypes[\"installation_id\"] = types.index\ntypes = types.loc[:, [\"installation_id\", \"Activity\", \"Assessment\", \"Clip\", \"Game\"]]\ntypes = types.loc[:, col[\"types\"]]\ntypes = types.fillna(0)\n\n# title\ninstallation_id = []\ntitle = []\nsize = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    # title\n    _ = i.groupby([\"installation_id\", \"title\"]).size().reset_index()\n    installation_id.extend(_.installation_id)\n    title.extend(_.title)\n    size.extend(_[0])\n\ntitle = pd.DataFrame(data={\"installation_id\":installation_id, \"title\":title, \"size\":size})\ntitle = title.pivot_table(index=\"installation_id\", columns=\"title\", values=\"size\")\ntitle.columns.name = None\ntitle.index.name = None\ntitle = title.fillna(0)\ntitle[\"installation_id\"] = title.index\ntitle = title.loc[:, col[\"title\"]]\ntitle = title.fillna(0)\n\n# assessment\nassessment = pd.DataFrame(columns=[\"installation_id\", \"title\"])\nfor i in tqdm.tqdm_notebook(df_test):\n    # assessment\n    _ = i.tail(1).loc[:, [\"installation_id\", \"title\"]].reset_index(drop=True)\n    assessment = pd.concat([assessment, _], sort=False)\n\nassessment['Assessment_1'] = 0\nassessment['Assessment_2'] = 0\nassessment['Assessment_3'] = 0\nassessment['Assessment_4'] = 0\nassessment['Assessment_5'] = 0\n\nassessment.loc[assessment.title=='Mushroom Sorter (Assessment)', 'Assessment_1'] = 1\nassessment.loc[assessment.title=='Cauldron Filler (Assessment)', 'Assessment_2'] = 1\nassessment.loc[assessment.title=='Chest Sorter (Assessment)', 'Assessment_3'] = 1\nassessment.loc[assessment.title=='Cart Balancer (Assessment)', 'Assessment_4'] = 1\nassessment.loc[assessment.title=='Bird Measurer (Assessment)', 'Assessment_5'] = 1\ndel assessment[\"title\"]\nassessment = assessment.loc[:, col[\"assessment\"]]\nassessment = assessment.fillna(0)\n\n# time\ntime = pd.DataFrame(columns=[\"installation_id\", \"timestamp\"])\nfor i in tqdm.tqdm_notebook(df_test):\n    # time\n    _ = i.tail(1).loc[:, [\"installation_id\", \"timestamp\"]]\n    time = pd.concat([time, _], sort=False)\n\ntime.reset_index(drop=True, inplace=True)\ntime[\"hour\"] = time.timestamp.dt.hour\ntime[\"hour\"] = time.hour.astype(\"object\")\ntime = time.merge(pd.get_dummies(time.hour, prefix=\"hour\"), how=\"left\", \n                  left_index=True, right_index=True)\ntime.drop(columns=[\"timestamp\", \"hour\"], inplace=True)\ntime = time.loc[:, col[\"time\"]]\ntime = time.fillna(0)\n\n# game\ninstallation_id = []\ngame_title = []\ngame_round = []\n\nfor i in tqdm.tqdm_notebook(df_test):\n    if \"Game\" in i.type.unique():\n        _ = i.query(\"type=='Game'\").loc[:, [\"installation_id\", \"title\", \"event_data\"]].set_index([\"installation_id\", \"title\"]).event_data.apply(lambda x: json.loads(x)[\"round\"]).groupby([\"installation_id\", \"title\"]).max().reset_index()\n        installation_id.extend(list(_.installation_id))\n        game_title.extend(_.title)\n        game_round.extend(_.event_data)\n        \ngame = pd.DataFrame(data={\"installation_id\":installation_id, \"game_title\":game_title, \"round\":game_round})\ngame = game.pivot_table(index=\"installation_id\", columns=\"game_title\", values=\"round\")\ngame.reset_index(inplace=True)\ngame.columns.name = None\ngame = game.fillna(-1)\n\ninstallation_id = pd.DataFrame(data={\"installation_id\":label})\ngame = installation_id.merge(game, how=\"left\")\ngame = game.fillna(-1)\ngame = game.add_suffix(\"_r\")\ngame.rename(columns={\"installation_id_r\":\"installation_id\"}, inplace=True)\ngame = game.loc[:, col[\"game\"]]\ngame = game.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if dp_log==True:\n    world_log.iloc[:, 1:] = np.log(world_log.iloc[:, 1:]+1)\n\n# world_time.drop(columns=[\"NONE\"], inplace=True)\nif dp_log==True:\n    world_time.iloc[:, 1:] = np.log(world_time.iloc[:, 1:]+2)\n\nif dp_log==True:\n    world_session.iloc[:, 1:] = np.log(world_session.iloc[:, 1:]+1)\n\nif dp_log==True:\n    event_id.iloc[:, :-1] = np.log(event_id.iloc[:, :-1]+1)\n#     event_id.iloc[:, 1:] = np.log(event_id.iloc[:, 1:]+1)\n\nif dp_log==True:\n    play_time.iloc[:, 1:] = np.log(play_time.iloc[:, 1:]+1)\n\n# gap_time.drop(columns=[\"max\", \"min\"], inplace=True)\nif dp_log==True:\n    gap_time.iloc[:, 1:] = np.log(gap_time.iloc[:, 1:]+1)\n\nif dp_log==True:\n    session_count.iloc[:, 1:] = np.log(session_count.iloc[:, 1:])\n\nif dp_log==True:\n    session_length.iloc[:, 1:] = np.log(session_length.iloc[:, 1:])\n\nif dp_log==True:\n    session_time.iloc[:, 1:] = np.log(session_time.iloc[:, 1:]+1)\n\nif dp_log==True:\n    types.iloc[:, 1:] = np.log(types.iloc[:, 1:]+1)\n\nif dp_log==True:\n    title.iloc[:, :-1] = np.log(title.iloc[:, :-1]+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = world_log.merge(world_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(world_session, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(event_id, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(play_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(gap_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_count, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_length, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(session_time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(types, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(title, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(assessment, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(time, how=\"left\", on=[\"installation_id\"])\n_ = _.merge(game, how=\"left\", on=[\"installation_id\"])\ntrain_x_col[0] = \"installation_id\"\n_ = _.loc[:, train_x_col]\n_ = _.fillna(-1)\n_.to_csv(\"test.csv\", index=False)\n\ntest_x = scaler.transform(_.loc[:, train_x_col[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result[result <= 1.12232214] = 0\n# result[np.where(np.logical_and(result > 1.12232214, result <= 1.73925866))] = 1\n# result[np.where(np.logical_and(result > 1.73925866, result <= 2.22506454))] = 2\n# result[result > 2.22506454] = 3\n# result = result.astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if loss_type==\"mse\":\n    submission = pd.DataFrame({\"installation_id\":_.installation_id, \"accuracy_group\":result.flatten()})\n    submission.to_csv(\"submission.csv\", index=False)\nelif loss_type==\"category\":\n    submission = pd.DataFrame({\"installation_id\":_.installation_id, \"accuracy_group\":result.argmax(axis=1)})\n    submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.hist(submission.accuracy_group)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"np.unique(submission.accuracy_group, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nThe end of notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}