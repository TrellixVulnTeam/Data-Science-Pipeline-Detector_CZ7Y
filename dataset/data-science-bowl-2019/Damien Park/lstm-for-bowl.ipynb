{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"2019 Data Science Bowl\n===\nDamien Park  \n2020.01.21  \nLSTM"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport keras\n\n# import pprint\nimport gc\nimport os\nimport tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pandas display option\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_row', 1500)\npd.set_option('max_colwidth', 150)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# data load option\ndtypes = {\"event_id\":\"object\", \"game_session\":\"object\", \"timestamp\":\"object\", \n          \"event_data\":\"object\", \"installation_id\":\"object\", \"event_count\":\"int16\", \n          \"event_code\":\"int16\", \"game_time\":\"int32\", \"title\":\"category\", \n          \"type\":\"category\", \"world\":\"category\"}\nlabel = {\"game_session\":\"object\", \"installation_id\":\"object\", \"title\":\"category\", \n         \"num_correct\":\"int8\", \"num_incorrect\":\"int8\", \n         \"accuracy\":\"float16\", \"accuracy_group\":\"int8\"}\n\n# hyper parameter\nloss_type = \"category\" # mse/category\nwindow = 70\nbatch_sizes = 20\nvalidation = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparing"},{"metadata":{},"cell_type":"markdown","source":"### Split data by ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\", dtype=dtypes)\ntest = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\", dtype=dtypes)\nlabel_ = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\", dtype=label)\n# sample = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/sample_submission.csv\")\n# specs = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/specs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating accuracy\nclass accuracy:\n    def __init__(self, df):\n        self.df = df\n\n        \n    # Assessment evaluation-Cart Balancer (Assessment)\n    def cart_assessment(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='d122731b'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cart_assessment_2(self):\n        _ = self.df.query(\"title=='Cart Balancer (Assessment)' and event_id=='b74258a0'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Chest Sorter (Assessment)\n    def chest_assessment(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='93b353f2'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def chest_assessment_2(self):\n        _ = self.df.query(\"title=='Chest Sorter (Assessment)' and event_id=='38074c54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"]=1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Cauldron Filler (Assessment)\n    def cauldron_assessment(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='392e14df'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n\n    def cauldron_assessment_2(self):\n        _ = self.df.query(\"title=='Cauldron Filler (Assessment)' and event_id=='28520915'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Mushroom Sorter (Assessment)\n    def mushroom_assessment(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='25fa8af4'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def mushroom_assessment_2(self):\n        _ = self.df.query(\"title=='Mushroom Sorter (Assessment)' and event_id=='6c930e6e'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n    \n    \n    # Assessment evaluation-Bird Measurer (Assessment)\n    def bird_assessment(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='17113b36'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"correct\"] = _.event_data.apply(lambda x:(json.loads(x)[\"correct\"] if \"correct\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 0\n        _[\"num_incorrect_\"] = 0\n        _.loc[_.correct==True, \"num_correct_\"] = 1\n        _.loc[_.correct==False, \"num_incorrect_\"] = 1\n        _ = _.groupby([\"installation_id\", \"game_session\"]).sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"num_incorrect_\"])\n        _[\"accuracy_group\"] = _[\"num_incorrect_\"].apply(lambda x : 3 if x==0 else (2 if x==1 else 1))*_[\"num_correct_\"]\n\n#         return _.loc[:, [\"installation_id\", \"game_session\", \"num_correct_\", \"num_incorrect_\", \"accuracy_\", \"accuracy_group\"]]\n        return _.loc[:, [\"installation_id\", \"game_session\", \"accuracy_group\"]]\n    \n    def bird_assessment_2(self):\n        _ = self.df.query(\"title=='Bird Measurer (Assessment)' and event_id=='f6947f54'\")\n        _ = _.loc[:, [\"game_session\", \"installation_id\", \"event_data\"]]\n        _[\"misses\"] = _.event_data.apply(lambda x:(json.loads(x)[\"misses\"] if \"misses\" in json.loads(x).keys() else -999))\n        _[\"num_correct_\"] = 1\n        _ = _.groupby(\"game_session\").sum().reset_index()\n        _[\"accuracy_\"] = _[\"num_correct_\"]/(_[\"num_correct_\"]+_[\"misses\"])\n\n        return _.loc[:, [\"game_session\", \"num_correct_\", \"misses\", \"accuracy_\"]]\n\n# quadratic kappa\ndef quadratic_kappa(actuals, preds, N=4):\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E/E.sum();\n    O = O/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num/den))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"timestamp\"] = pd.to_datetime(test.timestamp)\ntest.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\n\n_ = accuracy(test).cart_assessment()\n_ = _.append(accuracy(test).chest_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).cauldron_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).mushroom_assessment(), ignore_index=True)\n_ = _.append(accuracy(test).bird_assessment(), ignore_index=True)\n\ntest = test[test.installation_id.isin(pd.unique(_.installation_id))]\ntest = test.merge(_, how=\"left\", on=[\"installation_id\", \"game_session\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = []\nidx = 0\nfor _, val in tqdm.tqdm_notebook(test.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(test.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    _ = val.query(\"type=='Assessment'\")\n    _ = _[~_.accuracy_group.isnull()]\n    session = _.reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        sample = val[:j+1]\n        sample[\"ID\"] = idx\n        idx += 1\n        df_test.append(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = pd.DataFrame(columns=[\"ID\", \"accuracy_group\"])\nfor i in tqdm.tqdm_notebook(df_test):\n# for i in tqdm.notebook.tqdm(df_test):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train[train.installation_id.isin(pd.unique(label_.installation_id))]\ndel train\ndf = df.merge(label_.loc[:, [\"installation_id\", \"game_session\", \"title\", \"accuracy_group\"]], \n              on=[\"installation_id\", \"game_session\", \"title\"], how=\"left\")\ndf[\"timestamp\"] = pd.to_datetime(df.timestamp)\ndf.sort_values([\"timestamp\", \"event_count\"], ascending=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = []\nidx = max(label.ID)+1\nfor _, val in tqdm.tqdm_notebook(df.groupby(\"installation_id\", sort=False)):\n# for _, val in tqdm.notebook.tqdm(df.groupby(\"installation_id\", sort=False)):\n    val.reset_index(drop=True, inplace=True)\n    session = val.query(\"type=='Assessment'\").reset_index().groupby(\"game_session\", sort=False).index.first().values\n    for j in session:\n        if ~np.isnan(val.iat[j, -1]):\n            sample = val[:j+1]\n            sample[\"ID\"] = idx\n            idx += 1\n            df_train.append(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm_notebook(df_train):\n# for i in tqdm.notebook.tqdm(df_train):\n    label = pd.concat([label, i.iloc[-1:, -2:]], sort=False)\n\nlabel.reset_index(drop=True, inplace=True)\nlabel.accuracy_group = label.accuracy_group.astype(\"int8\")\nlabel = label.merge(pd.get_dummies(label.accuracy_group, prefix=\"y\"), left_on=[\"ID\"], right_index=True)\n\ndf_test.extend(df_train)\ndf_train = df_test\ndel df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train[0].head()), display(label.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aligned data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"event_id_col = ['003cd2ee', '0086365d', '00c73085', '01ca3a3c', '022b4259',\n                '02a42007', '0330ab6a', '0413e89d', '04df9b66', '05ad839b',\n                '06372577', '070a5291', '08fd73f3', '08ff79ad', '0a08139c',\n                '0ce40006', '0d18d96c', '0d1da71f', '0db6d71d', '119b5b02',\n                '1325467d', '1340b8d7', '1375ccb7', '13f56524', '14de4c5d',\n                '155f62a4', '1575e76c', '15a43e5b', '15ba1109', '15eb4a7d',\n                '15f99afc', '160654fd', '16667cc5', '16dffff1', '17113b36',\n                '17ca3959', '19967db1', '1996c610', '1af8be29', '1b54d27f',\n                '1bb5fbdb', '1beb320a', '1c178d24', '1cc7cfca', '1cf54632',\n                '1f19558b', '222660ff', '2230fab4', '250513af', '25fa8af4',\n                '262136f4', '26a5a3dd', '26fd2d99', '27253bdc', '28520915',\n                '28a4eb9a', '28ed704e', '28f975ea', '29a42aea', '29bdd9ba',\n                '29f54413', '2a444e03', '2a512369', '2b058fe3', '2b9272f4',\n                '2c4e6db0', '2dc29e21', '2dcad279', '2ec694de', '2fb91ec1',\n                '30614231', '30df3273', '31973d56', '3323d7e9', '33505eae',\n                '3393b68b', '363c86c9', '363d3849', '36fa3ebe', '37937459',\n                '37c53127', '37db1c2f', '37ee8496', '38074c54', '392e14df',\n                '3a4be871', '3afb49e6', '3afde5dd', '3b2048ee', '3babcb9b',\n                '3bb91ced', '3bb91dda', '3bf1cf26', '3bfd1a65', '3ccd3f02',\n                '3d0b9317', '3d63345e', '3d8c61b0', '3dcdda7f', '3ddc79c3',\n                '3dfd4aa4', '3edf6747', '3ee399c3', '4074bac2', '44cb4907',\n                '45d01abe', '461eace6', '46b50ba8', '46cd75b4', '47026d5f',\n                '47efca07', '47f43a44', '48349b14', '4901243f', '499edb7c',\n                '49ed92e9', '4a09ace1', '4a4c3d21', '4b5efe37', '4bb2f698',\n                '4c2ec19f', '4d6737eb', '4d911100', '4e5fc6f5', '4ef8cdd3',\n                '51102b85', '51311d7a', '5154fc30', '5290eab1', '532a2afb',\n                '5348fd84', '53c6e11a', '55115cbd', '562cec5f', '565a3990',\n                '56817e2b', '56bcd38d', '56cd3b43', '5859dfb6', '587b5989',\n                '58a0de5c', '598f4598', '5a848010', '5b49460a', '5be391b5',\n                '5c2f29ca', '5c3d2b2f', '5d042115', '5dc079d8', '5de79a6a',\n                '5e109ec3', '5e3ea25a', '5e812b27', '5f0eb72c', '5f5b2617',\n                '6043a2b4', '6077cc36', '6088b756', '611485c5', '63f13dd7',\n                '65a38bf7', '65abac75', '67439901', '67aa2ada', '69fdac0a',\n                '6aeafed4', '6bf9e3e1', '6c517a88', '6c930e6e', '6cf7d25c',\n                '6d90d394', '6f445b57', '6f4adc4b', '6f4bd64e', '6f8106d9',\n                '7040c096', '709b1251', '71e712d8', '71fe8f75', '731c0cbe',\n                '736f9581', '7372e1a5', '73757a5e', '7423acbc', '74e5f8a7',\n                '7525289a', '756e5507', '763fc34e', '76babcde', '77261ab5',\n                '77c76bc5', '77ead60d', '792530f8', '795e4a37', '7961e599',\n                '7ab78247', '7ad3efc6', '7cf1bc53', '7d093bf9', '7d5c30a2',\n                '7da34a02', '7dfe6d8a', '7ec0c298', '7f0836bf', '7fd1ac25',\n                '804ee27f', '828e68f9', '832735e1', '83c6c409', '84538528',\n                '84b0e0c8', '857f21c0', '85d1b0de', '85de926c', '86ba578b',\n                '86c924c4', '87d743c1', '884228c8', '88d4a5be', '895865f3',\n                '89aace00', '8ac7cce4', '8af75982', '8b757ab8', '8d748b58',\n                '8d7e386c', '8d84fa81', '8f094001', '8fee50e2', '907a054b',\n                '90d848e0', '90ea0bac', '90efca10', '91561152', '923afab1',\n                '92687c59', '93b353f2', '93edfe2e', '9554a50b', '9565bea6',\n                '99abe2bb', '99ea62f3', '9b01374f', '9b23e8ee', '9b4001e4',\n                '9c5ef70c', '9ce586dd', '9d29771f', '9d4e7b25', '9de5e594',\n                '9e34ea74', '9e4c8c7b', '9e6b7fb5', '9ed8f6da', '9ee1c98c',\n                'a0faea5d', 'a1192f43', 'a16a373e', 'a1bbe385', 'a1e4395d',\n                'a29c5338', 'a2df0760', 'a44b10dc', 'a52b92d5', 'a592d54e',\n                'a5be6304', 'a5e9da97', 'a6d66e51', 'a76029ee', 'a7640a16',\n                'a8876db3', 'a8a78786', 'a8cc6fec', 'a8efe47b', 'ab3136ba',\n                'ab4ec3a4', 'abc5811c', 'ac92046e', 'acf5c23f', 'ad148f58',\n                'ad2fc29c', 'b012cd7f', 'b120f2ac', 'b1d5101d', 'b2dba42b',\n                'b2e5b0f1', 'b5053438', 'b738d3d3', 'b74258a0', 'b7530680',\n                'b7dc8128', 'b80e5e84', 'b88f38da', 'bb3e370b', 'bbfe0445',\n                'bc8f2793', 'bcceccc6', 'bd612267', 'bd701df8', 'bdf49a58',\n                'beb0a7b9', 'bfc77bd6', 'c0415e5c', 'c189aaf2', 'c1cac9a2',\n                'c277e121', 'c2baf0bd', 'c51d8688', 'c54cf6c5', 'c58186bf',\n                'c6971acf', 'c7128948', 'c74f40cd', 'c7f7f0e1', 'c7fe2a55',\n                'c952eb01', 'ca11f653', 'cb1178ad', 'cb6010f8', 'cc5087a3',\n                'cdd22e43', 'cf7638f3', 'cf82af56', 'cfbd47c8', 'd02b7a8e',\n                'd06f75b5', 'd122731b', 'd185d3ea', 'd2278a3b', 'd2659ab4',\n                'd2e9262e', 'd3268efa', 'd3640339', 'd38c2fd7', 'd3f1e122',\n                'd45ed6a1', 'd51b1749', 'd88ca108', 'd88e8f25', 'd9c005dd',\n                'daac11b0', 'db02c830', 'dcaede90', 'dcb1663e', 'dcb55a27',\n                'de26c3a6', 'df4940d3', 'df4fe8b6', 'e04fb33d', 'e080a381',\n                'e37a2b78', 'e3ff61fb', 'e4d32835', 'e4f1efe6', 'e5734469',\n                'e57dd7af', 'e5c9df6f', 'e64e2cfd', 'e694a35b', 'e720d930',\n                'e7561dd2', 'e79f3763', 'e7e44842', 'e9c52111', 'ea296733',\n                'ea321fb1', 'eb2c19cd', 'ec138c1c', 'ecaab346', 'ecc36b7f',\n                'ecc6157f', 'f28c589a', 'f32856e4', 'f3cd5473', 'f50fc6c1',\n                'f54238ee', 'f56e0afc', 'f5b8c21a', 'f6947f54', 'f71c4741',\n                'f7e47413', 'f806dc10', 'f93fc684', 'fbaf3456', 'fcfdffb6',\n                'fd20ea40']\n\nevent_code_col = [2000, 2010, 2020, 2025, 2030, 2035, 2040, \n                  2050, 2060, 2070, 2075, 2080, 2081, 2083, \n                  3010, 3020, 3021, 3110, 3120, 3121, 4010, \n                  4020, 4021, 4022, 4025, 4030, 4031, 4035, \n                  4040, 4045, 4050, 4070, 4080, 4090, 4095, \n                  4100, 4110, 4220, 4230, 4235, 5000, 5010]\n\nworld_col = [\"NONE\", \"CRYSTALCAVES\", \"MAGMAPEAK\", \"TREETOPCITY\"]\n\ntype_col = [\"Activity\", \"Assessment\", \"Clip\", \"Game\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(event_code_col), len(world_col), len(type_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# event_code\ndf = []\nmax_ = 0\nfor val in tqdm.tqdm_notebook(df_train):\n# for val in tqdm.notebook.tqdm(df_train):\n    # game_session\n    game_session = val[-2:].game_session.values\n    val = val.query(\"game_session in @game_session\")\n    # event_code\n    event_code = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].event_code).loc[:, event_code_col].fillna(0).astype(\"int8\").values\n    event_code = np.append(event_code, np.zeros((window-event_code.shape[0], 42)), axis=0)\n    # world\n    world = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].world).loc[:, world_col].fillna(0).astype(\"int8\").values\n    world = np.append(world, np.zeros((window-world.shape[0], 4)), axis=0)\n    # type\n    types = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].type).loc[:, type_col].fillna(0).astype(\"int8\").values\n    types = np.append(types, np.zeros((window-types.shape[0], 4)), axis=0)\n    # game_time\n    game_time = val.timestamp.diff()[::-1].reset_index(drop=True)[:window].values\n    game_time[np.isnat(game_time)] = 0\n    game_time = game_time.reshape(len(game_time), 1)\n    game_time\n#     game_time = MinMaxScaler().fit_transform(game_time)\n    game_time.dtype = \"int\"\n    if max_<game_time.max():\n        max_ = game_time.max()\n    game_time = np.append(game_time, np.zeros((window-game_time.shape[0], 1)), axis=0)\n    \n    _ = np.hstack((event_code, world, types, game_time))\n    \n    if val[\"title\"].iloc[-1] == 'Mushroom Sorter (Assessment)':\n        _ = np.hstack((_, np.array([1, 0, 0, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1] == 'Cauldron Filler (Assessment)':\n        _ = np.hstack((_, np.array([0, 1, 0, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1] == 'Chest Sorter (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 1, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1] == 'Cart Balancer (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 0, 1, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1] == 'Bird Measurer (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 0, 0, 1])*np.ones((window, 5))))\n\n    df.append(np.flip(_, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = np.array(df)\ndf[:, :, -6] = df[:, :, -6]/max_\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### class weight / Train and validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"label.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove 8 data for fit\ndf = df[:19700]\nlabel = label.loc[:19699, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = np.unique(label.accuracy_group, return_counts=True)\nclass_weight_ = {_[0][0]:_[1][0]/len(label), \n                 _[0][1]:_[1][1]/len(label), \n                 _[0][2]:_[1][2]/len(label), \n                 _[0][3]:_[1][3]/len(label)}\nclass_weight_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(label.accuracy_group),\n                                                  label.accuracy_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(label.accuracy_group), class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label[\"class_weight\"] = 0\n# label.loc[label.accuracy_group==0, \"class_weight\"] = class_weight_[0]\n# label.loc[label.accuracy_group==1, \"class_weight\"] = class_weight_[1]\n# label.loc[label.accuracy_group==2, \"class_weight\"] = class_weight_[2]\n# label.loc[label.accuracy_group==3, \"class_weight\"] = class_weight_[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label[\"class_weight\"] = 0\nlabel.loc[label.accuracy_group==0, \"class_weight\"] = class_weights[0]\nlabel.loc[label.accuracy_group==1, \"class_weight\"] = class_weights[1]\nlabel.loc[label.accuracy_group==2, \"class_weight\"] = class_weights[2]\nlabel.loc[label.accuracy_group==3, \"class_weight\"] = class_weights[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    train_x, val_x, train_y, val_y = train_test_split(df, label, test_size=4700, random_state=1228)\n    plt.hist(train_y.accuracy_group, align=\"left\", label=\"train\", alpha=.7)\n    plt.hist(val_y.accuracy_group, align=\"right\", label=\"val\", alpha=.7)\n    plt.show()\n    display(train_x.shape, train_y.shape)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# import keras.backend as K\n\n# def quadratic_kappa(y_true, y_pred, N=4):\n#     w = np.zeros((4,4))\n#     O = confusion_matrix(y_true, y_pred)\n#     for i in range(len(w)): \n#         for j in range(len(w)):\n#             w[i][j] = float(((i-j)**2)/(4-1)**2)\n    \n#     act_hist=np.zeros([4])\n#     for item in actuals: \n#         act_hist[item]+=1\n    \n#     pred_hist=np.zeros([4])\n#     for item in preds: \n#         pred_hist[item]+=1\n                         \n#     E = np.outer(act_hist, pred_hist);\n#     E = E/E.sum();\n#     O = O/O.sum();\n    \n#     num=0\n#     den=0\n#     for i in range(len(w)):\n#         for j in range(len(w)):\n#             num+=w[i][j]*O[i][j]\n#             den+=w[i][j]*E[i][j]\n#     return K.mean(1 - (num/den))\n\n# import keras.backend as K\n# def _cohen_kappa(y_true, y_pred, num_classes, weights=None, metrics_collections=None, updates_collections=None, name=None):\n#     kappa, update_op = tf.contrib.metrics.cohen_kappa(y_true, y_pred, num_classes, weights, metrics_collections, updates_collections, name)\n#     K.get_session().run(tf.local_variables_initializer())\n#     with tf.control_dependencies([update_op]):\n#         kappa = tf.identity(kappa)\n#     return kappa\n\n# def cohen_kappa_loss(num_classes, weights=None, metrics_collections=None, updates_collections=None, name=None):\n#     def cohen_kappa(y_true, y_pred):\n#         return -_cohen_kappa(y_true, y_pred, num_classes, weights, metrics_collections, updates_collections, name)\n#     return cohen_kappa\n# def mean_pred(y_true, y_pred):\n#     return K.mean(y_pred)\n\n# model.compile(optimizer='rmsprop',\n#               loss='binary_crossentropy',\n#               metrics=['accuracy', mean_pred])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RNN = keras.models.Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN.add(keras.layers.LSTM(units=30, stateful=False, return_sequences=True, return_state=False, \n#                           recurrent_dropout=.2, batch_input_shape=(batch_sizes, df.shape[1], df.shape[2])))\n# RNN.add(keras.layers.LSTM(units=20, stateful=False, return_sequences=True, return_state=False, \n#                           recurrent_dropout=.2))\n# # RNN.add(keras.layers.LSTM(units=10, stateful=False, return_sequences=True, return_state=False, \n# #                           recurrent_dropout=.2))\n# RNN.add(keras.layers.TimeDistributed(keras.layers.Dense(units=10, activation=\"relu\", \n#                                                         kernel_initializer=\"he_normal\")))\n# RNN.add(keras.layers.LSTM(units=5, stateful=False, return_sequences=False, return_state=False, \n#                           recurrent_dropout=.2))\n# RNN.add(keras.layers.Dense(units=10, activation=\"relu\", kernel_initializer=\"he_normal\"))\n# RNN.add(keras.layers.Dropout(.2))\n# RNN.add(keras.layers.Dense(units=10, activation=\"relu\", kernel_initializer=\"he_normal\"))\n\n# if loss_type==\"mse\":\n#     RNN.add(keras.layers.Dense(units=1, activation=\"relu\", name=\"output\"))\n# elif loss_type==\"category\":\n#     RNN.add(keras.layers.Dense(units=4, activation=\"softmax\", name=\"output\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN.add(keras.layers.LSTM(units=50, stateful=False, return_sequences=True, return_state=False, \n#                           dropout=.2, recurrent_dropout=.2, \n#                           batch_input_shape=(batch_sizes, df.shape[1], df.shape[2])))\n# RNN.add(keras.layers.LSTM(units=20, stateful=False, return_sequences=False, return_state=False, \n#                           dropout=.2, recurrent_dropout=.2))\n# RNN.add(keras.layers.Dense(units=10, activation=\"relu\", kernel_initializer=\"he_normal\"))\n# RNN.add(keras.layers.Dropout(.2))\n\n# if loss_type==\"mse\":\n#     RNN.add(keras.layers.Dense(units=1, activation=\"relu\", name=\"output\"))\n# elif loss_type==\"category\":\n#     RNN.add(keras.layers.Dense(units=4, activation=\"softmax\", name=\"output\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RNN.add(keras.layers.LSTM(units=30, stateful=False, return_sequences=True, return_state=False, \n                          dropout=.3, recurrent_dropout=.3, batch_input_shape=(batch_sizes, df.shape[1], df.shape[2])))\nRNN.add(keras.layers.LSTM(units=20, stateful=False, return_sequences=True, return_state=False, \n                          dropout=.3, recurrent_dropout=.3))\nRNN.add(keras.layers.TimeDistributed(keras.layers.Dense(units=10, activation=\"relu\", \n                                                        kernel_initializer=\"he_normal\")))\nRNN.add(keras.layers.Flatten())\nRNN.add(keras.layers.Dense(units=10, activation=\"relu\", kernel_initializer=\"he_normal\"))\nRNN.add(keras.layers.Dropout(.3))\nRNN.add(keras.layers.Dense(units=10, activation=\"relu\", kernel_initializer=\"he_normal\"))\n\nif loss_type==\"mse\":\n    RNN.add(keras.layers.Dense(units=1, activation=\"relu\", name=\"output\"))\nelif loss_type==\"category\":\n    RNN.add(keras.layers.Dense(units=4, activation=\"softmax\", name=\"output\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if loss_type==\"mse\":\n    RNN.compile(loss=\"mse\", optimizer=\"rmsprop\")\nelif loss_type==\"category\":\n    RNN.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", \n                metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n# keras.backend.reset_uids()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"RNN.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf \"/kaggle/working/model/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(\"model\"):\n    os.mkdir(\"model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation==False:\n    if loss_type==\"mse\":\n        RNN.fit(x=df[:19700], y=label.loc[:19699, [\"accuracy_group\"]], \n                epochs=10, batch_size=batch_sizes, shuffle=True, class_weight=class_weight)\n    elif loss_type==\"category\":\n        RNN.fit(x=df[:19700], y=label.loc[:19699, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]],\n                epochs=1000, batch_size=batch_sizes, shuffle=True, \n                sample_weight=label.loc[:, [\"class_weight\"]].values.flatten(), \n                callbacks=[keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", \n                                                         patience=50, mode=\"auto\"), \n                           keras.callbacks.ModelCheckpoint(\"model/weights.{epoch:02d}-{categorical_accuracy:.3f}.hdf5\", \n                                                           monitor='categorical_accuracy', \n                                                           verbose=0, save_best_only=True, save_weights_only=False, \n                                                           mode=\"auto\", period=1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    if loss_type==\"mse\":\n        RNN.fit(x=train_x, y=train_y.loc[:, [\"accuracy_group\"]], \n                validation_data=[val_x, val_y.loc[:, [\"accuracy_group\"]]], \n                epochs=50, batch_size=batch_sizes, shuffle=True, class_weight=class_weight)\n    elif loss_type==\"category\":\n        RNN.fit(x=train_x, y=train_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]], \n                validation_data=[val_x, val_y.loc[:, [\"y_0\", \"y_1\", \"y_2\", \"y_3\"]]], \n                epochs=1000, batch_size=batch_sizes, shuffle=True, \n                sample_weight=train_y.loc[:, [\"class_weight\"]].values.flatten(), \n                callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", \n                                                         patience=50, mode=\"auto\"), \n                           keras.callbacks.ModelCheckpoint(\"model/weights.{epoch:02d}-{val_categorical_accuracy:.3f}.hdf5\", \n                                                           monitor='val_categorical_accuracy', \n                                                           verbose=0, save_best_only=True, save_weights_only=False, \n                                                           mode=\"auto\", period=1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    if loss_type==\"mse\":\n        plt.figure(figsize=(20, 10))\n        plt.plot(RNN.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n        plt.plot(RNN.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n        plt.axhline(1.2, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.show()\n    elif loss_type==\"category\":\n        plt.figure(figsize=(20, 10))\n        plt.subplot(2, 1, 1)\n        plt.plot(RNN.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n        plt.plot(RNN.history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\n        plt.axhline(1.25, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.subplot(2, 1, 2)\n        plt.plot(RNN.history.history[\"categorical_accuracy\"], \"o-\", alpha=.4, label=\"categorical_accuracy\")\n        plt.plot(RNN.history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.4, label=\"val_categorical_accuracy\")\n        plt.axhline(.6, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.show()\n        \nif not validation:\n    if loss_type==\"mse\":\n        plt.figure(figsize=(20, 10))\n        plt.plot(RNN.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n        plt.axhline(1.2, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.show()\n    elif loss_type==\"category\":\n        plt.figure(figsize=(20, 10))\n        plt.subplot(2, 1, 1)\n        plt.plot(RNN.history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\n        plt.axhline(1.25, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.subplot(2, 1, 2)\n        plt.plot(RNN.history.history[\"categorical_accuracy\"], \"o-\", alpha=.4, label=\"categorical_accuracy\")\n        plt.axhline(.6, linestyle=\"--\", c=\"C2\")\n        plt.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RNN = keras.models.load_model(\"model/\"+os.listdir(\"model\")[-2])\nos.listdir(\"model\")[-2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"result = RNN.predict(df, batch_size=batch_sizes)\nplt.figure(figsize=(20, 10))\nif loss_type==\"mse\":\n    plt.hist(result, alpha=.7, label=\"predict\")\n    plt.hist(label.accuracy_group, alpha=.7, label=\"real\")\n    plt.legend()\nelif loss_type==\"category\":\n    plt.hist(result.argmax(axis=1), alpha=.4, align=\"left\", label=\"predict\")\n    plt.hist(label.accuracy_group, alpha=.4, align=\"right\", label=\"real\")\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation:\n    result = RNN.predict(val_x, batch_size=batch_sizes)\n    plt.figure(figsize=(20, 10))\n    if loss_type==\"mse\":\n        plt.hist(result, alpha=.7, label=\"predict\")\n        plt.hist(val_y.accuracy_group, alpha=.7, label=\"real\")\n        plt.legend()\n    elif loss_type==\"category\":\n        plt.hist(result.argmax(axis=1), alpha=.4, align=\"left\", label=\"predict\")\n        plt.hist(val_y.accuracy_group, alpha=.4, align=\"right\", label=\"real\")\n        plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if validation and loss_type==\"category\":\n    print(np.unique(result.argmax(axis=1), return_counts=True))\n    print(quadratic_kappa(val_y.accuracy_group.reset_index(drop=True), result.argmax(axis=1)))\n    print(confusion_matrix(val_y.accuracy_group.reset_index(drop=True), result.argmax(axis=1)))\n    print(confusion_matrix(result.argmax(axis=1), val_y.accuracy_group.reset_index(drop=True)))\n    \nif not validation and loss_type==\"category\":\n    print(np.unique(result.argmax(axis=1), return_counts=True))\n    print(quadratic_kappa(label.accuracy_group.reset_index(drop=True), result.argmax(axis=1)))\n    print(confusion_matrix(label.accuracy_group.reset_index(drop=True), result.argmax(axis=1)))\n    print(confusion_matrix(result.argmax(axis=1), label.accuracy_group.reset_index(drop=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"del train_x, val_x, train_y, val_y, result\ngc.collect()\n\ntest = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\", dtype=dtypes)\ntest[\"timestamp\"] = pd.to_datetime(test.timestamp)\n\n# event_code\ndf_test = []\ndf_id = []\nfor idx, val in tqdm.tqdm_notebook(test.groupby(\"installation_id\", sort=False)):\n# for idx, val in tqdm.notebook.tqdm(test.groupby(\"installation_id\", sort=False)):\n    df_id.append(idx)\n    # game_session\n    game_session = val[-2:].game_session.values\n    val = val.query(\"game_session in @game_session\")\n    # event_code\n    event_code = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].event_code).loc[:, event_code_col].fillna(0).astype(\"int8\").values\n    event_code = np.append(event_code, np.zeros((window-event_code.shape[0], 42)), axis=0)\n    # world\n    world = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].world).loc[:, world_col].fillna(0).astype(\"int8\").values\n    world = np.append(world, np.zeros((window-world.shape[0], 4)), axis=0)\n    # type\n    types = pd.get_dummies(val[::-1].reset_index(drop=True)[:window].type).loc[:, type_col].fillna(0).astype(\"int8\").values\n    types = np.append(types, np.zeros((window-types.shape[0], 4)), axis=0)\n    # game_time\n    game_time = val.timestamp.diff()[::-1].reset_index(drop=True)[:window].values\n    game_time[np.isnat(game_time)] = 0\n    game_time = game_time.reshape(len(game_time), 1)\n#     game_time = MinMaxScaler().fit_transform(game_time)\n    game_time.dtype = \"int\"\n    game_time = np.append(game_time, np.zeros((window-game_time.shape[0], 1)), axis=0)\n    \n    _ = np.hstack((event_code, world, types, game_time))\n    \n    if val[\"title\"].iloc[-1]=='Mushroom Sorter (Assessment)':\n        _ = np.hstack((_, np.array([1, 0, 0, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1]=='Cauldron Filler (Assessment)':\n        _ = np.hstack((_, np.array([0, 1, 0, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1]=='Chest Sorter (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 1, 0, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1]=='Cart Balancer (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 0, 1, 0])*np.ones((window, 5))))\n    elif val[\"title\"].iloc[-1]=='Bird Measurer (Assessment)':\n        _ = np.hstack((_, np.array([0, 0, 0, 0, 1])*np.ones((window, 5))))\n\n    df_test.append(np.flip(_, axis=0))\n\ndf_test = np.array(df_test)\ndf_test[:, :, -6] = df_test[:, :, -6]/max_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = RNN.predict(df_test, batch_size=batch_sizes)\nif loss_type==\"mse\":\n    plt.figure(figsize=(20, 10))\n    sns.distplot(result)\n    plt.show()\nelif loss_type==\"category\":\n    plt.figure(figsize=(20, 10))\n    plt.hist(result.argmax(axis=1))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if loss_type==\"mse\":\n    _ = pd.qcut(result.flatten(), 14)\n    result[result<=_.categories[2].right] = 0\n    result[np.where(np.logical_and(result>_.categories[2].right, \n                                   result<=_.categories[4].right))] = 1\n    result[np.where(np.logical_and(result>_.categories[4].right, \n                                   result<=_.categories[6].right))] = 2\n    result[result>_.categories[6].right] = 3\n    result = result.astype(\"int\")\n    \n#     result[result<= 1.12232214] = 0\n#     result[np.where(np.logical_and(result>1.12232214, \n#                                    result<=1.73925866))] = 1\n#     result[np.where(np.logical_and(result>1.73925866, \n#                                    result<=2.22506454))] = 2\n#     result[result> 2.22506454] = 3\n#     result = result.astype(\"int\")\n\n    submission = pd.DataFrame({\"installation_id\":df_id, \"accuracy_group\":result.flatten()})\n    submission.to_csv(\"submission.csv\", index=False)\nelif loss_type==\"category\":\n    submission = pd.DataFrame({\"installation_id\":df_id, \"accuracy_group\":result.argmax(axis=1)})\n    submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.hist(submission.accuracy_group)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(submission.accuracy_group, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nThe end of notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}