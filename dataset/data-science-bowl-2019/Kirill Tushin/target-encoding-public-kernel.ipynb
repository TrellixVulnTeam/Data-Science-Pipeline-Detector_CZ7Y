{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/KirillTushin/target_encoding\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nos.chdir('../input/target-encoding')\nfrom target_encoding import TargetEncoderClassifier, TargetEncoder\nos.chdir('/kaggle/working')\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', usecols=keep_cols)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group and Reduce"},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = group_and_reduce(train)\ntest = group_and_reduce(test)\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_labels[['installation_id', 'accuracy_group']]\ntrain = train.merge(labels, how='left', on='installation_id').dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(\n    train.drop(['installation_id', 'accuracy_group'], axis=1),\n    train['accuracy_group'],\n    test_size=0.15,\n    random_state=2019,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_uniques = []\ntrain_labeled = train.fillna(-999)\ntest_labeled = test.fillna(-999)\n\nfor c in train.columns.drop(['installation_id', 'accuracy_group']):\n    le = LabelEncoder()\n    le.fit(pd.concat([train_labeled[c], test_labeled[c]])) \n    train_labeled[c] = le.transform(train_labeled[c])\n    test_labeled[c] = le.transform(test_labeled[c])\n    len_uniques.append(len(le.classes_))\n\nx_train_labeled, x_val_labeled = train_test_split(\n    train_labeled.drop(['installation_id', 'accuracy_group'], axis=1),\n    test_size=0.15,\n    random_state=2019,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALPHA = 10\nMAX_UNIQUE = 50\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsplit: list of int or cross-validator class,\n            if split is [], then algorithm will encode features without cross-validation\n            This situation features will overfit on target\n\n            if split len is 1 for example [5], algorithm will encode features by using cross-validation on 5 folds\n            This situation you will not overfit on tests, but when you will validate, your score will overfit\n\n            if split len is 2 for example [5, 3], algorithm will separate data on 5 folds, afterwords\n            will encode features by using cross-validation on 3 folds\n            This situation is the best way to avoid overfit, but algorithm will use small data for encode.\n'''\n\n\nenc = TargetEncoder(alpha=ALPHA, max_unique=MAX_UNIQUE, split=[cv])\nx_train_encoded = enc.transform_train(x_train_labeled, y=y_train)\nx_val_encoded = enc.transform_test(x_val_labeled)\nx_test_encoded = enc.transform_test(test.drop(['installation_id'], axis=1))\n\nx_train_encoded = pd.DataFrame(x_train_encoded)\nx_val_encoded = pd.DataFrame(x_val_encoded)\nx_test_encoded = pd.DataFrame(x_test_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_all = pd.concat([x_train.reset_index(drop=True), x_train_encoded], axis=1)\nx_val_all = pd.concat([x_val.reset_index(drop=True), x_val_encoded], axis=1)\nx_test_all = pd.concat([test.drop(['installation_id'], axis=1), x_test_encoded], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(x_train_all, y_train)\nval_set = lgb.Dataset(x_val_all, y_val)\n\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.9,\n    'feature_fraction': 0.9,\n    'num_leaves': 14,\n    'lambda_l1': 0.1,\n    'lambda_l2': 1,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\n\nmodel = lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=300, valid_sets=[train_set, val_set], verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = model.predict(x_val_all).argmax(axis=1)\nprint(classification_report(y_val, val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_test_all).argmax(axis=1)\ntest['accuracy_group'] = y_pred\ntest[['installation_id', 'accuracy_group']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}