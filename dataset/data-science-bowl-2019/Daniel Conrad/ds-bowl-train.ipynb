{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/data-science-bowl-2019/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If quick set to true, only uses 5% of training data. \nquick = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To save time, we will only use 5% of the installation_ids for preparing the model.\n\nimport random\n\nif quick:\n    print(quick)\n    # Grab all unique item_nbr from items file\n    f = train['installation_id'].unique()\n\n    # Count the lines\n    num_lines = f.size\n    \n\n    # Sample size - in this case ~5% of items\n    size = int(num_lines / 20)\n\n    # Grab a random subset of size size from f\n    skip_idx = random.sample(list(f), size)\n    print(len(skip_idx))\n\n    # Filter to only include training data for the subset of items we want\n    train = train[train['installation_id'].isin(skip_idx)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store the installation_ids separately so that we can extract the train and test data from our features DataFrame \ntrain_ids = train.installation_id\ntest_ids = test.installation_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleanup\n\n### Target Variable\nFor this competition, we are asked to predict each child's performance on the last assessment they take. It appears we are not given this information directly in the training data, so we'll need to extract the target variable ourselves. We do this by extracting all the assessment completion events (code 4100 or 4110 for Bird Measurer.) We sort these events by timestamp to grab the last one for each user. We then pull all completion attempts in that game session to get the full picture of how the child performed on their last assessment. With this, we can assign each installation_id with the target variable that we'll need to train against."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because the Bird Measurer assessment uses a distinct event code, let's separate it from the data\nbird_measure_assess = train[train['title'] == \"Bird Measurer (Assessment)\"]\n\n# Capture bird_meas assessment attempts\nbird_measure_assess = bird_measure_assess[bird_measure_assess['event_code'] == 4110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab all assessment attempts that are not for bird measurer\ntrain_assess = train[train['title'] != \"Bird Measurer (Assessment)\"]\n\ntrain_assess = train_assess[train_assess['event_code'] == 4100]\n\n# Some non-assessment activities have a 4100 code, ignore those\ntrain_assess = train_assess[train_assess['type'] == 'Assessment']\n\n# Append bird measure assessment attempts to have a log of all assessment attempts\ntrain_assess = train_assess.append(bird_measure_assess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get each child's performance on their last assessment, extract only the last assessment per installation_id\nlast_assessments = train_assess.sort_values(by=\"timestamp\").drop_duplicates(subset=[\"installation_id\"], keep=\"last\")\n\n# Grab the game_sessions of the last assessment events\nlast_game_sessions = last_assessments.game_session.unique()\n\n# Extract all assessment completion attempts in the final session\nlast_assessment_sessions = train_assess[train_assess['game_session'].isin(last_game_sessions)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We want to grab all the last game_sessions of the test data. This will allow us to create features on all the test data except for the last assessment attempt\ntest_assess = test[test.type == 'Assessment']\n\nlast_test_assess = test_assess.sort_values(by='timestamp').drop_duplicates(subset=['installation_id'], keep='last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_test_assess.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add columns that store if the attempt was a Pass or a Fail\nlast_assessment_sessions['Pass'] = last_assessment_sessions.apply(lambda row: '\"correct\":true' in row.event_data, axis = 1)\n\nlast_assessment_sessions['Fail'] = last_assessment_sessions.apply(lambda row: '\"correct\":false' in row.event_data, axis = 1)\n\n# Create a pivot table that logs pass/fail for each installation_id for the last assessment\npass_fail_log = last_assessment_sessions.pivot_table(['Pass', 'Fail'], index='installation_id', aggfunc = 'sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the rules provided in the Data tab of the competition, create a function that outputs the accuracy group\ndef accuracy_group_calculator(row):\n    \n    if row.Pass == 0:\n        #0: the assessment was never solved\n        return 0.0\n    \n    elif row.Fail == 0:\n        # 3: the assessment was solved on the first attempt\n        return 3.0\n    \n    elif row.Fail == 1:\n        # 2: the assessment was solved on the second attempt\n        return 2.0\n    \n    else:\n        # 1: the assessment was solved after 3 or more attempts\n        return 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can finally calculate the target variable for our training data set\npass_fail_log['Accuracy_Group'] = pass_fail_log.apply(lambda row: accuracy_group_calculator(row), axis = 1)\n\ntrain_targets = pass_fail_log.drop(['Fail', 'Pass'], axis=1)\n\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical EDA\nNow that we have our target variable extracted from the training data, we can start exploring the data for features that might help us predict our target variable. Because the last assessment taken by each user is not always the same, let's see if there is a difference in performance based on which assessment was taken last. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the training and test data so that we can build the same features on both\ndata = train.append(test)\n\n# We will build the features of the train and test data at the same time\nfeatures = train_targets.index.values\nfeatures = np.concatenate([features, test.installation_id.unique()])\nfeatures = pd.DataFrame(features, index=features)\nfeatures.index.name = 'installation_id'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_assessment_sessions.drop(['Pass', 'Fail'], axis=1, inplace=True)\n\nlast_assessments = last_assessment_sessions.append(last_test_assess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_game_sessions = last_assessments.game_session.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join the game_session, title, and world to the features dataframe.\nfeatures = pd.merge(features, last_assessments[['installation_id', 'game_session', 'title', 'world']], on='installation_id', how='left')\n\n# Remove any duplicate lines created by the merge.\nfeatures = features.drop_duplicates(subset=['installation_id'], keep=\"first\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets['Accuracy_Group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets['Accuracy_Group'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.rename(columns = {'game_session':'last_assess_game_session', 'title': 'last_assess_title', 'world':'last_assess_world'}, inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.drop([0], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join the game_session, title, and world to the features dataframe.\ntrain_targets = pd.merge(train_targets, features[['installation_id', 'last_assess_game_session', 'last_assess_title', 'last_assess_world']], on='installation_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessments = train_targets.last_assess_title.unique()\nworlds = train_targets.last_assess_world.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"worlds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nfor i in range(1, 5):\n    if i == 1:\n        counts = train_targets['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(2, 2, i).set_title(\"All Worlds\")\n    \n    if i == 2:\n        counts = train_targets[train_targets['last_assess_world'] == 'MAGMAPEAK']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(2, 2, i).set_title(\"MAGMAPEAK\")\n        \n    if i == 3:\n        counts = train_targets[train_targets['last_assess_world'] == 'TREETOPCITY']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(2, 2, i).set_title(\"TREETOPCITY\")\n    \n    if i == 4:\n        counts = train_targets[train_targets['last_assess_world'] == 'CRYSTALCAVES']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(2, 2, i).set_title(\"CRYSTALCAVES\")\n    \n    plt.bar(counts.index,counts)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see immediately that the performance on the final assessment vary somewhat depending on the world of the assessment taken. Regardless of the world of the last assessment, the children were likely to pass on the first attempt. We do see a difference in the breakup of children who never passed the last assessment. MAGMAPEAK has very few children who were unable to pass the last assessment, whereas a significant chunk of the CRYSTALCAVES group did not pass. \n\nThere are multiple hypotheses that could support this data. Perhaps the MAGMAPEAK assessments are the easiest and the CRYSTALCAVES assessments are the most difficult. One thing to keep in mind is that there are 5 assessments and only 3 worlds. This means that some worlds have multiple assessments. Let's see how the assessments are split among the worlds. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.pivot_table(['last_assess_game_session'], index='last_assess_title', columns = ['last_assess_world'], aggfunc = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few facts are apparent based on this breakdown. First, we see that the assessments are split among the worlds as follows:\n\nCRYSTALCAVES : Cart Balancer, Chest Sorter\n\nMAGMAPEAK : Cauldron Filler\n\nTREETOPCITY : Bird Measurer, Mushroom Sorter\n\nWe see that within each of the worlds, there's a fairly equal split between which assessment was the last taken. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 7):\n    if i == 1:\n        counts = train_targets['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title(\"All Assessments\")\n    \n    if i == 2:\n        counts = train_targets[train_targets['last_assess_title'] == 'Mushroom Sorter (Assessment)']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title('Mushroom Sorter (Assessment)')\n               \n    if i == 3:\n        counts = train_targets[train_targets['last_assess_title'] == 'Bird Measurer (Assessment)']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title('Bird Measurer (Assessment)')\n              \n    if i == 4:\n        counts = train_targets[train_targets['last_assess_title'] == 'Cauldron Filler (Assessment)']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title('Cauldron Filler (Assessment)')\n     \n    if i == 5:\n        counts = train_targets[train_targets['last_assess_title'] == 'Cart Balancer (Assessment)']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title('Cart Balancer (Assessment)')\n        \n    if i == 6:\n        counts = train_targets[train_targets['last_assess_title'] == 'Chest Sorter (Assessment)']['Accuracy_Group'].value_counts().sort_index()\n        plt.subplot(3, 2, i).set_title('Chest Sorter (Assessment)')\n    \n    \n    plt.bar(counts.index,counts)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see the performance on the assessments. These are shown in the order that the game was designed to be followed. The first world is TREETOPCITY, then MAGMAPEAK, and then CRYSTALCAVES. However, the player is not locked into to any particular track. We do see however, that the performance on the first assessment in a world is better than the performance on the last assessment in the world. For example, in TREETOPCITY, we see that the largest AccuracyGroup is 3 for Mushroom Sorter, but then the largest of Bird Measurer AccuracyGroups are 1 and 0. "},{"metadata":{},"cell_type":"markdown","source":"### Complete Assessment History\nAs we're seeing that not every assessment has the same performance outcome, it seems worthwhile to note the Accuracy Group for every installation_id on every assessment attempted prior to the last assessment. That way we can see if past performance has any indication on the last assessment attempted. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because the Bird Measurer assessment uses a distinct event code, let's separate it from the data\nbird_measure_assess = data[data['title'] == \"Bird Measurer (Assessment)\"]\n\n# Capture bird_meas assessment attempts\nbird_measure_assess = bird_measure_assess[bird_measure_assess['event_code'] == 4110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab all assessment attempts that are not for bird measurer\nprior_assessments = data[data['title'] != \"Bird Measurer (Assessment)\"]\n\nprior_assessments = prior_assessments[prior_assessments['event_code'] == 4100]\n\n# Some non-assessment activities have a 4100 code, ignore those\nprior_assessments = prior_assessments[prior_assessments['type'] == 'Assessment']\n\n# Append bird measure assessment attempts to have a log of all assessment attempts\nprior_assessments = prior_assessments.append(bird_measure_assess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all assessment sessions that were part of the child's last assessment. That way, our features don't leak information about the target variable. \nprior_assessments = prior_assessments[~prior_assessments['game_session'].isin(last_game_sessions)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_assessments.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a Pass and Fail column that will allow us to count each user's performance history\nprior_assessments['Pass'] = prior_assessments.apply(lambda row: '\"correct\":true' in row.event_data, axis = 1)\n\nprior_assessments['Fail'] = prior_assessments.apply(lambda row: '\"correct\":false' in row.event_data, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_assessments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pivot table that logs pass/fail for each installation_id for the last assessment\nprior_pass_fail_log = prior_assessments.pivot_table(['Pass', 'Fail'], index=['installation_id','title',], aggfunc = 'sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_pass_fail_log.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a history of how each user in the data set performed historically on each of the assessments they completed. Another potentially useful feature is to look at how many assessments a user started but did not complete. I will do that after this section. "},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = prior_assessments.title.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each assessment title, filter on that title and pull each user's history into the features dataframe we're constructing.\nfor title in titles:\n    features = pd.merge(features, prior_pass_fail_log.filter(like=title, axis=0), on=\"installation_id\", how='left')\n    features.rename(columns = {'Fail': title + \"_Fail\", 'Pass': title + \"_Pass\"}, inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If the user has no attempt history for an assessment, enter a 0 for both pass and fail\nfeatures.fillna(value = 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Assessments Started Not Completed\nIt might be interesting to know if there is a significant number of assessments that users started but did not complete. These might suggest that the assessment was too difficult and the player simply gave up. "},{"metadata":{"trusted":true},"cell_type":"code","source":"assessments = data[data['type'] == 'Assessment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessments.title.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because the Bird Measurer assessment uses a distinct event code, let's separate it from the data\nbird_measure_assess = data[data['title'] == \"Bird Measurer (Assessment)\"]\n\nmeasure_assess = data[data['title'] != \"Bird Measurer (Assessment)\"]\nmeasure_assess = measure_assess[measure_assess.type == 'Assessment']\n\n# Capture assessment attempts\ngame_sessions_start = bird_measure_assess[bird_measure_assess['event_code'] == 2000].game_session.unique()\n\ngame_sessions_start = np.concatenate([game_sessions_start, measure_assess[measure_assess['event_code'] == 2000].game_session.unique()])\n\n# Capture completed assessment game sessions\ngame_sessions_complete = bird_measure_assess[bird_measure_assess.event_code == 4110].game_session.unique()\n\ngame_sessions_complete = np.concatenate([game_sessions_complete, measure_assess[measure_assess['event_code'] == 4100].game_session.unique()])\n\n# Capture all game_sessions where an assessment is initiated but never completed\nincomplete_gs = [gs for gs in game_sessions_start if gs not in game_sessions_complete]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capture the train data for the game_sessions that have incomplete assessments\nincomplete_sessions = data[data.game_session.isin(incomplete_gs)]\nincomplete_sessions = incomplete_sessions[incomplete_sessions.event_code == 2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_sessions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_sessions_table = incomplete_sessions.pivot_table(['event_count'],columns=incomplete_sessions.title, index='installation_id', aggfunc = 'count', fill_value = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_sessions_table.columns = [str(col) + '_incomplete_attempts' for col in incomplete_sessions_table.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_sessions_table.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.merge(features, incomplete_sessions_table, on='installation_id', how='left')\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results\nUpon adding the features created in this section, our performance on the test data actually worsened. We dropped from a score of .399 to .361. For now I will keep these features. "},{"metadata":{},"cell_type":"markdown","source":"## Feature Idea: Time Spent in Each Title\nNow I will check to see if there is a correlation between the time spent on each activity and the performance of the child on their last assessment. To do this, I will grab the maximum game time in each title as a proxy for approximately how much time was spent on the activity during the game session. We can group these by Title and then sum them up for each installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get each child's performance on their last assessment, extract only the last assessment per installation_id\nlast_event_per_game_session = data.sort_values(by=\"timestamp\").drop_duplicates(subset=[\"game_session\"], keep=\"last\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_event_per_game_session1 = last_event_per_game_session.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_event_per_game_session = last_event_per_game_session[last_event_per_game_session.type != \"Assessment\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_per_activity = last_event_per_game_session.groupby(['installation_id','world'])[['game_time']].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_per_activity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"worlds = data.world.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"worlds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1 = features.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each assessment title, filter on that title and pull each user's history into the features dataframe we're constructing.\nfor world in worlds:\n    features1 = pd.merge(features1, time_per_activity.filter(like=world, axis=0), on=\"installation_id\", how='left')\n    #print(features1.columns)\n    features1.rename(columns = {'game_time': world + \"_game_time\"}, inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1.fillna(value = 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = []\nfor col in features1.columns:\n    if features1[col].unique().size == 1:\n        columns_to_drop.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features1.drop(columns_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features1.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing and Training Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.concat([features, pd.get_dummies(features['last_assess_title'],prefix='last_title', drop_first=True)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.drop(labels=[\"last_assess_game_session\", \"last_assess_title\", \"last_assess_world\"], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets.iloc[:, :2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join the game_session, title, and world to the features dataframe.\ntrain_targets = pd.merge(train_targets, features, on='installation_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join the game_session, title, and world to the features dataframe.\ntest_X = pd.merge(sample_sub, features, on='installation_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_targets.iloc[:, 2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_targets.iloc[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_input =  test_X.iloc[:, 2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_input.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.fillna(0, inplace=True, axis=1)\ntest_X_input.fillna(0, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model, metrics \nfrom sklearn.naive_bayes import MultinomialNB\n   \n# defining feature matrix(X) and response vector(y) \nX = train_X\ny = train_y\n  \n# splitting X and y into training and testing sets \nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=1) \n   \n# create logistic regression object \n#reg = linear_model.LogisticRegression() \nnb = MultinomialNB()  \n# train the model using the training sets \n#reg.fit(X_train, y_train) \nnb.fit(X_train, y_train)\n  \n# making predictions on the testing set \n#y_pred = reg.predict(X_test) \ny_pred = nb.predict(X_test)\n   \n# comparing actual response values (y_test) with predicted response values (y_pred) \nprint(\"Logistic Regression model accuracy(in %):\",  \nmetrics.accuracy_score(y_test, y_pred)*100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.drop(['accuracy_group'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_y = reg.predict(test_X_input)\ntest_y = nb.predict(test_X_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_df = pd.DataFrame(test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_df.rename(columns={0: 'accuracy_group'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.merge(sample_sub, test_y_df, left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output['accuracy_group'] = output['accuracy_group'].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}