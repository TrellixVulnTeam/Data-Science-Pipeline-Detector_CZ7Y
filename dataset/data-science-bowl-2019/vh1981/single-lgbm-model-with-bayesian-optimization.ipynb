{"cells":[{"metadata":{"_cell_guid":"6a810af4-40f8-40ee-8419-e6c5cffbcc99","_uuid":"38294338-d438-4a96-ba41-0385e4115cf1"},"cell_type":"markdown","source":"# Single Model(LGBMRegressor) with Bayesian Optimization\n\n\nusing codes from https://www.kaggle.com/braquino/convert-to-regression very thanks for sharing~\n\nmake lgb models with different hyperparameters & blend result."},{"metadata":{"_cell_guid":"fa58cf1f-2874-4bb2-bb47-38647e24071e","_kg_hide-input":true,"_uuid":"f1c8fdb3-3de2-4105-9888-59875e71ffb5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport copy\nimport random\n\nrandom.seed(69)\n\nimport time\nfrom collections import Counter\nfrom typing import List, Any\nfrom itertools import product\nfrom collections import defaultdict\nimport datetime\nimport json\nimport gc\nfrom numba import jit\nimport warnings\nimport re\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport altair as alt\nimport networkx as nx\n\nfrom joblib import Parallel, delayed\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom functools import partial\nimport numpy as np\nnp.random.seed(69)\n\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm\npd.set_option('max_rows', 500)\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn import linear_model\nfrom category_encoders.ordinal import OrdinalEncoder\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dac51c9f-bc82-4a1d-8879-c213598d25a1","_uuid":"72e7cac1-751c-465d-89ad-cf66144e576b","trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab75e26a-7447-43fb-924f-773917a8b0b7","_uuid":"f427ff1a-25ba-4d35-9de8-e805a94b1d2e"},"cell_type":"markdown","source":"Flags"},{"metadata":{"_cell_guid":"971b16ec-c646-44bc-a197-3d9769151578","_uuid":"308de59b-6e30-4328-b9a6-786bbaa15e57","trusted":true},"cell_type":"code","source":"# global flags here:\nBOOTSTRAP_MODE = True # set True if check code error quickly.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee108b13-7a9e-49d3-9f78-aeeb78a22a64","_uuid":"b8feaf73-bd20-4a81-af50-3b3f4f852d36"},"cell_type":"markdown","source":"# Make Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_cyclic(value, max, amp=1):\n    return (np.sin(np.pi * value / max) * amp)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48d7662b-e01b-41ea-84d7-8486335b2044","_uuid":"cdf1a2e8-575b-4fee-8e2c-2454d7d6abc3","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"afd5f297-685b-429e-8c1f-b38f4267fbdf","_uuid":"d1a03c96-a20e-4525-8b16-13374b7afa99","trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    # 여기서는 title과 event_code를 _를 사이에 두고 붙여 버린다.\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = sorted(list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())))\n\n    # make a list with all the unique 'titles' from the train and test set\n    # title들을 모아 숫자값으로 변경\n    list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n    list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n    # replace the text titles with the number titles from the dict\n\n    # 문자열을 위에서 생성한 dictionary에 해당하는 숫자로 바꾼다.\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    \n    # win_code 생성(Bird Measurer (Assessment)만 4110, 그 외에는 4100)\n    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    train['hour'] = train['timestamp'].dt.hour\n    test['hour'] = test['timestamp'].dt.hour    \n    \n    event_data = {\"train_labels\":train_labels, \"win_code\":win_code, \"list_of_user_activities\":list_of_user_activities, \"list_of_event_code\":list_of_event_code,\n                 \"activities_labels\":activities_labels, \"assess_titles\":assess_titles, \"list_of_event_id\":list_of_event_id, \"all_title_event_code\":all_title_event_code,\n                 \"activities_map\":activities_map}    \n\n    return train, test, event_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clip_time = {'Welcome to Lost Lagoon!':19,'Tree Top City - Level 1':17,'Ordering Spheres':61, 'Costume Box':61,\n        '12 Monkeys':109,'Tree Top City - Level 2':25, 'Pirate\\'s Tale':80, 'Treasure Map':156,'Tree Top City - Level 3':26,\n        'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18,\n        'Balancing Act':72, 'Lifting Heavy Things':118,'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19,\n        'Heavy, Heavier, Heaviest':61}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnt_miss(df):\n    cnt = 0\n    for e in range(len(df)):\n        x = df['event_data'].iloc[e]\n        y = json.loads(x)['misses']\n        cnt += y\n    return cnt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change event data to statistical game session data group by 'game_session'."},{"metadata":{"_cell_guid":"96892178-45f6-4959-9ebd-e89681205083","_uuid":"1696d89c-6707-4fae-8c2c-99ce0fac0559","trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    \n    args:\n        user_sample --- DataFrameGroupBy object group by 'installation_id'\n        test_set -- on test data, only last game session need to be predicted.\n        \n    return:\n        array of assessments(by game session).\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    game_time_dict = {'Clip_gametime':0, 'Game_gametime':0, 'Activity_gametime':0, 'Assessment_gametime':0}\n    Assessment_mean_event_count = 0\n    Game_mean_event_count = 0\n    Activity_mean_event_count = 0\n    mean_game_round = 0\n    mean_game_duration = 0 \n    mean_game_level = 0\n    accumulated_game_miss = 0\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])    \n    durations = []\n    clip_durations = []\n    Activity_durations = []\n    Game_durations = []\n    \n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # last features\n    sessions_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        \n        if session_type == 'Clip':\n            clip_durations.append((clip_time[activities_labels[session_title]]))\n        \n        if session_type == 'Activity':\n            Activity_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0\n        \n        if session_type == 'Game':\n            Game_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n            \n            game_s = session[session.event_code == 2030]   \n            misses_cnt = cnt_miss(game_s)\n            accumulated_game_miss += misses_cnt\n            \n            try:\n                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n                mean_game_round =  (mean_game_round + game_round)/2.0\n            except:\n                pass\n\n            try:\n                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n                mean_game_duration = (mean_game_duration + game_duration) /2.0\n            except:\n                pass\n            \n            try:\n                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n                mean_game_level = (mean_game_level + game_level) /2.0\n            except:\n                pass\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features['installation_session_count'] = sessions_count\n            features['hour'] = session['hour'].iloc[-1]\n            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n            features['Game_mean_event_count'] = Game_mean_event_count\n            features['Activity_mean_event_count'] = Activity_mean_event_count\n            features['mean_game_round'] = mean_game_round\n            features['mean_game_duration'] = mean_game_duration\n            features['mean_game_level'] = mean_game_level\n            features['accumulated_game_miss'] = accumulated_game_miss\n            \n            variety_features = [('var_event_code', event_code_count),\n                                ('var_event_id', event_id_count),\n                                ('var_title', title_count),\n                                ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                \n            if clip_durations == []:\n                features['Clip_duration_mean'] = 0\n                features['Clip_duration_std'] = 0\n            else:\n                features['Clip_duration_mean'] = np.mean(clip_durations)\n                features['Clip_duration_std'] = np.std(clip_durations)\n                \n            if Activity_durations == []:\n                features['Activity_duration_mean'] = 0\n                features['Activity_duration_std'] = 0\n            else:\n                features['Activity_duration_mean'] = np.mean(Activity_durations)\n                features['Activity_duration_std'] = np.std(Activity_durations)\n                \n            if Game_durations == []:\n                features['Game_duration_mean'] = 0\n                features['Game_duration_std'] = 0\n            else:\n                features['Game_duration_mean'] = np.mean(Game_durations)\n                features['Game_duration_std'] = np.std(Game_durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1])/2.0\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n\n            counter += 1\n\n        sessions_count += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(session[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if col == 'title':\n                    x = activities_labels[k]\n                counter[x] += num_of_session_count[k]\n            return counter\n            \n        game_time_dict[session_type+'_gametime'] = (game_time_dict[session_type+'_gametime'] + (session['game_time'].iloc[-1]/1000.0))/2.0\n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c22b8d0-4fef-4e79-92ee-7dae97e6cbac","_uuid":"120abaa9-2eda-4912-83bd-7e243fd0d0ba","trusted":true},"cell_type":"code","source":"def get_session_data(df, is_test=False, cut_last=False):\n    compiled_df = []\n    total_cnt = df.installation_id.unique().shape[0]\n    for i, (ins_id, user_sample) in tqdm(enumerate(df.groupby('installation_id', sort = False)), total = total_cnt):\n        session_data = get_data(user_sample, test_set=is_test)        \n        if type(session_data) is list:\n            if cut_last:\n                session_data = session_data[:-1]\n            compiled_df += session_data\n        else:\n            compiled_df.append(session_data)\n\n    reduce_df = pd.DataFrame(compiled_df)    \n    categoricals = ['session_title']\n    return reduce_df, categoricals\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f343869a-cee9-40d0-8375-715d3bea99ff","_uuid":"0239457d-1b1f-414f-bab2-7456efc3f5c5","trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count') #installation_id 마다 이루어진 session의 수\n    df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')        \n    df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n\n    df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                    4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                    2040, 4090, 4220, 4095]].sum(axis = 1)\n\n    df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean') # or 'std'\n    \n    \n    # cyclic categorical values conversion.\n    df['hour'] = to_cyclic(df['hour'], 24, 4)    \n    \n    # remove special characters from column names\n    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d45f946-6d04-4737-8ab9-2c8ab3b5a5a5","_uuid":"d674d0a2-a012-4986-ba06-7a30604d5a32","trusted":true},"cell_type":"code","source":"# read data\ntry:\n    del train\n    del test\n    gc.collect()\nexcept:\n    pass\n\ntrain, test, train_labels, specs, sample_submission = read_data()\n\n# get usefull dict with maping encode\ntrain, test, event_data = encode_title(train, test, train_labels)\n\ntrain_labels = event_data['train_labels']\nwin_code = event_data['win_code']\nlist_of_user_activities = event_data['list_of_user_activities']\nlist_of_event_code = event_data['list_of_event_code']\nactivities_labels = event_data['activities_labels']\nassess_titles = event_data['assess_titles']\nlist_of_event_id = event_data['list_of_event_id']\nall_title_event_code = event_data['all_title_event_code']\n\nreduce_train, categoricals = get_session_data(train, is_test=False)\nreduce_train = preprocess(reduce_train)\n\nreduce_test, categoricals = get_session_data(test, is_test = True)\nreduce_test = preprocess(reduce_test)\n\ndel train\ndel test","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6331bbaa-6133-40ba-b98d-daa4e4d3a75e","_uuid":"43adf968-7ec3-4488-bedb-58d538759e8c","trusted":true},"cell_type":"code","source":"reduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa984c91-4101-4229-8cb5-04f4e004337a","_uuid":"eebee2c3-109f-4a5f-aca5-0d0d2cece262","trusted":true},"cell_type":"code","source":"reduce_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate']\nall_features = [x for x in reduce_train.columns if x not in cols_to_drop]\ncat_features = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove duplicated features by calculate correlation coefficients between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_remove = []\ndef get_duplicated_features(df, features, threshold = 0.995):\n    counter = 0\n    to_remove = []\n    for feat_a in features:\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n                c = np.corrcoef(df[feat_a], df[feat_b])[0][1] # (2x2) table by (x,y)\n                if c > threshold:\n                    counter += 1\n                    to_remove.append(feat_b)\n                    #print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n    print(f'{counter} duplicated features found.')\n    return to_remove\n\n#feat_remove = get_duplicated_features(reduce_train, all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove features that have different distribution between train & test."},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    mse = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(mse)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n        plt.show()\n    return mse\n\nfeat_exclude = []\nfeatures_except = ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']\ndef get_different_dist_features(features_except = features_except):\n    to_exclude = []\n    ajusted_test = reduce_test.copy()\n    for feature in ajusted_test.columns:\n        if feature not in features_except:\n            data = reduce_train[feature]\n            train_mean = data.mean()\n            data = ajusted_test[feature] \n            test_mean = data.mean()\n            try:\n                error = stract_hists(feature, adjust=True)\n                ajust_factor = train_mean / test_mean\n                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n                    to_exclude.append(feature)\n                    #print(feature, train_mean, test_mean, error)\n                else:\n                    ajusted_test[feature] *= ajust_factor\n            except:\n                to_exclude.append(feature)\n                #print(feature, train_mean, test_mean)\n    return to_exclude\n\n#feat_exclude = get_different_dist_features()\nprint(f\"{len(feat_exclude)} features to exclude : \\n{feat_exclude}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = [x for x in reduce_train.columns if x not in cols_to_drop + feat_remove + feat_exclude]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7833daf-41ce-4627-b866-490f172b804b","_uuid":"f5db4aad-c969-4240-bdbd-b691519153ec"},"cell_type":"markdown","source":"# Model training & predict\n \n - Single LightGBM model with different hyperparameters"},{"metadata":{"_cell_guid":"b6ead2ec-5e0f-4b0a-aab4-3b9b4cd19dec","_kg_hide-input":true,"_uuid":"b10c884b-e478-4efe-8d3a-26a981610888","trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(reduce_train['accuracy_group'])\n    for k in dist:\n        dist[k] /= len(reduce_train)\n    #reduce_train['accuracy_group'].hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"105cd1a6-0593-459c-928e-4d46cb392434","_uuid":"8d220bff-490d-4703-a0ea-312cd4a778ea","trusted":true},"cell_type":"code","source":"def make_model_data(df, is_train = True):\n    _X = df[all_features]    \n    _X = _X.reset_index(drop=True)\n    _y = df['accuracy_group']\n    _y = _y.reset_index(drop=True)\n    if is_train:\n        return _X, _y\n    else:\n        return _X, None","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e74a5671-0596-4779-affe-e6b8cd9e35c8","_uuid":"7d49c619-2c18-422e-ad90-7fbe608705a1","trusted":true},"cell_type":"code","source":"#make data for train\nX, y = make_model_data(reduce_train, is_train = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data holders for blending\n\nThe holder is dictionary of model name as key and tuple of score and prediction as value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions dictionary : model_name : (score, prediction)\npredictions = {}\nmodels = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Base model : LGBMRegressor\n\nUsed BayesianOptimization to find the optimal hyperparameters for the final models."},{"metadata":{"trusted":true},"cell_type":"code","source":"class lgb_model():\n    def __init__(self, X, y, categoricals, n_folds=2):\n        self.X = X\n        self.y = y\n        self.categoricals = categoricals\n        self.n_folds = n_folds\n        self.models = []\n        print(f'input shape={X.shape} label shape={y.shape}')\n        \n    def make_params(self, learning_rate, max_depth, lambda_l1, lambda_l2, \n                    bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction):\n        params = {'n_estimators':5000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    #'eval_metric': 'cappa',\n                    'subsample' : 1.0,\n                    'subsample_freq' : subsample_freq,\n                    'feature_fraction' : feature_fraction,\n                    'n_jobs': -1,\n                    'seed': 42,                    \n                    'learning_rate': learning_rate,\n                    'max_depth': int(max_depth),\n                    'lambda_l1': lambda_l1,\n                    'lambda_l2': lambda_l2,\n                    'bagging_fraction' : bagging_fraction,\n                    'bagging_freq': int(bagging_freq),\n                    'colsample_bytree': colsample_bytree,\n                    'early_stopping_rounds': 100,\n                    'verbose' : 0\n                 }\n        return params\n\n    def opt_func(self, learning_rate, max_depth, lambda_l1, lambda_l2, \n                     bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction):\n        params = self.make_params(learning_rate, max_depth, lambda_l1, lambda_l2, \n                     bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction)\n        \n        folds = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=345)\n        oof = np.zeros(len(self.X))\n        self.models = []\n        for fold, (train_idx, test_idx) in enumerate(folds.split(self.X, self.y)):\n            #print(f'running fold {fold}')\n            X_train, y_train = self.X.iloc[train_idx], self.y.iloc[train_idx]\n            X_valid, y_valid = self.X.iloc[test_idx], self.y.iloc[test_idx]\n            \n            model = lgb.LGBMRegressor()\n            model.set_params(**params)\n            eval_set = [(X_valid, y_valid)]\n            eval_names = ['valid']\n            model.fit(X=X_train, y=y_train, eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb_regr,\n                      verbose=0, categorical_feature='auto')            \n\n            pr = model.predict(self.X.loc[test_idx]).reshape(len(test_idx))\n            oof[test_idx] = pr[:]\n            self.models.append(model)\n\n        _, score, _ = eval_qwk_lgb_regr(y, oof)        \n        return score\n    \n    \n    def predict(self, X):\n        if len(self.models) == 0:\n            return        \n        preds = []\n        for m in self.models:\n            pred = m.predict(X).reshape(X.shape[0])\n            preds.append(pred)\n        preds = np.array(preds)\n        final_pred = np.mean(preds, axis=0)\n        print(\"final_pred.shape = \", final_pred.shape)\n            \n        return final_pred\n    \ndef make_opt_hyperparams_lgb():\n    params = {'learning_rate' : (0.02, 0.3),\n                   'max_depth': (13, 20),\n                   'lambda_l1': (1, 12),\n                   'lambda_l2': (1, 12),\n            'bagging_fraction': (0.7, 1.0),\n                'bagging_freq': (1, 10),\n            'colsample_bytree': (0.6, 1.0),\n              'subsample_freq': (1, 10),\n            'feature_fraction': (0.6, 1.0)\n             }\n    \n    n_folds = (2 if BOOTSTRAP_MODE else 4)\n    modelWrapper = lgb_model(X, y, categoricals, n_folds = n_folds)\n    lgbBO = BayesianOptimization(modelWrapper.opt_func, params, random_state=1030)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        init_points = (2 if BOOTSTRAP_MODE else 16)\n        n_iter = (2 if BOOTSTRAP_MODE else 16)\n        lgbBO.maximize(init_points = init_points, n_iter = n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n        return lgbBO\n\ndef make_pred_lgb():\n    # run bayesian optimization\n    optimizer = make_opt_hyperparams_lgb()\n    \n    # get best hyperparameter from optimizer result\n    params = optimizer.max['params'] # use best parameter from optimization result\n    \n    # make model with best hyperparameters and build model\n    n_folds = (2 if BOOTSTRAP_MODE else 6)\n    modelWrapper = lgb_model(X, y, categoricals, n_folds=n_folds)\n    score = modelWrapper.opt_func(**params) # train with optimized hyperparameters\n    \n    # predict with reduce_test\n    X_test, _ = make_model_data(reduce_test, is_train = False)\n    pred = modelWrapper.predict(X_test)\n    \n    # for OptimizedRounder\n    pred_train = modelWrapper.predict(X)\n\n    print(\"lgb model score : {}\".format(score))\n    print(\"pred.shape : \", pred.shape)\n    predictions[\"lgb\"] = (score, pred)\n    \n    models.extend(modelWrapper.models)\n    \n    #print(\"test pred:\", pred)\n    \n    return pred_train\n\n#pred_train = make_pred_lgb()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\noptimized_params_list = [\n    {'learning_rate' : 0.0657,\n     'max_depth': 17,\n     'lambda_l1': 8,\n     'lambda_l2': 9,\n     'bagging_fraction': 0.92,\n     'bagging_freq': 9.2,\n     'colsample_bytree': 0.96,\n     'subsample_freq' : 5.6,\n     'feature_fraction' : 0.686\n     },\n    {'learning_rate' : 0.05789,\n     'max_depth': 16,\n     'lambda_l1': 9.2,\n     'lambda_l2': 9.768,\n     'bagging_fraction': 0.7364,\n     'bagging_freq': 1.02,\n     'colsample_bytree': 0.7993,\n     'subsample_freq' : 1.0,\n     'feature_fraction' : 0.9\n     },\n    {'learning_rate' : 0.023,\n     'max_depth': 19,\n     'lambda_l1': 5.0,\n     'lambda_l2': 11.0,\n     'bagging_fraction': 0.9465,\n     'bagging_freq': 1.4,\n     'colsample_bytree': 0.94,\n     'subsample_freq' : 7.0,\n     'feature_fraction' : 0.657\n     },    \n    {'learning_rate' : 0.05399,\n     'max_depth': 14,\n     'lambda_l1': 7.812,\n     'lambda_l2': 8.155,\n     'bagging_fraction': 0.9331,\n     'bagging_freq': 8.897,\n     'colsample_bytree': 0.738,\n     'subsample_freq' : 1.0,\n     'feature_fraction' : 0.9\n     },\n    {'learning_rate' : 0.04307,\n     'max_depth': 16,\n     'lambda_l1': 9.917,\n     'lambda_l2': 9.89,\n     'bagging_fraction': 0.9523,\n     'bagging_freq': 1.042,\n     'colsample_bytree':0.7772,\n     'subsample_freq' : 1.0,\n     'feature_fraction' : 0.9\n     },\n    {'learning_rate' : 0.05789,\n     'max_depth': 16,\n     'lambda_l1': 9.2,\n     'lambda_l2': 9.768,\n     'bagging_fraction': 0.7364,\n     'bagging_freq': 1.02,\n     'colsample_bytree':0.7993,\n     'subsample_freq' : 1.0,\n     'feature_fraction' : 0.9\n     },\n    {'learning_rate' : 0.0269,\n     'max_depth': 16,\n     'lambda_l1': 9.78,\n     'lambda_l2': 9.691,\n     'bagging_fraction': 0.9205,\n     'bagging_freq': 1.023,\n     'colsample_bytree':0.7992,\n     'subsample_freq' : 1.0,\n     'feature_fraction' : 0.9\n     },\n    {'learning_rate' : 0.04897,\n     'max_depth': 20,\n     'lambda_l1': 12,\n     'lambda_l2': 5.3,\n     'bagging_fraction': 0.9131,\n     'bagging_freq': 6.63,\n     'colsample_bytree':0.6854,\n     'subsample_freq' : 7.79,\n     'feature_fraction' : 0.7599\n     },\n    {'learning_rate' : 0.028,\n     'max_depth': 20,\n     'lambda_l1': 11.66,\n     'lambda_l2': 11,\n     'bagging_fraction': 0.9723,\n     'bagging_freq': 3.987,\n     'colsample_bytree':0.726,\n     'subsample_freq' : 6.748,\n     'feature_fraction' : 0.61\n     },\n    {'learning_rate' : 0.031,\n     'max_depth': 17,\n     'lambda_l1': 5.5,\n     'lambda_l2': 5.2,\n     'bagging_fraction': 0.9413,\n     'bagging_freq': 2.771,\n     'colsample_bytree':0.6012,\n     'subsample_freq' : 2.7,\n     'feature_fraction' : 0.6168\n     }\n]\n\nn_folds = 6\nX_test, _ = make_model_data(reduce_test, is_train = False)\ntmp_modelWrapper = None\nfor i, params in enumerate(optimized_params_list):\n    name = f'lgb_model_{i}'\n    print(\"running model : {}\".format(name))\n    modelWrapper = lgb_model(X, y, categoricals, n_folds=n_folds)\n    tmp_modelWrapper = modelWrapper\n    score = modelWrapper.opt_func(**params) # train with optimized hyperparameters\n    print(f'score : {score}')\n    \n    # predict with reduce_test\n    pred = modelWrapper.predict(X_test)\n    models.extend(modelWrapper.models)\n    \n    predictions[name] = (score, pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot graph of the predictions for each 'accuracy_group'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_pred_dist_by_accuracy_group():\n    X, y = make_model_data(reduce_train, is_train = True)\n    py = tmp_modelWrapper.predict(X)\n    X['accuracy_group_label'] = y\n    X['accuracy_group_pred'] = py\n\n    fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(20,8))\n    plt.rcParams.update({'font.size': 16})\n\n    ax.set_xlim(-1, 4)\n    ax.set_title(\"dist by accuray_group\")\n    for i in range(4):\n        subdf = X[X['accuracy_group_label'] == i]\n        print(i, \" = \", subdf['accuracy_group_pred'].min(), \" ~ \", subdf['accuracy_group_pred'].max())\n        subdf['accuracy_group_pred'].plot.kde(ax=ax, label=str(i), legend=True)\n\nshow_pred_dist_by_accuracy_group()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa126ab6-5699-467e-9777-b7d85ae4c3c3","_uuid":"ff1d0c81-1107-43ae-a053-7c40e516c308"},"cell_type":"markdown","source":"# blend results & submit\n\nBlend submodels predictions weighted by evaluation accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# blend data\n\ntotal_accuracy = 0.0\nfinal_pred = None\nfor p in predictions:    \n    total_accuracy += predictions[p][0]\n\nprint(f\"mean accuracy : {total_accuracy / len(predictions)}\")\n    \nfinal_pred = np.zeros(predictions[p][1].shape)\n# print(predictions[p][1].shape)\n# print(final_pred.shape)\n\nfor p in predictions:\n    accuracy = predictions[p][0]\n    #print(f'{p} current acc:{accuracy}     total acc sum:{total_accuracy}')\n    final_pred += predictions[p][1]\n    \nfinal_pred = final_pred / len(predictions)\n\nprint(\"blended final_pred.shape : \", final_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use percentile of final_pred using train's accuracy_group percentage for rounding predictions.\n\nhttps://www.kaggle.com/braquino/convert-to-regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit_with_ag_variance(final_pred):    \n    dist = Counter(reduce_train['accuracy_group'])\n\n    # change 'accuracy_group' counts to percentage\n    for k in dist:\n        dist[k] /= len(reduce_train)\n\n    # get percentile from final_pred using train's accuracy_group variance.\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(final_pred, acum * 100)\n\n    print(\"bounds : \", bound)\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    # get final prediction\n    final_pred = np.array(list(map(classify, final_pred)))\n    plt.hist(final_pred)\n\n    # make submit.\n    sample_submission['accuracy_group'] = final_pred.astype(int)\n    sample_submission.to_csv('submission.csv', index=False)\n    sample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_with_ag_variance(final_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}