{"cells":[{"metadata":{"_uuid":"38294338-d438-4a96-ba41-0385e4115cf1","_cell_guid":"6a810af4-40f8-40ee-8419-e6c5cffbcc99","trusted":true},"cell_type":"markdown","source":"\ncode from : https://www.kaggle.com/hengzheng/bayesian-optimization-seed-blending \n \n요약:\n \n1. 데이터 생성(read_data(), encode_title(), get_data(), get_train_and_test(), preprocess().\n2. catboost 모델에 사용할 파라메터를 bayesion optimization으로 구한다.\n3. 최적화된 파라메터를 사용하여 모델들을 구하고 test 데이터로 여러 prediction을 구한다.\n4. 구한 prediction에 대해 voting을 적용해서 최종 submission을 생성한다.\n5. submit.   \n"},{"metadata":{"_uuid":"f1c8fdb3-3de2-4105-9888-59875e71ffb5","_cell_guid":"fa58cf1f-2874-4bb2-bb47-38647e24071e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport copy\nimport random\nimport time\nfrom collections import Counter\nfrom typing import List, Any\nfrom itertools import product\nfrom collections import defaultdict\nimport datetime\nimport json\nimport gc\nfrom numba import jit\nimport warnings\nimport re\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport altair as alt\nimport networkx as nx\n\nfrom joblib import Parallel, delayed\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom functools import partial\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm\npd.set_option('max_rows', 500)\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import linear_model\nfrom category_encoders.ordinal import OrdinalEncoder\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e7cac1-751c-465d-89ad-cf66144e576b","_cell_guid":"dac51c9f-bc82-4a1d-8879-c213598d25a1","trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f427ff1a-25ba-4d35-9de8-e805a94b1d2e","_cell_guid":"ab75e26a-7447-43fb-924f-773917a8b0b7","trusted":true},"cell_type":"markdown","source":"Flags"},{"metadata":{"_uuid":"308de59b-6e30-4328-b9a6-786bbaa15e57","_cell_guid":"971b16ec-c646-44bc-a197-3d9769151578","trusted":true},"cell_type":"code","source":"# global flags here:\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8feaf73-bd20-4a81-af50-3b3f4f852d36","_cell_guid":"ee108b13-7a9e-49d3-9f78-aeeb78a22a64","trusted":true},"cell_type":"markdown","source":"# EDA"},{"metadata":{"_uuid":"cdf1a2e8-575b-4fee-8e2c-2454d7d6abc3","_cell_guid":"48d7662b-e01b-41ea-84d7-8486335b2044","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e67ef99c-654d-48d1-b99a-c9876a15afe4","_cell_guid":"749a11d3-e048-455b-b985-e1ed586affd4","trusted":true},"cell_type":"markdown","source":"[title, world]를 문자열에서 숫자값으로 매칭할 수 있는 dictionary를 만들고 변환한다.\n\ntrain_labels를 별도로 사용할 필요가 있는가? 필요없을 것 같은데..."},{"metadata":{"_uuid":"d1a03c96-a20e-4525-8b16-13374b7afa99","_cell_guid":"afd5f297-685b-429e-8c1f-b38f4267fbdf","trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    # 여기서는 title과 event_code를 _를 사이에 두고 붙여 버린다.\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    \n    # make a list with all the unique 'titles' from the train and test set\n    # title들을 모아 숫자값으로 변경\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    \n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))    \n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    \n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    \n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    # replace the text titles with the number titles from the dict\n    # 문자열을 위에서 생성한 dictionary에 해당하는 숫자로 바꾼다.\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    \n    # win_code 생성(Bird Measurer (Assessment)만 4110, 그 외에는 4100)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))    \n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    \n    # convert text into datetime\n    # 시간 문자열을 실제 시간 데이터 타입으로 변경\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0c04af0-ef6d-401a-a786-45a883f2c426","_cell_guid":"124a90ec-0f7a-4e6f-a93c-aaa6e648958b","trusted":true},"cell_type":"markdown","source":"get_data()는 installation_id로 묶인 group을 받아서 game_session 단위로 데이터를 생성해서 리턴한다."},{"metadata":{"_uuid":"1696d89c-6707-4fae-8c2c-99ce0fac0559","_cell_guid":"96892178-45f6-4959-9ebd-e89681205083","trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n    \n    # test_set인 경우 마지막 assessment만 예측하면 하므로 마지막 데이터만 리턴한다.\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"120abaa9-2eda-4912-83bd-7e243fd0d0ba","_cell_guid":"5c22b8d0-4fef-4e79-92ee-7dae97e6cbac","trusted":true},"cell_type":"code","source":"def get_train_and_test(train, test):\n    \"\"\"\n    event 데이터를 session에 대한 데이터로 생성해서 DataFrame으로 생성한다.\n    \"\"\"\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0239457d-1b1f-414f-bab2-7456efc3f5c5","_cell_guid":"f343869a-cee9-40d0-8375-715d3bea99ff","trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test):\n    \"\"\"\n    get_train_and_test()의 DataFrame에 통계 열을 추가함.\n    \"\"\"\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count') #installation_id 마다 이루어진 session의 수\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')        \n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean') # or 'std'\n        \n    return reduce_train, reduce_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d674d0a2-a012-4986-ba06-7a30604d5a32","_cell_guid":"4d45f946-6d04-4737-8ab9-2c8ab3b5a5a5","trusted":true},"cell_type":"code","source":"# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n\n# tranform function to get the train and test set\nreduce_train, reduce_test, categoricals = get_train_and_test(train, test)\n\n# call feature engineering function\nreduce_train, reduce_test = preprocess(reduce_train, reduce_test)\n\ndel train\ndel test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43adf968-7ec3-4488-bedb-58d538759e8c","_cell_guid":"6331bbaa-6133-40ba-b98d-daa4e4d3a75e","trusted":true},"cell_type":"code","source":"reduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eebee2c3-109f-4a5f-aca5-0d0d2cece262","_cell_guid":"aa984c91-4101-4229-8cb5-04f4e004337a","trusted":true},"cell_type":"code","source":"reduce_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5db4aad-c969-4240-bdbd-b691519153ec","_cell_guid":"c7833daf-41ce-4627-b866-490f172b804b","trusted":true},"cell_type":"markdown","source":"# Helper functions and classes"},{"metadata":{"_uuid":"b10c884b-e478-4efe-8d3a-26a981610888","_cell_guid":"b6ead2ec-5e0f-4b0a-aab4-3b9b4cd19dec","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    y_pred[y_pred <= 1.12232214] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n    y_pred[y_pred > 2.22506454] = 3\n\n    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n\n    return 'cappa', qwk(y_true, y_pred), True\n\n\n    \ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n#                          or 'attempt' in col]\n        \n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ff282cd-dad0-4b29-857b-4cde37b35774","_cell_guid":"19057daa-1a95-47dc-a961-063377f16141","trusted":true},"cell_type":"markdown","source":"# Model training & predict\n \n catboost with bayesian optimization\n \n 참조 코드:\n https://www.kaggle.com/sp1thas/dsb2019-catboost-lgbm-blending\n https://www.kaggle.com/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm"},{"metadata":{"_uuid":"6b648f51-793d-4f07-b4c6-34a4df104615","_cell_guid":"7c1ef1a7-1b40-4419-bb43-f73272328ee5","trusted":true},"cell_type":"code","source":"cols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate']\nall_features = [x for x in reduce_train.columns if x not in cols_to_drop]\ncat_features = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d220bff-490d-4703-a0ea-312cd4a778ea","_cell_guid":"105cd1a6-0593-459c-928e-4d46cb392434","trusted":true},"cell_type":"code","source":"def make_model_data(df, is_train = True):\n    _X = df[all_features]    \n    _X.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in _X.columns]\n    \n    if is_train:\n        return _X, df['accuracy_group']\n    else:\n        return _X, None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcc9488d-b637-4349-b82a-fed120fbe36b","_cell_guid":"89a5a735-4a95-471e-a603-dd6b5e57f640","trusted":true},"cell_type":"code","source":"# FIXME: need to refactoring. (merge with above code block...)\n_all_features = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in all_features]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d49c619-2c18-422e-ad90-7fbe608705a1","_cell_guid":"e74a5671-0596-4779-affe-e6b8cd9e35c8","trusted":true},"cell_type":"code","source":"#make data for train\nX, y = make_model_data(reduce_train, is_train = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df595a84-8556-4ab6-ae62-42b6ef784d2e","_cell_guid":"8320965d-a504-4c1c-8a21-0798b2d0ffaf","trusted":true},"cell_type":"code","source":"def func_catboost(bagging_temperature, depth, learning_rate, border_count, verbose=0, bo_use = True, NFOLDS = 2):    \n    # split train/test    \n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=345)\n    oof = np.zeros(len(X))\n    models = []\n    \n    for fold, (train_idx, test_idx) in enumerate(folds.split(X, y)):\n        if verbose:\n            print(f\"training on fold {fold + 1}\")\n            \n        params = {'loss_function' : 'MultiClass',\n                  'eval_metric' : \"WKappa\",\n                  'task_type' : \"CPU\",\n                  'verbose' : 1,\n                  'bagging_temperature' : bagging_temperature,\n                  'learning_rate' : learning_rate,\n                  'depth' : int(depth),\n                  'border_count' : int(border_count)}        \n        \n        clf = CatBoostClassifier(**params)\n        \n        args = (X.loc[train_idx, _all_features], y.loc[train_idx])\n        kwargs = {\n            'verbose': 0,\n            'eval_set': (X.loc[test_idx, _all_features], y.loc[test_idx]),                \n        }\n        kw = kwargs.copy()\n        kw.update({\n            'use_best_model': True,\n            'cat_features': cat_features,\n        })\n        clf.fit(*args, **kw)\n        \n        if verbose:\n            print(f\"training on fold {fold + 1} finished... eval model\")\n        \n        pr = clf.predict(X.loc[test_idx, _all_features]).reshape(len(test_idx))\n        oof[test_idx] = pr[:]\n        models.append(clf)\n\n    if bo_use:\n        return qwk(y, oof)\n    else:\n        return qwk(y, oof), models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39f39e97-4dff-4d4c-9ff9-e780ef6cff9a","_cell_guid":"5db7b253-6e32-4979-82e8-5cc380c86bad","trusted":true},"cell_type":"code","source":"# make parameters with bayesian optimization\ndef bo_catboost(X, y):\n    params = {'bagging_temperature': (0, 20),\n              'depth': (5, 10) ,\n              \"learning_rate\" : (0.001, 0.1) , \n              'border_count': (1, 20)}\n    \n    catBO = BayesianOptimization(func_catboost,\n                                 params,\n                                 random_state=0)\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        catBO.maximize(init_points = 10, n_iter = 16)\n    return catBO","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cf1716b-a602-42c0-bf44-76b9b9910182","_cell_guid":"156ef65c-05d9-4d30-9c66-a4c2722d2a36","trusted":true},"cell_type":"code","source":"# optimize parameters\noptimizer = bo_catboost(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e737b36a-4e5f-4171-b721-1532949652f7","_cell_guid":"3e99c789-28bc-4702-a6bf-5c0ed55fbf19","trusted":true},"cell_type":"code","source":"# make models\nparams = optimizer.max['params'] # use best parameter from optimization result\nscore, models = func_catboost(**params, verbose=1, bo_use=False, NFOLDS=6)\n\n# save models to file\nfor i, m in enumerate(models):\n    fname = f'pretrained_models_{i}'\n    m.save_model(fname)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff1d0c81-1107-43ae-a053-7c40e516c308","_cell_guid":"aa126ab6-5699-467e-9777-b7d85ae4c3c3","trusted":true},"cell_type":"markdown","source":"test데이터의 prediction들로 voting을 해서 최종값을 생성한다."},{"metadata":{"_uuid":"ac1bedad-e359-4e06-965a-c3d60b7581d4","_cell_guid":"713abd58-65e8-44a5-8f8a-078e74b3efb9","trusted":true},"cell_type":"code","source":"X, _ = make_model_data(reduce_test, is_train = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed5039b-e358-479a-8e0e-c69f3ec51208","_cell_guid":"f2e1ec0b-8055-497f-9e27-04c8cb016fe5","trusted":true},"cell_type":"code","source":"from collections import Counter \n\n# make test input data\nX, _ = make_model_data(reduce_test, is_train = False)\n\n# make prediction with test data\npredictions = []\nfor model in models:\n    p = np.array(model.predict(X))    \n    if len(p.shape) == 2:\n        p = p.reshape([p.shape[0],])\n    predictions.append(p)\n    \npredictions = np.array(predictions)\np2 = np.transpose(predictions)\n\n# make final prediction with voting.\nfinal_pred = []\nfor i in range(p2.shape[0]):    \n    cur = p2[i]\n    voted_result = Counter(cur).most_common(1)[0][0]\n    final_pred.append(voted_result)\nprint(\"final result shape : \", np.array(final_pred).shape)\npreds = np.array(final_pred)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6260e9e9-0843-401e-b3db-cd4234861232","_cell_guid":"dbd17438-e679-4180-b85b-54b628cd13de","trusted":true},"cell_type":"markdown","source":"# sumbit"},{"metadata":{"_uuid":"c9d810bb-4ee6-4094-b2f8-a0a16b4c06f5","_cell_guid":"7515e781-02bd-4eb2-b79f-d69c619155f4","trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = preds.astype(int)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678a0737-8f35-4b1c-a22b-dc58056ab4c2","_cell_guid":"11f2d06f-96e7-4df5-bc90-45d0668c2dd0","trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}