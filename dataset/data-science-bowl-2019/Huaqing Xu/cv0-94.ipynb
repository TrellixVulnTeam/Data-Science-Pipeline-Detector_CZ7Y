{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# import datetime\n# from catboost import CatBoostClassifier\n# from time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\n# from scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"filepath = '/kaggle/input/data-science-bowl-2019/'\ntrain = pd.read_csv(filepath+'train.csv')\ntrain_labels = pd.read_csv(filepath+'train_labels.csv')\nspecs = pd.read_csv(filepath+'specs.csv')\ntest = pd.read_csv(filepath+'test.csv')\nsubmission = pd.read_csv(filepath+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# encode title\ntrain['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\ntest['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\nall_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n# make a list with all the unique 'titles' from the train and test set\nlist_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n# make a list with all the unique 'event_code' from the train and test set\nlist_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\nlist_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n# make a list with all the unique worlds from the train and test set\nlist_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n# create a dictionary numerating the titles\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\nactivities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\nassess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index)\\\n                     .union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n# replace the text titles with the number titles from the dict\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain['world'] = train['world'].map(activities_world)\ntest['world'] = test['world'].map(activities_world)\ntrain_labels['title'] = train_labels['title'].map(activities_map)\nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110\n# convert text into datetime\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def type_duration_features(type_durations, type_str):\n    \"\"\"\n        type_durations: clip_durations, activity_durations, game_durations, assessment_durations\n        type_str: \"clip\", \"activity\", \"game\", \"assessment\"\n    \"\"\"\n    features = {}\n    if type_durations == []:\n        features[type_str+'_duration_mean'] = 0\n        features[type_str+'_duration_last'] = 0\n        features[type_str+'_duration_max'] = 0\n        features[type_str+'_duration_min'] = 0\n        features[type_str+'_duration_percentil_10'] = 0\n        features[type_str+'_duration_percentil_20'] = 0\n        features[type_str+'_duration_percentil_30'] = 0\n        features[type_str+'_duration_percentil_40'] = 0\n        features[type_str+'_duration_percentil_50'] = 0\n        features[type_str+'_duration_percentil_60'] = 0\n        features[type_str+'_duration_percentil_70'] = 0\n        features[type_str+'_duration_percentil_80'] = 0\n        features[type_str+'_duration_percentil_90'] = 0\n        \n        features[type_str+'_duration_percentil_1'] = 0\n        features[type_str+'_duration_percentil_5'] = 0\n        features[type_str+'_duration_percentil_15'] = 0\n        features[type_str+'_duration_percentil_25'] = 0\n        features[type_str+'_duration_percentil_35'] = 0\n        features[type_str+'_duration_percentil_45'] = 0\n        features[type_str+'_duration_percentil_55'] = 0\n        features[type_str+'_duration_percentil_65'] = 0\n        features[type_str+'_duration_percentil_75'] = 0\n        features[type_str+'_duration_percentil_85'] = 0\n        features[type_str+'_duration_percentil_95'] = 0\n        features[type_str+'_duration_percentil_99'] = 0\n        \n        features[type_str+'_duration_std'] = 0\n        features[type_str+'_duration_var'] = 0\n        features[type_str+'_duration_std_top'] = 0\n        features[type_str+'_duration_std_bottom'] = 0\n        features[type_str+'_duration_max_range'] = 0\n        for i in range(21):\n            features[type_str+'_duration_relative_percentile_{}'.format(i)] = 0\n    else:\n        features[type_str+'_duration_std'] = np.std(type_durations)\n        features[type_str+'_duration_var'] = np.var(type_durations)\n        features[type_str+'_duration_std_top'] = np.mean(type_durations) + np.std(type_durations) \n        features[type_str+'_duration_std_bottom'] = np.mean(type_durations) - np.std(type_durations)\n        features[type_str+'_duration_max_range'] = max(type_durations) - min(type_durations)\n        \n        features[type_str+'_duration_mean'] = np.mean(type_durations)\n        features[type_str+'_duration_last'] = type_durations[-1]\n        features[type_str+'_duration_max'] = max(type_durations)\n        features[type_str+'_duration_min'] = min(type_durations)\n        percentil_calc = np.percentile(type_durations, [10, 20, 30, 40, 50, 60, 70, 80, 90])\n        features[type_str+'_duration_percentil_10'] = percentil_calc[0]\n        features[type_str+'_duration_percentil_20'] = percentil_calc[1]\n        features[type_str+'_duration_percentil_30'] = percentil_calc[2]\n        features[type_str+'_duration_percentil_40'] = percentil_calc[3]\n        features[type_str+'_duration_percentil_50'] = percentil_calc[4]\n        features[type_str+'_duration_percentil_60'] = percentil_calc[5]\n        features[type_str+'_duration_percentil_70'] = percentil_calc[6]\n        features[type_str+'_duration_percentil_80'] = percentil_calc[7]\n        features[type_str+'_duration_percentil_90'] = percentil_calc[8]\n        for i in range(9):\n            features[type_str+'_duration_relative_percentile_{}'.format(i)] = percentil_calc[i] - np.mean(type_durations)\n        \n        percentil_calc = np.percentile(type_durations, [1, 5, 15, 25, 35, 45, 55, 65, 75, 85, 95, 99])\n        features[type_str+'_duration_percentil_1'] = percentil_calc[0]\n        features[type_str+'_duration_percentil_5'] = percentil_calc[1]\n        features[type_str+'_duration_percentil_15'] = percentil_calc[2]\n        features[type_str+'_duration_percentil_25'] = percentil_calc[3]\n        features[type_str+'_duration_percentil_35'] = percentil_calc[4]\n        features[type_str+'_duration_percentil_45'] = percentil_calc[5]\n        features[type_str+'_duration_percentil_55'] = percentil_calc[6]\n        features[type_str+'_duration_percentil_65'] = percentil_calc[7]\n        features[type_str+'_duration_percentil_75'] = percentil_calc[8]\n        features[type_str+'_duration_percentil_85'] = percentil_calc[9]\n        features[type_str+'_duration_percentil_95'] = percentil_calc[10]\n        features[type_str+'_duration_percentil_99'] = percentil_calc[11]\n        for i in range(9,21):\n            features[type_str+'_duration_relative_percentile_{}'.format(i)] = percentil_calc[i-9] - np.mean(type_durations)\n        \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    \n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    assessment_durations = []\n    \n    features = {}\n    \n    for _, session in user_sample.groupby('game_session', sort=False):\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features.update(user_activities_count.copy())\n            features.update(time_spent_each_act.copy())\n            \n            features.update(event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            \n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session_title\n            features['session_world'] = session['world'].iloc[0]\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            ass_duration_fea = type_duration_features(assessment_durations, 'assessment')\n            features.update(ass_duration_fea)\n            assessment_durations.append((session.iloc[-1,2]-session.iloc[0,2]).seconds)\n            \n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0: \n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n                \n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n         # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        \n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n            \n    if test_set:\n        return all_assessments[-1], all_assessments[:-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"compiled_data = []\nfor _, user_sample in tqdm(train.groupby('installation_id', sort=False), total=17000):\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# function that creates more features\ndef preprocess(df):\n    df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n    df['installation_duration_mean'] = df.groupby(['installation_id'])['assessment_duration_mean'].transform('mean')\n    df['installation_duration_std'] = df.groupby(['installation_id'])['assessment_duration_mean'].transform('std')\n    df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n\n    df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, \\\n                                     2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                    4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, \\\n                                     3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                    2040, 4090, 4220, 4095]].sum(axis = 1)\n\n    df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n    df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_train = preprocess(new_train)\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are the features I have generated. Note that all of them are **prior** to each event. For example, the first row shows **before** this assessment, the player have watched 3 clips, did 3 activities, played 4 games and solved 0 assessments, so on so forth."},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_features = [x for x in new_train.columns if x not in ['accuracy_group', 'installation_id', 4070, 'Clip']]\ncat_features = ['session_title','session_world']\nX, y = new_train[all_features], new_train['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, y_train = X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rate_0 = train_labels.query(\"accuracy_group==0\")['accuracy_group'].count() / len(train_labels)\nrate_1 = train_labels.query(\"accuracy_group==1\")['accuracy_group'].count() / len(train_labels)\nrate_2 = train_labels.query(\"accuracy_group==2\")['accuracy_group'].count() / len(train_labels)\nrate_3 = train_labels.query(\"accuracy_group==3\")['accuracy_group'].count() / len(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_test = []\ntest_history = []\nfor _, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    a, history = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    test_history += history\nX_test = pd.DataFrame(new_test)\ntest_his = pd.DataFrame(test_history)\nX_test = preprocess(X_test.append(test_his)).iloc[:len(X_test)]\nX_test = X_test.loc[:, all_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def LGB_bayesian(**var_dict):\n\n    var_dict['num_leaves'] = int(var_dict['num_leaves'])\n    var_dict['min_data_in_leaf'] = int(var_dict['min_data_in_leaf'])\n    var_dict['max_depth'] = int(var_dict['max_depth'])\n\n    assert type(var_dict['num_leaves']) == int\n    assert type(var_dict['min_data_in_leaf']) == int\n    assert type(var_dict['max_depth']) == int\n    \n    param = {\n              'num_leaves': var_dict['num_leaves'], \n              'min_child_samples': var_dict['min_child_samples'], \n              'min_data_in_leaf': var_dict['min_data_in_leaf'],\n              'min_child_weight': var_dict['min_child_weight'],\n              'bagging_fraction' : var_dict['bagging_fraction'],\n              'feature_fraction' : var_dict['feature_fraction'],\n              'learning_rate' : var_dict['learning_rate'],\n              'subsample': var_dict['subsample'], \n              'max_depth': var_dict['max_depth'],\n              'colsample_bytree': var_dict['colsample_bytree'],\n              'reg_alpha': var_dict['reg_alpha'],\n              'reg_lambda': var_dict['reg_lambda'],\n              'objective': 'multiclass',\n              'save_binary': True,\n              'seed': 1337,\n              'feature_fraction_seed': 1337,\n              'bagging_seed': 1337,\n              'drop_seed': 1337,\n              'data_random_seed': 1337,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': True,\n              'boost_from_average': True,\n              'metric':'multi_logloss',\n              'num_class': 4,\n              'device': 'cpu'}    \n    \n    oof = np.zeros((len(X_train),4))\n    score = 0\n    for train_idx, valid_idx in skf.split(X_train, y_train.values):\n        trn_data = lgb.Dataset(X_train.loc[train_idx, all_features].values,\n                                       label=y_train.loc[train_idx].values\n                                       )\n        val_data = lgb.Dataset(X_train.loc[valid_idx, all_features].values,\n                                       label=y_train.loc[valid_idx].values\n                                       ) \n        clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, \\\n                        early_stopping_rounds = 50)\n    \n        oof[valid_idx]  = clf.predict(X_train.loc[valid_idx, all_features].values, \\\n                                      num_iteration=clf.best_iteration)\n        \n    result = np.argmax(oof, axis=1)\n    score = qwk(y_train.values, result)\n    \n    trn_data = lgb.Dataset(X_train.loc[:,all_features].values, label=y_train.values)\n    clf = lgb.train(param, trn_data, num_boost_round=50, verbose_eval=0)\n    \n    preds = np.round(np.argmax(clf.predict(X_test.values), axis=1)).astype('int')\n    \n    return score - ( \n                        abs(preds.tolist().count(0)/1000 - rate_0) \\\n                      + abs(preds.tolist().count(1)/1000 - rate_1) \\\n                      + abs(preds.tolist().count(2)/1000 - rate_2) \\\n                      + abs(preds.tolist().count(3)/1000 - rate_3)\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (10, 900), \n    'min_data_in_leaf': (2, 900),\n    'bagging_fraction' : (0.1, 0.99999999),\n    'feature_fraction' : (0.0001, 0.99),\n    'learning_rate': (0.01, 0.99999999),\n    'min_child_weight': (0.000001, 0.05),   \n    'min_child_samples':(10, 900), \n    'subsample': (0.001, 0.8),\n    'colsample_bytree': (0.0001, 0.99), \n    'reg_alpha': (0.1, 3), \n    'reg_lambda': (1, 5),\n    'max_depth': (-2, 90),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('-' * 30)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=10, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BO = {'bagging_fraction': 0.99999999,\n 'colsample_bytree': 0.99,\n 'feature_fraction': 0.99,\n 'learning_rate': 0.99999999,\n 'max_depth': 90,\n 'min_child_samples': 10.0,\n 'min_child_weight': 0.05,\n 'min_data_in_leaf': 351,\n 'num_leaves': 900,\n 'reg_alpha': 3.0,\n 'reg_lambda': 5.0,\n 'subsample': 0.8}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param_lgb = {\n        'objective': 'multiclass',\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': True,\n        'boost_from_average': True,\n        'metric':'multi_logloss',\n        'num_class': 4,\n        'device': 'CPU'\n    }\nparam_lgb.update(BO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# CV \noof = np.zeros((len(X_train),4))\noof_train = np.zeros((len(X_train),4))\nscore = 0\nfeature_importance_df = pd.DataFrame()\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train.values)):\n    print('-'*30)\n    print(f'fold {fold+1}')\n    print('-'*30)\n    trn_data = lgb.Dataset(X_train.loc[train_idx, all_features].values,\n                                   label=y_train.loc[train_idx].values\n                                   )\n    val_data = lgb.Dataset(X_train.loc[valid_idx, all_features].values,\n                                   label=y_train.loc[valid_idx].values\n                                   ) \n    clf = lgb.train(param_lgb, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=10, \\\n                    early_stopping_rounds = 50)\n\n    oof[valid_idx]  = clf.predict(X_train.loc[valid_idx, all_features].values, \\\n                                  num_iteration=clf.best_iteration)\n    oof_train[train_idx] += clf.predict(X_train.loc[train_idx, all_features].values, \\\n                                  num_iteration=clf.best_iteration) / (nfold-1)\n    \n    # Features imp\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = all_features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \nresult_train = np.argmax(oof_train, axis=1)    \nscore_train = qwk(y_train.values, result_train)\nresult = np.argmax(oof, axis=1)\nscore = qwk(y_train.values, result)\nprint(f'cv train qwk: {score_train}')\nprint(f'cv valid qwk: {score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# featrue importance plot \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('dark_background')\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", \n            data=pd.DataFrame(feature_importance_df.groupby('Feature')['importance'].mean().sort_values(ascending=False))\\\n            .reset_index().iloc[:30],\n        edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('LGB Features importance (averaged/folds)', fontsize=18)\nplt.tight_layout()\n\n# useful features : 4035, 'session_title', 'Sandcastle Builder (Activity)', 2000, 4025, 4030, 4020, \\\n# 'assessment_duration_last', 'accumulated_actions', 'Chow Time', 4040, 3120, 'session_world', 'installation_session_count', \\\n# 'installation_duration_mean'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train on all data once\ntrn_data = lgb.Dataset(X_train.loc[:,all_features].values, label=y_train.values)\nclf = lgb.train(param_lgb, trn_data, num_boost_round=50, verbose_eval=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds = np.argmax(clf.predict(X_test.values), axis=1)\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission['accuracy_group'] = np.round(preds).astype('int')\nsubmission.to_csv('submission.csv', index=None)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.Series(result).plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"00232c2fa6c044d8b49a7b950a2138f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cfd0579bb7a46339a49011de13f0b84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_f420f346866241c8af0ff73e202a8c08","max":17000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_231d0baa88e34824b78ab36305efdf29","value":17000}},"158a91fbd530494caab543688c79e28f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a33d3e7633a45618212d0ff02689a5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ab059c28dda4d328bd1a3d6af86a3b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"231d0baa88e34824b78ab36305efdf29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"2c1778a748ac4e259ab1dbfdb08d0938":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d7ed437d07640e29cad03db74dbf826":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4174ae35e7ea488ab473043cecaf5e62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"7777aa81d56c4b4f89be5d164bed9059":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f19c6bdcba2f46fcb18de971f104c264","placeholder":"​","style":"IPY_MODEL_00232c2fa6c044d8b49a7b950a2138f9","value":" 1000/1000 [00:41&lt;00:00, 24.37it/s]"}},"7c146404b28b4b978885b08ca6772eeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_1ab059c28dda4d328bd1a3d6af86a3b5","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4174ae35e7ea488ab473043cecaf5e62","value":1000}},"89e56379eadf46a7863844b680da6794":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a33d3e7633a45618212d0ff02689a5d","placeholder":"​","style":"IPY_MODEL_158a91fbd530494caab543688c79e28f","value":" 17000/17000 [06:59&lt;00:00, 40.54it/s]"}},"93fb17ea9529448bb911a03eba422985":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cfd0579bb7a46339a49011de13f0b84","IPY_MODEL_89e56379eadf46a7863844b680da6794"],"layout":"IPY_MODEL_2d7ed437d07640e29cad03db74dbf826"}},"f19c6bdcba2f46fcb18de971f104c264":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f420f346866241c8af0ff73e202a8c08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f461b6651a644f00866df8a80bf65395":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c146404b28b4b978885b08ca6772eeb","IPY_MODEL_7777aa81d56c4b4f89be5d164bed9059"],"layout":"IPY_MODEL_2c1778a748ac4e259ab1dbfdb08d0938"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}