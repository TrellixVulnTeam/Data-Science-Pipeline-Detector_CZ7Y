{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### generating final test data having 1000 unique installation_id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference : https://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline\n# In the reference blog they have used two separate function for featurization. I slightly modified and compressed it in only one function.\ndef get_features(installation_id , dataframe_for_an_id , test_flag=False):\n    '''\n    \n    This function will calculate features for train and test data. \n    It will create 4 columns for four unique world(including None) and\n    will create 44 columns for 44 unique title and\n    will create 4 columns for four unique type and\n    will create 42 columns for 42 unique event_code and\n    will create 6 more columns for 'total_duration','total_action','correct_count','incorrect_count','accuracy','accuracy_group'\n               ---\n        total  100 columns \n    \n    except total_duration, accuracy and accuracy_group all other features is number of counts of those feature in a game_session\n    if test_flag is True then return last entry of list \n    '''\n    # temp_dict initialized with keys (100 columns) and value = 0\n    features = []\n    features.extend(list(set(train['world'].unique()).union(set(test['world'].unique())))) # all unique worlds in train and test data\n    features.extend(list(set(train['title'].unique()).union(set(test['title'].unique())))) # all unique title in train and test data\n    features.extend(list(set(train['type'].unique()).union(set(test['type'].unique())))) # all unique type in train and test data\n    features.extend(list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))) # all unique event_code in train and test data \n    features.extend(['total_duration','total_action','correct_count','incorrect_count','accuracy','accuracy_group'])\n    temp_dict = dict.fromkeys(features,0)\n    list_of_features = []\n    \n    \n    def get_all_attempt(sample_df):\n        '''\n        This fuction will return the dataframe which is used to calculate accuracy_group\n        '''\n        if sample_df['title'].iloc[0] != 'Bird Measurer (Assessment)':\n            all_attempt = sample_df.query('event_code == 4100')\n        elif sample_df['title'].iloc[0] == 'Bird Measurer (Assessment)':\n            all_attempt = sample_df.query('event_code == 4110')\n        return all_attempt\n    \n    for i, sample_df in dataframe_for_an_id.groupby(by = ['game_session']):\n        # sample_df is groupby object \n        # In sample_df 'type','title' and 'world' will not change so first entry of those column is piced\n        temp_dict['installation_id'] = installation_id\n        temp_type = sample_df['type'].iloc[0]\n        temp_title = sample_df['title'].iloc[0]\n        temp_world = sample_df['world'].iloc[0]\n        temp_event_code = Counter(sample_df['event_code'].values)\n\n        session_size = len(sample_df)\n\n        temp_dict[temp_type]+=session_size\n        temp_dict[temp_title]+=session_size    # corresponding type , title and world is incremented by session size\n        temp_dict[temp_world]+=session_size                 \n\n        for code, code_count in temp_event_code.items():    # corresponding event_code is incremented\n            temp_dict[code]+= code_count                  \n            \n        duration_in_sec = float(sample_df['game_time'].iloc[-1])/1000   # total_duration is duration of game_session in seconds\n        temp_dict['total_duration'] += duration_in_sec\n        \n        action_count_in_game_session = session_size     # total number of action performed in game_session\n        temp_dict['total_action'] += action_count_in_game_session  \n        \n        isAssessment = temp_type == 'Assessment'\n        isBirdMeasureAssessment = isAssessment and temp_title == 'Bird Measurer (Assessment)'\n        isAssessment_with_code4110 = isBirdMeasureAssessment and 4110 in list(sample_df['event_code'])\n        isNonBirdMeasureAssessment = isAssessment and temp_title != 'Bird Measurer (Assessment)'\n        isAssessment_with_code4100 = isNonBirdMeasureAssessment and 4100 in list(sample_df['event_code'])\n        \n        criterion_to_accuracy_group = isAssessment_with_code4110 or isAssessment_with_code4100 \n        \n        \n        if test_flag and isAssessment and (criterion_to_accuracy_group == False):\n            temp_dict['accuracy'] = 0           # there are lots of installation_id in test data that attempt for\n            temp_dict['accuracy_group'] = 0     # Assessment but not with event_code 4100 or 4110\n            list_of_features.append(temp_dict)  # So I assumed those id belongs to class 0\n            \n            \n        if criterion_to_accuracy_group == False:\n            continue\n        \n        # below section is only performed when criterion_to_accuracy_group is True\n        \n        all_attempt = get_all_attempt(sample_df)\n        correct_count = all_attempt['event_data'].str.contains('true').sum()     \n        incorrect_count = all_attempt['event_data'].str.contains('false').sum()\n        temp_dict['correct_count'] = correct_count  \n        temp_dict['incorrect_count'] = incorrect_count\n\n        if correct_count == 0 and incorrect_count == 0:\n            temp_dict['accuracy'] = 0\n        else:\n            temp_dict['accuracy'] = correct_count/(correct_count + incorrect_count)\n\n        if temp_dict['accuracy']==1:\n            temp_dict['accuracy_group']=3\n        elif temp_dict['accuracy']==0.5:\n            temp_dict['accuracy_group']=2\n        elif temp_dict['accuracy']==0:\n            temp_dict['accuracy_group']=0\n        else :\n            temp_dict['accuracy_group']=1\n\n        list_of_features.append(temp_dict)\n        temp_dict = dict.fromkeys(features,0)\n        \n        \n    if test_flag:                    # If given data is from test data then return only the last entry of the list\n        return list_of_features[-1]\n    \n    return list_of_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = train[train.installation_id == \"0006a69f\"]\nlist_of_feature = get_features('0006a69f',sample_df,True)\nlist_of_feature\ntemp_df = pd.DataFrame(list_of_feature,index=[0])\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nfinal_test_data_list = []\ntest_groupby_id = test.groupby(by=['installation_id'])\nfor installation_id , df_with_unique_id in tqdm(test_groupby_id):\n    final_test_data_list.append(get_features(installation_id,df_with_unique_id,True))\nfinal_test_data = pd.DataFrame(final_test_data_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## generating 1000 random integer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import seed\nfrom random import randint\n\nseed(1)\nrandom_int = []\nfor _ in range(1000):\n    value = randint(0,3)\n    random_int.append(value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'installation_id':final_test_data['installation_id'],'accuracy_group':random_int})\nmy_submission.to_csv('submission.csv',index =False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}