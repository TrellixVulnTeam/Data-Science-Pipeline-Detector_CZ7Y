{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom catboost import CatBoostRegressor\nfrom matplotlib import pyplot\nimport shap\nimport random\nrandom.seed(42)\nimport os\n\n\n# Any results you write to the current directory are saved as output.\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport json\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/data-science-bowl-2019/')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_labels, specs, sample_submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_feature(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['hour'] = df['timestamp'].dt.hour\n    df['weekday'] = df['timestamp'].dt.weekday\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = time_feature(train)\ntest = time_feature(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_list = list(set(train['title'].unique()).union(set(test['title'].unique())))\nevent_id_list = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\nevent_code_list = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\nworld_list = list(set(train['world'].unique()).union(set(test['world'].unique())))\ntype_list = list(set(train['type'].unique()).union(set(test['type'].unique())))\nhour_list = list(set(train['hour'].unique()).union(set(test['hour'].unique())))\nassessment_list = ['Bird Measurer (Assessment)', 'Cart Balancer (Assessment)', 'Cauldron Filler (Assessment)',\n                   'Chest Sorter (Assessment)', 'Mushroom Sorter (Assessment)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mini_train = train.iloc[:50000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef tianqi_get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    \n    type_count: Dict[str, int] = {str(tp)+'_cnt': 0 for tp in type_list}\n    event_code_count: Dict[str, int] = {str(event_code)+'_cnt': 0 for event_code in event_code_list}\n    event_id_count: Dict[str, int] = {str(event_id)+'_cnt': 0 for event_id in event_id_list}\n    title_count: Dict[str, int] = {str(title)+'_cnt': 0 for title in title_list}\n    world_count: Dict[str, int] = {str(world)+'_cnt': 0 for world in world_list}\n    hour_count: Dict[str, int] = {str(hour)+'_cnt': 0 for hour in hour_list}\n            \n               \n    all_assessments = []\n    since_last_assessment = []\n    last_assessment_incorret = None\n    last_title_assessment_accuracy = {'last_'+str(assess)+'_acc': None for assess in assessment_list}\n    accumulated_assessment_correct = {'accumulated_'+str(assess)+'_correct': 0 for assess in assessment_list}\n    accumulated_assessment_incorrect = {'accumulated_'+str(assess)+'_incorrect': 0 for assess in assessment_list}\n    \n    all_attemp_correct = 0\n    all_attemp_incorrect = 0\n    \n    \n    recent_attempts = [None, None, None, None, None]\n    \n    \n    accumulated_incorrect = 0\n    accumulated_correct = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        session_title = session['title'].iloc[0]\n        session_type = session['type'].iloc[0]\n        win_code = 4110 if session_title == 'Bird Measurer (Assessment)' else 4100\n        session_hour = session['hour'].iloc[0]\n        hour_count[str(session_hour)+'_cnt'] += 1\n        \n        if (session_type == 'Clip'):\n            type_count[session_type+'_cnt'] += 1\n            title_count[session_title+'_cnt'] += 1\n            \n        if (session_type == 'Activity'):\n            type_count[session_type+'_cnt'] += 1\n            title_count[session_title+'_cnt'] += 1\n            event_code_map = Counter(session['event_code'])\n            event_id_map = Counter(session['event_id'])\n            \n            for event_code in event_code_map:\n                event_code_count[str(event_code)+'_cnt'] += event_code_map[event_code]\n                \n            event_id_map = dict(session['event_id'].value_counts())\n            for event_id in event_id_map:\n                event_id_count[str(event_id)+'_cnt'] += event_id_map[event_id]\n                \n        if (session_type == 'Game'):\n            type_count[session_type+'_cnt'] += 1\n            title_count[session_title+'_cnt'] += 1\n            \n            event_code_map = Counter(session['event_code'])\n            event_id_map = Counter(session['event_id'])\n            \n            true_attempts = len(session[session['event_code']==3021])\n            false_attempts = len(session[session['event_code']==3020])\n            \n            all_attemp_correct += true_attempts\n            all_attemp_incorrect += false_attempts\n            \n            if true_attempts+false_attempts>0:\n                if true_attempts==0: accuracy=-1\n                else: accuracy = true_attempts/(true_attempts+false_attempts)\n                _ = recent_attempts.pop(0)\n                recent_attempts.append(accuracy)\n            \n            \n            for event_code in event_code_map:\n                event_code_count[str(event_code)+'_cnt'] += event_code_map[event_code]\n                \n            event_id_map = dict(session['event_id'].value_counts())\n            for event_id in event_id_map:\n                event_id_count[str(event_id)+'_cnt'] += event_id_map[event_id]\n                \n            \n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            \n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session[session['event_code']==win_code]\n\n            \n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n#             start_time = session['timestamp'].iloc[0]\n#             end_time = session['timestamp'].iloc[-1]\n\n            features = {'correct_num': true_attempts,\n                        'incorrect_num': false_attempts}\n            \n            features['installation_id'] = session['installation_id'].iloc[-1]\n            features['assess_hour_precnt'] = hour_count[str(session['hour'].iloc[-1])+'_cnt']\n            features['game_session'] = i\n            features['world']  = session['world'].iloc[-1]\n            \n            features['assess_weekday']  = session['timestamp'].dt.weekday.iloc[-1]\n            features['title'] = session_title\n            features['accumulated_incorrect'] = accumulated_incorrect\n            features['accumulated_correct'] = accumulated_correct\n            features['accumulated_accuracy'] = None if (accumulated_incorrect+accumulated_correct)==0 else\\\n                                                accumulated_correct/(accumulated_incorrect+accumulated_correct)\n            \n            features['accumulated_title_correct'] = accumulated_assessment_correct['accumulated_'+str(session_title)+'_correct']\n            features['accumulated_title_incorrect'] = accumulated_assessment_incorrect['accumulated_'+str(session_title)+'_incorrect']\n            features['accumulated_title_accuracy'] = None if (features['accumulated_title_incorrect']+features['accumulated_title_correct'])==0 else\\\n                                                features['accumulated_title_correct']/(features['accumulated_title_incorrect']+features['accumulated_title_correct'])\n            features['last_title_acc'] = last_title_assessment_accuracy['last_'+str(session_title)+'_acc']\n            \n            recent_acc = {\n                'pre_five_attempt_acc': recent_attempts[0],\n                'pre_four_attempt_acc': recent_attempts[1],\n                'pre_three_attempt_acc': recent_attempts[2],\n                'pre_two_attempt_acc': recent_attempts[3],\n                'pre_one_attempt_acc': recent_attempts[4],\n                          \n            }\n            \n            features['all_attemp_correct'] = all_attemp_correct\n            features['all_attemp_incorrect'] = all_attemp_incorrect\n            features['all_attemp_accuracy'] = None if (features['all_attemp_correct']+features['all_attemp_incorrect'])==0 else\\\n                                                features['all_attemp_correct']/(features['all_attemp_correct']+features['all_attemp_incorrect'])\n            features.update(recent_acc)\n            features.update(type_count)\n            features.update(title_count)\n            features.update(event_code_count)\n            features.update(event_id_count)\n            \n#             features['assess_hour']  = session['timestamp'].dt.hour.iloc[-1]\n#             features.update(hour_count)\n            \n            if test_set or (true_attempts+false_attempts > 0):\n                all_assessments.append(features)\n                \n                last_title_assessment_accuracy['last_'+str(session_title)+'_acc'] = true_attempts/(true_attempts+false_attempts)\n                \n            \n            \n            type_count[session_type+'_cnt'] += 1\n            title_count[session_title+'_cnt'] += 1\n            \n            \n            accumulated_correct += true_attempts\n            accumulated_incorrect += false_attempts\n            accumulated_assessment_correct['accumulated_'+str(session_title)+'_correct'] += true_attempts\n            accumulated_assessment_incorrect['accumulated_'+str(session_title)+'_incorrect'] += false_attempts\n            \n            event_code_map = Counter(session['event_code'])\n            event_id_map = Counter(session['event_id'])\n            \n            for event_code in event_code_map:\n                event_code_count[str(event_code)+'_cnt'] += event_code_map[event_code]\n                \n            event_id_map = dict(session['event_id'].value_counts())\n            for event_id in event_id_map:\n                event_id_count[str(event_id)+'_cnt'] += event_id_map[event_id]\n\n                \n            true_attempts = len(session[session['event_code']==3021])\n            false_attempts = len(session[session['event_code']==3020])\n            \n            all_attemp_correct += true_attempts\n            all_attemp_incorrect += false_attempts\n            \n            if true_attempts+false_attempts>0:\n                if true_attempts==0: accuracy=-1\n                else: accuracy = true_attempts/(true_attempts+false_attempts)\n                _ = recent_attempts.pop(0)\n                recent_attempts.append(accuracy)\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_and_test(train, test=None):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += tianqi_get_data(user_sample)\n    reduce_train = pd.DataFrame(compiled_train)\n    \n    if test is not None:\n        for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n            test_data = tianqi_get_data(user_sample, test_set = True)\n            compiled_test.append(test_data)\n        reduce_test = pd.DataFrame(compiled_test)\n        return reduce_train, reduce_test\n    return reduce_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick check on mini data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = get_train_and_test(mini_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train, reduce_test = get_train_and_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(mark, y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(mark)\n    for k in dist:\n        dist[k] /= len(mark)\n#     mark.hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train = reduce_train[reduce_train['correct_num']+reduce_train['incorrect_num']>0]\nfinal_test = reduce_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train['accuracy_group'] = 3\nfinal_train.loc[final_train['incorrect_num']==1,'accuracy_group'] = 2\nfinal_train.loc[final_train['incorrect_num']>1,'accuracy_group'] = 1\nfinal_train.loc[final_train['correct_num']==0,'accuracy_group'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_features = ['installation_id', 'game_session', 'correct_num', 'incorrect_num']\nTARGET = 'accuracy_group'\nfeatures = [col for col in final_train.columns if col not in remove_features and col!=TARGET]\ncat_features = [col for col in features if final_train[col].dtype == 'O' or final_test[col].dtype == 'O']\nfor col in cat_features:\n    final_train[col] = final_train[col].astype('category')\n    final_test[col] = final_test[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in final_train.columns]\nfinal_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in final_test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in final_train.columns if col not in remove_features and col!=TARGET]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Quick Single Fold Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninstallation_id_list = list(set(train_labels.installation_id.unique()))\ntrain_install_id = set(random.sample(installation_id_list, 2800))\nvalid_install_id = set(installation_id_list) - train_install_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = final_train[final_train['installation_id'].isin(train_install_id)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data = pd.DataFrame()\nvalid_session_id = final_train[final_train['installation_id'].\\\n                           isin(valid_install_id)].groupby('installation_id').\\\n                            apply(lambda x: random.choice(x['game_session'].values))\nvalid_data = final_train[final_train['game_session'].isin(valid_session_id)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data[features].shape)\nprint(valid_data[features].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':5000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.01,\n            'feature_fraction': 0.6,\n            'max_depth': 10,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'early_stopping_rounds': 100,\n            'seed': 42,\n            'n_jobs': 8\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data = lgb.Dataset(train_data[features], train_data[TARGET], categorical_feature=cat_features)\nvl_data = lgb.Dataset(valid_data[features], valid_data[TARGET], categorical_feature=cat_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.train(\n                params,\n                tr_data,\n                valid_sets = [tr_data,vl_data],\n                verbose_eval = 100,\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(valid_data[features])\ny_true = valid_data[TARGET]\nkappa = eval_qwk_lgb_regr(train_data[TARGET], y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kappa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=28)\ninstallation_id_list = final_train.installation_id.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Hour count does not work\n - Last title accuracy work 0.539 => 0.542\n - Accumulated title correct/incorrect work 0.542 => 0.546"},{"metadata":{},"cell_type":"markdown","source":"TO DO:\n - Monitoring best threshold for cv, stabilize.\n - Adding group activity/game info in the model.\n - Monitoring the performance on each title."},{"metadata":{"trusted":true},"cell_type":"code","source":"# put numerical value to one of bins\ndef to_bins(x, borders):\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)\n\nclass OptimizedRounder3(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _loss(self, coef, X, y, idx):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -cohen_kappa_score(y, X_p, weights='quadratic')\n        return ll\n\n    def fit(self, X, y):\n        coef = [0.15, 0.3, 0.50]\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(0.075, 0.225), (0.2, 0.4), (0.4, 0.6)]\n        for it1 in range(10):\n            for idx in range(3):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                coef[idx] = a\n                la = self._loss(coef, X, y, idx)\n                coef[idx] = b\n                lb = self._loss(coef, X, y, idx)\n                for it in range(20):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        coef[idx] = a\n                        la = self._loss(coef, X, y, idx)\n                    else:\n                        b = b - (b - a) * golden2\n                        coef[idx] = b\n                        lb = self._loss(coef, X, y, idx)\n        self.coef_ = {'x': coef}\n\n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cauchyobj(preds, dtrain):\n    labels = dtrain.get_label()\n    c = 5000 \n    x =  preds-labels    \n    grad = x / (x**2/c**2+1)\n    hess = -c**2*(x**2-c**2)/(x**2+c**2)**2\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_predict = []\npred = np.zeros(shape = (len(sample_submission), 5))\nAVG_NUM = 50\nall_round_kappa = 0\nall_round_truncated_kappa = 0\noverall_truncated_kappa = 0\noverall_kappa = 0\nrmse = 0\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(installation_id_list)):\n    print(f'Fold: {fold}')\n    print(f'Train installation: {len(train_idx)} Valid installation: {len(valid_idx)}')\n    train_install_id = installation_id_list[train_idx]\n    valid_install_id = installation_id_list[valid_idx]\n\n    random.seed(23)\n    \n    train_data = final_train[final_train['installation_id'].isin(train_install_id)]\n    valid_data = final_train[final_train['installation_id'].isin(valid_install_id)].reset_index()\n    \n    print(f'Train data size: {len(train_data)}, Valid data size: {len(valid_data)}')\n    tr_data = lgb.Dataset(train_data[features], train_data[TARGET], categorical_feature=cat_features)\n    vl_data = lgb.Dataset(valid_data[features], valid_data[TARGET], categorical_feature=cat_features)\n    \n    \n    \n    model = lgb.train(\n                params,\n                tr_data,\n                valid_sets = [tr_data,vl_data],\n                verbose_eval = 100,\n                fobj=cauchyobj\n            )\n    y_pred = model.predict(valid_data[features])\n    y_pred_rank = rankdata(y_pred)/len(y_pred)\n    y_true = valid_data[TARGET]\n    rmse += np.sqrt(np.mean(np.square(y_true - y_pred)))/5\n#     print(rmse)\n    kappa = eval_qwk_lgb_regr(train_data[TARGET], y_true, y_pred)\n    print(f'Kappa on all valid: {kappa:.3f}')\n    \n    y_pred_train = model.predict(train_data[features])\n    y_pred_train_rank = (rankdata(y_pred_train)/len(y_pred_train))\n    optR = OptimizedRounder3()\n    optR.fit(y_pred_train_rank, train_data[TARGET].values)\n    round_kappa = cohen_kappa_score(optR.predict(y_pred_rank, coef=optR.coefficients()),y_true,weights='quadratic')\n    print(f'Round Kappa on all valid: {round_kappa:.3f}')\n\n    truncated_rmse = 0\n    truncated_kappa = 0\n    truncated_round_kappa = 0\n\n    for i in range(AVG_NUM):\n        random.seed(28+i)\n        eval_idx = valid_data.groupby('installation_id')['game_session'].\\\n                                        apply(lambda x: random.choice(x.index.values))\n        y_eval_pred = model.predict(valid_data.loc[eval_idx, features])\n        y_eval_pred_rank = (rankdata(y_eval_pred)/len(y_eval_pred))\n        y_eval_true = valid_data.loc[eval_idx, TARGET]\n        truncated_kappa += eval_qwk_lgb_regr(train_data[TARGET], y_eval_true, y_eval_pred)/AVG_NUM\n        truncated_rmse += np.sqrt(np.mean(np.square(y_eval_true - y_eval_pred)))/AVG_NUM\n        truncated_round_kappa += cohen_kappa_score(optR.predict(y_eval_pred_rank, coef=optR.coefficients()),y_eval_true,weights='quadratic')/AVG_NUM\n    \n    print(f'Truncated Kappa: {truncated_kappa:.3f}')\n    print(f'Truncated Round Kappa: {truncated_round_kappa:.3f}')\n    print(f'Truncated RMSE: {truncated_rmse:.3f}')\n    \n    all_round_kappa += round_kappa/5\n    overall_kappa += kappa/5\n    overall_truncated_kappa += truncated_kappa/5\n    all_round_truncated_kappa += truncated_round_kappa/5\n    \n    pred[:, fold] = rankdata(model.predict(final_test[features]))/len(final_test)\nprint(f'CV Kappa: {overall_kappa}, RMSE: {rmse}')\nprint(f'CV Round Kappa: {all_round_kappa}')\nprint(f'Truncated CV Kappa: {overall_truncated_kappa}')\nprint(f'Truncated Round CV Kappa: {all_round_truncated_kappa}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 0 0.25740936499993927\n- 1 0.1254762196691288\n- 2 0.12295055710383329\n- 3 0.49416385822709863"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr_data = lgb.Dataset(final_train[features], final_train[TARGET], categorical_feature=cat_features)\n# params['n_estimators']=1000\n# model = lgb.train(\n#             params,\n#             tr_data,\n#             num_boost_round=1000,\n#             valid_sets = [tr_data],\n#             verbose_eval = 100,\n#             fobj=cauchyobj,\n#         )\n# pred = model.predict(final_test[features])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train = model.predict(final_train[features])\npred_train_rank = rankdata(pred_train)/len(pred_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder3()\n\noptR.fit(pred_train_rank, final_train[TARGET].values)\nfinal_pred = optR.predict(pred.mean(axis=1), coef=optR.coefficients())\nfinal_pred[final_pred==4]=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calibrate(y_pred, train_t):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n#     dist = Counter(train_t['accuracy_group'])\n#     for k in dist:\n#         dist[k] /= len(train_t)\n    dist = [0.25740936499993927, 0.1254762196691288, 0.12295055710383329]\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred)))\n    \n    return y_pred\n\ndef predict(sample_submission, y_pred):\n    sample_submission['accuracy_group'] = y_pred\n    sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n    sample_submission.to_csv('submission.csv', index = False)\n    print(sample_submission['accuracy_group'].value_counts(normalize = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_pred = calibrate(pred, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(sample_submission,final_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}