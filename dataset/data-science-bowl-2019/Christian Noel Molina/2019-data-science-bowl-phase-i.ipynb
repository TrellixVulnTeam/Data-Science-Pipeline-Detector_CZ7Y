{"cells":[{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# !cat /proc/meminfo # Inquire system memory\n# !df -h / | awk '{print $4}' # Inquire system disk\n# !lscpu  # Inquire system processor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport gc\nimport ast\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n!pip install --upgrade orjson\nimport orjson\n\nimport os\nimport sys\ndef print_log(string):\n    os.system(f'echo \\\"{string}\\\"')\n\nimport dask\nimport dask.dataframe as dd\nfrom dask.distributed import Client, progress, wait\nfrom dask.diagnostics import ProgressBar\npbar = ProgressBar()\npbar.register()\nimport numexpr \nnumexpr.set_num_threads(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading and viewing the dataset"},{"metadata":{},"cell_type":"markdown","source":"Let's load all the concerned datasets. So the *df_train* here contains main dataset, *df_media* contains all the **title**s with their corresponding duration if they are of **type** *Clip* and ordered in the ideal sequence of the game, and the *df_specs* contains the specifications of all **event_id**s with their information usage and arguments."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def load_dataset():\n    data_dtypes = {\"type\": \"category\", \"title\": \"category\", \"event_data\": \"str\", \"event_id\": \"str\", \"installation_id\": \"str\", \"event_code\": \"category\",\n                    \"world\": \"category\", \"str\": \"category\",\"event_count\": \"uint8\", \"game_time\": \"uint32\"}\n\n    print_log(\"Reading training data ...\")\n    df_train = dd.read_csv(\"../input/data-science-bowl-2019/train.csv\", parse_dates=[\"timestamp\"], dtype=data_dtypes)\n    \n    media_dtypes = {\"title\": \"category\",\"type\": \"category\", \"duration\": \"float64\"}\n    print_log(\"Reading media sequence ...\")\n    df_media = pd.read_csv(\"../input/data-science-bowl-2019-media-sequence/media_sequence.csv\", dtype=media_dtypes)\n\n    print_log(\"Reading specifications for event ids ...\")\n    df_specs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\n    \n    return df_train, df_media, df_specs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_train, df_media, df_specs = load_dataset()\ndf_media.duration = df_media.duration.fillna(0).astype('float32')\ndf_titles = df_media.title.unique().tolist()\nassessment_attempt_event_code_query = \"((event_code=='4100' & title!='Bird Measurer (Assessment)') | (event_code=='4110' & title=='Bird Measurer (Assessment)'))\"\nattempts_query = \"type=='Assessment' & \" +  assessment_attempt_event_code_query\nprint_log(\"Finised loading datasets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data doesn't have really that much columns where we can gain additional insights/signals for making our predictions later and is really straightforward as it is. But notice the **event_data** column seems to contain much more data giving details in a specific event of the gameplay.\n\nWhen we take a peek at one of the specifications for the event ids, we can see that each **event_id** represents a specific detail about the gameplay supported with data provided by the **args** column. We'll take a look at a particular **event_id** namely ***2b9272f4*** in this case to get an idea of what an **info** and **args** value looks like."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs.query(\"event_id=='2b9272f4'\")[\"info\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs.query(\"event_id=='2b9272f4'\")[\"args\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though as we investigate further, even though there are 386 unique event ids, there are only 168 unique values and 191 unique values for the **info** and **args** attributes respectively. This might mean that there are event ids that totally represent the same info, but might represent only a specific **title** (e.g Sandcastle Builder)\n\nIn this case we'll focus on the unique **args** for the event ids since we can only based from there what data we can extract from any given **event_id** after all. From now here on, when we say **args**, we could refer it as the actual array of arguments/attributes like the ones shown above or the actual argument/attribute contained in the array (e.g *round*), and we'll use the context interchangeably."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let's encode those **args** and map them correspondingly on the **event_id**s of our dataset. After that, we'll then plot a heatmap to see which kind of **args** are represented by the different **event_id**s\n\n> df_specs contains the specifications and info about each available event_id that can be seen in the dataset which is contained in df_train"},{"metadata":{},"cell_type":"markdown","source":"## Label-encoding each unique args in the df_specs and df_train"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"print_log(\"Encoding args column ...\")\nargs_to_int_map = {args: i for i, args in enumerate(df_specs[\"args\"].unique())}\ndf_specs[\"args_encoded\"] = df_specs[\"args\"].map(args_to_int_map)\n\neventid_to_argsint_map = df_specs.set_index(\"event_id\")[\"args_encoded\"].to_dict()\ndf_train = df_train.assign(**{\"args_encoded\": df_train[\"event_id\"].map(eventid_to_argsint_map)})\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def generate_argstype_map(argstype_encoded, use_client=False):\n    unique_args = df_specs[argstype_encoded].unique()\n    dask_with_args = [df_train[df_train[argstype_encoded]==i].title.unique() for i in range(len(unique_args))]\n    if use_client:\n        dask_with_args = client.compute(dask_with_args)\n        progress(dask_with_args, notebook=False)\n        titles_with_args = client.gather(dask_with_args)\n    else:\n        titles_with_args = dask.compute(dask_with_args)[0]\n    titles_with_args_i = {i: titles_with_args[i].tolist() for i in range(len(unique_args))}\n    return titles_with_args_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"def generate_titleargs_table(titles_with_args):\n    # Create a dataframe with all the titles and the corresponding args an event id might represent\n    # The dataframe is binary, 1 if present, 0 otherwise\n    args_length = len(titles_with_args)\n\n    title_args_table = list()\n    for title in df_titles:\n        title_args_row = [0.0 for i in range(args_length)]\n        title_args_table.append(title_args_row)\n\n    args_encoded_col = list(range(args_length))\n    title_args_table = pd.DataFrame(title_args_table, \n                                    index=df_titles,\n                                    columns=args_encoded_col)\\\n        .reset_index().rename(columns={\"index\":\"title\"}).set_index(\"title\")\n\n    # Change the value to 1 if a given args can be represented by a given title\n    for i in range(args_length):\n        titles_with_args_i = titles_with_args[i]\n        title_args_table.loc[titles_with_args_i, i] = 1\n\n    # Add the type (e.g Game) of each title and group them accordingly\n    title_args_table = title_args_table.join(df_media.set_index(\"title\")[\"type\"])\\\n            .sort_values([\"type\", \"title\"]).reset_index()\n    title_args_table[\"title\"] = title_args_table[\"title\"].str.cat(title_args_table[\"type\"], sep=\" \")\n    title_args_table = title_args_table.set_index(\"title\").drop(columns=[\"type\"])\n    \n    return title_args_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def clear_figure(f):\n    f.clf()\n    plt.clf()\n    plt.close(f)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"args_encoded_map = generate_argstype_map(\"args_encoded\")\ntitle_args_table = generate_titleargs_table(args_encoded_map)\n\nf = plt.figure(figsize=(20,20))\nsns.heatmap(title_args_table, linewidths=.25).set_title(\"Encoded Args per Title\")\nf.savefig(\"encoded_args_vs_title.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There are indeed **args** which can represent a majority of the **title**s and take note that there are four types of **title** in the dataset: *Activity*, *Game*, *Clip*, and *Assessment*. You could notice that all *Clips* are only represented by one kind of **args** so we'll disregard it for now so we can see a clearer picture for the rest of the **args**."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"clear_figure(f)\nf = plt.figure(figsize=(20,20))\nsns.heatmap(title_args_table[~title_args_table.index.str.contains(\"Clip\")], linewidths=.25).set_title(\"Encoded Args per Title (excluding Clip Titles)\")\nf.savefig(\"encoded_args_vs_title2.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Re-encoding args based only on int values they contain"},{"metadata":{},"cell_type":"markdown","source":"You could notice that some **args**, like the ones on the right most side of the heatmap, are only present in a specific **title**. But remember each unique **args** contains mixed datatypes such as string or an int and we only based their uniqueness based on their *string representation* per se, that's probably why other **args** are so unique that they're only present on a specific title. \n\nFor now, we'll be only examining **args** which has a type *int*, so let's do that and remove non-numeric data in each **args** and re-encode our **args**"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"clear_figure(f)\nprint_log(\"Extracting and encoding different args type ...\")\nint_to_args_map = dict()\ntype_list = [\"int\", \"string\", \"object\", \"array\"]\nfor _type in type_list:\n    int_to_args_map[_type] = dict()\n\n# Extract numeric attributes of a given args\nfor i, args in enumerate(df_specs[\"args\"].unique()):\n    args_json = orjson.loads(args)\n    args_list = dict()\n    for _type in type_list:\n        args_list[_type] = list()\n    \n    for args in args_json:\n        if (args[\"name\"] != \"event_code\" and args[\"name\"] != \"game_time\" and args[\"name\"] != \"event_count\"):\n            # Include numeric types and the \"correct\" attribute and don't consider those that are already extracted (e.g game_time)\n            if (args[\"type\"] == \"int\" or args[\"type\"] == \"number\" or args[\"type\"] == \"boolean\"):  \n                args_list[\"int\"].append(args[\"name\"])\n            elif (\"array\" in args[\"type\"]):\n                args_list[\"array\"].append(args[\"name\"])\n            else:\n                args_list[args[\"type\"]].append(args[\"name\"])\n    \n    for _type in type_list:\n        int_to_args_map[_type][i] = args_list[_type]\n\n# Encode corresponding numeric_args\nfor _type in type_list:\n    args_name = \"_\".join([\"args\", _type])\n    args_encode_name = \"\".join([\"args\", _type]) + \"_encoded\"\n    \n    args_to_encode_map = {args: int_to_args_map[_type][i] for i, args in enumerate(df_specs[\"args\"].unique())}\n    df_specs[args_name] = df_specs[\"args\"].map(args_to_encode_map).astype(str)\n\n    argstype_to_int_map = {args: i for i, args in enumerate(df_specs[args_name].unique())}\n    df_specs[args_encode_name] = df_specs[args_name].map(argstype_to_int_map)\n\n    # Set the encoding in the dataset\n    eventid_to_args_map = df_specs.set_index(\"event_id\")[args_encode_name].to_dict()\n    df_train = df_train.assign(**{args_encode_name: df_train[\"event_id\"].map(eventid_to_args_map)})\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"print_log(\"Plotting heatmap for encoded args of type int ...\")\nargsint_encoded_map = generate_argstype_map(\"argsint_encoded\")\ntitle_args_table = generate_titleargs_table(argsint_encoded_map)\n\n# plot heatmap\nf = plt.figure(figsize=(20,10))\nsns.heatmap(title_args_table[~title_args_table.index.str.contains(\"Clip\")], linewidths=.25).set_title(\"Int type Encoded Args per Title\")\nf.savefig(\"int_encoded_args_vs_title.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that we have greatly reduced the uniqueness of our **args**, but also take note that we've loss valuable information from the *string* type attributes of **args** but we'll examine them later on. There's still some **args** that are very specific to a particular **title**, so let's inspect the args of each title available to see more clearly what they have to offer.\n\nWe will not be considering *Clips* since it has no any additional info in the event data and we won't also consider *Assessments* since those are the ones that are involved in our prediction. We will need to predict how many attempts a player will make before getting the correct answer regardless of the kind of Assessment (there are 4 types of *Assessments*), so in my opinion it won't make any sense to consider those."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"clear_figure(f)\nprint_log(\"Plotting heatmap title per int args ...\")\ntitle_allargs = set()\ntitle_args_map = dict()\ntitle_singleargs_map = dict()\n\nfor title in title_args_table.itertuples():\n    args_list = list()\n    args_set = set()\n    title = pd.Series(title)\n    \n    if \"Clip\" not in title.get(0) and \"Assessment\" not in title.get(0):\n        for i in title_args_table.columns:\n            # add +1 since 0 represents the title of the series\n            if title.get(i+1) == 1:\n                args = ast.literal_eval(df_specs[df_specs.argsint_encoded==i].head(1).args_int.values[0])\n                title_allargs.update(args)\n                args_set.update(args)\n                args_list.append((i, args))\n                \n        title_args_map[title.get(0)] = args_list\n        title_singleargs_map[title.get(0)] = args_set    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll then create a heatmap to plot the filtered **args** against each **title** to get a better grasp of the commonalities they have with other titles."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"title_singleargs_table = [[1 if arg in args else 0 for arg in title_allargs] for title, args in title_singleargs_map.items()]\ntitle_singleargs_table = pd.DataFrame(title_singleargs_table, index=title_singleargs_map.keys(), columns=title_allargs)\n\nf = plt.figure(figsize=(20,10))\nsns.heatmap(title_singleargs_table, linewidths=.25).set_title(\"Individual Int Args per Title\")\nf.savefig(\"individual_int_args_vs_title.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that almost of the title args are somewhat related. They almost all have *round*, *duration*, *level*, etc. but others have a very specific args such as *flower* and alike. We could clearly see that both *Activities* and *Games* have *duration*, *total_duration*, and *dwell_time* in common. While in *Games*, they have *misses*, *correct*, and *round* in common."},{"metadata":{},"cell_type":"markdown","source":"## Utilizing the info column in df_specs"},{"metadata":{},"cell_type":"markdown","source":"But to be sure that we can filter thoroughly and extract useful insights for a given **event_id**, let's utilize the **info** column we had in the event ids specifications. Let's take a rough look at few of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_specs.sample(10, random_state=1)[\"info\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, the **info** are somehow structured in the way what it describes the whole event. It first describes the event itself, then it tells what it contains, then how the event can be utilized including what questions can be answered by them or what the player is currently experiencing. Let's parse each of those info to see in general what type of info these **event_id**s can offer."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"# Looking for phrases that contains \"It contains information\"\nimport random\nrandom.sample(set([info[info.find(\"It contains\"):info.find(\"We\")] for info in df_specs[\"info\"].unique().tolist()]), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Looking for phrases that contains \"players feel are too difficult\"\nset(info for info in df_specs[\"info\"].unique().tolist() if \"difficult\" in info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Looking for phrases that contains \"We can answer questions like\"\nset([info[info.find(\"We can\"):] for info in df_specs[\"info\"].unique().tolist()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a potential information we can get for considering the **info** of each event ids that had such statements like **\"We can answer questions like how much time elapsed while the game was presenting instruction?\"** or **\"It helps identify points that players feel are too difficult\"**. "},{"metadata":{},"cell_type":"markdown","source":"## Classifying event_id by info column"},{"metadata":{},"cell_type":"markdown","source":"We'll take a look at each info and classify it into four based on the insights we've gathered in the info column; namely, **Diagnosis, AnswerQuestions (we may just refer to it as either Answer or Question), Difficult/Difficulty**, and the rest will be **None**."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"clear_figure(f)\nprint_log(\"Classifying event ids based on info column ...\")\n\ndf_specs[\"info_str\"] = df_specs[\"info\"].map(lambda info: \"\".join(e for e in info if e.isalnum()))\ndf_specs[\"contains_diagnosis\"] = df_specs.info_str.str.contains(\"diagnose\")\ndf_specs[\"contains_answer\"] =  df_specs.info_str.str.contains(\"answerquestions\") \ndf_specs[\"is_difficult\"] = df_specs.info_str.str.contains(\"difficult\") \ndf_specs[\"info_type\"] = \"none\"\ndf_specs.loc[df_specs.contains_diagnosis, \"info_type\"] = \"contains_diagnosis\"\ndf_specs.loc[df_specs.contains_answer, \"info_type\"] = \"contains_answer\"\ndf_specs.loc[df_specs.is_difficult, \"info_type\"] = \"is_difficult\"\n\nf, axes = plt.subplots(1, 4, figsize=(20,5))\naxes[0].set_title(\"Event ID count per Info Type\")\nsns.countplot(y=\"info_type\", data=df_specs, ax=axes[0], order=[\"contains_answer\", \"contains_diagnosis\", \"is_difficult\", \"none\"])\naxes[1].set_title(\"Event ID count per Diagnosis Info Type\")\nsns.countplot(x=\"contains_answer\", data=df_specs, hue=\"contains_diagnosis\", ax=axes[1])\naxes[2].set_title(\"Event ID count per Difficult Info Type\")\nsns.countplot(x=\"contains_diagnosis\", data=df_specs, hue=\"is_difficult\", ax=axes[2])\naxes[3].set_title(\"Event ID count per Answer Info Type\")\nsns.countplot(x=\"is_difficult\", data=df_specs, hue=\"contains_answer\", ax=axes[3])\nf.savefig(\"event_id_count_per_info.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph clearly indicates that no **event_id** have really the same type and cannot be classified more than one. But mainly, we will be focusing on those **event_id**s who can be used to diagnose the player's gameplay based on their respective stated **info**. "},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"clear_figure(f)\ndef extract_singleargs_per_infotype(args_type, title_args_table):\n    reduced_all_args = dict()\n    title_singleargs_map = dict()\n\n    args_name = \"_\".join([\"args\", args_type])\n    args_encode_name = \"\".join([\"args\", args_type]) + \"_encoded\"\n    \n    info_types = df_specs.info_type.unique().tolist()\n    for info_t in info_types:\n        reduced_all_args[info_t] = set()\n\n    for title in title_args_table.itertuples():\n        args_list = dict()\n        args_set = dict()\n        for info_t in info_types:\n            args_list[info_t] = list()\n            args_set[info_t] = set()\n\n        title = pd.Series(title)\n        if \"Clip\" not in title.get(0) and \"Assessment\" not in title.get(0):\n            for i in title_args_table.columns:\n                # add +1 since 0 represents the title of the series\n                if title.get(i+1) == 1:\n                    args_row = df_specs[(df_specs[args_encode_name]==i)].groupby([\"info_type\"]).head(1)\n                    for args_by_type in args_row.itertuples():\n                        args = ast.literal_eval(getattr(args_by_type, args_name))\n                        info_type = args_by_type.info_type\n                        args_set[info_type].update(args)\n                        args_list[info_type].append((i, args))\n                        reduced_all_args[info_type].update(args)\n            title_singleargs_map[title.get(0)] = args_set \n    return title_singleargs_map, reduced_all_args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"import math\ndef plot_eventids_per_infotype(title_singleargs_map, reduced_all_args, title):\n    info_types = df_specs.info_type.unique().tolist()\n    row_plot_count = math.ceil(len(info_types))\n    f, axes = plt.subplots(row_plot_count, 1, figsize=(20,15*row_plot_count))\n    i = j = 0\n    \n    for info_t in info_types:\n        title_singleargs_table = [[1 if arg in args[info_t] else 0 for arg in reduced_all_args[info_t]] for title, args in title_singleargs_map.items()]\n        title_singleargs_table = pd.DataFrame(title_singleargs_table, index=title_singleargs_map.keys(), columns=reduced_all_args[info_t])\n        \n        \n#         i = (i+1) if j==2 else i\n#         j = 0 if j==2 else j\n        axes[i].title.set_text(\"Event IDs with \" + info_t + \" info type\")\n        if len(reduced_all_args[info_t]) == 0:\n            title_singleargs_table = pd.DataFrame(np.zeros((len(title_singleargs_map.keys()), 1)), index=title_singleargs_map.keys(), columns=[\"none\"])\n        sns.heatmap(title_singleargs_table, linewidths=.25, ax=axes[i])\n        i = i + 1\n        \n    f.savefig(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"print_log(\"Plotting and processing all args type ...\")\nplot_eventids_per_infotype(*extract_singleargs_per_infotype(\"int\", title_args_table), \"individual_args_vs_title_per_info_type.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the *Diagnosis* type heatmap, we can clearly utilize the *round* and *correct* for the *Games*, and *dwell_time* for both *Games* and *Activites* as they're not bound to any specific **title**s though two of them don't have it. While in the *Questions* type heatmap, there's also *round*, *duration*, and *total_duration* that we can use though we must take note that the context/usage of the *round* might be different from the *round* **args** mentioned in the first heatmap as they're totally represent a different set of **event_id**s.\n\nFor the *Difficulty* type heatmap, we only have again, a *round* **args**. But maybe, what we can do instead is for each presence of one of those **event_id**s, we'll take note that the player is already finding the game hard to continue.\n\nFor the last one, the *None* type heatmap, it seems we can somehow utilize the *round*, *misses*, and *duration* args. We'll take a glimpse of what kind of **info** they might be able to give us so let's have a closer look."},{"metadata":{},"cell_type":"markdown","source":"## Discovering a new Info Type from None: Missed"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look if those **event_id**s that contains misses as there int **args**"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs[df_specs.args_int.str.contains(\"misses\")].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs[df_specs.args_int.str.contains(\"misses\")][\"info\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, those concerned **event_id**s that contains the *misses* **args** can give us the time spent of a player in a level for speed and accuracy, and the number of levels the player has completed in a *Game*, that's great so we'll consider those and create a new info type that we will named as Missed later on."},{"metadata":{},"cell_type":"markdown","source":"## Clarifying context usage of Args per Info Type"},{"metadata":{},"cell_type":"markdown","source":"Since we've already identified what **args** and type of **event_id**s we'll be using, we'll proceed with clarifying if indeed the context usage of each **args** in a given type of **event_id** is the same (*if they mean what we expected them to mean*), so let's do that."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_definition(args_list, info_type, args_type):\n    args_list_info = dict()\n\n    for args in args_list:\n        args_with_diagnosis_args = df_specs[(df_specs.info_type==info_type) & (df_specs[args_type].str.contains(args))].groupby([\"args_encoded\"]).head(1)\n        args_list_info[args] = set()\n\n        for _args in args_with_diagnosis_args.itertuples():\n            for args_comp in orjson.loads(_args.args):\n                if args_comp[\"name\"] == args:\n                    args_list_info[args].add(args_comp[\"info\"])\n\n    return args_list_info                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Context usage for Diagnosis Info Type of event_id(s)\ndiagnosis_args = [\"round\", \"correct\", \"dwell_time\"]\nfind_definition(diagnosis_args, \"contains_diagnosis\", \"args_int\")              ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Context usage for Answer Info Type of event_id(s)\nanswer_args = [\"round\", \"duration\", \"total_duration\"]\nfind_definition(answer_args, \"contains_answer\", \"args_int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Context usage for Difficult Info Type of event_id(s)\ndifficult_args = [\"round\"]\nfind_definition(difficult_args, \"is_difficult\", \"args_int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Context usage for None Info Type of event_id(s)\nnone_args = [\"round\", \"misses\", \"duration\"]\nfind_definition(none_args, \"none\", \"args_int\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously, for the three types of info namely *Diagnosis*, *Answers* and *Difficult*, the context usage of each **args** were concise and clearly defined. As for the *correct* **args**, it is expected to have a lot of definitions since it really differs per title, but they still refer to the same \"correct\".\n\nWhile for the *None* type, only the *misses* **args** is clear and the remaining two **args** are ambiguous, so we may need to separate the concerns for those **event_id**s that contains it."},{"metadata":{},"cell_type":"markdown","source":"## Creating the Missed Info Type"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"# Created the new Missed Info Type\ndf_specs[\"is_missed\"] = df_specs.args_int.str.contains(\"misses\")\ndf_specs.loc[df_specs.args_int.str.contains(\"misses\"), \"info_type\"] = \"is_missed\"\nmissed_args = [\"round\", \"misses\", \"duration\"]\nfind_definition(missed_args, \"is_missed\", \"args_int\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've narrowed down the definition of *round* and *duration* for the *misses* args, you can notice that we have now a different definition of the aforementioned **args** in regards to the three types of info evaluated earlier (e.g *Diagnosis*). So to avoid confusion and for future references, we'll mark the corresponding **event_id**s on which type of *round* and *duration* they are representing."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"df_specs[\"current_round\"] = df_specs.args.str.contains(\"number of the current round\")\ndf_specs[\"completed_round\"] = df_specs.args.str.contains(\"number of the round that has just been completed\")\ndf_specs[\"feedback_media_playback_duration\"] = df_specs.args.str.contains(\"media playback in milliseconds\")\ndf_specs[\"total_fbmedia_playback_duration\"] = df_specs.args.str.contains(\"media playback in milliseconds (if it ran uninterrupted)\")\ndf_specs[\"level_duration\"] = df_specs.args.str.contains(\"duration of the level in milliseconds\")\ndf_specs[\"round_duration\"] = df_specs.args.str.contains(\"duration of the round in milliseconds\")\ndf_specs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the new heatmaps for the following **info_type** as we added a new ones named *is_missed*."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"plot_eventids_per_infotype(*extract_singleargs_per_infotype(\"int\", title_args_table), \"individual_args_vs_title_per_info_type2.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examining other types of Args"},{"metadata":{},"cell_type":"markdown","source":"Now that we're done analyzing those **args** with *int* type, we'll proceed with examining the other types (e.g *string*, *array*) and let's see if we can get more insights in them. What we'll do is like we did for type *int*, plot each *args type* (e.g *string*) and if we find any common **args** for each **title**, we'll find it's definition and decide whether to consider it or not in our analysis later on.\n\nFirst, we'll go with the *string* type."},{"metadata":{"trusted":true},"cell_type":"code","source":"argsstring_encoded_map = generate_argstype_map(\"argsstring_encoded\")\ntitle_args_table = generate_titleargs_table(argsstring_encoded_map)\nplot_eventids_per_infotype(*extract_singleargs_per_infotype(\"string\", title_args_table), \"individual_stringargs_vs_title_per_info_type.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"answer_args = [\"identifier\", \"media_type\", \"description\"]\nfind_definition(answer_args, \"contains_answer\", \"args_string\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"find_definition([\"object\"], \"contains_diagnosis\", \"args_string\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"find_definition([\"object\"], \"none\", \"args_string\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look on how the **args** *identifier*, *media*, *description*, and *object*  relates to the *int* **args** we've chosen to consider."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_specs.loc[df_specs.contains_answer & df_specs.args_string.str.contains(\"media_type\"), df_specs.columns.str.contains(\"args\")].head(1).args.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_specs.loc[df_specs.contains_diagnosis & df_specs.args_string.str.contains(\"object\"), df_specs.columns.str.contains(\"args\")].head(1).args.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that the **args** *identifier*, *media*, *description* is related to the *duration* in that it describes what kind of media is presented in the duration. While the *object* describes the kind of object that is being dwelled on by the player and *dwell_time* defines its duration.\n\nLet's proceed with the *object* type of args"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"argsobject_encoded_map = generate_argstype_map(\"argsobject_encoded\")\ntitle_args_table = generate_titleargs_table(argsobject_encoded_map)\nplot_eventids_per_infotype(*extract_singleargs_per_infotype(\"object\", title_args_table), \"individual_objectargs_vs_title_per_info_type.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"find_definition([\"coordinates\"], \"contains_diagnosis\", \"args_object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"find_definition([\"coordinates\"], \"none\", \"args_object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"find_definition([\"coordinates\"], \"is_difficult\", \"args_object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_specs[df_specs.contains_diagnosis & df_specs.args_object.str.contains(\"coordinates\") & df_specs.args_int.str.contains(\"correct\")].head(1).args.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"scrolled":true},"cell_type":"code","source":"df_specs[df_specs.is_difficult & df_specs.args_object.str.contains(\"coordinates\") & df_specs.args_int.str.contains(\"round\")].head(1).args.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, we won't be considering the *coordinates* **args** as I think it won't really be able to relate this to *correct* **args** for *Diagnosis info type*. Obviously, knowing a coordinate of an answer whether it's correct or not doesn't make sense. For the *Difficult* info type, we might consider the *coordinates* and observe the behavior of a mouse click that may indicate that a player is having difficulties.\n\nBut with that in mind, we'll extract all *coordinates* regardless of the *info type* (we'll include the *coordinates* for *Diagnosis* now) to be able to discern mouse clicks that players are having difficulty and players that are not.\n\nLet's proceed with the *array* args type."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"argsarray_encoded_map = generate_argstype_map(\"argsarray_encoded\")\ntitle_args_table = generate_titleargs_table(argsarray_encoded_map)\nplot_eventids_per_infotype(*extract_singleargs_per_infotype(\"array\", title_args_table), \"individual_arrayargs_vs_title_per_info_type.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we can't find any useful **args** for the *array*. As you can see there are also no consistent **args** in the **title**s. So I think we're good and we could start compiling all the **args** and **event_id**s that we'll be extracting in the dataset."},{"metadata":{},"cell_type":"markdown","source":"## Filtering the Args to be extracted in the dataset"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"print_log(\"Filtering dataset based on selected event ids ...\")\ndiagnosis_args_int = [\"round\", \"correct\", \"dwell_time\"]\nanswer_args_int = [\"round\", \"duration\", \"total_duration\"]\nmissed_args_int = [\"round\", \"misses\", \"duration\"]\ndifficult_args_int = [\"round\"]\n\nanswer_args_string = [\"identifier\", \"media_type\", \"description\"]\n\ndiagnosis_filter = df_specs.contains_diagnosis &\\\n         (df_specs.args_string.str.contains(\"object\") |\\\n          df_specs.args_int.str.contains(diagnosis_args_int[1]) | df_specs.args_int.str.contains(diagnosis_args_int[2]))\n\nanswer_filter = df_specs.contains_answer &\\\n            (df_specs.args_int.str.contains(answer_args_int[1]) |\\\n            df_specs.args_int.str.contains(answer_args_int[2]) | df_specs.args_string.str.contains(answer_args_string[0]) |\\\n            df_specs.args_string.str.contains(answer_args_string[1]) | df_specs.args_string.str.contains(answer_args_string[2]))\n\ndifficult_filter = df_specs.is_difficult & (df_specs.args_object.str.contains(\"coordinates\"))\n\nmissed_filter = df_specs.is_missed &\\\n         (df_specs.args_int.str.contains(missed_args_int[1]) |\\\n          df_specs.args_int.str.contains(missed_args_int[2]))\n\ndf_specs[\"filtered_id\"] = diagnosis_filter | answer_filter | missed_filter\nevent_id_filter = df_specs[df_specs.filtered_id].event_id.values\n\ndf_specs.to_parquet(\"df_specs.parquet\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We didn't consider the presence of *round* **args** when we build the filter since we expect it to be present for the remaining selected **args**. Also it is too common to all **event_id**s which would expand our **event_id** selection needlessly. We won't also include the args_object such as the *coordinates* and will be reserved for future work and improvement. \n\nFinally we'll filter out those event ids that are of **type** *Clip* and *Assessment* and use that final list to filter out the whole dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"dask.compute(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":false},"cell_type":"code","source":"print_log(\"Removing installation ids without attempts ...\")\nids = dask.compute(df_train.query(attempts_query).installation_id.unique())[0]  # ids who took assessments\ndf_train = df_train[df_train.installation_id.isin(ids.tolist())]\ndf_train = df_train.assign(**{\"index\":df_train.installation_id.str.cat([\n                    df_train.game_session, \n                    df_train.event_count.astype(\"str\"), \n                    df_train.game_time.astype(\"str\"), \n                    df_train.timestamp.dt.strftime(\"%y%m%d%H%M%S\")])})\\\n                .set_index(\"index\")\n\ndf_event_id_filter = df_train[df_train.event_id.isin(event_id_filter) & (df_train[\"type\"]!=\"Clip\") & (df_train[\"type\"]!=\"Assessment\")]\ndf_event_id_filter.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting the Args from the filtered dataset"},{"metadata":{},"cell_type":"markdown","source":"To fully utilize this json extraction process, we would extract all *int* and *string* types of **args** that we could from the **event_id**s we've filtered since it just takes a matter of getting the value of the attribute from the already parsed json."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"all_args_int = set()\nall_args_string = set()\n# all_args_object = dict()\n\ndf_event_id_filter_list = dask.compute(df_event_id_filter.event_id.unique())[0].tolist()\nselected_event_ids = df_specs[df_specs.event_id.isin(df_event_id_filter_list)]\nfor event_id in selected_event_ids.itertuples():\n    all_args_int.update(ast.literal_eval(event_id.args_int))\n    all_args_string.update(ast.literal_eval(event_id.args_string))\n\n#     all_args.update(ast.literal_eval(event_id.args_object))\n#     event_args = orjson.loads(event_id.args)\n#     for arg_obj in ast.literal_eval(event_id.args_object):\n#         for event_arg in event_args:\n#             if event_arg[\"name\"] == arg_obj:\n#                 event_arg_items = event_arg[\"info\"][event_arg[\"info\"].find(\"{\") : event_arg[\"info\"].find(\"}\")+1]\n#                 event_arg_items = re.findall(r\"(?!\\\")\\w*(?=\\\")\", event_arg_items)\n#                 if len(event_arg_items) > 0:\n#                     all_args_object[event_arg[\"name\"]] = event_arg_items\n# import pickle\n# with open(\"all_args.pkl\", \"wb\") as write_args:\n#     pickle.dump(all_args, write_args)\n# all_args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"len(all_args_int) + len(all_args_string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 36 **args** to extract, we'll then save this as a parquet file so we can use its output to proceed with our data analysis without rerunning the whole notebook."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"filter_columns = [col for col in df_train.columns if \"encoded\" in col]\ndf_train_filter = df_train[df_train.event_id.isin(df_event_id_filter_list)]\ndf_train_filter = df_train_filter.assign(**{args: -1 for args in all_args_int})\\\n    .assign(**{args: \"\" for args in all_args_string})\\\n    .drop(columns=[\"event_id\", \"installation_id\", \"event_code\", \"title\", \"type\", \"world\", \"game_session\", \"event_count\", \"game_time\"])\\\n    .drop(columns=filter_columns)\ndf_train_filter.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_event(df):\n    index_list = df.index.tolist()\n    event_object_list = df.event_data.map(lambda x: orjson.loads(x)).tolist()\n    event_object_map = dict(zip(index_list, event_object_list))\n    for args in all_args_int:\n        df[args] = df.index.map(lambda x: event_object_map[x].get(args, -1))\n        df[args] = df[args].astype(\"int\") \n    for args in all_args_string:\n        df[args] = df.index.map(lambda x: event_object_map[x].get(args, \"\"))\n        df[args] = df[args].astype(\"category\") \n    return df\nprint(\"Extracting json args in the dataset ...\")\nclient = Client()\ndf_train_filter = df_train_filter.map_partitions(extract_event, meta=df_train_filter).drop(columns=[\"event_data\", \"timestamp\"])\ndf_train_join = df_train.join(df_train_filter)\ndd_train_future = client.persist(df_train_join)\nprogress(dd_train_future, notebook=False)   \ndd_train_future.to_parquet(\"df_train.parquet\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}