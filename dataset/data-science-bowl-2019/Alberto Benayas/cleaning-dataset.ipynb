{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2019 Data Science Bowl"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ast import literal_eval\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport sklearn as sk\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom functools import partial\nimport category_encoders as ce\nimport scipy as sp\nfrom scipy.stats import ks_2samp, ttest_ind, kstest\nimport os\nimport json as json\nfrom hyperopt import hp, Trials, STATUS_OK, fmin, tpe, anneal\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"whitegrid\")\n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_columns', None)\npd.set_option('display.min_rows', 100)\npd.set_option('display.max_rows', 200)\n\nkaggle = True\ngenerate_or_load = 'generate'\ndirname = '/kaggle/input/data-science-bowl-2019/' if kaggle == True else ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def read_dics_event(df, fields):\n    event_df = read_dics(df)   \n    keep = [x for x in fields if x in event_df.columns]\n    event_df = event_df[keep]\n    return pd.concat([df.reset_index(drop=True), event_df.reset_index(drop=True)], axis=1)\n\ndef read_dics(df):\n    df['event_data'] = df['event_data'].str.replace('false', 'False')\n    df['event_data'] = df['event_data'].str.replace('true', 'True')\n    return pd.DataFrame( [ eval(x) for x in df['event_data'] ] )\n\ndef get_accuracy_group(completed, errors=0):\n    if completed == False:\n        return 0\n    if errors == 0:\n        return 3\n    if errors == 1:\n        return 2\n    return 1\n\ndef unfold(df):\n    return pd.concat([df.drop(['event_data','event_count','event_code','game_time'], axis=1).reset_index(drop=True), read_dics(df).reset_index(drop=True)], axis=1)\n\ndef get_categorical_index(df, cat_cols):\n    cat_features_index = np.where(df.columns.isin(cat_cols))[0].tolist()\n    return cat_features_index\n\ndef pca_num(pca, required=99.0):\n    var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n    n = 0\n    for i in range(0,len(var)):\n        if var[i] >= 99:\n            n = i\n            break\n    return n+1\n\ndef remove_correlation(df, threshold = 0.99, verbose=False):\n    columns = df.columns.copy()\n    counter = 0\n    to_remove = []\n    for a in tqdm(range(0,len(columns))):\n        for b in range(a,len(columns)):\n            if a == b:\n                if df[columns[b]].std() == 0:\n                    counter += 1\n                    to_remove.append(columns[b])\n                    if verbose==True:\n                        print('{}: {} vs {} : Correlation= {}'.format(counter, columns[a], columns[b], 'Constant'))\n                continue\n            if columns[a] not in to_remove and columns[b] not in to_remove:\n                c = np.corrcoef(df[columns[a]], df[columns[b]])[0][1]\n                if c > threshold:\n                    counter += 1\n                    to_remove.append(columns[b])\n                    if verbose==True:\n                        print('{}: {} vs {} : Correlation= {}'.format(counter, columns[a], columns[b], c))\n\n    df = df.drop(columns=to_remove)\n    print(\"New shape after remove correlations {}\".format(df.shape))\n    return df\n\ndef expand_features(X, categorical):\n    if not isinstance(X, pd.DataFrame):\n        df = pd.DataFrame(data=X, index=[ i for i in range(0,X.shape[0])], columns=[ 'C'+str(i) for i in range(0,X.shape[1])] )\n    else:\n        df = X\n    cols = df.columns.copy()\n    for a in tqdm(cols):\n        for b in cols:\n            if a == b:\n                continue\n            if ((a in categorical) or (b in categorical)):\n                continue\n            df[a+'_'+b+'_1'] = (df[a] - df[b]) / df[b]\n            df[a+'_'+b+'_2'] = (df[a] + df[b]) / df[b]\n\n    print(\"New shape after feature expansion {}\".format(df.shape))\n    return df\n\ndef remove_different_features(df, test_start, categorical=[], threshold=0.5, verbose=False):\n    columns = df.columns\n    df_train = df[:test_start]\n    df_test = df[test_start:]\n    counter = 0\n    to_remove = []\n    for c in columns:\n        if c in categorical:\n            continue\n        if np.issubdtype(df[c].dtype, np.number):\n            o = overlapping(df_train[c], df_test[c], n_bins=1000)\n            if o < threshold: # the samples are not from the same distribution\n                if verbose == True:\n                    print(\"Feature {} overlaps in : {}\".format(c,o))\n                to_remove.append(c)\n    \n    df = df.drop(columns=to_remove)\n    print(\"New shape after removing different distribution features {}\".format(df.shape))\n    return df\n\ndef overlapping(xtr, xts, n_bins = 100):\n    xtr = xtr.sort_values()\n    xts = xts.sort_values()\n    step = (max(xtr.max(), xts.max()) - min(xtr.min(), xts.min())) / n_bins\n    if step == 0.0:\n        return 1.0\n    bins_tr = [0 for i in range(0,n_bins)]\n    bins_ts = [0 for i in range(0,n_bins)]\n    for x in xtr:\n        index = min(int(np.floor(x/step)),n_bins-1)\n        bins_tr[index] = bins_tr[index] + 1\n    for x in xts:\n        index = min(int(np.floor(x/step)),n_bins-1)\n        bins_ts[index] = bins_ts[index] + 1\n    bins_tr = np.array(bins_tr) / len(xtr)\n    bins_ts = np.array(bins_ts) / len(xts)\n    overlapping = 0\n    for i in range(0,len(bins_tr)):\n        overlapping = overlapping + min(bins_tr[i], bins_ts[i])\n    return overlapping\n\ndef divide_data_for_models(train, test, multikid_ids):\n    train_multikid = train[train['installation_id'].isin(multikid_ids)]\n    train = train[~train['installation_id'].isin(multikid_ids)]\n    test_multikid = test[test['installation_id'].isin(multikid_ids)]\n    test = test[~test['installation_id'].isin(multikid_ids)]\n    \n    print(\"Shape of train set {}\".format(train.shape))\n    print(\"Shape of test set {}\".format(test.shape))\n    \n    print(\"Shape of train multikid set {}\".format(train_multikid.shape))\n    print(\"Shape of test multikid set {}\".format(test_multikid.shape))\n    \n    return train, test, train_multikid, test_multikid\n\ndef divide_data_for_models2(df):\n    #df = df_all.drop('set', axis=1)\n    train_multikid = df[df['cluster']==0]\n    train = df[df['cluster']==1]\n    test_multikid = df[df['cluster']==2]\n    test = df[df['cluster']==3]\n    \n    print(\"Shape of train set {}\".format(train.shape))\n    print(\"Shape of test set {}\".format(test.shape))\n    \n    print(\"Shape of train multikid set {}\".format(train_multikid.shape))\n    print(\"Shape of test multikid set {}\".format(test_multikid.shape))\n    \n    return train, test, train_multikid, test_multikid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load\nraw_train = pd.read_csv(os.path.join(dirname, 'train.csv'), dtype={\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32})\nraw_test = pd.read_csv(os.path.join(dirname, 'test.csv'), dtype={\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32})\nspecs = pd.read_csv(os.path.join(dirname, 'specs.csv'))\nspecs_list = specs['event_id']\n\n# Remove installations without assessments\nraw_train = raw_train[raw_train['installation_id'].isin( raw_train[raw_train['type']=='Assessment']['installation_id'])]\nraw_train['timestamp'] = pd.to_datetime(raw_train['timestamp'])\nraw_test['timestamp'] = pd.to_datetime(raw_test['timestamp'])\n\n# Convert certain variables to categoricals\nraw_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"raw_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Raw train data contains {0} rows'.format(len(raw_train)))\nprint('Raw test data contains {0} rows'.format(len(raw_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore Data"},{"metadata":{},"cell_type":"markdown","source":"Not in this notebook"},{"metadata":{},"cell_type":"markdown","source":"### Extract Features from Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to prepare data\ndef prepare_data(df, specs, is_test=False):\n    fields_event_data = ['exit_type', 'level', 'round', 'correct']\n    assessments = df[df['type']=='Assessment']['title'].unique()\n    types = df['type'].unique() # Game, assessment, clip , activity\n    worlds = df['world'].unique()\n    titles = df['title'].unique()\n    extracted_data = []\n    cumulative_all = {}\n    df = df.sort_values(by=['installation_id','game_session','timestamp'])\n\n    # For each game_session\n    for i, g_installation_id in tqdm(df.groupby('installation_id')):\n        \n        g_installation_id = read_dics_event(g_installation_id, fields_event_data)\n        if 'level' in g_installation_id:\n            g_installation_id['level'] = g_installation_id['level'].fillna(0).astype(int)\n        \n        if 'round' in g_installation_id:\n            g_installation_id['round'] = g_installation_id['round'].fillna(0).astype(int)\n        \n         # Cumulative dict initialization\n        cumulative = {'total_cum_sessions':int(0),\n                      'total_cum_events_count':int(0), \n                      'total_cum_events_id':dict(),                     \n                      'total_cum_events_code':dict(), \n                      'durations':[]}\n        for x in assessments:\n            cumulative['incorrect_'+str(x)] = 0\n            cumulative['correct_'+str(x)] = 0\n        for x in types:\n            cumulative[x] = 0\n            cumulative['durations_'+x] = []\n        for x in worlds:\n            cumulative[x] = 0\n            cumulative['durations_'+x] = []\n            cumulative[x+'_max_level'] = 0\n            cumulative[x+'_max_round'] = 0\n        for x in titles:\n            cumulative[x] = 0\n            cumulative['durations_'+x] = []\n            \n        cumulative['exit_type'] = 0    \n        cumulative['max_level'] = 0 \n        cumulative['max_round'] = 0 \n        \n\n            \n        # Sort by timestamp so for testing the last row is the one to be predicted\n        g_installation_id = g_installation_id.sort_values(by='timestamp')\n        \n        # Separe the last row if this is to build a test dataset\n        if is_test == True:\n            to_predict = g_installation_id[-1:]\n            g_installation_id = g_installation_id[:-1]     \n        \n        # Process each game_session\n        for s, df_unfold in g_installation_id.groupby('game_session'):\n            # Unfold game_data\n            #df_unfold = unfold(df_unfold)\n\n            features = process_session(i, s, df_unfold.sort_values(by='timestamp'), cumulative, types, worlds, titles, False)\n            if features:\n                extracted_data.append(features)\n                \n        if is_test == True:\n            extracted_data.append(process_session(i, to_predict.iloc[0]['game_session'], to_predict, cumulative, types, worlds, titles, is_test))\n            \n    # Create dataframe from the extracted data\n    df_all = pd.DataFrame(extracted_data).fillna(0)\n        \n    # Complete the missing columns from specs\n    '''for c in specs:\n        if not c in df_all.columns:\n            df_all[c] = 0\n        df_all[c] = df_all[c].astype(np.int16)'''\n        \n    # Sort columns\n    df_all = df_all.reindex(sorted(df_all.columns), axis=1)\n        \n    df_all['accuracy_group'] = df_all['accuracy_group'].astype(np.int16)\n    df_all.columns = [ x.replace('!','').replace(' ','_').replace(',','') for x in df_all.columns]\n    return df_all\n            \ndef process_session(installation_id, game_session, df, cumulative, types, worlds, titles, is_test=False):\n    # If not an assessment, nothing to be returned, just cumulate\n    if df.iloc[0]['type'] != 'Assessment':\n        cumulate(df,cumulative)\n        return\n    \n    # Features is the dict to be returned\n    features = {}\n    # Basic ID features\n    features['installation_id'] = installation_id\n    features['game_session'] = game_session\n    \n    # Session data (Categorical)\n    features['title'] = df.iloc[0]['title']\n    features['world'] = df.iloc[0]['world']\n    features['hour'] = df.iloc[-1]['timestamp'].hour\n    features['weekday'] = df.iloc[-1]['timestamp'].weekday()\n    features['month'] = df.iloc[-1]['timestamp'].month\n    \n    # Game Session cumulatives\n    features['total_cum_sessions'] = cumulative['total_cum_sessions']\n    features['log_total_cum_sessions'] = np.log(1+cumulative['total_cum_sessions'])\n    features['total_cum_sessions_world'] = cumulative[features['world']]\n    features['total_cum_sessions_title'] = cumulative[features['title']]\n    \n    # Events cumulatives\n    #features.update(cumulative['total_cum_events_id'])\n    features.update(cumulative['total_cum_events_code'])\n    features['total_cum_events_count'] = cumulative['total_cum_events_count']\n    features['log_total_cum_events_counts'] = np.log(1+cumulative['total_cum_events_count'])\n    features['distinct_event_codes'] = len(np.unique(list(cumulative['total_cum_events_code'].keys())))\n      \n    # Game time duration cumulatives\n    features['log_total_cum_time'] = np.log(1+np.array(cumulative['durations']).sum())\n    features['total_cum_time'] = np.array(cumulative['durations']).sum()\n    features['total_avg_time'] = np.array(cumulative['durations']).mean()\n    features['total_std_time'] = np.array(cumulative['durations']).std()\n    features['total_cum_time_world'] = np.array(cumulative['durations_'+features['world']]).sum()\n    features['total_avg_time_world'] = np.array(cumulative['durations_'+features['world']]).mean()\n    features['total_std_time_world'] = np.array(cumulative['durations_'+features['world']]).std()\n    for x in types:\n        features[x] = cumulative[x]\n        features['log_total_cum_time_'+x] = np.log(1+np.array(cumulative['durations_'+x]).sum())\n        features['total_cum_time_'+x] = np.array(cumulative['durations_'+x]).sum()\n        features['total_avg_time_'+x] = np.array(cumulative['durations_'+x]).mean()\n        features['total_std_time_'+x] = np.array(cumulative['durations_'+x]).std()\n    for x in worlds:\n        features[x] = cumulative[x]\n        features['log_total_cum_time_'+x] = np.log(1+np.array(cumulative['durations_'+x]).sum())\n        features['total_cum_time_'+x] = np.array(cumulative['durations_'+x]).sum()\n        features['total_avg_time_'+x] = np.array(cumulative['durations_'+x]).mean()\n        features['total_std_time_'+x] = np.array(cumulative['durations_'+x]).std()\n        features[x+'_max_level'] = cumulative[x+'_max_level']\n        features[x+'_max_round'] = cumulative[x+'_max_round']\n    for x in titles:\n        features[x] = cumulative[x]\n        features['log_total_cum_time_'+x] = np.log(1+np.array(cumulative['durations_'+x]).sum())\n        features['total_cum_time_'+x] = np.array(cumulative['durations_'+x]).sum()\n        features['total_avg_time_'+x] = np.array(cumulative['durations_'+x]).mean()\n        features['total_std_time_'+x] = np.array(cumulative['durations_'+x]).std()\n        \n    features['exit_type'] = cumulative['exit_type']\n    features['max_level'] = cumulative['max_level']\n    features['max_round'] = cumulative['max_round']\n\n        \n    # Cumulate\n    cumulate(df,cumulative)\n    \n    \n    # If this is a sample to be predicted\n    if (is_test == True) & (len(df)==1) & (df.iloc[0]['event_code']==2000):\n        features['prev_errors'] = cumulate_errors(cumulative, df.iloc[0]['title'], 0)\n        features['prev_correct'] = cumulate_correct(cumulative, df.iloc[0]['title'], 0)\n        features['accuracy_group'] = -1 # this is a flag\n        return features\n        \n    # Calculate label\n    df_assessment = df[ ((df['event_code']==4100)&(~df['title'].str.startswith('Bird'))) | ((df['event_code']==4110)&(df['title'].str.startswith('Bird'))) ]\n    if len(df_assessment) == 0:\n        return\n    for t, g_title in df_assessment.groupby('title'):\n        if True in g_title['correct'].unique():\n            correct_attempt = g_title[g_title['correct'] == True].iloc[0]\n            errors = len(g_title.loc[:correct_attempt.name]) -1\n            features['prev_errors'] = cumulate_errors(cumulative, t, errors)\n            features['prev_correct'] = cumulate_correct(cumulative, t, 1)\n            features['accuracy_group'] = get_accuracy_group(True, errors)\n        else:\n            features['prev_errors'] = cumulate_errors(cumulative, t, len(g_title)) \n            features['prev_correct'] = cumulate_correct(cumulative, t, 0)\n            features['accuracy_group'] = get_accuracy_group(False)  \n    return features\n\n# Cumulate data for the same installation id        \ndef cumulate(df,cumulative):\n    \n    # Type of game session\n    type_session = df.iloc[-1]['type']\n    cumulative[type_session] = cumulative[type_session] + 1\n    \n    # World of game session\n    world_session = df.iloc[-1]['world']\n    cumulative[world_session] = cumulative[world_session] + 1\n    \n    # Title of game session\n    title_session = df.iloc[-1]['title']\n    cumulative[title_session] = cumulative[title_session] + 1\n    \n    # Game time duration\n    if len(df) > 1:\n        duration = df.iloc[-1]['game_time']/1000\n        cumulative['durations'] = cumulative['durations'] + [duration]\n        cumulative['durations_'+type_session] = cumulative['durations_'+type_session] + [duration]\n        cumulative['durations_'+world_session] = cumulative['durations_'+world_session] + [duration]\n        cumulative['durations_'+title_session] = cumulative['durations_'+title_session] + [duration]\n    else:\n        cumulative['durations'] = cumulative['durations'] + [0.0]\n\n    # Game sessions\n    cumulative['total_cum_sessions'] = cumulative['total_cum_sessions'] + 1\n    \n    # Event count\n    cumulative['total_cum_events_count'] = cumulative['total_cum_events_count'] + df.iloc[-1]['event_count']\n    \n    # Events ID\n    '''events = cumulative['total_cum_events_id']\n    for x in df['event_id']:\n        if x in events:\n            events[x] = events[x] + 1\n        else:\n            events[x] = 1\n    cumulative['total_cum_events_id'] = events'''\n    \n    # Events code\n    events_code = cumulative['total_cum_events_code']\n    for e in df['event_code']:\n        x = str(e)\n        if x in events_code:\n            events_code[x] = events_code[x] + 1\n        else:\n            events_code[x] = 1\n    cumulative['total_cum_events_code'] = events_code\n    \n    # Exit type correct\n    if 'exit_type' in df.columns:\n        cumulative['exit_type'] = cumulative['exit_type'] + df['exit_type'].count()\n     \n    # Level\n    if 'level' in df.columns:\n        cumulative['max_level'] = max(cumulative['max_level'], max(df['level']))\n        cumulative[world_session+'_max_level'] =  max(cumulative[world_session+'_max_level'], df['level'].max())\n    \n    # Round\n    if 'round' in df.columns:\n        cumulative['max_round'] = max(cumulative['max_round'], max(df['round']))\n        cumulative[world_session+'_max_round'] =  max(cumulative[world_session+'_max_round'], df['round'].max())\n\n\n# Cumulate the number of errors for each assessment type\ndef cumulate_errors(cumulative,title,errors):\n    t = 'incorrect_'+title\n    previous = cumulative[t]\n    cumulative[t] = previous + errors\n    return previous\n\n# Cumulate the number of errors for each assessment type\ndef cumulate_correct(cumulative,title,correct):\n    t = 'correct_'+title\n    previous = cumulative[t]\n    cumulative[t] = previous + correct\n    return previous\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train Data\nif (generate_or_load == 'generate') | (kaggle == True):\n    train = prepare_data(raw_train, specs_list)\n    if kaggle == False:\n        train.to_csv('new_train.csv',index=False)\n        print('Saved to {0}'.format('new_train.csv'))        \nelse:\n    train = pd.read_csv('new_train.csv')\n    print('Data loaded from to {0}'.format('new_train.csv'))\nprint(\"Train's shape is {0}\".format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Consistency check\ntemp_df = train[['installation_id','game_session','accuracy_group']]\nmerged = temp_df.merge(pd.read_csv(os.path.join(dirname, 'train_labels.csv')), on=['installation_id','game_session'], how='left')\nmerged['OK'] = merged['accuracy_group_x'] == merged['accuracy_group_y']\nprint('Generated values not in train_label {0}'.format(str(len(merged[pd.isnull(merged['accuracy_group_y'])]))))\nlimpios = merged.dropna(subset=['accuracy_group_y'], axis=0)\nprint('Generated values with different accuracy_group {0}'.format(str(len(limpios[limpios['OK']==False]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test Data\nif (generate_or_load == 'generate') | (kaggle == True):\n    test = prepare_data(raw_test, specs_list, is_test=True)\n    if kaggle == False:\n        test.to_csv('new_test.csv',index=False)\n        print('Saved to {0}'.format('new_test.csv'))       \nelse:\n    test = pd.read_csv('new_test.csv')\n    print('Data loaded from to {0}'.format('new_test.csv'))\nprint(\"Test's shape is {0}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assert (list(train.columns) == list(test.columns))\ncategorical = ['hour', 'month', 'weekday','world','title']\nuseless = ['installation_id', 'game_session']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# From the X_test dataset just 1000 are to be predicted\n# The rest will join the X_train dataset\ntrain = pd.concat([train,test[test['accuracy_group']!=-1]], ignore_index=True)\ntest = test[test['accuracy_group']==-1]\ntest = test.drop('accuracy_group',axis=1)\nprint(\"Train's shape is {0}\".format(train.shape))\nprint(\"Test's shape is {0}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Encode certain features\n\n# concatenate train and test data\ntemp_df = pd.concat([train, test])\n# encode\nencoder = ce.ordinal.OrdinalEncoder(cols = ['world','title'])\ntemp_df = encoder.fit_transform(temp_df)\n# dataset\ntrain = temp_df.iloc[:len(train),:]\ntest = temp_df.iloc[len(train):,:]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Divide the data to try to find installation id's with multiple kids\ncum_time_mean = train['total_cum_time'].mean()\ncum_time_std = train['total_cum_time'].std()\ncum_sessions_mean = train['total_cum_sessions'].mean()\ncum_sessions_std = train['total_cum_sessions'].std()\n\nmask_train = ((train['total_cum_time']>=cum_time_mean+1*cum_time_std)|(train['total_cum_sessions']>=cum_sessions_mean+1*cum_sessions_std))\nmask_test = ((test['total_cum_time']>=cum_time_mean+1*cum_time_std)|(test['total_cum_sessions']>=cum_sessions_mean+1*cum_sessions_std))\n\ndf_0 = train[mask_train]\ndf_0['cluster'] = 0\ndf_1 = train[~mask_train]\ndf_1['cluster'] = 1\ndf_2 = test[mask_test]\ndf_2['cluster'] = 2\ndf_3 = test[~mask_test]\ndf_3['cluster'] = 3\ndf_mask = pd.concat([df_0,df_1,df_2,df_3], ignore_index=True)\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", hue=\"cluster\", palette=sns.color_palette(\"husl\", 4), data=df_mask)\nprint(df_mask['cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Group the data by installation id, and keeping the last sample of each\ndf_installations = pd.concat([train, test]).groupby('installation_id').max()\ndf_installations = df_installations.reset_index()\nfig, ax = plt.subplots(figsize=(20,10))\ndf_installations['cluster'] = df_installations.apply(lambda x: 'outsiders' if ((x['total_cum_time']>=cum_time_mean+2*cum_time_std)|(x['total_cum_sessions']>=cum_sessions_mean+2*cum_sessions_std)) else 'normal', axis=1 )\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", hue=\"cluster\", data=df_installations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installation ID's that are outside the \"normality\" border are marked in orange"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_0 = train\ndf_0['set'] = 'train'\ndf_1 = test\ndf_1['set'] = 'test'\n\ndf_all = pd.concat([df_0, df_1])\ndf_all = df_all.merge(df_installations[['installation_id','cluster']], on='installation_id', how='left')\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", hue=\"cluster\",style=\"set\", data=df_all)\nprint(df_all['cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Full dataset (train and test), marking in orange all samples that belongs to an installation ID that falls out of the \"normality\" border"},{"metadata":{"trusted":false},"cell_type":"code","source":"# This set we have to keep\ndf_installation_test = df_all[(df_all['cluster']=='outsiders')&(df_all['installation_id'].isin(test['installation_id']))]\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", hue=\"set\",style=\"cluster\", data=df_installation_test)\nprint(df_all['cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the group of samples having any sample with its installation ID into the test set. As expected, the last sample from each installation ID is the one in the test set. We have to keep these samples"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_remove = df_all[(df_all['cluster']=='outsiders')&(~df_all['installation_id'].isin(test['installation_id']) )] #Installations ID not in test\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", data=df_remove)\nprint(df_remove['cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the group of samples that have any installation ID falling outside the normality border. We have to remove these samples"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_mask = df_mask[(~df_mask['installation_id'].isin(df_remove['installation_id']) )]\ndf_mask.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nax = sns.scatterplot(x=\"total_cum_time\", y=\"total_cum_sessions\", hue=\"cluster\", palette=sns.color_palette(\"husl\", 4), data=df_mask)\nprint(df_mask['cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the final picture. The dataset is divided into two sets: normal and multikid. Normal-training is the yellow one, Normal-test is the purple one. Multikid-training is the red one and multikid-test is the teal one"},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"def feature_pipeline(train, test, categorical):\n    temp_df = pd.concat([train.drop('accuracy_group',axis=1), test.drop('accuracy_group',axis=1)])\n    temp_df = feature_reduction(temp_df, categorical)\n    \n    # Scaling numerical features\n    '''scaler = MinMaxScaler()\n    mask = [c for c in temp_df.columns if c not in categorical]\n    scaler.fit(temp_df[mask])\n    temp_df[mask] = scaler.transform(temp_df[mask])\n    \n    # Features expansion       \n    temp_df = expand_features(temp_df, categorical)'''\n    \n    # Divide into train and test again\n    X_train = temp_df[:len(train)]\n    X_test = temp_df[len(train):]\n    print(\"Train's shape is {0}\".format(X_train.shape))\n    print(\"Test's shape is {0}\".format(X_test.shape))\n    return X_train, X_test\n\ndef feature_reduction(df, categorical):\n    df = remove_different_features(df, len(train), categorical = categorical, threshold=0.5)\n    df = remove_correlation(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom sklearn import preprocessing\n%matplotlib inline\n\n# Divide datasets\ntrain_normal, test_normal, train_multikid, test_multikid = divide_data_for_models2(df_mask)\n\nprint('Train and Test set (normal)')\ny_train = train_normal['accuracy_group']\nX_train, X_test = feature_pipeline(train_normal.drop(useless, axis=1), test_normal.drop(useless, axis=1), categorical)\n\nprint('Train and Test set (multikid)')\ny_train_multikid = train_multikid['accuracy_group']\nX_train_multikid, X_test_multikid = feature_pipeline(train_multikid.drop(useless, axis=1), test_multikid.drop(useless, axis=1), categorical)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models"},{"metadata":{"trusted":false},"cell_type":"code","source":"import catboost as cb\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"trusted":false},"cell_type":"code","source":"best_n_cb = 1747\nbest_cb = {'bagging_temperature': 0.7907291047410311,\n 'border_count': 35,\n 'depth': 6,\n 'l2_leaf_reg': 6.697930677527237,\n 'learning_rate': 0.01586403145995819,\n 'random_state': 1,\n 'random_strength': 2.76835390842624,\n 'loss_function': 'RMSE',\n 'one_hot_max_size': 31,\n 'eval_metric': 'RMSE',\n 'od_type': 'Iter',\n 'od_wait': 20}\n\nbest_n_cb_mk = 77\nbest_cb_mk = {'bagging_temperature': 0.5640162442730875,\n 'border_count': 60,\n 'depth': 4,\n 'l2_leaf_reg': 19.707986433630612,\n 'learning_rate': 0.1286788580745031,\n 'random_state': 2,\n 'random_strength': 1.542224177415203,\n 'loss_function': 'RMSE',\n 'one_hot_max_size': 31,\n 'eval_metric': 'RMSE',\n 'od_type': 'Iter',\n 'od_wait': 20}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":false},"cell_type":"code","source":"best_xgb = {'alpha': 18.659799567977437,\n 'bagging_temperature': 0.6414551219335474,\n 'colsample_bytree': 0.5098810850299129,\n 'gamma': 0.1,\n 'lambda': 12.181369439828163,\n 'learning_rate': 0.02025268651064681,\n 'max_depth': 6,\n 'min_child_weight': 8.0,\n 'seed': 0,\n 'subsample': 0.7993323980076127,\n 'loss_function': 'rmse',\n 'eval_metric': 'rmse'}\nbest_n_xgb = 614\n\nbest_xgb_mk = {'alpha': 11.433288052255918,\n 'bagging_temperature': 0.33942590071593925,\n 'colsample_bytree': 0.9979068963608676,\n 'gamma': 0.30000000000000004,\n 'lambda': 2.668300231808874,\n 'learning_rate': 0.028968044125941405,\n 'max_depth': 2,\n 'min_child_weight': 6.0,\n 'seed': 0,\n 'subsample': 0.6144708440137876,\n 'loss_function': 'rmse',\n 'eval_metric': 'rmse'}\nbest_n_xgb_mk = 211","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"trusted":false},"cell_type":"code","source":"best_n_lgb = 1338\nbest_lgb = {'bagging_temperature': 0.9611498217282982,\n 'colsample_bytree': 0.6771533510903894,\n 'gamma': 0.30000000000000004,\n 'lambda_l1': 9.189847609240765,\n 'lambda_l2': 2.728003044833198,\n 'learning_rate': 0.010244282681882995,\n 'max_depth': 4,\n 'min_child_weight': 2.0,\n 'num_leaves': 2,\n 'seed': 1,\n 'subsample': 0.3414568373784109,\n 'loss_function': 'rmse',\n 'eval_metric': 'rmse'}\n\nbest_n_lgb_mk = 1107\nbest_lgb_mk = {'bagging_temperature': 0.7960888097194333,\n 'colsample_bytree': 0.6813552383711141,\n 'gamma': 0.4,\n 'lambda_l1': 11.856321953925471,\n 'lambda_l2': 9.609965355663928,\n 'learning_rate': 0.010167529279903848,\n 'max_depth': 10,\n 'min_child_weight': 8.0,\n 'num_leaves': 3,\n 'seed': 3,\n 'subsample': 0.20000221570501625,\n 'loss_function': 'rmse',\n 'eval_metric': 'rmse'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"class OptRounder(object):\n    def __init__(self):\n        self.res_ = []\n        self.coef_ = []\n        \n    def get_res(self):\n        return self.res_\n    \n    # objective function\n    def func(self, coef, X, y):\n        kappa = cohen_kappa_score(self.bincut(coef, X), y, weights='quadratic')\n        return -kappa\n\n    def bincut(self, coef, X):\n        return pd.cut(X,\n                      [-np.inf] + list(np.sort(coef)) + [np.inf],\n                      labels = [0, 1, 2, 3])\n        \n    def fit(self, X, y):\n        pfunc = partial(self.func, X=X, y=y)\n        self.res_ = sp.optimize.minimize(fun = pfunc,           # objective func\n                                         x0 = [0.9, 1.8, 2.3],  # initial coef\n                                         method='nelder-mead')  # solver\n        self.coef_ = self.res_.x\n        \n    def predict(self, X, coef):\n        return self.bincut(coef, X)\n    \ndef voting(x):\n    votes = [0,0,0,0]\n    for i,v in x.iteritems():\n        votes[v] = votes[v] + 1\n    return np.asarray(votes).argmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nclass TrainModels():   \n    def __init__(self, cb_data=[], xgb_data=[], lgb_data=[]):\n        self.cb_data = cb_data\n        self.xgb_data = xgb_data\n        self.lgb_data = lgb_data\n        \n        self.training_data = {'CB': cb_data, 'XGB': xgb_data, 'LGB': lgb_data}\n        \n        self.cb = []\n        self.xgb = []\n        self.lgb = []\n        \n        self.models = {'CB': [], 'XGB': [], 'LGB': []}\n        \n        self.fselection = {}\n        \n    def _get_categorical_index(self, df, cat_cols):\n        cat_features_index = np.where(df.columns.isin(cat_cols))[0].tolist()\n        return cat_features_index\n    \n    def _get_mean_coeff(self):\n        coeffs = []\n        m = self.cb + self.xgb + self.lgb\n        for x in m:\n            res = x[1].get_res().x\n            coeffs.append({'C0':res[0], 'C1':res[1], 'C2':res[2], 'kappa':-(x[1].get_res().fun)})\n            \n        df = pd.DataFrame(coeffs)\n        coeffs =[]\n        for c in df.columns[:-1]:\n            coeffs.append(np.average(np.array(df[c]), weights=np.array(df['kappa'])))\n            \n        return coeffs\n    \n    def _build_predict(self, df, installation_id):\n        try:\n            df_new = installation_id.reset_index(drop=True)\n            for c in df.columns:\n                df_new[c] = df[c].astype(int)\n            return df_new\n        except:\n            return df\n    \n    def _train(self,library,model,X,y,categorical):\n        rounder = OptRounder()\n        if library == 'CB':\n            m=cb.train(dtrain=cb.Pool(data=X, label=y, cat_features=self._get_categorical_index(X, categorical)), params=model[0], num_boost_round=model[1], verbose_eval=False)\n            rounder.fit(m.predict(X), y)\n        if library == 'XGB':\n            m=xgb.train(params=model[0], dtrain=xgb.DMatrix(X, y), num_boost_round=model[1], verbose_eval =False)\n            rounder.fit(m.predict(xgb.DMatrix(X)), y)\n        if library == 'LGB':\n            m=lgb.train(params=model[0], train_set=lgb.Dataset(X, label=y, categorical_feature = self._get_categorical_index(X, categorical), free_raw_data = False), num_boost_round=model[1], verbose_eval =False)         \n            rounder.fit(m.predict(X), y)     \n        return (m, rounder)\n    \n    def fit(self, X, y, categorical=[], folds=1, feature_selection=-1):\n        self.rounders = {}\n        self._fit('CB', X, y, categorical, folds, feature_selection)\n        self._fit('XGB', X, y, categorical, folds, feature_selection)\n        self._fit('LGB', X, y, categorical, folds, feature_selection)\n    \n    def _fit(self, library, X, y, categorical=[], folds=1, feature_selection=-1):\n        print(\"Training \" + library + \" models\")\n        for model in self.training_data[library]:\n            if feature_selection > 0:\n                print(\"\\tFeature Selection for \" + library + \" models\")\n                fselection = select_features(library)\n                fselection.fit(self._train(library, model, X, y, categorical)[0], feature_selection)\n                self.fselection[library] = fselection\n            X_data = self.fselection[library].transform(X) if library in self.fselection else X\n            if folds > 1:\n                print(\"\\tTraining with \" + str(folds) + \" folds\")\n                kf = KFold(n_splits=folds, random_state=7)                       \n                for train_index, test_index in kf.split(X):\n                    self.models[library].append(self._train(library, model, X_data.iloc[train_index], y.iloc[train_index], categorical))\n            else:\n                self.models[library].append(self._train(library, model, X_data, y, categorical))\n    \n    def predict(self, X, mean=False, installation_id=None):\n        df = pd.DataFrame()\n        \n        for i in range(0,len(self.models['CB'])):\n            X_data = self.fselection['CB'].transform(X) if 'CB' in self.fselection else X\n            df['cb'+str(i)] = self.models['CB'][i].predict(X_data)\n            \n        for i in range(0,len(self.models['XGB'])):\n            X_data = self.fselection['XGB'].transform(X) if 'XGB' in self.fselection else X\n            df['xgb'+str(i)] = self.models['XGB'][i].predict(xgb.DMatrix(X_data))\n            \n        for i in range(0,len(self.models['LGB'])):\n            X_data = self.fselection['LGB'].transform(X) if 'LGB' in self.fselection else X\n            df['lgb'+str(i)] = self.models['LGB'][i].predict(X_data)\n            \n        if mean == True:\n            df['mean'] = df.mean(axis=1)\n            \n        return self._build_predict(df, installation_id) \n        \n    def predict_class(self, X, individual_rounder=True, mean_function=None, installation_id=None):\n        if individual_rounder == False:\n            coeffs = self._get_mean_coeff()\n        \n        df = pd.DataFrame()\n        for i in range(0,len(self.models['CB'])):\n            X_data = self.fselection['CB'].transform(X) if 'CB' in self.fselection else X\n            df['cb_class'+str(i)] = self.models['CB'][i][1].predict(self.models['CB'][i][0].predict(X_data), self.models['CB'][i][1].get_res().x if individual_rounder==True else coeffs).astype(int)\n            \n        for i in range(0,len(self.models['XGB'])):\n            X_data = self.fselection['XGB'].transform(X) if 'XGB' in self.fselection else X\n            df['xgb_class'+str(i)] = self.models['XGB'][i][1].predict(self.models['XGB'][i][0].predict(xgb.DMatrix(X_data)), self.models['XGB'][i][1].get_res().x if individual_rounder==True else coeffs).astype(int)\n            \n        for i in range(0,len(self.models['LGB'])):\n            X_data = self.fselection['LGB'].transform(X) if 'LGB' in self.fselection else X\n            df['lgb_class'+str(i)] = self.models['LGB'][i][1].predict(self.models['LGB'][i][0].predict(X_data), self.models['LGB'][i][1].get_res().x if individual_rounder==True else coeffs).astype(int)\n            \n        if not mean_function is None:\n            df['mean_class'] = df.apply(mean_function, axis = 1)\n            \n        return self._build_predict(df, installation_id)\n    \n\ndef select_features(library):\n    if library == 'CB':\n        return SelectFeaturesCB()\n    if library == 'XGB':\n        return SelectFeaturesXGB()\n    if library == 'LGB':\n        return SelectFeaturesLGB()\n    \nclass SelectFeatures():\n    def transform(self,df):\n        return df[self.features]\n    \nclass SelectFeaturesCB(SelectFeatures):        \n    def fit(self,model, threshold):\n        lista = model.get_feature_importance(prettified=True)\n        lista['Importances'] = lista['Importances']/100\n        lista['cum'] = lista['Importances'].cumsum()\n        self.features = list(lista[lista['cum']<threshold]['Feature Id'])\n        print('\\tNumber of selected features for CB: ' + str(len(self.features)))\n        \nclass SelectFeaturesXGB(SelectFeatures):\n    def fit(self,model, threshold):\n        lista = list(model.get_score(importance_type='gain').items())\n        suma = sum([x[1] for x in lista])\n        lista = [(x[0],x[1]/suma) for x in lista]\n        lista.sort(key = lambda x: x[1], reverse=True)\n        v = 0\n        i = 0\n        while v < threshold:\n            v = v + lista[i][1]\n            i = i+1\n        lista = [x[0] for x in lista]\n        self.features = lista[:i-1]\n        print('\\tNumber of selected features for XGB: ' + str(len(self.features)))\n\nclass SelectFeaturesLGB(SelectFeatures):\n    def fit(self,model, threshold):\n        names = model.feature_name()\n        imp = model.feature_importance(importance_type='gain')\n        lista = [(names[i], imp[i]/imp.sum()) for i in range(len(names))]\n        lista.sort(key = lambda x: x[1], reverse=True)\n        v = 0\n        i = 0\n        while v < threshold:\n            v = v + lista[i][1]\n            i = i+1\n        lista = [x[0] for x in lista]\n        self.features = lista[:i-1]\n        print('\\tNumber of selected features for LGB: ' + str(len(self.features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"m_normal = TrainModels(cb_data=[(best_cb, best_n_cb)], xgb_data=[(best_xgb, best_n_xgb)], lgb_data=[(best_lgb, best_n_lgb)])\nm_normal.fit(X_train, y_train, categorical, folds=5, feature_selection=0.9)\ndf_preds_train_normal = m_normal.predict_class(X_train, mean_function=voting, installation_id=train_normal[useless] )\ndf_preds_train_normal.plot(title= 'Prediction - Training - Normal', subplots=True, layout=(4, 5), figsize=(16, 16), kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"m_multikid = TrainModels(cb_data=[(best_cb_mk, best_n_cb_mk)], xgb_data=[(best_xgb_mk, best_n_xgb_mk)], lgb_data=[(best_lgb_mk, best_n_lgb_mk)])\nm_multikid.fit(X_train_multikid, y_train_multikid, categorical, folds=5, feature_selection=0.9)\ndf_preds_train_mk = m_multikid.predict_class(X_train_multikid, mean_function=voting, installation_id=train_multikid[useless] )\ndf_preds_train_mk.plot(title= 'Prediction - Training - Multikid', subplots=True, layout=(4, 5), figsize=(16, 16), kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_preds_train = pd.concat([train_normal[useless+['accuracy_group']], train_multikid[useless+['accuracy_group']]])\ndf_preds_train['y'] = df_preds_train['accuracy_group']\ndf_preds_train = df_preds_train.drop('accuracy_group', axis=1)\ndf_preds_train = df_preds_train.merge( pd.concat([df_preds_train_normal, df_preds_train_mk]), on=useless, how='left' )\n\nbest_kappa = 0\nfor a in df_preds_train_normal.columns[3:]:\n    for b in df_preds_train_mk.columns[3:]:\n        df = pd.concat([df_preds_train_normal[useless], df_preds_train_mk[useless] ], ignore_index=True)\n        df['c'] = pd.concat([ df_preds_train_normal[a], df_preds_train_mk[b] ], ignore_index=True)\n        df = df_preds_train.merge( df, on=useless, how='left' )\n        kappa = cohen_kappa_score( df['c'], df_preds_train['y'], weights='quadratic')\n        if kappa > best_kappa:\n            best_kappa = kappa\n            df_preds_train['best'] = df['c']\n            col_a = a\n            col_b = b\n\n\nfor c in df_preds_train.columns[3:]:\n    print('%s => Kappa: %.3f, RMSE: %.3f, Accuracy: %.3f' % (c, cohen_kappa_score(df_preds_train[c], df_preds_train['y'], weights='quadratic'), \n                                                                     np.sqrt(mean_squared_error( df_preds_train['y'], df_preds_train[c])),\n                                                                     accuracy_score( df_preds_train['y'], df_preds_train[c]) ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_preds_test_normal = m_normal.predict_class(X_test, mean_function=voting, installation_id=test_normal[useless] )\ndf_preds_test_normal['best'] = df_preds_test_normal[col_a]\ndf_preds_test_mk = m_multikid.predict_class(X_test_multikid, mean_function=voting, installation_id=test_multikid[useless] )\ndf_preds_test_mk['best'] = df_preds_test_mk[col_b]\n\n\ndf_preds_test = test[useless].reset_index(drop=True)\ndf_preds_test = df_preds_test.merge( pd.concat([df_preds_test_normal, df_preds_test_mk]), on=useless, how='left' )\ndf_preds_test.plot(title= 'Prediction - Test', subplots=True, layout=(4, 5), figsize=(16, 16), kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(df_preds_test[c].value_counts()/1000 for c in df_preds_test.columns[2:]).append(train['accuracy_group'].astype(int).value_counts()/len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = df_preds_test[['installation_id']]\nsubmission['accuracy_group'] = df_preds_test['best']\nsubmission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}