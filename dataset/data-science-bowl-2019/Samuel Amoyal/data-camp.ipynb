{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport operator\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport json\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First we will see what files we are given to work with. Note the train.csv file is quite large at 3.7G. From the data description we know:\n\n* train.csv & test.csv : These are the main data files which contain the gameplay events.\n* specs.csv : This file gives the specification of the various event types.\n* train_labels.csv : This file demonstrates how to compute the ground truth for the assessments in the training set.\n* sample_submission.csv : A sample submission in the correct format."},{"metadata":{},"cell_type":"markdown","source":"## The target.\n### First we will look at the target we intend to predict.\n\nWe are told: The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n* 3: the assessment was solved on the first attempt\n* 2: the assessment was solved on the second attempt\n* 1: the assessment was solved after 3 or more attempts\n* 0: the assessment was never solved"},{"metadata":{},"cell_type":"markdown","source":"\n## train.csv / test.csv\n### The data provided in these files are as follows:\n\n* event_id - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n* game_session - Randomly generated unique identifier grouping events within a single game or video play session.\n* timestamp - Client-generated datetime\n* event_data - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n* installation_id - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n* event_count - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n* event_code - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n* game_time - Time in milliseconds since the start of the game session. Extracted from event_data.\n* title - Title of the game or video.\n* type - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n* world - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement), 'CRYSTALCAVES' (Weight)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n\nprint('train', train.shape)\nprint('test', test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 11 million rows and just 11 columns. However, Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nAs there is no point in keeping training data that cannot be used for training anyway, I am getting rid of the installation_ids that never took an assessment"},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at event_data column :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_extraction(input_df):\n    \"\"\"\n    Extract keys and values from dict and store them in a DataFrame.\n    \n    input_df : Dataframe like train or test (intput)\n    df : DataFrame (output)\n    \"\"\"\n    df = pd.DataFrame()\n    \n    for i in tqdm(range(len(input_df))):\n        temp = pd.DataFrame(json.loads(input_df['event_data'][i]).items()).transpose()\n        temp.columns = temp.iloc[0]\n        temp = temp.drop([0])\n\n        df = pd.concat([df, temp], ignore_index=True)\n    \n    return df\n\nextract = feature_extraction(train.iloc[:100,:])\nextract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmissions = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\nprint('train_labels', train_labels.shape)\nprint('submissions', submissions.shape)\ntrain_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 16})\n\nse = train_labels.groupby(['title', 'accuracy_group'])['accuracy_group'].count().unstack('title')\nse.plot.bar(stacked=True, rot=0, figsize=(12,10))\nplt.title(\"Counts of accuracy group\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Could there be installation_id's who did assessments (we have already taken out the ones who never took one), but without results in the train_labels? As you can see below, yes there are 628 of those."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train.installation_id.isin(train_labels.installation_id.unique())].installation_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can not train on those installation_id's anyway, I am taking them out of the train set. This reduces our train set further from 8.3 million rows to 7.7 million."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.installation_id.isin(train_labels.installation_id.unique())]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Nombre de num_correct = 1:', train_labels[train_labels['num_correct']==1].shape[0])\nprint('Nombre de num_correct = 0:', train_labels[train_labels['num_correct']==0].shape[0])\nprint('game_session unique:', len(train_labels['game_session'].drop_duplicates()))\nprint('installation_id unique:', len(train_labels['installation_id'].drop_duplicates()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(data):\n    \"\"\"\n    input: data\n    output: data_labels for each of the 5 games\n    \"\"\"\n    \n    df = pd.DataFrame()\n    \n    games = ['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter']\n    \n    # Filtre parmis 1 des 5 jeux (game)\n    for game in games:\n        tmp = data[data['title'].str.contains(game)]\n        \n        # Filtre dernier event = assessment 4110/4100 (code)\n        if game == 'Bird Measurer':\n            tmp = tmp[tmp['event_code'] == 4110]\n        else:\n            tmp = tmp[tmp['event_code'] == 4100]\n    \n        # num_correct and num_incorrect\n        correct = [\"NA\" for i in range(np.shape(tmp)[0])]\n        incorrect = [\"NA\" for i in range(np.shape(tmp)[0])]\n        for i in range(np.shape(tmp)[0]):\n            if ('correct\":false' in tmp.loc[tmp.index[i], 'event_data']):\n                correct[i] = 0\n                incorrect[i] = 1\n            elif ('correct\":true' in tmp.loc[tmp.index[i], 'event_data']):\n                correct[i] = 1\n                incorrect[i] = 0\n            else:\n                correct[i] = 'NA'\n                incorrect[i] = 'NA'\n        tmp['num_correct'] = correct\n        tmp['num_incorrect'] = incorrect\n        tmp = pd.DataFrame(tmp.groupby(('installation_id','game_session','title')).sum())\n            \n        # accuracy\n        accuracy = tmp['num_correct'] / (tmp['num_correct'] + tmp['num_incorrect'])\n        tmp['accuracy'] = accuracy\n\n        # accuracy_group\n        tmp[\"accuracy_group\"] = tmp[\"accuracy\"].apply(lambda x: 0 if x==0 else (1 if x<0.5 else (2 if x<0.9 else 3)))\n        df = pd.concat([df, tmp])\n        \n    df = df.reset_index()[['game_session','installation_id','title','num_correct','num_incorrect','accuracy','accuracy_group']]\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_train_labels = get_accuracy(train)\nmy_train_labels = my_train_labels.sort_values(['installation_id', 'game_session']).reset_index(drop=True)\n\nprint(np.shape(my_train_labels), np.shape(train_labels))\n\nmy_train_labels == train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = get_accuracy(test)\ntest_labels = test_labels.sort_values(['installation_id', 'game_session']).reset_index(drop=True)\n\nprint(test_labels.shape)\ntest_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['event_data'].str.contains('false') & test['event_code'].isin([4100, 4110])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n#Credit to Erik Bruin\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 3614):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n# get usefull dict with maping encode\ntrain2, test2, train_labels2, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n# tranform function to get the train and test set\nreduce_train, reduce_test, categoricals = get_train_and_test(train2, test2)\n\nprint(reduce_train.shape)\nprint(reduce_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, features\n# call feature engineering function\nreduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)\n\nprint(reduce_train.shape)\nprint(reduce_test.shape)\nreduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"#on importe les données\nreduced_truncated_data = pd.read_csv(\"../input/reduced-truncated-data/reduced_truncated_data.csv\")\naccuracy_group_to_predict = pd.read_csv('../input/accuracy-group-to-predict/accuracy_group_to_predict.csv')\n\n\n#on ne garde que les colonnes communes, afin que le modèle puisse s'appliquer sur l'un puis sur l'autre\nreduced_truncated_data = reduced_truncated_data[reduced_truncated_data.columns.intersection(reduce_test.columns)]\nreduced_test = reduce_test[reduced_truncated_data.columns.intersection(reduce_test.columns)]\nprint(reduced_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On obtient Y contenant l'accuracy_group pour chaque ligne de reduced_truncated_data\nY = pd.merge(reduced_truncated_data, accuracy_group_to_predict, on='installation_id', how='outer')\nY = Y[[\"installation_id\",\"accuracy_group_y\"]]\nY.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(reduced_truncated_data).any(axis=1).any(axis=0)) #aucun NA\nprint(pd.isnull(Y).any(axis=1).any(axis=0)) #au moins un NA\n\nrows_to_keep = list(map(operator.not_,pd.isnull(Y[Y.columns[1]])))\nY = Y[rows_to_keep]\nreduced_truncated_data = reduced_truncated_data[rows_to_keep]\n\nprint(np.shape(reduced_truncated_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.shape(Y))\nprint(np.shape(reduced_truncated_data))\nprint(np.shape(reduced_test))\nprint(np.shape(accuracy_group_to_predict))\nY.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#il faut enlever les colonnes non numériques \nmsk = reduced_truncated_data.dtypes == np.object\nprint(reduced_truncated_data.loc[:,msk].columns)\n\nX = reduced_truncated_data.copy()\ndel X['installation_id']\n\nY2 = Y.copy()\ndel Y2['installation_id']\n\nprint(np.shape(X))\nprint(np.shape(reduced_truncated_data))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression().fit(X, Y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred = model.predict(X)\ny_pred = np.around(y_pred)\nprint(y_pred)\n\n#les valeurs sont-elles toutes 0,1,2 ou3 ? Si oui, on les transforme en 0 ou 3\nmsk = y_pred>3\nprint(y_pred[msk])\ny_pred[msk] = 3\n\nmsk = y_pred<0\nprint(y_pred[msk])\ny_pred[msk] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Par rapport aux vraies valeurs, quelle est notre proportion de bonnes valeurs\ncompar = (Y2 == y_pred)\ncompar[compar.columns[0]].value_counts()\n#Rq : ici, on a un accuracy_group par ligne (par action) alors qu'on veut seulement l'accuracy_group pour la dernière action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on récupère les installations id qu'on avait dû enlever pour lancer l'apprentissage\ninstall_ids = pd.DataFrame(reduced_truncated_data['installation_id']).reset_index(drop=True)\n\n#on obtient l'accuracy_group par ligne, avec à chaque fois l'installation_id\ny_pred2 = pd.concat([install_ids,pd.DataFrame(y_pred)], axis=1)\ny_pred2.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On récupère maintenant l'accuracy correspondant à la dernière ligne de chaque installation_id, c'est-à-dire à l'assessment à prédire\ndef accuracy_by_installation_id(y_pred2):\n    unique_id = np.unique(y_pred2['installation_id'])\n    \n    new_data = pd.DataFrame(columns = [\"installation_id\",\"accuracy_group\"])\n    \n    for id in unique_id:\n        #last line, so the assessment we want to predict\n        last_truncated_id = y_pred2[y_pred2['installation_id'] == id].tail(1)\n\n        #Update new_data\n        new_data = pd.concat([new_data, last_truncated_id])\n    return(new_data.reset_index(drop=True))\n\n\ny_pred_final = accuracy_by_installation_id(y_pred2)\nY_final = accuracy_by_installation_id(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Enfin, on compare l'accruacy_group pour chaque assessment qu'on avait à prédire\ncompar = (Y_final == y_pred_final)\ncompar[compar.columns[0]].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On a donc que des réussites, ce qui est rassurant car ça veut dire que l'entrainement s'est bien passé\n#Toutefois, il y a éventuellement un risque de surapprentissage\n#Passons maintenant aux données à prédire (reduced_test)\n\n#il faut enlever les colonnes non numériques \nmsk = reduced_test.dtypes == np.object\nprint(reduced_test.loc[:,msk].columns)\n\nX_test = reduced_test.copy()\ndel X_test['installation_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test = model.predict(X_test)\ny_pred_test = np.around(y_pred_test)\n\n#les valeurs sont-elles toutes 0,1,2 ou3 ? Si non, on les transforme en 0 ou 3\nmsk = y_pred_test>3\nprint(y_pred_test[msk])\ny_pred_test[msk] = 3\n\nmsk = y_pred_test<0\nprint(y_pred_test[msk])\ny_pred_test[msk] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on récupère les installations id qu'on avait dû enlever pour lancer l'apprentissage\ninstall_ids = pd.DataFrame(reduced_test['installation_id']).reset_index(drop=True)\n\n#on obtient l'accuracy_group par ligne, avec à chaque fois l'installation_id\ny_pred_test2 = pd.concat([install_ids,pd.DataFrame(y_pred_test)], axis=1)\ny_pred_test2.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test_final = accuracy_by_installation_id(y_pred_test2)\ny_pred_test_final[\"accuracy_group\"] = y_pred_test_final[\"accuracy_group\"].astype(int)\nprint(y_pred_test_final.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissions = y_pred_test_final\nsubmissions.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom keras.callbacks import Callback\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = reduce_train['accuracy_group']\n\nX =reduce_train[features]\n\ndummy_y = np_utils.to_categorical(y)\n\ninput_dim= X.shape[1]\nprint('input_dim is:', input_dim)\nfeatures = X.columns\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = eval_qwk_lgb(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = eval_qwk_lgb(self.y_val, y_pred_val)\n        print('\\rqwk: %s - qwk_val: %s' % (str(roc),str(roc_val)),end=100*' '+'\\n')\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX = sc.fit_transform(X)\n\nmodel = Sequential()\nmodel.add(Dense(80, input_dim=input_dim, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4, activation='softmax'))\n\n# model.add(Dense(300, input_dim=input_dim, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(200, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(150, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(100, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(4, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n\ntrain_x, valid_x , train_y, valid_y = train_test_split(X, dummy_y, test_size=0.2, random_state=2020)\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.008)\nmodel.fit(train_x, train_y, batch_size = 32, epochs = 100,validation_data=(valid_x, valid_y),\n               callbacks=[reduce_lr,roc_callback(training_data=(train_x, train_y),validation_data=(valid_x, valid_y)),early_stopping],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(sc.transform(reduce_test[features]))\n\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\nsample_submission['accuracy_group'] = pd.DataFrame(preds).idxmax(axis=1).astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}