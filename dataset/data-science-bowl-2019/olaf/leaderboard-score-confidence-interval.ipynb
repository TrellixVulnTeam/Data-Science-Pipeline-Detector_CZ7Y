{"cells":[{"metadata":{},"cell_type":"markdown","source":"Public score can be considered an estimator of the private score.\nThis notebook attempts to calculate a confidence interval for this score, in a non-parametric way.\n\n**Assumptions:**\n* the real accuracy groups for the public test set and the private test set are drawn from the same distribution as train_labels.csv\n* the public test set size is 1000 installations (as observed)\n* the private test set size is 86/14*1000 and doesn't contain public test data (because \"Public leaderboard is calculated with approximately 14% of the test data. The final results will be based on the other 86%, so the final standings may be different.\")\n\n**Conclusions:**\n* The test set is **much** too small\n* It doesn't really matter if you improve by 0.001, or even by 0.010 – on the private leaderboard you may be worse by 0.050 as well\n* Anybody from the first ~25 places in public leaderboard may be the winner on the private leaderboard\n* Kappa seems to be biased – private scores are about 2% higher than the corresponding public scores\n* Private test set is about 6 times larger, so its standard error should be about (1/6)^(1/2) = 40% of the standard error in the public test score – still we are playing roulette here, the best solution on the private leaderboard may not be the best one in reality\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport random\nimport time\nfrom IPython.display import display\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#From https://www.kaggle.com/cpmpml/ultra-fast-qwk-calc-method\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    1. Start from perfect agreement, \n    2. Add mistakes one by one, watching how the public and private score change\n    3. Do it until public and private scores are too low to be interesting\n    4. Repeat the whole process 1000 times\n\"\"\"\ndef test(public_size, private_size, min_score):\n    size = public_size + private_size\n    n_iter = 1000\n    n_iter2 = 200\n\n    stats = []\n    for i in range(n_iter):\n        data['submission'] = data['accuracy_group']\n\n        while(True):\n            for m in range(n_iter2):\n                n = random.randrange(0, size)\n                v = data.loc[n, 'submission']\n                if v == 0:\n                    data.loc[n,'submission'] = 1\n                elif v == 3:\n                    data.loc[n,'submission'] = 2\n                else:\n                    data.loc[n,'submission'] = v - 1 + 2 * random.randint(0, 1)\n            public_set = data.iloc[private_size : size]\n            public_kappa = qwk3(public_set['accuracy_group'], public_set['submission'])\n            private_set = data.iloc[:private_size]\n            private_kappa = qwk3(private_set['accuracy_group'], private_set['submission'])\n            if public_kappa < min_score and private_kappa < min_score:\n                break\n            d = {\n                'public_score': public_kappa,\n                'private_score': private_kappa,\n                'public_score_bin': int(public_kappa * 1000) / 1000,\n                'private_score_bin': int(private_kappa * 1000) / 1000,\n            }\n            stats.append(d)\n\n    return pd.DataFrame(stats)\n\nstats = test(1000, round(86/14*1000), 0.300)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(df, cat, val, min_score, max_score, title):\n    groups = df.where((df[cat] >= min_score) & (df[cat] <= max_score)).groupby([cat], as_index = False).agg({val: [\n        ('5%', (lambda x: x.quantile(.05))),\n        ('avg', 'mean'),\n        ('95%', (lambda x: x.quantile(.95))),\n        ('standard deviation', 'std')\n    ]})\n    \n    mean = groups[val]['avg']\n    error_low = mean - groups[val]['5%']\n    error_high = groups[val]['95%'] - mean\n    \n    fig = plt.figure()\n    fig.suptitle(title)\n    plt.xlabel(cat)\n    plt.ylabel(val)\n    plt.errorbar(groups[cat], mean, yerr=[error_low, error_high], markersize=5, markeredgecolor='red', markerfacecolor='red', linestyle='')\n    plt.grid()\n    plt.show()\n\n    return groups\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groups1 = plot(stats, 'public_score_bin', 'private_score', 0.400, 0.600, 'Private score 95% confidence intervals assuming given public score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Full table below:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 1000)\ndisplay(groups1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}