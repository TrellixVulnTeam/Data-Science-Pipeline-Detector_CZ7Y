{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science Bowl 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data exploration\n### Lets take a look at our data first. It consists of:\n* <span style=\"background-color:lightgray\">train.csv, test.csv</span> - main data files, which contain the gameplay events.\n* <span style=\"background-color:lightgray\">specs.csv</span> - this file gives the specification of the various event types.\n* <span style=\"background-color:lightgray\">train_labels.csv</span> - this file demonstrates how to compute the ground truth for the assessments in the training set.\n\n<br><br>\n#### As train data set is really big, for now we'll only use a part of it - randomly chosen entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# random sample\nfilename = \"../input/data-science-bowl-2019/train.csv\"\nn = sum(1 for line in open(filename)) - 1\ns = 1000000 #desired sample size\nskip = sorted(random.sample(range(1,n+1),n-s))\n\ntrain_data = pd.read_csv(filename, skiprows=skip)\nsample_submission = pd.read_csv(\"../input/data-science-bowl-2019/sample_submission.csv\")\nspecs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\ntest_data = pd.read_csv(\"../input/data-science-bowl-2019/test.csv\")\ntrain_labels = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n#### We can start by getting rid of users that didnt take assessments, as we cant use them for training."},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"users = train_data['installation_id'].drop_duplicates()\nprint('unique users: {}'.format(users.size))\nattempted_users = train_data[train_data['type']=='Assessment'][['installation_id']].drop_duplicates() \nprint('users, who attempted assessments: {}'.format(attempted_users.size))\ntrain_data = pd.merge(train_data, attempted_users, on=\"installation_id\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n### Now lets visualize some data.\n#### First of all, some info about events might be usefull"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\ntype_count = train_data.groupby('type').count()\nfor t in train_data['type'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.type == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\nfor t in train_data['title'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.title == t]))\n\nfig = plt.figure(figsize=(13, 15))\nplt.barh(names, values)\nplt.title('Number of events by title')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Each event belongs to section, we need to learn about those. Most importanlty, how assessments are divided"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.world.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MAGMAPEAK - {}\\n'.format(pd.unique(train_data[(train_data.world == 'MAGMAPEAK') & (train_data.type == 'Assessment')].title)))\nprint('CRYSTALCAVES - {}\\n'.format(pd.unique(train_data[(train_data.world == 'CRYSTALCAVES') & (train_data.type == 'Assessment')].title)))\nprint('TREETOPCITY - {}\\n'.format(pd.unique(train_data[(train_data.world == 'TREETOPCITY') & (train_data.type == 'Assessment')].title)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\ntype_count = train_data.groupby('world').count()\nfor t in train_data['world'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.world == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by world')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So, magmapeak have only one assessment, but bigger number of events? Interesting. We will adress this later. <br>\n#### We know that assessments results are captured with event code 4100 and 4110 for Bird Measurer. Lets check "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data.event_code == 4100].title.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are unnecessary stuff, seems like event type must be taken into account."},{"metadata":{},"cell_type":"markdown","source":"#### After that we may find something in connection between events and time of their accurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['timestamp'] = pd.to_datetime(train_data['timestamp'])\ntrain_data['weekday'] = train_data['timestamp'].dt.dayofweek\ntrain_data['hour'] = train_data['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8))\nnames = ['Mon', 'Tue', 'Wd', 'Thu', 'Fri', 'Sat', 'Sun']\nvalues = []\nfor d in range(7):\n    values.append(len(train_data[train_data.weekday == d]))\nplt.bar(names, values)\nplt.title('Event count by weekday')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14, 9))\nnames = range(24)\nvalues = []\nfor h in range(24):\n    values.append(len(train_data[train_data.hour == h]))\nplt.bar(names, values, width=0.5)\nplt.title('Event count by hour')\nplt.xticks(range(24))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig = plt.figure(figsize=())\ntime_by_session = train_data[['game_session', 'world', 'game_time']].groupby(['game_session', 'world']).max()\n\nattempted_users\nplaytime = []\nfor u in attempted_users['installation_id']:\n    time_by_world = {'MAGMAPEAK':0,\n                'TREETOPCITY':0,     \n                 'CRYSTALCAVES':0,\n                 'NONE':0}\n    sessions_by_user = train_data[train_data.installation_id == u]['game_session'].drop_duplicates()\n    for s in sessions_by_user:\n        tmp = time_by_session.loc[s]['game_time'].iloc[0]\n        time_by_world[time_by_session.loc[s].index.to_list()[0]] += tmp\n    playtime.append(time_by_world)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,8))\nplt.plot([u['MAGMAPEAK'] + u['TREETOPCITY'] + u['CRYSTALCAVES'] for u in playtime])\nplt.title('Users playtime')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like day of week doesnt matter, but time of day and total playtime really differ from user to user. What about time spent per world?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5))\nval = [0, 0, 0]\nfor u in playtime:\n    val[0] += u['MAGMAPEAK']\n    val[1] += u['TREETOPCITY']\n    val[2] += u['CRYSTALCAVES']\nplt.bar(['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES'], val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Its actually roughly the same as event count. That means we can trear all actions equally, as they take almost the same time, which is helpfull. Also, time spent on magmapeak is not that impactfull as on two other worlds. <br>\n### Next lets look at train_labels as it contains results for the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head(9)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"train_labels[['installation_id', 'accuracy_group']].groupby(['accuracy_group']).count().plot.bar(figsize=(10, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Half of users solve correctly on first try, okay. Now what about distribution between assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = pd.unique(train_labels.title)\nmean_wrong = []\nfor t in tasks:\n    mean_wrong.append(train_labels[train_labels.title == t].num_incorrect.mean())\nfig = plt.figure(figsize=(7, 7))\nplt.pie(mean_wrong, labels=tasks)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Seems like two of them are particulary hard. Need to keep close eye on them."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n### Now when we have some data to work with we can assemble it for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(data):\n    global attempted_users, playtime\n    labels = ['activities', 'games', 'clips', 'assessments', 'mean_activity_daytime', 'mean_game_daytime', 'mean_clip_daytime', 'mean_assessment_daytime', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'Cauldron_Filler_failed', 'Cauldron_Filler_success', 'Cart_Balancer_failed', 'Cart_Balancer_success', 'Chest_Sorter_failed', 'Chest_Sorter_success','Mushroom_Sorter_failed', 'Mushroom_Sorter_success', 'Bird_Measurer_failed', 'Bird_Measurer_success']\n    result = pd.DataFrame(columns=labels)\n    \n    for i, u in enumerate(attempted_users.installation_id):\n        tmp = pd.DataFrame(columns=labels)\n        cur_user = data[data.installation_id == u]\n        \n        sub = cur_user[cur_user.type == 'Activity']\n        tmp['activities'] = pd.Series(len(sub))\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        if len(m) > 0:\n            tmp['mean_activity_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_activity_daytime'] = pd.Series(None)\n        \n        sub = cur_user[cur_user.type == 'Game']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['games'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_game_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_game_daytime'] = pd.Series(None)\n            \n        sub = cur_user[cur_user.type == 'Clip']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['clips'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_clip_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_clip_daytime'] = pd.Series(None)\n            \n        sub = cur_user[cur_user.type == 'Assessment']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['assessments'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_assessment_daytime'] = pd.Series(m[0][0]) \n        else:\n            tmp['mean_assessment_daytime'] = pd.Series(None)\n        \n        tmp['MAGMAPEAK'] = pd.Series(playtime[i]['MAGMAPEAK'])\n        tmp['TREETOPCITY'] = pd.Series(playtime[i]['TREETOPCITY'])\n        tmp['CRYSTALCAVES'] = pd.Series(playtime[i]['CRYSTALCAVES'])\n       \n        sub = cur_user[((cur_user.event_code == 4100) & (cur_user.title != 'Bird Measurer (Assessment)') & (cur_user.type == 'Assessment')) | ((cur_user.event_code == 4110) & (cur_user.title == 'Bird Measurer (Assessment)') & (cur_user.type == 'Assessment'))]\n        tmp.loc[:, 'Cauldron_Filler_failed':] = np.ndarray(10)\n        for i, r in sub.iterrows():\n            if json.loads(r.event_data)['correct']:\n                if r.title == 'Cauldron Filler (Assessment)':\n                    tmp.Cauldron_Filler_success += 1\n                elif r.title == 'Cart Balancer (Assessment)':\n                    tmp.Cart_Balancer_success += 1\n                elif r.title == 'Chest Sorter (Assessment)':\n                    tmp.Chest_Sorter_success += 1\n                elif r.title == 'Mushroom Sorter (Assessment)':\n                    tmp.Mushroom_Sorter_success += 1\n                elif r.title == 'Bird Measurer (Assessment)':\n                    tmp.Bird_Measurer_success += 1\n            else:\n                if r.title == 'Cauldron Filler (Assessment)':\n                    tmp.Cauldron_Filler_failed += 1\n                elif r.title == 'Cart Balancer (Assessment)':\n                    tmp.Cart_Balancer_failed += 1\n                elif r.title == 'Chest Sorter (Assessment)':\n                    tmp.Chest_Sorter_failed += 1\n                elif r.title == 'Mushroom Sorter (Assessment)':\n                    tmp.Mushroom_Sorter_failed += 1\n                elif r.title == 'Bird Measurer (Assessment)':\n                    tmp.Bird_Measurer_failed += 1\n        \n        result = result.append(tmp, ignore_index=True)\n        \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = create_features(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}