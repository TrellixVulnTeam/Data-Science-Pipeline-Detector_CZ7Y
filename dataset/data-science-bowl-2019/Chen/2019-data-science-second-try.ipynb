{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette(n_colors=10)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, BatchNormalization, Activation\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nimport tensorflow as tf\nimport keras\nfrom functools import reduce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step1: Define Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column to capture the correct/incorrect info from specs data\nspecs['attempts'] = ''\n\nfor i in range(len(specs)):\n    if ('(Correct)' in specs['info'][i]) or ('(Incorrect)' in specs['info'][i]):\n        specs['attempts'][i] = 1\n    else:        \n        specs['attempts'][i] = 0\n# It is clearly that some event_id are not in assessment maps, \n# so only some of the event_id have the value from attempts.\n# Next, we drop the useless columns to make it clear bacause the next step will be merged with train data\n\nspecs_drop = specs.drop(['info','args'],axis=1)\n\n# merge the specs_attempts data with train data\ntrain_cor = pd.merge(train,specs_drop,on='event_id',how='left')\n        \n# merge the specs_attempts data with test data\ntest_cor = pd.merge(test,specs_drop,on='event_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Create a new variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_new_variables(data):\n    \n    # 1. create new variables towards 'timestamp'\n    \n    '''\n    change the data format first, and then create new variables\n    '''\n    \n    data['timestamp'] =  pd.to_datetime(data['timestamp'])\n    data['hour'] = data['timestamp'].dt.hour\n    data['weekday']  = data['timestamp'].dt.dayofweek\n    \n    data['weekend'] = ['yes' if index in([5,6])  else 'no' for  index in data['weekday']]\n    data['evening'] = ['yes' if index in([17,23]) else 'no' for index in data['hour']]\n    data['freetime'] = [1 if (index_1 =='yes' or index_2 == 'yes') else 0 \\\n                               for (index_1,index_2) in zip(data['weekend'],data['evening'])]\n    \n    # 2. # create a new variable named as assessment. 'yes' represents it is a assessment type.\n    \n    data['assessment'] = [1 if index =='Assessment'  else 0 for  index in data['type']]\n    \n    # 3. create five new variables towards titile\n    titles = ['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter']\n    for each_var in titles:\n        data[each_var] = [1 if (each_var) in index else 0 for index in data['title']]\n    \n    # 4. create four new variables towards \n    world = ['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES']\n    for each_wor in world:\n        data[each_wor] = [1 if (each_wor) in index else 0 for index in data['world']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. create different dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_data(data):\n    '''\n    groupby different key to get different dataset,\n    and then merge them together\n    '''\n    \n    # 1. groupby attempts\n    attempts = data.groupby(['installation_id','game_session'],as_index=False)['attempts'].sum()\n    \n    # 2. groupby freetime\n    freetime = data.groupby(['installation_id','game_session'],as_index=False)['freetime'].last()\n    \n    # 3. groupby event_id\n    eventid = data.groupby(['installation_id','game_session'])['event_id'].nunique().reset_index()\n    \n    # 4. groupby gametime\n    gametime = data.groupby(['installation_id','game_session'],as_index=False)['game_time'].max()\n    \n    # 5. groupby event_count\n    eventcount = data.groupby(['installation_id','game_session'],as_index=False)['event_count'].max()\n    \n    # 6. groupby type\n    ass_type = data.groupby(['installation_id','game_session'],as_index=False)['assessment'].last()\n    \n    # 7. groupby event_code\n    etcode = data.groupby(['installation_id','game_session'])['event_code'].nunique().reset_index()\n    \n    # 8. groupby all kinds of title\n    title =  data.groupby(['installation_id','game_session'],as_index=False)\\\n    ['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter'].last()\n    \n    # 9. groupby all kinds of worlds\n    world = data.groupby(['installation_id','game_session'],as_index=False)['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES'].last()\n    \n    \n    '''\n    merge all data together\n    '''\n    \n    datalist = [attempts,freetime,eventid,gametime,eventcount,ass_type,etcode,title,world]\n    new_data = reduce(lambda x,y: pd.merge(x,y, on=['installation_id','game_session'], how='outer'), datalist)\n    \n    return new_data\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step2: Apply to test/train data to train Model"},{"metadata":{},"cell_type":"markdown","source":"1. Apply to train data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"create_new_variables(data=train_cor)\ntrain_merge_data = merge_data(data=train_cor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Apply to test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"create_new_variables(data=test_cor)\ntest_merge_data = merge_data(data=test_cor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. merge train_merge_data with train_laebls to get the result"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = pd.merge(train_merge_data,train_labels,on=['installation_id','game_session'],how='outer')\n# fill nan values in dataset\ntrainset.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset = test_merge_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Define train_X, train_ybin,train_ynum"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = trainset[['attempts','freetime','event_id','game_time','event_count','assessment',\\\n              'event_code','Bird Measurer','Cart Balancer','Cauldron Filler','Chest Sorter','Mushroom Sorter',\n                    'NONE','TREETOPCITY','CRYSTALCAVES','MAGMAPEAK']].values.astype('int')  \ntrain_ybin = trainset[['num_correct']].values.astype('int')\ntrain_ynum = trainset[['num_incorrect']].values.astype('int')\n\ntest_X = testset[['attempts','freetime','event_id','game_time','event_count','assessment',\\\n              'event_code','Bird Measurer','Cart Balancer','Cauldron Filler','Chest Sorter','Mushroom Sorter',\n                    'NONE','TREETOPCITY','CRYSTALCAVES','MAGMAPEAK']].values.astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Trian the Model using ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_input = Input(shape=(16,), name='one_input') # pass by one input\n\n# show one output: y_bin\ny_bin_output = Dense(1, activation='sigmoid', name='y_bin_output')(one_input)\n# merge one output with all predictors from input\nx = keras.layers.concatenate([one_input, y_bin_output]) \n# stack all other layers\nx = Dense(64, activation='relu')(x)\nx = BatchNormalization()(x)\n#another output\ny_num_output = Dense(1, activation='sigmoid', name='y_num_output')(x)\n\n\nmodel = Model(inputs=one_input, outputs=[y_bin_output, y_num_output])\nmodel.compile(optimizer='Adam', loss=['binary_crossentropy', 'mean_squared_error'])\n\nmodel.fit(train_X, [train_ybin, train_ynum],epochs=30,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Get the final prediction"},{"metadata":{},"cell_type":"markdown","source":"Apply the model on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"testset['num_correct'] = 0\ntestset['num_incorrect'] = 0\n\nfor i in range(len(testset)):\n    value = testset.iloc[i:i+1,2:18].values\n    pred_y = model.predict(value)\n    testset['num_correct'][i] = pred_y[0].astype('int')\n    testset['num_incorrect'][i] = np.around(pred_y[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset['accuracy'] = testset['num_correct']/(testset['num_correct'] + testset['num_incorrect'])\n\n# fill nan\ntestset.fillna(0,inplace=True)\n\n# calculate accuracy_group\ntestset['accuracy_group'] = 0\nfor m in range(len(testset)):\n    if testset['accuracy'][m] == 1:\n        testset['accuracy_group'][m] =3\n    elif 0.5 <= testset['accuracy'][m] < 1:\n        testset['accuracy_group'][m] =2\n    elif 0 < testset['accuracy'][m] < 0.5:\n        testset['accuracy_group'][m] =1\n    elif testset['accuracy'][m] == 0:\n        testset['accuracy_group'][m] =0\n        \nfinal_pred_1 = testset[(testset['Bird Measurer'] !=0) | (testset['Cart Balancer'] !=0)| (testset['Cauldron Filler'] !=0)\\\n                    | (testset['Chest Sorter'] !=0)| (testset['Mushroom Sorter'] !=0)]\n\nfinal_pred = final_pred_1.groupby('installation_id',as_index=False)['accuracy_group'].mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred['accuracy_group'] = (np.around(final_pred['accuracy_group'])).astype('int')\nfinal_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save as csv\nfinal_pred.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}