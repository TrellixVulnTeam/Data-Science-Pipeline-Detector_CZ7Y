{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        \n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette(n_colors=10)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, BatchNormalization, Activation\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nimport tensorflow as tf\nimport keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1 : Get the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in the data CSV files\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we get into detail of the model, we should have a general idea of what is the data, what kind of predictors that the data have. First, we will use .head() to check the first several rows of the data. In this case, we will only care about train, train_label, and specs data, because the predictors in test data are the same with train data. Therefore, we will use train data to train the models and choose the model with best performance, and then apply to the test and get the result. \n\nBased on my understanding of this project, I will groupby installation_id and game_session, transform the data into different shapes, and then merge train data with train_label data to train the model. "},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Filter/Clean the data: Define Functions"},{"metadata":{},"cell_type":"markdown","source":"1. Attempts: I will use specs dataset to extract the type of attempts (including correct and incorrect) based on event_info. I will count how many attempts that eachinstallation_id tried. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column to capture the correct/incorrect info from specs data\nspecs['attempts'] = ''\n\nfor i in range(len(specs)):\n    if ('(Correct)' in specs['info'][i]) or ('(Incorrect)' in specs['info'][i]):\n        specs['attempts'][i] = 1\n    else:        \n        specs['attempts'][i] = 0\n# It is clearly that some event_id are not in assessment maps, \n# so only some of the event_id have the value from attempts.\n# Next, we drop the useless columns to make it clear bacause the next step will be merged with train data\n\nspecs_drop = specs.drop(['info','args'],axis=1)\n\n# merge the specs_attempts data with train data\ntrain_cor = pd.merge(train,specs_drop,on='event_id',how='left')\n\n# Finally, I count the total attempts groupby installation_id and game_session.\ntrain_attempts = train_cor.groupby(['installation_id','game_session'],as_index=False)['attempts'].sum()\n\n# Plot\ntrain_attempts['attempts'] \\\n    .plot(kind='hist',\n          figsize=(10, 5),\n          xlim = (0,100),\n          bins=100,\n          title='Total attempts',\n         color = color[1])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It is clearly to see that a lot of installation_id didn't even try to pass the given assessment."},{"metadata":{},"cell_type":"markdown","source":"2. Freetime: each game_session has it's unique time_stamp. Check each game_seesion's timestamp is in freetime or not.In order to do this, I will create two new predictors named 'weekend' and 'evening'. If is in weekend/evening, the value will be 'Yes', otherwise, it will be 'No'. (Notice: This predictor is a dummy varlable.). Freetime means either the timestamp is in weekend or in evening."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each data, create two new variablse named'weekend' and 'evening'\n# Because within one game_session, the timestamp is continuous, so we only consider the last timestamp\ntrain_date_temp = train[['installation_id','game_session','timestamp']]\\\n.groupby(['installation_id','game_session'],as_index=False)['timestamp'].last()\ntrain_date_temp['timestamp'] = pd.to_datetime(train_date_temp['timestamp'])\n\n# transform timestamp into hour and dayofweek(0 represents Monday, 6 represents Sunday.). \ntrain_date_temp['hour'] = train_date_temp['timestamp'].dt.hour\ntrain_date_temp['weekday']  = train_date_temp['timestamp'].dt.dayofweek\n\n# create a new variable named as weekend.\ntrain_date_temp['weekend'] = ['yes' if index in([5,6])  else 'no' for  index in train_date_temp['weekday']]\n\n# create a new variable named as Evening.\ntrain_date_temp['evening'] = ['yes' if index in([17,23]) else 'no' for index in train_date_temp['hour']]\n\n# create a new variable named as freetime\ntrain_date_temp['freetime'] = ['yes' if (index_1 =='yes' or index_2 == 'yes') else 'no' \\\n                               for (index_1,index_2) in zip(train_date_temp['weekend'],train_date_temp['evening'])]\n\n# drop useless variables\ntrain_date = train_date_temp.drop(['timestamp','hour','weekday','weekend','evening'],axis=1)\n\n# merge with train_attempts\ntrain_prep1 = pd.merge(train_attempts,train_date,on=['installation_id','game_session'],how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a graph to see the relationship between freetime and total attempts\naf_plot = train_prep1.groupby('freetime',as_index=False)['attempts'].sum()\nnames = list(af_plot['freetime'].unique())\nvalues = list(af_plot['attempts'])\n\nplt.bar(names, values)\nplt.title('total attempts in weekend')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It is clearly to see that in weekend, children tend not to try again. "},{"metadata":{},"cell_type":"markdown","source":"3. Unique Event_id: within one installation_id and game_session, how many unique values of event_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a copy of train dataset\ntrain_event_temp = train.copy()\n# groupby installation_id and game_sesssion to count the unique values of event_id\ntrain_event = train_event_temp.groupby(['installation_id','game_session'])['event_id'].nunique().reset_index()\n\n# rename the column's name\ntrain_event.columns = ['installation_id','game_session','uni_eventid']\n\n# merge with train_prep1\ntrain_prep2 = pd.merge(train_prep1,train_event,on=['installation_id','game_session'],how='outer')\n\n# plot\ntrain_event['uni_eventid'].plot(kind='hist',\n                               figsize=(6, 4), title='Unique values of event_id',\n                               color = color[2])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Total Game Time: count total game time based on game_time. Becase time will increase since the start of the game session, it is easy to keep the last value of each unique game_session within one installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session to get the maxium value of game_time.\ntrain_gametime = train.groupby(['installation_id','game_session'],as_index=False)['game_time'].max()\n\n# merge with train_prep2\ntrain_prep3 = pd.merge(train_prep2,train_gametime,on=['installation_id','game_session'],how='outer')\n\n# plot\ntrain_gametime['game_time'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log od total game time',\n                                                 color = color[3])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Based on this graph, we could see that a lot of installation didn't even try this game becase a lot of game times are 0."},{"metadata":{},"cell_type":"markdown","source":"5. Total Event Count: event count is the incremental counter of events within a game session. We could find the maximum value of each game session within one installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session to get the maxium value of event_count.\ntrain_etcount = train.groupby(['installation_id','game_session'],as_index=False)['event_count'].max()\n\n# merge with train_prep3\ntrain_prep4 = pd.merge(train_prep3,train_etcount,on=['installation_id','game_session'],how='outer')\n\n# plot\ntrain_etcount['event_count'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log od total event count',\n                                                 color = color[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Assessment(dummy variable): for each game session, we need to check if the type of game session is assessment or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# before we get into detail, we need to check if one game seesion has only one type.\ntrain_gtcheck = train.groupby(['installation_id','game_session'])['type'].nunique().reset_index()\nlen(train_gtcheck[train_gtcheck['type'] != 1]) # 0\n# which means one game_session has only one unique type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session\ntrain_assess = train.groupby(['installation_id','game_session'],as_index=False)['type'].last()\n\n# create a new variable named as assessment. 'yes' represents it is a assessment type.\ntrain_assess['assessment'] = ['yes' if index =='Assessment'  else 'no' for  index in train_assess['type']]\n\n# drop useless variable and then merge with train_prep4\ntrain_assess = train_assess.drop('type',axis=1)\ntrain_prep5 = pd.merge(train_prep4,train_assess,on=['installation_id','game_session'],how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a graph to see the relationship between 'assessment type' and total attempts\nat_plot = train_prep5.groupby('assessment',as_index=False)['attempts'].sum()\nnames_at = list(at_plot['assessment'].unique())\nvalues_at = list(at_plot['attempts'])\n\nplt.bar(names_at, values_at)\nplt.title('total attempts in assessment type')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Unique event_code: how many unique values of event_code in one game session."},{"metadata":{"trusted":true},"cell_type":"code","source":"# count values\ntrain_etcode = train.groupby(['installation_id','game_session'])['event_code'].nunique().reset_index()\n\n# merge data\ntrain_prep6 = pd.merge(train_prep5,train_etcode,on=['installation_id','game_session'],how='outer')\n\n# plot\ntrain_etcode['event_code'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log of total event code',\n                                                 color = color[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. For each game seesion,which 'title' that the game session belongs to if the type of this game session is 'Assessment'. There are five different assessment titles including Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter. Note: if the type of game session is not assessment, then all of the five values equals to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# before we get into more detail, we need to check is there only one unique title within one game session.\ntrain_gticheck = train.groupby(['installation_id','game_session'])['title'].nunique().reset_index()\nlen(train_gticheck[train_gtcheck['type'] != 1]) #0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_title = train.copy() # make a copy of train data set.\n\n# write a loop to create five new variables\ntitles = ['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter']\nfor each_var in titles:\n    train_title[each_var] = 0  # initializing \n    train_title[each_var] = [1 if (each_var) in index else 0 for index in train_title['title']]\n    \n# groupby installation_id and game_session\ntrain_five_title = train_title.groupby(['installation_id','game_session'],as_index=False)\\\n['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter'].last()\n\n# merge with train_prep6\ntrain_prep7 = pd.merge(train_prep6,train_five_title,on=['installation_id','game_session'],how='outer')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\ntrain_title_names = list(titles)\ntrain_title_values = []\nfor var in train_title_names:\n    train_title_values.append(len(train_five_title[train_five_title[var] != 0]))\n\nplt.bar(train_title_names, train_title_values)\nplt.title('Freq of Each Assessment')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9. Similar to (8). this time I care about the which world it is in the same game session. There are four different world, including  'NONE' (at the app's start screen), 'TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement) and 'CRYSTALCAVES' (Weight)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# based on 8, we know that one game_session only has one unique world, therefore we omit the checking procedure for simplicity.\n\ntrain_world = train.copy() # make a copy of train data set.\n# write a loop to create four new variables\nworld = ['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES']\nfor each_wor in world:\n    train_world[each_wor] = 0  # initializing \n    train_world[each_wor] = [1 if (each_wor) in index else 0 for index in train_world['world']]\n    \n# groupby installation_id and game_session\ntrain_four_world = train_world.groupby(['installation_id','game_session'],as_index=False)\\\n['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES'].last()\n\n# merge with train_prep7\ntrain_prep8 = pd.merge(train_prep7,train_four_world,on=['installation_id','game_session'],how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\ntrain_world_names = list(world)\ntrain_world_values = []\nfor wor in train_world_names:\n    train_world_values.append(len(train_four_world[train_four_world[wor] != 0]))\n\nplt.bar(train_world_names, train_world_values)\nplt.title('Freq of Each World')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Filter/Clean Test data set"},{"metadata":{},"cell_type":"markdown","source":"Because test data is similar with train data, so we will use same way to clean and filter data"},{"metadata":{},"cell_type":"markdown","source":"1. Attempts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge the specs_attempts data with test data\ntest_cor = pd.merge(test,specs_drop,on='event_id',how='left')\n\n# Finally, I count the total attempts groupby installation_id and game_session.\ntest_attempts = test_cor.groupby(['installation_id','game_session'],as_index=False)['attempts'].sum()\n\n# Plot\ntest_attempts['attempts'] \\\n    .plot(kind='hist',\n          figsize=(10, 5),\n          xlim = (0,100),\n          bins=100,\n          title='Total attempts',\n         color = color[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Freetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each data, create two new variablse named'weekend' and 'evening'\n# Because within one game_session, the timestamp is continuous, so we only consider the last timestamp\ntest_date_temp = test[['installation_id','game_session','timestamp']]\\\n.groupby(['installation_id','game_session'],as_index=False)['timestamp'].last()\ntest_date_temp['timestamp'] = pd.to_datetime(test_date_temp['timestamp'])\n\n# transform timestamp into hour and dayofweek(0 represents Monday, 6 represents Sunday.). \ntest_date_temp['hour'] = test_date_temp['timestamp'].dt.hour\ntest_date_temp['weekday']  = test_date_temp['timestamp'].dt.dayofweek\n\n# create a new variable named as weekend.\ntest_date_temp['weekend'] = ['yes' if index in([5,6])  else 'no' for  index in test_date_temp['weekday']]\n\n# create a new variable named as Evening.\ntest_date_temp['evening'] = ['yes' if index in([17,23]) else 'no' for index in test_date_temp['hour']]\n\n# create a new variable named as freetime\ntest_date_temp['freetime'] = ['yes' if (index_1 =='yes' or index_2 == 'yes') else 'no' \\\n                               for (index_1,index_2) in zip(test_date_temp['weekend'],test_date_temp['evening'])]\n\n# drop useless variables\ntest_date = test_date_temp.drop(['timestamp','hour','weekday','weekend','evening'],axis=1)\n\n# merge with test_attempts\ntest_prep1 = pd.merge(test_attempts,test_date,on=['installation_id','game_session'],how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a graph to see the relationship between freetime and total attempts\naf_plot = test_prep1.groupby('freetime',as_index=False)['attempts'].sum()\nnames = list(af_plot['freetime'].unique())\nvalues = list(af_plot['attempts'])\n\nplt.bar(names, values)\nplt.title('total attempts in weekend')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Unique Event_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a copy of test dataset\ntest_event_temp = test.copy()\n# groupby installation_id and game_sesssion to count the unique values of event_id\ntest_event = test_event_temp.groupby(['installation_id','game_session'])['event_id'].nunique().reset_index()\n\n# rename the column's name\ntest_event.columns = ['installation_id','game_session','uni_eventid']\n\n# merge with test_prep1\ntest_prep2 = pd.merge(test_prep1,test_event,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_event['uni_eventid'].plot(kind='hist',\n                               figsize=(6, 4), title='Unique values of event_id',\n                               color = color[2])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Total Game Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session to get the maxium value of game_time.\ntest_gametime = test.groupby(['installation_id','game_session'],as_index=False)['game_time'].max()\n\n# merge with test_prep2\ntest_prep3 = pd.merge(test_prep2,test_gametime,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_gametime['game_time'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log od total game time',\n                                                 color = color[3])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Total Event Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session to get the maxium value of event_count.\ntest_etcount = test.groupby(['installation_id','game_session'],as_index=False)['event_count'].max()\n\n# merge with test_prep3\ntest_prep4 = pd.merge(test_prep3,test_etcount,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_etcount['event_count'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log od total event count',\n                                                 color = color[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Assessment(dummy variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby installation_id and game_session\ntest_assess = test.groupby(['installation_id','game_session'],as_index=False)['type'].last()\n\n# create a new variable named as assessment. 'yes' represents it is a assessment type.\ntest_assess['assessment'] = ['yes' if index =='Assessment'  else 'no' for  index in test_assess['type']]\n\n# drop useless variable and then merge with test_prep4\ntest_assess = test_assess.drop('type',axis=1)\ntest_prep5 = pd.merge(test_prep4,test_assess,on=['installation_id','game_session'],how='outer')\n\n# make a graph to see the relationship between 'assessment type' and total attempts\nat_plot = test_prep5.groupby('assessment',as_index=False)['attempts'].sum()\nnames_at = list(at_plot['assessment'].unique())\nvalues_at = list(at_plot['attempts'])\n\nplt.bar(names_at, values_at)\nplt.title('total attempts in assessment type')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Unique event_code"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count values\ntest_etcode = test.groupby(['installation_id','game_session'])['event_code'].nunique().reset_index()\n\n# merge data\ntest_prep6 = pd.merge(test_prep5,test_etcode,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_etcode['event_code'].apply(np.log1p).plot(kind='hist',\n                                                 figsize=(6, 4), title='Log of total event code',\n                                                 color = color[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_title = test.copy() # make a copy of test data set.\n\n# write a loop to create five new variables\ntitles = ['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter']\nfor each_var in titles:\n    test_title[each_var] = 0  # initializing \n    test_title[each_var] = [1 if (each_var) in index else 0 for index in test_title['title']]\n    \n# groupby installation_id and game_session\ntest_five_title = test_title.groupby(['installation_id','game_session'],as_index=False)\\\n['Bird Measurer', 'Cart Balancer', 'Cauldron Filler', 'Chest Sorter', 'Mushroom Sorter'].last()\n\n# merge with test_prep6\ntest_prep7 = pd.merge(test_prep6,test_five_title,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_title_names = list(titles)\ntest_title_values = []\nfor var in test_title_names:\n    test_title_values.append(len(test_five_title[test_five_title[var] != 0]))\n\nplt.bar(test_title_names, test_title_values)\nplt.title('Freq of Each Assessment')\nplt.xticks(rotation=45)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9. Similar to (8). this time I care about the which world it is in the same game session. There are four different world, including  'NONE' (at the app's start screen), 'TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement) and 'CRYSTALCAVES' (Weight)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# based on 8, we know that one game_session only has one unique world, therefore we omit the checking procedure for simplicity.\n\ntest_world = test.copy() # make a copy of test data set.\n# write a loop to create four new variables\nworld = ['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES']\nfor each_wor in world:\n    test_world[each_wor] = 0  # initializing \n    test_world[each_wor] = [1 if (each_wor) in index else 0 for index in test_world['world']]\n    \n# groupby installation_id and game_session\ntest_four_world = test_world.groupby(['installation_id','game_session'],as_index=False)\\\n['NONE', 'TREETOPCITY','MAGMAPEAK','CRYSTALCAVES'].last()\n\n# merge with test_prep7\ntest_prep8 = pd.merge(test_prep7,test_four_world,on=['installation_id','game_session'],how='outer')\n\n# plot\ntest_world_names = list(world)\ntest_world_values = []\nfor wor in test_world_names:\n    test_world_values.append(len(test_four_world[test_four_world[wor] != 0]))\n\nplt.bar(test_world_names, test_world_values)\nplt.title('Freq of Each World')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Model"},{"metadata":{},"cell_type":"markdown","source":"We merge the train_prep7 data with train_labels data. Based on my understanding, this question is actually a both regression and classification problem. In this data set, the num_correct only has two values which are 0 and 1, therefore  predicting num_correct is a classification problem. But the num_incorrect has multiple values, so predicting num_incorrect is a regression problem. Based on train_laebls dataset. We know that accuracy = num_correct/(num_correct + num_incorrect). Based on the accuracy, we also know the accuracy group:\n>  accuracy_group = 3 if accuracy = 1\n\n>  accuracy_group = 2 if 0.5 <= accuracy < 1\n\n>  accuracy_group = 1 if 0 < accuracy < 0.5\n\n>  accuracy_group = 0 if accuracy = 0\n \nTherefore, according to the rule, we only need to predict the num_correct and num_incorrect."},{"metadata":{},"cell_type":"markdown","source":"1. Drop useless variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge train data with train label\ntrain_final = pd.merge(train_prep8,train_labels,on=['installation_id','game_session'],how='left')\n\n# fill NaN values with 0\ntrain_final.fillna(0,inplace=True)\n\n# drop some varibales, including installation_id, game_session,title, accuracy and accuracy_group.\ntrainset = train_final.drop(['title','accuracy','accuracy_group'],axis=1)\n\n# change 'freetime' variable as category variable\ntrainset['freetime'] = trainset['freetime'].astype('category').cat.codes\n\n# change 'assessment' variable as category variable\ntrainset['assessment'] = trainset['assessment'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Split train and test in trainset, Define X and y_num, y_bin. X is predictors, y_num is num_incorrect, y_bin is num_correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset_train,trainset_test = train_test_split(trainset,test_size=0.2,random_state=42) # split data\n\ntrainset_train_X = trainset_train[['attempts','freetime','uni_eventid','game_time','event_count','assessment',\\\n              'event_code','Bird Measurer','Cart Balancer','Cauldron Filler','Chest Sorter','Mushroom Sorter',\n                    'NONE','TREETOPCITY','CRYSTALCAVES','MAGMAPEAK']].values.astype('float32')  \ntrainset_train_ybin = trainset_train[['num_correct']].values.astype('float32')\ntrainset_train_ynum = trainset_train[['num_incorrect']].values.astype('float32')\n\ntrainset_test_X = trainset_test[['attempts','freetime','uni_eventid','game_time','event_count','assessment',\\\n              'event_code','Bird Measurer','Cart Balancer','Cauldron Filler','Chest Sorter','Mushroom Sorter',\n                    'NONE','TREETOPCITY','CRYSTALCAVES','MAGMAPEAK']].values.astype('float32')  \ntrainset_test_ybin = trainset_test[['num_correct']].values.astype('float32')\ntrainset_test_ynum = trainset_test[['num_incorrect']].values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Train Model using trainset_train data"},{"metadata":{},"cell_type":"markdown","source":"> In order to make a good prediction, I will use netural network, which  are very flexible, and allow you to define one model that does multiple things. For example, you can have one model that predicts both a classification target and a regression target. In this case, I just need to predict both a classfication and regression labels. Therefore, I will use keras neural network to make a good prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"one_input = Input(shape=(16,), name='one_input') # pass by one input\n\n# show one output: y_bin\ny_bin_output = Dense(1, activation='sigmoid', name='y_bin_output')(one_input)\n# merge one output with all predictors from input\nx = keras.layers.concatenate([one_input, y_bin_output]) \n# stack all other layers\nx = Dense(64, activation='relu')(x)\nx = BatchNormalization()(x)\n#another output\ny_num_output = Dense(1, activation='sigmoid', name='y_num_output')(x)\n\n\nmodel = Model(inputs=one_input, outputs=[y_bin_output, y_num_output])\nmodel.compile(optimizer='Adam', loss=['binary_crossentropy', 'mean_squared_error'])\n\nmodel.fit(trainset_train_X, [trainset_train_ybin, trainset_train_ynum],epochs=30,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. apply to trainaet_test to get the error rate and mean square error"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict using trainset_test_X\ntrainset_pred = model.predict(trainset_test_X)\n\n# transform the data type\ntrainset_pred_bin = trainset_pred[0].astype('int')\ntrainset_pred_num = np.around(trainset_pred[1])\ntrainset_conf_mx = confusion_matrix(trainset_test_ybin,trainset_pred_bin)\n\n# based on the confusion matrix, we could calculate the error rate.\ntrain_pred_er = (trainset_conf_mx[0,1] + trainset_conf_mx[1,0])/len(trainset_pred_bin)\n\n# calculate the mse\ntrainset_mse = mean_squared_error(trainset_test_ynum,trainset_pred_num)\n\nprint('The error rate is: ',train_pred_er)\nprint('The mean square error is: ',trainset_mse )\n\n# It is clearly to see that the model is pretty good","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Apply to Testset."},{"metadata":{},"cell_type":"markdown","source":"1. Data Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"testset = test_prep8.copy()\n# change 'freetime' variable as category variable\ntestset['freetime'] =testset['freetime'].astype('category').cat.codes\n\n# change 'assessment' variable as category variable\ntestset['assessment'] = testset['assessment'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Write a loop to predict the testset one row by one row."},{"metadata":{"trusted":true},"cell_type":"code","source":"testset['num_correct'] = 0\ntestset['num_incorrect'] = 0\n\nfor i in range(len(testset)):\n    value = testset.iloc[i:i+1,2:18].values\n    pred_y = model.predict(value)\n    testset['num_correct'][i] = pred_y[0].astype('int')\n    testset['num_incorrect'][i] = np.around(pred_y[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. calculate the accuracy and accuracy_group"},{"metadata":{"trusted":true},"cell_type":"code","source":"testset['accuracy'] = testset['num_correct']/(testset['num_correct'] + testset['num_incorrect'])\n\n# fill nan\ntestset.fillna(0,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy_group\ntestset['accuracy_group'] = 0\nfor m in range(len(testset)):\n    if testset['accuracy'][m] == 1:\n        testset['accuracy_group'][m] =3\n    elif 0.5 <= testset['accuracy'][m] < 1:\n        testset['accuracy_group'][m] =2\n    elif 0 < testset['accuracy'][m] < 0.5:\n        testset['accuracy_group'][m] =1\n    elif testset['accuracy'][m] == 0:\n        testset['accuracy_group'][m] =0\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred_1 = testset[(testset['Bird Measurer'] !=0) | (testset['Cart Balancer'] !=0)| (testset['Cauldron Filler'] !=0)\\\n                    | (testset['Chest Sorter'] !=0)| (testset['Mushroom Sorter'] !=0)]\n\nfinal_pred = final_pred_1.groupby('installation_id',as_index=False)['accuracy_group'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = final_pred.round(0)\nfinal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save as csv\nfinal.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}