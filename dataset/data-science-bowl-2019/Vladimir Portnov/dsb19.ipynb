{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science Bowl 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data exploration\n### Lets take a look at our data first. It consists of:\n* <span style=\"background-color:lightgray\">train.csv, test.csv</span> - main data files, which contain the gameplay events.\n* <span style=\"background-color:lightgray\">specs.csv</span> - this file gives the specification of the various event types.\n* <span style=\"background-color:lightgray\">train_labels.csv</span> - this file demonstrates how to compute the ground truth for the assessments in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# random sample\n#filename = \"../input/data-science-bowl-2019/train.csv\"\n#n = sum(1 for line in open(filename)) - 1\n#s = 1000000 #desired sample size\n\n#skip = sorted(random.sample(range(1,n+1),n-s))\n\n#train_data = pd.read_csv(filename)\ntrain_data = pd.read_csv(\"../input/data-science-bowl-2019/train.csv\")\nspecs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\ntest_data = pd.read_csv(\"../input/data-science-bowl-2019/test.csv\")\ntrain_labels = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_labels = train_labels.drop_duplicates(subset=\"installation_id\", keep=\"last\")\n\ntmp_labels.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_groups = tmp_labels[\"accuracy_group\"]\nacc_groups.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n#### We can start by getting rid of users that didnt take assessments, as we cant use them for training."},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"users = train_data['installation_id'].drop_duplicates()\nprint('unique users: {}'.format(users.size))\nattempted_users = train_data[train_data['type']=='Assessment'][['installation_id']].drop_duplicates() \nprint('users, who attempted assessments: {}'.format(attempted_users.size))\ntrain_data = pd.merge(train_data, attempted_users, on=\"installation_id\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n### Now lets visualize some data.\n#### First of all, some info about events might be usefull"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\ntype_count = train_data.groupby('type').count()\nfor t in train_data['type'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.type == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\nfor t in train_data['title'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.title == t]))\n\nfig = plt.figure(figsize=(13, 15))\nplt.barh(names, values)\nplt.title('Number of events by title')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Each event belongs to section, we need to learn about those. Most importanlty, how assessments are divided"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.world.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MAGMAPEAK - {}\\n'.format(pd.unique(train_data[(train_data.world == 'MAGMAPEAK') & (train_data.type == 'Assessment')].title)))\nprint('CRYSTALCAVES - {}\\n'.format(pd.unique(train_data[(train_data.world == 'CRYSTALCAVES') & (train_data.type == 'Assessment')].title)))\nprint('TREETOPCITY - {}\\n'.format(pd.unique(train_data[(train_data.world == 'TREETOPCITY') & (train_data.type == 'Assessment')].title)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"names = []\nvalues = []\ntype_count = train_data.groupby('world').count()\nfor t in train_data['world'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.world == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by world')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So, magmapeak have only one assessment, but bigger number of events? Interesting. We will adress this later. <br>\n#### We know that assessments results are captured with event code 4100 and 4110 for Bird Measurer. Lets check "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data[train_data.event_code == 4100].title.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are unnecessary stuff, seems like event type must be taken into account."},{"metadata":{},"cell_type":"markdown","source":"#### After that we may find something in connection between events and time of their accurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['timestamp'] = pd.to_datetime(train_data['timestamp'])\ntrain_data['weekday'] = train_data['timestamp'].dt.dayofweek\ntrain_data['hour'] = train_data['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8))\nnames = ['Mon', 'Tue', 'Wd', 'Thu', 'Fri', 'Sat', 'Sun']\nvalues = []\nfor d in range(7):\n    values.append(len(train_data[train_data.weekday == d]))\nplt.bar(names, values)\nplt.title('Event count by weekday')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14, 9))\nnames = range(24)\nvalues = []\nfor h in range(24):\n    values.append(len(train_data[train_data.hour == h]))\nplt.bar(names, values, width=0.5)\nplt.title('Event count by hour')\nplt.xticks(range(24))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig = plt.figure(figsize=())\ntime_by_session = train_data[['game_session', 'world', 'game_time']].groupby(['game_session', 'world']).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_playtime():\n    result = list()\n    for u in attempted_users['installation_id']:\n        time_by_world = {'MAGMAPEAK':0,'TREETOPCITY':0,'CRYSTALCAVES':0,'NONE':0}\n        sessions_by_user = train_data[train_data.installation_id == u]['game_session'].drop_duplicates()\n        for s in sessions_by_user:\n            tmp = time_by_session.loc[s]['game_time'].iloc[0]\n            time_by_world[time_by_session.loc[s].index.tolist()[0]] += tmp\n        result.append(time_by_world)\n    return result\n\nplaytime = calc_playtime()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,8))\nplt.plot([u['MAGMAPEAK'] + u['TREETOPCITY'] + u['CRYSTALCAVES'] for u in playtime])\nplt.title('Users playtime')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like day of week doesnt matter, but time of day and total playtime really differ from user to user. What about time spent per world?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5))\nval = [0, 0, 0]\nfor u in playtime:\n    val[0] += u['MAGMAPEAK']\n    val[1] += u['TREETOPCITY']\n    val[2] += u['CRYSTALCAVES']\nplt.bar(['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES'], val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Its actually roughly the same as event count. That means we can trear all actions equally, as they take almost the same time, which is helpfull. Also, time spent on magmapeak is not that impactfull as on two other worlds. <br>\n### Next lets look at train_labels as it contains results for the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head(9)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"train_labels[['installation_id', 'accuracy_group']].groupby(['accuracy_group']).count().plot.bar(figsize=(10, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Half of users solve correctly on first try, okay. Now what about distribution between assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = pd.unique(train_labels.title)\nmean_wrong = []\nfor t in tasks:\n    mean_wrong.append(train_labels[train_labels.title == t].num_incorrect.mean())\nfig = plt.figure(figsize=(7, 7))\nplt.pie(mean_wrong, labels=tasks)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Seems like two of them are particulary hard. Need to keep close eye on them."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"### First we need to precalculate labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_labels(data):\n    temp_data = data.query('event_code == [4100,4110] and type == \"Assessment\"').copy()\n    sessions = temp_data[temp_data.type == 'Assessment'][['game_session']].drop_duplicates()\n    labels = ['game_session', 'installation_id', 'title', 'num_correct', 'num_incorrect', 'accuracy', 'accuracy_group']\n    result = pd.DataFrame(columns=labels)    \n    for s in sessions['game_session']:\n        tmp = pd.DataFrame(columns=labels)\n        events_by_session = temp_data[temp_data.game_session == s]['event_data']\n        num_correct = 0\n        num_incorrect = 0\n        for e in events_by_session:\n            if json.loads(e)['correct']:\n                num_correct += 1\n            else:\n                num_incorrect += 1\n        if num_correct < 1:\n            accuracy = 0.0\n        else:\n            accuracy = num_correct / (num_correct + num_incorrect)\n        if num_incorrect == 0 and num_correct > 0:\n            accuracy_group = 3\n        elif num_incorrect == 1 and num_correct > 0:\n            accuracy_group = 2\n        elif num_incorrect >= 2 and num_correct > 0:\n            accuracy_group = 1\n        else:\n            accuracy_group = 0            \n        tmp['game_session'] = pd.Series(s)\n        tmp['installation_id'] = temp_data.loc[temp_data.game_session == s].iloc[0]['installation_id']\n        tmp['title'] = temp_data.loc[temp_data.game_session == s].iloc[0]['title']\n        tmp['num_correct'] = num_correct\n        tmp['num_incorrect'] = num_incorrect\n        tmp['accuracy'] = accuracy\n        tmp['accuracy_group'] = accuracy_group        \n        result = result.append(tmp, ignore_index=True)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = calc_labels(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = calc_labels(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now when we have some data to work with we can assemble it for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(data, data_labels):\n    global attempted_users, playtime\n    labels = ['id', 'activities', 'games', 'clips', 'assessments', 'mean_activity_daytime', 'mean_game_daytime', 'mean_clip_daytime', 'mean_assessment_daytime', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'accuracy']\n    result = pd.DataFrame(columns=labels)\n    \n    for i, u in enumerate(attempted_users.installation_id):\n        tmp = pd.DataFrame(columns=labels)\n        cur_user = data[data.installation_id == u]\n        \n        tmp['id'] = pd.Series(u)\n        sub = cur_user[cur_user.type == 'Activity']\n        tmp['activities'] = pd.Series(len(sub))\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        if len(m) > 0:\n            tmp['mean_activity_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_activity_daytime'] = pd.Series(0.)\n        \n        sub = cur_user[cur_user.type == 'Game']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['games'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_game_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_game_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Clip']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['clips'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_clip_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_clip_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Assessment']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['assessments'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_assessment_daytime'] = pd.Series(m[0][0]) \n        else:\n            tmp['mean_assessment_daytime'] = pd.Series(0.)\n        \n        tmp['MAGMAPEAK'] = pd.Series(playtime[i]['MAGMAPEAK'])\n        tmp['TREETOPCITY'] = pd.Series(playtime[i]['TREETOPCITY'])\n        tmp['CRYSTALCAVES'] = pd.Series(playtime[i]['CRYSTALCAVES'])\n        \n        acc_mode = data_labels[data_labels.installation_id == u]['accuracy_group'].mode()\n        if acc_mode.dropna().empty:\n            tmp['accuracy'] = pd.Series(0)\n        else:\n            tmp['accuracy'] = pd.Series(acc_mode.max())\n            \n        result = result.append(tmp, ignore_index=True)        \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = create_features(train_data, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Slightly modified data processing for test data, which includes operations run at analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['timestamp'] = pd.to_datetime(test_data['timestamp'])\ntest_data['weekday'] = test_data['timestamp'].dt.dayofweek\ntest_data['hour'] = test_data['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(data, data_labels):\n    all_users = data[data['type']=='Assessment'][['installation_id']].drop_duplicates()\n    time_by_session = data[['game_session', 'world', 'game_time']].groupby(['game_session', 'world']).max()\n    ext_playtime = []\n    for u in all_users['installation_id']:\n        time_by_world = {'MAGMAPEAK':0,'TREETOPCITY':0,'CRYSTALCAVES':0,'NONE':0}\n        sessions_by_user = data[data.installation_id == u]['game_session'].drop_duplicates()\n        for s in sessions_by_user:\n            tmp = time_by_session.loc[s]['game_time'].iloc[0]\n            time_by_world[time_by_session.loc[s].index.tolist()[0]] += tmp\n        ext_playtime.append(time_by_world)\n    labels = ['id', 'activities', 'games', 'clips', 'assessments', 'mean_activity_daytime', 'mean_game_daytime', 'mean_clip_daytime', 'mean_assessment_daytime', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'accuracy']\n    result = pd.DataFrame(columns=labels)\n    \n    for i, u in enumerate(all_users.installation_id):\n        tmp = pd.DataFrame(columns=labels)\n        cur_user = data[data.installation_id == u]\n        \n        tmp['id'] = pd.Series(u)\n        sub = cur_user[cur_user.type == 'Activity']\n        tmp['activities'] = pd.Series(len(sub))\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        if len(m) > 0:\n            tmp['mean_activity_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_activity_daytime'] = pd.Series(0.)\n        \n        sub = cur_user[cur_user.type == 'Game']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['games'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_game_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_game_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Clip']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['clips'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_clip_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_clip_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Assessment']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['assessments'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_assessment_daytime'] = pd.Series(m[0][0]) \n        else:\n            tmp['mean_assessment_daytime'] = pd.Series(0.)\n        \n        tmp['MAGMAPEAK'] = pd.Series(ext_playtime[i]['MAGMAPEAK'])\n        tmp['TREETOPCITY'] = pd.Series(ext_playtime[i]['TREETOPCITY'])\n        tmp['CRYSTALCAVES'] = pd.Series(ext_playtime[i]['CRYSTALCAVES'])\n        \n        acc_mode = data_labels[data_labels.installation_id == u]['accuracy_group'].mode()\n        if acc_mode.dropna().empty:\n            tmp['accuracy'] = pd.Series(0)\n        else:\n            tmp['accuracy'] = pd.Series(acc_mode.max())\n            \n        result = result.append(tmp, ignore_index=True)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = extract_features(test_data, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = features.loc[:, 'activities':'CRYSTALCAVES'].copy()\ny_train = features.loc[:, 'accuracy'].astype(int).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_features.loc[:, 'activities':'CRYSTALCAVES'].copy()\ny_test = test_features.loc[:, 'accuracy'].astype(int).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### At last, run the classifier itself"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GradientBoostingClassifier()\nclf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = clf.predict(x_test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analyze and output results"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_submission(data, result):\n    labels = ['installation_id', 'accuracy_group']\n    submission = pd.DataFrame(columns=labels)\n    submission.installation_id = data['id']\n    submission.accuracy_group = pd.Series(result)\n    submission.to_csv('submission.csv', index=False)\ngen_submission(test_features, res)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}