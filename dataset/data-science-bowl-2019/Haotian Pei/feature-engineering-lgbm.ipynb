{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nos.chdir('/kaggle/input/data-science-bowl-2019/')\nos.getcwd()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport random\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)\nimport collections\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('seaborn')\nsns.set(font_scale=1.8)\nimport random\nfrom sklearn import metrics\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom statistics import mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read datasets\nraw_train = pd.read_csv(\"train.csv\")\nspecs = pd.read_csv(\"specs.csv\")\nraw_labels = pd.read_csv(\"train_labels.csv\")\ntest = pd.read_csv(\"test.csv\")\nsubmission = pd.read_csv(\"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Dataset Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_labels.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon examining the datasets, we noted that the training dataset includes a lot of installation_ids that never took the assessments. Also, we noted that there are some records of Bird Measurers but are coded to event_code 4100, which is not included in the training labels. Therefore, the following steps are performed to align the training dataset to the labeling dataset, which is based on the below info provided by Kaggle:"},{"metadata":{},"cell_type":"markdown","source":"\"\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\""},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_count = raw_labels['num_correct'].sum()\nincorrect_count = raw_labels['num_incorrect'].sum()\nprint(f'According to the count of correct and incorrect attempts within the labels dataset, the number of assessment records within the training set should be {correct_count + incorrect_count}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train1 = raw_train[((raw_train['event_code']==4100) | (raw_train['event_code'] == 4110)) & (raw_train['type'] == 'Assessment')]\nraw_train2 = raw_train1[((raw_train1['event_code'] == 4100) & (raw_train1['title'] != 'Bird Measurer (Assessment)')) | (raw_train1['event_code'] == 4110)]\nraw_train2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are fairly comfortable that the two dataset should be good to join and align. Therefore we join the training dataset with the labeling dataset:\nWe noted that in the training dataset, the correct/incorrect result is reflected in the event_data column with \"true\" or \"false\" syntax. Therefore we implement an edit check to verify that the correct and incorrect attempts from the training set aligned to the calculation per the labeling dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_correct = raw_train2[raw_train2['event_data'].str.contains(\"true\")].shape[0]\ntrain_incorrect = raw_train2[raw_train2['event_data'].str.contains(\"false\")].shape[0]\nprint (f'The number of correct attempts in the training set is {train_correct}. The number of correct attempts in the labeling dataset is {correct_count}.')\nprint (f'The number of correct attempts in the training set is {train_incorrect}. The number of correct attempts in the labeling dataset is {incorrect_count}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above checks, we noted that we need to exclude the records that has 4100 event_code and bird assessment and remove the insallation ids that never took an assessment. Then we need to bring back all the other activities to the final training dataset. Then join the two tables (train & tarin_labels) to form the final training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_4 = raw_train[((raw_train['event_code'] == 4100) & (raw_train['title'] != 'Bird Measurer (Assessment)')) | (raw_train['event_code'] != 4100)]\ntrain = train_4[train_4.installation_id.isin(raw_labels.installation_id.unique())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"1. change the timestamp to date-time"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = raw_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Aggregation of data and features:\n\nBy logic, sufficient practice will increase the chance of successfully passing an assessment. Therefore the game play data before each assessment is crucial to the prediction. As such, we need to aggregate the training and testing datasets so that the added features represent the game play statistics before each assessment attempt. I used the code from Erik Bruin to perform the aggregation.\nhttps://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n\ncategoricals = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title] #from Andrew\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1] #from Andrew\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if test_set=True, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in train_set, all assessments are kept\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compiled_data = []\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_train = new_train[['Activity','Assessment','Clip','Game','installation_id','session_title','accumulated_correct_attempts','accumulated_uncorrect_attempts','duration_mean','accumulated_accuracy','accuracy_group','accumulated_accuracy_group','accumulated_actions']]\nreduced_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nreduce_test = pd.DataFrame(new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_test = reduce_test[['Activity','Assessment','Clip','Game','installation_id','session_title','accumulated_correct_attempts','accumulated_uncorrect_attempts','duration_mean','accumulated_accuracy','accuracy_group','accumulated_accuracy_group','accumulated_actions']]\nreduced_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a result of the feature engineering, we have the below features generated that we think will be useful to predict the accuracy group:\n\n1. Acitivity stands for the number of activities performed prior to this assessment attempt\n2. Assessment represents the number of assessment took prior to this assessment attempt\n3. Clip represents the number of games played before taking this assessment attempt\n4. Accumulated_correct_attempts represents the accumulated correct attempts for assessments prior to this assessment attempt\n5. Accumulated_uncorrect_attempts represents the accumulated incorrect attempts for assessments prior to this assessment attempt\n6. Duration_mean represents the time spent in this app so far\n7. Accumulated_actions represents the total of actions taken prior to this assessment attempt."},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"Model preparation: we split the training set into two sub-datasets: one is used for training and one is used for model validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf1 = reduced_train.head(16000)\ntrainf1_input = trainf1[['Activity','Assessment','Clip','Game','session_title','accumulated_correct_attempts','accumulated_uncorrect_attempts','duration_mean','accumulated_actions']]\ntrainf1_target = trainf1[['accuracy_group']]\ntrainf1_input.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf2 = reduced_train.tail(1690)\ntrainf2_input = trainf2[['Activity','Assessment','Clip','Game','session_title','accumulated_correct_attempts','accumulated_uncorrect_attempts','duration_mean','accumulated_actions']]\ntrainf2_target = trainf2[['accuracy_group']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input = reduced_test[['Activity','Assessment','Clip','Game','session_title','accumulated_correct_attempts','accumulated_uncorrect_attempts','duration_mean','accumulated_actions']]\ntest_target = reduced_test[['accuracy_group']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We tried the below machine learning models to perform the prediction:\n    1. Logistic Regression\n    2. Decision Tree\n    3. SVM\n    4. Random Forest\n    5. LGBM\n\nWe calculated the accuracy scores and generated the confusion matrix to visualize the model results."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nlr_c=LogisticRegression(random_state=0)\nlr_c.fit(trainf1_input,trainf1_target)\nlr_pred=lr_c.predict(trainf2_input)\nlr_cm=confusion_matrix(trainf2_target,lr_pred)\nlr_ac=accuracy_score(trainf2_target, lr_pred)\nprint('LogisticRegression_accuracy:',lr_ac)\nplt.figure(figsize=(10,5))\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(lr_cm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree\ndtree_c=DecisionTreeClassifier(criterion='entropy',random_state=0)\ndtree_c.fit(trainf1_input,trainf1_target)\ndtree_pred=dtree_c.predict(trainf2_input)\ndtree_cm=confusion_matrix(trainf2_target,dtree_pred)\ndtree_ac=accuracy_score(dtree_pred,trainf2_target)\nplt.figure(figsize=(10,5))\nplt.title(\"dtree_cm\")\nsns.heatmap(dtree_cm,annot=True,fmt=\"d\",cbar=False)\nprint('DecisionTree_Classifier_accuracy:',dtree_ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM Model\nsvc_r=SVC(kernel='rbf')\nsvc_r.fit(trainf1_input,trainf1_target)\nsvr_pred=svc_r.predict(trainf2_input)\nsvr_cm=confusion_matrix(trainf2_target,svr_pred)\nsvr_ac=accuracy_score(trainf2_target, svr_pred)\nplt.figure(figsize=(10,5))\nplt.title(\"svm_cm\")\nsns.heatmap(svr_cm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)\nprint('SVM_regressor_accuracy:',svr_ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest\nrdf_c=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nrdf_c.fit(trainf1_input,trainf1_target)\nrdf_pred=rdf_c.predict(trainf2_input)\nrdf_cm=confusion_matrix(trainf2_target,rdf_pred)\nrdf_ac=accuracy_score(rdf_pred,trainf2_target)\nplt.figure(figsize=(10,5))\nplt.title(\"rdf_cm\")\nsns.heatmap(rdf_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nprint('RandomForest_accuracy:',rdf_ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM\nlgbm_c=LGBMClassifier()\nkfold = KFold(n_splits=5, random_state=1989)\nlgbm_c.fit(trainf1_input,trainf1_target)\nlgbm_pred = lgbm_c.predict(trainf2_input)\nlgbm_cm=confusion_matrix(trainf2_target,lgbm_pred)\nlgbm_ac=accuracy_score(lgbm_pred,trainf2_target)\nplt.figure(figsize=(10,5))\nplt.title(\"lgbm_cm\")\nsns.heatmap(lgbm_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nprint('LGBM_accuracy:',lgbm_ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_accuracy = pd.Series(data=[lr_ac,dtree_ac,svr_ac,rdf_ac,lgbm_ac], \n        index=['Logistic_Regression','DecisionTree_Classifier','SVM_regressor_accuracy','RandomForest','LGBM'])\nfig= plt.figure(figsize=(9,9))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Accracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since LGBM appears to be the best model, we apply it to the test dataset and finalize the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_pred = lgbm_c.predict(test_input)\nsubmission['accuracy_group'] = lgbm_pred\nsubmission['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}