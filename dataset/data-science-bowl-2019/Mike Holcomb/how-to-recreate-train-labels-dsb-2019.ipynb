{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DSB Assessment Grades"},{"metadata":{},"cell_type":"markdown","source":"## Objective: Reconcile train and train_labels"},{"metadata":{},"cell_type":"markdown","source":"In trying to understand the requirements of the competition, I created a kernel to reconcile the event data in `train.csv` with the assessment accuracy groups in `train_labels.csv`.  In doing so, I have also extracted a corresponding `test_labels.csv` for the graded assessments found in the `test` split.  These data could be potentially used to augment the training set."},{"metadata":{},"cell_type":"markdown","source":"## Background"},{"metadata":{},"cell_type":"markdown","source":"As stated in the data description, there are four accuracy groups:\n> 3: the assessment was solved on the first attempt\n>\n> 2: the assessment was solved on the second attempt\n>\n> 1: the assessment was solved after 3 or more attempts\n>\n> 0: the assessment was never solved\n\nIn the event data of `train.csv`, we are told  that the number of attempts correspond to the 4100 event code for four of the assessment types and 4110 for one of the event types.  Within the `event_data` field, there is a flag for whether the attempt was correct or not."},{"metadata":{},"cell_type":"markdown","source":"## Procedure"},{"metadata":{},"cell_type":"markdown","source":"As such, we should be able to recreate the `train_labels.csv` file by:\n1. Extracting all of the assessment scoring events (4100 or 4110 depending on type).\n2. Counting the number of correct and incorrect attempts by `installation_id` and `game_session`\n3. Applying a heuristic on the attempt counts to create the accuracy group."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndef extract_accuracy_group(df: pd.DataFrame) -> pd.DataFrame:\n    # Regex strings for matching Assessment Types\n    assessment_4100 = '|'.join(['Mushroom Sorter',\n                                'Chest Sorter',\n                                'Cauldron Filler',\n                                'Cart Balancer'])\n    assessment_4110 = 'Bird Measurer'\n    \n    # 1. Extract all assessment scoring events\n    score_events = df[((df['title'].str.contains(assessment_4110)) & (df['event_code']==4110)) |\\\n                      ((df['title'].str.contains(assessment_4100)) & (df['event_code']==4100))]\n    \n    # 2. Count number of correct vs. attempts\n    # 2.a. Create flags for correct vs incorrect\n    score_events['correct'] = 1\n    score_events['correct'] = score_events['correct'].where(score_events['event_data'].str.contains('\"correct\":true'),other=0)\n    \n    score_events['incorrect'] = 1\n    score_events['incorrect'] = score_events['incorrect'].where(score_events['event_data'].str.contains('\"correct\":false'),other=0)\n    \n    # 2.b. Aggregate by `installation_id`,`game_session`,`title`\n    score_events_sum = score_events.groupby(['installation_id','game_session','title'])['correct','incorrect'].sum()\n    \n    # 3. Apply heuristic to convert counts into accuracy group\n    # 3.a. Define heuristic\n    def acc_group(row: pd.Series) -> int:\n        if row['correct'] == 0:\n            return 0\n        elif row['incorrect'] == 0:\n            return 3\n        elif row['incorrect'] == 1:\n            return 2\n        else:\n            return 1\n        \n    # 3.b. Apply heuristic to count data\n    score_events_sum['accuracy_group'] = score_events_sum.apply(acc_group,axis=1)\n    \n    return score_events_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nDATA_DIR = '/kaggle/input/data-science-bowl-2019'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Reconciliation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read `train.csv`\ntrain = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Run reconciliation\ntrain_labels_extracted = extract_accuracy_group(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sample first rows"},{"metadata":{},"cell_type":"markdown","source":"First, we can eyeball the top several rows to see if we are in the ballpark."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_extracted.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read `train_labels.csv`\ntrain_labels = pd.read_csv(os.path.join(DATA_DIR,'train_labels.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.drop(['accuracy'], axis=1).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare Counts"},{"metadata":{},"cell_type":"markdown","source":"The distribution of accuracy groups appears to match between the two."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_extracted['accuracy_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['accuracy_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ensure matching `game_session`s"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flatten multi-index\ntrain_labels_extracted.reset_index(inplace=True)\ntrain_labels_extracted.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_train_sessions = set(train_labels_extracted['game_session'])\ntrain_sessions = set(train_labels['game_session'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_train_sessions.symmetric_difference(train_sessions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the symmetric difference of the sets of the session ids is empty, then the session ids must match exactly between the two sets."},{"metadata":{},"cell_type":"markdown","source":"#### Check `accuracy_group` column"},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_train_groups = list(train_labels_extracted['accuracy_group'])\ntrain_groups = list(train_labels['accuracy_group'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_match = True\nfor extract, gold in zip(extracted_train_groups, train_groups):\n    if extract != gold:\n        all_match = False\n        break\n\nif(all_match):\n    print(f\"All {len(extracted_train_groups)} groups match\")\nelse:\n    print(f\"Found at least one mismatched group\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting labels from test set"},{"metadata":{},"cell_type":"markdown","source":"Since all of our tests pass, we should feel comfortable that our function is able to extract the accuracy score labels from the event data.  As such, this method should be able to extract scored assessments from the event histories in the test set as well.  By subsampling the event histories, we may be able to create more \"training\" examples that potentially correlate better with the user behavior profiles of the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_labels = extract_accuracy_group(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels['accuracy_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have roughly 2,000 more labeled assessments to incorporate in our training procedures in `test_labels.csv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels.to_csv('test_labels.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}