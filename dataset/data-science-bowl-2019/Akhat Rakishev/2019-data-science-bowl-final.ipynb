{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About"},{"metadata":{},"cell_type":"markdown","source":"This is the kernel that was used in Data Science Bowl 2019 competition. The kernel scored in top 2% of participats, on 35th place out of 3497.\n\nThe [competition's](https://www.kaggle.com/c/data-science-bowl-2019) goal was to predict accuracy group of children in assessments taken in game-like iOS app. The app is designed for children of ages 3-5 to learn basic mathematical concepts (e.g. sorting objects). The app offers children a series of acivities that they may undertake in a free order as they travel across virtual island. THe activities consist of various clips, activities, games and assessments. Children participate in all these activites while app generates log of all the actions that have occured within each activity. As children use the app, they eventually pass one or more assessments, which takes them one, two, three or more attempts to successfuly pass (if they pass at all). Based on attempts taken to pass an assessment, children are categorised as belonging to one of four accuracy groups. The goal is then to use the event log to predict accuracy group of a child in his/her last assessment at a given time period.\n\n**Key ideas:**\n\n**Pre-processing**\n* A lot of feature engineering, more than 1000 features generated\n* Using data points from test set in addition to train set to train models\n* Normalisation of features in train and test using cubic root\n* Exclusion of highly correlated features using full test as opposed to truncated version used in for actual scoring (see pre-processing section for details)\n\n**Modelling**\n* Increased complexity models - more parameters in neural networks and deeper decision trees\n\n**Post-processing**\n* No Nelder-Mead optimisation. Instead, rounding based on both distribution of response variable in train set and actual model predictions (see post-processing section for details).\n\n\n**Acknowledgements**\nThis kernel is a long evolved version Bruno Aquino's public kernel"},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing and modelling config"},{"metadata":{},"cell_type":"markdown","source":"MIX_UP - I have experimented with mixing up samples and their labels in different propoertions to obtain new pseudo training samples. This did not work in short time I have experimented with it, so I have turned it off.\n\nUSE_FORGETTING_CURVE_DISCOUNT - there is a timestampt of each event that is recorded when child interacts with the app. I have used that timestamp to measure how much time have passed between different events. Then I have used that time to discount older data points where a lot of time has passed between relevant events. This idea is taken from a number of psychological studies that have shown that people tend to forget information quickly if they are not using it. The recall rate compared against time passed since initial learning was best described by exponential curve.\n\nMEMORY_STABILITY - this is parameter that is used to calculate exponential cureve mentioned above. The larger this parameter is - the better is the hypothetical retention in memory of a child. The code its current version required re-run each time this parameter is changed and such re-run took a long time (~3-4 hours) so I have not tried to iteratively estimate this parameter but instead hand-picked it after some trial-and-error.\n\nUSE_DIFFERENT_BOUNDS - as I describe in the last sectino of this kernel, I have used distributino of response variable in training set to round my models' predictions (response variable had to be an interger in 0-3 range). This parameter have allowed me to easily experiement with modified distributions like distribution of response variable in training set, but only certain samples.\n\nRETURN_PREDS_LO - we have to predict accuracy group on a last assessment that a child understakes. Consequently assessments can be categorised as last and not last. Last assessments where always different as children tended to accumulate more experience than when they had undergone all non-last assessments. Consequenty I have thought it may make sense to only use last assessments in a sequence of assessments for training my models. This however resulted very small training sizes and so did not work well. Retrospectively I have also though that any assessment can be last in sequence, so it should not matter conceptually.\n\nPOWER - this is power used to normalise features, many of which where highly skewed. Cubic root seemed to work best after some trial-and-error."},{"metadata":{"trusted":true},"cell_type":"code","source":"MIX_UP = False\n\nUSE_FORGETTING_CURVE_DISCOUNT = True\nMEMORY_STABILITY = 100000\n\nUSE_DIFFERENT_BOUNDS = False\nRETURN_PREDS_LO = False\n\nPOWER = 1/3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import libraries & set configurations"},{"metadata":{},"cell_type":"markdown","source":"### Imports and library configs"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error, confusion_matrix\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n\nimport os\nfrom time import time\nfrom collections import Counter\nfrom scipy import stats\nimport gc\nimport json\nfrom functools import partial\nfrom scipy import stats\nimport pickle\nfrom tqdm import tqdm\nimport itertools\nimport matplotlib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nCOLOR = 'black' # used 'white' when editing in interactive mode with dark theme ON\nmatplotlib.rcParams['text.color'] = COLOR\nmatplotlib.rcParams['axes.labelcolor'] = COLOR\nmatplotlib.rcParams['xtick.color'] = COLOR\nmatplotlib.rcParams['ytick.color'] = COLOR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data pre-processing and modelling support functions"},{"metadata":{},"cell_type":"markdown","source":"### Support functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred, is_last=False):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    if MIX_UP:\n        idx_int = y_true % 1 == 0\n        y_true = y_true[idx_int]\n        y_pred = y_pred[idx_int]\n    \n    dist_source = reduce_train.loc[reduce_train.is_last==1, 'accuracy_group'].reset_index(drop=True) if is_last else reduce_train['accuracy_group']\n    dist = Counter(dist_source)\n    for k in dist:\n        dist[k] /= len(dist_source)\n    dist_source.hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cohenkappa(ypred, y):\n    y = y.get_label().astype(\"int\")\n    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n    return \"cappa\", loss, True\n\ndef get_event_data(x, key, prefix):\n    x = json.loads(x)\n    try: res = f'{prefix}_{x[key]}'\n    except: res = f'{prefix}_None'\n    return res\n\ndef get_correct(x):\n    x = json.loads(x)\n    try: res = 1 if x['correct'] else 0\n    except: res = -1\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    \n    media_durations = pd.read_csv('/kaggle/input/basedon460/media_sequence.csv')\n    return train, test, train_labels, specs, sample_submission, media_durations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_categoricals(train, test, train_labels):\n    # concat title and event_code\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    \n    # unqiue lists\n    unique_title_event_codes = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    unique_titles = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    unique_event_codes = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    unique_event_ids = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    unique_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    unique_assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    unique_game_titles = list(set(train[train['type'] == 'Game']['title'].value_counts().index).union(set(test[test['type'] == 'Game']['title'].value_counts().index)))\n    \n    # maps\n    titles_map = dict(zip(unique_titles, np.arange(len(unique_titles))))\n    titles_inverse_map = dict(zip(np.arange(len(unique_titles)), unique_titles))\n    worlds_map = dict(zip(unique_worlds, np.arange(len(unique_worlds))))\n    \n    # apply maps\n    train['title'] = train['title'].map(titles_map)\n    test['title'] = test['title'].map(titles_map)\n    train['world'] = train['world'].map(worlds_map)\n    test['world'] = test['world'].map(worlds_map)\n    \n    train_labels['title'] = train_labels['title'].map(titles_map)\n    \n    # do those win_codes - kind of maps, not sure why it is done this way yet\n    win_code = dict(zip(titles_map.values(), (4100*np.ones(len(titles_map))).astype('int')))\n    win_code[titles_map['Bird Measurer (Assessment)']] = 4110\n    \n    # format dates\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    train['game_cor'] = train.loc[:,'event_data'].apply(get_correct)\n    test['game_cor'] = test.loc[:,'event_data'].apply(get_correct)\n    \n    return train, test, train_labels, win_code, unique_titles, unique_event_codes, titles_inverse_map, unique_assess_titles, unique_event_ids, unique_title_event_codes, unique_game_titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this piece counts how many actions was made in each event_code so far\ndef update_counters(counter: dict, col: str, session_group, disc_rate, cur_rate):\n    num_of_session_count = Counter(session_group[col])\n    for k in num_of_session_count.keys():\n        x = k\n        if col == 'title':\n            x = titles_inverse_map[k]\n        counter[x] = counter[x] + num_of_session_count[k]\n    return counter\n\ndef discount(store_dict, disc_rate):\n    for key in store_dict:\n        store_dict[key] = disc_rate*store_dict[key]\n    return store_dict\n\ndef get_dr(t):\n    DR = np.exp(-(t/MEMORY_STABILITY)) if USE_FORGETTING_CURVE_DISCOUNT else 1\n    return DR\n\ndef get_basic_stats(arr, prefix):\n    result = {}\n    result[f'{prefix}_mean'] = np.mean(arr) if len(arr) != 0 else 0\n    result[f'{prefix}_med'] = np.median(arr) if len(arr) != 0 else 0\n    result[f'{prefix}_mode'] = stats.mode(arr)[0][0] if len(arr) != 0 else 0\n    result[f'{prefix}_sd'] = np.std(arr) if len(arr) != 0 else 0\n    result[f'{prefix}_sum'] = np.sum(arr) if len(arr) != 0 else 0\n    result[f'{prefix}_max'] = np.max(arr) if len(arr) != 0 else 0\n    result[f'{prefix}_min'] = np.min(arr) if len(arr) != 0 else 0\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aei = {}\naemt = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example of exponential discounting based on time-off between assessment at t and t-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = 14\nt = np.arange(0,(60*60*24*days+1))\nplt.plot(t,np.exp(-(t/400000)))\nplt.plot(t,np.exp(-(t/200000)))\nplt.plot(t,np.exp(-(t/100000)))\nplt.xticks(np.arange(0,(60*60*24*days+1),60*60*24), labels = np.arange(0,days+1))\nplt.legend('Legend', facecolor='Black', labels=['4k','2k','1k'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Key feature engineering function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, disc_rate, cur_rate, is_test=False):\n    \"\"\" \n    Extracts prediction items and featrues from event series of a single user\n  \n    This function contains most of feature engineering code for this kernel. It has originated as a smaller functions however grew bigger overtime. \n  \n    Parameters: \n    user_sample (pd.DataFrame): pandas dataframe containing events of a single user\n    disc_rate (float): deprecated, was used for constant rate discounting of older data points aka momentum\n    cur_rate (float): deprecated, used for discounting present features\n    is_test: used to indicate that dataset is test in oder to process it slightly differently. This is because test dataset contains no events other than the first one in the last assessment, to prevent data leaks.\n  \n    Returns: \n    list: list of dictionaries containing user events groupped by assessment with features engineered\n  \n    \"\"\"\n    all_assessments = []\n    durations = []\n    durations_all = []\n    durations_preass = [] \n    \n    last_session_time_sec = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_incorrect_attempts = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    user_activities_time = {'Clip_time':0, 'Activity_time': 0, 'Assessment_time': 0, 'Game_time':0}\n    accumulated_actions = 0 \n    last_activity = 0\n    counter = 0\n    \n    last_accuracy_title = {f'acc_{i}': -1 for i in unique_assess_titles}\n    time_spent_each_title = {f'{i}_time': 0 for i in unique_titles}\n        \n    event_code_count = {i: 0 for i in unique_event_codes}\n    event_id_count = {i: 0 for i in unique_event_ids}\n    title_count = {i: 0 for i in unique_titles} \n    title_event_code_count = {i: 0 for i in unique_title_event_codes}\n    \n    accumulated_game_correctness = {f'game_{i}_cor':0 for i in unique_game_titles}\n    accumulated_game_incorrectness = {f'game_{i}_incor':0 for i in unique_game_titles}\n    accumulated_incomplete_games = {f'game_{i}_incomplete':0 for i in unique_game_titles}\n    accumulated_per_ass_corr = {f'{i}_cor': 0 for i in unique_assess_titles}\n    accumulated_per_ass_incorr = {f'{i}_incor': 0 for i in unique_assess_titles}\n    \n    last_ass_timestamp = 0\n    prev_session_timestamp = 0\n    pre_ass_between_sessions_time = []\n    \n    first_session_time_stamp = 0\n\n    accumulated_event_identifiers = {i:0 for i in aei}\n    accumulated_event_media_types = {i:0 for i in aemt}\n    \n    incomplete_assessments = {f'{i}_incomplete': 0 for i in unique_assess_titles}\n    \n    session_count = 0\n    session_count_discounted = 0\n    prev_ses_last_timestamp = 0\n    prev_ses_title = 0\n    orderliness = 0\n    orderliness_neg = 0\n    orderliness_disc = 0\n    prev_ses_count = 0 \n    \n    type_speed_accum = {'Game_speed_accum':[], 'Activity_speed_accum':[]}\n    type_speed_preass = {'Game_speed_preass':[], 'Activity_speed_preass':[]}\n    \n    time_offs_all = []\n    time_off_before_ass_all = []\n    intra_ses_durations_latest = []\n    intra_ses_durations_all = []\n    intra_ses_durations_preass = []\n    \n    # --- SESSIONS LOOP START --- #\n    for session_id, session_group in user_sample.groupby('game_session', sort=False):\n        session_type = session_group['type'].iloc[0]\n        session_installation_id = session_group['installation_id'].iloc[-1]\n        session_title = session_group['title'].iloc[0]\n        session_title_text = titles_inverse_map[session_title]\n        session_timestamp = session_group['timestamp'].iloc[0]\n        time_spent = int(session_group[\"game_time\"].iloc[-1] / 1000)\n        \n        first_session_time_stamp = session_timestamp if first_session_time_stamp == 0 else first_session_time_stamp\n        \n        t = np.round(((session_timestamp - prev_ses_last_timestamp).seconds / 60), 3) if prev_ses_last_timestamp != 0 else 0\n        DR = get_dr(t)\n        time_offs_all.append(t)\n        \n        # discounting\n        event_code_count = discount(event_code_count, DR)\n        event_id_count = discount(event_id_count, DR)\n        title_count = discount(title_count, DR)\n        title_event_code_count = discount(title_event_code_count, DR)\n        time_spent_each_title = discount(time_spent_each_title, DR)\n        accumulated_event_identifiers = discount(accumulated_event_identifiers, DR)\n        accumulated_event_media_types = discount(accumulated_event_media_types, DR)\n        user_activities_count = discount(user_activities_count, DR)\n        user_activities_time = discount(user_activities_time, DR)\n        accumulated_game_correctness = discount(accumulated_game_correctness, DR)\n        accumulated_game_incorrectness = discount(accumulated_game_incorrectness, DR)\n        accumulated_incomplete_games = discount(accumulated_incomplete_games, DR)\n        accumulated_per_ass_corr = discount(accumulated_per_ass_corr, DR)\n        accumulated_per_ass_incorr = discount(accumulated_per_ass_incorr, DR)\n        accuracy_groups = discount(accuracy_groups, DR)\n        incomplete_assessments = discount(incomplete_assessments, DR)\n        \n        accumulated_accuracy *= DR\n        accumulated_correct_attempts *= DR\n        accumulated_incorrect_attempts  *= DR\n        accumulated_accuracy_group *= DR\n        accumulated_actions *= DR\n        session_count_discounted *= DR\n        orderliness_disc *= DR\n        \n        # updating orderliness\n        if (prev_ses_title == media_durations.loc[media_durations.title==session_title_text, 'prev_title'].values[0]) or (media_durations.loc[media_durations.title==session_title_text, 'prev_title'].values[0] == 0):\n            orderliness += 1\n            orderliness_neg += 1\n            orderliness_disc += 1\n        else:\n            orderliness_neg -= 1\n            \n        orderliness_prop = orderliness / (session_count + 1)\n        orderliness_prop_neg = orderliness_neg / (session_count + 1)\n        orderliness_prop_disc = orderliness_disc / session_count_discounted\n        \n        if session_type == 'Game': #DO NOT BLINDLY COMBINE THIS ONE WITH IF BELOW! WOULD BREAK LOGIC\n            game_cor = len(session_group.loc[session_group.game_cor == 1, 'game_cor'])\n            accumulated_game_correctness[f'game_{titles_inverse_map[session_title]}_cor'] = accumulated_game_correctness[f'game_{titles_inverse_map[session_title]}_cor'] + game_cor\n            game_incor = len(session_group.loc[session_group.game_cor == 0, 'game_cor'])\n            accumulated_game_incorrectness[f'game_{titles_inverse_map[session_title]}_incor'] = accumulated_game_incorrectness[f'game_{titles_inverse_map[session_title]}_incor'] + game_incor\n            if game_cor + game_incor == 0:\n                accumulated_incomplete_games[f'game_{titles_inverse_map[session_title]}_incomplete'] = accumulated_incomplete_games[f'game_{titles_inverse_map[session_title]}_incomplete'] + 1\n        if session_type == 'Game' or session_type == 'Activity':\n            ts = session_group.shape[0] / time_spent if time_spent != 0 else 0\n            type_speed_accum[f'{session_type}_speed_accum'].append(ts)\n            type_speed_preass[f'{session_type}_speed_preass'].append(ts)\n        \n        if session_type != 'Assessment':\n            time_spent_each_title[f'{titles_inverse_map[session_title]}_time'] = time_spent_each_title[f'{titles_inverse_map[session_title]}_time'] + time_spent\n            durations_all.append(time_spent)\n            durations_preass.append(time_spent)\n            intra_ses_durations_latest = ((session_group.loc[:,'timestamp'].iloc[1:].values - session_group.loc[:,'timestamp'].iloc[0:-1].values) / np.timedelta64(1, 's')).tolist()\n            intra_ses_durations_all += intra_ses_durations_latest\n            intra_ses_durations_preass += intra_ses_durations_latest\n            \n            for i in session_group['event_data']:\n                identifier = get_event_data(i, key='identifier', prefix=session_type)\n                media_type = get_event_data(i, key='media_type', prefix=session_type)\n                try:\n                    accumulated_event_identifiers[identifier] = accumulated_event_identifiers[identifier] + 1\n                except:\n                    if is_test != True: accumulated_event_identifiers[identifier] = 0\n                try:\n                    accumulated_event_media_types[media_type] = accumulated_event_media_types[media_type] + 1\n                except:\n                    if is_test != True: accumulated_event_identifiers[media_type] = 0\n            \n            # updating counters without discounting as betweena assessments\n            event_code_count = update_counters(event_code_count, \"event_code\", session_group, 1, 1)\n            event_id_count = update_counters(event_id_count, \"event_id\", session_group, 1, 1)\n            title_count = update_counters(title_count, 'title', session_group, 1, 1)\n            title_event_code_count = update_counters(title_event_code_count, 'title_event_code', session_group, 1, 1)\n            \n            time_diff = np.round(((session_timestamp - prev_session_timestamp).seconds / 60), 3) if prev_session_timestamp != 0 else 0\n            prev_session_timestamp = session_timestamp\n            pre_ass_between_sessions_time.append(time_diff)\n\n            # counts how many actions the player has done so far, used in the feature of the same name\n            accumulated_actions = accumulated_actions + len(session_group)\n\n            user_activities_count[session_type] = user_activities_count[session_type] + 1\n            user_activities_time[f'{session_type}_time'] = user_activities_time[f'{session_type}_time'] + time_spent\n            last_activitiy = session_type \n            \n            session_count += 1\n            session_count_discounted += 1\n        \n        elif (session_type == 'Assessment') & (is_test or len(session_group)>1):\n            all_attempts = session_group.query(f'event_code == {win_code[session_title]}')\n            \n            session_correct_attempts = all_attempts['event_data'].str.contains('true').sum()\n            session_incorrect_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            # adding features\n            if (session_correct_attempts + session_incorrect_attempts == 0) & (is_test==False):\n                incomplete_assessments[f'{titles_inverse_map[session_title]}_incomplete'] += 1\n                continue\n            elif (is_test==True):\n                if (session_correct_attempts + session_incorrect_attempts == 0) & (session_id != last_ass_session.loc[last_ass_session.installation_id==session_installation_id,'game_session'].values[0]):\n                    incomplete_assessments[f'{titles_inverse_map[session_title]}_incomplete'] += 1\n                    continue\n            \n            time_since_last_ass = np.round(((session_timestamp - last_ass_timestamp).seconds / 60), 3) if last_ass_timestamp != 0 else 0\n            last_ass_timestamp = session_timestamp\n        \n            features = user_activities_count.copy()\n            features['time_since_last_ass'] = time_since_last_ass\n            features.update(get_basic_stats(type_speed_accum['Activity_speed_accum'], 'Activity_speed_accum'))\n            features.update(get_basic_stats(type_speed_accum['Game_speed_accum'], 'Game_speed_accum'))\n            features.update(get_basic_stats(type_speed_preass['Activity_speed_preass'], 'Activity_speed_preass'))\n            features.update(get_basic_stats(type_speed_preass['Game_speed_preass'], 'Game_speed_preass'))\n            type_speed_preass['Activity_speed_preass'], type_speed_preass['Game_speed_preass'] = [], []\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(time_spent_each_title.copy())\n            features.update(accuracy_groups.copy())\n            features.update(accumulated_game_correctness.copy())\n            features.update(accumulated_game_incorrectness.copy())\n            features.update(accumulated_incomplete_games.copy())\n            features.update(get_basic_stats([accumulated_game_correctness[i] for i in accumulated_game_correctness], 'game_cor'))\n            features.update(get_basic_stats([accumulated_game_incorrectness[i] for i in accumulated_game_incorrectness], 'game_incor'))\n            features.update(get_basic_stats([accumulated_incomplete_games[i] for i in accumulated_incomplete_games], 'game_incomp'))\n            features.update(accumulated_per_ass_corr.copy())\n            features.update(accumulated_per_ass_incorr.copy())\n            features.update(user_activities_time.copy())\n            features.update(accumulated_event_identifiers.copy())\n            features.update(accumulated_event_media_types.copy())\n            features.update(incomplete_assessments.copy())\n            features['sum_ass_incomp'] = np.sum([incomplete_assessments[i] for i in incomplete_assessments])\n            features['accumulated_session_count'] = session_count\n            features['discounted_session_count'] = session_count_discounted\n            features['orderliness'] = orderliness \n            features['orderliness_prop'] = orderliness_prop\n            features['orderliness_prop_neg'] = orderliness_prop_neg\n            features['orderliness_prop_disc'] = orderliness_prop_disc\n            time_off_before_ass = np.round(((prev_ses_last_timestamp - session_timestamp).seconds / 60), 3) if prev_ses_last_timestamp != 0 else 1000\n            features['time_off_before_ass'] = time_off_before_ass\n            time_off_before_ass_all.append(time_off_before_ass)\n            features.update(get_basic_stats(time_off_before_ass_all, 'time_off_before_ass_all'))\n            features.update(get_basic_stats(time_offs_all, 'time_offs_sum'))\n            features.update(get_basic_stats(durations_all, 'durations_all'))\n            features.update(get_basic_stats(durations_preass, 'durations_preass'))\n            durations_preass = []\n            features.update(get_basic_stats(intra_ses_durations_latest, 'intra_ses_durations_latest'))\n            features.update(get_basic_stats(intra_ses_durations_all, 'intra_ses_durations_all'))\n            features.update(get_basic_stats(intra_ses_durations_preass, 'intra_ses_durations_preass'))\n            intra_ses_durations_preass = []\n            \n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n            \n            features[f'{titles_inverse_map[session_title]}_cor'] = accumulated_per_ass_corr[f'{titles_inverse_map[session_title]}_cor']\n            accumulated_per_ass_corr[f'{titles_inverse_map[session_title]}_cor'] = features[f'{titles_inverse_map[session_title]}_cor'] + session_correct_attempts\n            features[f'{titles_inverse_map[session_title]}_incor'] = accumulated_per_ass_incorr[f'{titles_inverse_map[session_title]}_incor']\n            accumulated_per_ass_incorr[f'{titles_inverse_map[session_title]}_incor'] = features[f'{titles_inverse_map[session_title]}_incor'] + session_incorrect_attempts\n            \n            features['installation_id'] = session_installation_id\n            features['session_title'] = session_title\n            \n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_incorrect_attempts'] = accumulated_incorrect_attempts\n            accumulated_correct_attempts = accumulated_correct_attempts + session_correct_attempts \n            accumulated_incorrect_attempts = accumulated_incorrect_attempts + session_incorrect_attempts\n            \n            features.update(get_basic_stats(durations, 'durations'))\n            durations.append((session_group.iloc[-1, 2] - session_group.iloc[0, 2] ).seconds)\n            \n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = session_correct_attempts/(session_correct_attempts+session_incorrect_attempts) if (session_correct_attempts+session_incorrect_attempts) != 0 else 0\n            accumulated_accuracy = accumulated_accuracy + accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            \n            if accuracy == 0: features['accuracy_group'] = 0\n            elif accuracy == 1: features['accuracy_group'] = 3\n            elif accuracy == 0.5: features['accuracy_group'] = 2\n            else: features['accuracy_group'] = 1\n                \n            accuracy_groups[features['accuracy_group']] = accuracy_groups[features['accuracy_group']] + 1\n            \n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group = accumulated_accuracy_group + features['accuracy_group']\n            \n            features['accumulated_actions'] = accumulated_actions\n            \n            features['pre_ass_ses_count'] = session_count - prev_ses_count\n            \n            features.update(get_basic_stats(pre_ass_between_sessions_time, 'pre_ass_between_sessions_time'))\n            \n            pre_ass_between_sessions_time = []\n            prev_session_timestamp = 0\n            \n            features['time_since_installation'] = np.round(((session_timestamp - first_session_time_stamp).seconds / 60), 3)\n            \n            for i in session_group['event_data']:\n                identifier = get_event_data(i, key='identifier', prefix='Ass')\n                media_type = get_event_data(i, key='media_type', prefix='Ass')\n                try:\n                    accumulated_event_identifiers[identifier] = accumulated_event_identifiers[identifier] + 1\n                except:\n                    if is_test != True: accumulated_event_identifiers[identifier] = 0\n                try:\n                    accumulated_event_media_types[media_type] = accumulated_event_media_types[media_type] + 1\n                except:\n                    if is_test != True: accumulated_event_media_types[media_type] = 0\n            \n            # updating counters           \n            event_code_count = update_counters(event_code_count, \"event_code\", session_group, 1, 1)\n            event_id_count = update_counters(event_id_count, \"event_id\", session_group, 1, 1)\n            title_count = update_counters(title_count, 'title', session_group, 1, 1)\n            title_event_code_count = update_counters(title_event_code_count, 'title_event_code', session_group, 1, 1) \n            \n            user_activities_count[session_type] = user_activities_count[session_type] + 1\n            user_activities_time[f'{session_type}_time'] = user_activities_time[f'{session_type}_time'] + time_spent\n            last_activitiy = session_type \n            \n            counter += 1\n            session_count += 1\n            session_count_discounted += 1\n            prev_ses_count = session_count\n            \n            all_assessments.append(features)\n        \n        prev_ses_last_timestamp = session_group['timestamp'].iloc[-1]\n        prev_ses_title = session_title_text\n    \n    if is_test != True:\n        aei.update(accumulated_event_identifiers)\n        aemt.update(accumulated_event_media_types)\n            \n    # --- SESSIONS LOOP END --- # \n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_all_installation_ids(df, **kwargs):\n    compiled_df = []\n    for _, user_sample in tqdm(df.groupby('installation_id', sort = False), total = df.installation_id.nunique()):\n        compiled_df += get_data(user_sample, **kwargs)\n    reduced_df = pd.DataFrame(compiled_df)\n    return reduced_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_installation_aggr_features(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['durations_mean'].transform('mean')\n        df['installation_duration_std'] = df.groupby(['installation_id'])['durations_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df.loc[:,[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    return reduce_train, reduce_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models' code"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Base_Model(object):\n    \"\"\"\n    Parent model class, contains functions universal for all models used. Initial implementation credits to Bruno Aquino.\n    \"\"\"\n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True, target='accuracy_group'):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.is_last = train_df.is_last\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = target\n        self.cv = self.get_cv()\n        self.verbose = verbose\n        self.params = self.get_params()\n        self.y_pred, self.score, self.model, self.val_ys, self.val_preds = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        if MIX_UP:\n            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        else:\n            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n#         oof_pred = np.zeros((len(unseen_valid), )) if MIX_UP else np.zeros((len(self.train_df), ))\n        oof_pred = []\n        oof_ys_all = []\n        oof_pred_lastonly = []\n        oof_ys_lastonly = []\n        y_pred = np.zeros((len(self.test_df), ))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            if MIX_UP:\n                x_train, x_val = self.train_df[self.features].iloc[train_idx], unseen_valid[self.features]\n                y_train, y_val = self.train_df[self.target][train_idx], unseen_valid[self.target]\n                self.is_last = unseen_valid.is_last\n                cur_is_last = unseen_valid.is_last\n            else:\n                x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n                y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n                cur_is_last = self.is_last.iloc[val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            conv_x_val = self.convert_x(x_val.reset_index(drop=True))\n            conv_x_val_lastonly = self.convert_x(x_val.loc[cur_is_last==1,:].reset_index(drop=True))\n#             oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n            preds_all = model.predict(conv_x_val)\n            preds_lastonly = model.predict(conv_x_val_lastonly)\n            oof_pred_lastonly.append(preds_lastonly)\n            oof_pred.append(preds_all)\n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n#             print('Partial score (all) of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n            print('Partial score (all) of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, np.array(preds_all).reshape(-1,1))[1]))\n            print('Partial score (lastonly) of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val.loc[cur_is_last==1].reset_index(drop=True), np.array(preds_lastonly).reshape(-1,1), is_last=True)[1]))\n            oof_ys_lastonly.append([i for i in y_val.loc[cur_is_last==1].reset_index(drop=True).values])\n            oof_ys_all.append(y_val.reset_index(drop=True).values)\n#         _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n        oof_ys_all = np.concatenate(oof_ys_all).reshape(-1,1)\n        oof_pred = np.concatenate(oof_pred).reshape(-1,1)\n        _, loss_score, _ = eval_qwk_lgb_regr(oof_ys_all, oof_pred)\n        try:\n            oof_ys_lastonly = np.concatenate(oof_ys_lastonly).reshape(-1,1)\n            oof_pred_lastonly = np.concatenate(oof_pred_lastonly).reshape(-1,1)\n            _, loss_score_lastonly, _ = eval_qwk_lgb_regr(oof_ys_lastonly, oof_pred_lastonly, is_last=True)\n        except:\n            loss_score_lastonly,oof_ys_lastonly,oof_pred_lastonly = None,None,None\n        if self.verbose:\n            print('Our oof cohen kappa score (all) is: ', loss_score)\n            print('Our oof cohen kappa score (lastonly) is: ', loss_score_lastonly)\n        if RETURN_PREDS_LO:\n            return y_pred, loss_score, model, oof_ys_lastonly, oof_pred_lastonly\n        else:\n            return y_pred, loss_score, model, self.train_df[self.target], oof_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Lgb_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.75,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.8,\n                    'max_depth': 150, # was 15\n                  'num_leaves': 50,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 300,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 400\n                    }\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Xgb_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return xgb.train(self.params, train_set, \n                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=verbosity, early_stopping_rounds=100)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        return train_set, val_set\n    \n    def convert_x(self, x):\n        return xgb.DMatrix(x)\n        \n    def get_params(self):\n        params = {'colsample_bytree': 0.8,                 \n            'learning_rate': 0.01,\n            'max_depth': 25,\n            'subsample': 1,\n            'objective':'reg:squarederror',\n            #'eval_metric':'rmse',\n            'min_child_weight':3,\n            'gamma':0.25,\n            'n_estimators':5000}\n\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Catb_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        clf = CatBoostRegressor(**self.params)\n        clf.fit(train_set['X'], \n                train_set['y'], \n                eval_set=(val_set['X'], val_set['y']),\n                verbose=verbosity, \n                cat_features=self.categoricals)\n        return clf\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'loss_function': 'RMSE',\n                   'task_type': \"CPU\",\n                   'iterations': 5000,\n                   'od_type': \"Iter\",\n                    'depth': 10,\n                  'colsample_bylevel': 0.5, \n                   'early_stopping_rounds': 300,\n                    'l2_leaf_reg': 18,\n                   'random_seed': 421,\n                    'use_best_model': True\n                    }\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Nn_Model(Base_Model):\n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        features = features.copy()\n        if len(categoricals) > 0:\n            for cat in categoricals:\n                enc = OneHotEncoder()\n                train_cats = enc.fit_transform(train_df[[cat]])\n                test_cats = enc.transform(test_df[[cat]])\n                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n                features += cat_cols\n                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n                train_df = pd.concat([train_df, train_cats], axis=1)\n                test_df = pd.concat([test_df, test_cats], axis=1)\n        scalar = MinMaxScaler()\n        train_df[features] = scalar.fit_transform(train_df[features])\n        test_df[features] = scalar.transform(test_df[features])\n        print(train_df[features].shape)\n        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n        \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n            tf.keras.layers.Dense(400, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(200, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.20),\n            tf.keras.layers.Dense(100, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.15),\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.1),\n            tf.keras.layers.Dense(1, activation='relu')\n        ])\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n        print(model.summary())\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                 callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n        return model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import choice\n\nclass Cnn_Model(Base_Model):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        features = features.copy()\n        if len(categoricals) > 0:\n            for cat in categoricals:\n                enc = OneHotEncoder()\n                train_cats = enc.fit_transform(train_df[[cat]])\n                test_cats = enc.transform(test_df[[cat]])\n                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n                features += cat_cols\n                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n                train_df = pd.concat([train_df, train_cats], axis=1)\n                test_df = pd.concat([test_df, test_cats], axis=1)\n        scalar = MinMaxScaler()\n        train_df[features] = scalar.fit_transform(train_df[features])\n        test_df[features] = scalar.transform(test_df[features])\n        self.create_feat_2d(features)\n        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n        \n    def create_feat_2d(self, features, n_feats_repeat=50):\n        self.n_feats = len(features)\n        self.n_feats_repeat = n_feats_repeat\n        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n        for i in range(self.n_feats_repeat):\n            l = list(range(self.n_feats))\n            for j in range(self.n_feats):\n                c = l.pop(choice(range(len(l))))\n                self.mask[i, j] = c\n        self.mask = tf.convert_to_tensor(self.mask)\n        print(self.mask.shape)\n       \n        \n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n\n        inp = tf.keras.layers.Input(shape=(self.n_feats))\n        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(200, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.2)(x)\n        x = tf.keras.layers.Dense(100, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.15)(x)\n        x = tf.keras.layers.Dense(50, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.1)(x)\n        out = tf.keras.layers.Dense(1)(x)\n        \n        model = tf.keras.Model(inp, out)\n    \n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n        print(model.summary())\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                 callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n        return model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"Read data and determine last assesment session id in the test dataset. Test dataset has last assessment sessions truncated to first event only for all the users, to prevent data leakage. However, non-last assessment sessions may not contain all expected events as well, simply because child has not finished an assessment. Thus we need to distinguish between genuinely incomplete assessments versus those that were truncated by the organisers. Hence here we get all last assessment session ids in test dataset and use this in get_data() function to appropriately parse last assessments in test set despite that they are truncated."},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain, test, train_labels, specs, sample_submission, media_durations = read_data()\n\n# get last game sessions in test\nlast_ass_session = test.loc[:,['installation_id','type','game_session'], ].groupby(['installation_id','type'], sort=False,as_index=False).last()\nlast_ass_session = last_ass_session.loc[last_ass_session.type=='Assessment',['installation_id', 'game_session']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial competition datasets did not contain duration of clip titles. The organisers later released a separate file \"media durations\" that contained durations of all the clips in the game. THe cell below adds duration of clips to the main dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(media_durations, on=['title'], how='left')\ntrain.loc[train.type_x == 'Clip','game_time'] = train.loc[train.type_x == 'Clip','duration'] \ntrain.drop(['type_y','duration'], axis=1, inplace=True)\ntrain.rename({'type_x':'type'}, axis=1, inplace=True)\n\ntest = test.merge(media_durations, on=['title'], how='left')\ntest.loc[test.type_x == 'Clip','game_time'] = test.loc[test.type_x == 'Clip','duration'] \ntest.drop(['type_y','duration'], axis=1, inplace=True)\ntest.rename({'type_x':'type'}, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Children may play the game in any order. However, there is a default order that is suggested by the developers.\n\nInitial datasets did not contain any indication of titles order. The \"media durations\" file that was mentioned above contained all event titles in an intended order. I have used this information to obtain previouts intended title for each event tile. This is used in get_data() function to create \"orderliness\" features that indicate if child tends to interact with the app in inteded order or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"media_durations['prev_title'] = 0\nfor i in range(media_durations.shape[0]):\n    media_durations.iloc[i, 3] =  media_durations.iloc[i-1, 0] if media_durations.iloc[i-1, 1] != 'Assessment' else 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I convert categorical to numeric features and get dictionaries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code,  unique_game_titles = encode_categoricals(train, test, train_labels)\n\nunique_titles = list_of_user_activities\nunique_event_codes = list_of_event_code\ntitles_inverse_map = activities_labels\nunique_assess_titles = assess_titles\nunique_event_ids = list_of_event_id\nunique_title_event_codes = all_title_event_code","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running parsing functions (they take a while due to unoptimized code, as execution speed was not a priority in this comopetition)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tranform function to get the train and test set\nPARSE_ATTEMPTED = True\n\nreduce_train = parse_all_installation_ids(train, disc_rate = 1, cur_rate = 1, is_test = False)\nreduce_test = parse_all_installation_ids(test, disc_rate = 1, cur_rate = 1, is_test = True)\n\ncategoricals = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting a few more features"},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train, reduce_test = get_installation_aggr_features(reduce_train, reduce_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for title in unique_titles:\n   reduce_train.loc[:,f'{title}_speed'] = reduce_train.loc[:,f'{title}_time']/reduce_train.loc[:,title]\n   reduce_test.loc[:,f'{title}_speed'] = reduce_test.loc[:,f'{title}_time']/reduce_test.loc[:,title]\n    \n    \nreduce_train.replace([np.inf, -np.inf], np.nan, inplace=True)\nreduce_train.fillna(value=0, inplace=True)\n\nreduce_test.replace([np.inf, -np.inf], np.nan, inplace=True)\nreduce_test.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles_numbers = [i for i in titles_inverse_map]\ntitles_titles = [titles_inverse_map[i] for i in titles_inverse_map]\ntitles_map = {titles_titles[i]:titles_numbers[i] for i in range(len(titles_titles))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ass_features = [i for i in reduce_train.columns if i in unique_assess_titles]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving parsed datasets or loading existing ones if parsing failed. This was done to allow testing modell fitting at early stage when parsing functions where too buggy,"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ntry:\n    if PARSE_ATTEMPTED:\n        pickle.dump(reduce_train, open(\"/kaggle/working/reduce_train.p\", \"wb\"))\n        pickle.dump(reduce_test, open(\"/kaggle/working/reduce_test.p\", \"wb\"))\n        pickle.dump(titles_map, open(\"/kaggle/working/titles_map.p\", \"wb\"))\n        pickle.dump(ass_features, open(\"/kaggle/working/ass_features.p\", \"wb\"))\n        print('Saved data')\nexcept:\n    reduce_train = pickle.load(open(\"../input/basedon460/reduce_train.p\", \"rb\"))\n    reduce_test = pickle.load(open(\"../input/basedon460/reduce_test.p\", \"rb\"))\n    titles_map = pickle.load(open(\"../input/basedon460/titles_map.p\", \"rb\"))\n    ass_features = pickle.load(open(\"../input/basedon460/ass_features.p\", \"rb\"))\n    print('Loaded data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is one of the key competition success components - a lot of data from test set could have been used for training purposes. The code below adds such data from test to train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# add is_last and non-last-assessment rows from test to train\n\nuse_rows_from_test = True\n\nreduce_train.loc[:,'index'] = reduce_train.index\nlast_idx = reduce_train.loc[:,['index','installation_id'], ].groupby(['installation_id'], sort=False).last()['index']\nreduce_train.loc[:,'is_last'] = 0\nreduce_train.loc[last_idx,'is_last'] = 1\nreduce_train.drop(['index'], axis=1, inplace=True)\n\nreduce_test.loc[:,'index'] = reduce_test.index\nlast_idx = reduce_test.loc[:,['index','installation_id'], ].groupby(['installation_id'], sort=False).last()['index']\nreduce_test.loc[:,'is_last'] = 0\nreduce_test.loc[last_idx,'is_last'] = 1\nreduce_test.drop(['index'], axis=1, inplace=True)\n\nif use_rows_from_test:\n    reduce_train = pd.concat([reduce_train, reduce_test.loc[reduce_test.is_last==0,:]], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions below where used to experiment with oversampling and mix-up. Oversampling was attempted for under-represented samples (e.g. last assessments vs non-last). \"Mix-up\" was an augmentation idea from image recognition practices where 2 images are overlayed on each other with less than 100% transaparency. Same \"overlaying\" is done to labels where a new label is a weighted composition of intiial labels. \n\nNeither of the ideas have worked in short time I have spent on them. I bevlieve both ideas could have worked if done right, however I had no time to experiemnt with them further."},{"metadata":{"trusted":true},"cell_type":"code","source":"def balance(dim_to_balance, df, shuffle=True):\n    np.random.seed(2103)\n    counts = df.loc[:,[dim_to_balance, 'installation_id']].groupby([dim_to_balance], as_index=False).count().sort_values(['installation_id'], ascending=False).reset_index(drop=True)\n    print('initial dist: '); print(counts)\n    balanced_df = df\n    max_len = counts.loc[0,'installation_id']\n    for i in counts.loc[1:,dim_to_balance]:\n        cur_df = df.loc[df.loc[:,dim_to_balance]==i,]\n        cur_len = cur_df.shape[0]\n        copy_size = max_len - cur_len\n        idx = np.random.randint(0, cur_len, copy_size)\n        copy_df = cur_df.iloc[idx, :]\n        balanced_df = pd.concat([balanced_df, copy_df], ignore_index = True)\n    if shuffle:   \n        idx = np.random.permutation(balanced_df.shape[0])\n        balanced_df = balanced_df.iloc[idx,].reset_index(drop=True)\n    print('corrected dist: '); print(balanced_df.loc[:,[dim_to_balance, 'installation_id']].groupby([dim_to_balance], as_index=False).count())\n    return balanced_df\n\ndef mix_up(dim_to_balance, df, shuffle=True):\n    np.random.seed(2103)\n    counts = df.loc[:,[dim_to_balance, 'installation_id']].groupby([dim_to_balance], as_index=False).count().sort_values(['installation_id'], ascending=False).reset_index(drop=True)\n    print('initial dist: '); print(counts)\n    balanced_df = df\n    max_len = counts.loc[0,'installation_id']\n    cols_to_mix = [i for i in df.columns if i not in ['installation_id', 'is_last']]\n    for i in counts.loc[1:,dim_to_balance]:\n        cur_df = df.loc[df.loc[:,dim_to_balance]==i,]\n        cur_len = cur_df.shape[0]\n        copy_size = max_len - cur_len\n        idx_1 = np.random.randint(0, cur_len, copy_size)\n        idx_2 = np.random.randint(0, cur_len, copy_size)\n        m_1 = np.random.beta(0.5,0.5,copy_size)\n        m_2 = 1-m_1\n        mixed_rows = []\n        for q in tqdm(range(copy_size)):\n            mixed_rows += [m_1[q]*cur_df.loc[:,cols_to_mix].iloc[idx_1[q],:] + m_2[q]*cur_df.loc[:,cols_to_mix].iloc[idx_2[q],:]]\n        copy_df = pd.DataFrame(mixed_rows)\n        balanced_df = pd.concat([balanced_df, copy_df], ignore_index = True)\n    if shuffle:   \n        idx = np.random.permutation(balanced_df.shape[0])\n        balanced_df = balanced_df.iloc[idx,].reset_index(drop=True)\n    print('corrected dist: '); print(balanced_df.loc[:,[dim_to_balance, 'installation_id']].groupby([dim_to_balance], as_index=False).count())\n    return balanced_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if MIX_UP:\n#     np.random.seed(1997)\n#     idx = np.random.permutation(reduce_train.shape[0])[0:round(0.2*reduce_train.shape[0])]\n#     reduce_train_unbalanced = reduce_train.copy()\n#     unseen_valid = reduce_train_unbalanced.iloc[idx,:]\n#     reduce_train = mix_up('is_last', reduce_train.iloc[[i for i in range(reduce_train.shape[0]) if i not in idx],:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if MIX_UP:\n    np.random.seed(1997)\n    idx = np.random.permutation(reduce_train.shape[0])[0:round(0.2*reduce_train.shape[0])]\n#     reduce_train_unbalanced = reduce_train.copy()\n    unseen_valid = reduce_train.iloc[idx,:].copy()\n    reduce_train = balance('is_last', reduce_train.iloc[[i for i in range(reduce_train.shape[0]) if i not in idx],:])\n    \n    reduce_train.loc[reduce_train.is_last.isna(),'is_last'] = 1\n    print(reduce_train.is_last.value_counts())\n    \n    reduce_train.accuracy_group.hist(bins=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming features - dividing test and train by train's standard deviation and applying cubic root transform. This is because I have noticed that a lot of variables' distributions are skewed and hypothesized some normalisation may help. I have tried using other powers but cubic root seemed to work best."},{"metadata":{"trusted":true},"cell_type":"code","source":"no_transform = ['accuracy_group', 'installation_id', 'session_title', 'is_last'] + [f'{ass}_onehot' for ass in ass_features]\n\nfor i in tqdm([c for c in reduce_train.columns if c not in no_transform]):\n#     mean = reduce_train[i].mean()\n    sd = reduce_train[i].std()\n    reduce_train.loc[:,i] = np.power((reduce_train.loc[:,i])/(sd+0.01), POWER)\n    reduce_test.loc[:,i] = np.power((reduce_test.loc[:,i])/(sd+0.01), POWER)\n    if MIX_UP: unseen_valid.loc[:,i] = np.power((unseen_valid.loc[:,i])/(sd+0.01), POWER)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning data from infinities and nulls - some features may have had such numbers due to divisions of very large by very small numbers or by zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.replace([np.inf, -np.inf], np.nan, inplace=True)\nreduce_test.replace([np.inf, -np.inf], np.nan, inplace=True)\nif MIX_UP: unseen_valid.replace([np.inf, -np.inf], np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.fillna(value=0, inplace=True)\nreduce_test.fillna(value=0, inplace=True)\nif MIX_UP: unseen_valid.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\nreduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]\nif MIX_UP: unseen_valid.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in unseen_valid.columns]\n\nfull_reduce_test = reduce_test.copy()\nreduce_test = reduce_test.loc[reduce_test.is_last==1,:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# call feature engineering function\nfeatures = reduce_train.columns\nfeatures = [x for x in features if x not in ['accuracy_group', 'installation_id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Throw away features with too high correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nto_remove = []\nfor feat_a in features:\n    for feat_b in features:\n        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n            if c > 0.995:\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Throw away features if they differ between train and test sets too much"},{"metadata":{"trusted":true},"cell_type":"code","source":"differences = []\nadjusted_counter = 0\nunadjusted_counter = 0\n\nto_exclude = [] \najusted_test = reduce_test.copy()\nfor feature in ajusted_test.columns:\n    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title', 'is_last'] + [f'{ass}_onehot' for ass in ass_features] + to_remove:\n        data = reduce_train[feature]\n        train_mean = data.mean()\n        data = full_reduce_test[feature] \n        test_mean = data.mean()\n\n        adjust_factor = (train_mean + 0.01) / (test_mean + 0.01)\n        \n        differences.append((feature, adjust_factor))\n\n        if adjust_factor > 5 or adjust_factor < 0.2:\n            to_exclude.append(feature)\n            print(feature, train_mean, test_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [x for x in features if x not in (to_exclude + to_remove)]\nreduce_train[features].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB model did not seem to perform well and was very slow\n\n# xgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_model = Catb_Model(reduce_train, ajusted_test, features, categoricals=categoricals) #if MIX_UP != True else None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = Cnn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Post-processing and submission"},{"metadata":{},"cell_type":"markdown","source":"### Mixing models together and saving submission file"},{"metadata":{},"cell_type":"markdown","source":"Mix models' predictions together using hand-picked weights. In restrospective could have weighted by validation scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    weights = {'lbg': 0.20, 'nn': 0.30, 'cat':0.30, 'cnn':0.20}\n    final_pred = (lgb_model.y_pred * weights['lbg'])  + (nn_model.y_pred * weights['nn']) + (cat_model.y_pred * weights['cat']) + (cnn_model.y_pred * weights['cnn'])\n    final_val_ys = (lgb_model.val_ys * weights['lbg'])  + (nn_model.val_ys * weights['nn']) + (cat_model.val_ys * weights['cat']) + (cnn_model.val_ys * weights['cnn'])\n    final_val_preds = (lgb_model.val_preds * weights['lbg']) + (nn_model.val_preds * weights['nn']) + (cat_model.val_preds * weights['cat']) + (cnn_model.val_preds * weights['cnn'])\n    print('Used all models')\nexcept:\n    weights = {'lbg': 0.20, 'nn': 0.40, 'cnn': 0.40}\n    final_pred = (lgb_model.y_pred * weights['lbg']) + (nn_model.y_pred * weights['nn']) + (cnn_model.y_pred * weights['cnn'])\n    final_val_ys = (lgb_model.val_ys * weights['lbg']) + (nn_model.val_ys * weights['nn']) + (cnn_model.val_ys * weights['cnn'])\n    final_val_preds = (lgb_model.val_preds * weights['lbg']) + (nn_model.val_preds * weights['nn']) + (cnn_model.val_preds * weights['cnn'])\n    print('Used only 3 models')\n\nprint(final_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rounding accuracy groups. Goal of the competition was to predict accuracy group of a child which could only have been 0, 1, 2 or 3. Simple rounding yielded poor results, so most of the participants used Nelder-Mead optimisatio to get thresholds for rounding. However such approach led to very high QWK on validation (~0.8) and that did not translate to good public LB score (~0.6 at most). So I have decided to get thresholds using train distributino of accuracy groups. That is, for example, if 10% of children where in group 0 I have taken sorted models' prediction value at 10th percentile as threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_source = reduce_train.loc[reduce_train.is_last==1,'accuracy_group'].reset_index(drop=True) if USE_DIFFERENT_BOUNDS else reduce_train['accuracy_group']\n\ndist = Counter(dist_source)\nfor k in dist:\n    dist[k] /= len(dist_source)\ndist_source.hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(final_val_preds, acum * 100)\nprint(bound)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, final_pred)))\n\nsample_submission['accuracy_group'] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nax1.hist(final_pred)\nax1.set_title('Accuracy group in final predictions')\nax2.hist(reduce_train.loc[reduce_train.is_last==1,'accuracy_group'])\nax2.set_title('Accuracy group in train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at variable importance from LGB model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = [(i,f) for i, f in zip(lgb_model.model.feature_name(), lgb_model.model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(lgb_model.model, max_num_features = 50)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}