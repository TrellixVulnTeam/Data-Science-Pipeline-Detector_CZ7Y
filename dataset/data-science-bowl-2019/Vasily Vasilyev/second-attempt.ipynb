{"cells":[{"metadata":{"id":"Yf5eTkLPG4BF","colab_type":"text"},"cell_type":"markdown","source":"# Интелектуальные системы. Практическое задание. Гусев Д., Васильев В., Салимов Т.\n\n---\n"},{"metadata":{"id":"nmHr42X2Ks2-","colab_type":"text"},"cell_type":"markdown","source":"Данный `Jupyter Notebook` был использован для участия в соревновании [2019 Data Science Bowl — Uncover the factors to help measure how young children learn](https://www.kaggle.com/c/data-science-bowl-2019/) на сайте `Kaggle.com`.\n\n## Участники соревнования и авторы данного `Jupyter Notebook`\n\n### Гусев Данила — студент СПбГУ, группа 16.Б13-пу (433)\n\nЗанимался чтением, обработкой и очисткой данных, которые впоследствии использовались в моделях для обучения и предсказания.\n\n### Васильев Василий — студент СПбГУ, группа 16.Б13-пу (433)\n\nЗанимался выделением признаков, составил базовое решение (baseline), создал набор вспомогательных классов и функций для работы с моделями.\n\n### Салимов Тимур — студент СПбГУ, группа 16.Б11-пу (431)\n\nЗанимался улучшением базового решения, проводил эксперименты, оптимизировал и настраивал параметры моделей для получения наилучшего результата.  \n"},{"metadata":{"id":"qiqxKbWxU_D0","colab_type":"text"},"cell_type":"markdown","source":"## Введение\n\n---\n"},{"metadata":{"id":"CTOICnMJT8ys","colab_type":"text"},"cell_type":"markdown","source":"\n### Описание задачи\n\nСуществует популярное приложения для детей `PBS KIDS Measure Up!`. В нём дети могут смотреть обучающие видеоролики и после решать задачи, применяя полученную информацию из просмотренных видеороликов. Авторы приложения хотят улучшить приложение и задачи, которые ставятся перед детьми. В первую очередь разработчики хотят откорректировать сложность текущих задач на основе собранной статистики использования приложения.\n\nОднако на данный момент было собрано большой объём статистики, поэтому авторы приложения хотели бы получить модель, которая сможет предсказывать количество попыток, требуемое детям, чтобы решить поставленную задачу. Отметим, что задача может быть не решена вовсе.\n\nНа основе данной модели разработчики смогут корректировать сложность задач, повышая удовольствие и от использования приложения для детей.\n"},{"metadata":{"id":"kNI3NVOUT_yO","colab_type":"text"},"cell_type":"markdown","source":"### Оценка результатов и используемая метрика в соревновании\n\nМатериалы оцениваются на основе метрики Quadratic Weighted Kappa (QWK), которая измеряет согласие между двумя результатами. Этот показатель обычно варьируется от 0 (случайное согласие) до 1 (полное согласие). В случае если совпадения меньше, чем ожидалось, метрика может опуститься ниже 0.\n\nИзначально QWK была предложена как характеристика согласия между двумя ранжированиями (например, экспертными оценками). Формально данная величина является некоторой характеристикой связи двух случайных величин. При этом эти величины не являются числовыми, поэтому термин «случайная величина» мы используем с той оговоркой, что при необходимости нечисловые значения могут формально быть «закодированы» числами.\n\nРезультаты в этом соревновании сгруппированы в 4 группы (помечены как `accuracy_group` в данных):\n\n- 3: задача была решена с первой попытки;\n- 2: задача была решена со второй попытки;\n- 1: задача была решена после 3 или более попыток;\n- 0: задача никогда не была решена.\n\nПусть у нас есть оценка (estimator) α, тогда формула для расчёта метрики:\n\n![image.png](https://imgur.com/efFuDBa.png)\n\nгде R(α):\n\n![image.png](https://imgur.com/mtypPlA.png)\n\nи U(α):\n\n![image.png](https://imgur.com/E3OZGjQ.png)\n"},{"metadata":{"id":"latQVCcvKbsg","colab_type":"text"},"cell_type":"markdown","source":"## Подключение библиотек и настройка исполняющей среды\n\n---\n"},{"metadata":{"id":"0gPMtc6AJFp3","colab_type":"text"},"cell_type":"markdown","source":"В работе был использован язык программирования `Python 3.7`. Для проведения дальнейших вычислений, обучения модели и вычисления предсказаний подключим вспомогательные библиотеки.\n"},{"metadata":{"id":"JIYA97a6G4BH","colab_type":"code","outputId":"a9a1ce9b-5d1d-4629-c211-385cf02662ca","colab":{"base_uri":"https://localhost:8080/","height":277},"trusted":true},"cell_type":"code","source":"import copy\nimport os\nimport re\nimport time\nimport warnings\n\nfrom collections import Counter\nfrom functools import partial\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.model_selection import GroupKFold\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom typing import Dict, Any\n\nfrom tqdm import tqdm\nfrom numba import jit\n","execution_count":null,"outputs":[]},{"metadata":{"id":"r1eHz0fpJf0p","colab_type":"text"},"cell_type":"markdown","source":"Настроим исполняемую среду для `Jupyter Notebook`. В первую очередь включим отображение графиков и диаграм. Далее выставим настройки для библиотеки `Pandas`. Установим точность и ограничение на максимальное количество рядов, которое будет отображать библиотека при выводе `DataFrame`. "},{"metadata":{"id":"qP7qLvf0G4BK","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\npd.options.display.precision = 15\npd.set_option('max_rows', 500)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6HeeVKj0G4BL","colab_type":"text"},"cell_type":"markdown","source":"## Вспомогательные функции и классы для работы с моделями\n\n---\n"},{"metadata":{"id":"AMliVeMzcRcp","colab_type":"text"},"cell_type":"markdown","source":"Определим метод для эффективного вычисления метрики QWK. Данный метод используется вместо стандатрного метода `sklearn.metrics.cohen_kappa_score` из пакета `scikit-learn` ввиду неудовлетворительной производительности последнего.\n"},{"metadata":{"id":"feNGQTjHG4BM","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hiEVZE-1dXMD","colab_type":"text"},"cell_type":"markdown","source":"Для конвертации результатов предсказания регрессионной модели в требуемую форму ответа для задачи введём следующую функцию. Значения границ для конвертации вычислялись эмпирическим путём (для максимизации метрики).\n"},{"metadata":{"id":"Tzo8q9wPd5Uz","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def round_values(y_pred):\n    # This coefficients were calculated using emperical approach.\n    lower_bound = 1.12232214\n    middle_bound = 1.73925866\n    upper_bound = 2.22506454\n\n    y_pred[y_pred <= lower_bound] = 0\n\n    y_pred[\n        np.where(np.logical_and(y_pred > lower_bound, y_pred <= middle_bound))\n    ] = 1\n\n    y_pred[\n        np.where(np.logical_and(y_pred > middle_bound, y_pred <= upper_bound))\n    ] = 2\n\n    y_pred[y_pred > upper_bound] = 3\n\n    return y_pred\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"IsfP02E9fsWH","colab_type":"text"},"cell_type":"markdown","source":"Введём класс для обучения и предсказания. В текущем решении мы используем пакет `LightGBM`. `LightGBM` — фреймворк для градиентного бустинга, использующий алгоритмы обучения на основе деревьев решений.\n"},{"metadata":{"id":"vkNnV4s-G4BN","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb(y_true, y_pred):\n    y_pred = round_values(y_pred)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\nclass LGBWrapper:\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None,\n            y_holdout=None, params=None):\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb\n        else:\n            eval_metric = 'auc'\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [\n                col for col in params['cat_cols'] if col in X_train.columns\n            ]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(\n            X=X_train,\n            y=y_train,\n            eval_set=eval_set,\n            eval_names=eval_names,\n            eval_metric=eval_metric,\n            verbose=params['verbose'],\n            early_stopping_rounds=params['early_stopping_rounds'],\n            categorical_feature=categorical_columns\n        )\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PZ1AKBp5ggwJ","colab_type":"text"},"cell_type":"markdown","source":"Для преобразования данных создадим пару классов, унаследованных от `BaseEstimator`, `TransformerMixin`. Это стандартный подход для пакета `scikit-learn`, который позволит нам добавить эти классы в итоговый конвеер данных (data pipeline).\n"},{"metadata":{"id":"0UhE6IQeG4BP","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool=False,\n                 create_interactions: bool=False,\n                 n_interactions: int=20):\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n        if self.create_interactions:\n            self.feats_for_interaction = [\n                col for col in X.columns\n                if \"sum\" in col or\n                   \"mean\" in col or\n                   \"max\" in col or\n                   \"std\" in col or\n                   \"attempt\" in col\n            ]\n\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f\"{col1}_int_{col2}\"] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data[\"timestampHour\"] = np.sin(2 * np.pi * data[\"timestampHour\"] / 23.0)\n            data[\"timestampMonth\"] = np.sin(2 * np.pi * data[\"timestampMonth\"] / 23.0)\n            data[\"timestampWeek\"] = np.sin(2 * np.pi * data[\"timestampWeek\"] / 23.0)\n            data[\"timestampMinute\"] = np.sin(2 * np.pi * data[\"timestampMinute\"] / 23.0)\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list=None, num_cols: list=None):\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JN0aTSu5iuU1","colab_type":"text"},"cell_type":"markdown","source":"Для удобства использования модели из выбранного ранее фреймворка введём класс-обёртку. В дальнейшем мы можем поменять используемую модель или фреймворк без модификации всего исходного кода.\n"},{"metadata":{"id":"7DAPqV0uG4BQ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class RegressorModel:\n\n    def __init__(self, columns: list=None, model_wrapper=None):\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self,\n            X: pd.DataFrame,\n            y,\n            X_holdout: pd.DataFrame=None,\n            y_holdout=None,\n            folds=None,\n            params: dict=None,\n            eval_metric: str='rmse',\n            cols_to_drop: list=None,\n            preprocesser=None,\n            transformers: dict=None,\n            adversarial: bool=False,\n            plot: bool=True):\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(\n                list(zip(X_train.columns, model.feature_importances_)),\n                columns=['feature', 'importance']\n            )\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n\n        self.calc_scores_()\n\n        if plot:\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=20)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions')\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str='usual'):\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # If case transformation changes the number of the rows.\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool=True,\n                                top_n: int=10):\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool=True, top_n: int=10):\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results\\\n            .melt(id_vars=['iteration'])\\\n            .rename(columns={'value': self.eval_metric, 'variable': 'dataset'})\n\n        sns.lineplot(\n            data=full_evals_results,\n            x='iteration',\n            y=self.eval_metric,\n            hue='dataset'\n        )\n        plt.title('Training progress')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pTU8LAbqG4BR","colab_type":"text"},"cell_type":"markdown","source":"## Чтение и первичная обработка исходных данных\n\n---\n"},{"metadata":{"id":"z1gQSu-TjOjD","colab_type":"text"},"cell_type":"markdown","source":"Перед тем, как считать и обратотать данные, введём дополнительные классы-агрегаторы для сохранения в них промежуточных результатов.\n"},{"metadata":{"id":"Sku3gUjbG4BS","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class ReadResult:\n\n    def __init__(self, train, test, train_labels, specs, sample_submission):\n        self.train = train\n        self.test = test\n        self.train_labels = train_labels\n        self.specs = specs\n        self.sample_submission = sample_submission\n\n\nclass EncodedResult:\n\n    def __init__(self, train, test, train_labels, win_code,\n                 list_of_user_activities, list_of_event_code,\n                 activities_labels, assess_titles, list_of_event_id,\n                 all_title_event_code):\n        self.train = train\n        self.test = test\n        self.train_labels = train_labels\n        self.win_code = win_code\n        self.list_of_user_activities = list_of_user_activities\n        self.list_of_event_code = list_of_event_code\n        self.activities_labels = activities_labels\n        self.assess_titles = assess_titles\n        self.list_of_event_id = list_of_event_id\n        self.all_title_event_code = all_title_event_code\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TUJpUrXsjiNa","colab_type":"text"},"cell_type":"markdown","source":"Приступим к чтению. В соревновании предоставляются несколько файлов:\n\n- `train.csv & test.csv` —  основные файлы данных, которые содержат события игрового процесса ([подробнее](https://www.kaggle.com/c/data-science-bowl-2019/data));\n- `specs.csv` — файл содержит спецификацию различных типов игровых событий ([подробнее](https://www.kaggle.com/c/data-science-bowl-2019/data));\n- `sample_submission.csv` — пример ожидаемого ответа для соревнования;\n\nСтоит отметить, что основные данные по игровому процессу представлены в формате JSON. Учтём это при конвертации и обработки данных, чтобы не получить ошибки сериализации/десериализации при работе с данными.\n"},{"metadata":{"id":"wP66eL_NkyfJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def read_data(input_path: str, additional_path: str=None):\n    if additional_path is None:\n        additional_path = input_path\n\n    print(\"Reading train.csv file....\")\n    train = pd.read_csv(f\"{additional_path}/train.csv\")\n    print(f\"Training.csv file have {train.shape[0]} rows and {train.shape[1]} columns\")\n\n    print(\"Reading test.csv file....\")\n    test = pd.read_csv(f\"{input_path}/test.csv\")\n    print(f\"Test.csv file have {test.shape[0]} rows and {test.shape[1]} columns\")\n\n    print(\"Reading train_labels.csv file....\")\n    train_labels = pd.read_csv(f\"{input_path}/train_labels.csv\")\n    print(f\"Train_labels.csv file have {train_labels.shape[0]} rows and {train_labels.shape[1]} columns\")\n\n    print(\"Reading specs.csv file....\")\n    specs = pd.read_csv(f\"{input_path}/specs.csv\")\n    print(f\"Specs.csv file have {specs.shape[0]} rows and {specs.shape[1]} columns\")\n\n    print(\"Reading sample_submission.csv file....\")\n    sample_submission = pd.read_csv(f\"{input_path}/sample_submission.csv\")\n    print(f\"Sample_submission.csv file have {sample_submission.shape[0]} rows and {sample_submission.shape[1]} columns.\")\n\n    return ReadResult(train, test, train_labels, specs, sample_submission)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"YSddmPf2lFxs","colab_type":"text"},"cell_type":"markdown","source":"Считаем данные и посмотрим на некоторые объекты из предоставленной выборки. Чтобы пример заработал укажите собственные пути к файлам из соревнования. В данном решении основной файл `train.csv` подгружается из `Google Drive`, поскольку `Google Colab` имеет ограничения на размер файлов. Если файл `train.csv` лежит в одной директории с остальными, не указывайте второй параметр (`additional_path`) для функции `read_data`. \n"},{"metadata":{"id":"MRzGJ9dfG4BV","colab_type":"code","outputId":"0f55d0d4-924e-4bfb-e0a8-eabf2137dd87","colab":{"base_uri":"https://localhost:8080/","height":191},"trusted":true},"cell_type":"code","source":"# Read data.\nread_result = read_data(input_path=\"../input/data-science-bowl-2019\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"FE6qNiCjmTup","colab_type":"code","outputId":"d5a3f9bd-f314-4c55-a065-ef526a140985","colab":{"base_uri":"https://localhost:8080/","height":355},"trusted":true},"cell_type":"code","source":"read_result.train.head(n=10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5CLmkYW6k1ud","colab_type":"text"},"cell_type":"markdown","source":"Исходные данные содержат в основном строковые значения. Поэтому сконвертируем строки в наиболее подходящие объекты (числа, даты и т.п.), чтобы их можно было использовать для обучения модели.\n"},{"metadata":{"id":"DmbCt815owI1","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # Encode title.\n    train[\"title_event_code\"] = list(map(lambda x, y: str(x) + \"_\" + str(y), train[\"title\"], train[\"event_code\"]))\n    test[\"title_event_code\"] = list(map(lambda x, y: str(x) + \"_\" + str(y), test[\"title\"], test[\"event_code\"]))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n\n    # Make a list with all the unique \"titles\" from the train and test set.\n    list_of_user_activities = list(set(train[\"title\"].unique()).union(set(test[\"title\"].unique())))\n\n    # Make a list with all the unique \"event_code\" from the train and test set.\n    list_of_event_code = list(set(train[\"event_code\"].unique()).union(set(test[\"event_code\"].unique())))\n    list_of_event_id = list(set(train[\"event_id\"].unique()).union(set(test[\"event_id\"].unique())))\n\n    # Make a list with all the unique worlds from the train and test set.\n    list_of_worlds = list(set(train[\"world\"].unique()).union(set(test[\"world\"].unique())))\n\n    # Create a dictionary numerating the titles.\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train[\"type\"] == \"Assessment\"][\"title\"].value_counts().index).union(set(test[test[\"type\"] == \"Assessment\"][\"title\"].value_counts().index)))\n\n    # Replace the text titles with the number titles from the dict.\n    train[\"title\"] = train[\"title\"].map(activities_map)\n    test[\"title\"] = test[\"title\"].map(activities_map)\n    train[\"world\"] = train[\"world\"].map(activities_world)\n    test[\"world\"] = test[\"world\"].map(activities_world)\n    train_labels[\"title\"] = train_labels[\"title\"].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype(\"int\")))\n\n    # Then, it set one element, the \"Bird Measurer (Assessment)\" as 4110, 10 more than the rest\n    win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n\n    # Convert text into datetime.\n    train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n    test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n\n    return EncodedResult(\n        train, test, train_labels, win_code, list_of_user_activities,\n        list_of_event_code, activities_labels, assess_titles, list_of_event_id,\n        all_title_event_code\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jyX4Lf5vG4BX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Get usefull dict with maping encode.\nencoded_result = encode_title(\n    read_result.train, read_result.test, read_result.train_labels\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"WZsOdG4YqEqr","colab_type":"code","outputId":"565970de-6d11-45a0-85a9-8e18880991d9","colab":{"base_uri":"https://localhost:8080/","height":355},"trusted":true},"cell_type":"code","source":"encoded_result.train.head(n=10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"AB3KMa3sqmLi","colab_type":"text"},"cell_type":"markdown","source":"Теперь обработаем каждую строку из обучающей и тестовой выборок. Сгруппируем данные по `installation_id`, извлечём и обработаем данные по игровым сессиям.\n"},{"metadata":{"id":"3Q-cG0aSG4BT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def get_data(user_sample, encoded_result: EncodedResult, test_set=False):\n    \"\"\"\n    The user_sample is a DataFrame from train or test where the only one\n    installation_id is filtered.\n    And the test_set parameter is related with the labels processing, that is\n    only requered if test_set=False.\n    \"\"\"\n    # Constants and parameters declaration.\n    last_activity = 0\n\n    user_activities_count = {\"Clip\": 0, \"Activity\": 0, \"Assessment\": 0, \"Game\": 0}\n\n    # New features: time spent in each activity.\n    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0\n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    durations = []\n    last_accuracy_title = {\"acc_\" + title: -1 for title in encoded_result.assess_titles}\n\n    # These 4 variables are Dict[str, int].\n    event_code_count = {ev: 0 for ev in encoded_result.list_of_event_code}\n    event_id_count = {eve: 0 for eve in encoded_result.list_of_event_id}\n    title_count = {eve: 0 for eve in encoded_result.activities_labels.values()}\n    title_event_code_count = {t_eve: 0 for t_eve in encoded_result.all_title_event_code}\n\n    # Itarates through each session of one instalation_id.\n    for i, session in user_sample.groupby(\"game_session\", sort=False):\n        # session is a DataFrame that contain only one game_session.\n\n        # Get some sessions information.\n        session_type = session[\"type\"].iloc[0]\n        session_title = session[\"title\"].iloc[0]\n        session_title_text = encoded_result.activities_labels[session_title]\n\n        # For each assessment, and only this kind off session, the features\n        # below are processed and a register are generated.\n        if (session_type == \"Assessment\") & (test_set or len(session) > 1):\n            # Search for event_code 4100, that represents the assessments\n            # trial.\n            all_attempts = session.query(f\"event_code == {encoded_result.win_code[session_title]}\")\n\n            # Then, check the numbers of wins and the number of losses.\n            true_attempts = all_attempts[\"event_data\"].str.contains(\"true\").sum()\n            false_attempts = all_attempts[\"event_data\"].str.contains(\"false\").sum()\n\n            # Copy a dict to use as feature template,\n            # it's initialized with some itens:\n            # {\"Clip\": 0, \"Activity\": 0, \"Assessment\": 0, \"Game\": 0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n\n            # Get installation_id for aggregated features.\n            features[\"installation_id\"] = session[\"installation_id\"].iloc[-1]\n\n            # Add title as feature, remembering that title represents the name\n            # of the game.\n            features[\"session_title\"] = session[\"title\"].iloc[0]\n\n            # The 4 lines below add the feature of the history of the trials of\n            # this player.\n            # It is based on the all time attempts so far, at the moment of\n            # this assessment.\n            features[\"accumulated_correct_attempts\"] = accumulated_correct_attempts\n            features[\"accumulated_uncorrect_attempts\"] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n\n            # the time spent in the app so far\n            if durations == []:\n                features[\"duration_mean\"] = 0\n            else:\n                features[\"duration_mean\"] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n\n            # The accurace is the all time wins divided by the all time\n            # attempts.\n            features[\"accumulated_accuracy\"] = accumulated_accuracy / counter if counter > 0 else 0\n            accuracy = true_attempts / (true_attempts+false_attempts) if (true_attempts + false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title[\"acc_\" + session_title_text] = accuracy\n\n            # A feature of the current accuracy categorized\n            # It is a counter of how many times this player was in each\n            # accuracy group\n            if accuracy == 0:\n                features[\"accuracy_group\"] = 0\n            elif accuracy == 1:\n                features[\"accuracy_group\"] = 3\n            elif accuracy == 0.5:\n                features[\"accuracy_group\"] = 2\n            else:\n                features[\"accuracy_group\"] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features[\"accuracy_group\"]] += 1\n\n            # Mean of the all accuracy groups of this player.\n            features[\"accumulated_accuracy_group\"] = accumulated_accuracy_group / counter if counter > 0 else 0\n            accumulated_accuracy_group += features[\"accuracy_group\"]\n\n            # How many actions the player has done so far, it is initialized\n            # as 0 and updated some lines below.\n            features[\"accumulated_actions\"] = accumulated_actions\n\n            # There are some conditions to allow this features to be inserted\n            # in the datasets.\n            # If it's a test set, all sessions belong to the final dataset.\n            # It it's a train, needs to be passed throught this clausule:\n            # session.query(f\"event_code == {win_code[session_title]}\").\n            # That means, must exist an event_code 4100 or 4110.\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n\n            counter += 1\n\n        # This piece counts how many actions was made in each event_code so\n        # far.\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == \"title\":\n                        x = encoded_result.activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n\n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, \"title\")\n        title_event_code_count = update_counters(title_event_code_count, \"title_event_code\")\n\n        # Counts how many actions the player has done so far, used in the\n        # feature of the same name.\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n\n    # If it's the test_set, only the last assessment must be predicted,\n    # the previous are scraped.\n    if test_set:\n        return all_assessments[-1]\n\n    # In the train_set, all assessments goes to the dataset.\n    return all_assessments\n\n\ndef get_train_and_test(train, test, encoded_result: EncodedResult):\n    compiled_train = []\n    compiled_test = []\n\n    range_train = tqdm(\n        train.groupby(\"installation_id\", sort=False), total=17000\n    )\n    for (ins_id, user_sample) in range_train:\n        compiled_train += get_data(user_sample, encoded_result)\n\n    range_test = tqdm(\n        test.groupby(\"installation_id\", sort=False), total=1000\n    )\n    for ins_id, user_sample in range_test:\n        test_data = get_data(user_sample, encoded_result, test_set=True)\n        compiled_test.append(test_data)\n\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = [\"session_title\"]\n\n    return reduce_train, reduce_test, categoricals\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wbgE6_nqG4BY","colab_type":"code","outputId":"48a71142-05ec-4560-e637-c5406af043ab","colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"code","source":" # Tranform function to get the train and test set.\nreduce_train, reduce_test, categoricals = get_train_and_test(\n    encoded_result.train, encoded_result.test, encoded_result\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fOjj3rsGscl1","colab_type":"code","outputId":"81e65619-e706-4df3-d49b-8223fc1bd7f1","colab":{"base_uri":"https://localhost:8080/","height":474},"trusted":true},"cell_type":"code","source":"reduce_train.head(n=10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2tvBOgVIsmSS","colab_type":"text"},"cell_type":"markdown","source":"## Обработка, группировка и очистка данных\n\n---\n"},{"metadata":{"id":"RJahqKh-ugoZ","colab_type":"text"},"cell_type":"markdown","source":"После получения данных и их первичной обработки сгруппируем и выделим основные признаки, которые в дальнейшем будем использовать в модели для обучения и предсказания. Также провалидируем имеющиеся JSON-объекты, избежав возможных ошибок с сериализацией/десериализацией. Наконец, удалим лишние, неиспользуемые столбцы с данными.\n"},{"metadata":{"id":"8ytQyZ_JG4BZ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test, assess_titles):\n    for df in [reduce_train, reduce_test]:\n        df[\"installation_session_count\"] = df.groupby([\"installation_id\"])[\"Clip\"].transform(\"count\")\n        df[\"installation_duration_mean\"] = df.groupby([\"installation_id\"])[\"duration_mean\"].transform(\"mean\")\n        df[\"installation_title_nunique\"] = df.groupby([\"installation_id\"])[\"session_title\"].transform(\"nunique\")\n\n        df[\"sum_event_code_count\"] = df[\n            [2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080,\n             2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, 4022, 4025, 4030,\n             4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020,\n             4070, 2025, 2030, 4080, 2035, 2040, 4090, 4220, 4095]\n            ].sum(axis=1)\n\n        df[\"installation_event_code_count_mean\"] = df.groupby([\"installation_id\"])[\"sum_event_code_count\"].transform(\"mean\")\n\n        # Remove invalid characters from titles for json serialization.\n        df.columns = [\n            \"\".join(c if c.isalnum() else \"_\" for c in str(x))\n            for x in df.columns\n        ]\n\n    # Delete useless columns.\n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns\n    features = [\n        x for x in features\n        if x not in [\"accuracy_group\", \"installation_id\"]\n    ] + [\"acc_\" + title for title in assess_titles]\n\n    return reduce_train, reduce_test, features\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jlvFsVbDG4Bc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Call feature engineering function.\nreduce_train, reduce_test, features = preprocess(\n    reduce_train, reduce_test, encoded_result.assess_titles\n)\nreduce_train.columns = reduce_train.columns.str.replace(\",\", \"\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"rYQB8RMxvMQa","colab_type":"code","outputId":"806fa49a-43ab-4b76-c6fd-c5c0b5d6c06d","colab":{"base_uri":"https://localhost:8080/","height":405},"trusted":true},"cell_type":"code","source":"reduce_train.head(n=10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IpXteAYttj1C","colab_type":"text"},"cell_type":"markdown","source":"## Обучение регрессионной модели на подготовленных данных\n\n---\n"},{"metadata":{"id":"f8J20hJJxxw4","colab_type":"text"},"cell_type":"markdown","source":"Поскольку это задача классификации, объясним, как мы будем использовать регрессионную модель. Мы использовали следующий подход:\n\n1. Считать данные и выделить признаки (см. предыдущие разделы);\n2. Выбрать модель и обучить её на входных данных (текущий раздел);\n3. Преобразовать полученные предсказанные моделью значения на основе порогов.\n\nС помощью порогов мы можем определить, к какому классу относится предсказанное моделью значение.\n\nНапример, для текущей задачи нужно отнести значение к одному из 4 классов. Пусть пороговыми значениями будет массив \\[0.5, 1.5, 2.5\\]. То есть, если регриссионная модель выдаёт результаты в диапозоне от 0 до +∞, то:\n\n- значение из \\[0,   0.5\\] — принадлежит к классу 0;\n- значение из \\[0.5, 1.5\\] — принадлежит к классу 1;\n- значение из \\[1.5, 2.5\\] — принадлежит к классу 2;\n- значение из \\[2.5, +∞\\]  — принадлежит к классу 3.\n\nБолее подробно про нахождение и оптимизацию этих коэффициентов будет описано в разделе с предсказанием результатов.\n\nНо сначала нужно обучить модель. Поэтому зададим набор параметров и выделим столбец с решениями для обучения модели.\n"},{"metadata":{"id":"1HQoNaHtG4Bd","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Define parameters.\nparams = {\n    \"n_estimators\": 2000,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"subsample\": 0.75,\n    \"subsample_freq\": 1,\n    \"learning_rate\": 0.04,\n    \"feature_fraction\": 0.9,\n    \"max_depth\": 15,\n    \"lambda_l1\": 1,\n    \"lambda_l2\": 1,\n    \"verbose\": 100,\n    \"early_stopping_rounds\": 100,\n    \"eval_metric\": \"cappa\"\n}","execution_count":null,"outputs":[]},{"metadata":{"id":"qOWsDIQqG4Bf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Define training target.\ny = reduce_train[\"accuracy_group\"]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nTac_aTYx7_u","colab_type":"text"},"cell_type":"markdown","source":"Разделим исходную выборку на тестовую и тренировочную для обучения модели. Также укажем, какие столбцы модель должна игнорировать в процессе обучения. Это необходимо, поскольку мы не дробим исходные данные ввиду их большого объёма. В противном случае, дробление данных может привести к просадкам производительности и чрезмерному потреблению памяти.\n"},{"metadata":{"id":"TaPEuZpdG4Bg","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Split data on test and train set.\nn_fold = 5\nfolds = GroupKFold(n_splits=n_fold)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PF64EsePG4Bh","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Define colums to drop.\ncols_to_drop = [\n    \"game_session\", \"installation_id\", \"timestamp\",\n    \"accuracy_group\", \"timestampDate\"\n]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HGEoxvf4y_47","colab_type":"text"},"cell_type":"markdown","source":"Наконец, создадим модель, настроим конвеер данных (data pipeline) и обучим модель. В качестве используемой модели был выбран градиентный бустинг из пакета `LightGBM`, использующий алгоритмы обучения на основе деревьев решений.\n"},{"metadata":{"id":"O3UqKUpcG4Bj","colab_type":"code","outputId":"8fe13333-1281-4268-e71c-6038bb699e58","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"# Train model.\nmt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {\n    \"ft\": ft\n}\n\nregressor_model = RegressorModel(\n    model_wrapper=LGBWrapper()\n)\n\nregressor_model.fit(\n    X=reduce_train,\n    y=y,\n    folds=folds,\n    params=params,\n    preprocesser=mt,\n    transformers=transformers,\n    eval_metric=\"cappa\",\n    cols_to_drop=cols_to_drop\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"bXm_O8o0G4Bk","colab_type":"text"},"cell_type":"markdown","source":"## Использование обученной модели для предсказания результатов\n\n---\n"},{"metadata":{"id":"m2BuvdtouWWt","colab_type":"text"},"cell_type":"markdown","source":"Теперь нам нужно вычислить пороговые значения для конвертации предсказанных результатов регрессионной модели. Введём дополнительный класс для нахождения оптимальных коэффициентов. Для оптимизации использовался метод Нелдера—Мида (симплекс-метод), поскольку он не использует производной (точнее — градиентов) функции, а поэтому легко применим к негладким и/или зашумлённым функциям.\n"},{"metadata":{"id":"On8YZJAWG4Bl","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class OptimizedRounder:\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n\n    def coefficients(self):\n        return self.coef_[\"x\"]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Y9UaF3jL9dzb","colab_type":"text"},"cell_type":"markdown","source":"Используем обученную модель на подвыборке из тренировочных данных, которые мы отложили несколькими этапами ранее. На данном этапе мы вычисляем оптимальные пороговые значения для конвертации предсказанных результатов регрессионной модели.\n"},{"metadata":{"id":"AEGJFnGwG4Bn","colab_type":"code","outputId":"0637b444-95f6-4a6b-d736-711f7276caf5","colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"code","source":"%%time\n\n# Make predictions on for marked data.\npr = regressor_model.predict(reduce_train)\n\noptR = OptimizedRounder()\noptR.fit(pr.reshape(-1,), y)\ncoefficients = optR.coefficients()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5gvCdS5A92fF","colab_type":"text"},"cell_type":"markdown","source":"Выводим итоговые результаты метрики на тренировочной выборке.\n"},{"metadata":{"id":"cH2KDE3sG4Bp","colab_type":"code","outputId":"2f1a2ef3-e9ff-4309-9fbe-0f930c85ba80","colab":{"base_uri":"https://localhost:8080/","height":35},"trusted":true},"cell_type":"code","source":"# Check metric value.\nopt_preds = optR.predict(pr.reshape(-1, ), coefficients)\nqwk(y, opt_preds)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2aEqqOd_9-Nk","colab_type":"text"},"cell_type":"markdown","source":"Используем обученную модель вместе с оптимальными пороговыми значениями на выборке из тестовых данных. Сохраняем полученные предскзанные результаты в итоговый файл.\n"},{"metadata":{"id":"7_Kda_H5G4Br","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Make predictions for test data without answers.\npr = regressor_model.predict(reduce_test)\npr = round_values(pr)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fodWl4nAG4Bs","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Save predictions to file with appropriate format.\nsample_submission = read_result.sample_submission\n\nsample_submission[\"accuracy_group\"] = pr.astype(int)\nsample_submission.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"FmGJcode3eVU","colab_type":"code","outputId":"8aa5a8a6-93ba-4ad2-df06-06c249291222","colab":{"base_uri":"https://localhost:8080/","height":355},"trusted":true},"cell_type":"code","source":"sample_submission.head(n=10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"LhVJ_G0FG4Bt","colab_type":"code","outputId":"de908439-20a9-48e1-febb-4853278117db","colab":{"base_uri":"https://localhost:8080/","height":104},"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'].value_counts(normalize=True)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"IntelligentSystems.ipynb","provenance":[],"collapsed_sections":["qiqxKbWxU_D0"],"toc_visible":true}},"nbformat":4,"nbformat_minor":1}