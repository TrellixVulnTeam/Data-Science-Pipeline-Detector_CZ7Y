{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier, Pool\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nspecs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# encode title"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train title값과 test title값의 합집합을 구하기 위해 set()을 이용하여 index 추출\nlist_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\nlist_of_user_activities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train event_code값과 test event_code값의 합집합을 구하기 위해 set()을 이용하여 index 추출\nlist_of_event_code = list(set(train['event_code'].value_counts().index).union(set(test['event_code'].value_counts().index)))\nlist_of_event_code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zip은 동일한 개수로 이루어진 자료형을 묶어주는 역할을 하는 함수이다.\n# title값에 번호를 매겨줌\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\nactivities_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 넘버링된 타이틀을 train, test에 mapping 시켜줌\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bird Measurer를 제외한 나머지 들은 4100, Bird Measurer는 4110으로 ..\nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set = False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    '''\n    user_sample은 train, test의 데이터 프레임이고, 여기서만 installation_id가 필터링됨\n    test_set = False일때 test_set 파라미터는 labels processing과 관련있음\n    '''\n    \n    #새로운 변수: 각 activitiy 시간들..\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # i : game_session, session = frame\n    for i, session in user_sample.groupby('game_session', sort = False):\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # 현재 세션 시간을 초단위로 가져옴\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n       \n        # 각 assessment,그리고 아래와 같은 오프세션에서만 아래의 특징들이 처리되고, 레지스터를 생성함\n        if (session_type =='Assessment') & (test_set or len(session)>1):\n            # 평가를 나타내는 event_code의 4100을 찾습니다\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # 승, 패 수를 샌다\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            # user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            features['session_title'] = session['title'].iloc[0]\n            \n            #아래 4줄은 게임유저가 시도했었던 기록들(attempts)을 더해줍니다.\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n            \n            # 앱 사용시간 입니다\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1,2] - session.iloc[0,2]).seconds)\n            \n            # 정확도는 항상 승리하는것을 모든 시도로 나눈 값이다.\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter >0 else 0\n            accuracy = true_attempts/(true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            \n            # 플에이어가 어느 accuracy 그룹에 속해져있는지 count한것이 현재 accuracy로 분류되어져있다\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # 플레이어의 정확도 그룹 평균\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter >0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            \n            #플레이어는 얼마나 많은 엑션을 취했는가? , 그것은 0으로 초기화되고 아래 라인에서 업데이트된다\n            features['accumulated_actions'] = accumulated_actions\n            \n            #test set인경우 dataset에 feature들이 삽입될수있는 조건들이 있고, 모든 session은 train인 final datasets에 속하며\n            \n            #다음과같은 클라우슬을 거쳐야함: session.query(f'event_code == {win_code[session_title]}'), 즉 event_code에 4100 or 4110이 존재해야함\n            \n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts >0:\n                all_assessments.append(features)\n            \n            counter += 1\n        \n           #지금까지 각 event_code에서 얼마나 많이 action이 세어졌는지 보겠습니다\n        n_of_event_codes = Counter(session['event_code'])\n        \n        # 동일한 이름의 feature에서 사용되어진 actions들이 얼마나 많은지 세어보겠습니다.\n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n            \n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activity = session_type\n        # test_set이 아닐경우 이전에 스크랩되어진 마지막 assessment만이 예측됨\n        if test_set:\n            return all_assessments[-1]\n        return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compiled_data = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n    # user_sample is a DataFrame that contains only one installation_id\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"아래 features들은 제가 만든것들입니다. 각 event들은 앞서 진행된다는것에 유의하자, 예를들어 첫번째 row는 assessment전을 보여준다 -> player가 3번 clip을 보고 3먼 activities를 하고, 4 game을 플레이한다음 0번의 assessment를 달성.. 등등.."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\nnew_train[:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 다음 list는 포괄적으로 입력 데이터 집합 X를 제외한 accuracy_group에 사용될 feature list를 생성하며, 이것은 라벨 y로 할것입니다\nall_features = [x for x in new_train.columns if x not in ['accuracy_group']]\n\n# cat_features는 모델에 잘 훈련될수있도록 파라미터들을 선언해야함\ncat_features = ['session_title']\n\n# X와 y값을 나눔\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train\n\nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델과 파라미터를 만드는 함수를 만들었습니다\n# 다른파라미터는 아래 documentation을 참조하였습니다.\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier(iterations = 5000):\n    clf = CatBoostClassifier(loss_function = 'MultiClass',\n                            eval_metric = 'WKappa',\n                            task_type = 'CPU',\n                            #learning_rate = 0.01,\n                            iterations = iterations,\n                            od_type = 'Iter',\n                            depth = 8,\n                            early_stopping_rounds = 500,\n                            random_seed = 2019,\n                            use_best_model=True,\n                            border_count = 128,\n                            \n                            )\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n# oof는 input dataset과 동일한 크기의 0행렬입니다\noof = np.zeros(len(X))\nNFOLDS = 5\n# KFold class는 n 개의 다른 training, validation sets를 나눠줍니다\n# 이 기술은 모델의 오버피팅을 방지하고 보이지않는 데이터를 잘 학습할수 있도록 사용된다. \n#split/fold의 수가 많을수록 test에 무작위성으로 영향을 끼치는것이 줄어듭니다\nfolds = KFold(n_splits = NFOLDS, shuffle = True, random_state = 2019)\ntraining_start_time = time()\nmodels = []\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X,y)):\n    # folds.split의 각 iteration은 새로운 training data와 validation data의 index array를 반환한다\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # 모델 생성\n    clf = make_classifier()\n    # 전체 데이터에서 선택되어진 index와 사용된 features들을 .loc를 이용하여 모델에 적합하게 해줌\n    clf.fit(X.loc[trn_idx, all_features], y.loc[trn_idx], eval_set = (X.loc[test_idx, all_features], y.loc[test_idx]), use_best_model = True, verbose = 500, cat_features = cat_features)\n    # oof 행렬에 split된 각 에측값을 넣어줌\n    oof[test_idx] = clf.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n    models.append(clf)\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds = time() - start_time))))\n    print('____________________________________________________________________________________________\\n')\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(y, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp': clf.feature_importances_, 'col':X.columns})\nfea_imp.set_index(['col'], inplace = True)\nfea_imp = fea_imp.sort_values(['imp','col'], ascending = True,)\nfea_imp.plot(kind = 'barh', figsize = (15,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n    a = get_data(user_sample, test_set = True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\nfor model in models:\n    predictions.append(model.predict(X_test))\npredictions = np.concatenate(predictions, axis = 1)\nprint(predictions.shape)\npredictions = stats.mode(predictions, axis =1)[0].reshape(-1)\nprint(predictions.shape)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'] = np.round(predictions).astype('int')\nsubmission.to_csv('submission.csv', index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}