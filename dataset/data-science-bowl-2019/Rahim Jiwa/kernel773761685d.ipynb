{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro"},{"metadata":{},"cell_type":"markdown","source":"This challenge was attempted a ML project done for UofT SCS Machine Learning. This is only the kernel used for submission, with the best performing algorithm after hyperparameter tuning. I utilized: https://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline and https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model using both code and their work as a reference for this competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial Imports\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as plt\n%matplotlib inline\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# load the data\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  From Erik Bruin on Kaggle\n# filter to only include installation_ids that have taken an assessment\nkeep_id = train[train.type == 'Assessment'][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on='installation_id', how='inner')\ndel keep_id\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['installation_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reassign the column with a conversion of its value to dateitme\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the event_data."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_data'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like they all have different fields within the JSON."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_data'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_data'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_data'][3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_data'][4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['event_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of event_codes, many which are not clear as to what they do mean. We do know, that event_code 2000 is treated as a start. 4100, is the code for an assessment attempt for all assessments other than Bird Measurer, which records the assessment attempt in 4110."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['timestamp'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['world'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Labels"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['installation_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of unique installation ids in train_labels\nunique_ids_train_labels = list(train_labels['installation_id'].unique())\n# filter the train dataset for values whose installation_id appears in train_labels\ntrain = train[train['installation_id'].isin(unique_ids_train_labels)]\n# delete unique_ids_train_labels to save memory\ndel unique_ids_train_labels\n# check the number of unique installation_ids in train\ntrain['installation_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['game_session'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['installation_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"installation_id_attempts = train_labels['installation_id'].value_counts()\ninstallation_id_attempts.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['accuracy_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the test data."},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The testing data looks the same as the training data. This means we will have to perform similar operations as the training data like adjusting the timestamp and extracting the JSON. The JSON extraction will happen later in the preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# deal with timestamp\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['installation_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test.type == 'Assessment'][['installation_id']].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Submission"},{"metadata":{},"cell_type":"markdown","source":"The question here, is this file useful at all or is merely for output format. Can it be used in evaluating our model? It is unlikely, but worth checking."},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# how many installation_ids are unique\nsample_submission['installation_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# are the installation_ids in test and sample_submission the same\nlen(set.intersection(set(sample_submission['installation_id']), set(test['installation_id'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# value_counts of accuracy_group\nsample_submission['accuracy_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Specs"},{"metadata":{"trusted":false},"cell_type":"code","source":"specs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs['info'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs['info'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs['args'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"specs['args'][1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The specs file does not really tell us anything. This can be deleted."},{"metadata":{"trusted":false},"cell_type":"code","source":"# delete specs\ndel specs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Here, I am going to use code written by another Kaggler in order to preprocess the data. The code goes through and extract the event_data, by iterating through every observation. It gets the: event_data, activities counts, their accuracy_group, accumulated accuracy information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Erik Bruin\n#Credits go to Massoud Hosseinali\n\n# encode title\n# make a list with all the unique 'titles' from the train and test set\nlist_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\n# make a list with all the unique 'event_code' from the train and test set\nlist_of_event_code = list(set(train['event_code'].value_counts().index).union(set(test['event_code'].value_counts().index)))\n# create a dictionary numerating the titles\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n\n# replace the text titles withing the number titles from the dict\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)\n\n# I didnt undestud why, but this one makes a dict where the value of each element is 4100 \nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110\n\n# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Erik Bruin\n#Credits go to Massoud Hosseinali\n# here the get_data function is applyed to each installation_id and added to the compile_data list\ncompiled_data = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique()):\n    # user_sample is a DataFrame that contains only one installation_id\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Erik Bruin\n#Credits go to Massoud Hosseinali\n\n# the compiled_data is converted to DataFrame and deleted to save memmory\nnew_train = pd.DataFrame(compiled_data)\n\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Erik Bruin\n# modified by rahim\nnew_test = []\n\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n\n    \nnew_test = pd.DataFrame(new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have preprocessed both our training data and our testing data. Looking at this, the new_train and new_test groups look at each assessment and include a record of all activity before each assessment. The code that was used in order to get this data, and get the information from the event_data, re-creates the labels for the dataset. Also the train_labels values have been recalculated and we no longer need to use this datafame. However, since we have done this, we need to separate the training columns from the training label. We also need to do the same for the test dataset, as we have put that dataframe through the same preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(new_train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the above list of columns, the column that needs to be removed (and be used as the label) is the 'accuracy_group'. The other features around here, represent accumulated attempts of previous assessments for the installation_id. The '0','1','2','3', features are a count of how many times the installation_id has previous achieved an accuracy fitting within the accuracy_group."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of the features\nfeatures = list(new_train.columns.values)\nfeatures.remove('accuracy_group')\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removes accuracy_group from the train data\nX_train = new_train[features]\n# create a variable to contain just the accuracy_group label of the train data\ny_train = new_train['accuracy_group']\n# remove accuracy_group from the test data\nX_test = new_test[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's just take a quick look at the variables we have created."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Predictions"},{"metadata":{},"cell_type":"markdown","source":"Now we need to generate our predictions for our X_test data to get our y_pred to submit to Kaggle. For this we will use the Gradient Boosting Classifier with the number of estimators = 100."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_gbc = GradientBoostingClassifier(random_state=42, n_estimators=100)\nclf_gbc.fit(X_train, y_train)\ny_pred = clf_gbc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(sample_submission['installation_id'])\ny_pred = pd.DataFrame({'accuracy_group':y_pred[:]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.join(y_pred)\nsubmission['accuracy_group'] = submission['accuracy_group'].astype(int)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}