{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport os\nimport csv\nimport json\nimport gc\nfrom functools import partial\nfrom pathlib import Path\nfrom multiprocessing import Pool\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\nfrom tqdm import tqdm_notebook\nfrom pandas_summary import DataFrameSummary\nimport seaborn as sns\nfrom pandas.io.json import json_normalize\nfrom hashlib import md5\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def compare_cat_frequency(df1, df2, column, df1_name='train', df2_name='test', top_n=25, figsize=(20, 16)):\n    fig, (ax, ax2) = plt.subplots(ncols=2, sharey=True, figsize=figsize)\n    \n    ax.yaxis.tick_right()\n\n    df1_values = df1[column].value_counts()[:25]\n    df1_values.plot.barh(ax=ax, title=f'{df1_name} {column} distribution')\n\n    test_df[column].value_counts()[df1_values.keys()].plot.barh(ax=ax2, title=f'{df2_name} {column} distribution')\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_grid_hist(df, columns, test_df=None, ncols=3, figsize=(20, 12)):\n    sns.set_palette(\"Spectral_r\")\n\n    fig, axes = plt.subplots(nrows=len(columns) // ncols, ncols=ncols, figsize=figsize)\n\n    count = 0\n    for ax_row in axes:\n        for ax in ax_row:\n            count += 1\n            try:\n                key = columns[count]\n                print(key)\n                ax.hist(df[key], label='Train', edgecolor='black', linewidth=0, bins=100, histtype='stepfilled', density=True)\n                if test_df is not None:\n                    ax.hist(test_df[key], label='Test', bins=100, linewidth=1, linestyle='dashed', alpha = 0.5, histtype='stepfilled', density=True)\n                    ax.legend()\n                ax.set_title(f'Distribution of {key}')\n            except IndexError:\n                continue","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# DSB 2019: EDA and data preparation\n\nThe goal of this notebook is to explore the dataset from the 2019 Data Science Bowl Kaggle competition and prepare it ready it for a sequence model.\n\n## Competition overview\n\nThe dataset is provided by the PBS Measure Up app and is a timeseries of a user's activity across game sessions. The goal is to predict how they will do at an evaluation based using the features.\n\nIt appears the organisers are looking to use the model to improve the games they develop and maybe give a user the best possible experience based on their performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = Path('/kaggle/input/data-science-bowl-2019/')\nOUTPUT_PATH = Path('/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data"},{"metadata":{},"cell_type":"markdown","source":"Based on some earlier analysis I've done, I'm setting the datatypes of a few fields to minimise memory consumption."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DTYPES = {\n    'event_count': np.uint16,\n    'event_code': np.uint16,\n    'game_type': np.uint32\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_df = pd.read_csv(\n    DATA_PATH/'train.csv', parse_dates=['timestamp'],\n    dtype=TRAIN_DTYPES\n).sort_values('timestamp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_df = pd.read_csv(\n    DATA_PATH/'test.csv',\n    parse_dates=['timestamp'],\n    dtype=TRAIN_DTYPES\n).sort_values('timestamp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Column overview\n\nI'll use `DataFrameSummary` from [pandas-summary](https://github.com/mouradmourafiq/pandas-summary) to get a high-level overview of the data we're working with."},{"metadata":{"trusted":true},"cell_type":"code","source":"DataFrameSummary(train_df).columns_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DataFrameSummary(test_df).columns_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, it appears that there are no missing values. However, the `event_data` is a JSON field, so I'm sure when that's expanded it will be a different story."},{"metadata":{},"cell_type":"markdown","source":"### timestamp distribution\n\nSince time seems to be a key feature in this dataset, it will be interesting to see how the train and test set differ in their time distribution.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"Spectral_r\")\n\nplt.figure(figsize=(14, 4))\nplt.title(\"timestamp frequency\")\n\nplt.hist(train_df.timestamp, edgecolor='black', linewidth=1.2, label='Train', histtype='stepfilled', density=True)\nplt.hist(test_df.timestamp, edgecolor='black', linewidth=1.2, linestyle='dashed', label='Test', alpha = 0.5, histtype='stepfilled', density=True)\n\nplt.xticks(rotation=70)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the train and test are roughly the same time period across a ~3 month window."},{"metadata":{},"cell_type":"markdown","source":"### event_time distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patheffects as pe\n\nsns.set_palette(\"RdBu_r\")\n\nplt.figure(figsize=(14, 4))\nplt.title(\"event_time frequency\")\nplt.hist(train_df.game_time, label='Train', bins=100, path_effects=[pe.Stroke(linewidth=2, foreground='black'), pe.Normal()])\nplt.hist(test_df.game_time, label='Test', bins=100, path_effects=[pe.Stroke(linewidth=2, foreground='black'), pe.Normal()])\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hard to see from this plot but it appears the event_time are mostly very short with some very long times."},{"metadata":{},"cell_type":"markdown","source":"### title distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_cat_frequency(train_df, test_df, column='title')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, titles played is quite similar across train and test set, albeit the train set has a lot more examples."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of other categorical fields"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['event_code', 'event_id', 'world', 'type']:\n    compare_cat_frequency(train_df, test_df, column=column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set categorical columns"},{"metadata":{},"cell_type":"markdown","source":"To save space, I'll use the Pandas categorical type for categorical columns. I'll concat the train and test datasets together to ensure the types have the full range of values."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = pd.concat([train_df, test_df], axis=0)\n\nWORLD_VALS = all_df.world.unique()\nTITLE_VALS = all_df.title.unique()\nTYPE_VALS = all_df.type.unique()\nEVENT_CODE = all_df.event_code.unique()\nEVENT_ID = all_df.event_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_categorical(df):\n    df.world = pd.Categorical(df.world, categories=WORLD_VALS)\n    df.title = pd.Categorical(df.title, categories=TITLE_VALS)\n    df.type = pd.Categorical(df.type, categories=TYPE_VALS)\n    df.event_code = pd.Categorical(df.event_code, categories=EVENT_CODE)\n    df.event_id = pd.Categorical(df.event_id, categories=EVENT_ID)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = set_categorical(train_df)\ntest_df = set_categorical(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del all_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Event data"},{"metadata":{},"cell_type":"markdown","source":"### Flattening"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The event data is in represented as JSON objects for each row. The first step to performing analysis on it will be to flatten it into a sparse matrix.\n\nSince it's a lot of information, I'm going to start by analysis a sample 100 thousands rows. Since the event ordering is essentially, I'm going to sample by taking a slice of the first 100k rows, not randomly sampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"def flatten_json(nested_json):\n    nested_json = json.loads(nested_json)\n\n    out = {}\n\n    def _flatten(x, name=''):\n        if type(x) is dict:\n            for a in x: _flatten(x[a], name + a + '_')\n        elif type(x) is list:\n            i = 0\n            for a in x:\n                _flatten(a, name + str(i) + '_')\n                i += 1\n        else:\n            out[name[:-1]] = x\n\n    _flatten(nested_json)\n\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_sample = train_df.sample(n=100_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_event_data_norm_sample = json_normalize(train_df_sample.event_data.progress_apply(flatten_json))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values"},{"metadata":{},"cell_type":"markdown","source":"Show percentage of missing values across each field."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_event_na_perc = (\n    train_event_data_norm_sample.isna().sum().sort_values() /\n     len(train_event_data_norm_sample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_event_na_perc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to limit the columns to just the ones that are in at least 5% of rows. There's a lot of very rare fields and I feel like they're going to need some feature engineering. That seems like something to be working on toward the end of the competition.\n\nSince `event_count`, `event_code` and `game_time` are already provided, that seems like a column that can be excluded too."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_include = train_event_na_perc[train_event_na_perc <= 0.95].keys()\ncolumns_to_include = [c for c in columns_to_include if c not in ('event_count', 'event_code', 'game_time')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DataFrameSummary(train_event_data_norm_sample[columns_to_include]).columns_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ed_summary = DataFrameSummary(train_event_data_norm_sample[columns_to_include]).summary(); ed_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of numeric types."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols = ed_summary.T[ed_summary.T.types == 'numeric'].index\npath_effects = [pe.Stroke(linewidth=1, foreground='black'), pe.Normal()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_grid_hist(train_event_data_norm_sample, columns=list(numeric_cols)[:16])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next up, I want to load the whole dataset joined to the `event_data` DataFrame, but I intend to convert some of the large columns to a lookup using the hash of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_include","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DESCRIPTIONS = []\nSOURCE_CATS = []\nIDENTIFIER_CATS = set([])\nMEDIA_TYPE_CATS = set([])\nCOORD_STAGE_HEIGHT = set([])\nCOORD_STAGE_WIDTH = set([])\n\n\ndef do_event_data(event_data: dict, output_file: str):\n    csv_file = open(f'{OUTPUT_PATH}/{output_file}', 'w')\n    csv_writer = csv.writer(csv_file, delimiter=',')\n\n    for data in tqdm(event_data.values, total=len(event_data)):\n        row_flattened = flatten_json(data)\n\n        # map description to its hash.\n        desc = row_flattened.get('description')\n        if desc:\n            if desc not in DESCRIPTIONS:\n                DESCRIPTIONS.append(desc)\n            row_flattened['description'] = DESCRIPTIONS.index(desc)\n            \n        source = row_flattened.get('source')\n        if source:\n            source = str(source).lower()\n            if source not in SOURCE_CATS:\n                SOURCE_CATS.append(source)\n            row_flattened['source'] = SOURCE_CATS.index(source)\n            \n        for col, l in [\n            ('identifier', IDENTIFIER_CATS),\n            ('media_type', MEDIA_TYPE_CATS),\n            ('coordinates_stage_height', COORD_STAGE_HEIGHT),\n            ('coordinates_stage_width', COORD_STAGE_WIDTH)\n        ]:\n            value = row_flattened.get(col)\n            if value: l.add(value)\n\n        csv_writer.writerow(row_flattened.get(k, None) for k in columns_to_include)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_event_data(train_df.event_data, 'train_event_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_event_data(test_df.event_data, 'test_event_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtypes = dict(\n    source=pd.CategoricalDtype(list(range(len(SOURCE_CATS)))),\n    media_type=pd.CategoricalDtype(MEDIA_TYPE_CATS),\n    identifier=pd.CategoricalDtype(IDENTIFIER_CATS),\n    description=pd.CategoricalDtype(list(range(len(DESCRIPTIONS)))),\n    coordinates_stage_height=pd.CategoricalDtype(list(range(len(COORD_STAGE_HEIGHT)))),\n    coordinates_stage_width=pd.CategoricalDtype(list(range(len(COORD_STAGE_WIDTH))))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_event_data = pd.read_csv(\n    OUTPUT_PATH/'train_event_data.csv', names=columns_to_include, header=None, dtype=dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_event_data = pd.read_csv(OUTPUT_PATH/'test_event_data.csv', names=columns_to_include, header=None, dtype=dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols_revised = ['round', 'coordinates_x', 'coordinates_y', 'duration', 'total_duration', 'level', 'size', 'weight']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_grid_hist(\n    train_event_data.sample(n=500_000),\n    test_df=test_event_data,\n    columns=list(numeric_cols_revised),\n    ncols=2, figsize=(16, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def join_event_data(df, df_event):\n    return pd.concat([\n        df[[i for i in df.columns if i != 'event_data']].reset_index(drop=True),\n        df_event.reset_index(drop=True)], axis=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_comb = join_event_data(train_df, train_event_data)\ntest_df_comb = join_event_data(test_df, test_event_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last thing I want to do is sort by `installation_id` and `timestamp` which should make convert the data into a sequence nice and easy in future."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_comb = train_df_comb.sort_values(['installation_id', 'timestamp']).reset_index(drop=True)\ntest_df_comb = test_df_comb.sort_values(['installation_id', 'timestamp']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_comb.to_feather(OUTPUT_PATH/'train.fth')\ntest_df_comb.to_feather(OUTPUT_PATH/'test.fth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df_comb\ndel test_df_comb\ndel train_df\ndel test_df\ndel train_event_data\ndel test_event_data\ndel train_event_data_norm_sample\ndel train_df_sample\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add labels"},{"metadata":{},"cell_type":"markdown","source":"I'll add a column for the assessment titles and use it to understand how many assessments happen per `installation_id`.\n\nBased on the following bit of information: \"Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_comb = pd.read_feather(OUTPUT_PATH/'train.fth')\ntest_df_comb = pd.read_feather(OUTPUT_PATH/'test.fth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(DATA_PATH/'train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, it sounds like we want to calculate `num_correct`, `num_incorect`, `accuracy` and use that to set the `accuracy_group`.\n\nI'll start by adding an assessment column to determine if the activity is an assessment."},{"metadata":{},"cell_type":"markdown","source":"I'll start by adding a `attempt` column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# thanks to https://www.kaggle.com/artgor/oop-approach-to-fe-and-models\n\ndef set_attempt_label(df):\n    df['attempt'] = 0\n    df.loc[\n        (df['title'] == 'Bird Measurer (Assessment)') &\n        (df['event_code'] == 4110), 'attempt'] = 1\n\n    df.loc[\n        (df['title'] != 'Bird Measurer (Assessment)') &\n        (df['event_code'] == 4100) & (df['type'] == 'Assessment'), 'attempt'] = 1\n\n    return df\n    \ntrain_df_comb = set_attempt_label(train_df_comb)\ntest_df_comb = set_attempt_label(test_df_comb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's group by game session and installation id for assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy_group(row):\n    if row.correct == 0:\n        return 0\n    \n    if row.attempt > 2:\n        return 1\n    \n    if row.attempt == 2:\n        return 2\n    \n    if row.attempt == 1:\n        return 3\n\n\ndef get_labels(df):\n    num_correct = df[df.attempt == 1].groupby(['game_session', 'installation_id']).correct.sum().astype(int)\n    num_attempts = df[df.attempt == 1].groupby(['game_session', 'installation_id']).attempt.sum().astype(int)\n    titles = df[df.attempt == 1].groupby(['game_session', 'installation_id']).title.agg(lambda x: x.iloc[0])\n    labels_joined = num_correct.to_frame().join(num_attempts).join(titles).reset_index()\n    labels_joined['accuracy_group'] = labels_joined.apply(get_accuracy_group, axis=1)\n    return labels_joined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_joined = get_labels(train_df_comb)\ntest_labels_joined = get_labels(test_df_comb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_joined.accuracy_group.value_counts().plot.bar(title='Train labels dist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels_joined.accuracy_group.value_counts().plot.bar(title='Test labels dist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Storing sequence start and end positions"},{"metadata":{},"cell_type":"markdown","source":"In order to build a disk space efficient sequence model, I'm going to store the start and end position of each sequence, based on the assumption that the train and test data is sorted by `installation_id` and `timestamp`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _do_installation_id(inp, df):\n    (installation_id, row) = inp\n\n    game_sessions = row.game_session.unique()\n\n    filtered_rows = df[df.installation_id == installation_id]\n\n    start_idx = filtered_rows.head(1).index[0]\n\n    output = []\n    for game_session in game_sessions:\n        assessment_row = filtered_rows[(filtered_rows.game_session == game_session) & (filtered_rows.event_code == 2000)]\n        output.append((installation_id, game_session, start_idx, assessment_row.index[0]))\n\n    return output\n\n\ndef add_start_and_end_pos(labels, df):\n    labels_grouped = labels.groupby('installation_id')\n    \n    labels['start_idx'] = -1\n    labels['end_idx'] = -1\n    \n    for row in tqdm(labels_grouped, total=len(labels_grouped)):\n        results = _do_installation_id(row, df=df)\n\n        for (installation_id, game_session, start_pos, end_pos) in results:\n            filt = (labels.installation_id == installation_id) & (labels.game_session == game_session)\n\n            labels.loc[filt, 'start_idx'] = start_pos\n            labels.loc[filt, 'end_idx'] = end_pos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_start_and_end_pos(train_labels_joined, train_df_comb)\nadd_start_and_end_pos(test_labels_joined, test_df_comb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_joined.to_feather(OUTPUT_PATH/'train_labels.fth')\ntest_labels_joined.to_feather(OUTPUT_PATH/'test_labels.fth')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}