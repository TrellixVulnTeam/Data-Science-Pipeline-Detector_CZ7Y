{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages = train_labels.groupby('title')['accuracy_group'].agg(['median','count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = {'Bird Measurer (Assessment)':1,\n       'Cart Balancer (Assessment)': 3,\n       'Cauldron Filler (Assessment)':3,\n       'Chest Sorter (Assessment)': 0,\n       'Mushroom Sorter (Assessment)':3\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission1 = sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission1['accuracy_group'] = test.groupby('installation_id').last().title.map(ans).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission1.to_csv('submission_0.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_in_sub(test):\n    tgms = test.groupby('installation_id').last().game_session\n    tgms1 = tgms.reset_index()\n    test_ass = test[test.type == \"Assessment\"]\n    tgms1[\"title\"] = str(test_ass[test_ass.game_session == tgms1[\"game_session\"][0]].title.reset_index(drop=True)[0])\n    \n    for i in range(0,len(tgms1)):\n        tgms1[\"title\"][i] = str(test_ass[test_ass.game_session==tgms1[\"game_session\"][i]].title.reset_index(drop=True)[0])\n    return tgms1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def c_accuracy_group(df):\n    df[\"accuracy_group\"]=0\n    for i in range(0,len(df)):\n        acc = float(df[\"accuracy\"][i])\n        if (acc == float(0)):\n            df[\"accuracy_group\"][i]=0\n        elif (acc < float(0.5)):\n            df[\"accuracy_group\"][i]=1\n        elif (acc < float(1)):\n            df[\"accuracy_group\"][i]=2\n        elif (acc == float(1)):\n            df[\"accuracy_group\"][i]=3\n        else:\n            df[\"accuracy_group\"][i] = None\n    return df\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_to_label(test):\n    print(\"Converting to label format, as of submissions done in assessment\")\n    test_ass = test[test.type == \"Assessment\"]\n    test_ass_sub = test_ass[(((test.event_code == 4100) & (test.title != 'Bird Measurer (Assessment)'))) | (((test.event_code == 4110) & (test.title == 'Bird Measurer (Assessment)')))]\n    test_ass_sub_inf = test_ass_sub[[\"installation_id\",\"game_session\",\"timestamp\",\"title\",\"event_data\"]]\n    test_ass_sub_inf0 = test_ass_sub_inf\n    test_ass_sub_inf0[\"correct\"] = 0\n    test_ass_sub_inf0[\"incorrect\"] = 0\n    \n    for i in range(0,len(test_ass_sub_inf0)):\n        if \"\\\"correct\\\":true\" in test_ass_sub_inf0[\"event_data\"][test_ass_sub_inf0.index[i]]:\n            test_ass_sub_inf0[\"correct\"][test_ass_sub_inf0.index[i]] = 1\n        else:\n            test_ass_sub_inf0[\"incorrect\"][test_ass_sub_inf0.index[i]] = 1\n    test_ass_sub_inf1 = test_ass_sub_inf0.groupby(by=[\"installation_id\",\"game_session\",\"title\"],sort=False).sum()\n    test_ass_sub_inf2 = test_ass_sub_inf1\n    test_ass_sub_inf2 = test_ass_sub_inf2.reset_index()\n    test_ass_sub_inf2[\"accuracy\"] =float(0)\n    \n    for i in range(0,len(test_ass_sub_inf2)):\n        corr = test_ass_sub_inf2[\"correct\"][i]\n        incor = test_ass_sub_inf2[\"incorrect\"][i]\n        test_ass_sub_inf2[\"accuracy\"][i] = float(corr)/(incor+corr)\n    \n    test_ass_sub_inf3 = test_ass_sub_inf2\n    test_ass_sub_inf3 = c_accuracy_group(test_ass_sub_inf3)\n    return test_ass_sub_inf3\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_time_gm(train, train_labels):\n    print(\"Adding cumulative game played time for each session\")\n    train_data_1 = train[[\"installation_id\", \"game_session\", \"title\",\"game_time\"]]\n    train_time_god_2 = train[[\"installation_id\", \"game_session\", \"title\",\"game_time\"]]\n    ttg_time = train_time_god_2.groupby(by=['game_session'], sort=False).last().game_time.reset_index()\n    ttg_time0 = train_time_god_2[[\"installation_id\", \"game_session\", \"title\"]].merge(ttg_time, on = 'game_session', how = 'left')\n    ttg_time00 = ttg_time0.groupby(by=['installation_id','game_session'], sort=False).sum().groupby(level=[0]).cumsum()\n    ttg_time1 = ttg_time00.reset_index()\n    ttg_time2 = ttg_time1[[\"game_session\",\"game_time\"]]\n    train_labels1 = train_labels\n    # join train with train labels\n    train_labels_t = train_labels1.merge(ttg_time2, on = 'game_session', how = 'left')\n\n    return train_labels_t\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_final_feat(train, train_labels_derive_time):   \n    # train, train_labels_derive_time\n    print(\"Adding more time sensitive features such as: all correct, all incorrect, score, average score, corr to incorr ratio and vice versa until any game session\")\n    \n    train_edit_c = train[[\"installation_id\", \"game_session\", \"title\", \"event_data\"]]\n    trc_ic = train_edit_c[(train_edit_c.event_data.str.contains(\"\\\"correct\\\":true\") | train_edit_c.event_data.str.contains(\"\\\"correct\\\":false\")) ]\n    trc_ic1 = trc_ic.groupby([\"installation_id\", \"game_session\", \"title\"]).size().reset_index().drop(columns = [0])\n    \n    trc = train_edit_c[train_edit_c.event_data.str.contains(\"\\\"correct\\\":true\")]\n    trc_edit = trc[[\"installation_id\",\"game_session\"]]\n    trc_edit[\"correct_all\"] = 1\n    trc_edit_all1 = trc_edit.groupby(by=['installation_id','game_session'], sort = False).sum().groupby(level=[0]).cumsum()\n    trc_edit_all1 = trc_edit_all1.reset_index()\n    \n    tric = train_edit_c[train_edit_c.event_data.str.contains(\"\\\"correct\\\":false\")]\n    tric_edit = tric[[\"installation_id\",\"game_session\"]]\n    tric_edit[\"incorrect_all\"] = 1\n    tric_edit_all1 = tric_edit.groupby(by=['installation_id','game_session'], sort=False).sum().groupby(level=[0]).cumsum()\n    tric_edit_all1 = tric_edit_all1.reset_index()\n    \n    print(\"Adding correct all and incorrect all feature, later we might wanna add specific accuracy groups of titles/assesssments, to record history of gameplay of user\")\n    # join train with train labels\n    train_c_1 = trc_ic1.merge(trc_edit_all1[[\"game_session\",\"correct_all\"]], on = 'game_session', how = 'left')\n    train_c_2 = train_c_1.merge(tric_edit_all1[[\"game_session\",\"incorrect_all\"]], on = 'game_session', how = 'left')\n    \n    # join train with train labels\n    train_c_2[\"correct_all\"].fillna(0, inplace=True)\n    train_c_2[\"incorrect_all\"].fillna(0, inplace=True)\n    to_get_acc = train_c_2 # contains all the gms with either true or false\n    \n    print(\"Adding score and score count\")\n    to_get_acc1 = to_get_acc\n    \n    to_get_acc1[\"score\"] = 0.000001\n    to_get_acc1[\"score_c\"] = 0\n   # to_get_acc1[\"acc_r\"] = 0.000001\n    #to_get_acc1[\"inacc_r\"] = 0.000001\n    \n   \n    for i in range(0,len(to_get_acc1)):\n        acc = to_get_acc1[\"correct_all\"][i]\n        ina = to_get_acc1[\"incorrect_all\"][i]\n        if((acc == 0) and (ina) == 0):\n            to_get_acc1[\"score_c\"][i] = 0\n            to_get_acc1[\"score\"][i] = 0\n         #   to_get_acc1[\"acc_r\"][i] = 0\n         #   to_get_acc1[\"inacc_r\"][i] = 0\n            continue\n        elif(acc == 0):\n            to_get_acc1[\"score\"][i] = round(float(ina),3)*(-5)\n            to_get_acc1[\"score_c\"][i] = 1\n            #to_get_acc1[\"acc_r\"][i] = 0\n          #  to_get_acc1[\"inacc_r\"][i] = ina\n        elif(ina == 0):\n            to_get_acc1[\"score\"][i] = round(float(acc),3)*(5)\n            to_get_acc1[\"score_c\"][i] = 1\n          #  to_get_acc1[\"acc_r\"][i] = acc\n          #  to_get_acc1[\"inacc_r\"][i] = 0\n        elif((ina != 0) and (acc != 0)):\n            to_get_acc1[\"score\"][i] = round((float(acc)),3)*3-round((float(ina)),3)*1\n            to_get_acc1[\"score_c\"][i] = 1\n           # to_get_acc1[\"acc_r\"][i] = round(float(acc)/ina,3)\n           # to_get_acc1[\"inacc_r\"][i] = round(float(ina)/acc,3)\n            \n    #to_get_acc1\n    \n    train_copy = train[[\"installation_id\", \"game_session\", \"title\"]].groupby(by = [\"installation_id\",\"game_session\",\"title\"], sort=False).size().reset_index()\n    # join train with train labels\n    train_t_1 = train_copy.drop(columns=[0]).merge(to_get_acc1[[\"game_session\",\"correct_all\", \"incorrect_all\", \"score\", \"score_c\"]], on = 'game_session', how = 'left')\n    train_t_2 = train_t_1\n    train_t_2[\"correct_all\"].fillna(0, inplace=True)\n    train_t_2[\"incorrect_all\"].fillna(0, inplace=True)\n    train_t_2[\"score\"].fillna(0, inplace=True)\n    train_t_2[\"score_c\"].fillna(0, inplace=True)\n    \n    train_t_3 = train_t_2\n    train_t_3 = train_t_3.groupby(by=['installation_id','game_session','title'],sort=False).sum().groupby(level=[0]).cumsum()\n    train_t_3 = train_t_3.reset_index()\n    \n    print(\"Adding average score\")\n    train_t_4 = train_t_3\n    # now count average score till that point, acc_r, inacc_r\n    train_t_4[\"average_score\"] = float(0)\n    train_t_4[\"acc_r\"] = float(0)\n    train_t_4[\"inacc_r\"] = float(0)\n    for i in range(0,len(train_t_4)):\n        acc = train_t_4[\"correct_all\"][i]\n        inacc = train_t_4[\"incorrect_all\"][i]\n        score = round(float(train_t_4[\"score\"][i]))\n        count = train_t_4[\"score_c\"][i]\n        if (count!=0):\n            train_t_4[\"average_score\"][i] = round(float(score)/count,3)\n        else:\n            train_t_4[\"average_score\"][i] = 0\n        if((inacc == 0)&(acc == 0)):\n            train_t_4[\"acc_r\"][i] = 0\n            train_t_4[\"inacc_r\"][i] = 0\n        elif(inacc == 0):\n            train_t_4[\"acc_r\"][i] = acc\n            train_t_4[\"inacc_r\"][i] = 0\n        elif(acc == 0):\n            train_t_4[\"acc_r\"][i] = 0\n            train_t_4[\"inacc_r\"][i] = inacc\n        elif((inacc != 0) & (acc != 0)):\n            train_t_4[\"acc_r\"][i] = round(float(acc)/inacc,3)\n            train_t_4[\"inacc_r\"][i] = round(float(inacc)/acc,3)\n                \n    train_t_5 = train_t_4        \n    \n    print(\"Almost done, combining all into label format\")\n    # join train with train labels\n    train_labels_derive_time_corr = train_labels_derive_time.merge(train_t_5[[\"game_session\",\"correct_all\",\"incorrect_all\",\"score\",\"score_c\",\"average_score\",\"acc_r\",\"inacc_r\"]], on = 'game_session', how = 'left')\n    return train_labels_derive_time_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all(train):\n    train_labels_derive = test_to_label(train)\n    train_labels_derive_time = get_time_gm(train,train_labels_derive)\n    get = get_final_feat(train, train_labels_derive_time)\n    return get","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sub(test):\n    test_labels_derive = test_in_sub(test)\n    test_labels_derive_time = get_time_gm(test,test_labels_derive)\n    get = get_final_feat(test, test_labels_derive_time)\n    return get","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    %%time\n    get_train = get_all(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    %%time\n    get_test = get_all(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_sub_a = get_sub(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub_a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sub = test_sub_a.drop(columns = [\"game_session\", \"installation_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_train.to_csv(\"train_converted.csv\", index = None)\nget_test.to_csv(\"test_converted_train.csv\", index = None)\ntest_sub_a.to_csv(\"test_sub_converted.csv\", index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainA = get_train.drop(columns = [\"correct\", \"incorrect\", \"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_testA = get_test.drop(columns = [\"correct\", \"incorrect\", \"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_map = {\"Mushroom Sorter (Assessment)\":1,\"Bird Measurer (Assessment)\":2,\"Cauldron Filler (Assessment)\":3,\"Chest Sorter (Assessment)\":4,\"Cart Balancer (Assessment)\":5}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainB = get_trainA.drop(columns =[\"installation_id\",\"game_session\"])\nget_trainB['title'] = get_trainB['title'].map(labels_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainC = get_trainB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffling randomly for better variability and better chances of prediction, NOTE We havent normalized the distribution yet...!!!!\nget_trainD = get_trainC.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_testB = get_testA.drop(columns =[\"installation_id\",\"game_session\"])\nget_testB['title'] = get_testB['title'].map(labels_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_testC = get_testB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffling randomly for better variability and better chances of prediction, NOTE We havent normalized the distribution yet...!!!!\nget_testD = get_testC.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subA = get_sub\nget_subA['title'] = get_subA['title'].map(labels_map) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = get_trainD.drop(columns = [\"accuracy_group\"])\nY1 = get_trainD[\"accuracy_group\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nX1r = scaler.fit_transform(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test1 = pd.DataFrame(get_testD.drop(columns = [\"accuracy_group\"]))\nX_test1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y_test1 = pd.DataFrame([\"accuracy_group\"])\nY_test1 = get_testD[\"accuracy_group\"]\nY_test1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n#rms = sqrt(mean_squared_error(y_actual, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Report\n#from pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\nX_train = get_trainD.drop(columns = [\"accuracy_group\"])\nY_train = get_trainD[\"accuracy_group\"]\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, Y_train)\npredicted1 = model.predict(X_test1)\nreport1 = classification_report(Y_test1, predicted1)\nprint(report1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms1 = sqrt(mean_squared_error(Y_test1, predicted1))\nprint(rms1)\nprint(str(rms1**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noof2 = np.zeros(len(X_test1))\noof2 = model.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oof2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Report\n#from pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = get_trainD.drop(columns = [\"accuracy_group\"])\n#Y = get_trainD[\"accuracy_group\"]\ntest_size = 0.33\nseed = 7\n\nX_1 = get_trainD.drop(columns = [\"accuracy_group\"])\nY_1 = get_trainD[\"accuracy_group\"]\nX_train, X_test, Y_train, Y_test = train_test_split(X_1, Y_1, test_size=test_size,\nrandom_state=seed)\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(X_train, Y_train)\npredicted2 = model2.predict(X_test1)\n#from sklearn.metrics import classification_report\nreport2 = classification_report(Y_test1, predicted2)\nprint(report2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms2 = sqrt(mean_squared_error(Y_test1, predicted2))\nprint(rms2)\nprint(str(rms2**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noof1 = np.zeros(len(X_test1))\noof1 = modellgr.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oof1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Report\n#from pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n#filename = 'pima-indians-diabetes.data.csv'\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = read_csv(filename, names=names)\n#array = dataframe.values\n#X = get_trainD.drop(columns = [\"accuracy_group\"])\n#Y = get_trainD[\"accuracy_group\"]\n#test_size = 0.33\n#seed = 7\nmodellgrr = LogisticRegression()\nmodellgrr.fit(X1r, Y1)\npredicted2r = modellgr.predict(X_test1)\n#from sklearn.metrics import classification_report\nreportlgrr = classification_report(Y_test1, predicted2r)\nprint(reportlgrr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = [x for x in get_trainD.columns if x not in ['accuracy_group']]\n# this list comprehension create the list of features that will be used on the input dataset X\n# all but accuracy_group, that is the label y\n# this cat_feature must be declared to pass later as parameter to fit the model\ncat_features = ['title']\n# here the dataset select the features and split the input ant the labels\nXc, yc = get_trainD[all_features], get_trainD['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier():\n    clf = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                                # depth=8,\n                               early_stopping_rounds=500,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\n#oof = np.zeros(len(Xc))\n#oof[test_idx] = clf.predict(Xc.loc[test_idx, all_features]).reshape(len(test_idx))\n#print('OOF QWK:', qwk(yc, oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(Xc))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(Xc, yc)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf = make_classifier()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf.fit(Xc.loc[trn_idx, all_features],yc.loc[trn_idx], eval_set=(Xc.loc[test_idx, all_features], yc.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf.predict(Xc.loc[test_idx, all_features]).reshape(len(test_idx))\n    \n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(yc, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf1 = make_classifier()\nclf1.fit(Xc, yc, verbose=500, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted3 = clf1.predict(X_test1)\nfrom sklearn.metrics import classification_report\nreportcat = classification_report(Y_test1, predicted3)\nprint(reportcat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms3 = sqrt(mean_squared_error(Y_test1, predicted3))\nprint(rms3)\nprint(str(rms3**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test1.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noof3 = np.zeros(len(X_test1))\noof3 = clf.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oof3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noof3 = np.zeros(len(X_test1))\noof3 = clf1.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oof3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms2 = round(rms2,4)\nms2 = rms2**2\nprint(\"Logistic regression : \"+str(rms2)+\" , \"+str(ms2))\nrms1 = round(rms1,4)\nms1 = rms1**1\nprint(\"CART : \"+str(rms1)+\" , \"+str(ms1))\nrms3 = round(rms3,4)\nms3 = rms3**2\nprint(\"Cat booster : \"+str(rms3)+\" , \"+str(ms3)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD0 = get_trainD[get_trainD.accuracy_group == 0]\nget_trainD0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD1 = get_trainD[get_trainD.accuracy_group == 1]\nget_trainD1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD2 = get_trainD[get_trainD.accuracy_group == 2]\nget_trainD2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD3 = get_trainD[get_trainD.accuracy_group == 3]\nget_trainD3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Smallest is 2205, so let's extract 2205 entries from each accuracy group and combine them to make an unbiased dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_trainD0_s = get_trainD0.sample(n=2205,random_state = 1).reset_index(drop = True)\nget_trainD1_s = get_trainD1.sample(n=2205,random_state = 1).reset_index(drop = True)\nget_trainD2_s = get_trainD2.sample(frac = 1, random_state = 1).reset_index()\nget_trainD3_s = get_trainD3.sample(n=2205,random_state = 1).reset_index(drop = True)\ntrain_equal_data = pd.concat([get_trainD0_s,get_trainD1_s,get_trainD2_s,get_trainD3_s], ignore_index=True, sort =False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_equal_data1 = train_equal_data.drop(columns = [\"index\"]).sample(frac = 1, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_equal_data1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_equal_data1.groupby([\"accuracy_group\"]).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xa = train_equal_data1.drop(columns = [\"accuracy_group\"])\nXa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xa1 = Xa[[\"title\",\"average_score\",\"acc_r\",\"inacc_r\"]]\nXa1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ya = train_equal_data1[\"accuracy_group\"]\nya","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ya1 = ya","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NOW WE HAVE AN UNBIASED TOWARDS TARGET TRAIN DATA SET, LET'S CHECK ITS PERFORMANCE, WHETHER GOOD OR BAD"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier2():\n    clf1 = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=500,\n                               od_type=\"Iter\",\n                                # depth=8,\n                               early_stopping_rounds=100,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier2aa():\n    clf1 = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                                # depth=8,\n                               early_stopping_rounds=100,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(Xa))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(Xa, ya)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf2 = make_classifier2()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf2.fit(Xa.loc[trn_idx, all_features],ya.loc[trn_idx], eval_set=(Xa.loc[test_idx, all_features], ya.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf2.predict(Xa.loc[test_idx, all_features]).reshape(len(test_idx))\n    print(clf2.get_feature_importance())\n    print('OOF QWK:', qwk(ya.loc[test_idx], oof[test_idx]))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(ya, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test1 = get_testD[[\"title\",\"average_score\",\"acc_r\",\"inacc_r\"]]\nX_test1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test1 = get_testD[\"accuracy_group\"]\nY_test1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(Xa))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(Xa1, ya1)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf21 = make_classifier2()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf21.fit(Xa1.loc[trn_idx, all_features],ya1.loc[trn_idx], eval_set=(Xa1.loc[test_idx, all_features], ya1.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf21.predict(Xa1.loc[test_idx, all_features]).reshape(len(test_idx))\n    print(clf21.get_feature_importance())\n    print('OOF QWK:', qwk(ya1.loc[test_idx], oof[test_idx]))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(ya1, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf21 = make_classifier2()\nclf21.fit(Xa1, ya1, verbose=300, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicteda = clf21.predict(X_test1)\nfrom sklearn.metrics import classification_report\nreportcata = classification_report(Y_test1, predicteda)\nprint(reportcat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf2 = make_classifier2()\nclf2.fit(Xa, ya, verbose=500, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf2 = make_classifier()\nclf2.fit(Xa, ya, verbose=500, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf2a = make_classifier2()\nclf2a.fit(Xa, ya, verbose=100, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf2aa = make_classifier2aa()\nclf2aa.fit(Xa, ya, verbose=100, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2.get_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2a.get_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2aa.get_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xa.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicteda1 = clf21.predict(X_test1[[\"title\",\"average_score\",\"acc_r\",\"inacc_r\"]])\nfrom sklearn.metrics import classification_report\nreportcata1 = classification_report(Y_test1, predicteda1)\nprint(reportcata1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa21 = np.zeros(len(X_test1[[\"title\",\"average_score\",\"acc_r\",\"inacc_r\"]]))\noofa21 = clf21.predict(X_test1[[\"title\",\"average_score\",\"acc_r\",\"inacc_r\"]])\nprint('OOF QWK:', qwk(Y_test1, oofa21))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_testD55 = get_testD\nget_testD55[\"wrongly_labeled\"] = 0\nfor i in range(0,len(get_testD)):\n    if (get_testD[\"accuracy_group\"][i] != int(oofa21[i])):\n        get_testD55[\"wrongly_labeled\"] = 1\nincorr55 = get_testD55[get_testD55.wrongly_labeled == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(incorr55)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import set_option\npd.set_option('display.max_rows', 500)\nincorr55","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicteda2 = clf2a.predict(X_test1)\nfrom sklearn.metrics import classification_report\nreportcata2 = classification_report(Y_test1, predicteda2)\nprint(reportcata2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicteda2a = clf2aa.predict(X_test1)\nfrom sklearn.metrics import classification_report\nreportcata2a = classification_report(Y_test1, predicteda2a)\nprint(reportcata2a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmsa = sqrt(mean_squared_error(Y_test1, predicteda))\nprint(rmsa)\nprint(str(rmsa**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmsa2 = sqrt(mean_squared_error(Y_test1, predicteda2))\nprint(rmsa2)\nprint(str(rmsa2**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmsa2a = sqrt(mean_squared_error(Y_test1, predicteda2a))\nprint(rmsa2a)\nprint(str(rmsa2a**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa = np.zeros(len(X_test1))\noofa = clf2.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oofa))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa = np.zeros(len(X_test1))\noofa = clf2a.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oofa))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa = np.zeros(len(X_test1))\noofa = clf2aa.predict(X_test1)\nprint('OOF QWK:', qwk(Y_test1, oofa))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier3():\n    clf1 = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                                # depth=8,\n                               early_stopping_rounds=500,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(Xa))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(Xa, ya)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf3 = make_classifier3()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf3.fit(Xa.loc[trn_idx, all_features],ya.loc[trn_idx], eval_set=(Xa.loc[test_idx, all_features], ya.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf3.predict(Xa.loc[test_idx, all_features]).reshape(len(test_idx))\n    print(clf3.get_feature_importance())\n    print('OOF QWK:', qwk(ya.loc[test_idx], oof[test_idx]))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(ya, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf3 = make_classifier3()\nclf3.fit(Xa, ya, verbose=500, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_equal_data1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2.get_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xa.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport math\ntrain_equal_data2 = train_equal_data1\ntrain_equal_data2[\"acc_r_log\"] = 0.001\ntrain_equal_data2[\"inacc_r_log\"] = 0.001\nfor i in range(0,len(train_equal_data2)):\n    acc = train_equal_data2[\"acc_r\"][i]\n    inacc = train_equal_data2[\"inacc_r\"][i]\n    if (acc!= 0):\n        train_equal_data2[\"acc_r_log\"][i] = round(math.log(acc),3)\n    if (inacc!=0):\n        train_equal_data2[\"inacc_r_log\"][i] = round(math.log(inacc),3)\ntrain_equal_data2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_testD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ntest_l2 = get_testD\ntest_l2[\"acc_r_log\"] = 0.001\ntest_l2[\"inacc_r_log\"] = 0.001\nfor i in range(0,len(test_l2)):\n    acc = test_l2[\"acc_r\"][i]\n    inacc = test_l2[\"inacc_r\"][i]\n    if (acc!= 0):\n        test_l2[\"acc_r_log\"][i] = round(math.log(acc),3)\n    if (inacc!=0):\n        test_l2[\"inacc_r_log\"][i] = round(math.log(inacc),3)\ntest_l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xl1 = train_equal_data2.drop(columns = [\"accuracy_group\"])\nYl1 = train_equal_data2[\"accuracy_group\"]\nX_testl1 = test_l2.drop(columns = [\"accuracy_group\"])\nY_testl1 = test_l2[\"accuracy_group\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\ndef make_classifier4():\n    clf1 = CatBoostClassifier(\n                               loss_function='MultiClass',\n                                # eval_metric=\"AUC\",\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                                # depth=8,\n                               early_stopping_rounds=100,\n                                #l2_leaf_reg=1,\n                                #border_count=96,\n                               random_seed=42\n                              )\n        \n    return clf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold\n# oof is an zeroed array of the same size of the input dataset\noof = np.zeros(len(Xl1))\nNFOLDS = 5\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(Xl1, Yl1)):\n    # each iteration of folds.split returns an array of indexes of the new training data and validation data\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    # creates the model\n    clf4 = make_classifier4()\n    # fits the model using .loc at the full dataset to select the splits indexes and features used\n    clf4.fit(Xl1.loc[trn_idx, all_features],Yl1.loc[trn_idx], eval_set=(Xl1.loc[test_idx, all_features], Yl1.loc[test_idx]),\n                          use_best_model=True, verbose=500, cat_features=cat_features)\n    \n    # then, the predictions of each split is inserted into the oof array\n    oof[test_idx] = clf4.predict(Xl1.loc[test_idx, all_features]).reshape(len(test_idx))\n    print(clf4.get_feature_importance())\n    print('OOF QWK:', qwk(ya.loc[test_idx], oof[test_idx]))\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\n# and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\nprint('OOF QWK:', qwk(Yl1, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# train model on all data once\nclf4 = make_classifier3()\nclf4.fit(Xl1, Yl1, verbose=300, cat_features=cat_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf4.get_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicteda4 = clf4.predict(X_testl1)\nfrom sklearn.metrics import classification_report\nreportcata4 = classification_report(Y_testl1, predicteda4)\nprint(reportcata4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmsa2a4 = sqrt(mean_squared_error(Y_testl1, predicteda4))\nprint(rmsa2a4)\nprint(str(rmsa2a4**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa4 = np.zeros(len(X_testl1))\noofa4 = clf4.predict(X_testl1)\nprint('OOF QWK:', qwk(Y_testl1, oofa4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oofa4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof is an zeroed array of the same size of the input dataset\noofa4l = np.zeros(len(Xl1))\noofa4l = clf4.predict(Xl1)\nprint('OOF QWK:', qwk(Yl1, oofa4l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}