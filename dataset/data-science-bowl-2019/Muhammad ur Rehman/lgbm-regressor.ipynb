{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold #CV purposes\nimport time #for time related tasks\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef read_data(folder_path):\n#     print(\"reading Files ...\")\n    train = pd.read_csv(folder_path+\"/train.csv\")\n    test = pd.read_csv(folder_path+\"/test.csv\")\n    train_labels = pd.read_csv(folder_path+\"/train_labels.csv\")\n    submission_sample = pd.read_csv(folder_path+\"/sample_submission.csv\")\n    return train, test, train_labels, submission_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_lables, submission_sample = read_data(\"/kaggle/input/data-science-bowl-2019\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_values():\n#     print(\"getting encoded Values\")\n    unique_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    unique_game_session = list(set(train['game_session'].unique()).union(set(test['game_session'].unique())))\n    unique_installation_id = list(set(train['installation_id'].unique()).union(set(test['installation_id'].unique())))\n    unique_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    unique_titles = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    unique_type = list(set(train['type'].unique()).union(set(test['type'].unique())))\n    unique_world = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    \n    # map titles by numbers\n    titles_to_number = dict(zip(unique_titles, np.arange(len(unique_titles))))\n    # map number by titles\n    numbers_to_title = dict(zip(np.arange(len(unique_titles)), unique_titles))\n    # map world by numbers\n    world_to_number = dict(zip(unique_world, np.arange(len(unique_world))))\n    # map world by numbers\n    type_to_number = dict(zip(unique_type, np.arange(len(unique_type))))\n    # get list of all the assessments titles\n    assessment_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    \n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    return unique_event_id, unique_game_session, unique_installation_id, unique_event_code, unique_titles, unique_type, unique_world, assessment_titles,world_to_number,  type_to_number, numbers_to_title, titles_to_number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_event_id, unique_game_session, unique_installation_id, unique_event_code, unique_titles, unique_type, unique_world, assessment_titles, world_to_number, type_to_number, numbers_to_title, titles_to_number = encode_values()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_data(dataframe, test_dataset):\n    records = []\n\n    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n    activites_count = {'Clip': 0, 'Game': 0, 'Activity': 0, 'Assessment': 0}\n    every_assessments_count = {title: 0 for title in assessment_titles}\n    every_assessments_accuracy = {title + \"_accuracy\": 0 for title in assessment_titles}  # keep record for last played\n\n    accuracy_sum = 0\n    num_correct = 0\n    num_incorrect = 0\n    count = 0\n    durations = []\n    game_sessions = dataframe.groupby(\"game_session\")\n    sessions_count = len(game_sessions)\n\n    for i, game_session in game_sessions:\n        event_count = game_session.iloc[len(game_session) - 1]['event_count']\n        installation_id = game_session.iloc[len(game_session) - 1]['installation_id']\n        session_type = game_session.iloc[len(game_session) - 1]['type']\n        session_title = game_session.iloc[len(game_session) - 1]['title']\n        # keeping the count of each type\n        activites_count[session_type] += 1\n\n        if (session_type == \"Assessment\") & (test_dataset or len(game_session) > 1):\n            # keep record of how many time this assessments has been taken\n            every_assessments_count[session_title] += 1\n\n            if session_title == 'Bird Measurer (Assessment)':\n                all_attempts = game_session[game_session['event_code'].isin([4110])]\n                correct = all_attempts['event_data'].str.contains('true').sum()\n                incorrect = all_attempts['event_data'].str.contains('false').sum()\n#                 event_code = game_session['event_code']\n\n            else:\n                all_attempts = game_session[game_session['event_code'].isin([4100])]\n                correct = all_attempts['event_data'].str.contains('true').sum()\n                incorrect = all_attempts['event_data'].str.contains('false').sum()\n#                 event_code = game_session['event_code']\n\n\n            num_correct += correct\n            num_incorrect += incorrect\n            count += 1\n\n            accuracy = correct / (correct + incorrect) if (correct + incorrect)>0 else 0\n            accuracy_sum += accuracy\n            mean_accuracy = accuracy_sum / count\n            every_assessments_accuracy[session_title + \"_accuracy\"] = accuracy\n            if accuracy == 0:\n                accuracy_groups[0] += 1\n                accuracy_group = 0\n            elif accuracy == 1:\n                accuracy_groups[3] += 1\n                accuracy_group = 3\n            elif accuracy == 0.5:\n                accuracy_groups[2] += 1\n                accuracy_group = 2\n            else:\n                accuracy_groups[1] += 1\n                accuracy_group = 1\n            durations.append((game_session.iloc[-1, 2] - game_session.iloc[0, 2]).seconds)\n            duration_mean = np.mean(durations)\n\n            generated_data = accuracy_groups.copy()\n            generated_data.update(activites_count.copy())\n            generated_data.update(every_assessments_count.copy())\n            generated_data.update(every_assessments_accuracy.copy())\n            generated_data['sessions_count'] = sessions_count\n            generated_data['mean_time'] = duration_mean\n            generated_data['event_count'] = event_count\n#             generated_data['event_code'] = event_code\n            generated_data['title'] = session_title\n            generated_data['assessment_session_count'] = count\n            generated_data['num_correct'] = num_correct\n            generated_data['num_incorrect'] = num_incorrect\n            generated_data['accuracy'] = accuracy\n            generated_data['mean_accuracy'] = mean_accuracy\n            generated_data['accuracy_group'] = accuracy_group\n            generated_data['installation_id'] = installation_id\n            \n            records.append(generated_data)\n\n    # if it't the test_set, only the last assessment must be predicted\n    if test_dataset:\n        return records[-1]\n    # in the train_set, all assessments goes to the dataset\n    return records\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_data():\n    trian_preprocessed_data =[]\n    test_preprocessed_data =[]\n    \n    train_ID_groups = train.groupby('installation_id', sort = False)\n    test_ID_groups = test.groupby('installation_id', sort = False)\n    \n    for i, (installation_id, history) in tqdm(enumerate(train_ID_groups), total = len(train_ID_groups)):\n        trian_preprocessed_data += parse_data(history, False)\n    \n    for i, (installation_id, history) in tqdm(enumerate(test_ID_groups), total = len(test_ID_groups)):\n        t = parse_data(history, True)\n        test_preprocessed_data.append(t)\n        \n    t = pd.DataFrame(trian_preprocessed_data)\n    ts = pd.DataFrame(test_preprocessed_data)\n    return t, ts\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_train, p_test = get_preprocessed_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_train['title'] = p_train['title'].map(titles_to_number)\np_test['title'] = p_test['title'].map(titles_to_number)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qwk_loss(a1, a2):\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e / a1.shape[0]\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regr_resl_to_label(true_labels, preds_labels):\n    preds_labels[preds_labels <= 1.12232214] = 0\n    preds_labels[np.where(np.logical_and(preds_labels > 1.12232214, preds_labels <= 1.73925866))] = 1\n    preds_labels[np.where(np.logical_and(preds_labels > 1.73925866, preds_labels <= 2.22506454))] = 2\n    preds_labels[preds_labels > 2.22506454] = 3\n    return 'cappa', qwk_loss(true_labels, preds_labels), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\ndef train_model(X: pd.DataFrame,\n                y,\n            folds = None,\n            params: dict = None,\n            del_cols: list = None):\n\n    \"\"\"Basic parameters\n        1. X: train_data\n        2. y: ground truth labels\n        3. params: lightGBM parameters\n        4. del_cols: columns to be avoided while training like accuracy_group must not be a column! \n    \"\"\"\n    global scores\n    eval_metric = regr_resl_to_label #custom metric as defined above\n    columns = [col for col in X.columns.values if not col in del_cols] #features\n    \n    models = [] #save n_folds models\n    n_target = 1 # number of targets\n    oof = np.zeros((len(X), n_target)) # out of fold predictions\n\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n        \n        print('Fold {} started at {}'.format(fold_n + 1,time.ctime()))\n        X_train, X_valid = X.loc[train_index,columns], X.loc[valid_index,columns]\n        y_train, y_valid = y.loc[train_index], y.loc[valid_index]\n        print(X_train.shape)\n        \n        #Eval set preparation\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        eval_set.append((X_valid, y_valid))\n        eval_names.append('valid')\n        categorical_columns = 'auto'\n        \n        model = lgb.LGBMRegressor(**params)\n        model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n        \n        oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n        score = regr_resl_to_label(X.loc[valid_index,\"accuracy_group\"],oof[valid_index])\n        scores.append(score)\n        models.append(model)\n    scores = [score[1][0] for score in scores]\n    print(scores)\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'verbose': 100,\n          'learning_rate': 0.010514633017309072,\n          'metric': 'rmse',\n          'bagging_freq': 3,\n          'boosting_type': 'gbdt',\n          'eval_metric': 'cappa',\n          'lambda_l1': 4.8999704874480745,\n          'colsample_bytree': 0.4236269531042225,\n          'early_stopping_rounds': 100,\n          'max_depth': 12,\n          'lambda_l2': 0.054084652510602016,\n          'bagging_fraction': 0.7931423220563563,\n          'n_jobs': -1,\n          'n_estimators': 2000,\n          'objective': 'regression',\n          'seed': 42}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in p_train.columns]\np_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in p_test.columns]\n\n# no need for these columns in training\ncols_to_drop = ['installation_id','accuracy_group'] + [col for col in p_train.columns.values if \"_time\" in str(col)]#ground truth fact labels\n\ny = p_train['accuracy_group']\nn_fold = 5\nfolds = GroupKFold(n_splits=n_fold)\nmodels = train_model(X = p_train, y = y,folds = folds, params = params, del_cols = cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(models, X_test, averaging: str = 'usual'):\n    full_prediction = np.zeros((X_test.shape[0], 1))\n    for i in range(len(models)):\n        X_t = X_test.copy()\n        if cols_to_drop is not None:\n            del_cols = [col for col in cols_to_drop if col in X_t.columns.values]\n            X_t = X_t.drop(del_cols, axis=1)\n        y_pred = models[i].predict(X_t).reshape(-1, full_prediction.shape[1])\n        if full_prediction.shape[0] != len(y_pred):\n            full_prediction = np.zeros((y_pred.shape[0], 1))\n        if averaging == 'usual':\n            full_prediction += y_pred\n        elif averaging == 'rank':\n            full_prediction += pd.Series(y_pred).rank().values\n    return full_prediction / len(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = predict(models, p_test)\n    \ncoefficients = [1.12232214, 1.73925866, 2.22506454]\npreds[preds <= coefficients[0]] = 0\npreds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\npreds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\npreds[preds > coefficients[2]] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_sample['accuracy_group'] = preds.astype(int)\nsubmission_sample.to_csv('submission.csv', index=False)\nsubmission_sample['accuracy_group'].value_counts(normalize=True)\nsubmission_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_sample.to_csv('/kaggle/working/submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}