{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Questions I am Exploring #\n\nLooking for some support with the community.  I can't quite figure out why:\n* My PCA based model is spitting out all 3's.  \n* My model performance is so bad on the public leaderboard (0.1-0.25, not close to what I am getting in CV).\n* My submissions no longer work.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Parameters for my workflow for more easeful experimenting (instead of commenting out code)\n# Also gives a great overall picture of what I'm including\n\nVERBOSITY = 0\nHIDE_CODE_CELLS = False\nIGNORE_WARNINGS = True\n\n# Baseline model before any major cleaning or feature engineering\nDO_BASELINE_MODEL = True\n\n# Data Cleaning & Class Imbalance\nVALIDATE_DATA = False\n\nMERGE_ON_COLS = ['installation_id', 'game_session']  # This indicates what constitutes an 'observation'\nRESAMPLE_TRUE_DATA = False\nSHOW_CLASS_BALANCE = False\nCOMPUTE_CLASS_WEIGHTS = False\nIMPUTE_VALUES = True\nIMPUTE_FILLNA_VAL = 0\nIMPUTE_METHOD = 'fill_na'\n\n# Semi Supervised Learning\nLABEL_UBLABELLED_DATA = True\n\n# Exploratory Data Analysis\n\nRESAMPLE_EDA_DATA = True\nDO_PANDAS_PROFILE = False\nEXPORT_PANDAS_PROFILE = False\n\n# Exploratory Clustering\n\nEXPLORE_WITH_HIERARCHICAL_C = False\nHIERARCHICAL_METHOD = ''\nEXPLORE_WITH_PCA = False\nEXPLORE_WITH_TSNE = True\nEXPLORE_WITH_DBSCAN = False\nDO_CO_OCCURENCE_MATRIX = False\nENSEMBLE_CLUSTERS = False\ncluster_parameter_dict = {}\n\n# Explanatory Data Analysis\nSHOW_EXPLANATORY_DATA = False\n\n# Feature Engineering\nBINARIZE_FEATURES = False\nBIN__NUMERICAL_FEATURES = False\nENCODE_CATEGORICAL = False\nGROUPBY_COUNTS_ON_TARGET = True  # ?\nGENERATE_CATEGORICAL_COUNTS = True\nTARGET_ENCODE_CATEGORIES = False\nCALCULATE_POLYNOMIAL_FEATURES = False\nCALCULATE_SIMILARITY_FEATURES = False\nEMBED_PREDICTIONS_AS_FEATURES = False\nEMBED_TEST_FIT_CLUSTER_ON_TRAIN_FEATURE = False\nADD_NOISY_FEATURE = False\nFEATURES_WITH_POTENTIAL_RELATIONSHIPS = []\n\n# NLP Engineering\nENGINEER_NLP_FEATURES = False\nPSEUDO_LABEL = False\n\n# Time Series Engineering\nCREATE_DATETIME_FEATURES = False\nENGINEER_TIME_SERIES_FEATURES = True\n\n# Feature Preprocessing\nSCALE_METHOD = \"min_max\"\nSCALE_FEATURES = True\n\n# Feature Selection\nREDUCE_TO_N_FEATURES = 30  # \nUSE_BEST_FEATURES_ONLY = False\nSELECT_WITH_PCA = True\nPCA_N_COMPONENTS = 30\nSELECT_WITH_LDA = False\nLDA_N_COMPONENTS = REDUCE_TO_N_FEATURES\nN_FEATURES = 30\n\n# Cross Validation and Model Validation\nMIMIC_KAGGLE_TRAIN_TEST = False\nDO_TRAIN_TEST_SPLIT = True\nTRAIN_TEST_PERCENTAGE = 0.75\nDO_KFOLD = True\nCOMPETITION_METRIC = 'kappa_quadratic'\nPLOT_DECISION_REGIONS = False\n\n# Models\nDO_LINEAR_REGRESSION = False\nDO_LOGISTIC_REGRESSION = False\n\nDO_RANDOM_FOREST = False\nDO_CATBOOST = True\nDO_LIGHTBOOST = True\nDO_XGBOOST = True\n\n# Deep Learning\nUSE_DEEP_LEARNING = False\nUSE_AUTO_KERAS = False\nNN_LOSS_FUNCTION = 'categorical_cross_entropy'\nNN_OPTIMIZER = 'adam'\nNN_N_LAYERS = 8\nNN_BATCH_SIZE = 32\nNN_N_NODES = 100\n\n# Ensembling\nUSE_VOTING_CLASSIFIER = False\nUSE_STACKING = False\n\n# Hyperparameter Tuning\nDO_GRID_SEARCH = False\nUSE_TPOT = False\nUSE_AUTO_KERAS = False\n\n# Submission\nCREATE_SUBMISSION = True\nSUBMISSION_CLF = 'cat_clf'\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import itertools\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport scipy\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (16,12)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\nimport seaborn as sns\nsns.set()  # This make matplotlib plots look like seaborn plots.\nsns.set_context(\"talk\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Stats\nfrom scipy.stats import zscore\n\n# EDA and Unsupervised Exploring and Feature Visualization\nimport pandas_profiling\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport missingno as msno  # Missing Data\nfrom yellowbrick.features import Manifold, ParallelCoordinates\nfrom yellowbrick.datasets import load_occupancy\n\n# Class Balancing\nfrom imblearn.under_sampling import ClusterCentroids, NearMiss\n\n# Resampling\nfrom sklearn.utils import resample\n\n# Feature Processing and Engineering\nfrom sklearn.feature_extraction.text import CountVectorizer  # NLP\nfrom sklearn.preprocessing import PolynomialFeatures, label_binarize, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# Models \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\n# Ensembling\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier\nfrom sklearn.pipeline import make_pipeline\n\n# Dimensionality Reduction\nif SELECT_WITH_PCA == True | EXPLORE_WITH_PCA == True:\n    from sklearn.decomposition import PCA\n\nfrom sklearn.manifold import TSNE\n\n# Model Validation and Metrics\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics.cluster import homogeneity_score\n\n\n# from imblearn.ensemble import BalancedRandomForestClassifier\n\n#AutoML\nfrom tpot import TPOTClassifier\n\n# Yellowbrick Reports\nfrom yellowbrick.classifier import ClassBalance, ClassificationReport, ConfusionMatrix, ROCAUC, DiscriminationThreshold\nfrom yellowbrick.model_selection import FeatureImportances, LearningCurve, RFECV\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\n\n# Optimization\nfrom numba import jit \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_current_fig(filename):\n    # Save a figure to the output path.\n    \n    plt.gcf()\n    plt.savefig(filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If Loading The Data From Saved External Dataframe ##"},{"metadata":{},"cell_type":"markdown","source":"## If Loading The Data in Real Time ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Import\nSAMPLE_SIZE = 500000000000\nsample_size = SAMPLE_SIZE # Remove this redundancy\nCHUNK_SIZE = 2000\nEXPORE_MEM_TYPES = False\nREDUCE_MEMORY = True\nLOAD_ON_KAGGLE = True\n\n# Data Import Specifics\ndatafiles = ['sample_submission', 'train_labels', 'train', 'test']\ndataframe_names = ['sample_submission', 'train_labels', 'train', 'test']  # Remove this redundancy!\n\ncsv_filenames = map(lambda  x: str(x) + '.csv', datafiles)\nindex_columns = [None, 'installation_id', 'installation_id', 'installation_id']\ndatetime_cols = [None, 'timestamp', 'timestamp', 'timestamp']\nthepath = '/kaggle/input/data-science-bowl-2019/'\n\nCATEGORICAL_VARIABLES = []\nNUMERICAL_VARIABLES = []\nORDINAL_VARIABLES = []\nTARGET_VARIABLE = \"accuracy_group\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading Data in chunks and processing per ID to save ram**"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\"\"\"\n# Load Preserving Ram\n# Still overloads the ram!!!!\n# I Think test_df = test_df.append() is the problem.  \n# Just using append wasn't doing anything.\n\n# Use a with statement to close after done with the chunk?\n\ndf_dict = {}\nfilepath = '/kaggle/input/data-science-bowl-2019/train.csv'\n\n# Read one column to get the column names.\ndf_info = pd.read_csv(filepath, nrows = 1, index_col = 'installation_id')\ncol_names = df_info.columns\n\nread_chunk = pd.read_csv(filepath, \n                         chunksize = 50000, \n                         index_col = 'installation_id')\n\n\nwhile True:\n    try:\n        df_dict.update(next(read_chunk))\n    except StopIteration:\n        break\n\ndf = pd.DataFrame.from_dict(data = df_dict)\nprint(df.shape)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load all at once**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_csv(datafiles, path = thepath):\n    return [pd.read_csv(str(path + datafiles[df_i] + '.csv'), \n                        nrows = sample_size,\n                        index_col=index_columns[df_i])\n            for df_i in range(len(datafiles))]\n\n# Unpack\nsample_submission, train_labels, train, test = read_csv(datafiles)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to datetimes.\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Update memory types ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updated function for converting memory types.  DRY method.   \n# Not active yet.\n\nmem_updates_dict = {'train_labels': {'title':'category',\n                               'num_correct': 'np.int8',\n                               'num_incorrect': 'np.int8',\n                               'accuracy': 'np.float16',\n                               'accuracy_group': 'np.int8'},\n                      'train': {'type':'category',\n                               'world':'category',\n                               'event_count':'np.int8',\n                               'event_code':'np.int8',\n                               'game_time':'np.int8'},\n                      'test': {'type':'category',\n                               'world':'category',\n                               'event_count':'np.int8',\n                               'event_code':'np.int8',\n                               'game_time':'np.int8'}}\n\ndef convert_memtypes(memdict):\n    pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert memory types WET method.\n\ntrain_labels['title'] = train_labels['title'].astype('category')\ntrain_labels['num_correct'] = train_labels['num_correct'].astype(np.int16)\ntrain_labels['num_incorrect'] = train_labels['num_incorrect'].astype(np.int16)\ntrain_labels['accuracy'] = train_labels['accuracy'].astype(np.float16)\ntrain_labels['accuracy_group'] = train_labels['accuracy_group'].astype(np.int16)\n\ntrain['type'] = train['type'].astype('category')\ntest['type'] = test['type'].astype('category')\n\ntrain['world'] = train['world'].astype('category')\ntest['world'] = test['world'].astype('category')\n\ntrain['event_code'] = train['event_code'].astype('category')\ntest['event_code'] = test['event_code'].astype('category')\n\n# train['event_count'] = train['event_count'].astype(np.int8)\n# test['event_count'] = test['event_count'].astype(np.int8)\n\ntrain['game_time'] = train['game_time'].astype(np.int)\ntest['game_time'] = test['game_time'].astype(np.int)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do a quick look at the distributions of important factors ###"},{"metadata":{},"cell_type":"markdown","source":"### Create Datetime Features + Do Timeseries Statistics ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_time_features(df):\n# Inspired by Gabriel Preda: https://www.kaggle.com/gpreda/2019-data-science-bowl-eda\n    df['date'] = df['timestamp'].dt.date.astype('category')\n    df['month'] = df['timestamp'].dt.month.astype('category')\n    df['hour'] = df['timestamp'].dt.hour.astype('category')\n    df['year'] = df['timestamp'].dt.year.astype('category')\n    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('category')\n    df['week_of_year'] = df['timestamp'].dt.weekofyear.astype('category')\n    df['day_of_year'] = df['timestamp'].dt.dayofyear.astype('category')\n    df['quarter'] = df['timestamp'].dt.quarter.astype('category')\n    df['is_month_start'] = df['timestamp'].dt.is_month_start.astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = extract_time_features(train)\n#test = extract_time_features(test)\n#train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preliminary cleaning of outliers in original sets before aggregating ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_outliers(df, low_q = 0, high_q = 1):  # Does better without removing outliers\n    \n    # Chose 0 and 1 to bypass without changing code everywhere.\n    # Check to see if outliers exist.  Will return an error if they don't.\n    # Right now, nothing should be passed in the test set\n    # In the future, update so that only df.index.isin(train.index)\n    # Are removed.\n    \n    print('Original shape', df.shape)\n    outliers_removed = pd.DataFrame()\n    \n    for column in df.columns:\n        # Do this only if column is numeric\n        \n        try:\n            q1 = df[column].quantile(low_q)\n            q3 = df[column].quantile(high_q)\n            mask = df[column].between(q1, q3, inclusive=True)\n\n            iqr = df.loc[mask, column]\n            outliers_removed[column] = iqr\n        except:\n            print('Did not remove outliers from' + column)\n        finally:\n            print('After removing outliers: ', outliers_removed.shape)\n            \n    return outliers_removed\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display some visuals here."},{"metadata":{},"cell_type":"markdown","source":"** Model does better if we leave 0 game times in**"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"REMOVE_0_GAME_TIME = False # Does better when not removed.\n\nif REMOVE_0_GAME_TIME == True:\n    # Remove game times of 0 since these will not likely contain useful information.\n    train_is_game_time_0 = train['game_time'] == 0\n    train_is_not_game_time_0 = train['game_time'] != 0\n    train = train[train_is_not_game_time_0]\n\n    print('Training set:')\n    print(train_is_game_time_0.sum())\n    print(train.shape[0])\n    print(round(train_is_game_time_0.sum() / train.shape[0] * 100, 2), '% of entries in train have 0 game time')\n    print('\\n')\n\n    # Does the test set have any 0 game time entries?\n    print('Testing set:')\n    test_is_game_time_0 = test['game_time'] == 0\n    print(test_is_game_time_0.sum())\n    print(test.shape[0])\n    print(round(test_is_game_time_0.sum() / test.shape[0] * 100, 2), '% of entries in test have 0 game time')\n\n    # Should we just assign these entries as accuracy of 0?\n\n    # Maybe they just left the games on and weren't actually playing?\n    train_gt_is_not_too_long = train.game_time < train.game_time.quantile(.9999)\n    train = train[train_gt_is_not_too_long]\n\n    # Do note remove from test set!!!\n    # test_gt_is_not_too_long = test.game_time < test.game_time.quantile(.9999)\n    # test = test[test_gt_is_not_too_long]\n\n    # train = remove_outliers(train, low_q = 0, high_q = .9999)  # this should remove the 'problem' data points.\n\n    # Do not remove from test set!!!\n    # test = remove_outliers(test, low_q = 0, high_q = .9999)  # this should remove the 'problem' data points.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering ##"},{"metadata":{},"cell_type":"markdown","source":"### Do some Groupby's on the whole dataset before I focus in on assessments ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def groupby_categorical_counts(df, df_name, by_col, on_cols, stat_features = True, verbose = 1):\n    # Inputs a categorical column in a dataframe and \n    # Returns the count of that column of each unique value.\n    # on_cols must be a list of cols.  If one col, pass ['col'].\n    \n    added_features = []\n    total_features = 0\n    \n    for i in range(len(on_cols)):\n        \n        on_col = on_cols[i]  # Set the column to the i'th element in the list.\n        \n        # Do Groupby value_counts.\n        if verbose == 1:\n            print(\"Creating groupby counts\", by_col, \"on\", on_col, 'in dataframe', df_name)\n\n        added_feature = df.groupby(by_col)[on_col].value_counts().unstack().fillna(0)\n        n_new_features = len(added_feature.columns)\n        \n        added_features.append(added_feature)\n        total_features = total_features + n_new_features\n        \n        if verbose == 1:\n            print(\"Added: \", n_new_features, \"features from\", df_name, \"Filled Na with 0 \\n\")        \n    \n    if verbose == 1:\n        print(\"Total features returned:\", total_features)\n        \n    return added_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BASELINE_WITH_SCALING = True\nCATEGORICAL_COLS_TO_GROUPBY = ['event_code', \n                               'title', 'type', \n                               'world', 'event_id']\n\n# add_later = [, 'is_month_start', 'quarter']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_MODEL == True:\n    print('Creating categorical features for the training set')    \n    tr_baseline_event_code, tr_baseline_title, tr_baseline_type, tr_baseline_world, tr_baseline_event_id = groupby_categorical_counts(df = train, \n                                                                                df_name = 'train', \n                                                                                by_col = 'installation_id', \n                                                                                on_cols = CATEGORICAL_COLS_TO_GROUPBY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_MODEL == True:\n    print('\\n\\n Creating categorical features for the test set')\n    test_baseline_event_code, test_baseline_title, test_baseline_type, test_baseline_world, test_baseline_event_id = groupby_categorical_counts(df = test, \n                                                                            df_name = 'test', \n                                                                            by_col = 'installation_id', \n                                                                            on_cols = CATEGORICAL_COLS_TO_GROUPBY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Work only with the assessments for now. Training data.\n\nis_assessment_train = train['type'] == 'Assessment'\ntrain = train[is_assessment_train]\n\n# All other assessments.\n\n\n# Work only with the assessments for now. Training data.\nis_assessment_test = test['type'] == 'Assessment'\ntest = test[is_assessment_test]\n\n# Find the strings that indicate 'correct and incorrect' assessments\nis_correct = '\"correct\":true'\nis_incorrect = '\"correct\":false'\n\ntrain['num_correct'] = train.loc[:, 'event_data'].str.find(is_correct)\ntrain['num_correct'] = train['num_correct'] >= 0\ntrain['num_correct'] = train['num_correct'].astype(int)\n\ntrain['num_incorrect'] = train.loc[:, 'event_data'].str.find(is_incorrect)\ntrain['num_incorrect'] = train['num_incorrect'] >= 0\ntrain['num_incorrect'] = train['num_incorrect'].astype(int)\n\ntest['num_correct'] = test.loc[:, 'event_data'].str.find(is_correct)\ntest['num_correct'] = test['num_correct'] >= 0\ntest['num_correct'] = test['num_correct'].astype(int)\n\ntest['num_incorrect'] = test.loc[:, 'event_data'].str.find(is_incorrect)\ntest['num_incorrect'] = test['num_incorrect'] >= 0\ntest['num_incorrect'] = test['num_incorrect'].astype(int)\n\n# Collect bird train assessments\n# is_bird_train = train['event_code'] == 4110\n# bird_assess_train = pd.DataFrame(train[is_bird_train])\n\n# Collect bird train assessments\n# is_bird_test = test['event_code'] == 4110\n# bird_assess_test = pd.DataFrame(test[is_bird_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate the num_correct and num_incorrect by user, game sesion, and title\ntr_baseline_num_correct = train.groupby(['installation_id', 'game_session', 'title'])['num_correct'].sum().fillna(0).astype(int)\ntr_baseline_num_incorrect = train.groupby(['installation_id', 'game_session', 'title'])['num_incorrect'].sum().fillna(0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make dataframes out of these for concatenation next.\ntr_num_correct_df = pd.DataFrame(tr_baseline_num_correct.unstack().fillna(0).astype(int))\ntr_num_incorrect_df = pd.DataFrame(tr_baseline_num_incorrect.unstack().fillna(0).astype(int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate num correct and num incorrect and calculate accuracy like train_labels ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is validated.  Calculate the accuracy per game and then do statistics on it.\ntrain_accuracy_per_game = tr_baseline_num_correct / (tr_baseline_num_correct + tr_baseline_num_incorrect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_accuracy_avg = train_accuracy_per_game.groupby(['installation_id']).mean().fillna(0).rename('accuracy', inplace = True).astype(float)\ntrain_accuracy_std = train_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\ntrain_accuracy_std = train_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\n\ntrain_accuracy_min = train_accuracy_per_game.groupby(['installation_id']).min().fillna(0).rename('accuracy_min', inplace = True).astype(float)\ntrain_accuracy_max = train_accuracy_per_game.groupby(['installation_id']).max().fillna(0).rename('accuracy_max', inplace = True).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum totals of num correct and num incorrect.\ntr_baseline_num_correct = tr_baseline_num_correct.groupby(['installation_id']).sum().fillna(0).rename('num_correct', inplace = True)\ntr_baseline_num_incorrect = tr_baseline_num_incorrect.groupby(['installation_id']).sum().fillna(0).rename('num_incorrect', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_accuracy_per_game.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Other statistics on accuracy\ntrain_accuracy_gmean = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.gmean).fillna(0).rename('accuracy_gmean', inplace = True).astype(float)\ntrain_accuracy_skew = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.skew).fillna(0).rename('accuracy_skew', inplace = True).astype(float)\ntrain_accuracy_kurt = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.kurtosis).fillna(0).rename('accuracy_kurtosis', inplace = True).astype(float)\n\ntrain_statistics_cols = [train_accuracy_gmean, train_accuracy_skew, train_accuracy_kurt]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_baseline_num_correct_eventid = train.groupby(['installation_id','event_id'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntr_baseline_num_correct_eventid.columns = tr_baseline_num_correct_eventid.columns.map(''.join).str.strip('')\n\ntest_baseline_num_correct_eventid = test.groupby(['installation_id','event_id'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntest_baseline_num_correct_eventid.columns = test_baseline_num_correct_eventid.columns.map(''.join).str.strip('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num_correct_per_title = train.groupby(['installation_id', 'title'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntrain_num_correct_per_title.columns = train_num_correct_per_title.columns.map('_'.join).str.strip('_')\n\ntest_num_correct_per_title = test.groupby(['installation_id', 'title'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntest_num_correct_per_title.columns = test_num_correct_per_title.columns.map('_'.join).str.strip('_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_num_correct_per_world = train.groupby(['installation_id', 'world'])['num_correct','num_incorrect'].sum().unstack().fillna(0)\ntr_num_correct_per_world.columns = tr_num_correct_per_world.columns.map('_'.join).str.strip('_')\n\ntest_num_correct_per_world = test.groupby(['installation_id', 'world'])['num_correct','num_incorrect'].sum().unstack().fillna(0)\ntest_num_correct_per_world.columns = test_num_correct_per_world.columns.map('_'.join).str.strip('_')\ntr_num_correct_per_world.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_baseline_num_correct = test.groupby(['installation_id', 'game_session', 'title'])['num_correct'].sum().fillna(0).astype(int)\ntest_baseline_num_incorrect = test.groupby(['installation_id', 'game_session', 'title'])['num_incorrect'].sum().fillna(0).astype(int)\n\ntest_accuracy_per_game = test_baseline_num_correct / (test_baseline_num_correct + test_baseline_num_incorrect)\n\n\ntest_accuracy_avg = test_accuracy_per_game.groupby(['installation_id']).mean().fillna(0).rename('accuracy', inplace = True).astype(float)\ntest_accuracy_std = test_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\ntest_accuracy_min = test_accuracy_per_game.groupby(['installation_id']).min().fillna(0).rename('accuracy_min', inplace = True).astype(float)\ntest_accuracy_max = test_accuracy_per_game.groupby(['installation_id']).max().fillna(0).rename('accuracy_max', inplace = True).astype(float)\n\ntest_baseline_num_correct = test_baseline_num_correct.groupby(['installation_id']).sum().fillna(0).rename('num_correct', inplace = True)\ntest_baseline_num_incorrect = test_baseline_num_incorrect.groupby(['installation_id']).sum().fillna(0).rename('num_incorrect', inplace = True)\n\ntest_accuracy_gmean = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.gmean).fillna(0).rename('accuracy_gmean', inplace = True).astype(float)\ntest_accuracy_skew = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.skew).fillna(0).rename('accuracy_skew', inplace = True).astype(float)\ntest_accuracy_kurt = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.kurtosis).fillna(0).rename('accuracy_kurtosis', inplace = True).astype(float)\n\ntest_statistics_cols = [test_accuracy_gmean, test_accuracy_skew, test_accuracy_kurt]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add and sort the event_id columns for training and test sets.\n\nprint('Are there missing event ID counts in the test variable to be concatenated later?')\nprint('train_baseline_event_id,shape', tr_baseline_event_id.shape)\nprint('test_baseline_event_id,shape', test_baseline_event_id.shape)\n\nprint('\\nAdding to test event_id counts')\n\n# Loop through missing columns and broadcast them.\nevent_ids_add_to_test = list(set(tr_baseline_event_id).difference(set(test_baseline_event_id)))\nfor col in event_ids_add_to_test:\n    test_baseline_event_id[col] = 0\n\nprint('train_baseline_event_id,shape', tr_baseline_event_id.shape)\nprint('test_baseline_event_id,shape', test_baseline_event_id.shape)\n\nprint('\\nAdding to test num_correct_event_id')\n\nprint('tr_baseline_num_correct_eventid', tr_baseline_num_correct_eventid.shape)\nprint('test_baseline_num_correct_eventid', test_baseline_num_correct_eventid.shape)\n\nnum_correct_event_ids_add_to_test = list(set(tr_baseline_num_correct_eventid).difference(set(test_baseline_num_correct_eventid)))\nfor col in num_correct_event_ids_add_to_test:\n    test_baseline_num_correct_eventid[col] = 0\n    \nprint('tr_baseline_num_correct_eventid', tr_baseline_num_correct_eventid.shape)\nprint('test_baseline_num_correct_eventid', test_baseline_num_correct_eventid.shape)\n\nprint('\\nSorting the Columns...')\ntr_baseline_event_id.sort_index(axis=1, inplace=True)\ntest_baseline_event_id.sort_index(axis=1, inplace=True)\ntr_baseline_num_correct_eventid.sort_index(axis=1, inplace=True)\ntest_baseline_num_correct_eventid.sort_index(axis=1, inplace=True)\nprint('Complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_accuracy_group(the_series):\n    if the_series == 0:\n        return 0\n    elif the_series > 0 and the_series < 0.5:\n        return 1\n    elif the_series > 0 and the_series >= 0.5 and the_series < 0.75:\n        return 2\n    elif the_series >= 0.75:\n        return 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_estimate = train_accuracy_avg.apply(convert_to_accuracy_group).rename('accuracy_group_estimate', inplace = True).fillna(0).astype(int)\ny_test_estimate = test_accuracy_avg.apply(convert_to_accuracy_group).rename('accuracy_group_estimate', inplace = True).fillna(0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate - 'Got the Third One'\n# if event_count == 3 and type == 'Assessment':\n# Check if success\n# \"won first assessment.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate - Only got it after the third one.\n# else won after third try.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate everything to train/test_baseline DataFrames"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef aggregate_targets_on_id(y):\n    # Aggregate by the mode.  \n    # Preserves structure of distribution most.\n    \n    mode_agg = lambda x: scipy.stats.mode(x)[0]\n    aggregation_function = mode_agg\n\n    # Just aggregate the y_train data. \n    return y.groupby(['installation_id'])['accuracy_group'].agg(aggregation_function)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate targets\nif DO_BASELINE_MODEL == True:\n    print('Aggregating training set with the MODE so there is one row per ID')\n    train_y_baseline = aggregate_targets_on_id(train_labels)\n    print(train_y_baseline.shape)\n    print('assigning this to y')\n    \n    # Code smell - organize this so I don't need to assign it twice.\n    y = train_y_baseline\n    y_train = train_y_baseline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate all features engineering to train dataframe.\n\nif DO_BASELINE_MODEL == True:\n    print('Concatenating engineered features to the train_baseline')\n    \n    train_features = [tr_num_correct_per_world, tr_baseline_event_code, \n                      tr_baseline_title, tr_baseline_type, tr_baseline_world, \n                      tr_baseline_event_id, tr_baseline_num_correct, \n                      tr_baseline_num_incorrect, tr_baseline_num_correct_eventid, \n                      train_accuracy_avg, train_accuracy_std, train_accuracy_min, \n                      train_accuracy_max, train_num_correct_per_title, y_train_estimate,\n                      train_accuracy_gmean, train_accuracy_skew, train_accuracy_kurt]\n    \n    train_baseline = pd.concat(train_features, axis = 1)\n    train_baseline.fillna(0, inplace = True)\n    print(train_baseline.shape)\n    train_baseline.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate Test Set\n\nif DO_BASELINE_MODEL == True:\n    # print('\\nRemoving Outliers...')\n    # train_baseline = train_baseline.fillna(0)\n    # train_baseline = remove_outliers(train_baseline)\n    \n\n    print('\\nConcatenating engineered features to the test_baseline')\n    test_baseline = pd.concat([test_num_correct_per_world, test_baseline_event_code, \n                               test_baseline_title, test_baseline_type, \n                               test_baseline_world, test_baseline_event_id, \n                               test_baseline_num_correct, test_baseline_num_incorrect, \n                               test_baseline_num_correct_eventid, test_accuracy_avg, \n                               test_accuracy_std, test_accuracy_min, test_accuracy_max, \n                               test_num_correct_per_title, y_test_estimate,\n                               test_accuracy_gmean, test_accuracy_skew, test_accuracy_kurt], axis = 1)\n\n    print('test_baseline.shape' , test_baseline.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove features that are not represented in the train set ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_in_train = list(train_baseline.columns)\nlen(cols_in_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_baseline.shape)\ntest_baseline = test_baseline[cols_in_train]\nprint(test_baseline.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split into labelled and unlabelled train for semi-supervised learning ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"    print('\\nStoring unlabelled data in the training seet as: train_baseline_labelled for semi-supervised learning')\n    train_baseline_labelled = train_baseline[train_baseline.index.isin(train_y_baseline.index.unique())]\n    train_unlabelled = train_baseline[~train_baseline.index.isin(train_y_baseline.index.unique())]\n    \n    print('train_unlabelled shape', train_unlabelled.shape)\n    print('train_labelled shape', train_baseline_labelled.shape)\n    \n    # print('Using generated targets')\n    # train_y_baseline = y_train_estimate\n    \n    # print('\\nOnly including samples in train which have accuracy group labels in train_labels')\n    # print(train_y_baseline.shape)\n    # train_y_baseline = train_y_baseline[train_y_baseline.index.isin(train_baseline.index.unique())]\n    # print(train_y_baseline.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impute Missing Values ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n\n# Impute test with 0's for now.\ntest_baseline = test_baseline.fillna(0)\n# ax = msno.matrix(test_baseline.fillna(0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_missing_vals(df):\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_baseline_labelled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_unlabelled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scale features for models that require scaling  ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign current working dataset as training set X\nX_train = train_baseline_labelled\nX_test = test_baseline\n\nSCALE_FEATURES = False\n\nif SCALE_FEATURES == True:\n    print('Generating scaled features as X_scaled_train \\ntest as X_scaled_test')\n    \n    X_scaled_train = StandardScaler()\n    X_scaled_train = X_scaled_train.fit_transform(X_train)\n    print('Sample from X_scaled_train', X_scaled_train[0][:5])\n    \n    X_scaled_test = StandardScaler()\n    X_scaled_test = X_scaled_test.fit_transform(test_baseline)\n    print('Sample from X_scaled_test', X_scaled_test[0][:5])\n    \nelse:\n    print('X_train and X_test assigned as unscaled feature data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, roc_auc_score\n\ndef fit_baseline_classifiers(classifiers, X, y, use_polynomial = False, use_scaled = False, do_kfold = False, graph_results = False):\n    \n    if use_polynomial == True:\n        polyX_train = PolynomialFeatures(degree = 2, interaction_only = True)\n        X = polyX_train.fit_transform(X)\n    \n    if use_scaled == True:\n        print('Scaling for Classification using MinMax\\n')\n        X_scaled = MinMaxScaler()\n        X_scaled = X_scaled.fit_transform(X)\n        X = X_scaled\n    \n    # For collecting and graphing results at the end.\n    all_accuracies = {}\n    all_balanced_acc = {}\n    all_validation_accuracies = {}\n    all_acc_differences = {}\n    all_quadratic_kappas = {}\n    all_f1_scores = {}\n    \n    # Loop across each classifier.\n    for clf_name, classifier in classifiers:\n        \n        # Split for validation scoring.\n        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, shuffle = True)\n        \n        # Fit the classifier on the labels.\n        classifier.fit(X_train, y_train)\n        \n        # Predict targets after fitting.\n        y_pred = classifier.predict(X_test)\n        \n        # Use .score method if it exists.\n        try:\n            training_set_accuracy = round(classifier.score(X_train, y_train), 2)\n        except:\n            training_set_accuracy = None\n            print(clf_name, \"doesn't have an internal accuracy metric set up\")\n        \n        # Compute metrics\n        accuracy = round(accuracy_score(y_test, y_pred), 2)\n        balanced_acc = round(balanced_accuracy_score(y_test, y_pred), 2)\n        recall_sc = round(recall_score(y_test, y_pred, average = 'weighted'), 2)\n        precision_sc = round(precision_score(y_test, y_pred, average = 'weighted'))\n        quadratic_kappa = round(cohen_kappa_score(y_test, y_pred, weights=\"quadratic\"))\n        the_f1_score = round(f1_score(y_test, y_pred, average = 'weighted'))\n        # roc_auc_sc = round(roc_auc_score(y_test, y_pred, average = None))\n        \n        # Collect all accuracy scores and return the highest one?\n        print(clf_name)\n        print('Accuracy score - training set:', training_set_accuracy)\n        print('Accuracy score - validation set:', accuracy)\n        print('Difference of  - training and val sets:', round(training_set_accuracy - accuracy, 2))\n        print('Balanced accuracy is', balanced_acc)\n        print('Recall (What proportion of actual positives was predicted correctly?):', recall_sc)\n        print('Precision (What proportion of positive predictions was actually correct?):', precision_sc)\n        print('F1 Score - validation set: ', the_f1_score)\n        # print('Area under the ROC curve of', clf_name, 'is', roc_auc_sc)\n        print('Quadratic Kappa - validation set - is: ', quadratic_kappa)\n        \n        if do_kfold == True:\n            print('Cross Val Score of', clf_name, ' is: ', cross_val_score(classifier, X, y, cv=3))\n        \n        print('\\n')\n        \n        if graph_results == True:\n            pass\n            # Graph this!\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_MODEL == True:\n    # 'Linear' Classifiers\n    dum_clf = ('Dummy_Classifier', DummyClassifier())\n    logreg_clf = ('Logistic_Regression', LogisticRegression(solver = 'lbfgs', \n                                                            multi_class = 'auto', \n                                                            max_iter = 100))\n    svc_clf = ('SVC', SVC(gamma = 'scale'))\n    svc_poly = ('SVC_Poly', SVC(kernel = 'poly', degree = 3, C = 5, coef0 = 1, gamma = 'scale'))\n    \n    # Tree Classifiers\n    lb_clf = ('LightGBM', lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                             objective = 'multiclass',\n                                             is_unbalance = True,\n                                             lambda_l1 = 8))\n    rf_clf = ('Random_Forest', RandomForestClassifier(n_estimators = 500,\n                                                      min_impurity_decrease =.0065,\n                                                      class_weight = 'balanced_subsample'))\n    xgb_clf = ('XGBoost', xgb.XGBClassifier(reg_alpha = 10))\n    cb_clf = ('Catboost', cb.CatBoostClassifier(verbose=0))\n    \n    # Lists of these classifiers\n    scaled_classifiers = [dum_clf,logreg_clf, svc_clf,]\n    tree_classifiers = [lb_clf, rf_clf, xgb_clf]\n    quarantined_classifiers = [cb_clf]\n    all_classifiers = scaled_classifiers + tree_classifiers + quarantined_classifiers\n    \n    # Function call to fit.  Set the first position to what classifier list you want to use.\n    fit_baseline_classifiers(tree_classifiers, X_train, y_train, use_scaled = True, do_kfold = False)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FastAI Baseline #"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_FAST_AI = False:\n    \nif DO_FAST_AI = True:\n    from fastai.tabular import *\n\n    dep_var = 'accuracy_group'\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n    cont_names = ['education-num', 'hours-per-week', 'age', 'capital-loss', 'fnlwgt', 'capital-gain']\n    procs = [FillMissing, Categorify, Normalize]\n    \n    learn = tabular_learner(data, layers=[200,100], metrics=accuracy)\n    learn.fit(5, 1e-2)\n    learn.save('mini_train')\n    \n    learn.show_results()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reporting Pipeline for One Classifier #"},{"metadata":{},"cell_type":"markdown","source":"Best parameters so far\n* xgb max depth = ~22\n* reg_alpha = 3\n* reg_lambda\n* n_estimators\n* learning_rate\n* booster\n* gamma \n* importance_type"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_BASELINE_REPORT = False\nDO_VALIDATION_CURVE = False\nDO_LEARNING_CURVE = False\nDO_CV_SCORE = False\nDO_MUTUAL_INFORMATION = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_REPORT == True:\n    \n    from yellowbrick.model_selection import ValidationCurve, LearningCurve, CVScores, RFECV\n    \n    # Reporting parameters applicable to all reports\n    DO_BASELINE_REPORT = True\n    b_clf_to_study_name = \"xgb\"\n    b_clf_to_study = xgb.XGBClassifier(max_depth = 22, \n                                       reg_alpha = 3)\n    cv = StratifiedKFold(n_splits=12)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_REPORT == True & DO_VALIDATION_CURVE == True:\n\n    parameter_to_study = 'reg_lambda'\n    param_range_study = np.arange(1, 15, 2)\n\n    # Validation curve to see performance changes on a particular parameter\n    val_curve = ValidationCurve(b_clf_to_study,\n                               param_name = parameter_to_study,\n                               param_range = param_range_study,\n                               n_jobs = -1)\n    val_curve.fit(X_train, y_train)\n    val_curve.finalize()\n    val_curve.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'val_curve.jpg'))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_REPORT == True & DO_LEARNING_CURVE == True:\n    \n    train_sizes = np.linspace(0.3, 1.0, 10) # in percentages\n    \n    # Learning curve to see performance changes with increased sample size.\n    cv = StratifiedKFold(n_splits=12)\n    \n    learn_curve = LearningCurve(b_clf_to_study, \n                                scoring='f1_weighted',\n                                cv = cv,\n                                train_sizes=train_sizes, \n                                n_jobs=-1)\n    \n    learn_curve.fit(X_train, y_train)\n    learn_curve.finalize()\n    learn_curve.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'learn_curve.jpg'))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_REPORT == True & DO_CV_SCORE == True:\n    \n    # CV Score to see performance changes changes within a fold dramatically.\n    cv = StratifiedKFold(n_splits=12)\n    \n    cv_scores = CVScores(b_clf_to_study, \n                                scoring='f1_weighted',\n                                cv = cv)\n    \n    cv_scores.fit(X_train, y_train)\n    cv_scores.finalize()\n    cv_scores.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'cv_scores.jpg'))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_BASELINE_REPORT == True & DO_MUTUAL_INFORMATION == True:\n    from sklearn.feature_selection import mutual_info_classif\n    \n    mutual_info_rep = mutual_info_classif(X_train, y_train)\n                        \n    fig, ax = plt.subplots()\n    \n    mutual_info_df = pd.DataFrame({'feature                  m      rd':X_train.columns,\n                                  'bits_shared':mutual_info_rep}).set_index('feature')\n    mutual_info_df_top25 = mutual_info_df.sort_values(by = 'bits_shared',\n                          ascending = False)[:25]\n    \n    mutual_info_df_top25.plot.barh(ax = ax)\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'mutual_information.jpg'))\n    except:\n        pass    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Load feature importances of the baseline so we can reduce the dimensionality with feature selection. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_feature_importances(clf_name, classifier, X, y):\n    viz = FeatureImportances(classifier)\n    viz.fit(X, y)\n    viz.show()\n    save_current_fig(str(clf_name + 'feature_imp.jpg'))\n    \n    saved_feature_importances = pd.DataFrame(list(zip(viz.features_, viz.feature_importances_)))\n    \n    feature_importances_df = pd.DataFrame()\n    feature_importances_df[str(clf_name + '_features')] = viz.features_\n    \n    feature_importances_df[str(clf_name + '_importances')] = viz.feature_importances_\n    feature_importances_df.sort_values(inplace = True, by = str(clf_name + '_importances'), ascending = False)\n    \n    feature_importances_df.to_csv(str(clf_name +' feat_imp.csv'), index=False)\n    \n    return feature_importances_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_feat = xgb.XGBClassifier(reg_alpha = 10)\n\nxgb_feat.fit(X_train, y)\nxgb_feature_importances = report_feature_importances('XGBoost', xgb_feat, X_train, y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce test_baseline to just the 100 top features. ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('top100_xgb has the filtered feature importances of X_train')\nprint('X_train still has all of the features')\n\ntop100_xgb = xgb_feature_importances[:100]\ntop100_xgb_names = top100_xgb['XGBoost_features'].reset_index(drop = True)\n\nprint('\\nSample of top 25 feature importances')\nprint(top100_xgb_names.head(25))\n\n# xgb_clf[1] is the classifier in the named tuples of classifiers above.\nxgb_feat = xgb_clf[1]\nX_top100 = X_train.loc[:,top100_xgb_names]\n\nxgb_feat.fit(X_top100 , y)\nxgb_feature_importances = report_feature_importances('XGBoost', xgb_feat, X_top100, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the best 100 features in the test set and assign to test_baseline_top100\ntest_baseline_top100 = test_baseline.loc[:,top100_xgb_names]\nprint('Top 100 features filtered in test set and assigned to test_baseline_top100')\nprint('test_baseline still has all the features.')\n\ntest_baseline_top100.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Submission and 100 Features Submission ##"},{"metadata":{},"cell_type":"markdown","source":"**Baseline submission with all features **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check shape of everything:\nbs_X_train, bs_y_train, bs_X_test = X_train, y, X_test\nprint(bs_X_train.shape, bs_y_train.shape, bs_X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Are any columns missing from the training and testing set?\nprint('Are any columns missing from the training and testing set?')\nprint(list(set(bs_X_train).difference(set(bs_X_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a submission dataframe from the best baseline.\n# Fit to best 100 feature set or whole thing.\n# Set the X training set to top100_xgb or X_train\n# y set to y or estimated targets.\n\n# classifier[1] is the named classifier in the named tuples\n# Converted to an np.array because of 'JSON column names' problem.\nbaseline_submission_clf = rf_clf[1]\nbaseline_submission_clf.fit(bs_X_train, bs_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_submision_preds = baseline_submission_clf.predict(bs_X_test.fillna(0)).astype(int)\nprint(baseline_submision_preds[:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission with just 100 features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_100_submission_clf = rf_clf[1]\nbaseline_100_submission_clf.fit(X_top100, bs_y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_100_submision_preds = baseline_100_submission_clf.predict(test_baseline_top100.fillna(0)).astype(int)\nprint(baseline_100_submision_preds[:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TESTING Disabled - Grid Search for LightGBM Submission ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_GRID_SEARCH = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DO_GRID_SEARCH == True:\n    from sklearn.model_selection import GridSearchCV\n\n    param_grid = {\n        'num_leaves': [31, 127],\n        'reg_alpha': [0.1, 0.5],\n        'min_data_in_leaf': [30, 50, 100, 300, 400],\n        'lambda_l1': [0, 1, 1.5],\n        'lambda_l2': [0, 1]\n        }\n\n    grid_kfold = KFold(n_splits = 3, shuffle = True, random_state = 42).split(X = X, y = y)\n    gsearch = GridSearchCV(estimator = baseline_submission_clf, param_grid = param_grid, cv = grid_kfold, verbose = 2)\n    lgb_model = gsearch.fit(X=X, y=y)\n    \n    print(lgb_model.best_params_, lgb_model.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and Processing Data after Baseline #"},{"metadata":{},"cell_type":"markdown","source":"** Create a dataframe with all labelled training data merged on installation ID and game_session **"},{"metadata":{"trusted":true},"cell_type":"code","source":"MERGE_TRAIN_AND_LABELS = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# If you wanted to graph things to see their relationships with the targets\n\nif MERGE_TRAIN_AND_LABELS == True:\n    annotated_train = train_labels.merge(train, on = MERGE_ON_COLS)\n    annotated_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_EDA = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DO_EDA == True:\n    # Create a drop function here\n    # IF DO_DROP_COLUMNS == True\n    # DROPPED_COLS == [\"event_id\", \"game_session\", \"timestamp\"]\n\n    dropped_cols = [\"game_session\", \"timestamp\"]\n\n    train.drop(dropped_cols, axis = 1, inplace = True)\n    test.drop(dropped_cols, axis = 1, inplace = True)\n    \n    # Get rid of training samples that don't have labels.\n    # Check https://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline\n    train = train[train.index.isin(train_labels.index.unique())]\n    train.shape\n    \n    # Is this redundant?\n    annotated_train = annotated_train[annotated_train.index.isin(train_labels.index.unique())]\n    annotated_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLP Feature Engineering ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"ENGINEER_NLP_FEATURES = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\nif ENGINEER_NLP_FEATURES == True:\n    event_stream = train['event_data']\n    \n    # Ignore these words b/c they are redundant categories.\n    # What do 'coordinates' do?\n    stop_word_list = ['event_code', 'game_time', 'event_count', 'game_time', 'title', 'type', 'world', 'media_type', 'audio', 'duration', 'total_duration']\n\n    important_words = ['description', 'identifier']\n    count_vec = CountVectorizer(stop_words = stop_word_list,\n                               token_pattern = ':\\D+:',\n                               max_df = 1000,\n                               min_df = 2)\n\n    # BUG: Still catches digits and stopwords...\n    # Make this into an 'apply' function or a for loop that adds this to each installation ID.\n    ##### count_vec.fit_transform(event_stream)\n\n    ##### print(count_vec.vocabulary_)\n    \n    # Export this for faster processing time?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unsupervised Exploration and Visualization ##\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_CLUSTERS = 4  # For kmeans and embedding.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Semi-Supervised Confidence Labelling With LogReg, XGB ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group all samples of test and unlabeled + labeled train under total_samples\ntotal_samples = pd.concat([X_train, train_unlabelled, X_test])\ntotal_samples.shape\ntotal_index = total_samples.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode Clusters With Semi-Supervised Learning ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"PSEUDO_LABEL = False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nif PSEUDO_LABEL == False:\n    pass\n    \nif PSEUDO_LABEL == True:\n    train_unlabelled_top100 = train_unlabelled.loc[:, top100_xgb_names]\n\n    semi_preds = pd.DataFrame()\n    semi_preds['installation_id'] = train_unlabelled_top100.index\n    semi_preds.set_index('installation_id', inplace = True)\n\n    lr_semi = LogisticRegression(verbose = 1)\n    lr_semi.fit(X_top100, y)\n\n    semi_preds['lr_cat'] = lr_semi.predict(train_unlabelled_top100)\n    print(semi_preds.head())\n\n    lr_semi_prob = pd.DataFrame(lr_semi.predict_proba(train_unlabelled_top100))\n    lr_semi_prob['installation_id'] = train_unlabelled_top100.index\n    lr_semi_prob.set_index('installation_id', inplace = True)\n\n    lr_semi_colnames = ['lr_prob_0', 'lr_prob_1', 'lr_prob_2', 'lr_prob_3']\n    lr_semi_prob.columns = lr_semi_colnames\n\n    # print(lr_semi_prob.shape)\n    print(lr_semi_prob.head())\n\n    confidence_threshold = 0.9\n\n    \"\"\"\n    # Skip Logreg\n    lr_is_confident0_mask = lr_semi_prob.loc[:, 'lr_prob_0'] > confidence_threshold\n    lr_confident0s = lr_semi_prob[lr_is_confident0_mask]\n    print('Logistic Regression found', lr_confident0s.shape[0], 'rows with above threshold confidence for class 0')\n\n    lr_is_confident1_mask = lr_semi_prob.loc[:, 'lr_prob_1'] > confidence_threshold\n    lr_confident1s = lr_semi_prob[lr_is_confident1_mask]\n    print('Logistic Regression found', lr_confident1s.shape[0], 'rows with above threshold confidence for class 1')\n\n    lr_is_confident2_mask = lr_semi_prob.loc[:, 'lr_prob_2'] > confidence_threshold\n    lr_confident2s = lr_semi_prob[lr_is_confident2_mask]\n    print('Logistic Regression found', lr_confident2s.shape[0], 'rows with above threshold confidence for class 2')\n\n    lr_is_confident3_mask = lr_semi_prob.loc[:, 'lr_prob_3'] > confidence_threshold\n    lr_confident3s = lr_semi_prob[lr_is_confident3_mask]\n    print('Logistic Regression found', lr_confident3s.shape[0], 'rows with above threshold confidence for class 3')\n    \"\"\"\n\n    # Predict for labels\n    xgb_semi = xgb.XGBClassifier(verbose = 1, reg_alpha = 5)\n    xgb_semi.fit(X_top100, y)\n\n    xgb_semi_preds = xgb_semi.predict(train_unlabelled_top100)\n    print(xgb_semi_preds[:5])\n\n    xgb_semi_prob = pd.DataFrame(xgb_semi.predict_proba(train_unlabelled_top100))\n    xgb_semi_prob['installation_id'] = train_unlabelled_top100.index\n    xgb_semi_prob.set_index('installation_id', inplace = True)\n\n    xgb_semi_colnames = ['xgb_prob_0', 'xgb_prob_1', 'xgb_prob_2', 'xgb_prob_3']\n    xgb_semi_prob.columns = xgb_semi_colnames\n    xgb_semi_prob.head()\n\n    confidence_threshold = 0.9\n\n    xgb_is_confident0_mask = xgb_semi_prob.loc[:, 'xgb_prob_0'] > confidence_threshold\n    xgb_confident0s = xgb_semi_prob[xgb_is_confident0_mask]\n    print('XGBoost found', xgb_confident0s.shape[0], 'rows with above threshold confidence for class 0')\n\n    xgb_is_confident1_mask = xgb_semi_prob.loc[:, 'xgb_prob_1'] > confidence_threshold\n    xgb_confident1s = xgb_semi_prob[xgb_is_confident1_mask]\n    print('XGBoost found', xgb_confident1s.shape[0], 'rows with above threshold confidence for class 1')\n\n    xgb_is_confident2_mask = xgb_semi_prob.loc[:, 'xgb_prob_2'] > confidence_threshold\n    xgb_confident2s = xgb_semi_prob[xgb_is_confident2_mask]\n    print('XGBoost found', xgb_confident2s.shape[0], 'rows with above threshold confidence for class 2')\n\n    xgb_is_confident3_mask = xgb_semi_prob.loc[:, 'xgb_prob_3'] > confidence_threshold\n    xgb_confident3s = xgb_semi_prob[xgb_is_confident3_mask]\n    print('XGBoost found', xgb_confident3s.shape[0], 'rows with above threshold confidence for class 3')\n\n    print(xgb_confident0s.head())\n    print(xgb_confident3s.head())\n\n    # Define new X_train style dataframes with just the confident threes and zeroes\n    confident_zeroes = train_unlabelled.loc[xgb_confident0s.index, top100_xgb_names]\n    confident_threes = train_unlabelled.loc[xgb_confident3s.index, top100_xgb_names]\n    print(confident_zeroes.shape)\n    print(confident_threes.shape)\n\n    # Concatenate  0's\n    X_train = pd.concat([X_top100, confident_zeroes], axis = 0)\n    X_train.shape\n\n    # Concatenate 3's\n    X_train = pd.concat([X_train, confident_threes], axis = 0)\n    X_train.shape\n\n    print(y.shape)\n    y = pd.DataFrame(y)\n    print(y.head())\n\n    y_confident_zeroes = pd.DataFrame(confident_zeroes.index)\n    y_confident_zeroes.set_index('installation_id', inplace = True, drop = True)\n    y_confident_zeroes['accuracy_group'] = 0\n\n    print(y_confident_zeroes.shape)\n    print(y_confident_zeroes.head())\n\n    y = pd.concat([y, y_confident_zeroes], axis = 0)\n    print(y.shape)\n    print(y.head())\n\n    y_confident_threes = pd.DataFrame(confident_threes.index)\n    y_confident_threes.set_index('installation_id', inplace = True, drop = True)\n    y_confident_threes['accuracy_group'] = 3\n    print(y_confident_threes.shape)\n\n    y = pd.concat([y, y_confident_threes], axis = 0)\n    print(y.shape)\n    print(y.head())\n    print(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if EMBED_TEST_FIT_CLUSTER_ON_TRAIN_FEATURE == True:\n    # Create a function for this\n    \n    # Store the X index\n    X_index = X.index\n    test_index = test_baseline.index\n    # Does labelling the predictions lead to better models?\n\n    # Scale the testing data\n    test_scaled = MinMaxScaler()\n    test_scaled = test_scaled.fit_transform(test_baseline.fillna(0))\n\n    # The training data is already scaled at \n\n    # Fit kmeans to the scaled test data.\n    kmeans_test = KMeans(n_clusters = N_CLUSTERS, random_state=0)\n    kmeans_test.fit(test_scaled)\n\n    # Predict the clusters that would result with characteristics similar to the training set.\n    kmeans_preds_on_train = kmeans_test.predict(X_scaled)\n\n    # Categorically encode these categories.\n    kmeans_preds_cat_encoded_train = pd.get_dummies(kmeans_preds_on_train, prefix='kmeans_')\n    kmeans_preds_cat_encoded_train.index = X_index\n\n    # Do the same thing with the test set.\n    kmeans_preds_on_test = kmeans_test.predict(test_scaled)\n\n    kmeans_preds_cat_encoded_test = pd.get_dummies(kmeans_preds_on_test, prefix='kmeans_')\n    kmeans_preds_cat_encoded_test.index = test_index\n    \n    train = pd.concat([X, kmeans_preds_cat_encoded_train], axis = 1)\n    test = pd.concat([test_baseline, kmeans_preds_cat_encoded_test], axis = 1)\n    \n    train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MinMax Scale Data for Clustering (Hierarchical, Kmeans, Other) ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reassign inputs for unsupervised learning: \nX_train, y_train, X_test = X_train, y, test_baseline\nprint(X_train.shape, y_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data for cluster analysis\ntrain_scaled = MinMaxScaler()\ntrain_scaled = train_scaled.fit_transform(X_train)\n\n# Scale the testing data.\n# test_preprocessed = X_test.fillna(0).astype(int)\n\ntest_scaled = MinMaxScaler()\ntest_scaled = test_scaled.fit_transform(X_test)\n\n# Scale everything\ntotal_samples_scaled = MinMaxScaler()\ntotal_samples_scaled = total_samples_scaled.fit_transform(total_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hierarchical Clustering Exploration ###"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if EXPLORE_WITH_HIERARCHICAL_C == True:\n    from scipy.cluster.hierarchy import dendrogram\n\n    def get_hc_distances(X, verbose = 0):\n        distances = linkage(X, method=\"centroid\", metric=\"euclidean\")\n        return distances\n\n    def plot_dendrogram(distances):\n        dn = dendrogram(distances)\n\n    def create_hc_clusters(distances):\n        hc_clusters = fcluster(distances, 4, criterion=\"distance\")\n        return hc_clusters\n\n    if EXPLORE_WITH_HIERARCHICAL_C == True:\n        distances = get_hc_distances(train_scaled_eda)\n        plot_dendrogram(distances)\n\n        # Save the figure\n        save_current_fig('Dendrogram.jpg')\n\n        # Clusters not working yet.\n        hc_clusters = create_hc_clusters(distances)\n        hc_clusters[:50]\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection ###\n\n* Use principal component analysis (PCA) to determine features that explain the most variance.\n* Use Nonlinear PCA\n* Use random forests to determine feature importances."},{"metadata":{},"cell_type":"markdown","source":"## PCA ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatterplot_of_2pca(X, X_pca, labels, use_matplotlib = True, use_plotly = False):\n    X_pca = pd.DataFrame(X_pca, index = X.index)\n\n    if use_matplotlib == True:\n        plt.scatter(x=X_pca.loc[:,0], y=X_pca.loc[:,1], c = labels, cmap = 'RdBu')\n        plt.title('Scatterplot of 2 highest principal components')\n        plt.show()\n\n    if use_plotly == True:\n        fig = px.scatter(X_pca.loc[:,0], y = X_pca.loc[:,1], color = y_train)\n        fig.update_layout(title = 'Scatterplot of 2 highest principal components')\n        fig.show()\n    \n    save_current_fig('Scattplot_2_pcas_train.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA on everything**"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_PCA = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nif DO_PCA == True:\n    PCA_N_COMPONENTS = 100\n\n    # Instantiate components of the pipeline\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n    pipeline.fit(total_samples.fillna(0))   \n\n    total_pca = pca.transform(total_samples_scaled)\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_ratio_)\n    plt.title(\"Principal Component Analysis of Total Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Total_Set.jpg\")\n    \n    scatterplot_of_2pca(total_samples, total_pca, labels = None)\n    \n    total_pca_df = pd.DataFrame(total_pca, index = total_index)\n\n    X_tr_pca_labelled = total_pca_df.loc[train_y_baseline.index.unique(), :]\n    X_tr_pca_unlabelled = total_pca_df.loc[train_unlabelled.index.unique(), :]\n    X_test_pca = total_pca_df.loc[test_baseline.index.unique(), :]\n    \n    X_tr_pca_labelled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predict accuracy group on unlabelled data**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DO_PCA == True:\n    pca_xgb = xgb.XGBClassifier()\n    pca_xgb.fit(X_tr_pca_labelled, y)\n    \n    unlabelled_pca_preds = pca_xgb.predict(X_tr_pca_unlabelled)\n    unlabelled_preds_df = pd.DataFrame(unlabelled_pca_preds, index = train_unlabelled.index, columns = ['accuracy_group'])\n    unlabelled_preds_df.head()\n    \n    # Concatenate the unlabelled preds to total_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA on the training set **"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DO_PCA == True:\n    PCA_N_COMPONENTS = 100\n\n    # Instantiate components of the pipeline\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n    pipeline.fit(X_train.fillna(0))   # On the whole thing or just the training set?\n\n    X_train_pca = pca.transform(train_scaled)\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_ratio_)\n    plt.title(\"Principal Component Analysis of Train Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Train_Set.jpg\")\n    \n    # This Decodes the encoded compressed PCA information.  You can use this for your models.\n    X_train_PCA_inverse = pca.inverse_transform(X_train_pca)\n    X_train_PCA_inverse = pd.DataFrame(data = X_train_PCA_inverse, index = X_train.index)\n    \n    scatterplot_of_2pca(X_train, X_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA on the test set**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Instantiate components of the pipeline\nDO_PCA_ON_TEST = False\n\nif DO_PCA_ON_TEST == True:\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n\n    pipeline.fit(X_test.fillna(0))\n\n    X_test_pca = pca.transform(X_test.fillna(0))\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_)\n    plt.title(\"Principal Component Analysis of Test Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Test_Set.jpg\")\n\n    scatterplot_of_2pca(X_test, X_test_pca, None)\n\n    X_test_PCA_inverse = pca.inverse_transform(X_test_pca)\n    X_test_PCA_inverse = pd.DataFrame(data = X_test_PCA_inverse, index = X_test.index)\n\n    fit_baseline_classifiers(tree_classifiers, X_train_pca, y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Kernel PCA ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_KERNEL_PCA = True\nPCA_N_COMPONENTS = 50\n\nif DO_KERNEL_PCA == True:\n    from sklearn.decomposition import KernelPCA\n    \n    def do_kernel_pca():\n        pass\n    \n    #Apply kernel to train\n    \n    kernel_pca_train = KernelPCA(n_components = PCA_N_COMPONENTS, kernel = 'linear', fit_inverse_transform = True)\n    \n    kernel_pca_train_arr = kernel_pca_train.fit_transform(train_scaled)\n    kernel_pca_train_df = pd.DataFrame(data = kernel_pca_train_arr, index = X_train.index)\n    \n    kernel_pca_train_inverse = kernel_pca_train.inverse_transform(kernel_pca_train_arr)\n    kernel_pca_train_inverse_df = pd.DataFrame(data = kernel_pca_train_inverse, index = X_train.index)\n    \n    # Apply kernel to test\n    kernel_pca_test = KernelPCA(n_components = PCA_N_COMPONENTS, kernel = 'rbf', fit_inverse_transform = True)\n    \n    kernel_pca_test_arr = kernel_pca_test.fit_transform(test_scaled)\n    kernel_pca_test_df = pd.DataFrame(data = kernel_pca_test_arr, index = X_test.index)\n    \n    kernel_pca_test_inverse = kernel_pca_test.inverse_transform(kernel_pca_test_arr)\n    kernel_pca_test_inverse_df = pd.DataFrame(data = kernel_pca_test_inverse, index = X_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(kernel_pca_train_df.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_KERNEL_PCA == True:\n    scatterplot_of_2pca(X = X_train, \n                        X_pca = kernel_pca_train_df, \n                        labels = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(kernel_pca_test_df.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_KERNEL_PCA == True:\n    scatterplot_of_2pca(X = X_test, \n                        X_pca = kernel_pca_test_df, \n                        labels = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_KERNEL_PCA == True:\n    fit_baseline_classifiers(tree_classifiers, kernel_pca_train_df, y)\n    kernel_pca_train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create submission file with nonlinear pca since it did well? ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pca = xgb.XGBClassifier()\nxgb_pca.fit(kernel_pca_train_df, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_preds = xgb_pca.predict(kernel_pca_test_df)\npca_preds[200:600]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manifold Visualizations After PCA ##"},{"metadata":{},"cell_type":"markdown","source":"### T-SNE Grid Search Experiment ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"EXPLORE_WITH_TSNE = False\n\nif EXPLORE_WITH_TSNE == True:\n    from sklearn.manifold import TSNE\n    \n    tsne_model = TSNE(learning_rate = 250, verbose = 1, perplexity = 500)\n    print(tsne_model)\n    \n    tsne_transformed = tsne_model.fit_transform(kernel_pca_train_df)\n    \n    # Define the x and y axes of the TSNE plot.\n    tsne_xs = tsne_transformed[:,0]\n    tsne_ys = tsne_transformed[:,1]\n\n    # Plot the TSNE\n    plt.scatter(tsne_xs, tsne_ys, c = y.values, cmap = 'RdBu')\n    plt.title('T-SNE Clusters of Training Dataset - Axes do not have meaning')\n    plt.show()\n    save_current_fig('T_SNE_of_Training_Dataset.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if EXPLORE_WITH_TSNE == True:\n    from sklearn.manifold import TSNE\n    \n    tsne_model = TSNE(learning_rate = 250, verbose = 1, perplexity = 500)\n    print(tsne_model)\n    \n    tsne_transformed = tsne_model.fit_transform(total_pca)\n    \n    # Define the x and y axes of the TSNE plot.\n    tsne_xs = tsne_transformed[:,0]\n    tsne_ys = tsne_transformed[:,1]\n\n    # Plot the TSNE\n    plt.scatter(tsne_xs, tsne_ys)\n    plt.title('T-SNE Clusters of All datapoints - Axes do not have meaning')\n    plt.show()\n    save_current_fig('T_SNE_of_Total_Dataset.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EXPLORE_WITH_KMEANS = True\n\nfrom sklearn.metrics.cluster import homogeneity_score\n\ndef explore_with_kmeans(X, data_name, ground_truth = None, n_clusters = N_CLUSTERS):\n    # Scale before using with minmax or standardscaler.\n    \n    kmeans = KMeans(n_clusters = n_clusters, random_state=0)\n    kmeans.fit(X)\n    kmeans_preds = kmeans.predict(X)\n\n    clusters = kmeans_preds\n\n    # KMeans Distribution\n    # Be nice if this was normalized?\n    \n    kmeans_filename = 'Count_Plot_'+ str(data_name) + '_Cluster_Counts.jpg'\n    \n    sns.countplot(x = clusters).set_title(kmeans_filename)\n    plt.plot()\n    \n    save_current_fig(kmeans_filename)\n    \n    print('KMEANS Intertia of', data_name, ':', kmeans.inertia_)\n    # ADD - Write intertia to report.\n    \n    # Homogeneity of classes y (ground_truth) is given\n    try:\n        print('Homogeneity score is:', homogeneity_score(ground_truth, kmeans_preds))\n    except:\n        pass\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore_with_kmeans(X = kernel_pca_train_df, \n                    data_name = 'Train', \n                    ground_truth = y_train, \n                    n_clusters = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore_with_kmeans(X = kernel_pca_test_df, \n                    data_name = 'Test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Splits - Several Splits to test Comparative Accuracy ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which train and test set would you like to use?\n# Use train and test for \"original\" dataframe\n# Use X_train_poly and X_test_poly for polynomial features.\n# X_train = train\n# X_test = test\n\ny_train = y_train[y_train.index.isin(X_train.index.unique())]\ny_train.shape\n\nprint('Make sure features have good shape')\nprint(X_train.shape, '\\n', y_train.shape, '\\n', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test splitting.\n# Use this if doing original dataframe\n# If you use this you must retrain classifiers from scratch so as not to overfit.\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train, y_train, stratify=y_train, shuffle = True)\nprint('Splits', X_train_s.shape, y_train_s.shape, X_test_s.shape, y_test_s.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit Models After Feature Selection, Semi-Supervised Labelling, etc. ###"},{"metadata":{},"cell_type":"markdown","source":"### Instantiate All Classifiers and Fit ###"},{"metadata":{},"cell_type":"markdown","source":"Feature elimination and tpot automl"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Feature Selection + Hyperparameter Tuning Functions\n\n# Recursive Feature Elimination... Return Best Features\ndef recursive_feature_elimination(classifier, X_train, y_train):\n    rec_feat_elim = RFECV(classifier, C=1)\n    rec_feat_elim.fit(X_train, y_train) \n    rec_feat_elim.show()\n\n# Bayesian Hyperparamter Tuning\n\n# TPOT AutoML  Tuning\n# Add y_test as a parameter when doing train_test splits and validation.\ndef tpot_automl(X_train, y_train, X_test, generations=4, population_size=20, verbosity=3):\n    tpot = TPOTClassifier(generations=generations, \n                          population_size=population_size, \n                          cv=5,\n                          random_state=42, \n                          verbosity=verbosity)\n    \n    tpot.fit(X_train, y_train)\n    \n    # tpot.score(X_test, y_test)\n    \n    tpot_preds = tpot.predict(X_test)\n    \n    print(tpot.fitted_pipeline_)\n    tpot.export('tpot_pipeline.py')\n    \n    return tpot_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is just the instances, we will have to refit them to the splits to not overfit.\nUSE_SAME_CLASSIFIERS_FOR_END = True\n\nif USE_SAME_CLASSIFIERS_FOR_END == True:\n    pass\nelse:\n    # Random Forest Classifier\n    rf_clf = RandomForestClassifier(n_estimators = 500,\n                                   max_depth = 7,\n                                   class_weight='balanced',\n                                   n_jobs=-1)\n\n    # XGBoost Classifier\n    # xgb_data_matrix = xgb.DMatrix(data = X_train_s, label = y_train_s)\n    xgb_clf = xgb.XGBClassifier(n_jobs=-1,\n                                num_feature = 30,\n                                nfold=5)\n\n    # LightGBM Classifier\n    lb_clf = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                             objective = 'multiclass',\n                                             is_unbalance = True,\n                                             lambda_l1 = 8)\n\n    # Catboost Classifier\n    cb_clf = cb.CatBoostClassifier(verbose=0)\n\n    #Bagging Classifier\n    bag_clf = BaggingClassifier(base_estimator=cb_clf, n_estimators=10, random_state=0, n_jobs=-1)\n\n    #TPOT Classifier\n\n    # Old List of classifiers\n    ## classifiers = [('Random Forest', rf_clf), ('XGBoost', xgb_clf), ('Bagging Classifier', bag_clf)]\n\n    # Just the three best.\n    classifiers = [('XGBoost', xgb_clf), ('LightGBM', lgb_clf), ('Catboost', cb_clf)]\n    \n    # Empty list to return predictions\n    preds = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stratified KFold Loop**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfold_loop(classifiers, X_train, y_train):\n    str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n    fold = 0\n    \n    for train_index, test_index in str_kf.split(X_train, y_train):\n        # Obtain training and testing folds\n        \n        cv_X_train, cv_X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n        cv_y_train, cv_y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n        \n        print('Fold: {}'.format(fold))\n        print('CV train shape: {}'.format(cv_X_train.shape))\n        \n        fit_classifiers(classifiers, cv_X_train, cv_y_train, cv_X_test, cv_y_test)\n        \n        fold += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kfold_loop(classifiers, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Fit to Experimental Validation Sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run Functions Without Polynomail Features\n# Remove 'baseline' from classifier name.\n\nprint('Running classifiers after feature modifications, compared to baseline')\nfit_baseline_classifiers(tree_classifiers, np.array(X_train_s), y_train_s)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lightboost Feature Importances ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"LB_FEAT_IMP = False\n\nif LB_FEAT_IMP == True:  \n    lb_clf_feat = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                                 objective = 'multiclass',\n                                                 is_unbalance = True,\n                                                 lambda_l1 = 8)\n\n    lb_clf_feat.fit(X_train_s, y_train_s)\n    lgb.plot_importance(lb_clf_feat, max_num_features = 50, figsize = (20,20), title = 'Feature Importance of LightGBM Model')\n    save_current_fig('Feature_importance_of_lightGBM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Decision Regions with several high-importance variables ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_2d_decision_regions(X, y, clf):\n    return plot_decision_regions(X = X, y = y , clf = clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the two variables\n\nvar1 = 'accuracy'\nvar2 = 'accuracy_min'\n\nX_plot_decision = pd.DataFrame(X_train_s[var1].fillna(0))  # Create first variabel column\nX_plot_decision[var2] = X_train_s[var2].fillna(0)  # Create second variable column\n\nX_plot_decision = np.array(X_plot_decision)\ny_plot_decision = np.array(y_train_s)\n\nxgb_dec_clf = xgb.XGBClassifier()\nxgb_dec_clf.fit(X_plot_decision, y_train_s)\n\n# See if two dimensions is enough to converge on a decision region.\nax = plot_2d_decision_regions(X_plot_decision, y_plot_decision, xgb_dec_clf)\n# print(accuracy_score())\nplt.title('Decision Regions of ' + str(var1) + ' and ' + str(var2))\nsave_current_fig('Decision_Region.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Metrics Helper Functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reporting Functions\ndef display_confusion_matrix(classifier, X_train, y_train, X_test, y_test):\n    try:\n        cm = ConfusionMatrix(classifier)\n        cm.fit(X_train, y_train)\n        cm.score(X_test, y_test)\n        cm.show() \n    except:\n        pass\n\n\ndef display_classification_report(classifier, X_train, y_train, X_test, y_test):\n    try:\n        cr = ClassificationReport(classifier)\n        cr.fit(X_train, y_train)\n        cr.score(X_test, y_test)\n        cr.show()\n    except:\n        pass\n    \ndef display_ROC(classifier, X_train, y_train, X_test, y_test):\n    try:\n        roc = ROCAUC(classifier)\n        roc.fit(X_train, y_train)\n        roc.score(X_test, y_test)\n        roc.show()\n    except:\n        pass\n    \ndef display_feature_importances(classifier, X_train, y_train):\n    try:\n        feature_imp = FeatureImportances(classifier)\n        feature_imp.fit(X_train, y_train)\n        print(list(feature_imp.features_))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display important reportings here\n# Ideally modify functions for subplots\n\ndef report_all(classifiers, X_train_display, y_train_display, X_test_display, y_test_display):\n    for clf_name, classifier in classifiers:\n\n        display_confusion_matrix(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n        display_classification_report(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n        display_ROC(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n\n        # Not all classifiers will have this.\n        try:\n            display_feature_importances(display_feature_importances, X_train_display, y_train_display)\n        except:\n            print('did not compute feature importance for: ', clf_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the report to the features and then display\nX_train_display, y_train_display, X_test_display, y_test_display = X_train_s, y_train_s, X_test_s, y_test_s\n\n# Report all regular features\n## Remove Catboost b/c it is causing problems in the reporting.\nclassifiers_report = tree_classifiers.copy()\n\nreport_all(classifiers_report, X_train_display, y_train_display, X_test_display, y_test_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reporting code - Consider removing? **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the report to the regular features and then display\n## X_train_display, y_train_display, X_test_display, y_test_display = X_train_s, y_train_s, X_test_s, y_test_s\n\n# Report all regular features\n# This throws an error because I fit the same classifiers and now they have different features.\n# I'd have to run this before and after I fit them unless I have the functions return values.\n\n## report_all(classifiers, X_train_display, y_train_display, X_test_display, y_test_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_DEEP_LEARNING = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if USE_DEEP_LEARNING == True:\n    from keras.utils import to_categorical\n    from keras import models, layers\n    from keras.callbacks import EarlyStopping, ModelCheckpoint\n    from keras.optimizers import SGD\n    \n    print('Splits', X_train_s.shape, y_train_s.shape, X_test_s.shape, y_test_s.shape)\n\n    # DL Parameters\n    var_input_shape = (X_train_s.shape[1], )\n    layer_num = 400\n    activation = 'relu'\n    epochs = 40\n    batch_size = 20\n    \n    # For scaling\n    X_train_s_imp = X_train_s.fillna(0)\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train_s_imp)\n    X_train_dl = X_train_scaled\n    print('Scaled the X training split')\n    print(X_train_dl[1][:5])\n\n    X_test_s_imp = X_test_s.fillna(0)\n    scaler_test = MinMaxScaler()\n    X_test_scaled = scaler_test.fit_transform(X_test_s_imp)\n    X_test_dl = X_test_scaled\n    print('Scaled the X test split')\n    print(X_test_dl[1][:5])\n\n\n    # To_Categorical so output shape is (n,4)\n    y_train_dl = to_categorical(np.array(y_train_s))\n    print('Categorically encoded targets of training data.')\n    print(y_train_dl[1])\n\n    y_test_dl = to_categorical(np.array(y_test_s))\n    print('Categorically encoded targets of testing data.')\n    print(y_test_dl[1])\n\n    print('Splits', X_train_dl.shape, y_train_dl.shape, X_test_dl.shape, y_test_dl.shape)\n\n\n\n    # # This standard model has 3 layers with ____ nodes with \n    # a dropout layer that gets rid of 50% of the learning\n    # To encourage the learning to 'spread' throughout\n    # the network.\n    \n\n    network = models.Sequential()\n\n    network.add(layers.Dense(layer_num, input_shape = var_input_shape, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n\n    # Final layer\n    network.add(layers.Dense(4, activation='softmax'))\n\n    sgd = SGD(lr=0.005, momentum=0.4, nesterov=True)\n    \n    # Categorical cross entropy for multi-class problems\n    network.compile(optimizer = sgd,\n                   loss = 'categorical_crossentropy',\n                   metrics=['accuracy'])\n\n    # early_stopping = EarlyStopping(monitor='val_accuracy', patience = 8)\n\n    model_save = ModelCheckpoint('best_model.hdf5',\n                                save_best_only=True)\n\n    dl_history = network.fit(X_train_dl, \n                             y_train_dl, \n                             epochs = epochs, \n                             batch_size=batch_size, \n                             verbose=2,\n                            validation_data=(X_test_dl, y_test_dl),\n                            callbacks = [model_save])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if USE_DEEP_LEARNING == True:\n    plt.figure()\n    plt.plot(dl_history.history['accuracy'])\n    plt.plot(dl_history.history['val_accuracy'])\n    plt.title('Keras Model Accuracy on Training set and Validation Set')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train','Test'])\n    plt.show()\n\n    save_current_fig('Keras_Model_Acc_Training_vs_validation_set.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Preprocess Submission X_test for Neural Network**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if USE_DEEP_LEARNING == True:\n    # The submission file has more training samples,\n    # So should do better.  Process that.\n    # Process the submission training set.\n\n    X_train_submit_dl = X_train.fillna(0)\n    scaler_train_submit = MinMaxScaler()\n    X_submit_scaled = scaler_train_submit.fit_transform(X_train_submit_dl)\n    print('Scaling X of the entire TRAINING set for ensembling with neural network')\n    print(X_submit_scaled[1][:5])\n\n\n    X_test_submit_dl = X_test.fillna(0)\n    test_scaler_submit = MinMaxScaler()\n    X_test_submit_scaled = test_scaler_submit.fit_transform(X_test_submit_dl)\n    print('Scaling X of the entire TEST set for ensembling with neural network')\n    print(X_test_submit_scaled[1][:5])\n\n\n    y_train_submit_dl = to_categorical(np.array(y_train))\n    print('Categorically encode targets of ALL the training set.')\n    print(y_train_submit_dl[1])\n\n\n    layer_num = layer_num\n\n    network_submit = models.Sequential()\n\n    network_submit.add(layers.Dense(layer_num, input_shape = var_input_shape, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n\n    # Final layer\n    network_submit.add(layers.Dense(4, activation='softmax'))\n\n    # Categorical cross entropy for multi-class problems\n    network_submit.compile(optimizer = sgd,\n                   loss = 'categorical_crossentropy',\n                   metrics=['accuracy'])\n\n    # early_stopping = EarlyStopping(monitor='val_accuracy', patience = 8)\n\n    model_save = ModelCheckpoint('best_model.hdf5',\n                                save_best_only=True)\n\n    network_submit.fit(X_submit_scaled, \n                       y_train_submit_dl, \n                       epochs = epochs, \n                       batch_size=batch_size, \n                       verbose=2,\n                        callbacks = [model_save])\n\n    print(X_test_submit_scaled[1][:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if USE_DEEP_LEARNING == True:\n    dl_preds = network_submit.predict_classes(X_test_submit_scaled)\n    dl_preds.shape\n\n    model_prediction['NN_preds'] = dl_preds\n    model_prediction.head()\n\n    print(model_prediction.head(100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Post Submission Reports for Modification ###"},{"metadata":{},"cell_type":"markdown","source":"# File Submission #"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the best results here for the submission\n# This should NOT be the splits, but the whole thing.\nX_train_submit, y_train_submit, X_test_submit = X_train, y_train, X_test\n\n# CHOOSE WHICH SUBMISSION PREDICTIONS TO USE HERE\nfinal_submission_preds = dl_preds\nfinal_submission_preds_name = \"Neural Network Preds\"\n\nprint(X_train_submit.shape, y_train_submit.shape, X_test_submit.shape)\n\n# Create the empty dataframe and copy the indices of the test set.\nmodel_prediction = pd.DataFrame()\nmodel_prediction['installation_id'] = X_test_submit.index.astype(str)\n\nprint(model_prediction.shape)\n\n# Choose the final model here\nsubmission_model = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                     objective = 'multiclass',\n                                     is_unbalance = True,\n                                     lambda_l1 = 8)\n\n# Fit the model and create the predictions.\nsubmission_model.fit(np.array(X_train_submit), y_train_submit)\n\n# IF FITTING HERE: Predict on the test set from the fitted model.\n# Otherwise use another pred output from a different model.\n# like baseline_preds or PCA_preds.\n\n# MODEL\n# submission_preds = submission_model.predict(X_test_submit)\n\nsubmission_preds = final_submission_preds\n\n# load this into the submission dataframe. \nmodel_prediction['accuracy_group'] = submission_preds.astype(int)\n\n# Create a csv file out of these predictions.\nmodel_prediction.to_csv('submission.csv', index=False)\n\n# Confirm everythin looks ok.\nprint('Submission file created from', final_submission_preds_name)\nprint(model_prediction.head(30))\nprint(model_prediction.shape)\nprint(model_prediction.info())\nprint(model_prediction['accuracy_group'].value_counts())\n\n# Check output file was created in right spot.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}