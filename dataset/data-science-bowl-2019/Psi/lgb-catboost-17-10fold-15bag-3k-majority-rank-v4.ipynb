{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# if True:\n#     os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n#     os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n#     os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n#     os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n#     os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n\nimport pandas as pd\nimport numpy as np\n\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import Counter\nfrom scipy.stats import rankdata\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom itertools import chain\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom collections import defaultdict\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.spatial import Voronoi\nimport time\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm_notebook as tqdm\nimport time\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom random import random\nfrom copy import copy, deepcopy\n\nfrom joblib import Parallel, delayed\nimport multiprocessing\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, accuracy_score, cohen_kappa_score, confusion_matrix\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom functools import partial\nimport scipy as sp\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/data-science-bowl-2019/\"\n\ndef read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv(f'{path}/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv(f'{path}/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv(f'{path}/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv(f'{path}/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv(f'{path}/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef preprocess(train, test, train_labels):\n    # encode title\n    \n    #train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    #test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    train['hour'] = train['timestamp'].dt.hour\n    test['hour'] = test['timestamp'].dt.hour\n    train['weekday'] = train['timestamp'].dt.weekday\n    test['weekday'] = test['timestamp'].dt.weekday\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_labels, specs, sample_submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = preprocess(train, test, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values = {}\nfor f in [\"title\", \"event_code\", \"event_id\", \"type\"]:\n    unique_values[f] = pd.concat([train[f], test[f]]).unique()\n    \nunique_values[\"assessment\"] = pd.concat([train[train[\"type\"]==\"Assessment\"][\"title\"], test[test[\"type\"]==\"Assessment\"][\"title\"]]).unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom copy import deepcopy\nimport json\n\ndef parse_json(s):\n    s = json.loads(s)\n    ret = {}\n    for f in ['round', \"level\", \"misses\", \"correct\"]:\n        if f in s:\n            if f == \"correct\":\n                s[f] = int(s[f])\n            ret[f] = s[f]\n        else:\n            ret[f] = -1\n\n    return ret\n\ndef get_json(df):\n    x = df.event_data.apply(parse_json)\n\n    x = pd.io.json.json_normalize(x).fillna(-1)\n   # print(x)\n\n    return x\n\ndef get_data(user_sample, test_set=False):\n    \n\n    \n    all_assessments = []\n    \n    features = defaultdict(np.float32)\n    \n    for f in [\"title\", \"event_code\", \"event_id\", \"type\"]:\n        for k in unique_values[f]:\n            features[f\"{f}_{k}\"] = 0\n            \n    for f in [\"title\", \"event_code\", \"event_id\"]:\n        for k in unique_values[f]:\n            features[f\"assessment_{f}_{k}\"] = 0\n            \n    for f in [\"title\"]:\n        for k in unique_values[f]:\n            features[f\"{f}_{k}_correct\"] = -1\n            \n    user_sample = pd.concat([user_sample.reset_index(drop=True), get_json(user_sample).fillna(-1).reset_index(drop=True)], axis=1)\n            \n#     for f in range(4):\n#         features[f\"accuracy_group_{f}\"] = 0\n            \n    assessment_sequence = defaultdict(list)\n    \n    curr_assessment_sequence = defaultdict(lambda: defaultdict(list))\n\n    for i, session in user_sample.groupby('game_session', sort=False):\n        \n        for f in [\"title\"]:\n            for k in unique_values[f]:\n                features[f\"curr_assessment_{f}_{k}\"] = 0\n\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        features[\"installation_id\"] = session['installation_id'].iloc[0]     \n        features[\"timestamp\"] = session['timestamp'].iloc[0]   \n        features[\"s_title\"] = session_title\n        \n        #features[\"session_count\"] += 1\n\n        if (session_type == 'Assessment'):\n            \n            variables = {}\n            \n            features[f\"curr_assessment_title_{session_title}\"] = 1\n            \n            #features[\"assessment_count\"] += 1\n            \n            #features['hour'] = session['hour'].iloc[0]\n            #features['weekday'] = session['weekday'].iloc[0]\n            \n          #  features[\"event_count\"] = session.iloc[0][\"event_count\"]\n            \n            for f in [\"accuracy_group\", \"correct\"]:\n                features[f\"assessment_sequence_{f}_mean\"] = np.mean(assessment_sequence[f]) if len(assessment_sequence[f]) > 0 else -1\n                \n            for f in [\"accuracy_group\", \"false_attempts\", \"accuracy\"]:\n                features[f\"curr_assessment_sequence_{f}_mean\"] = np.mean(curr_assessment_sequence[session_title][f]) if len(curr_assessment_sequence[session_title][f]) > 0 else -1\n                features[f\"curr_assessment_sequence_{f}_sum\"] = np.sum(curr_assessment_sequence[session_title][f]) if len(curr_assessment_sequence[session_title][f]) > 0 else -1\n                features[f\"curr_assessment_sequence_{f}_lag1\"] = curr_assessment_sequence[session_title][f][-1] if len(curr_assessment_sequence[session_title][f]) > 0 else -1\n\n#             for a in unique_values[\"assessment\"]:\n#                 for f in [\"accuracy_group\"]:\n#                     features[f\"curr_assessment_sequence_{a}_{f}_sum\"] = np.sum(curr_assessment_sequence[a][f]) if len(curr_assessment_sequence[a][f]) > 0 else -1\n                    #features[f\"curr_assessment_sequence_{a}_{f}_lag1\"] = curr_assessment_sequence[a][f][-1] if len(curr_assessment_sequence[a][f]) > 0 else -1\n                \n            if session[\"title\"].iloc[0] == 'Bird Measurer (Assessment)':\n                all_attempts = session.query(f'event_code == 4110')\n            else:\n                all_attempts = session.query(f'event_code == 4100')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else -1\n            \n            \n            \n            \n            variables[\"true_attempts\"] = true_attempts\n            variables[\"false_attempts\"] = false_attempts\n            variables[\"accuracy\"] = accuracy\n            \n            if accuracy == -1:\n                features['accuracy_group'] = -1\n            elif accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n                \n            if true_attempts+false_attempts > 0:\n                all_assessments.append(deepcopy(features))\n                \n#                 #features[f\"assessment_title_{session_title}\"] += 1\n#                 for f in [\"event_id\"]:\n#                     for k in session[f].values:\n#                         features[f\"assessment_{f}_{k}\"] += 1\n                \n                for f in [\"accuracy_group\", \"accuracy\"]:\n                    assessment_sequence[f].append(features['accuracy_group'])\n                \n                    curr_assessment_sequence[session_title][f].append(features['accuracy_group'])\n                    \n                #features[\"accumulated_false_attempts\"] += false_attempts\n                \n                #features[f\"accuracy_group_{features['accuracy_group']}\"] += 1\n                \n            elif test_set:\n                all_assessments.append(deepcopy(features))\n                \n            for f in [\"true_attempts\", \"false_attempts\"]:\n                curr_assessment_sequence[session_title][f].append(variables[f])\n           # curr_assessment_sequence[session_title][\"len\"].append(len(session))\n        \n        else:\n            correct = [x for x in session[\"correct\"] if x != -1]\n            if len(correct) > 0:\n                assessment_sequence[\"correct\"].append(np.mean(correct))\n                \n#                 for c in correct:\n#                     if c == 0:\n#                         features[f\"{session_title}_incorrect\"] += 1\n#                     else:\n#                         features[f\"{session_title}_correct\"] += 1\n        \n        #features[f\"title_{session_title}\"] += 1\n        for f in [\"event_code\", \"type\", \"event_id\"]:\n            for k in session[f].values:\n                features[f\"{f}_{k}\"] += 1\n                \n        #features[\"accumulated_actions\"] += len(session)\n\n                        \n    if test_set:\n        return [all_assessments[-1]]\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = Parallel(n_jobs=4, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")( \\\n      delayed(get_data)(user_sample) for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total=train[\"installation_id\"].nunique()))\ntrain_df = list(chain.from_iterable(res))\ntrain_df = pd.DataFrame(train_df)\ntrain_df = train_df.fillna(-1)\ntrain_df = train_df.loc[:, (train_df != 0).any(axis=0)]\ntrain_df = train_df.loc[:, (train_df != -1).any(axis=0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = test\nres = Parallel(n_jobs=2, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")( \\\n      delayed(get_data)(user_sample, True) for i, (ins_id, user_sample) in tqdm(enumerate(tmp.groupby('installation_id', sort = False)), total=tmp[\"installation_id\"].nunique()))\ntest_df = list(chain.from_iterable(res))\ntest_df = pd.DataFrame(test_df)\ntest_df = test_df.fillna(-1)\nfor col in train_df.columns:\n    if col not in test_df.columns:\n        test_df[col] = 0\ntest_df = test_df[train_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = test\nres = Parallel(n_jobs=2, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")( \\\n      delayed(get_data)(user_sample, False) for i, (ins_id, user_sample) in tqdm(enumerate(tmp.groupby('installation_id', sort = False)), total=tmp[\"installation_id\"].nunique()))\ntest_train_df = list(chain.from_iterable(res))\ntest_train_df = pd.DataFrame(test_train_df)\ntest_train_df = test_train_df.fillna(-1)\nfor col in train_df.columns:\n    if col not in test_train_df.columns:\n        test_train_df[col] = 0\ntest_train_df = test_train_df[train_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train_df, test_train_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sort_values(\"timestamp\").reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef pred_to_int(x):\n    return np.round(np.clip(x, 0, 3))\n\ndef plot_hist(p):\n    #print(confusion_matrix(train_df[\"accuracy_group\"].values, pred_to_int(oof)))\n    \n    print(pd.Series(p).value_counts())\n\n    pd.Series(p).hist(color=\"blue\", alpha=0.5, normed=True)\n    train_df['accuracy_group'].hist(alpha=0.5, color=\"red\", normed=True)\n\ndef opt_dist(df, p_pred, verbose=True, dist=None):\n    \n    if dist is None:\n        dist = df['accuracy_group'].value_counts().sort_values() / len(df)\n    #print(dist)\n        \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(p_pred, acum * 100)\n    if verbose:\n        print(bound)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    return np.array(list(map(classify, p_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom collections import Counter, defaultdict\nfrom sklearn.utils import check_random_state\n\nclass RepeatedStratifiedGroupKFold():\n\n    def __init__(self, n_splits=5, n_repeats=1, random_state=None):\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        \n    # Implementation based on this kaggle kernel:\n    #    https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n    def split(self, X, y=None, groups=None):\n        k = self.n_splits\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std(\n                    [y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]\n                )\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n            \n        rnd = check_random_state(self.random_state)\n        for repeat in range(self.n_repeats):\n            labels_num = np.max(y) + 1\n            y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n            y_distr = Counter()\n            for label, g in zip(y, groups):\n                y_counts_per_group[g][label] += 1\n                y_distr[label] += 1\n\n            y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n            groups_per_fold = defaultdict(set)\n        \n            groups_and_y_counts = list(y_counts_per_group.items())\n            rnd.shuffle(groups_and_y_counts)\n\n            for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n                best_fold = None\n                min_eval = None\n                for i in range(k):\n                    fold_eval = eval_y_counts_per_fold(y_counts, i)\n                    if min_eval is None or fold_eval < min_eval:\n                        min_eval = fold_eval\n                        best_fold = i\n                y_counts_per_fold[best_fold] += y_counts\n                groups_per_fold[best_fold].add(g)\n\n            all_groups = set(groups)\n            for i in range(k):\n                train_groups = all_groups - groups_per_fold[i]\n                test_groups = groups_per_fold[i]\n\n                train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n                test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n                yield train_indices, test_indices\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.index.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom numba import jit\n\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor, Pool, cv\n\n\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\nif \"p\" in train_df:\n    del train_df[\"p\"]\n\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y, init=[1, 1.5, 2.5]):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = init\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead', options={\"maxiter\": 100_000, 'maxiter': 100_000})\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n    \nclass OptimizedRounderTruncated(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y, indices):\n\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        kappas = []\n        for idx in indices:\n#             print(y.shape)\n#             print(idx)\n            X_p = (pd.cut(X[idx], [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])).astype(np.int)\n            #print(y[idx])\n            #print(X_p.astype(np.int))\n            kappas.append(qwk(y[idx], X_p))\n        \n        #print(np.mean(kappas))\n        return -np.median(kappas)\n\n    def fit(self, X, y, indices, init=[1, 1.5, 2.5]):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        \n        dist = pd.Series(y).value_counts().sort_values() / len(y)\n        acum = 0\n        bound = []\n        for i in range(3):\n            acum += dist[i]\n            bound.append(np.percentile(X, acum * 100))\n        init = bound\n            \n        loss_partial = partial(self._kappa_loss, X=X, y=y, indices=indices)\n        initial_coef = init\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead', options={\"maxiter\": 10_000})\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n    \n\ndef eval_truncated(p, run_opt_dist=False, dist=None, test_preds=None, random_state=0):\n\n    train_df[\"p\"] = p\n    scores = []\n    hists = []\n    \n    trunc_indices = []\n    for j in tqdm(range(1000)):\n\n        np.random.seed(random_state+j)\n        s = train_df[[\"installation_id\"]].sample(frac=1.0, random_state=random_state+j).groupby('installation_id', sort=False).head(1)\n\n        trunc_indices.append(s.index.values)\n        \n    print(np.mean(trunc_indices[:1000]))\n    \n    optR = OptimizedRounderTruncated()\n    optR.fit(p.reshape(-1,), train_df[\"accuracy_group\"].values, trunc_indices)\n\n    coefficients = optR.coefficients()\n    print(coefficients)\n    \n    for idx in tqdm(trunc_indices):\n\n        #y = train_df[[\"accuracy_group\", \"installation_id\", \"p\", \"game_session\"]].sample(frac=1.0, random_state=j).groupby('installation_id', sort=False).head(1)\n        y = train_df.iloc[idx]\n        #if run_opt_dist == True:\n        #    y[\"p\"] = opt_dist(y, y[\"p\"], verbose=False, dist=dist)\n        \n        p = optR.predict(y[\"p\"], coefficients)\n        \n        #score = cohen_kappa_score(y[\"accuracy_group\"], pred_to_int(y[\"p\"]), weights='quadratic') \n        score = qwk(y[\"accuracy_group\"], p) \n        scores.append(score)\n        \n        #y = y.sort_values(\"accuracy_group\")\n        #hist = y[\"accuracy_group\"].value_counts().sort_values() / len(y)\n        #hists.append(hist)\n\n    print()\n    print(np.round(np.median(scores),5), np.round(np.mean(scores),5), np.round(np.std(scores),5), np.round(np.min(scores),5), np.round(np.max(scores),5))\n    #print(np.mean(hists, axis=0))\n    \n    del train_df[\"p\"]\n    \n    if test_preds is not None:\n        return optR.predict(test_preds, coefficients).astype(np.int), np.round(np.median(scores),5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns = train_df.columns.map(str)\ntest_df.columns = test_df.columns.map(str)\n\nnbags = 15\nn_splits = 10\ntest_preds_all = []\ntrunc_score_all = []\nfor bag in range(nbags):\n    kf = RepeatedStratifiedGroupKFold(n_splits=n_splits, random_state=42+bag)\n    #kf = StratifiedKFold(n_splits=n_splits, random_state=42+bag, shuffle=True)\n\n    folds = []\n    for trn_idx, val_idx in kf.split(np.arange(len(train_df)), train_df[\"accuracy_group\"].values, groups=train_df[\"installation_id\"].values):\n\n        df = train_df.iloc[trn_idx][[\"installation_id\"]].reset_index()\n        df[\"idx\"] = np.arange(len(trn_idx))\n        trn_idx_rnd = []\n        for j in range(1):\n            s = df.sample(frac=1.0, random_state=j).groupby('installation_id', sort=False).head(1)\n            trn_idx_rnd.append(s.index.values)\n\n        df = train_df.iloc[val_idx][[\"installation_id\"]].reset_index()\n        df[\"idx\"] = np.arange(len(val_idx))\n        val_idx_rnd = []\n        for j in range(1):\n            s = df[[\"installation_id\"]].sample(frac=1.0, random_state=j).groupby('installation_id', sort=False).head(1)\n            val_idx_rnd.append(s.index.values)\n        folds.append([trn_idx, val_idx, trn_idx_rnd, val_idx_rnd])\n\n    oof_lgb = np.zeros(len(train_df))\n    oof_catboost = np.zeros(len(train_df))\n\n    pred_test_lgb = []\n    pred_test_catboost = []\n\n    truncated_scores = []\n    \n    if \"p\" in train_df:\n        del train_df[\"p\"]\n\n    remove_col = [\"game_session\",   \"accuracy_group\"]\n    \n\n    for fold, (trn_idx, val_idx, trn_idx_rnd, val_idx_rnd) in enumerate(folds):\n\n        X_train = train_df[[c for c in train_df.columns if c not in remove_col]].iloc[trn_idx]\n        y_train = train_df[\"accuracy_group\"].iloc[trn_idx]\n\n        X_val = train_df[[c for c in train_df.columns if c not in remove_col]].iloc[val_idx]\n        y_val = train_df[\"accuracy_group\"].iloc[val_idx].values\n\n        #X_test_train = test_train_df[[c for c in train_df.columns if c not in remove_col]]\n        #y_test_train = test_train_df[\"accuracy_group\"]\n\n        X_test = test_df[[c for c in train_df.columns if c not in remove_col]]\n        \n#         print(\"y_train\", y_train.shape)\n        \n#         ts = X_val.groupby(\"installation_id\")[\"timestamp\"].last()\n#         X_train[\"ts_diff\"] = X_train[\"installation_id\"].map(ts).values\n#         X_train[\"ts_diff\"] = (X_train[\"timestamp\"].dt.tz_convert(None) - X_train[\"ts_diff\"]).astype('timedelta64[s]')\n\n#         X_train = X_train[X_train[\"ts_diff\"]>0]\n#         y_train = y_train[X_train.index]\n#         print(\"y_train\", y_train.shape)\n        \n#         del X_train[\"ts_diff\"]\n        \n\n        #X_train = pd.concat([X_train, X_test_train])\n        #y_train = pd.concat([y_train, y_test_train])\n\n        y_train = y_train.values\n        \n        params = {\n            'loss_function': 'RMSE',\n\n            'eval_metric': \"RMSE\",\n            'task_type' : 'CPU',\n            'early_stopping_rounds' : 300,\n            \"boosting_type\": \"Plain\",\n            \"learning_rate\": 0.05,\n            'thread_count': -1,\n            'max_depth': 6,\n            \"iterations\": 10_000,\n            'random_strength': 5,\n            'border_count': 64,\n            #'max_ctr_complexity': 10,\n            'bagging_temperature': 1,\n            'bootstrap_type': \"Bayesian\",\n            \"grow_policy\": \"SymmetricTree\",\n           # \"ignored_features\": [768, 769, 770, 771, 772],\n            \"simple_ctr\":'Buckets',\n            \"combinations_ctr\":'Buckets',\n           # 'per_feature_ctr':'CtrType=Buckets',\n           # \"max_leaves\": 31,\n           # \"one_hot_max_size\": 128,\n          # \"min_data_in_leaf\": 100,\n            'l2_leaf_reg': 2,\n            'feature_border_type': 'GreedyLogSum',\n           'has_time': True,\n            'colsample_bylevel': 0.3,\n            'use_best_model': True,\n           # 'od_type': \"Iter\",\n            #'logging_level':'Silent',\n        }\n        \n        for f in []:\n            if f in X_train:\n                del X_train[f]\n                del X_val[f]\n                del X_test[f]\n        \n        categorical_columns = [\"s_title\", \"installation_id\"]\n        \n        #print(\"cat\")\n        model = CatBoostRegressor(**params)\n        model.fit(X=X_train, y=y_train, \n               eval_set=(X_val, y_val), \n               early_stopping_rounds=params['early_stopping_rounds'], verbose=100,\n               cat_features=categorical_columns)\n\n\n        pred_val = model.predict(X_val)\n        oof_catboost[val_idx] = rankdata(pred_val) / len(pred_val)\n\n        p_t = model.predict(X_test)\n        pred_test_catboost.append(rankdata(p_t) / len(p_t))\n\n        params = {'n_estimators':10000,\n                'boosting_type': 'gbdt',\n                'objective': 'regression',\n                'metric': 'rmse',\n                'subsample': 0.3,\n                'subsample_freq': 1,\n                'learning_rate': 0.01,\n                'feature_fraction': 0.5,\n             'num_leaves': 32,\n                'lambda_l1': 1,  \n                'lambda_l2': 1,\n                'verbose': 100,\n              'min_data_in_leaf': 100,\n                'early_stopping_rounds': 300, 'eval_metric': 'cappa',\n              'n_jobs': -1\n                }#\n        \n        for f in [\"s_title\", \"timestamp\"]:\n            if f in X_train:\n                del X_train[f]\n                del X_val[f]\n                del X_test[f]\n                \n        categorical_columns = [\"installation_id\"]\n        \n        for f in categorical_columns:\n            if f in X_train:\n                X_train[f] = X_train[f].astype(\"category\")\n                X_val[f] = X_val[f].astype(\"category\")\n                X_test[f] = X_test[f].astype(\"category\")\n\n#         categorical_columns = [\"installation_id\"]\n        \n#         X_train[categorical_columns] = X_train[categorical_columns].astype(\"category\")\n#         X_val[categorical_columns] = X_val[categorical_columns].astype(\"category\")\n\n\n        \n        #print(\"lgb\")\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X=X_train, y=y_train, \n                   eval_set=[[X_val, y_val]], \n                   verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'], eval_metric=\"rmse\",\n                   categorical_feature=categorical_columns)\n        \n        pred_val = model.predict(X_val)\n        #oof_lgb[val_idx] = rankdata(pred_val) / len(pred_val)\n        oof_lgb[val_idx] = rankdata(pred_val) / len(pred_val)\n\n        p_t = model.predict(X_test)\n        pred_test_lgb.append(rankdata(p_t) / len(p_t))\n\n        print()\n        \n        #break\n        \n    pred_test_lgb = np.mean(pred_test_lgb, axis=0)\n    pred_test_catboost = np.mean(pred_test_catboost, axis=0)\n    \n    oof = 0.75 * oof_lgb + 0.25* oof_catboost\n    pred_test = 0.75 * pred_test_lgb + 0.25 * pred_test_catboost\n\n\n    score = cohen_kappa_score(train_df[\"accuracy_group\"], pred_to_int(oof), weights='quadratic') \n    print(\"Full CV Kappa: {:<8.4f}\".format(score))    \n\n    #score = cohen_kappa_score(train_df[\"accuracy_group\"], pred_to_int(oof), weights='quadratic') \n    print(\"Truncated CV Mean Kappa: {:<8.4f}\".format(np.mean(truncated_scores)))    \n\n    oof_opt = opt_dist(train_df, oof, verbose=False)\n    score = cohen_kappa_score(train_df[\"accuracy_group\"], pred_to_int(oof_opt), weights='quadratic')\n    print(\"Full CV Kappa DIST OPT: {:<8.4f}\".format(score))   \n    \n    score = np.sqrt(mean_squared_error(train_df[\"accuracy_group\"], oof))\n    print(\"Full CV RMSE: {:<8.4f}\".format(score))  \n\n    sns.distplot(pred_test)\n    sns.distplot(oof)\n    plt.show()\n    \n    pred_test_opt, trunc_score = eval_truncated(oof, test_preds = pred_test.reshape(-1), random_state=bag*10_000)\n    test_preds_all.append(pred_test_opt)\n    trunc_score_all.append(trunc_score)\n    \n    print()\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trunc_score_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(trunc_score_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_all = np.vstack(test_preds_all).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_opt = sp.stats.mode(test_preds_all, axis=1)[0].reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(pred_test_opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"accuracy_group\"] = pred_test_opt.astype(np.int)\nsample_submission[\"accuracy_group\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_df.columns == test_df.columns).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}