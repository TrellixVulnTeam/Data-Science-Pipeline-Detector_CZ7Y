{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import everything we need first"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nSTART = time.time()\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport itertools\nimport sys\nimport gc\nimport json\nimport random\nimport os\nimport warnings\nfrom copy import copy, deepcopy\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.optimize import minimize\nfrom lightgbm import LGBMRegressor\nfrom bayes_opt import BayesianOptimization\n\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading train data.\n\nKeep only installations with assessment attempts - we can't calcullate train targets for other anyway.\n\nMap categorical columns to integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\")\n\ntrainable = train[(train.type == 'Assessment') & (\n    (train.title == 'Bird Measurer (Assessment)') & (train.event_code == 4110) |\n    (train.title != 'Bird Measurer (Assessment)') & (train.event_code == 4100)\n)].installation_id.drop_duplicates()\ntrain = train[train.installation_id.isin(trainable)]\n\ntrain.sort_values([\"installation_id\", \"timestamp\", \"event_count\"], inplace=True)\n\nalltypes = list(train.type.drop_duplicates())\nalltitles = list(train.title.drop_duplicates())\nallevents = list(train.event_code.drop_duplicates())\nallworlds = list(train.world.drop_duplicates())\n\nCATEGORICAL = [\n    'previous_title',\n    'previous_type',\n    'previous_world',\n    'current_title',\n    'current_world',\n    'assessment_world',\n    'assessment_title',\n]\n\nCATMAPS = {} # will be filled in generate_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Big function to generate features from train or test data.\n\nEach assessment featured for train mode, only last - for test\n\nIt is A LOT of data in source train/test files, only some aggregates kept as features - this is probably main point to improve results"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def generate_features(df, only_test=False):\n    \n    global CATMAPS\n    \n    df[\"timestamp\"] = df.timestamp.apply(lambda x : pd.Timestamp(x))\n\n    total = df.installation_id.nunique()\n\n    X = []\n    \n    start = time.time()\n    cnt = 0\n    \n    for i, d in df.groupby(\"installation_id\"):\n        d.sort_values([\"timestamp\", \"event_count\"], inplace = True)\n\n        d[\"is_attempt\"] = ((d.type == 'Assessment') & (\n                           ((d.title == 'Bird Measurer (Assessment)') & (d.event_code == 4110)) |\n                           ((d.title != 'Bird Measurer (Assessment)') & (d.event_code == 4100))\n                          ))\n        attempt_sessions = list(d[d.is_attempt].game_session.drop_duplicates())\n\n        d[\"for_train\"] = (d.game_session.isin(attempt_sessions)) & (~d.game_session.duplicated(keep='first'))\n        \n        attempt_sessions = set(attempt_sessions)\n        \n        for_test = (d.type == \"Assessment\") & (~d.game_session.duplicated(keep='first'))\n        for_test = for_test[for_test].index[-1]\n        d[\"for_test\"] = False\n        d[\"for_test\"][for_test] = True\n        \n        first_x = {\n            \"events_count\": 0,\n            \"sessions_count\": 0,\n            \"sessions_duration\": 0,\n            \"age\": 0.0,\n            \"nonass_attempts\": 0,\n            \"nonass_successes\": 0,\n            \n            # Categorical\n            \"current_world\": \"\",\n            \"current_title\": \"\",\n            \"previous_world\": \"\",\n            \"previous_title\": \"\",\n            \"previous_type\": \"\",\n        }\n        for x in alltypes:\n            first_x[\"count_type_%s\" % x] = 0\n            first_x[\"duration_type_%s\" % x] = 0\n        for x in alltitles:\n            first_x[\"count_title_%s\" % x] = 0\n            first_x[\"duration_title_%s\" % x] = 0\n            \n            first_x[\"last_noass_round_%s\" % x] = 0\n            first_x[\"last_noass_level_%s\" % x] = 0\n            \n            first_x[\"noass_attempts_%s\" % x] = 0\n            first_x[\"noass_successes_%s\" % x] = 0\n            \n        for x in allevents:\n            first_x[\"count_event_%s\" % x] = 0\n            for y in alltitles:\n                first_x[\"count_titlevent_%s_%s\" % (y, x)] = 0\n        for x in allworlds:\n            first_x[\"count_world_%s\" % x] = 0\n            first_x[\"duration_world_%s\" % x] = 0\n\n        second_x = {\n            \"assessment_order\" : 0,\n            \"total_attempts\": 0,\n            \"total_successes\": 0,\n            \"last_attempts\" : 0,\n            \"last_successes\" : 0,\n            \n            # Categorical\n            \"assessment_world\": \"\",\n            \"assessment_title\": \"\",\n        }\n        for x in alltitles:\n            if 'Assessment' in x:\n                second_x[\"count_assessment_%s\" % x] = 0\n                second_x[\"attempts_assessment_%s\" % x] = 0\n                second_x[\"successes_assessment_%s\" % x] = 0\n                    \n        for x in allworlds:\n            second_x[\"world_assessment_count_%s\" % x] = 0\n            second_x[\"world_assessment_attempts_%s\" % x] = 0\n            second_x[\"world_assessment_successes_%s\" % x] = 0\n\n        session = \"\"\n        session_type = \"\"\n        session_title = \"\"\n        session_world = \"\"\n        session_start = 0\n        session_duration = 0\n        session_events = 0\n        \n        assessment = \"\"\n        assessment_attempts = 0\n        assessment_success = 0\n        assessment_start = 0\n        \n        prev_details = {\"session\": ''}\n        delta_X = []\n        delta_Y = []\n        delta_T = []\n        \n        \n        \n        session_intervals = []\n        \n        for j, r in d.iterrows():\n            \n            details = json.loads(r[\"event_data\"])\n            details[\"session\"] = r[\"game_session\"]\n\n            if details[\"session\"] == prev_details[\"session\"]:\n                if \"coordinates\" in details and \"coordinates\" in prev_details:\n                    delta_X.append(abs(details[\"coordinates\"][\"x\"] - prev_details[\"coordinates\"][\"x\"]))\n                    delta_Y.append(abs(details[\"coordinates\"][\"y\"] - prev_details[\"coordinates\"][\"y\"]))\n                    delta_T.append(details[\"game_time\"] - prev_details[\"game_time\"])\n            prev_details = details\n            \n            if \"correct\" in details:\n                if r[\"type\"] != \"Assessment\":\n\n                    first_x[\"nonass_attempts\"] += 1                    \n                    first_x[\"noass_attempts_%s\" % r[\"title\"]] += 1\n\n                    if details[\"correct\"]:\n                        first_x[\"nonass_successes\"] += 1\n                        first_x[\"noass_successes_%s\" % r[\"title\"]] = 0\n                        \n                        if \"level\" in details:\n                            first_x[\"last_noass_level_%s\" % r[\"title\"]] = details[\"level\"]\n                        \n                        if \"round\" in details:\n                            first_x[\"last_noass_round_%s\" % r[\"title\"]] = details[\"round\"]\n    \n            \n            first_x[\"events_count\"] += 1\n            first_x[\"count_event_%s\" % r[\"event_code\"]] += 1\n            first_x[\"count_titlevent_%s_%s\" % (r[\"title\"], r[\"event_code\"])] += 1\n            \n            first_x[\"weekday\"] = r[\"timestamp\"].weekday()\n            first_x[\"monthpart\"] = r[\"timestamp\"].day / float(r[\"timestamp\"].daysinmonth)\n            first_x[\"daytime\"] = (r[\"timestamp\"].value / 10**9 % (3600*24)) / 3600. / 24.\n                \n            if r[\"game_session\"] == session:\n                session_duration = r[\"timestamp\"].value / 10**6 - session_start\n                session_events += 1\n            \n            else:\n                first_x[\"sessions_count\"] += 1\n                \n                first_x[\"sessions_duration\"] += session_duration\n                first_x[\"last_session_duration\"] = session_duration\n                first_x[\"age\"] +=  session_duration\n                session_intervals.append(session_duration)\n                first_x[\"last_session_events\"] = session_events\n                first_x[\"since_last_session\"] = r[\"timestamp\"].value / 10**6 - session_start - session_duration\n            \n                if session != \"\":\n                    first_x[\"count_type_%s\" % session_type] += 1\n                    first_x[\"duration_type_%s\" % session_type] += session_duration\n\n                    first_x[\"count_title_%s\" % session_title] += 1\n                    first_x[\"duration_title_%s\" % session_title] += session_duration\n                    \n                    first_x[\"count_world_%s\" % session_world] += 1\n                    first_x[\"duration_world_%s\" % session_world] += session_duration\n                    \n                second_x[\"last_attempts\"] = assessment_attempts\n                second_x[\"last_successes\"] = assessment_success\n               \n                session = r[\"game_session\"]\n                session_start = r[\"timestamp\"].value / 10**6\n                session_duration = 0\n                session_events = 0\n                session_type = r[\"type\"]\n                session_title = r[\"title\"]\n                session_world = r[\"world\"]\n\n            if r[\"for_train\"] or (r[\"for_test\"] and only_test):\n                second_x[\"assessment_order\"] += 1\n                second_x[\"count_assessment_%s\" % r[\"title\"]] += 1\n                second_x[\"world_assessment_count_%s\" % r[\"world\"]] += 1\n                \n                first_x[\"current_world\"] = r[\"world\"]\n                first_x[\"current_title\"] = r[\"title\"]\n                \n                if not only_test or r[\"for_test\"]:\n                    \n                    moves = pd.DataFrame({\n                        'x': delta_X,\n                        'y': delta_Y,\n                        't': delta_T\n                    })\n                    moves[\"dist\"] = moves.x * moves.x + moves.y * moves.y\n                    moves[\"dist\"] = moves.dist.apply(sqrt)\n                    moves = moves[moves.t > 0]\n                    \n                    first_x[\"moves_count\"] = moves.shape[0]\n                    first_x[\"avg_move_duration\"] = moves.t.mean()\n                    first_x[\"avg_move_distanace\"] = moves.dist.mean()\n                    first_x[\"avg_total_speed\"] = moves.dist.sum() / moves.t.sum()\n                    first_x[\"avg_move_speed\"] = (moves.dist / moves.t).mean()\n                    first_x[\"avg_x_speed\"] = (moves.x / moves.t).mean()\n                    first_x[\"avg_y_speed\"] = (moves.y / moves.t).mean()\n                    \n                    first_x[\"session_interval_mean\"] = np.mean(session_intervals)\n                    first_x[\"session_interval_std\"] = np.std(session_intervals)\n                    \n                    xx1 = copy(first_x)\n                    xx1[\"game_session\"] = r[\"game_session\"]\n                    xx1[\"installation_id\"] = r[\"installation_id\"]\n\n                    xx2 = copy(second_x)\n                    xx1.update(xx2)\n                    X.append(xx1)\n            \n            else:\n                first_x[\"previous_world\"] = r[\"world\"]\n                first_x[\"previous_title\"] = r[\"title\"]\n                first_x[\"previous_type\"] = r[\"type\"]\n                \n            if r[\"is_attempt\"]:\n                second_x[\"assessment_world\"] = r[\"world\"]\n                second_x[\"assessment_title\"] = r[\"title\"]\n                \n                second_x[\"total_attempts\"] += 1\n                second_x[\"attempts_assessment_%s\" % r[\"title\"]] += 1\n                second_x[\"world_assessment_attempts_%s\" % r[\"world\"]] += 1\n                \n                if r[\"game_session\"] ==  assessment:\n                    assessment_attempts += 1\n                else:\n                    assessment = r[\"game_session\"]\n                    assessment_attempts = 1\n                    assessment_success = 0\n                    assessment_start = session_start\n                    \n                if json.loads(r[\"event_data\"])[\"correct\"]:\n                    second_x[\"total_successes\"] += 1\n                    second_x[\"successes_assessment_%s\" % r[\"title\"]] += 1\n                    second_x[\"world_assessment_successes_%s\" % r[\"world\"]] += 1\n                    \n                    if r[\"game_session\"] ==  assessment:\n                        assessment_success += 1\n        cnt += 1\n        if cnt % 100 == 0:\n            print(float(cnt) / total, time.time() - start)\n            sys.stdout.flush()\n            \n    X = pd.DataFrame(X)\n    \n    for col in list(X.columns):\n        if 'count' in col:\n            if col.replace(\"count\", \"duration\") in list(X.columns):\n                X[col.replace(\"count\", \"mean\")] = X[col.replace(\"count\", \"duration\")] / X[col]\n                \n    for col in list(X.columns):\n        if 'count' in col:\n            X[col.replace(\"count\", \"freq\")] = X[col] / X[\"age\"]\n                \n    def to_group(accuracy):\n        if accuracy < 0.005:\n            return 0\n        elif 0.995 < accuracy < 1.005:\n            return 3\n        elif 0.4995 < accuracy < 0.5005:\n            return 2\n        else:\n            return 1\n        \n    for col in list(X.columns):\n        if 'attempts' in col:\n            if col.replace(\"attempts\", \"successes\") in list(X.columns):\n                X[col.replace(\"attempts\", \"accuracy\")] = X[col.replace(\"attempts\", \"successes\")] / X[col]\n                X[col.replace(\"attempts\", \"group\")] = X[col.replace(\"attempts\", \"accuracy\")].apply(to_group)\n    \n    print(CATMAPS.keys())\n    if not only_test:\n        for col in CATEGORICAL:\n            uniques = list(X[col].drop_duplicates())\n            uniques.sort()\n            print(col)\n            print(str(len(uniques)))\n            CATMAPS[col] = {x : i for i, x in enumerate(uniques)}\n            X[col] = X[col].apply(lambda x : CATMAPS[col][x])\n    else:\n        for col in CATEGORICAL:\n            uniques = list(X[col].drop_duplicates())\n            print(col)\n            print(str(len(uniques)))\n            X[col] = X[col].apply(lambda x : CATMAPS[col][x])\n    \n    X.columns = [x.replace(\" \", \"_\").replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\") for x in X.columns] \n    \n    X = X[sorted(X.columns)]\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate features for train and test.\n\nDelete large source log data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = generate_features(train, only_test=False)\n\ndel train\ngc.collect()\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\")\ntest.sort_values([\"installation_id\", \"timestamp\", \"event_count\"], inplace=True)\n\nXtest = generate_features(test, only_test=True)\n\ndel test\ngc.collect()\n\nXtest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop constant features."},{"metadata":{"trusted":true},"cell_type":"code","source":"todrop = []\n\nfor i, col in enumerate(X.columns):\n    if X[col].nunique() < 2:\n        todrop.append(col)\n    elif Xtest[col].nunique() < 2:\n        todrop.append(col)\n\nprint(\"Totally constant features:\", len(todrop))\n\nX.drop(todrop, axis=1, inplace = True)\nXtest.drop(todrop, axis=1, inplace = True)\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.to_csv(\"X.tsv\", sep = '\\t', index=False)\nXtest.to_csv(\"Xtest.tsv\", sep = '\\t', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read target labels, move ids and target values to separate data frame y"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\")\n\nassert(X.shape[0] == y.shape[0])\nX = pd.merge(X, y[['game_session', \"installation_id\", \"accuracy_group\"]])\nassert(X.shape[0] == y.shape[0])\n\nX.sort_values(\"installation_id\", inplace=True)\n\ny = X[[\"installation_id\", \"game_session\", \"accuracy_group\"]]\nX.drop([\"accuracy_group\", \"installation_id\", \"game_session\"], axis=1, inplace=True)\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare submission table"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"installation_id\": Xtest[\"installation_id\"],\n    \"total_rank\": 0\n})\n\nXtest = Xtest[list(X.columns)]\nXtest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4-fold CV\n\n1) All assessments of one installation_id in one fold\n\n2) Each fold have same number of installations\n\n3) Each fold is balanced in terms of installations with one or many assessments\n\nAs a result, we should have roughly equal number of points in folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"y[\"fold\"] = -1\n\ninstallations = sorted(list(y.installation_id.value_counts().items()), key = lambda x : -x[1])\n\nfor i, item in enumerate(installations):\n    y.loc[y.installation_id == item[0], \"fold\"] = i % 4\n    \ny.fold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple OOF-score on CV isn't a good indication of model quality.\n\nIn our train set we have a lot of points for installations with many assessments - in test set only one point per installation.\n\nPoints with big assessment history are easier for model to evaluate => OOF score will be much better then actual test score.\n\nTo remove this bias we will \"truncate\" our train set to one point per installation for score evaluation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def truncate_df(df):\n\n    indices = []\n\n    for i, d in df.groupby(\"installation_id\"):\n        indices.append(random.choice(list(d.index)))\n\n    return df.loc[indices]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's calculate truncated OOF target distribution for each of our CV folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_truncated_target_ditribution = {}\n\nfor fold in range(4):\n    cv_truncated_target_ditribution[fold] = truncate_df(y[y.fold != fold]).accuracy_group.value_counts()\n    for i in range(19):\n        cv_truncated_target_ditribution[fold] += truncate_df(y[y.fold != fold]).accuracy_group.value_counts()\n    cv_truncated_target_ditribution[fold] /= 20.0 * y[y.fold != fold].installation_id.nunique()\n    \nassert(3.99 < sum(cv_truncated_target_ditribution.values()).sum() < 4.01)\ncv_truncated_target_ditribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And truncated target distribution for all dataset.\n\nInteresting, that it is really close to 50%-25%-12.5%-12.5%"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = sum(cv_truncated_target_ditribution.values()) / 4.0\nhist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QWK score function - copy/pasted from one of competition's kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Main model-training function.\n\nWe train separte model for each fold with early stopping by RMSE metric.\n\nAnd use multiple truncation of test set to get QWK distribution.\n\nWe don't use any thresholds to translate our continious predictions to classes.\n\nInstead, we use predictions fo ranking and distribute labels similar to train distribution.\n\nThis can be punished heavily by test set with only one class for example, but we assume that test and train data randomly sampled from same distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"ALL_CHECKED_CONFIGS = []\n\ndef check_cv_config(num_leaves, learning_rate):\n    \n    params = {\n        'n_estimators': 10000,\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'subsample': 0.75,\n        'subsample_freq': 1,\n        'learning_rate': learning_rate,\n        'feature_fraction': 0.9,\n        'lambda_l1': 1,  \n        'lambda_l2': 1,\n        'num_leaves': int(num_leaves),\n        'random_state': 42,\n        'n_jobs': 4,\n        'importance_type' : 'gain',\n    }\n    \n    FOLDED_MODELS = []\n    FOLDED_SCORES = []\n    \n    for fold in range(4):\n        \n        if time.time() - START > 3600 * 8:\n            raise Exception(\"OUT OF TIME!\")\n\n        clf = LGBMRegressor(**params)\n        \n        Xtr = X[y.fold != fold]\n        ytr = y[y.fold != fold]\n        Xte = X[y.fold == fold]\n        yte = y[y.fold == fold]\n        \n        clf.fit(Xtr,\n                ytr.accuracy_group,\n                categorical_feature = [x for x in X.columns if x in CATEGORICAL],\n                early_stopping_rounds = 100,\n                eval_set = [(Xte, yte.accuracy_group)],\n                verbose=False)\n        \n        pred = clf.predict(Xte)\n        \n        oof = yte[[\"installation_id\", \"accuracy_group\"]]\n        yte[\"prediction\"] = pred\n        \n        scores = []\n        \n        for idx in range(10):\n            \n            df = truncate_df(yte)\n                  \n            df.sort_values(\"prediction\", inplace = True)\n            df[\"predicted_group\"] = -1\n\n            idx_from = 0\n            for label in range(4):\n\n                idx_to = idx_from + int(cv_truncated_target_ditribution[fold][label] * df.shape[0])\n\n                df[\"predicted_group\"].iloc[idx_from:idx_to] = label\n\n                idx_from = idx_to\n\n            df.loc[df[\"predicted_group\"] < 0, \"predicted_group\"] = 3\n            \n            scores.append(qwk(df.accuracy_group, df.predicted_group))\n            \n        FOLDED_SCORES.append(scores)\n        FOLDED_MODELS.append(clf)\n        \n    scores = list(itertools.chain.from_iterable(FOLDED_SCORES))\n    score = np.mean(scores) - np.std(scores)\n        \n    ALL_CHECKED_CONFIGS.append((\n        score,\n        (num_leaves, learning_rate),\n        [(np.mean(x), np.std(x)) for x in FOLDED_SCORES],\n        FOLDED_MODELS,\n        FOLDED_SCORES\n    ))\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuning LGB parameters trying to maximize overall mean(qwk) - std(qwk)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bounds_LGB = {\n    'learning_rate': (0.0005, 0.05),\n    'num_leaves': (5, 60)\n}\n\nLGB_BO = BayesianOptimization(check_cv_config, bounds_LGB, random_state=42)\n\nLGB_BO.probe(\n    params={\n        'learning_rate': 0.03673,\n        'num_leaves': 32\n    },\n    lazy=True\n)\n\nLGB_BO.probe(\n    params={\n        'learning_rate': 0.0005,\n        'num_leaves': 40.53\n    },\n    lazy=True\n)\n\nLGB_BO.probe(\n    params={\n        'learning_rate': 0.008223,\n        'num_leaves': 13.58\n    },\n    lazy=True\n)\n\nLGB_BO.probe(\n    params={\n        'learning_rate': 0.04857,\n        'num_leaves': 20.32\n    },\n    lazy=True\n)\n\ntry:\n    LGB_BO.maximize(init_points=3, n_iter=25, acq='ucb', xi=0.0, alpha=1e-6)\nexcept Exception as e:\n    print(\"Stop - \", e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We select multiple trained models, optimal by MEAN - k * STD metric for different k for each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"FINAL_MODELS = []\n\nfor fold in range(4):\n    print(\"FOLD\", fold+1)\n    for sigmas in [0, 1, 2]:\n        ALL_CHECKED_CONFIGS.sort(key = lambda x : -(x[2][fold][0] - sigmas * x[2][fold][1]))\n        print(ALL_CHECKED_CONFIGS[0][2][fold][0], \"+-\", ALL_CHECKED_CONFIGS[0][2][fold][1])\n        FINAL_MODELS.append(ALL_CHECKED_CONFIGS[0][3][fold])\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For test set, we calculate rank for each point by each model.\n\nFinally, all points sotrted py their average rank.\n\nWith this tech we can not bother abouth thresholds fitting and over-fitting, about mixing models with different optimal thresholds and so on.\n\nOnce again, it can be heavily punished by completly different private test set. But really, all possible evaluation based on fact that test set has same distribute as train. It is also kind of \"not fare\" as we can't score one point, only whole test dataset.\n\nOn the other hand, is is making our life so mush easier, completly removing threshold-selection layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(FINAL_MODELS):\n    pred = model.predict(Xtest)\n    submission[\"pred_%d\" % i] = pred\n    \nfor i, model in enumerate(FINAL_MODELS):\n    submission.sort_values(\"pred_%d\" % i, inplace = True)\n    submission[\"total_rank\"] += np.array(range(submission.shape[0]))\n    \nsubmission.sort_values(\"total_rank\", inplace=True)\nsubmission[\"accuracy_group\"] = -1\n\nidx_from = 0\nfor label in range(4):\n\n    idx_to = idx_from + int(hist[label] * submission.shape[0])\n\n    submission[\"accuracy_group\"].iloc[idx_from:idx_to] = label\n\n    idx_from = idx_to\n\nsubmission.loc[submission[\"accuracy_group\"] < 0, \"accuracy_group\"] = 3\n\nsubmission.to_csv('full_submission.tsv', index=False, sep = '\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Writing final submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission[[\"installation_id\", \"accuracy_group\"]]\n\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.accuracy_group.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}