{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\n\nfrom tqdm.notebook import tqdm\nimport json\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular import * \n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_columns', None)\npd.set_option('display.min_rows', 100)\npd.set_option('display.max_rows', 100)\nhome = Path(\"/kaggle/input/data-science-bowl-2019/\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\nfrom sklearn.metrics import cohen_kappa_score\n\nclass OptimizedRounder():\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self, initial_coef, labels):\n        self.coef_ = 0\n        self.initial_coef = initial_coef\n        self.labels = labels\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n        return -cohen_kappa_score(X_p, y, weights=\"quadratic\")\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n\n    def coefficients(self): return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking at data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntypes = {\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32}\nraw_train = pd.read_csv(home/\"train.csv\", dtype=types)\nraw_train[\"timestamp\"] = pd.to_datetime(raw_train[\"timestamp\"]); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test = pd.read_csv(home/\"test.csv\", dtype=types)\nraw_test[\"timestamp\"] = pd.to_datetime(raw_test[\"timestamp\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove `installation_id` without any assesments\nids_with_subms = raw_train[raw_train.type == \"Assessment\"][['installation_id']].drop_duplicates()\nraw_train = pd.merge(raw_train, ids_with_subms, on=\"installation_id\", how=\"inner\"); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce event_id to make data preparation faster\nspecs = pd.read_csv(home/\"specs.csv\")\nspecs['hashed_info']=specs['info'].transform(hash)\nunique_specs=pd.DataFrame(specs[['hashed_info']].drop_duplicates())\nunique_specs[\"id\"] = np.arange(len(unique_specs))\nspecs = pd.merge(specs,unique_specs,on='hashed_info',how='left')\nevent_id_mapping = dict(zip(specs.event_id,specs.id))\nraw_train[\"event_id\"] = raw_train[\"event_id\"].map(event_id_mapping)\nraw_test[\"event_id\"] = raw_test[\"event_id\"].map(event_id_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(correct_data):\n    # Rounding correct > 1 to 1 lowers the score. Why?\n    correct = len(correct_data.loc[correct_data])\n    wrong = len(correct_data.loc[~correct_data])\n    accuracy = correct/(correct + wrong) if correct + wrong else 0\n    return accuracy, correct, wrong\n\ndef get_group(accuracy):\n    if not accuracy:\n        return 0\n    elif accuracy == 1:\n        return 3\n    elif accuracy >= 0.5:\n        return 2\n    return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I prefer this over calculating average\ndef lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare(data: pd.DataFrame, one_hot: List[str], test=False) -> pd.DataFrame:\n    one_hot_dict = defaultdict(int)\n\n    prepared = []\n    for id_, g in tqdm(data.groupby(\"installation_id\", sort=False)):\n        features = process_id(g, one_hot, one_hot_dict.copy(), test)\n        if not features:\n            continue\n        if test:\n            features[-1][\"is_test\"] = 1\n        prepared.extend(features)\n    return pd.DataFrame(prepared).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_id(id_data: pd.DataFrame, one_hot_cols, one_hot_dict, test: bool) -> pd.DataFrame:\n    a_accuracy, a_correct, a_wrong, counter = 0, 0, 0, 0\n    features = []\n\n    for s, gs in id_data.groupby(\"game_session\", sort=False):\n        def update_counter(counter: dict, column: str):\n            session_counter = Counter(gs.loc[:, column])\n            for value in session_counter.keys():\n                counter[f\"{column}_{value}\"] += session_counter[value]\n            return counter\n\n        def process_session(gs):\n            # share state with parent process_id()\n            nonlocal one_hot_dict, a_accuracy, a_correct, a_wrong, counter\n            # increment one hot columns for session, e.g. Bird Measurer: 50\n            def accumulate():\n                # accumulated one_hot features per id for a given session, e.g. Bird Measurer: 50\n                for c in one_hot_cols:\n                    one_hot_dict.update(update_counter(one_hot_dict, c))\n                duration = (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds\n                \n                cor_mask = gs[\"event_data\"].str.contains('\"correct\"')\n                corrects = gs.loc[cor_mask]\n                for c in corrects[\"event_id\"].unique():\n                    answers = corrects.loc[corrects[\"event_id\"] == c, \"event_data\"].apply(lambda x: json.loads(x).get(\"correct\"))\n                    event_accuracy, event_c, event_i = get_accuracy(answers)\n                    one_hot_dict[f\"accuracy_event_{c}\"] += event_accuracy\n                        \n            if gs[\"type\"].iloc[0] != \"Assessment\":\n                accumulate()\n                return\n\n            guess_mask = ((gs[\"event_data\"].str.contains(\"correct\")) & \n             (((gs[\"event_code\"] == 4100) &(~gs[\"title\"].str.startswith(\"Bird\")) | \n               ((gs[\"event_code\"] == 4110) & (gs[\"title\"].str.startswith(\"Bird\"))))))\n            answers = gs.loc[guess_mask, \"event_data\"].apply(lambda x: json.loads(x).get(\"correct\"))\n\n            # skip assessments without attempts in train\n            if answers.empty and not test:\n                accumulate()\n                return\n\n            accuracy, correct, wrong = get_accuracy(answers)\n            group = get_group(accuracy)\n            processed = {\"installation_id\": id_data[\"installation_id\"].iloc[0],\n                         \"title\": gs[\"title\"].iloc[0],\n                         \"accumulated_accuracy_mean\": a_accuracy/counter if counter > 0 else 0,\n                         \"accuracy_group\": group,\n                        }\n            processed.update(one_hot_dict)\n            counter += 1\n            a_accuracy += accuracy\n            a_correct += correct\n            a_wrong += wrong\n            accumulate()\n            return processed\n        \n        # skip sessions with 1 row\n        if len(gs) == 1 and not test:\n            continue\n        gs.reset_index(inplace=True, drop=True)\n        if (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds > 1800:\n            gs[\"passed\"] = gs.loc[:, \"timestamp\"].diff().apply(lambda x: x.seconds)\n            id_max = gs[\"passed\"].idxmax()\n            if gs[\"passed\"].max() > 1800:\n                session = gs.iloc[:id_max]\n                continued_session = gs.iloc[id_max:]\n                fs = process_session(session)\n                c_fs = process_session(continued_session)\n                if fs:\n                    features.append(fs)\n                if c_fs:\n                    features.append(c_fs)\n                continue\n\n        session_features = process_session(gs)\n        if session_features:\n            features.append(session_features)\n        \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_counters=[\"event_id\"]\ntrain = prepare(raw_train, one_hot_counters).sort_index(axis=1)\n# train = prepare(raw_train.iloc[:100_000], one_hot_counters).sort_index(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = prepare(raw_test, one_hot=one_hot_counters, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(test[test[\"is_test\"] == 1]) == 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# why discard good data from test, let's use all the taken assessments in train!\ntrain = (pd.concat([train, test[test[\"is_test\"] == 0].drop(columns=[\"is_test\"])],\n                   ignore_index=True, sort=False)).fillna(0).sort_index(axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.loc[test[\"is_test\"] == 1].reset_index(drop=True).sort_index(axis=1)\ntest.drop(columns=[\"accuracy_group\", \"is_test\"], inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = train.drop(columns=[\"accuracy_group\"]).columns.difference(test.columns)\ndisplay(f\"Test doesn't contain {diff.values}\")\ndisplay(f\"Train doesn't contain {test.columns.difference(train.columns).values}\")\ntrain.drop(columns=diff, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_train = train.copy()\n# train = main_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del_cols = []\nfor col in train.columns.values:\n    counts = train[col].value_counts().iloc[0]\n    if (counts / train.shape[0]) >= 0.99:\n        del_cols.append(col)\ntrain.drop(columns=del_cols, inplace=True, errors=\"ignore\")\ntest.drop(columns=del_cols, inplace=True, errors=\"ignore\")\ndisplay(f\"Dropped {del_cols}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proper validation dataset\n\nLet's assume the second hidden test is the same as this one. I.e. we predict the last assessment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outliers\n# train = train[train[train.columns[train.columns.str.startswith(\"duration_\", na=False)].to_list()].apply(sum, axis=1) < 10000].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab the last assessments per id\nvalid_idx = [g.iloc[-1].name for i, g in train.groupby(\"installation_id\", sort=False)]; len(valid_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = \"accuracy_group\"\ncat_names = [\"title\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.metrics import RegMetrics\nfrom fastai.callbacks import *\n\nclass KappaScoreRegression(RegMetrics):\n    def on_epoch_end(self, last_metrics, **kwargs):\n        preds = self.preds.flatten()\n        opt = OptimizedRounder([1, 1.5, 2.0], labels=[0, 1, 2, 3])\n        opt.fit(preds, self.targs)\n        coefs = opt.coefficients()\n        def rounder(preds):\n            y = preds.clone()\n            y[y < coefs[0]] = 0\n            y[y >= coefs[2]] = 3\n            y[(y >= coefs[0]) & (y < coefs[1])] = 1\n            y[(y >= coefs[1]) & (y < coefs[2])] = 2\n            return y.type(torch.IntTensor)\n\n        qwk = cohen_kappa_score(rounder(preds), self.targs, weights=\"quadratic\")\n        return add_metrics(last_metrics, qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drops = [\"baseline\",\n         \"accumulated_accuracy_mean\", \"accuracy_event_\",\n         \"event_id_\",\n         ]\n# goods = [\"event_id_\", \"title\", ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_features = pd.DataFrame(index=sorted(drops))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = 0\n# end = 10\n\n# for r in tqdm(range(start, end)):\n#     for d in drops:\n#         display(d)\n#         drop_column = train.columns[train.columns.str.startswith(d)].to_list()\n#         if not drop_column:\n#             drop_column = [f\"baseline_{d}\"]\n#         cont_names = list(filter(lambda x: x not in [\"installation_id\", dep_var] + cat_names + drop_column,\n#                              train.columns.to_list()))\n#         data = (TabularList.from_df(train, path=\"/kaggle/working\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n#             .split_by_idx(valid_idx=valid_idx)\n#             .label_from_df(cols=dep_var, label_cls=FloatList)\n#             .add_test(TabularList.from_df(test, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs))\n#             .databunch()\n#         )\n#         learn = tabular_learner(data, layers=[2000,100],\n#                             metrics=[KappaScoreRegression()],\n#                             y_range=[0, 3],\n#                             emb_drop=0.04,\n#                             ps=0.6,\n#                             callback_fns=[partial(EarlyStoppingCallback, monitor=\"kappa_score_regression\", mode=\"max\", patience=7),\n#                                           partial(SaveModelCallback, monitor=\"kappa_score_regression\", mode=\"max\", name=\"best_model\")]\n#                            )\n#         learn.fit_one_cycle(30, 3e-03)\n#         dropped_features.loc[d, r] = learn.validate()[-1].item()\n#         display(dropped_features.loc[d, r])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_features[\"mean\"] = dropped_features.apply(lambda x: x.mean(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropped features"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_features.sort_values(\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters search"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_names = list(filter(lambda x: x not in [\"installation_id\", dep_var] + cat_names,\n                         train.columns.to_list()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (TabularList.from_df(train, path=\"/kaggle/working\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n        .split_by_idx(valid_idx=valid_idx)\n        .label_from_df(cols=dep_var, label_cls=FloatList)\n        .add_test(TabularList.from_df(test, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs))\n        .databunch()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    layers = trial.suggest_categorical(\"layers\", [[2000, 100],\n                                                  [3000, 200]])\n    emb_drop = trial.suggest_discrete_uniform(\"emb_drop\", 0.04, 0.08, 0.04)\n    ps = trial.suggest_discrete_uniform(\"ps\", 0.2, 0.8, 0.2)\n\n    learn = tabular_learner(data, layers=layers,\n                            metrics=[KappaScoreRegression()],\n                            y_range=[0, 3],\n                            emb_drop=emb_drop,\n                            ps=ps,\n                            callback_fns=[partial(EarlyStoppingCallback, monitor=\"kappa_score_regression\", mode=\"max\", patience=7),\n                                          partial(SaveModelCallback, monitor=\"kappa_score_regression\", mode=\"max\", name=\"best_model\")]\n                       )\n\n    learn.fit_one_cycle(30, 3e-03)\n    return 1- learn.validate()[-1].item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study = optuna.create_study()\n# study.optimize(objective, n_trials=80)\n# study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.trials_dataframe().sort_values(by=\"value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = tabular_learner(data, layers=[2000,100],\n                        metrics=[KappaScoreRegression()],\n                        y_range=[0, 3],\n                        emb_drop=0.04,\n                        ps=0.6,\n                        callback_fns=[partial(EarlyStoppingCallback, monitor=\"kappa_score_regression\", mode=\"max\", patience=10),\n                                      partial(SaveModelCallback, monitor=\"kappa_score_regression\", mode=\"max\", name=\"best_model\")]\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.lr_find()\n# learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(30, 3e-03)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(30, 3e-04)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kappa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds_train, y = learn.get_preds(ds_type=DatasetType.Valid)\n# labels_train = preds_train.flatten()\n# opt = OptimizedRounder([1, 1.5, 2.0], labels=[0, 1, 2, 3])\n# opt.fit(labels_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coefs = opt.coefficients(); coefs\ncoefs = [1.04, 1.76, 2.18]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rounder(preds):\n    y = preds.clone()\n    y[y < coefs[0]] = 0\n    y[y >= coefs[2]] = 3\n    y[(y >= coefs[0]) & (y < coefs[1])] = 1\n    y[(y >= coefs[1]) & (y < coefs[2])] = 2\n    return y.type(torch.IntTensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, y = learn.get_preds(ds_type=DatasetType.Test)\nlabels = preds.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = rounder(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"installation_id\": test.installation_id, \"accuracy_group\": labels})\nsubmission.to_csv(\"submission.csv\", index=False)\nlen(submission), submission.accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}