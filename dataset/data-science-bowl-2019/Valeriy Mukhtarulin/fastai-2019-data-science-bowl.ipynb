{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\n\nfrom tqdm.notebook import tqdm\nimport json\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular import * \n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_columns', None)\npd.set_option('display.min_rows', 100)\npd.set_option('display.max_rows', 100)\nhome = Path(\"/kaggle/input/data-science-bowl-2019/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My approach to 2019 DS bowl with fastai v1\n\nI used these awesome notebooks:\n    \n    https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n    https://www.kaggle.com/amanooo/dsb-2019-regressors-and-optimal-rounding\n    https://www.kaggle.com/tarandro/regression-with-less-overfitting\n    https://www.kaggle.com/keremt/fastai-feature-engineering-part1-6160-features\n    https://www.kaggle.com/keremt/fastai-model-part1-regression\n    https://www.kaggle.com/keremt/fastai-model-part2-upgraded"},{"metadata":{},"cell_type":"markdown","source":"* [Looking at data](#Looking-at-data)\n* [Preparing data](#Preparing-data)\n* [Approach](#How-to-approach)\n* [Train](#Train)\n* [Submission](#Submission)"},{"metadata":{},"cell_type":"markdown","source":"# Looking at data"},{"metadata":{"trusted":true},"cell_type":"code","source":"specs = pd.read_csv(home/\"specs.csv\"); len(specs)\nspecs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(home/\"train_labels.csv\"); len(train_labels)\ntrain_labels.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv(home/\"sample_submission.csv\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntypes = {\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32}\nraw_train = pd.read_csv(home/\"train.csv\", dtype=types)\nraw_train[\"timestamp\"] = pd.to_datetime(raw_train[\"timestamp\"]); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test = pd.read_csv(home/\"test.csv\", dtype=types)\nraw_test[\"timestamp\"] = pd.to_datetime(raw_test[\"timestamp\"])\nraw_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems `game_time` is not captured correctly - there's a huge window in between some events in a given session."},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw_train[raw_train[\"game_session\"] == \"969a6c0d56aa4683\"].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target.\n\nWe are told: The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). The outcomes in this competition are grouped into 4 groups (labeled `accuracy_group` in the data):\n\n    3: the assessment was solved on the first attempt\n    2: the assessment was solved on the second attempt\n    1: the assessment was solved after 3 or more attempts\n    0: the assessment was never solved\n\nFor each installation_id represented in the test set, you must predict the accuracy_group **of the last assessment** for that installation_id"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We are told in the data description that:\n\n* The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set.\n* Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110.\n* If the attempt was correct, it contains \"correct\":true.\n\nWe also know:\n\n* The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt).\n* Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices.\n* In the training set, you are provided **the full history of gameplay data.**\n* In the test set, **we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.**\n* Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n"},{"metadata":{},"cell_type":"markdown","source":"# How to approach\n\nFor test set we guess the group based on **single** installation_id. But the train\\test datasets contain **repeatable** installation ids with different game sessions.\nHence it makes sense to guess the group for each assessment using history.\n\nA couple of more points:\n\n* do not use any assessment data for the current assessment prediction except title to prevent overfit\n* regression with the right coeff\n\nThe questions:\n* does randomly truncated history in test conflicts with the above?\nFrom the test dataset, the last asssessment data is cleaned. So it looks like a session with only 1 event.\nTo have a good validation dataset however, it should be the same as test - https://www.kaggle.com/tarandro/regression-with-less-overfitting\n* Is more than 1 correct submission impossible per session? Does it mean noise - two devices with the same id sharing the same session?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove `installation_id` without any assesments\nids_with_subms = raw_train[raw_train.type == \"Assessment\"][['installation_id']].drop_duplicates()\nraw_train = pd.merge(raw_train, ids_with_subms, on=\"installation_id\", how=\"inner\"); len(raw_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce event_id to make data preparation faster\n\nspecs['hashed_info']=specs['info'].transform(hash)\nunique_specs=pd.DataFrame(specs[['hashed_info']].drop_duplicates())\nunique_specs[\"id\"] = np.arange(len(unique_specs))\nspecs = pd.merge(specs,unique_specs,on='hashed_info',how='left')\nevent_id_mapping = dict(zip(specs.event_id,specs.id))\nraw_train[\"event_id\"] = raw_train[\"event_id\"].map(event_id_mapping)\nraw_test[\"event_id\"] = raw_test[\"event_id\"].map(event_id_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(correct_data):\n    # Rounding correct > 1 to 1 lowers the score. Why?\n    correct = len(correct_data.loc[correct_data])\n    wrong = len(correct_data.loc[~correct_data])\n    accuracy = correct/(correct + wrong) if correct + wrong else 0\n    return accuracy, correct, wrong\n\ndef get_group(accuracy):\n    if not accuracy:\n        return 0\n    elif accuracy == 1:\n        return 3\n    elif accuracy >= 0.5:\n        return 2\n    return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I prefer this over calculating average\ndef lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare(data: pd.DataFrame, one_hot: List[str], test=False) -> pd.DataFrame:\n    one_hot_dict = defaultdict(int)\n\n    prepared = []\n    for id_, g in tqdm(data.groupby(\"installation_id\", sort=False)):\n        features = process_id(g, one_hot, one_hot_dict.copy(), test)\n        if not features:\n            continue\n        if test:\n            features[-1][\"is_test\"] = 1\n        prepared.extend(features)\n    return pd.DataFrame(prepared).fillna(0).sort_index(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_id(id_data: pd.DataFrame, one_hot_cols, one_hot_dict, test: bool) -> pd.DataFrame:\n    a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean = 0, 0, 0, 0, 0, 0\n    a_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n    a_durations = defaultdict(int)\n    features = []\n\n    for s, gs in id_data.groupby(\"game_session\", sort=False):\n        def update_counter(counter: dict, column: str):\n            session_counter = Counter(gs.loc[:, column])\n            for value in session_counter.keys():\n                counter[f\"{column}_{value}\"] += session_counter[value]\n            return counter\n\n        def process_session(gs):\n            # share state with parent process_id()\n            nonlocal one_hot_dict, a_groups, a_durations, a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean\n            # increment one hot columns for session, e.g. Bird Measurer: 50\n            def accumulate():\n                nonlocal accumulated_duration_mean\n                # accumulated one_hot features per id for a given session, e.g. Bird Measurer: 50\n                for c in one_hot_cols:\n                    one_hot_dict.update(update_counter(one_hot_dict, c))\n                duration = (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds\n                # an accumulated session duration mean\n                accumulated_duration_mean = lin_comb(accumulated_duration_mean or duration,\n                                                     duration, beta=0.9)\n                a_durations[f\"duration_{gs.title.iloc[0]}\"] = duration\n                \n            if gs[\"type\"].iloc[0] != \"Assessment\":\n                accumulate()\n                return\n\n            guess_mask = ((gs[\"event_data\"].str.contains(\"correct\")) & \n             (((gs[\"event_code\"] == 4100) &(~gs[\"title\"].str.startswith(\"Bird\")) | \n               ((gs[\"event_code\"] == 4110) & (gs[\"title\"].str.startswith(\"Bird\"))))))\n            answers = gs.loc[guess_mask, \"event_data\"].apply(lambda x: json.loads(x).get(\"correct\"))\n\n            # skip assessments without attempts in train\n            if answers.empty and not test:\n                accumulate()\n                return\n\n            accuracy, correct, wrong = get_accuracy(answers)\n            group = get_group(accuracy)\n            processed = {\"installation_id\": id_data[\"installation_id\"].iloc[0],\n                         \"title\": gs[\"title\"].iloc[0],\n                         \"timestamp\": gs[\"timestamp\"].iloc[0],\n                         \"accumulated_duration_mean\": accumulated_duration_mean,\n                         \"accumulated_correct\": a_correct, \"accumulated_incorrect\": a_wrong,\n                         \"accumulated_accuracy_mean\": a_accuracy/counter if counter > 0 else 0,\n                         \"accumulated_accuracy_group_mean\": a_group/counter if counter > 0 else 0, \n                         \"accuracy_group\": group,\n                        }\n            processed.update(a_groups)\n            processed.update(one_hot_dict)\n            processed.update(a_durations)\n            counter += 1\n            a_accuracy += accuracy\n            a_correct += correct\n            a_wrong += wrong\n            a_group += group\n            a_groups[str(group)] += 1\n            accumulate()\n            return processed\n        \n        # skip sessions with 1 row\n        if len(gs) == 1 and not test:\n            continue\n        gs.reset_index(inplace=True, drop=True)\n        if (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds > 1800:\n            gs[\"passed\"] = gs.loc[:, \"timestamp\"].diff().apply(lambda x: x.seconds)\n            id_max = gs[\"passed\"].idxmax()\n            if gs[\"passed\"].max() > 1800:\n                session = gs.iloc[:id_max]\n                continued_session = gs.iloc[id_max:]\n                fs = process_session(session)\n                c_fs = process_session(continued_session)\n                if fs:\n                    features.append(fs)\n                if c_fs:\n                    features.append(c_fs)\n                continue\n\n        session_features = process_session(gs)\n        if session_features:\n            features.append(session_features)\n        \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_counters=[\"title\", \"type\", \"event_code\", \"event_id\"]\ntrain = prepare(raw_train, one_hot_counters)\n# train = prepare(raw_train.iloc[:1_000_000], one_hot_counters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(train, \"timestamp\", prefix=\"timestamp_\", time=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = prepare(raw_test, one_hot=one_hot_counters, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the case when one hot encoded columns don't match between datasets\nadd_datepart(test, \"timestamp\", prefix=\"timestamp_\", time=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# why discard good data from test, let's use all the taken assessments in train!\ntrain = (pd.concat([train, test[test[\"is_test\"] == 0].drop(columns=[\"is_test\"])],\n                   ignore_index=True, sort=False)).fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.loc[test[\"is_test\"] == 1].reset_index(drop=True)\ntest.drop(columns=[\"accuracy_group\", \"is_test\"], inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = train.drop(columns=[\"accuracy_group\"]).columns.difference(test.columns)\ndisplay(f\"Test doesn't contain {diff.values}\")\ndisplay(f\"Train doesn't contain {test.columns.difference(train.columns).values}\")\ntrain.drop(columns=diff, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_train = train.copy()\n# train = main_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del_cols = [\"timestamp_Second\"]\nfor col in train.columns.values:\n    counts = train[col].value_counts().iloc[0]\n    if (counts / train.shape[0]) >= 0.99:\n        del_cols.append(col)\ntrain.drop(columns=del_cols, inplace=True, errors=\"ignore\")\ntest.drop(columns=del_cols, inplace=True, errors=\"ignore\")\ndisplay(f\"Dropped {del_cols}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proper validation dataset\n\nLet's assume the second hidden test is the same as this one. I.e. we predict the last assessment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outliers\ntrain = train[train[train.columns[train.columns.str.startswith(\"duration_\", na=False)].to_list()].apply(sum, axis=1) < 10000].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab the last assessments per id\nvalid_idx = [g.iloc[-1].name for i, g in train.groupby(\"installation_id\", sort=False)]; len(valid_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[valid_idx].accuracy_group.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.title.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[valid_idx].title.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = train.columns[train.columns.str.startswith(\"timestamp_\", na=False)].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = \"accuracy_group\"\ncat_names = list(filter(lambda x: x not in [\"timestamp_Elapsed\"], date_cols)) + [\"title\"]\ncont_names = list(filter(lambda x: x not in [\"installation_id\", dep_var] + cat_names,\n                         train.columns.to_list()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (TabularList.from_df(train, path=\"/kaggle/working\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n        .split_by_idx(valid_idx=valid_idx)\n        .label_from_df(cols=dep_var, label_cls=FloatList)\n        .add_test(TabularList.from_df(test, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs))\n        .databunch()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kappa"},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\nfrom sklearn.metrics import cohen_kappa_score\n\nclass OptimizedRounder():\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self, initial_coef, labels):\n        self.coef_ = 0\n        self.initial_coef = initial_coef\n        self.labels = labels\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n        return -cohen_kappa_score(X_p, y, weights=\"quadratic\")\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n\n    def coefficients(self): return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.metrics import RegMetrics\n\nclass KappaScoreRegression(RegMetrics):\n    def on_epoch_end(self, last_metrics, **kwargs):\n        preds = self.preds.flatten()\n        opt = OptimizedRounder([0.5, 1.5, 2.0], labels=[0, 1, 2, 3])\n        opt.fit(preds, self.targs)\n        coefs = opt.coefficients()\n        def rounder(preds):\n            y = preds.clone()\n            y[y < coefs[0]] = 0\n            y[y >= coefs[2]] = 3\n            y[(y >= coefs[0]) & (y < coefs[1])] = 1\n            y[(y >= coefs[1]) & (y < coefs[2])] = 2\n            return y.type(torch.IntTensor)\n\n        qwk = cohen_kappa_score(rounder(preds), self.targs, weights=\"quadratic\")\n        return add_metrics(last_metrics, qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\n\nlearn = tabular_learner(data, layers=[2000,100],\n                        metrics=[KappaScoreRegression()],\n                        y_range=[0, 3],\n                        emb_drop=0.04,\n                        ps=0.6,\n                        callback_fns=[partial(EarlyStoppingCallback, monitor=\"kappa_score_regression\", mode=\"max\", patience=7),\n                                      partial(SaveModelCallback, monitor=\"kappa_score_regression\", mode=\"max\", name=\"best_model\")]\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.lr_find()\n# learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(30, 3e-03)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train, y = learn.get_preds(ds_type=DatasetType.Valid)\nlabels_train = preds_train.flatten()\nopt = OptimizedRounder([0.5, 1.5, 2.0], labels=[0, 1, 2, 3])\nopt.fit(labels_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = opt.coefficients(); coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rounder(preds):\n    y = preds.clone()\n    y[y < coefs[0]] = 0\n    y[y >= coefs[2]] = 3\n    y[(y >= coefs[0]) & (y < coefs[1])] = 1\n    y[(y >= coefs[1]) & (y < coefs[2])] = 2\n    return y.type(torch.IntTensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, y = learn.get_preds(ds_type=DatasetType.Test)\nlabels = preds.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = rounder(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"installation_id\": test.installation_id, \"accuracy_group\": labels})\nsubmission.to_csv(\"submission.csv\", index=False)\nlen(submission), submission.accuracy_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}