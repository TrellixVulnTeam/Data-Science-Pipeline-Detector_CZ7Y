{"cells":[{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T18:38:00.936409Z","start_time":"2019-11-08T18:38:00.929412Z"}},"cell_type":"markdown","source":"According to dataset description useful information can be found in files:\n* specs.csv\n* train.csv\n* test.csv\n\nLets try to get all useful info from these sets"},{"metadata":{},"cell_type":"markdown","source":"# Installs"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:30.542412Z","start_time":"2019-11-08T22:09:30.536416Z"},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pandas.io.json import json_normalize","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# specs.csv"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:30.748283Z","start_time":"2019-11-08T22:09:30.711307Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\nspecs.head()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Make a closer look to the args column"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:30.910183Z","start_time":"2019-11-08T22:09:30.895195Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"specs['args'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Looks like `args` is a pretty useless column, lets drop it"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:31.055096Z","start_time":"2019-11-08T22:09:31.0491Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"specs = specs.drop(columns='args')","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"The same procedure with the `info` column"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:31.212997Z","start_time":"2019-11-08T22:09:31.200007Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"specs['info'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"168 unique values and some of them repeated more than 10 times in a set. Lets  label the `info` column"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:31.3699Z","start_time":"2019-11-08T22:09:31.351912Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nspecs['info'] = le.fit_transform(specs['info'])\nspecs","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:31.395885Z","start_time":"2019-11-08T22:09:31.374898Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"specs.index = specs.event_id\nspecs = specs.drop(columns='event_id')\nspecs","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"The work with `specs.csv` is done for now. We will merge it with train/test dataframe later"},{"metadata":{},"cell_type":"markdown","source":"# test and train"},{"metadata":{},"cell_type":"markdown","source":"## Columns analysis"},{"metadata":{},"cell_type":"markdown","source":"Lets look on the test and train sets"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.079466Z","start_time":"2019-11-08T22:09:31.907569Z"},"trusted":true},"cell_type":"code","source":"test_head = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', nrows=10000)\ntrain_head = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', nrows=10000)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.095453Z","start_time":"2019-11-08T22:09:32.08446Z"},"trusted":true},"cell_type":"code","source":"print(f'Columns in test: {test_head.shape[1]}')\nprint(f'Columns in train: {train_head.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.159414Z","start_time":"2019-11-08T22:09:32.102448Z"},"trusted":true},"cell_type":"code","source":"test_head.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.19739Z","start_time":"2019-11-08T22:09:32.164411Z"},"trusted":true},"cell_type":"code","source":"train_head.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks life both sets have the same column structure. \n\nAs we need to work with a very large files, I will read both files column per column, slicing every column on a separate parts. "},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.226372Z","start_time":"2019-11-08T22:09:32.201389Z"},"trusted":true},"cell_type":"code","source":"train_head.info()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.383278Z","start_time":"2019-11-08T22:09:32.23037Z"},"trusted":true},"cell_type":"code","source":"print(f'Unique values:')\n\nfor column in train_head.columns:\n    unique = train_head[column].value_counts().count()\n    notNA = train_head[column].value_counts().sum()\n    \n    print(f'{column} : {(unique/notNA):.0%} ({unique} of {notNA}), type: {train_head[column].dtype}')\n    \ndel column    \ndel unique\ndel notNA\ndel train_head\ndel test_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Three previous cells help us to separate columns to several groups:\n\n**merging column** (as we should use data from the `specs` file) :\n* 'event_id' (index=0)\n\n**time parsing column**\n* 'timestamp'\n\n**numerical columns** (where there are only numericals already and the number is valuabe and should not be converted to categorical):\n* 'event_count'\n* 'game_time'\n\n**json parsing column**:\n* 'event_data'\n\n**categorical columns** (other object columns that have low rate of unique values):\n* 'game_session'\n* 'installation_id',\n* 'event_code'\n* 'title'\n* 'type'\n* 'world'"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:32.520194Z","start_time":"2019-11-08T22:09:32.512199Z"},"trusted":true},"cell_type":"code","source":"categorical_columns = [\n    'game_session', \n    'installation_id', \n    'event_code',\n    'title',\n    'type', \n    'world'\n]\n\nmerging_cols = [\n    'event_id',\n]\n\ncols_for_time_parsing = [\n    'timestamp',\n]\n\njson_cols = [\n    'event_data',\n]\n\nnumerical_cols = [\n    'event_count',\n    'game_time',\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## making functions"},{"metadata":{},"cell_type":"markdown","source":"### choose columns to save in json data"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:33.54456Z","start_time":"2019-11-08T22:09:32.777033Z"},"trusted":true},"cell_type":"code","source":"# event_data column\nimport json\ndef cols_in_json():\n    path = '/kaggle/input/data-science-bowl-2019/test.csv'\n    column = 'event_data'\n    size = 500000\n    \n    # read and transform data from train file\n    file_part_json = pd.read_csv(path, usecols=[column], nrows=size)\n    file_part_json = file_part_json['event_data'].apply(json.loads)\n    file_part_json = json_normalize(file_part_json)\n\n    # make a list of columns that have values in more than 30% of rows\n    cols_to_save = []\n    for col in file_part_json.columns:\n        has_value = file_part_json[col].value_counts().sum()\n\n        if (has_value/size > 0.3):\n            print(f'{(has_value):6.0f} values - {(has_value/size):4.0%} - in column {col}')\n            cols_to_save.append(col)\n\n\n    #print results\n    print('')\n    print(f'cols_to_save ({len(cols_to_save)} columns): {cols_to_save}')\n    \n    return cols_to_save\n\n# cols_to_save = cols_in_json() @making error on kaggle, but works at home, I'm trying to find out why \ncols_to_save = ['event_code', 'event_count', 'round', 'game_time', 'coordinates.x', 'coordinates.y', 'coordinates.stage_width', 'coordinates.stage_height', 'description', 'identifier', 'media_type', 'duration']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### processing"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:33.563549Z","start_time":"2019-11-08T22:09:33.549557Z"},"code_folding":[0],"trusted":true},"cell_type":"code","source":"def mem_reduce(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:33.650494Z","start_time":"2019-11-08T22:09:33.569546Z"},"trusted":true},"cell_type":"code","source":"size = 100001","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:33.691469Z","start_time":"2019-11-08T22:09:33.656492Z"},"trusted":true},"cell_type":"code","source":"def file_processing(path):\n    result_set = pd.DataFrame()\n    for column in range(2):\n\n        column_name = pd.read_csv(path, usecols=[column], nrows=1).columns[0]\n        if column_name in json_cols:\n            result_col = pd.DataFrame(columns=cols_to_save)\n        else:\n            result_col = pd.DataFrame(columns=[column_name])\n\n\n        #starting a loop\n        skiprows = 0\n        file_is_over=False\n        while file_is_over == False:\n\n            # read next part, rename the columns and concat with the result df\n            file_part = pd.read_csv(path, usecols=[column], nrows=size, skiprows=skiprows)\n\n\n            # json processing for event data col\n            if column_name in json_cols: \n                file_part = file_part[file_part.columns[0]].apply(json.loads)\n                file_part = json_normalize(file_part)\n\n                for col in file_part.columns:\n                    if col not in cols_to_save:\n                        file_part = file_part.drop(columns=col)\n\n            else:\n                file_part.columns=[column_name]\n\n            #time parsing\n            if column_name in cols_for_time_parsing: \n                file_part['timestamp'] = pd.to_datetime(file_part['timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ')    \n\n\n            result_col = pd.concat([result_col, file_part], sort=False)\n\n            #iterate until the 'tail' of the file\n            file_is_over = True if len(file_part) < size else False \n            skiprows += size\n\n            result_col = mem_reduce(result_col)\n\n            print(f'Read {(skiprows):.0f} rows of the column #{column+1} ({column_name})')\n\n\n        if column_name in categorical_columns:\n            result_col[column_name] = le.fit_transform(result_col.values)\n            result_col[column_name] = result_col[column_name].astype('category') \n\n        if column_name in merging_cols:\n            result_col = pd.merge(result_col, specs, on='event_id', how='left')\n\n\n        result_set = pd.merge(result_set, result_col, how='right', left_index=True, right_index=True)\n\n    return result_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying to sets"},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:09:55.585277Z","start_time":"2019-11-08T22:09:35.103901Z"},"trusted":true},"cell_type":"code","source":"test_set = file_processing('/kaggle/input/data-science-bowl-2019/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:10:13.252402Z","start_time":"2019-11-08T22:10:13.23941Z"},"trusted":true},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-11-08T22:10:04.59046Z","start_time":"2019-11-08T22:10:04.49252Z"},"trusted":true},"cell_type":"code","source":"test_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = file_processing('/kaggle/input/data-science-bowl-2019/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feel free to upvote! "}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":373,"position":{"height":"395px","left":"840px","right":"20px","top":"17px","width":"510px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"block","window_display":false}},"nbformat":4,"nbformat_minor":1}