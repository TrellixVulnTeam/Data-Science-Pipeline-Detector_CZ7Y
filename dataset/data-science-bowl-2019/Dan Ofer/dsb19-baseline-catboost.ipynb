{"cells":[{"metadata":{},"cell_type":"markdown","source":"Parts of the code are taken and modified from the following notebooks:\n\nhttps://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline\n\nhttps://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n\nhttps://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn.model_selection import GroupKFold\nfrom typing import Any\n#from numba import jit\nimport lightgbm as lgb\n#import xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom itertools import product\nimport copy\nimport time\n\nimport random\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#load training data, training_labels, event-specifications, testdata and sample submisson into pandas dataframes\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applied metric is quadratic weightes kappa:\n\nhttps://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    O = confusion_matrix(act,pred)\n    O = np.divide(O,np.sum(O))\n    \n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E))\n    \n    num = np.sum(np.multiply(W,O))\n    den = np.sum(np.multiply(W,E))\n        \n    return 1-np.divide(num,den)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Training Set "},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data provided in these files are as follows:\n- `event_id` - unique identifier for a single event. Infos about event are found in specs table.\n- `game_session` - unique identifier grouping events within a single game or video play session.\n- `timestamp` - datetime (local time as it seems)\n- `event_data` - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n- `installation_id` - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- `event_count` - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- `event_code` - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n- `game_time` - Time in milliseconds since the start of the game session. Extracted from event_data.\n- `title` - Title of the game or video.\n- `type` - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- `world` - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement), 'CRYSTALCAVES' (Weight)."},{"metadata":{},"cell_type":"markdown","source":"\"installation_id\" groups multiple \"game_session\"\n\"game_session\" groups multiple \"event_id\"\n\n\n\"event_data\" details an event\n\n\"event_count\", \"event_code\" and \"game_time\" are extracted from \"event_data\"\n\n\"game time\" is 0 for every start-event with code 2000\n\n\"game time\" reflects total time for games and assessments win_codes 4100 (4110 for bird measurer)\n\n\n\"type\" helps to concentrate on samples we need to train the algorithm on and that we need to predict (= assessments)\n\n\n\"world\" is redundant :\n\n--> \"Cart Balancer\" and \"Chest Sorter\" are in CRYSTALCAVES\n\n--> \"Cauldron Filler\" is in MAGMAPEAK\n\n--> \"Bird Measurer\" and \"Mushroom Sorter\" are in TREETOPCITY"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are over 11M lines of training data.\n\nThere are provided certain gaming histories for a number of installation ids. We need to predict the number of attempts of a future assessment with the knowledge of the gaming history.\n\nFrom Kaggle we know that there are installation_id that have not completet any assessments"},{"metadata":{},"cell_type":"markdown","source":"How many installation ids are in the training data set?\n\nHow many of them did never do any assessments?"},{"metadata":{"trusted":false},"cell_type":"code","source":"ids_all = train[\"installation_id\"].drop_duplicates()\nids_with_assessments = train[train.type == \"Assessment\"][\"installation_id\"].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are {} IDs\".format(len(ids_all)))\nprint(\"There are {} IDs with assessment(s)\".format(len(ids_with_assessments)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean = pd.merge(train, ids_with_assessments, on=\"installation_id\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 8M remaining lines, only containing IDs that did at least 1 assessment"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nax1 = fig.add_subplot(311)\nax1 = sns.countplot(y=\"type\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(312)\nax2 = sns.countplot(y=\"world\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by world\")\n\nax3 = fig.add_subplot(313)\nax3 = sns.countplot(y=\"title\", data=train_clean[train_clean[\"type\"] == \"Assessment\"], color=\"blue\")\nplt.title(\"number of evetns by assessment\")\n\nplt.tight_layout(pad=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"timestamp\" is of string-type\n\nextract \"date\", \"month\", \"hour\", \"dayofweek\" to get an overview of app-activity\n\nmaybe helpful maybe not, at least we get an overview"},{"metadata":{"trusted":false},"cell_type":"code","source":"def add_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain_clean = add_time_features(train_clean)\ntest = add_time_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nax1 = fig.add_subplot(311)\nax1 = sns.countplot(x=\"date\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by date\")\n\nax2 = fig.add_subplot(312)\nax2 = sns.countplot(x=\"dayofweek\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by day of week\")\n\nax3 = fig.add_subplot(313)\nax3 = sns.countplot(x=\"hour\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by local daytime\")\n\nplt.tight_layout(pad=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Test-Set"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are {} lines in sample submission\".format(sample_submission.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are {} unique installation_ids in testset\".format(test[\"installation_id\"].unique().shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, so good, for every installation_id in the testset there is exactly one line in the sample_submission"},{"metadata":{},"cell_type":"markdown","source":"# The labels"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels[\"title\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4 classes - accuracy_group\n\n3: solved on 1st attempt\n\n2: solved on 2nd attempt\n\n1: solved after 3 or more attempts\n\n0: never solved"},{"metadata":{"trusted":false},"cell_type":"code","source":"label_stats = train_labels.groupby(['title', 'accuracy_group'])['accuracy_group'].count().unstack(\"title\")\nlabel_stats.plot.bar(stacked=True, figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainable_ids = train_labels[\"installation_id\"].drop_duplicates()\ntrainable_ids.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3614 assessment results, but there are 4242 installation_ids in the training set.\nThis means that there are 628 installation_ids that are not trainable because of missing training-labels.\n--> remove all events that belong to any of these untrainable installation_ids"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean = pd.merge(train_clean, trainable_ids, on=\"installation_id\", how=\"inner\")\ntrain_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 7,7M remaining events"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean.groupby('installation_id').count()['event_id'].plot(kind='hist', bins=200, figsize=(15, 5),\n         title='Num Events by installation_id')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sessions_per_id = train_clean[[\"installation_id\", \"game_session\"]].drop_duplicates().groupby([\"installation_id\"])[\"game_session\"].count().plot(kind='hist', bins=200, figsize=(15, 5),\n         title='Num Sessions by installation_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean.groupby('event_code').count()['event_id'].sort_values().plot(kind='bar', figsize=(15, 5),\n         title='event code count.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there are lot of events with code 4070, 4030, 3010, 3110, 4020\n\nare they relevant, or is their count relevant?"},{"metadata":{},"cell_type":"markdown","source":"# Some Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels[train_labels[\"title\"].str.contains(\"Assessment\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Number of rows in train_labels: {}\".format(train_labels.shape[0]))\nprint(\"Number of unique game_sessions in train_labels: {}\".format(train_labels[\"game_session\"].nunique()))\nprint(\"Number of unique game_sessions that are Assessments in train_labels: {}\".format(train_labels[train_labels[\"title\"].str.contains(\"Assessment\")][\"game_session\"].nunique()))\nprint(\"Number of unique installation_ids in train_labels: {}\".format(train_labels[\"installation_id\"].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What to do:\n\nThere are certain game_sessions that are of type \"assessment\" for which we know the correct label.\n\nWe need to collect all features for such assessments that help to predict the labels for the testset"},{"metadata":{},"cell_type":"markdown","source":"## Basic Feature Engineering & Cleaning"},{"metadata":{"trusted":false},"cell_type":"code","source":"# generate a list of all unique titles (=activities), enumerate them and replace occurances by numbers\nlist_of_user_activities = list(set(train_clean['title'].unique()).union(set(test['title'].unique())))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain_clean['title'] = train_clean['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a dictionary that lists winning codes for all titles (activities)\n# every activity has a win_code 4100, only bird-measurer has 4110\nattempt_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nattempt_code[activities_map['Bird Measurer (Assessment)']] = 4110","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# user_sample contains lines with a unique installation_id\n\ndef get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_incorrect_attempts = 0 \n    accumulated_events = 0\n    counter = 0\n    durations = []\n    \n    for i, session in user_sample.groupby('game_session', sort=False):\n        #session type and title is unique within each session\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        #create feature vector for (session_type == Assessment) if (Testdata or (Trainingdata & len>1))\n        if (session_type == 'Assessment') & ((test_set == True) | ((test_set == False) & (len(session)>1))):\n            #add statistics of past events to feature vector and then update statistics\n            \n            #start feature vector with minimum content (Clip/Activity/Assessment/Game)\n            features = user_activities_count.copy()\n            \n            # add session_title (as a number)\n            features['session_title'] = session['title'].iloc[0] \n            \n            # get all attempts in current game_session\n            all_attempts = session.query(f'event_code == {attempt_code[session_title]}')\n            # get all \"true\" attempts in current game_session\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            # get all \"false\" attempts in current game_session\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            #compute accuracy of current game_session --> this is the parameter we need to predict later on\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            \n            #add accumulated correct attempts that occured before current session\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            #update accumulated attempt values\n            accumulated_correct_attempts += true_attempts \n            #same with incorrect attempts\n            features['accumulated_incorrect_attempts'] = accumulated_incorrect_attempts\n            #update accumulated attempt values\n            accumulated_incorrect_attempts += false_attempts\n\n            #add mean duration of previous sessions\n            if durations == []: #first session, durations is still empty\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            #add current duration to list, therefore use the timestamp\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds) #last timestamp - first timestamp\n\n            \n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            #update accumulated_accuracy with accuracy of current session\n            accumulated_accuracy += accuracy\n            \n            # add accuracy_groups to features\n            features.update(accuracy_groups)\n            # update accuracy groups with current game_session\n            accuracy_groups[features['accuracy_group']] += 1\n            \n            \n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            \n            \n            features['accumulated_actions'] = accumulated_events\n            if test_set == True:\n                all_assessments.append(features)\n            else:\n                if true_attempts+false_attempts > 0:\n                    all_assessments.append(features)\n                \n            counter += 1\n\n        accumulated_events += len(session)\n        if last_activity != session_type: #cout number of different consecutive user activities\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n\n    if test_set:\n        return all_assessments[-1] #for test data return last assessment \n    return all_assessments #for trainingdata return every assessment","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tqdm import tqdm\n\ncompiled_data = []\n#for i, (ins_id, user_sample) in tqdm(enumerate(train_clean.groupby('installation_id', sort=False))):\nfor (ins_id, user_sample) in tqdm(train_clean.groupby('installation_id')):\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate and train the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_features = [x for x in new_train.columns if x not in ['accuracy_group']]\ncat_features = ['session_title']\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Catboost Classifier\n\nhttps://catboost.ai/docs/concepts/about.html"},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_classifier():\n    clf = CatBoostClassifier(\n                               loss_function='MultiClass',\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                               early_stopping_rounds=300,\n                               random_seed=41\n                              )        \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from time import time\nfrom sklearn.model_selection import KFold\n\noof = np.zeros(len(X))\nn_folds = 5\nclassifiers = []\n\nfolds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ntraining_start_time = time()\nfor fold, (train_idx, validate_idx) in enumerate(folds.split(X, y)):\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    clf = make_classifier()\n    clf.fit(X.loc[train_idx, all_features], y.loc[train_idx], \n            eval_set=(X.loc[validate_idx, all_features], y.loc[validate_idx]),\n            use_best_model=True, verbose=500, cat_features=cat_features)\n    classifiers.append(clf)\n    oof[validate_idx] = clf.predict(X.loc[validate_idx, all_features]).squeeze()\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n\nprint('-' * 30)\nprint('QWK Score on Training Set using K-Fold CV:', qwk(y, oof))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# process test set\nnew_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False)):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\n#del test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# make predictions on test set once\npreds_proba = np.zeros((X_test.shape[0], 4))\n\nfor classifier in classifiers:\n    preds_proba += classifier.predict_proba(X_test)/len(classifiers)\npreds = np.argmax(preds_proba, axis=1)\n#del X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission['accuracy_group'] = np.round(preds).astype('int')\nsample_submission.to_csv('submission.csv', index=None)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}