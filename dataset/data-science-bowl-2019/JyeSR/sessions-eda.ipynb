{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sessions EDA\n\nFor this dataset, we have Installations (~Users), Game Sessions and Events. This seems like a good start, however a Game Session is not quite representative of learning patterns. It's well known that people should take breaks while studying because learning drastically drops off after 40 minutes or so. To get a better understanding of how much learning has taken place, rather than game sessions, we'll look at sessions. This approach is also commonly used in Ecommerce.\n\nSessions will be defined as consecutive events separated by no more than 15 minutes. This is motivated by the maximum length of clips (156s) as well as the recommended time for a break of 10 minutes."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom random import sample\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Data\n\nRead in all the data and process to get some convenient lists.\n\nFunctions taken from [this kernel](https://www.kaggle.com/braquino/890-features)."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation\n\nLet's augment the data to provide some usefil timings."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"if 'start_installation' in train.columns:\n    train.drop(['start_installation', 'start_game_session'],\n               axis=1, inplace=True)\n\n# Add time since start\ntrain = pd.merge(\n    left=train,\n    right=train \\\n        .groupby('installation_id') \\\n        .timestamp.min() \\\n        .to_frame() \\\n        .rename(columns={'timestamp': 'start_installation'}),\n    how='inner',\n    left_on='installation_id',\n    right_on='installation_id'\n) \\\n    .assign(seconds_since_installation =  \\\n                lambda x: (x.timestamp - x.start_installation) \\\n                .apply(lambda x: x.total_seconds()))\n\n# Add time since session start\n# Assuming game_session is unique\ntrain = pd.merge(\n    left=train,\n    right=train \\\n        .groupby('game_session') \\\n        .timestamp.min() \\\n        .to_frame() \\\n        .rename(columns={'timestamp': 'start_game_session'}),\n    how='inner',\n    left_on='game_session',\n    right_on='game_session'\n) \\\n    .assign(seconds_since_game_session_start = \\\n                lambda x: (x.timestamp - x.start_game_session) \\\n                .apply(lambda x: x.total_seconds()))\n\n# Add the mean timestamp and plot heatmap\ninstallation_props = train \\\n    .query('event_count == 1') \\\n    .groupby('installation_id') \\\n    .seconds_since_installation.mean() \\\n    .to_frame() \\\n    .rename(columns={'seconds_since_installation':'seconds_since_installation_mean'})\n\ntrain = pd.merge(\n    train,\n    installation_props,\n    how='inner',\n    left_on='installation_id',\n    right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Sessions\nTo create the sessions, we define the cutoff as mentioned above and then events together under each installation to find the '[islands](https://www.red-gate.com/simple-talk/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences/)' of events. These session ids are then merged back with the original data source."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"SESSION_CUTOFF=15*60  # seconds\n\nsession_ids = train \\\n    .groupby('installation_id') \\\n    .timestamp \\\n    .apply(lambda x: x.sort_values() \\\n               .diff() \\\n               .apply(lambda y: y.total_seconds() > SESSION_CUTOFF) \\\n               .cumsum()) \\\n    .reset_index() \\\n    .set_index('level_1') \\\n    .rename(columns={'timestamp': 'session_id'}) \\\n    .session_id\n\n# Add session_ids to train\ntrain = pd.merge(train,\n                 session_ids,\n                 how='inner',\n                 left_index=True,\n                 right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"def duration(x):\n    return x.max()-x.min()\n\nsessions = train\\\n    .groupby(['installation_id' , 'session_id']) \\\n    .agg({'seconds_since_installation': ['min', 'max', 'count', duration],\n          'game_session': 'nunique'})\n\nsessions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualisation\nWe can now visualise properties about these sessions."},{"metadata":{},"cell_type":"markdown","source":"### Session Counts\nHow many sessions per installation?"},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"train \\\n    .groupby(['installation_id']) \\\n    .session_id.nunique() \\\n    .value_counts(normalize=True, sort=True) \\\n    .head(10) \\\n    .apply(lambda x: x*100) \\\n    .plot(kind='bar')\nplt.xlabel('Number of Sessions')\nplt.ylabel('Percent of Installations')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"print('Cummulative Installations with Session Count')\ntrain \\\n    .groupby(['installation_id']) \\\n    .session_id.nunique() \\\n    .value_counts(normalize=True, sort=True) \\\n    .cumsum() \\\n    .head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unsurprisingly, over 50% of the installations have just one session. The user installs the game and then quickly forgets about it - oh, poor developer! That said, there is still a decent number of installations with multiple sessions, which means the app is getting more regular usage. "},{"metadata":{},"cell_type":"markdown","source":"### Game Session Counts\nHow many games being played in each session?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Game Session Counts')\nsessions.game_session['nunique'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sessions.game_session['nunique'] \\\n    .plot('box', vert=False, sym='')\nplt.xlabel('Game Session Count')\nplt.title('Boxplot of Game Session Counts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Session Durations\nHow long is the average session?"},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"print('Sessions Description')\nsessions.seconds_since_installation.duration.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The biggest session is a whopping 4 hours, that's some addictive gameplay right there! Though the quality of the output may have dropped a little...\n\nWe can also visualise this if we drop the outliers."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"sessions.seconds_since_installation.duration \\\n    .plot('box', vert=False, sym='')\nplt.xlabel('Session Duration (seconds)')\nplt.title('Boxplot of Session Durations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sessions Over Time\nHow do we sessions appearing over time? Are they all at once, or over multiple days? Are there big gaps?\n\nLet's plot the sessions in time for a random sample of installations."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"results ={}\nnum_samples = 50\nnum_days = 80\nfor installation in sample(list(train.installation_id.unique()), num_samples):\n    count, division = np.histogram(train \\\n        .query('installation_id == @installation') \\\n        .drop_duplicates(subset=['installation_id', 'session_id'], keep='first')\n        .seconds_since_installation \\\n        .apply(lambda x: x/60/60/24),\n        bins = np.linspace(0, num_days, num_days + 1))\n    results[installation] = count\ndivision = list(map(int, division))\nresults = pd.DataFrame.from_dict(results,\n                                 orient='index',\n                                 columns=division[:-1])\n\nsns.heatmap(\n    pd.merge(\n        results,\n        installation_props,\n        how='inner',\n        left_index=True,\n        right_index=True\n    ) \\\n        .sort_values(by='seconds_since_installation_mean', ascending=True) \\\n        .drop(['seconds_since_installation_mean'], axis=1),\n    yticklabels=False,\n    vmax=3,\n    xticklabels=5\n)\nfig = plt.gcf()\nfig.set_size_inches(12,8)\nplt.xlabel('Days since installation')\nplt.ylabel('Installations')\nplt.title('Daily Session Counts per Installation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we see that the majority of users are with few sessions, and even those with multiple sessions are seen to have them on just one day. Looking past that we do see some interesting behaviour, like users combing back every few days or even those coming back 70 days after the installation with nothing in between.\n\nWe can get a closer look at those installations with sessions over many days by doing some filtering."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"results ={}\nnum_samples = 50\nnum_days = 80\nfor installation in sample(list(train.query('seconds_since_installation_mean > 3*24*60*60').installation_id.unique()), num_samples):\n    count, division = np.histogram(train \\\n        .query('installation_id == @installation') \\\n        .query('seconds_since_installation_mean > 1*24*60*60') \\\n        .drop_duplicates(subset=['installation_id', 'session_id'], keep='first')\n        .seconds_since_installation \\\n        .apply(lambda x: x/60/60/24),\n        bins = np.linspace(0, num_days, num_days + 1))\n    results[installation] = count\ndivision = list(map(int, division))\nresults = pd.DataFrame.from_dict(results,\n                                 orient='index',\n                                 columns=division[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(\n    pd.merge(\n        results,\n        installation_props,\n        how='inner',\n        left_index=True,\n        right_index=True\n    ) \\\n        .sort_values(by='seconds_since_installation_mean', ascending=True) \\\n        .drop(['seconds_since_installation_mean'], axis=1),\n    yticklabels=False,\n    vmax=3,\n    xticklabels=5\n)\nfig = plt.gcf()\nfig.set_size_inches(12,8)\nplt.xlabel('Days since installation')\nplt.ylabel('Installations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see some very addicted users accessing almost ever day without fail for weeks on end! This sort of behaviour seems like the kind of training that would lead to good performance, vs. those that are coming back a lot in one day or randomly. This is because spaced repetition is much more powerful than compressed learning. Also on the hourly scale, (e.g. no more than 1 hour of sessions or it becomes less valuable)."},{"metadata":{},"cell_type":"markdown","source":"The sort of graphics above leads us to ask questions like how often is a user coming back? For this, we can calculate the frequency."},{"metadata":{},"cell_type":"markdown","source":"### Frequency\nWe will calculate frequency of an installation as the average time between the start of sessions. It is undefined for users with just one session."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"sessions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"sessions.loc['0006a69f', :]['seconds_since_installation']['min'].diff()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"results = {}\nfor i in list(set([a[0] for a in sessions.index]))[:20000]:\n    results[i] = [sessions.loc[i, :]['seconds_since_installation']['min'].diff().mean(),\n                  sessions.loc[i, :]['seconds_since_installation']['min'].diff().median()]\nfrequencies = pd.DataFrame.from_dict(results, orient='index', columns=['mean', 'median']) \\\n    .apply(lambda x: x/60/60/24)\nfrequencies[frequencies['mean'].notnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"frequencies.plot(kind='box', sym='')\nfig = plt.gcf()\nfig.set_size_inches(8, 5)\nplt.ylabel('Number of Days between Sessions')\nplt.title('Frequency of Installations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that ignoring the users that have just one session, the number of days between sessions is ~5, though the third quartile (50%-75%) extends up to ~10 days.\n\nThis sort of frequency feature might be useful as a feature to predict performance on the games as it describes what sort of habit the user has."}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}