{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Memory efficient, faster way (<10 min) to extract JSON data\nIn this notebook I'll show you a solution for handling the JSON event-data. The goal is a fast, memory-efficient way to load and prepare the train (or test) dataframe.\n\n\n- It loads and converts the selected JSON arguments into a dataframe less then 10 minutes.\n- It keeps the memory usage as low as possible (the final train dataframe is ~500 Mb)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport csv\nimport gc\n\nfrom collections import OrderedDict\nfrom tqdm import tqdm_notebook as tqdm\n\n# dtypes for pd.read_csv\n# These are help to reduce the memory usage.\nDTYPES_RAW = {\n    'event_id': 'object',\n    'game_session': 'object',\n    'installation_id': 'object',\n    'event_count': np.uint16,\n    'event_code': np.uint16,\n    'game_time': np.uint32,\n    'type': 'category',\n    'world': 'category',\n    'title': 'category',  \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract these arguments from JSON.\n# There is not enough memory to extract everything with this method.\n# You should try it whether it can process the private test set too\n# with your selected arguments\nFIELDS = {\n    # Extras from JSON\n    # If you add more data, do not forget\n    # to add default values below.\n    'level': np.uint8,\n    'round': np.uint8,\n    'correct': np.int8,\n    'misses': np.int8,\n    \n    # Nested object separated by '_'\n    # for example: {'coordinates': {'x': 12, 'y': 12}}\n    # 'coordinates_x': np.uint16\n    # 'coordinates_y': np.uint16\n}\n\nDTYPES = OrderedDict( (dt[0], (dt[1], i)) for i, dt in enumerate(FIELDS.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This only needs if you want to show a TQDM progress bar.\nimport subprocess\n\ndef file_len(fname):\n    \"\"\"Returns the number of lines in a file.\n       @see: https://www.kaggle.com/szelee/how-to-import-a-csv-file-of-55-million-rows\n    \"\"\"\n    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, \n                                              stderr=subprocess.PIPE)\n    result, err = p.communicate()\n    if p.returncode != 0:\n        raise IOError(err)\n    return int(result.strip().split()[0])+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flatten(dct, res, separator='_'):\n    \"\"\"Flatten a dictionary.\n       @see: https://stackoverflow.com/a/34094630/4158850\n    \"\"\"\n    queue = [('', dct)]\n\n    while queue:\n        prefix, d = queue.pop()\n        for k, v in d.items():\n            key = prefix + k\n            if not isinstance(v, dict):\n                if key in FIELDS.keys():\n                    res[0][DTYPES[key][1]] = v\n            else:\n                queue.append((key + separator, v))\n\n    return res\n\ndef records_from_json(fh, n_rows, event_ids_to_drop):\n    \"\"\"Yields the records from a file object.\"\"\"\n    rows = csv.reader(fh, delimiter=',')\n    skip_header = next(rows)\n    \n    # define dtype for more memory-efficiency.\n    dtype = dict(names=list(FIELDS.keys()), formats=list(FIELDS.values()))\n    defrow = np.zeros((1,), dtype=dtype)\n\n    for event_id, game_session, timestamp, event_data, installation_id, event_count, event_code, game_time, title, typ, world in tqdm(rows, total=n_rows):\n        \n        # It is more memory-efficient if we don't use the the train df's columns yet.\n        row = defrow.copy()\n\n        # Default (required because of the copy above) values for the extracted data\n        # you can use np.nan too (in this case the dtype should be np.float64)\n        row[0][DTYPES['level'][1]] = 0\n        row[0][DTYPES['round'][1]] = 0\n        row[0][DTYPES['correct'][1]] = -1\n        row[0][DTYPES['misses'][1]] = -1\n\n        if event_id not in event_ids_to_drop:\n            row = flatten(json.loads(event_data), row)\n\n        yield row[0]\n\ndef from_records(path, event_ids_to_drop):\n    n_rows = file_len(path)\n    with open(path) as fh:\n        return pd.DataFrame.from_records(records_from_json(fh, n_rows, event_ids_to_drop))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extract JSON event data"},{"metadata":{},"cell_type":"markdown","source":"This is [Miika](https://www.kaggle.com/taenareus)'s idea, see his comment below.\n> You can speed up processing significantly by not parsing rows that do not have json data of interest.\n> To determine which event_ids to drop, just read the `spec.csv` file!\n>\n> Using this trick allows you to parse all relevant data in a small number of minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\nspecs.args = specs.args.apply(lambda x: json.loads(str(x)))\neventIdsToDrop = []\n\nfor _, spec in specs.iterrows():\n    j = pd.io.json.json_normalize(spec.args)\n    vals = j.loc[(j.name.isin(FIELDS.keys()))].name.values\n\n    if len(vals) == 0:\n        eventIdsToDrop += [spec.event_id]\n\nset(eventIdsToDrop)\nprint(len(eventIdsToDrop))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"extras_df = from_records('/kaggle/input/data-science-bowl-2019/train.csv', eventIdsToDrop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', parse_dates=['timestamp'], dtype=DTYPES_RAW, usecols=['timestamp'] + list(DTYPES_RAW.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.merge(extras_df, left_index=True, right_index=True)\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv('train_extras.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del extras_df\ndel train_df\ngc.collect()\n\n%reset -f Out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"extras_df = from_records('/kaggle/input/data-science-bowl-2019/test.csv', eventIdsToDrop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', parse_dates=['timestamp'], dtype=DTYPES_RAW, usecols=['timestamp'] + list(DTYPES_RAW.keys()))\ntest_df = test_df.merge(extras_df, left_index=True, right_index=True)\n\ntest_df.to_csv('test_extras.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del extras_df\ndel test_df\ngc.collect()\n\n%reset -f Out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"**Thanks for reading**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}