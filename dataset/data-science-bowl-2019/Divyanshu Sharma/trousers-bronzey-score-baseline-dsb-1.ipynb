{"cells":[{"metadata":{},"cell_type":"markdown","source":"Inspired from:\n\n* https://www.kaggle.com/artgor/quick-and-dirty-regression\n* https://www.kaggle.com/pestipeti/memory-efficient-faster-way-to-extract-json-data\n* https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport shap\nimport warnings\nimport os\nfrom time import time\nimport scipy as sp\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error, classification_report\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom hyperopt.pyll import scope\nimport gc\nimport json\nimport copy\nfrom functools import partial\n\nfrom extract_json_script import extract_event_data\n\nwarnings.filterwarnings(\"ignore\")\ntqdm.pandas()\npd.set_option('display.max_columns', 1000)\npd.set_option('max_rows', 500)\nnp.random.seed(47)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flags"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEV = False\nFEATURE_SELECTION = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"DTYPES_RAW = {\n    'event_id': 'category',\n    'game_session': 'object',\n    'installation_id': 'object',\n    'event_count': np.uint16,\n    'event_code': np.uint16,\n    'game_time': np.uint32,\n    'type': 'category',\n    'world': 'category',\n    'title': 'category',  \n}\n\ntrain_df = pd.read_csv('../input/data-science-bowl-2019/train.csv', parse_dates=['timestamp'], usecols=list(DTYPES_RAW.keys()) + ['timestamp'], dtype=DTYPES_RAW, engine='c')\ntest_df = pd.read_csv('../input/data-science-bowl-2019/test.csv', parse_dates=['timestamp'], usecols=list(DTYPES_RAW.keys()) + ['timestamp'], dtype=DTYPES_RAW, engine='c')\ntrain_labels_df = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv', dtype=DTYPES_RAW)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Removing `installation_id`s with no training labels\nprint(train_df.shape)\ntrain_df = train_df[train_df['installation_id'].isin(train_labels_df['installation_id'].unique())] \nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add assessment counter\ndef assess_count(df):\n    df['Assessment'] = 0\n    cond_ind = df.query(\"type == 'Assessment' and (event_code == 4100 or event_code == 4110)\").index\n    df.loc[cond_ind, 'Assessment'] = 1\n    df['counter'] = df.groupby(['installation_id'])['Assessment'].cumsum() - df['Assessment']\n    return df.drop(['Assessment'], axis=1)\n\n# train_df = assess_count(train_df.sort_values(by=['timestamp']))\n# test_df = assess_count(test_df.sort_values(by=['timestamp']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def attempts_count(df):\n    count = df.query(\"event_code == 4100 or event_code == 4110\").shape[0]\n    df['attempts'] = count\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def attempts_flag(df):\n    cond_ind = df.query(\"event_code == 4100 or event_code == 4110\").index\n    df.loc[cond_ind, 'attempt'] = 1\n    return df\n\ntrain_df = attempts_flag(train_df)\ntest_df = attempts_flag(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = None\n\ndef event_data(fname):\n    agg_dict = {'level': np.max, 'round': np.max, 'correct': np.sum, 'misses':np.sum}\n    extras_df = extract_event_data(filename=f'{fname}.csv', type_defaults=list(agg_dict.keys()), nrows=nrows)\n    event_df = pd.merge(globals()[f'{fname}_df'].reindex(['installation_id', 'game_session'], axis=1), extras_df, left_index=True, right_index=True)\n    event_summary_df = event_df.groupby(['installation_id', 'game_session'], as_index=False).agg(agg_dict)\n#     event_summary_df.reset_index(inplace=True)\n    return event_summary_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# agg_dict = {'counter': np.max, 'game_time': np.max, 'event_count': np.max, 'timestamp': np.max}\nagg_dict = {'attempt': np.sum, 'game_time': np.max, 'event_count': np.max, 'timestamp': np.max}\n\ncomp_train_df = train_df.groupby(['installation_id', 'game_session', 'title', 'type', 'world'], observed=True, as_index=False).agg(agg_dict)\ncomp_test_df = test_df.groupby(['installation_id', 'game_session', 'title', 'type', 'world'], observed=True, as_index=False).agg(agg_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_train_df = comp_train_df.merge(event_data('train'), on=['installation_id', 'game_session'], how='left')\ncomp_test_df = comp_test_df.merge(event_data('test'), on=['installation_id', 'game_session'], how='left') \n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activities associated with each `Assessment`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def backfill(df):\n    df['counter'] = df['counter'].fillna(method='backfill')\n    return df.dropna(subset=['counter'], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `comp_train_df`"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_df = train_labels_df.merge(comp_train_df[['installation_id', 'game_session', 'timestamp']], on=['installation_id', 'game_session'], how='left')\ntrain_labels_df['Assessment'] = 1\ntrain_labels_df['counter'] = train_labels_df.sort_values('timestamp').groupby(['installation_id'])['Assessment'].cumsum()\ntrain_labels_df = train_labels_df.drop(['Assessment'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_train_df = comp_train_df.merge(train_labels_df[['installation_id', 'game_session', 'counter']], on=['installation_id', 'game_session'], how='left')\ncomp_train_df = comp_train_df.sort_values(by=['installation_id', 'timestamp'])\ncomp_train_df = comp_train_df.groupby('installation_id', as_index=False).apply(lambda df: backfill(df)).reset_index(drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(comp_train_df.groupby(['installation_id','counter'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `comp_test_df`"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_test_df.loc[comp_test_df['type'] == 'Assessment', 'Assessment'] = 1\ncomp_test_df['counter'] = comp_test_df.sort_values('timestamp').groupby(['installation_id'])['Assessment'].cumsum()\ncomp_test_df = comp_test_df.drop(['Assessment'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_test_df = comp_test_df.sort_values(by=['installation_id', 'timestamp'])\ncomp_test_df = comp_test_df.groupby('installation_id', as_index=False).apply(lambda df: backfill(df)).reset_index(drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(comp_test_df.groupby(['installation_id','counter'])))\nprint(comp_test_df.groupby(['installation_id'], as_index=False).last().shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging target labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"## For train\n_labels = train_labels_df[['game_session', 'accuracy_group']]\ncomp_train_assess_df = _labels.merge(right=comp_train_df, on='game_session', how='left')\n\n## For test -- random target labels\n_labels = comp_test_df.query(\"type == 'Assessment'\")[['game_session']]\n_labels['accuracy_group'] = np.random.randint(low=0, high=4) # random target labels\n# _last_assessements = _test_assess.groupby(by='installation_id', as_index=False).last()\ncomp_test_assess_df = _labels.merge(right=comp_test_df, on='game_session', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregating on Assessment Level"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cumulative(cumfunc_dict, column):\n    \n    try:\n        return cumfunc_dict[column.name[1]](column)\n    except Exception as e:\n        return column\n\ndef filter_activity(df):\n    \"\"\"\n    Returns `df` segregated into clips (`comp_clip`) and all others (`comp_noclip`)\n    \"\"\"\n    \n    comp_clip = df.loc[(df['type']=='Clip')]\n    comp_noclip = df.loc[(df['type']!='Clip')]\n    \n#     comp_clip = df.loc[(df['type']=='Clip') & (df['counter']<=25)]\n#     comp_noclip = df.loc[(df['type']!='Clip') & (df['counter']<=25)]\n    return comp_clip.reset_index(drop=True), comp_noclip.reset_index(drop=True)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregate_activity(df):\n    comp_df_clip, comp_df_noclip = filter_activity(df)\n    \n    agg_dict_clip = {'title': [pd.Series.nunique, pd.Series.count]} \n    agg_dict_noclip = {'game_time': [np.max, np.sum, np.mean], 'event_count': [np.max, np.sum, np.mean], 'correct': [np.sum], 'misses': [np.sum], 'attempt': [np.sum]}\n    pivot_df_noclip = pd.pivot_table(comp_df_noclip, index=['installation_id', 'counter'], values=list(agg_dict_noclip.keys()), columns=['world'], aggfunc=agg_dict_noclip)\n    pivot_df_clip = pd.pivot_table(comp_df_clip, index=['installation_id', 'counter'], values=list(agg_dict_clip.keys()), columns=['world'], aggfunc=agg_dict_clip)\n    \n    pivot_df_noclip.columns = pivot_df_noclip.columns.to_flat_index()\n    pivot_df_clip.columns = pivot_df_clip.columns.to_flat_index()\n    \n    pivot_df_noclip = pivot_df_noclip.reset_index().sort_values(by=['installation_id', 'counter'])\n    pivot_df_clip = pivot_df_clip.reset_index().sort_values(by=['installation_id', 'counter'])\n    \n    aggregate_activity_df = pivot_df_noclip \\\n                            .merge(pivot_df_clip, on=['installation_id', 'counter'], how='left')\n    \n    aggregate_activity_df = aggregate_activity_df.dropna(axis=1, how='all') #.fillna(0)\n    \n    cumfunc_dict = {'amax' : pd.Series.cummax, 'sum': pd.Series.cumsum, 'count': pd.Series.cumsum, 'mean': pd.Series.cumsum}\n    cumfunc = partial(cumulative, cumfunc_dict)\n    aggregate_activity_df = aggregate_activity_df.groupby(['installation_id']).apply(lambda df: df.apply(cumfunc))\n\n    return aggregate_activity_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(comp_train_df.shape)\nprint(comp_test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['game_session', 'type', 'world', 'level', 'round', 'correct', 'misses']\n\naggregate_activity_test_df = aggregate_activity(comp_test_df) \\\n                                .merge(comp_test_assess_df[['installation_id', 'counter', 'title', 'game_session']], on=['installation_id', 'counter'], how='left')\n#                                 .drop(columns=drop_cols)\n\ngc.collect()\n\naggregate_activity_train_df = aggregate_activity(comp_train_df) \\\n                                .merge(comp_train_assess_df[['installation_id', 'counter', 'title', 'game_session']], on=['installation_id', 'counter'], how='left')\n#                                 .drop(columns=drop_cols)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding mode as feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Most frequent accuracy group per assessement (to get a sense of what 'average' performance is)\nmode_per_title = train_labels_df.groupby('title').agg({'accuracy_group' : pd.Series.mode})['accuracy_group']\naggregate_activity_train_df['title_mode'] = aggregate_activity_train_df['title'].map(mode_per_title) \naggregate_activity_test_df['title_mode'] = aggregate_activity_test_df['title'].map(mode_per_title) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_activity_train_df = aggregate_activity_train_df.drop(['title'], axis=1)\naggregate_activity_test_df = aggregate_activity_test_df.drop(['title'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(aggregate_activity_train_df.shape)\nprint(aggregate_activity_test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_activity_features = set(aggregate_activity_train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, test_df, train_labels_df, comp_test_df, comp_train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    # hour\n    train['hour'] = train['timestamp'].dt.hour\n    test['hour'] = test['timestamp'].dt.hour\n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clip_time = {'Welcome to Lost Lagoon!':19,'Tree Top City - Level 1':17,'Ordering Spheres':61, 'Costume Box':61,\n        '12 Monkeys':109,'Tree Top City - Level 2':25, 'Pirate\\'s Tale':80, 'Treasure Map':156,'Tree Top City - Level 3':26,\n        'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18,\n        'Balancing Act':72, 'Lifting Heavy Things':118,'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19,\n        'Heavy, Heavier, Heaviest':61}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnt_miss(df):\n    cnt = 0\n    for e in range(len(df)):\n        x = df['event_data'].iloc[e]\n        y = json.loads(x)['misses']\n        cnt += y\n    return cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    game_time_dict = {'Clip_gametime':0, 'Game_gametime':0, 'Activity_gametime':0, 'Assessment_gametime':0}\n    Assessment_mean_event_count = 0\n    Game_mean_event_count = 0\n    Activity_mean_event_count = 0\n    mean_game_round = 0\n    mean_game_duration = 0 \n    mean_game_level = 0\n    accumulated_game_miss = 0\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    clip_durations = []\n    Activity_durations = []\n    Game_durations = []\n    \n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # last features\n    sessions_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n        if session_type == 'Clip':\n            clip_durations.append((clip_time[activities_labels[session_title]]))\n        \n        if session_type == 'Activity':\n            Activity_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0\n        \n        if session_type == 'Game':\n            Game_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n            \n            game_s = session[session.event_code == 2030]   \n            misses_cnt = cnt_miss(game_s)\n            accumulated_game_miss += misses_cnt\n            \n            try:\n                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n                mean_game_round =  (mean_game_round + game_round)/2.0\n            except:\n                pass\n\n            try:\n                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n                mean_game_duration = (mean_game_duration + game_duration) /2.0\n            except:\n                pass\n            \n            try:\n                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n                mean_game_level = (mean_game_level + game_level) /2.0\n            except:\n                pass\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            # features.update(game_time_dict.copy())\n            \n            features['installation_session_count'] = sessions_count\n            features['hour'] = session['hour'].iloc[-1]\n            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n            features['Game_mean_event_count'] = Game_mean_event_count\n            features['Activity_mean_event_count'] = Activity_mean_event_count\n            features['mean_game_round'] = mean_game_round\n            features['mean_game_duration'] = mean_game_duration\n            features['mean_game_level'] = mean_game_level\n            features['accumulated_game_miss'] = accumulated_game_miss\n            \n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            features['game_session'] = i\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n            if clip_durations == []:\n                features['Clip_duration_mean'] = 0\n                features['Clip_duration_std'] = 0\n            else:\n                features['Clip_duration_mean'] = np.mean(clip_durations)\n                features['Clip_duration_std'] = np.std(clip_durations)\n                \n            if Activity_durations == []:\n                features['Activity_duration_mean'] = 0\n                features['Activity_duration_std'] = 0\n            else:\n                features['Activity_duration_mean'] = np.mean(Activity_durations)\n                features['Activity_duration_std'] = np.std(Activity_durations)\n                \n            if Game_durations == []:\n                features['Game_duration_mean'] = 0\n                features['Game_duration_std'] = 0\n            else:\n                features['Game_duration_mean'] = np.mean(Game_durations)\n                features['Game_duration_std'] = np.std(Game_durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1])/2.0\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        game_time_dict[session_type+'_gametime'] = (game_time_dict[session_type+'_gametime'] + (session['game_time'].iloc[-1]/1000.0))/2.0\n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments #[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort = False), total=train['installation_id'].nunique()):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total=test['installation_id'].nunique()):\n        compiled_test += get_data(user_sample, True)\n        \n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_train, _test, _train_labels, specs, sample_submission = read_data()\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(_train, _test, _train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train, reduce_test, categoricals = get_train_and_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_features = set(reduce_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = ['installation_id', 'game_session']\ntarget = ['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train = reduce_train.merge(aggregate_activity_train_df, on=keys)\nreduce_test = reduce_test.merge(aggregate_activity_test_df, on=keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(reduce_train.shape)\nprint(reduce_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(reduce_train.select_dtypes(include=['object', 'category']).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `to_csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEV:\n    reduce_train.to_csv('final_train.csv', index=False)\n    reduce_test.to_csv('final_test.csv', index=False)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\nfeatures = [x for x in features if x not in (target + keys)]\nprint(len(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_correlated(features, threshold=0.995):\n    to_remove = []\n    counter = 0\n    for feat_a in tqdm(features):\n        for feat_b in features:\n            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1] # or [1][0]\n                if c > threshold:\n                    counter += 1\n                    to_remove.append(feat_b)\n#                     print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n                    \n    return to_remove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data) # or use pd.cut or any other normalisation method\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    msre = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(msre)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5, label='train')\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5, label='test')\n        plt.legend()\n        plt.show()\n    return msre\n\n# stract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_errored():\n    to_exclude = [] \n    ajusted_test = reduce_test.copy()\n    for feature in ajusted_test.columns:\n        if feature not in (target + keys + categoricals):\n            data = reduce_train[feature]\n            train_mean = data.mean()\n            data = ajusted_test[feature] \n            test_mean = data.mean()\n            try:\n                error = stract_hists(feature, adjust=True)\n                ajust_factor = train_mean / test_mean\n                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01: ## UNDERSTAND\n                    to_exclude.append(feature)\n#                     print(feature, train_mean, test_mean, error)\n                else:\n                    ajusted_test[feature] *= ajust_factor\n            except:\n                to_exclude.append(feature)\n#                 print(feature, train_mean, test_mean)\n\n    return to_exclude, ajusted_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FEATURE_SELECTION:\n    to_remove = remove_correlated(features, threshold=0.990)\n    to_exclude, ajusted_test = remove_errored()\n    features = [x for x in features if x not in (to_exclude + to_remove)]\n    from_1 = set(features) & aggregate_activity_features\n    from_2 = set(features) & reduce_features\n    print(f\"{len(from_1)} features from aggregate_activity_train_df: \", from_1)\n    print(f\"{len(from_2)} features from aggregate_activity_train_df: \", from_2)\n    \nelse:\n    _, ajusted_test = remove_errored()\n\nreduce_train[features].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def regr_to_label(y_pred):\n    \n    acums = train_labels['accuracy_group'].value_counts(normalize=True).sort_index().cumsum().to_numpy()\n    bound = np.percentile(y_pred, acums*100)\n    \n#     y_pred = pd.cut(y_pred, [-np.inf] + list(np.sort(bound)) + [np.inf], labels = [0, 1, 2, 3]).reshape(y_pred.shape)\n    \n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_pred.shape)\n    return y_pred\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \n    y_pred = regr_to_label(y_pred).reshape(y_true.shape)\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -cohen_kappa_score(y, X_p, weights='quadratic')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n#         initial_coef = [0.5, 1.5, 2.5]\n        acums = train_labels['accuracy_group'].value_counts(normalize=True).sort_index().cumsum().to_numpy()\n        initial_coef = np.percentile(X, acums*100)[:3]\n    \n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Implement a generalised pipeline, with optional StratifiedKFold\n\nclass Base_Model(object):\n    \n    def __init__(self, train_df, test_df, features, evaluator, params=None, categoricals=[], n_splits=5, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = 'accuracy_group'\n        self.cv = self.get_cv() if n_splits is not None else None\n        self.verbose = verbose\n        self.params = params # if params not None else self.get_params()\n        self.evaluator = evaluator\n        \n    def __call__(self):\n        self.oof_pred, self.y_pred, self.score, self.model = self.fit()\n        return self\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        oof_pred = np.zeros((len(self.train_df), ))\n        y_pred = np.zeros((len(self.test_df), ))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            conv_x_val = self.convert_x(x_val)\n            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n#             print(f'Partial score of fold {fold} is: {self.evaluator(y_val, oof_pred[val_idx])}')\n            \n        loss_score = self.evaluator(self.train_df[self.target], oof_pred)\n\n        if self.verbose:\n            print(f'oof {self.evaluator.__name__} score is {loss_score}')\n        return oof_pred, y_pred, loss_score, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Xgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return xgb.train(self.params, train_set, evals=[(train_set, 'train'), (val_set, 'val')], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        return train_set, val_set\n    \n    def convert_x(self, x):\n        return xgb.DMatrix(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mlr_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        clf = LogisticRegression(**self.params)\n        return clf.fit(train_set[:, :-1], train_set[:, -1:])\n\n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        \n        scaler = StandardScaler()\n        X_train_std = scaler.fit_transform(x_train.to_numpy())\n        X_val_std = scaler.fit_transform(x_val.to_numpy())\n        \n        y_train = np.expand_dims(y_train, 1)\n        y_val = np.expand_dims(y_val, 1)\n        \n        train_set = np.concatenate([X_train_std, y_train], axis=1)\n        val_set = np.concatenate([X_val_std, y_val], axis=1)\n        \n        return train_set, val_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper-parameter optimisation (Optional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_hyperopt(param_space, x_train, x_val, y_train, y_val, features, categoricals, num_eval):\n    \n    weights = {'lbg': 0.80, 'xgb': 0.20}\n\n    def objective_function(params):\n        lbg_params = {k[5:]: v for k, v in params.items() if k.startswith(\"l\")}\n        xbg_params = {k[5:]: v for k, v in params.items() if k.startswith(\"x\")}\n#         print(lbg_params, xbg_params)\n        lgb_model = Lgb_Model(train_df=None, test_df=None, features=None, evaluator=mean_squared_error, params=lbg_params, categoricals=categoricals, n_splits=None, verbose=False)\n        xgb_model = Xgb_Model(train_df=None, test_df=None, features=None, evaluator=mean_squared_error, params=xbg_params, categoricals=categoricals, n_splits=None, verbose=False)\n        train_set_lgb, val_set_lgb = lgb_model.convert_dataset(x_train, y_train, x_val, y_val)\n        train_set_xgb, val_set_xgb = xgb_model.convert_dataset(x_train, y_train, x_val, y_val)\n\n        lgb_model.model = lgb_model.train_model(train_set_lgb, val_set_lgb)\n        xgb_model.model = xgb_model.train_model(train_set_xgb, val_set_xgb)\n        regr_pred = (lgb_model.model.predict(x_val).reshape(y_val.shape) * weights['lbg']) + (xgb_model.model.predict(xgb.DMatrix(x_val, y_val)).reshape(y_val.shape) * weights['xgb']) \n        score = mean_squared_error(y_val, regr_pred)\n        return score\n    \n    trials = Trials()\n    best_param = fmin(objective_function, \n                      param_space, \n                      algo=tpe.suggest, \n                      max_evals=num_eval, \n                      trials=trials)\n\n    return trials, best_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyperopt(param_space, x_train, x_val, y_train, y_val, features, categoricals, num_eval):\n    \n    def objective_function(params):\n        lgb_model = Lgb_Model(train_df=None, test_df=None, features=None, evaluator=mean_squared_error, params=params, categoricals=categoricals, n_splits=None, verbose=False)\n        train_set, val_set = lgb_model.convert_dataset(x_train, y_train, x_val, y_val)\n        lgb_model.model = lgb_model.train_model(train_set, val_set)\n#         lgb_model = Lgb_Model(X_train, X_val, features=features, categoricals=categoricals, evaluator=mean_squared_error, n_splits=2, verbose=False)\n        y_pred = lgb_model.model.predict(x_val).reshape(y_val.shape)\n        score = mean_squared_error(y_val, y_pred)\n        return score\n    \n    trials = Trials()\n    best_param = fmin(objective_function, \n                      param_space, \n                      algo=tpe.suggest, \n                      max_evals=num_eval, \n                      trials=trials)\n\n    return trials, best_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_params(param):\n    new_param = {}\n    dtypes = {\n        'bagging_fraction': float,\n        'feature_fraction': float,\n        'learning_rate': float,\n        'max_depth': int,\n        'n_estimators': int,\n        'num_leaves': int,\n        'lambda_l1': float,\n        'lambda_l2': float,\n        'cat_smooth': int\n    }\n        \n    for k in param.keys():\n        new_param[k] = dtypes[k](param[k])\n#     new_param['boosting'] = 'gbdt' if param['boosting'] == 0 else 'dart'    \n\n    return new_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = scope.int(hp.quniform('max_depth', 5, 100, 1))\nbagging_fraction = hp.uniform('bagging_fraction', 0.5, 1.0)\nfeature_fraction = hp.uniform('feature_fraction', 0.1, 1.0)\nlambda_l1 = hp.uniform('lambda_l1', 0.0, 100.0)\nlambda_l2 = hp.uniform('lambda_l2', 0.0, 100.0)\ncat_smooth = scope.int(hp.quniform('cat_smooth', 10, 100, 1))\nboosting = hp.choice('boosting', ['gbdt', 'dart'])\nnum_iterations = scope.int(hp.quniform('num_iterations', 50, 500, 25))\nlearning_rate = hp.loguniform('learning_rate', np.log(0.01), np.log(1))\nnum_leaves = scope.int(hp.quniform('num_leaves', 5, 100, 1))\nn_estimators = scope.int(hp.quniform('n_estimators', 100, 10000, 50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comma_names = {col: col.replace(',', '') for col in train.filter(like=',', axis=1).columns}\nreduce_train = reduce_train.rename(columns=comma_names)\nreduce_test = reduce_test.rename(columns=comma_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_pct = 0.15\nsplit_size = int(split_pct * len(reduce_train))\n\n# indices = np.random.randint(low=0, high=len(reduce_train) -1, size=(split_size,))\n# X_train = reduce_train.drop(index=indices)\n# X_val = reduce_train.filter(items=indices, axis=0)\n\nx_train, x_val, y_train, y_val = train_test_split(\n    reduce_train[features],\n    reduce_train['accuracy_group'],\n    test_size=split_pct\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learnable_params= {\n    'learning_rate': learning_rate,\n    'n_estimators': n_estimators,\n    'num_leaves': num_leaves,\n    'bagging_fraction': bagging_fraction,\n    'feature_fraction': feature_fraction,\n    'lambda_l1': lambda_l1,\n    'lambda_l2': lambda_l2,\n    'cat_smooth': cat_smooth\n}\n\nstatic_params = {'objective': 'regression', 'metric': 'mse', 'n_estimators': 13000, 'early_stopping_round': 10}\nparam_hyperopt = {**static_params, **learnable_params}\n\nmax_eval = 200\n# _, para = hyperopt(param_hyperopt, x_train, y_train.astype(int), x_val, y_val.astype(int), categoricals, max_eval)\n# _, para = hyperopt(param_space, x_train, x_val, y_train, y_val, features, categoricals, max_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"other_params= {\n    'lgb__lambda_l2': hp.uniform('lambda_l2', 0.0, 100.0),\n    'xgb__lambda': hp.uniform('lambda', 0.0, 100.0),\n}\n\ncommon_params = {\n    'objective': ['mean_squared_error', 'reg:squarederror'],\n    'n_estimators': [6700, 5000],\n    'early_stopping_round': [100, 100],\n    'max_depth': [-1, 0],\n    'learning_rate': [0.01, 0.01]\n}\n\nsuffixes = ['lbg__', 'xgb__']\ncommon_params = {suffixes[i] + k: v[i] for k, v in common_params.items() for i in range(2)}\nparam_hyperopt = {**common_params, **other_params}\n\nmax_eval = 100\n# _, para = ensemble_hyperopt(param_hyperopt, x_train, x_val, y_train, y_val, features, categoricals, max_eval)\n## TD: Split para into params_lgb and params_xgb ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {\n    'n_estimators':5000,\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'subsample': 0.75,\n    'subsample_freq': 1,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.9,\n    'max_depth': 15,\n    'lambda_l1': 1,  \n    'lambda_l2': 1,\n    'early_stopping_rounds': 100\n }\n    \n    \nparams_xgb = {\n    'colsample_bytree': 0.8,                 \n    'learning_rate': 0.01,\n    'max_depth': 10,\n    'objective':'reg:squarederror',\n    'min_child_weight':3,\n    'gamma':0.25,\n    'n_estimators':5000\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params_lgb = {\n#     'objective': 'mean_squared_error',\n#     'learning_rate': 0.01,\n#     'n_estimators': 6700,\n#     'early_stopping_round': 100,\n#     'feature_fraction': 0.8\n    \n# }\n\n# params_xgb = {\n#     'objective':'reg:squarederror',\n#     'learning_rate': 0.01,\n#     'n_estimators': 6700,\n#     'early_stopping_round': 100,\n#     'max_depth': 0,\n#     'colsample_bytree': 0.8,\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(params_lgb)\nprint(params_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = eval_qwk_lgb_regr\nqwk = lambda y_true, y_pred: cohen_kappa_score(y_true, y_pred, weights='quadratic')\nxgb_model = Xgb_Model(reduce_train, ajusted_test, features=features, categoricals=categoricals, params=params_xgb, evaluator=evaluator, verbose=True)()\nlgb_model = Lgb_Model(reduce_train, ajusted_test, features=features, categoricals=categoricals, params=params_lgb, evaluator=evaluator, verbose=True)()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {'lbg': 0.80, 'xgb': 0.20}\nregr_pred = (lgb_model.y_pred * weights['lbg']) + (xgb_model.y_pred * weights['xgb']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# regr_pred = lgb_model.y_pred\n# regr_pred = xgb_model.y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performance on training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lgb = lgb_model.model.predict(reduce_train[features])\npreds_xgb = xgb_model.model.predict(xgb.DMatrix(reduce_train[features], reduce_train['accuracy_group']))\n\npreds = (preds_lgb * weights['lbg']) + (preds_xgb * weights['xgb']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using `OptimizedRounder`"},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(preds.reshape(-1,), reduce_train['accuracy_group'].values.reshape(-1,))\ncoefficients = optR.coefficients()\nprint(\"OptimizedRounder qwk = \", qwk(reduce_train['accuracy_group'].values, optR.predict(preds.reshape(-1, ), coefficients)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(reduce_train['accuracy_group'].values, optR.predict(preds.reshape(-1, ), coefficients), output_dict=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"eval_qwk_lgb_regr qwk = \", eval_qwk_lgb_regr(reduce_train['accuracy_group'].values, preds.reshape(-1, )))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(reduce_train['accuracy_group'].values, regr_to_label(preds.reshape(-1, )), output_dict=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_pred = regr_to_label(regr_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = optR.predict(regr_pred.reshape(-1, ), coefficients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(final_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_test[target[0]] = final_pred\nfinal_pred_df = reduce_test[keys + target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mode0(x): return pd.Series.mode(x)[0]\nmode1 = lambda x: pd.Series.mode(x)[1] if len(pd.Series.mode(x)) > 1 else pd.Series.mode(x)[0]\ndef last(x): return x.iloc[-1]\ndef first(x): return x.iloc[0]\nfinal_pred_df = final_pred_df.groupby('installation_id').agg({'accuracy_group': [np.max, mode0, np.min, last, first, mode1]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred_df.hist(figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = final_pred_df.iloc[:, 3].values.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sample_submission['accuracy_group'].value_counts(normalize=True))\nsample_submission['accuracy_group'].hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}