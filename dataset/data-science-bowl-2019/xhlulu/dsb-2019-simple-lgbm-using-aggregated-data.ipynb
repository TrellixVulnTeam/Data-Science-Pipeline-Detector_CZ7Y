{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this notebook\n\nYou might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting. The reason for that is there are many thousand different entries for each `installation_id`, each representing an `event`. This notebook simply gathers all the events into 17k groups, each group corresponds to an `installation_id`. Then, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each `installation_id`. After that, it simply fits a model on that dataset.\n\n## Updates\n\nV20:\n* Updated variable names for clarity.\n\nV17:\n* Removed statistics on event codes, since that created a lot of columns and LGBM seems to overfit on that information.\n\nV16:\n* Added mode of title `accuracy_group` (retrieved from training set) as a feature\n\nV10:\n* Fixed labelling problem. Before that, I was blindly predicting the target without even the title I was trying to assess ðŸ¤¦. I added that now by using the \"title\" column from `train_labels.csv`, and using the last row of each installation_id from `test.csv` to construct a `test_labels` dataframe.\n\nV8: \n* Added `cv_train`, a function that trains k-models on each of k-fold CV splits. Then, you can use function `cv_predict` to use the list of models to predict an output (and blend the results).\n* Added more summary statistics for `event_code` and `game_time`, including skewness of the distribution.\n\n## References\n\n* CV idea inspired from [this kernel](https://www.kaggle.com/tanreinama/ds-bowl-2019-simple-lgbm-aggregated-data-with-cv). Thank you!\n* Adding mode as a feature: https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport lightgbm as lgb\nimport scipy as sp\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', usecols=keep_cols)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_assess = test[test.type == 'Assessment'].copy()\ntest_labels = submission.copy()\ntest_labels['title'] = test_labels.installation_id.progress_apply(\n    lambda install_id: test_assess[test_assess.installation_id == install_id].iloc[-1].title\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group and Reduce"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_game_time_stats(group, col):\n    return group[\n        ['installation_id', col, 'event_count', 'game_time']\n    ].groupby(['installation_id', col]).agg(\n        [np.mean, np.sum, np.std]\n    ).reset_index().pivot(\n        columns=col,\n        index='installation_id'\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_and_reduce(df, df_labels):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    \n    # First only filter the useful part of the df\n    df = df[df.installation_id.isin(df_labels.installation_id.unique())]\n    \n    # group1 is am intermediary \"game session\" group,\n    # which are reduced to one record by game session. group_game_time takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    group_game_time = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    # group3, group4 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    title_group = (\n        pd.get_dummies(\n            group_game_time.drop(columns=['game_session', 'event_count', 'game_time']),\n            columns=['title', 'type', 'world'])\n        .groupby(['installation_id'])\n        .sum()\n    )\n\n    event_game_time_group = (\n        group_game_time[['installation_id', 'event_count', 'game_time']]\n        .groupby(['installation_id'])\n        .agg([np.sum, np.mean, np.std, np.min, np.max])\n    )\n    \n    # Additional stats on group1\n    world_time_stats = compute_game_time_stats(group_game_time, 'world')\n    type_time_stats = compute_game_time_stats(group_game_time, 'type')\n    \n    return (\n        title_group.join(event_game_time_group)\n        .join(world_time_stats)\n        .join(type_time_stats)\n        .fillna(0)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_small = group_and_reduce(train, train_labels)\ntest_small = group_and_reduce(test, test_labels)\n\nprint(train_small.shape)\ntrain_small.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding mode as feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_title_mode(train_labels):\n    titles = train_labels.title.unique()\n    title2mode = {}\n\n    for title in titles:\n        mode = (\n            train_labels[train_labels.title == title]\n            .accuracy_group\n            .value_counts()\n            .index[0]\n        )\n        title2mode[title] = mode\n    return title2mode\n\ndef add_title_mode(labels, title2mode):\n    labels['title_mode'] = labels.title.apply(lambda title: title2mode[title])\n    return labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title2mode = create_title_mode(train_labels)\ntrain_labels = add_title_mode(train_labels, title2mode)\ntest_labels = add_title_mode(test_labels, title2mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combine train/test labels with summary stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_train(train_labels, last_records_only=True):\n    \"\"\"\n    last_records_only (bool): Use only the last record of each user.\n    \"\"\"\n    final_train = pd.get_dummies(\n        (\n            train_labels.set_index('installation_id')\n            .drop(columns=['num_correct', 'num_incorrect', 'accuracy', 'game_session'])\n            .join(train_small)\n        ), \n        columns=['title']\n    )\n    \n    if last_records_only:\n        final_train = (\n            final_train\n            .reset_index()\n            .groupby('installation_id')\n            .apply(lambda x: x.iloc[-1])\n            .drop(columns='installation_id')\n        )\n    \n    return final_train\n\ndef preprocess_test(test_labels, test_small):\n    return pd.get_dummies(\n        test_labels.set_index('installation_id').join(test_small), columns=['title']\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train = preprocess_train(train_labels)\nprint(final_train.shape)\nfinal_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = preprocess_test(test_labels, test_small)\nprint(final_test.shape)\nfinal_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_train(X, y, cv, **kwargs):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    models = []\n    \n    kf = KFold(n_splits=cv, random_state=2019)\n    \n    for train, test in kf.split(X):\n        x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n        \n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], **kwargs)\n        models.append(model)\n        \n        if kwargs.get(\"verbose_eval\"):\n            print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    return models\n\ndef cv_predict(models, X):\n    return np.mean([model.predict(X) for model in models], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_train.drop(columns='accuracy_group').values\ny = final_train['accuracy_group'].values\n\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.9,\n    'feature_fraction': 0.2,\n    'max_height': 3,\n    'lambda_l1': 10,\n    'lambda_l2': 10,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\n\nmodels = cv_train(X, y, cv=20, params=params, num_boost_round=1000,\n                  early_stopping_rounds=100, verbose_eval=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = final_test.drop(columns=['accuracy_group'])\ntest_pred = cv_predict(models=models, X=X_test).argmax(axis=1)\n\nfinal_test['accuracy_group'] = test_pred\nfinal_test[['accuracy_group']].to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    lgb.plot_importance(model, max_num_features=15, height=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}