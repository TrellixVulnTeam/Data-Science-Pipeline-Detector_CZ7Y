{"cells":[{"metadata":{},"cell_type":"markdown","source":"### TabularLearner + LGBM with part 3\n\n\n**Additions on top of [previous model kernel](https://www.kaggle.com/keremt/fastai-model-part2-upgraded):**\n\n1) Features from [top scoring kernel](https://www.kaggle.com/braquino/convert-to-regression)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"### Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai.core import *\nPath.read_csv = lambda o: pd.read_csv(o)\ninput_path = Path(\"/kaggle/input/data-science-bowl-2019\") # kaggle\n# input_path = Path(\"data/\") # local\npd.options.display.max_columns=200\npd.options.display.max_rows=200\ninput_path.ls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# precomputed train df\ntrain_with_features_part3 = pd.read_feather(\"../input/dsbowlfengpart3/train_with_features_part3.fth\") #kaggle\n# train_with_features_part3 = pd.read_feather(\"output/dsbowl-feng-part3/train_with_features_part3.fth\") #local","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_subdf = (input_path/'sample_submission.csv').read_csv()\n# specs_df = (input_path/\"specs.csv\").read_csv()\n# train_labels_df = (input_path/\"train_labels.csv\").read_csv()\n# train_df = (input_path/\"train.csv\").read_csv()\ntest_df = (input_path/\"test.csv\").read_csv()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All unique values in test set are also present in training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for c in ['event_id', 'type', 'title', 'world', 'event_code']: print(c, set(test_df[c]).difference(set(train_df[c])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Feature Engineering\n\nTest set in LB and Private LB is different than what is publicly shared. So feature engineering and inference for test set should be done online."},{"metadata":{},"cell_type":"markdown","source":"### Test Features (part1)\n\nBasically here we redefine the feature generation code for test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nimport types\n\nstats = [\"median\",\"mean\",\"sum\",\"min\",\"max\"]\nUNIQUE_COL_VALS = pickle.load(open(\"../input/dsbowlfengpart3/UNIQUE_COL_VALS.pkl\", \"rb\")) #kaggle\n# UNIQUE_COL_VALS = pickle.load(open(\"output/dsbowl-feng-part3/UNIQUE_COL_VALS.pkl\", \"rb\")) #local","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in UNIQUE_COL_VALS.__dict__.keys(): print(k, len(UNIQUE_COL_VALS.__dict__[k]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def array_output(f):\n    def inner(*args, **kwargs): return array(listify(f(*args, **kwargs))).flatten()\n    return inner\n\nfeature_funcs = []\n\n@array_output\ndef time_elapsed_since_hist_begin(df):\n    \"total time passed until assessment begin\"\n    return df['timestampElapsed'].max() - df['timestampElapsed'].min()\n\nfeature_funcs.append(time_elapsed_since_hist_begin)\n\n@array_output\ndef time_elapsed_since_each(df, types, dfcol):\n    \"time since last occurence of each types, if type not seen then time since history begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    last_elapsed = df['timestampElapsed'].max()\n    _d = dict(df.iloc[:-1].groupby(dfcol)['timestampElapsed'].max())\n    return [last_elapsed - _d[t] if t in _d else time_elapsed_since_hist_begin(df)[0] for t in types]\n\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_codes\", dfcol=\"event_code\"))\n\n@array_output\ndef countfreqhist(df, types, dfcol, freq=False):\n    \"count or freq of types until assessment begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _d = dict(df[dfcol].value_counts(normalize=(True if freq else False)))\n    return [_d[t] if t in _d else 0 for t in types]\n\nfeature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=True))\n\n@array_output\ndef overall_event_count_stats(df):\n    \"overall event count stats until assessment begin\"\n    return df['event_count'].agg(stats)\nfeature_funcs.append(overall_event_count_stats)\n\n@array_output\ndef event_count_stats_each(df, types, dfcol):\n    \"event count stats per media types until assessment begin, all zeros if media type missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['event_count'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]\nfeature_funcs.append(partial(event_count_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_codes\", dfcol=\"event_code\"))\n\n@array_output\ndef overall_session_game_time_stats(df):\n    \"overall session game time stats until assessment begin\"\n    return df['game_time'].agg(stats)\nfeature_funcs.append(overall_session_game_time_stats)\n\n@array_output\ndef session_game_time_stats_each(df, types, dfcol):\n    \"session game time stats per media types until assessment begin, all zeros if missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['game_time'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_codes\", dfcol=\"event_code\"))\n\nlen(feature_funcs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_assessment_start_idxs(df): \n    return list(df.sort_values(\"timestamp\")\n                  .query(\"type == 'Assessment' & event_code == 2000\")\n                  .groupby(\"installation_id\").tail(1).index)\n\ndef get_sorted_user_df(df, ins_id):\n    \"extract sorted data for a given installation id and add datetime features\"\n    _df = df[df.installation_id == ins_id].sort_values(\"timestamp\").reset_index(drop=True)\n    add_datepart(_df, \"timestamp\", time=True)\n    return _df\n\ndef get_test_feats_row(idx, i):\n    \"get all faeatures by an installation start idx\"\n    df = test_df\n    ins_id = df.loc[idx, \"installation_id\"]\n    _df = get_sorted_user_df(df, ins_id)\n    assessment_row = _df.iloc[-1]\n    row_feats = np.concatenate([f(_df) for f in feature_funcs])\n    feat_row = pd.Series(row_feats, index=[f\"static_feat{i}\"for i in range(len(row_feats))])\n    row = pd.concat([assessment_row, feat_row])\n    return row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # testit with single assessment row\n# start_idxs = get_test_assessment_start_idxs(test_df)\n# row = get_test_feats_row(start_idxs[0], 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Feature Engineering part 1\nstart_idxs = get_test_assessment_start_idxs(test_df)\nres = parallel(partial(get_test_feats_row), start_idxs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = res[0].index\ntest_with_features_df = pd.DataFrame(np.vstack(res).tolist(), columns=colnames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del res; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_with_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Features (part2)\n\nTarget encoding features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_encoding_stats_dict(df, by, targetcol):\n    \"get target encoding stats dict, by:[stats]\"\n    _stats_df = df.groupby(by)[targetcol].agg(stats)   \n    _d = dict(zip(_stats_df.reset_index()[by].values, _stats_df.values))\n    return _d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _value_counts(o, freq=False): return dict(pd.value_counts(o, normalize=freq))\ndef countfreqhist_dict(df, by, targetcol, types, freq=False):\n    \"count or freq histogram dict for categorical targets\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _hist_df = df.groupby(by)[targetcol].agg(partial(_value_counts, freq=freq))\n    _d = dict(zip(_hist_df.index, _hist_df.values))\n    for k in _d: _d[k] = array([_d[k][t] for t in types]) \n    return _d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_incorrect\")\nf2 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_correct\")\nf3 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"accuracy\")\nf4 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_incorrect\")\nf5 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_correct\")\nf6 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"accuracy\")\nf7 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf8 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)\nf9 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf10 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering part 2\n_idxs = test_with_features_df.index\nfeat1 = np.stack(test_with_features_df['title'].map(f1(train_with_features_part3)).values)\nfeat2 = np.stack(test_with_features_df['title'].map(f2(train_with_features_part3)).values)\nfeat3 = np.stack(test_with_features_df['title'].map(f3(train_with_features_part3)).values)\nfeat4 = np.stack(test_with_features_df['world'].map(f4(train_with_features_part3)).values)\nfeat5 = np.stack(test_with_features_df['world'].map(f5(train_with_features_part3)).values)\nfeat6 = np.stack(test_with_features_df['world'].map(f6(train_with_features_part3)).values)\nfeat7 = np.stack(test_with_features_df['title'].map(f7(train_with_features_part3)).values)\nfeat8 = np.stack(test_with_features_df['title'].map(f8(train_with_features_part3)).values)\nfeat9 = np.stack(test_with_features_df['world'].map(f9(train_with_features_part3)).values)\nfeat10 = np.stack(test_with_features_df['world'].map(f10(train_with_features_part3)).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe with same index to merge later\n_test_feats_df = pd.DataFrame(np.hstack([feat1, feat2, feat3, feat4, feat5, feat6, feat7, feat8, feat9, feat10]).tolist(), index=_idxs)\n_test_feats_df.columns = [f\"targenc_feat{i}\"for i in range(_test_feats_df.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_with_features_df = pd.concat([test_with_features_df, _test_feats_df],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_with_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del _test_feats_df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check to see train and test have same features\nnum_test_feats = [c for c in test_with_features_df.columns if c.startswith(\"static\")]\nnum_train_feats = [c for c in train_with_features_part3.columns if c.startswith(\"static\")]\nassert num_train_feats == num_test_feats\n# check to see train and test have same features\nnum_test_feats = [c for c in test_with_features_df.columns if c.startswith(\"targenc\")]\nnum_train_feats = [c for c in train_with_features_part3.columns if c.startswith(\"targenc\")]\nassert num_train_feats == num_test_feats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Features (part3)\n\nTop scoring kernel features from https://www.kaggle.com/braquino/convert-to-regression. Only computing for test data otherwise kernel compute limit is exceeded."},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_pickle(obj,fn): pickle.dump(obj, open(fn, \"wb\"))\ndef load_pickle(fn): return pickle.load(open(fn, \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_map = load_pickle('../input/dsbowlfengpart3/activities_map.pkl') #kaggle\nactivities_world = load_pickle('../input/dsbowlfengpart3/activities_world.pkl') #kaggle\n\n# activities_map = load_pickle('output/dsbowl-feng-part3/activities_map.pkl') #local\n# activities_world = load_pickle('output/dsbowl-feng-part3/activities_world.pkl') #local","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(test):\n    # encode title    \n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    test['title'] = test['title'].map(activities_map)\n    test['title'] = test['title'].fillna(np.max(test['title'])+1).astype(int)\n    \n    test['world'] = test['world'].map(activities_world)\n    test['world'] = test['world'].fillna(np.max(test['world'])+1).astype(int)\n    \n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load same mappings used in creating train part3 data\n# kaggle\nassess_titles = load_pickle('../input/dsbowlfengpart3/assess_titles.pkl')\nlist_of_event_code = load_pickle('../input/dsbowlfengpart3/list_of_event_code.pkl')\nlist_of_event_id = load_pickle('../input/dsbowlfengpart3/list_of_event_id.pkl')\nactivities_labels = load_pickle('../input/dsbowlfengpart3/activities_labels.pkl')\nall_title_event_code = load_pickle('../input/dsbowlfengpart3/all_title_event_code.pkl')\nactivities_labels = load_pickle('../input/dsbowlfengpart3/activities_labels.pkl')\nwin_code = load_pickle('../input/dsbowlfengpart3/win_code.pkl')\n\n# assess_titles = load_pickle('output/dsbowl-feng-part3/assess_titles.pkl')\n# list_of_event_code = load_pickle('output/dsbowl-feng-part3/list_of_event_code.pkl')\n# list_of_event_id = load_pickle('output/dsbowl-feng-part3/list_of_event_id.pkl')\n# activities_labels = load_pickle('output/dsbowl-feng-part3/activities_labels.pkl')\n# all_title_event_code = load_pickle('output/dsbowl-feng-part3/all_title_event_code.pkl')\n# activities_labels = load_pickle('output/dsbowl-feng-part3/activities_labels.pkl')\n# win_code = load_pickle('output/dsbowl-feng-part3/win_code.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''    \n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # accuracies for each title in assess_titles\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n\n    # init counter dicts\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # last features\n    sessions_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels.get(session_title, None)\n                    \n            \n        # for each assessment, and only this kind of session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            _event_code = win_code.get(session_title, 4100)\n            all_attempts = session.query(f'event_code == {_event_code}')\n           \n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            # copy a dict to use as feature template, it's initialized with some items: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features['installation_session_count'] = sessions_count\n            \n            # also store game_session for merge with other datasets            \n            features['game_session'] = session['game_session'].values[0] \n            \n            variety_features = [('var_event_code', event_code_count),\n                               ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            if session_title_text:\n                last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0: features['accuracy_group'] = 0\n            elif accuracy == 1: features['accuracy_group'] = 3\n            elif accuracy == 0.5: features['accuracy_group'] = 2\n            else: features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set: all_assessments.append(features)\n            elif true_attempts+false_attempts > 0: all_assessments.append(features)\n                \n            counter += 1\n        \n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title': \n                        if k in activities_labels: x = activities_labels[k]\n                        else: x = None\n                    if (x in counter) and (x is not None): counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set: return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_and_test(test):\n    from tqdm import tqdm\n    compiled_test = []\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_test = pd.DataFrame(compiled_test)\n    return reduce_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode tet\ndel test_df; gc.collect()\ntest_df = (input_path/\"test.csv\").read_csv()\ntest = encode_title(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tranform function to get the train and test set\n_test_feats_df = get_train_and_test(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_test_feats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kernel feature columns\ntarget_cols, cat_cols, id_cols = ['accuracy', 'accuracy_group'], ['session_title'], ['installation_id', 'game_session']\nfeature_cols = [c for c in _test_feats_df.columns if c not in (target_cols + cat_cols + id_cols)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols[-10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extra column for identification or category\nextra_cols = target_cols + cat_cols + id_cols\ntest_extra_cols = [c for c in extra_cols if 'accuracy' not in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create feng part 3 test data\n_test_kernel_feats_df = _test_feats_df[feature_cols]\n_test_kernel_feats_df.columns = [f\"kernel_feat{i}\" for i in range(len(_test_kernel_feats_df.columns))]\n_test_kernel_feats_df[test_extra_cols] = _test_feats_df[test_extra_cols];\n\n# merge new features to test features df\ntest_with_features_df = test_with_features_df.merge(_test_kernel_feats_df, on=['installation_id', 'game_session'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del _test_feats_df, _test_kernel_feats_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_with_features_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part3.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len([c for c in test_with_features_df.columns if \"kernel\" in c]) == len([c for c in train_with_features_part3.columns if \"kernel\" in c])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_names = ['title','world','timestampMonth','timestampWeek','timestampDay','timestampDayofweek',\n             'timestampDayofyear','timestampHour'] + ['session_title']\ncont_names = [c for c in train_with_features_part3.columns if c.startswith(\"static\")]\ncont_names += [c for c in train_with_features_part3.columns if c.startswith(\"targenc\")]\ncont_names += [c for c in train_with_features_part3.columns if c.startswith(\"kernel\")]\n# cont_names += [c for c in train_with_features_part3.columns if c.startswith(\"kernel\")][-7:] # accuracy feats can't be used","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = cont_names + cat_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check train and test data\n\nCheck distributions, correlations, etc. between training and test set features."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\n# drop columns with 1 unique value in train\nfor c in feature_names:\n    if len(np.unique(train_with_features_part3[c])) == 1:\n        drop_cols.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns that have single value in all training data\ntrain_with_features_part3 = train_with_features_part3.drop(drop_cols, 1)\ntest_with_features_df = test_with_features_df.drop(drop_cols, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part3.shape, test_with_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check nan values\nassert not any(test_with_features_df.isna().sum() > 0)\nassert not any(train_with_features_part3.isna().sum() > 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_names = ['title','world','timestampMonth','timestampWeek','timestampDay','timestampDayofweek',\n             'timestampDayofyear','timestampHour'] + ['session_title']\ncont_names = [c for c in train_with_features_part3.columns if c.startswith(\"static\")]\ncont_names += [c for c in train_with_features_part3.columns if c.startswith(\"targenc\")]\ncont_names += [c for c in train_with_features_part3.columns if c.startswith(\"kernel\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cat_names), len(cont_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dimensionality Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for c in cont_names: if not any(train_with_features_part3[c] != train_with_features_part3['accuracy_group']): print(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TabularLearner Model\n\nHere we use a single validation but in later stages once we finalize features we should use cross-validation. We don't over optimize the model or do any hyperparameter search since the whole purpose is to get a baseline and build on top of it in upcoming parts."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nfrom fastai.callbacks import *\nfrom fastai.metrics import RegMetrics\nfrom sklearn.metrics import cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_with_features_df.shape, train_with_features_part3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(train_with_features_part3.columns).difference(set(test_with_features_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load CV installation_ids\ntrn_val_ids = pickle.load(open(\"../input/dsbowlfengpart3/CV_installation_ids.pkl\", \"rb\")) #kaggle\n# trn_val_ids = pickle.load(open(\"output/dsbowl-feng-part3/CV_installation_ids.pkl\", \"rb\")) #local","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label distribution for training fold to be used in metric\ntrain_labels_dist = (train_with_features_part3['accuracy_group'].value_counts(normalize=True))\nq = np.cumsum([train_labels_dist[i] for i in range(3)]); q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# metric\ndef convert_preds_with_search(preds, targs): \n    \"soft accuracy 0-1 preds to accuracy groups by optimized search thresholds\"\n    pass\n\ndef convert_preds(preds, q):\n    \"soft accuracy 0-1 preds to accuracy groups given quantiles 'q'\"\n    preds = preds.view(-1)\n    targ_thresh = np.quantile(preds, q)\n    hard_preds = torch.zeros_like(preds)\n    hard_preds[preds <= targ_thresh[0]] = 0\n    hard_preds[(preds > targ_thresh[0]) & (preds <= targ_thresh[1])] = 1\n    hard_preds[(preds > targ_thresh[1]) & (preds <= targ_thresh[2])] = 2\n    hard_preds[preds > targ_thresh[2]] = 3\n    return hard_preds\n\ndef convert_targs(targs):\n    \"convert accuracy to accuracy group for targs\"\n    targs = targs.view(-1)\n    hard_targs = torch.zeros_like(targs)\n    hard_targs[targs == 0] = 0\n    hard_targs[(targs > 0) & (targs < 0.5)] = 1\n    hard_targs[targs == 0.5] = 2\n    hard_targs[targs == 1] = 3\n    return hard_targs\n# assert not any(convert_targs(tensor(train_labels_df['accuracy'])) != tensor(train_labels_df['accuracy_group']))\n\nclass KappaScoreRegression(RegMetrics):\n    \"uses accuracy as target\"\n    def __init__(self): pass\n    def on_epoch_end(self, last_metrics, **kwargs):\n        preds = convert_preds(self.preds, q=q)\n        targs = convert_targs(self.targs)\n        qwk = cohen_kappa_score(preds, targs, weights=\"quadratic\")\n        return add_metrics(last_metrics, qwk)\n    \nclass KappaScoreRegressionv2(RegMetrics):\n    \"uses accuracy_group as target\"\n    def __init__(self): pass\n    def on_epoch_end(self, last_metrics, **kwargs):\n        \"convert preds and calc qwk\"\n        preds = convert_preds(self.preds, q=q)\n        targs = self.targs\n        qwk = cohen_kappa_score(preds, targs, weights=\"quadratic\")\n        return add_metrics(last_metrics, qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_learner_data(foldidx, bs=64, dep_var=\"accuracy\"):\n    # pick trn-val installation ids\n    trn_ids, val_ids = trn_val_ids[foldidx]\n    train_idx = (train_with_features_part3[train_with_features_part3.installation_id.isin(trn_ids)].index)\n    valid_idx = (train_with_features_part3[train_with_features_part3.installation_id.isin(val_ids)].index)\n\n    # get data\n    procs = [FillMissing, Categorify, Normalize]\n    data = TabularDataBunch.from_df(bs=bs, path=\".\", df=train_with_features_part3, dep_var=dep_var, \n                                    valid_idx=valid_idx, procs=procs, cat_names=cat_names, cont_names=cont_names)\n\n    data.add_test(TabularList.from_df(test_with_features_df, cat_names=cat_names, cont_names=cont_names));\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_learner(data):\n    learner = tabular_learner(data, [256,256], y_range=(0.,1), ps=0.5)\n    early_cb = EarlyStoppingCallback(learner, monitor=\"kappa_score_regression\", mode=\"max\", patience=5)\n    save_cb = SaveModelCallback(learner, monitor=\"kappa_score_regression\", mode=\"max\", name=f\"bestmodel\")\n    cbs = [early_cb, save_cb]\n    learner.metrics = [KappaScoreRegression()]\n    learner.fit_one_cycle(10, 1e-3, callbacks=cbs)\n    return learner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nlearner_preds = 0\nlearner_cv_scores = [] \nfor i in range(n_folds):\n    data = get_learner_data(i)\n    learner = fit_learner(data)\n    _learner_preds, _ = learner.get_preds(DatasetType.Test)\n    learner_preds += to_np(_learner_preds.view(-1)) / n_folds\n    learner_cv_scores.append(learner.validate()[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(learner_cv_scores), np.std(learner_cv_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(learner_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part3[cat_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# need to encode string type for xgboost\ntitle2codes = {v:k for k,v in enumerate(np.unique(train_with_features_part3['title']))}\nworld2codes = {v:k for k,v in enumerate(np.unique(train_with_features_part3['world']))}\ntrain_with_features_part3['title'] = train_with_features_part3['title'].map(title2codes)\ntrain_with_features_part3['world'] = train_with_features_part3['world'].map(world2codes)\ntest_with_features_df['title'] = test_with_features_df['title'].map(title2codes)\ntest_with_features_df['world'] = test_with_features_df['world'].map(world2codes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_preds_np(preds, q):\n    \"soft accuracy 0-1 preds to accuracy groups given quantiles 'q'\"\n    preds = preds.flatten()\n    targ_thresh = np.quantile(preds, q)\n    hard_preds = np.zeros_like(preds)\n    hard_preds[preds <= targ_thresh[0]] = 0\n    hard_preds[(preds > targ_thresh[0]) & (preds <= targ_thresh[1])] = 1\n    hard_preds[(preds > targ_thresh[1]) & (preds <= targ_thresh[2])] = 2\n    hard_preds[preds > targ_thresh[2]] = 3\n    return hard_preds\n\ndef convert_targs_np(targs):\n    \"convert accuracy to accuracy group for targs\"\n    targs = targs.flatten()\n    hard_targs = np.zeros_like(targs)\n    hard_targs[targs == 0] = 0\n    hard_targs[(targs > 0) & (targs < 0.5)] = 1\n    hard_targs[targs == 0.5] = 2\n    hard_targs[targs == 1] = 3\n    return hard_targs\n    \ndef kappa_score_regression(preds, targs):\n    preds = convert_preds_np(preds, q=q)\n    targs = convert_targs_np(targs.get_label())\n    qwk = cohen_kappa_score(preds, targs, weights=\"quadratic\")\n    return \"kappa_regression\", qwk, True\n\ndef kappa_score_regression_v2(preds, targs):\n    preds = convert_preds_np(preds, q=q)\n    targs = targs.get_label().flatten()\n    qwk = cohen_kappa_score(preds, targs, weights=\"quadratic\")\n    return \"kappa_regression\", qwk, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = cat_names + cont_names\ndep_var = \"accuracy\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lgb_data(foldidx):\n    trn_ids, val_ids = trn_val_ids[foldidx]\n    train_idx = (train_with_features_part3[train_with_features_part3.installation_id.isin(trn_ids)].index)\n    valid_idx = (train_with_features_part3[train_with_features_part3.installation_id.isin(val_ids)].index)\n    x_train = train_with_features_part3.loc[train_idx, features]\n    y_train = train_with_features_part3.loc[train_idx, dep_var]\n    x_val = train_with_features_part3.loc[valid_idx, features]\n    y_val = train_with_features_part3.loc[valid_idx, dep_var]\n    train_set = lgb.Dataset(x_train, y_train, categorical_feature=cat_names)\n    val_set = lgb.Dataset(x_val, y_val, categorical_feature=cat_names)\n    test_set = lgb.Dataset(test_with_features_df[features])\n    return train_set, val_set, test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_lgb_model(train_set, val_set, test_set):\n    model = lgb.train(params, train_set, valid_sets=[train_set, val_set], feval=kappa_score_regression, verbose_eval=100)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':5000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.01,\n            'feature_fraction': 0.9,\n            'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nlgb_preds = 0\nlgb_cv_scores = [] \nfor i in range(n_folds):\n    data = get_lgb_data(i)\n    lgb_model = fit_lgb_model(*data)\n    _lgb_preds = lgb_model.predict(test_with_features_df[features])\n    lgb_preds += _lgb_preds / n_folds\n    score = lgb_model.best_score['valid_1']['kappa_regression']\n    lgb_cv_scores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(lgb_cv_scores), np.std(lgb_cv_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine preds\n# _preds = learner_preds*0.3 + lgb_preds*0.7\n_preds = learner_preds*0.3 + lgb_preds*0.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label distribution for training fold to be used in metric\ntrain_labels_dist = (train_with_features_part3['accuracy_group'].value_counts(normalize=True))\nq = np.cumsum([train_labels_dist[i] for i in range(3)])\n# get accuracy groups\npreds = convert_preds_np(_preds, q=q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get installation ids for test set\ntest_ids = test_with_features_df['installation_id'].values; len(test_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate installation_id : pred dict\ntest_preds_dict = dict(zip(test_ids, preds)); len(test_preds_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create submission\nsample_subdf = (input_path/'sample_submission.csv').read_csv()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_subdf['accuracy_group'] = sample_subdf.installation_id.map(test_preds_dict).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_subdf.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_subdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### end"}],"metadata":{"kernelspec":{"display_name":"Python [conda env:dsbowl2019]","language":"python","name":"conda-env-dsbowl2019-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}