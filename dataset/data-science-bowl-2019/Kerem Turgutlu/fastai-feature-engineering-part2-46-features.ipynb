{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering with part 2\n\nIn this notebook we implement cross validated target encoding features.\n\n**To go back to previous modeling notebook:** [Part 1 Modeling Notebook](https://www.kaggle.com/keremt/fastai-model-part1-regression/)\n\n**To skip and go to next modeling notebook:** [Part 2 Modeling Notebook](https://www.kaggle.com/keremt/fastai-model-part2-regression/)"},{"metadata":{},"cell_type":"markdown","source":"### Imports\n\nWe will use fastai v1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.core import *\nPath.read_csv = lambda o: pd.read_csv(o)\ninput_path = Path(\"/kaggle/input/data-science-bowl-2019\")\npd.options.display.max_columns=200\npd.options.display.max_rows=200\ninput_path.ls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_subdf = (input_path/'sample_submission.csv').read_csv()\nspecs_df = (input_path/\"specs.csv\").read_csv()\ntrain_df = (input_path/\"train.csv\").read_csv()\ntrain_labels_df = (input_path/\"train_labels.csv\").read_csv()\ntest_df = (input_path/\"test.csv\").read_csv()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert set(train_df.installation_id).intersection(set(test_df.installation_id)) == set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load part 1 data too"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part1 = pd.read_feather(\"../input/dsbowl2019-feng-part1/train_with_features_part1.fth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part1.shape, test_df.shape, train_labels_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there shouldn't be any common installation ids between test and train \nassert set(train_df.installation_id).intersection(set(test_df.installation_id)) == set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train (also train labels) and test doesn't have any common installation ids. This means that we can't use past assessment target information for a given user, e.g. we can't use how a child did in his/her previous to predict for future assessment results. \n\nInstead we need to create global features from target information, e.g. using categorical target encoding and similar techniques."},{"metadata":{},"cell_type":"markdown","source":"### Cross Validated Target Encoding\n\nWe will use cv based target encoding for using label information. For more information see really nice documentation here in [h2o.ai](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html#holdout-type). For these calculations we will use `train_with_features_part1` and calculate stats on different target information by different categorical data. Note that we will not use `event_ids` since it has a 1:1 mapping `titles` adding no extra information, `media_types` since always `Assesment` and `event_codes` since it's always `2000`. \n\n**Disclaimer:** Target encoding on the following categories can be not that effective since their cardinality is low.\n\nValue: num_correct x num_incorrect x accuracy x hist of accuracy group\n\nBy: Title x World \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nimport types\n\nstats = [\"median\",\"mean\",\"sum\",\"min\",\"max\"]\nUNIQUE_COL_VALS = pickle.load(open(\"../input/dsbowl2019-feng-part1/UNIQUE_COL_VALS.pkl\", \"rb\"))\nlist(UNIQUE_COL_VALS.__dict__.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add accuracy_group unique vals\nUNIQUE_COL_VALS.__dict__['accuracy_groups'] = np.unique(train_with_features_part1.accuracy_group)\nUNIQUE_COL_VALS.accuracy_groups\npickle.dump(UNIQUE_COL_VALS, open( \"UNIQUE_COL_VALS.pkl\", \"wb\" ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_encoding_stats_dict(df, by, targetcol):\n    \"get target encoding stats dict, by:[stats]\"\n    _stats_df = df.groupby(by)[targetcol].agg(stats)   \n    _d = dict(zip(_stats_df.reset_index()[by].values, _stats_df.values))\n    return _d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _value_counts(o, freq=False): return dict(pd.value_counts(o, normalize=freq))\ndef countfreqhist_dict(df, by, targetcol, types, freq=False):\n    \"count or freq histogram dict for categorical targets\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _hist_df = df.groupby(by)[targetcol].agg(partial(_value_counts, freq=freq))\n    _d = dict(zip(_hist_df.index, _hist_df.values))\n    for k in _d: _d[k] = array([_d[k][t] for t in types]) \n    return _d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countfreqhist_dict(train_with_features_part1, \"title\", \"accuracy_group\", \"accuracy_groups\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_incorrect\")\nf2 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_correct\")\nf3 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"accuracy\")\nf4 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_incorrect\")\nf5 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_correct\")\nf6 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"accuracy\")\n\nf7 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf8 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)\nf9 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf10 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compute Features for Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n# create cross-validated indexes\nunique_ins_ids = np.unique(train_with_features_part1.installation_id)\ntrain_val_idxs = KFold(5, random_state=42).split(unique_ins_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_dfs = [] # collect computed _val_feats_dfs here\nfor train_idxs, val_idxs  in train_val_idxs:\n    # get train and val dfs\n    train_ins_ids, val_ins_ids = unique_ins_ids[train_idxs], unique_ins_ids[val_idxs]\n    _train_df = train_with_features_part1[train_with_features_part1.installation_id.isin(train_ins_ids)]\n    _val_df = train_with_features_part1[train_with_features_part1.installation_id.isin(val_ins_ids)]\n    assert (_train_df.shape[0] + _val_df.shape[0]) == train_with_features_part1.shape[0]\n    # compute features for val df\n    _idxs = _val_df['title'].map(f1(_train_df)).index\n    feat1 = np.stack(_val_df['title'].map(f1(_train_df)).values)\n    feat2 = np.stack(_val_df['title'].map(f2(_train_df)).values)\n    feat3 = np.stack(_val_df['title'].map(f3(_train_df)).values)\n    feat4 = np.stack(_val_df['world'].map(f4(_train_df)).values)\n    feat5 = np.stack(_val_df['world'].map(f5(_train_df)).values)\n    feat6 = np.stack(_val_df['world'].map(f6(_train_df)).values)\n    feat7 = np.stack(_val_df['title'].map(f7(_train_df)).values)\n    feat8 = np.stack(_val_df['title'].map(f8(_train_df)).values)\n    feat9 = np.stack(_val_df['world'].map(f9(_train_df)).values)\n    feat10 = np.stack(_val_df['world'].map(f10(_train_df)).values)\n    # create dataframe with same index for later merge\n    _val_feats = np.hstack([feat1, feat2, feat3, feat4, feat5, feat6, feat7, feat8, feat9, feat10])\n    _val_feats_df = pd.DataFrame(_val_feats, index=_idxs)\n    _val_feats_df.columns = [f\"targenc_feat{i}\"for i in range(_val_feats_df.shape[1])]\n    feature_dfs.append(_val_feats_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature_df = pd.concat(feature_dfs, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part2 = pd.concat([train_with_features_part1, train_feature_df],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_features_part2.to_feather(\"train_with_features_part2.fth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### end"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}