{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here is my starter codes, mainly dealing with feature engineering and without much parameter tuning. Current LB is around 0.485. \n\nThe features I used here include the statistics on event codes, time spent on each event/type/world, as well as the accuracy information of the previous plays. \n\nOne thing I want to model is that I want to define the \"session\" of each play. This \"session\" is not the same concept as the game_session. For example, a player might use this app from 4pm to 6pm, he might take many game sessions during this period but I want to define 4pm to 6pm as one session of his behavior. I think the reasonability is that if he has already played for a long time, even if he is talented on assessments, he is also very likely to quite and has a label '0'.\n\nI currently use XGBoost Regressor and only use the default settings. If there are any suggestions for my codes, I will be very glad to hear for your advice!"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\npd.options.display.max_colwidth = 500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Read the data."},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\")\ntrain_labels = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/sample_submission.csv\")\nspecs = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/specs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.timestamp = pd.to_datetime(train.timestamp)\ntest.timestamp = pd.to_datetime(test.timestamp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove all users that haven't taken any assessments.\nassessment_user = train[train.type == 'Assessment'].installation_id.unique()\ntrain = train[train.installation_id.isin(assessment_user)]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Train Labels Verification."},{"metadata":{"trusted":false},"cell_type":"code","source":"# This is not that relevant to predicting. The code here is only used to help to make sure my understanding of how to calculate lables is consistant with the competition holders!\ndef calculate_accuracy(row):\n    # print(row.game_session)\n    game_records = train[train.game_session == row.game_session]\n    if row.title == 'Bird Measurer (Assessment)':\n        attempts_df = game_records[game_records.event_code==4110]\n    else:\n        attempts_df = game_records[game_records.event_code==4100]\n    attempts_df['is_correct'] = attempts_df.event_data.str.contains('\"correct\":true')\n    num_correct = np.sum(attempts_df['is_correct'])\n    num_incorrect = attempts_df.shape[0] - num_correct\n    if num_correct == 0:\n        accuracy_group = 0\n    elif num_incorrect == 0:\n        accuracy_group = 3\n    elif num_incorrect == 1:\n        accuracy_group = 2\n    else:\n        accuracy_group = 1\n    return pd.Series([num_correct, num_incorrect, accuracy_group])\n        \nrandom_train_labels = train_labels.sample(n=10, random_state=1)\nrandom_train_labels[['my_correct', 'my_incorrect', 'my_accuracy_group']] = random_train_labels.apply(calculate_accuracy, axis=1)\nrandom_train_labels['correct_difference'] = random_train_labels.num_correct - random_train_labels.my_correct\nrandom_train_labels['group_diffrence'] = random_train_labels.accuracy_group - random_train_labels.my_accuracy_group\n# random_train_labels.head(500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Test Label Construction."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Only the last assessment in the test set should be used to do the prediction. \ntest.timestamp = pd.to_datetime(test.timestamp)\ntest_labels = test[['game_session', 'installation_id', 'title', 'timestamp']].drop_duplicates(subset='installation_id', keep='last')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Feature Calculation."},{"metadata":{},"cell_type":"markdown","source":"#### 4.0 Preparation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use train_labels to construct training set.\ntrain_labels = train_labels.merge(train[['game_session', 'timestamp', 'installation_id']].drop_duplicates( \\\n                                        subset=['game_session', 'installation_id']), how='left', \\\n                                 on=['game_session', 'installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clip_names = list(train[train.type=='Clip'].title.unique())\nactivity_names = list(train[train.type=='Activity'].title.unique())\ngame_names = list(train[train.type=='Game'].title.unique())\nassessment_names = list(train[train.type=='Assessment'].title.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"event_codes_for_game = list(train[train.type=='Game'].event_code.unique())\nevent_codes_for_assessment = list(train[train.type=='Assessment'].event_code.unique())\nevent_codes_for_activity = list(train[train.type=='Activity'].event_code.unique())\nevent_codes_for_game_uni_assessment_uni_activity = list(set(event_codes_for_game).union(set(event_codes_for_assessment)).union(set(event_codes_for_activity)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"type_names = ['Game', 'Activity', 'Assessment', 'Clip']\nworld_names = ['NONE', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Feature Names: Games"},{"metadata":{"trusted":false},"cell_type":"code","source":"game_event_code = train[train['type']=='Game'][['title', 'event_code']].drop_duplicates()\ngame_event_code.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"game_features_by_name = list() # For each of the game, calculate the different counts for each event code for each game.\nfor index, row in game_event_code.iterrows():\n    game_features_by_name.append(row.title + ' ' + str(row.event_code))\n# game_features_by_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"effective_game_codes = [2000, 2020, 2030, 3010, 3020, 3021, 4020, 4070, 4090]\nsummary_game_features = ['Num Give Up', 'Num Total', 'Give Up Rate', 'Mean Accuracy', 'Std Accuracy', 'Max Accuracy', 'Min Accuracy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"game_feature_1_names = ['Total Count (Game) ' + str(code) for code in effective_game_codes]\ngame_feature_2_names = ['Overall Game ' + name for name in summary_game_features]\ngame_feature_3_names = ['Current Group Game' + name for name in summary_game_features]\ngame_feature_4_names = ['Last 3 Group Game' + name for name in summary_game_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Feature Names: Assessments"},{"metadata":{"trusted":false},"cell_type":"code","source":"assessment_event_code = train[train['type']=='Assessment'][['title', 'event_code']].drop_duplicates()\nassessment_event_code.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"assessment_features_by_name = list() # For each of the game, calculate the different counts for each event code for each game.\nfor index, row in assessment_event_code.iterrows():\n    assessment_features_by_name.append(row.title + ' ' + str(row.event_code))\n# assessment_features_by_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"effective_assessment_codes = event_codes_for_assessment\nsummary_assessment_features = ['Num Give Up', 'Num Total', 'Give Up Rate', 'Mean Accuracy', 'Std Accuracy', 'Max Accuracy', 'Min Accuracy']\n\nassessment_feature_1_names = ['Total Count (Ass) ' + str(code) for code in effective_assessment_codes]\nassessment_feature_2_names = ['Overall Ass ' + name for name in summary_assessment_features]\nassessment_feature_3_names = ['Current Group Ass ' + name for name in summary_assessment_features]\nassessment_feature_4_names = ['Last 3 Group Ass ' + name for name in summary_assessment_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Feature Names: Activites"},{"metadata":{"trusted":false},"cell_type":"code","source":"activity_event_code = train[train['type']=='Activity'][['title', 'event_code']].drop_duplicates()\nactivity_event_code.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"activity_features_by_name = list() # For each of the game, calculate the different counts for each event code for each game.\nfor index, row in activity_event_code.iterrows():\n    activity_features_by_name.append(row.title + ' ' + str(row.event_code))\n# activity_features_by_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"effective_activity_codes = event_codes_for_activity\n\nactivity_feature_1_names = ['Total Count (Act) ' + str(code) for code in effective_activity_codes]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.4 Feature Names: Clips"},{"metadata":{"trusted":false},"cell_type":"code","source":"clip_length = {'Welcome to Lost Lagoon!': 19, 'Magma Peak - Level 1': 20, \n              'Slop Problem': 60, 'Tree Top City - Level 1': 17, 'Ordering Spheres': 61, \n              'Costume Box': 61, '12 Monkeys': 109, 'Tree Top City - Level 2': 25,\n              \"Pirate's Tale\": 80, 'Treasure Map': 156, 'Tree Top City - Level 3': 26,\n              'Rulers': 126, 'Magma Peak - Level 2': 22, 'Crystal Caves - Level 1': 18,\n              'Balancing Act': 72, 'Crystal Caves - Level 2': 24, 'Crystal Caves - Level 3': 19,\n              'Lifting Heavy Things': 118, 'Honey Cake':  142, 'Heavy, Heavier, Heaviest': 61}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.5 Feature Names: Time"},{"metadata":{"trusted":false},"cell_type":"code","source":"timeFeatureName_type = [name + ' Time Total' for name in type_names]\ntimeFeatureName_world = [name + ' Time Total' for name in world_names]\ntimeFeatureName_groupstat = ['Group Time Mean', 'Group Time Std', 'Group Time Max', 'Group Time Min', 'Current Elapsed']\ntimeFeatureName_title = [name + ' Time Total' for name in activity_names + assessment_names + game_names]\ntimeFeatureName_totaltime = ['Total Time']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.6 All Feature Names"},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_names = ['Have Records'] + game_features_by_name + assessment_features_by_name + activity_features_by_name + \\\n               timeFeatureName_type + timeFeatureName_world + timeFeatureName_groupstat +  \\\n               timeFeatureName_title + timeFeatureName_totaltime + game_feature_1_names + game_feature_2_names + game_feature_3_names + game_feature_4_names + \\\n               assessment_feature_1_names + assessment_feature_2_names + assessment_feature_3_names + assessment_feature_4_names + \\\n               clip_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Feature Calculation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# row = train_labels.iloc[3].squeeze()\n\ndef calculate_features(row):\n    # print(row.game_session)\n    # Step 0: Filter all relevant records.\n    if row['isTrain'] == 1:\n        all_user_records = train[train.installation_id == row.installation_id]\n    else:\n        all_user_records = test[test.installation_id == row.installation_id]\n\n    all_records = all_user_records[all_user_records.timestamp < row.timestamp]\n    \n    if all_records.shape[0] > 0:\n\n        # Step 1: Filter records according to types.\n        game_records = all_records[all_records.type == 'Game']\n        assessment_records = all_records[all_records.type == 'Assessment']\n        activity_records = all_records[all_records.type == 'Activity']\n        clip_records = all_records[all_records.type == 'Clip']\n\n        # Step 2: Game Records and Assessment Records Processing.\n\n        # Step 2.1: Features by Game name or Assessment name.\n        game_feature_values_by_name = [0] * len(game_features_by_name)\n        assessment_feature_values_by_name = [0] * len(assessment_features_by_name)\n        activity_feature_values_by_name = [0] * len(activity_features_by_name)\n\n        game_unstack_by_name = game_records.groupby(by=['title', 'event_code']).size().unstack().fillna(0)\n        assessment_unstack_by_name = assessment_records.groupby(by=['title', 'event_code']).size().unstack().fillna(0)\n        activity_unstack_by_name = activity_records.groupby(by=['title', 'event_code']).size().unstack().fillna(0)\n\n        for idx, row_ in game_unstack_by_name.iterrows():\n            for column in game_unstack_by_name.columns:\n                try:\n                    game_feature_values_by_name[game_features_by_name.index(idx + ' ' + str(column))] = row_[column]\n                except:\n                    continue\n\n        for idx, row_ in assessment_unstack_by_name.iterrows():\n            for column in assessment_unstack_by_name.columns:\n                try:\n                    assessment_feature_values_by_name[assessment_features_by_name.index(idx + ' ' + str(column))] = row_[column]\n                except:\n                    continue\n\n        for idx, row_ in activity_unstack_by_name.iterrows():\n            for column in activity_unstack_by_name.columns:\n                try:\n                    activity_feature_values_by_name[activity_features_by_name.index(idx + ' ' + str(column))] = row_[column]\n                except:\n                    continue\n\n        # Step 2.2: Features based on the time sequence.\n\n        # Step 2.2.1: Produce a summary table based on game session.\n        all_records = all_records.sort_values(by='timestamp') # To guarantee that all the records are within the time sequence.\n        all_session_id = all_records.game_session.unique()\n        session_start_df = all_records[['game_session', 'timestamp', 'title', 'type', 'world']].drop_duplicates(subset=['game_session'], keep='first')\n        session_end_df = all_records[['game_session', 'timestamp']].drop_duplicates(subset=['game_session'], keep='last')\n        session_statistics = session_start_df.merge(session_end_df, on='game_session', how='left')\n\n        def calculate_time_spent(row):\n            if row['type'] == 'Clip':\n                return clip_length[row['title']]\n            else:\n                return pd.Timedelta(row.timestamp_y - row.timestamp_x).seconds\n\n        session_statistics['time_spent'] = session_statistics.apply(calculate_time_spent, axis=1)\n\n        # Step 2.2.2: Divide the sessions into different groups.\n        threshold = 600 # If the gap between two sessions are more than 10 min, then there are in diffrent groups.\n        current_group = 1\n        session_statistics['group'] = 0\n        for idx, row_ in session_statistics.iterrows():\n            if idx == 0:\n                session_statistics.at[idx, 'group'] = 1\n            else:\n                if pd.Timedelta(row_['timestamp_x'] - session_statistics.iloc[idx-1]['timestamp_y']).seconds > threshold:\n                    current_group += 1\n                session_statistics.at[idx, 'group'] = current_group\n\n        # Step 2.2.3: Event Code Summarization by game session.\n        summarization_df = all_records.groupby(by=['game_session', 'event_code']).size().unstack()\n        other_columns = list(set(event_codes_for_game_uni_assessment_uni_activity).difference(set(all_records.groupby(by=['game_session', 'event_code']).size().unstack().columns)))\n        for col in other_columns:\n            summarization_df[col] = np.NaN\n        summarization_df.fillna(0, inplace=True)\n        session_statistics = session_statistics.merge(summarization_df, on='game_session', how='left')\n\n        def calculate_accuracy(row):\n            if row.type == 'Game' or row.type == 'Assessment':\n                if row.type == 'Game':\n                    num_correct = row[3021]\n                    num_incorrect = row[3020]\n                else:\n                    if row.title == 'Bird Measurer (Assessment)':\n                        attempts_df = all_records[(all_records.game_session == row.game_session) & (all_records.event_code==4110)]\n                    else:\n                        attempts_df = all_records[(all_records.game_session == row.game_session) & (all_records.event_code==4100)]\n                    attempts_df['is_correct'] = attempts_df.event_data.str.contains('\"correct\":true')\n                    num_correct = np.sum(attempts_df['is_correct'])\n                    num_incorrect = attempts_df.shape[0] - num_correct\n                if num_correct + num_incorrect == 0:\n                    accuracy = 0\n                else:\n                    accuracy = num_correct / (num_correct + num_incorrect)\n            else:\n                num_correct = np.NaN\n                num_incorrect = np.NaN\n                accuracy = np.NaN\n            return pd.Series([num_correct, num_incorrect, accuracy])\n        session_statistics[['num_correct', 'num_incorrect', 'accuracy']] = session_statistics.apply(calculate_accuracy, axis=1)   \n\n        # Step 2.2.4: Features w.r.t Time.\n        def my_squeeze(df, index_name):\n            s = df.squeeze()\n            if not isinstance(s, pd.Series):\n                s = pd.Series([s], index=[df.index[0]])\n                s = s.rename_axis('group')\n            return s\n\n\n        # Time Spent on each type.\n        timeFeatureValue_type = [0] * len([name + ' Time Total' for name in type_names])\n        time_type_Series = my_squeeze(session_statistics[['type', 'time_spent']].groupby('type').sum(), 'type')\n        for idx in time_type_Series.index:\n            timeFeatureValue_type[type_names.index(idx)] = time_type_Series.loc[idx]\n\n        # Time Spent on each world.\n        timeFeatureValue_world = [0] * len([name + ' Time Total' for name in world_names])\n        time_world_Series = my_squeeze(session_statistics[['world', 'time_spent']].groupby('world').sum(), 'world')\n        for idx in time_world_Series.index:\n            timeFeatureValue_world[world_names.index(idx)] = time_world_Series.loc[idx]\n\n        # Time Spent on each title. \n        timeFeatureValue_title = [0] * len(timeFeatureName_title)\n        time_title_Series = my_squeeze(session_statistics[['title', 'time_spent']].groupby('title').sum(), 'title')\n        for idx in time_title_Series.index:\n            try:\n                timeFeatureValue_title[timeFeatureName_title.index(idx+' Time Total')] = time_title_Series.loc[idx]\n            except:\n                pass\n\n        # Time Spent on each group.\n        is_label_new_group = False\n        time_group_Series = my_squeeze(session_statistics[['group', 'time_spent']].groupby('group').sum(), 'group')\n\n\n        if pd.Timedelta(row.timestamp - session_statistics.iloc[-1].timestamp_y).seconds > threshold:\n            is_label_new_group = True # The assessment is based on a new group.\n            current_group_time_spent = 0.00\n            previous_groups = time_group_Series\n        else:\n            current_group_time_spent = pd.Timedelta(row.timestamp - session_statistics.iloc[-1].timestamp_y).seconds + time_group_Series.iloc[-1]\n            previous_groups = time_group_Series.iloc[0:-1]\n\n        timeFeatureValue_groupstat = [np.mean(previous_groups), np.std(previous_groups), \\\n                                     np.max(previous_groups), np.min(previous_groups), \\\n                                     current_group_time_spent]\n\n\n        timeFeatureValue_totaltime = [np.sum(timeFeatureValue_world)]\n\n        # Step 2.2.5: Features w.r.t Games.\n        game_statistics = session_statistics[session_statistics.type=='Game']\n\n        if game_statistics.shape[0] > 0:\n\n            # Feature Category 1: Total count.\n            game_feature_1_values = list(game_statistics[effective_game_codes].sum())\n\n            # Feature Category 2: Overall Summary.\n            def calculate_statistics(s):\n                return np.mean(s), np.std(s), np.max(s), np.min(s)\n\n            def calculate_ga_summary_features(df):\n                num_give_up = df[df.accuracy==0].shape[0]\n                give_up_rate = num_give_up / df.shape[0]\n                accuracy_mean, accuracy_std, accuracy_max, accuracy_min = calculate_statistics(df.accuracy)\n                # round_complete_rate = np.sum(game_statistics.is_all_round_complete) / game_statistics.shape[0]\n                return [num_give_up, df.shape[0], give_up_rate, accuracy_mean, accuracy_std, accuracy_max, accuracy_min]\n\n            game_feature_2_values = calculate_ga_summary_features(game_statistics)\n\n            # Feature Category 3: Group Summary.\n            if is_label_new_group or (game_statistics.iloc[-1].group != session_statistics.iloc[-1].group):\n                game_feature_3_values = game_feature_2_values\n            else:\n                game_feature_3_values = calculate_ga_summary_features(game_statistics[game_statistics.group==game_statistics.iloc[-1].group])\n\n            # Feature Category 4: Recent Group Summary.\n            if game_statistics.iloc[-1].group >= 3:\n                game_feature_4_values = calculate_ga_summary_features(game_statistics[game_statistics.group>=game_statistics.iloc[-1].group-2])\n            else:\n                game_feature_4_values = game_feature_2_values\n\n        else:\n            game_feature_1_values = [0] * len(game_feature_1_names)\n            game_feature_2_values = [np.NaN] * len(game_feature_2_names)\n            game_feature_3_values = [np.NaN] * len(game_feature_3_names)\n            game_feature_4_values = [np.NaN] * len(game_feature_4_names)\n\n\n        # Step 2.2.6: Features w.r.t Assessments.\n        assessment_statistics = session_statistics[session_statistics.type=='Assessment']\n\n        if assessment_statistics.shape[0] > 0:\n\n            # Feature Category 1: Total count.\n            assessment_feature_1_values = list(assessment_statistics[effective_assessment_codes].sum())\n\n            # Feature Category 2: Overall Summary.\n            def calculate_statistics(s):\n                return np.mean(s), np.std(s), np.max(s), np.min(s)\n\n            def calculate_ga_summary_features(df):\n                num_give_up = df[df.accuracy==0].shape[0]\n                give_up_rate = num_give_up / df.shape[0]\n                accuracy_mean, accuracy_std, accuracy_max, accuracy_min = calculate_statistics(df.accuracy)\n                # round_complete_rate = np.sum(game_statistics.is_all_round_complete) / game_statistics.shape[0]\n                return [num_give_up, df.shape[0], give_up_rate, accuracy_mean, accuracy_std, accuracy_max, accuracy_min]\n\n            assessment_feature_2_values = calculate_ga_summary_features(assessment_statistics)\n\n            # Feature Category 3: Group Summary.\n            if is_label_new_group or (assessment_statistics.iloc[-1].group != session_statistics.iloc[-1].group):\n                assessment_feature_3_values = assessment_feature_2_values\n            else:\n                assessment_feature_3_values = calculate_ga_summary_features(assessment_statistics[assessment_statistics.group==assessment_statistics.iloc[-1].group])\n\n            # Feature Category 4: Recent Group Summary.\n            if assessment_statistics.iloc[-1].group >= 3:\n                assessment_feature_4_values = calculate_ga_summary_features(assessment_statistics[assessment_statistics.group>=assessment_statistics.iloc[-1].group-2])\n            else:\n                assessment_feature_4_values = assessment_feature_2_values\n\n        else:\n            assessment_feature_1_values = [0] * len(assessment_feature_1_names)\n            assessment_feature_2_values = [np.NaN] * len(assessment_feature_2_names)\n            assessment_feature_3_values = [np.NaN] * len(assessment_feature_3_names)\n            assessment_feature_4_values = [np.NaN] * len(assessment_feature_4_names)\n\n        # Missing Values.\n        if np.sum(np.isnan(game_feature_2_values)) > 0 or np.sum(np.isnan(assessment_feature_2_values)): \n            if np.sum(np.isnan(game_feature_2_values)) == 0:\n                assessment_feature_2_values = game_feature_2_values\n                assessment_feature_3_values = game_feature_3_values\n                assessment_feature_4_values = game_feature_4_values\n            elif np.sum(np.isnan(assessment_feature_2_values)) == 0:\n                game_feature_2_values = assessment_feature_2_values\n                game_feature_3_values = assessment_feature_3_values\n                game_feature_4_values = assessment_feature_4_values\n            else:\n                pass\n\n\n        # Step 2.2.7: Features w.r.t Activities.\n        activity_statistics = session_statistics[session_statistics.type=='Activity']\n        if activity_statistics.shape[0] > 0:\n\n            # Feature Category 1: Total count.\n            activity_feature_1_values = list(activity_statistics[effective_activity_codes].sum())\n\n        else:\n            activity_feature_1_values = [0] * len(activity_feature_1_names)\n\n\n        # Step 2.2.8: Features w.r.t Clips.\n        clip_statistics = session_statistics[session_statistics.type=='Clip']\n        total_time = 0\n        clip_feature_values = [0] * len(clip_names)\n        for idx, row_ in clip_statistics.iterrows():\n            clip_feature_values[clip_names.index(row_.title)] += 1\n\n        feature_values =  [1] + game_feature_values_by_name + assessment_feature_values_by_name + activity_feature_values_by_name + \\\n                   timeFeatureValue_type + timeFeatureValue_world + timeFeatureValue_groupstat +  \\\n                   timeFeatureValue_title + timeFeatureValue_totaltime + game_feature_1_values + game_feature_2_values + game_feature_3_values + game_feature_4_values + \\\n                   assessment_feature_1_values + assessment_feature_2_values + assessment_feature_3_values + assessment_feature_4_values + \\\n                   clip_feature_values\n\n        return pd.Series(feature_values)\n    \n    else:\n        return pd.Series([0] * len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.random.seed(0)\nidx = np.random.permutation(np.arange(len(train_labels)))\ntrain_labels_subset = train_labels.iloc[idx].drop_duplicates(subset=['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels_subset['isTrain'] = 1\ntest_labels['isTrain'] = 0\ntrain_labels['isTrain'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels_subset[feature_names] = train_labels_subset.apply(calculate_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_labels[feature_names] = test_labels.apply(calculate_features, axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def add_more_features(labels_dataset):\n    labels_dataset['Mushroom'] = np.where(labels_dataset.title=='Mushroom Sorter (Assessment)', 1, 0)\n    labels_dataset['Bird Measurer'] = np.where(labels_dataset.title=='Bird Measurer (Assessment)', 1, 0)\n    labels_dataset['Cauldron'] = np.where(labels_dataset.title=='Cauldron Filler (Assessment)', 1, 0)\n    labels_dataset['Cart'] = np.where(labels_dataset.title=='Cart Balancer (Assessment)', 1, 0)\n    labels_dataset['Chest'] = np.where(labels_dataset.title=='Chest Sorter (Assessment)', 1, 0)\n    labels_dataset['TreeTopCity'] = labels_dataset['Mushroom'] + labels_dataset['Bird Measurer']\n    labels_dataset['CrystalCaves'] = labels_dataset['Cart'] + labels_dataset['Chest']\n    return labels_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels_subset = add_more_features(train_labels_subset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_labels = add_more_features(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"training_data = train_labels_subset.drop(['game_session', 'title', 'installation_id', 'num_correct', 'num_incorrect', 'accuracy', 'timestamp', 'isTrain'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testing_data = test_labels.drop(['game_session', 'title', 'installation_id', 'timestamp', 'isTrain'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Training Models"},{"metadata":{"trusted":false},"cell_type":"code","source":"import itertools\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, color='w')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = training_data['accuracy_group']\nX = training_data.drop(['accuracy_group'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, cohen_kappa_score\nimport itertools\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.neural_network import MLPClassifier\n\n\nclf = XGBRegressor()\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"value_counts = y_train.value_counts()\nq1, q2, q3 = pd.Series(y_pred).quantile([value_counts.loc[0]/np.sum(value_counts), (value_counts.loc[0] \\\n                            +value_counts.loc[1])/np.sum(value_counts), \\\n                             (value_counts.loc[0]+value_counts.loc[1]+value_counts.loc[2])/np.sum(value_counts)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_final = np.where(y_pred<q1, 0, np.where(y_pred<q2,1,np.where(y_pred<q3,2,3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_confusion_matrix(confusion_matrix(y_test, y_final), classes=['0', '1', '2', '3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cohen_kappa_score(y_test, y_final, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = clf.predict(testing_data)\n\nq1, q2, q3 = pd.Series(y_pred).quantile([value_counts.loc[0]/np.sum(value_counts), (value_counts.loc[0] \\\n                            +value_counts.loc[1])/np.sum(value_counts), \\\n                             (value_counts.loc[0]+value_counts.loc[1]+value_counts.loc[2])/np.sum(value_counts)])\n\ndef find_accuracy_group(x):\n    if x < q1:\n        return 0\n    elif x < q2:\n        return 1\n    elif x < q3:\n        return 2\n    else:\n        return 3\n\ntest_labels['accuracy'] = y_pred\ntest_labels['accuracy_group'] = test_labels['accuracy'].apply(find_accuracy_group)\noutput = test_labels[['installation_id', 'accuracy_group']]\noutput.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}