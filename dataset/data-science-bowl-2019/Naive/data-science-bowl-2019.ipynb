{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\n# import shap\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport json\npd.set_option('display.max_columns', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Id = \"installation_id\"\ntarget = \"accuracy_group\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/data-science-bowl-2019/sample_submission.csv\")\nspecs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\ntest = pd.read_csv(\"../input/data-science-bowl-2019/test.csv\")\ntrain = pd.read_csv(\"../input/data-science-bowl-2019/train.csv\")\ntrain_labels = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above methods to transfer some categorical features to numerical ones manually by mapping dictionaries could be substituded by LabelEncode. We may discuss it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    \n    train['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), train['type'], train['world']))\n    test['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), test['type'], test['world']))\n    all_type_world = list(set(train[\"type_world\"].unique()).union(test[\"type_world\"].unique()))\n    \n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, all_type_world","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, all_type_world = encode_title(train, test, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnt_miss(df):\n    cnt = 0\n    for e in range(len(df)):\n        x = df['event_data'].iloc[e]\n        y = json.loads(x)['misses']\n        cnt += y\n    return cnt\n\ndef get_4020_acc(df,counter_dict):\n    \n    for e in ['Cauldron Filler (Assessment)', 'Bird Measurer (Assessment)', \n              'Mushroom Sorter (Assessment)','Chest Sorter (Assessment)']:\n        \n        Assess_4020 = df[(df.event_code == 4020) & (df.title==activities_map[e])]   \n        true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n        false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n\n        measure_assess_accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0\n        counter_dict[e+\"_4020_accuracy\"] += (counter_dict[e+\"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n    \n    return counter_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    assess_4020_acc_dict = {'Cauldron Filler (Assessment)_4020_accuracy': 0, \n                            'Mushroom Sorter (Assessment)_4020_accuracy': 0, \n                            'Bird Measurer (Assessment)_4020_accuracy': 0, \n                            'Chest Sorter (Assessment)_4020_accuracy': 0}\n    \n    game_time_dict = {'Clip_gametime': 0, 'Game_gametime': 0, \n                      'Activity_gametime': 0, 'Assessment_gametime': 0}\n    \n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    \n    # Newly added features\n    accumulated_game_miss = 0\n    Cauldron_Filler_4025 = 0\n    mean_game_round = 0\n    mean_game_duration = 0 \n    mean_game_level = 0\n    Assessment_mean_event_count = 0\n    Game_mean_event_count = 0\n    Activity_mean_event_count = 0\n    chest_assessment_uncorrect_sum = 0\n    \n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    durations_game = []\n    durations_activity = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    type_world_count: Dict[str, int] = {w_eve: 0 for w_eve in all_type_world}\n    session_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        \n        if session_type == \"Activity\":\n            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0\n            \n        if session_type == \"Game\":\n            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n            \n            game_s = session[session.event_code == 2030]\n            misses_cnt = cnt_miss(game_s)\n            accumulated_game_miss += misses_cnt\n            \n            try:\n                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n                mean_game_round =  (mean_game_round + game_round)/ 2.0\n            except:\n                pass\n\n            try:\n                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n                mean_game_duration = (mean_game_duration + game_duration) / 2.0\n            except:\n                pass\n            \n            try:\n                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n                mean_game_level = (mean_game_level + game_level) / 2.0\n            except:\n                pass\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(title_count.copy())\n            features.update(game_time_dict.copy())\n            features.update(event_id_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(assess_4020_acc_dict.copy())\n            features.update(type_world_count.copy())\n            features.update(last_game_time_title.copy())\n            features.update(ac_game_time_title.copy())\n            features.update(ac_true_attempts_title.copy())\n            features.update(ac_false_attempts_title.copy())\n            features['installation_session_count'] = session_count\n            \n            features['accumulated_game_miss'] = accumulated_game_miss\n            features['mean_game_round'] = mean_game_round\n            features['mean_game_duration'] = mean_game_duration\n            features['mean_game_level'] = mean_game_level\n            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n            features['Game_mean_event_count'] = Game_mean_event_count\n            features['Activity_mean_event_count'] = Activity_mean_event_count\n            features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n            \n            \n            \n            \n            variety_features = [('var_event_code', event_code_count), \n                                ('var_event_id', event_id_count), \n                                ('var_title', title_count), \n                                ('var_title_event_code', title_event_code_count), \n                                ('var_type_world', type_world_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n            # ----------------------------------------------\n            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n            \n            \n            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n            # ----------------------------------------------\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n                features['last_duration'] = 0\n                features['duration_max'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                features['last_duration'] = durations[-1]\n                features['duration_max'] = np.max(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n            if durations_game == []:\n                features['duration_game_mean'] = 0\n                features['duration_game_std'] = 0\n                features['game_last_duration'] = 0\n                features['game_max_duration'] = 0\n            else:\n                features['duration_game_mean'] = np.mean(durations_game)\n                features['duration_game_std'] = np.std(durations_game)\n                features['game_last_duration'] = durations_game[-1]\n                features['game_max_duration'] = np.max(durations_game)\n                \n            if durations_activity == []:\n                features['duration_activity_mean'] = 0\n                features['duration_activity_std'] = 0\n                features['game_activity_duration'] = 0\n                features['game_activity_max'] = 0\n            else:\n                features['duration_activity_mean'] = np.mean(durations_activity)\n                features['duration_activity_std'] = np.std(durations_activity)\n                features['game_activity_duration'] = durations_activity[-1]\n                features['game_activity_max'] = np.max(durations_activity)\n            \n            # the accuracy is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            # --------------------------\n            features['Cauldron_Filler_4025'] = Cauldron_Filler_4025/counter if counter > 0 else 0\n            Assess_4025 = session[(session.event_code == 4025) & (session.title=='Cauldron Filler (Assessment)')]\n            true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n            false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n            \n            cau_assess_accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0\n            Cauldron_Filler_4025 += cau_assess_accuracy_\n            \n            chest_assessment_uncorrect_sum += len(session[session.event_id==\"df4fe8b6\"])\n            \n            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1])/2.0\n            # ----------------------------\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n            \n        if session_type == 'Game':\n            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n        if session_type == 'Activity':\n            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n                \n        \n        session_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(session[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if col == 'title':\n                    x = activities_labels[k]\n                counter[x] += num_of_session_count[k]\n            return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        type_world_count = update_counters(type_world_count, 'type_world')\n        \n        assess_4020_acc_dict = get_4020_acc(session , assess_4020_acc_dict)\n        game_time_dict[session_type+'_gametime'] = (game_time_dict[session_type+'_gametime'] + (session['game_time'].iloc[-1]/1000.0))/2.0\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    compiled_test_his = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort = False)), total = 1000):\n        compiled_test_his += get_data(user_sample)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    reduce_test_his = pd.DataFrame(compiled_test_his)\n    \n    return reduce_train, reduce_test, reduce_test_his\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save to pickle and reload"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tag = 'encode_title'\n# # train\n# train_result_path = 'train_' + tag + '.pkl'\n# new_train.to_pickle(train_result_path)\n# # test\n# test_result_path = '.test_' + tag + '.pkl'\n# new_test.to_pickle(test_result_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tranform function to get the train and test set\nreduce_train, reduce_test, reduce_test_his = get_train_and_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, features\n\n# call feature engineering function\nreduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.shape, reduce_test.shape, reduce_test_his.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricals = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following is a very useful method to transfer regressor result to categorical result, and evaluate through [cohen_kappa](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(reduce_train['accuracy_group'])\n    for k in dist:\n        dist[k] /= len(reduce_train)\n#     reduce_train['accuracy_group'].hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n- Installation_id and accuracy_group are not features\n- Delete the columns that are all 0\n- Check the distribution of each column in reduce_train and reduce_test, and drop the columns that not follow the same distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(reduce_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function return the remaining valid features after deleting the 0 features.\ndef delete_zero_columns(reduce_train):\n    features = []\n    for column_name in reduce_train.columns:\n        if column_name not in ['accuracy_group', 'installation_id']:\n            if np.sum(reduce_train[column_name], axis = 0) != 0:\n                features.append(column_name)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del_features = delete_zero_columns(reduce_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(del_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = set(del_features).union(set(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Here](https://stats.stackexchange.com/questions/354035/how-to-compare-the-data-distribution-of-2-datasets) is  Kolmogorov-Smirnov test, which is used to test if two samples come from the same distribution. However, it is not suitable here because we only need train distribution and test distribution are similar, if not completely identical."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import ks_2samp\nfrom sklearn.preprocessing import power_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's first transform the data to make it more Gaussian-like. Use [sklearn.preprocessing.power_transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.power_transform.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def power_transform_data(reduce_train, reduce_test, features):\n    train_length = len(reduce_train)\n    train_test = pd.concat([reduce_train[features], reduce_test[features]], axis = 0)\n#     new_reduce_train = reduce_train.copy()\n#     new_reduce_test = reduce_test.copy()\n    for feature in features:\n#         new_reduce_train[feature] = power_transform((new_reduce_train[feature].values).reshape(-1,1), method = 'yeo-johnson').reshape(-1)\n#         new_reduce_test[feature] = power_transform((new_reduce_test[feature].values).reshape(-1,1), method = 'yeo-johnson').reshape(-1)\n        train_test[feature] = power_transform((train_test[feature].values).reshape(-1,1), method = 'yeo-johnson').reshape(-1)\n        new_reduce_train = train_test.iloc[:train_length, :]\n        new_reduce_test = train_test.iloc[train_length:, :]\n    return new_reduce_train, new_reduce_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function drop the columns with different distribution in train and test set\ndef check_distribution(reduce_train, reduce_test, features):\n    to_exclude = []\n    for feature in features:\n        train_mean = reduce_train[feature].mean()\n        train_std = reduce_train[feature].std()\n        test_mean = reduce_test[feature].mean()\n        test_std = reduce_test[feature].std()\n        #print('train_mean: {}, test_mean: {}, train_std: {}, test_std: {}'.format(train_mean, test_mean, train_std, test_std))\n        if test_mean != 0:\n            if abs(train_mean / test_mean) > 10 or abs(train_mean / test_mean) < 0.1:\n                print('**************************')\n                print('Feature: {}, train_mean: {}, test_mean: {}'.format(feature, train_mean, test_mean))\n                to_exclude.append(feature)\n        else:\n            if abs(train_mean) > 10:\n                print('**************************')\n                print('Feature: {}, train_mean: {}, test_mean: {}'.format(feature, train_mean, test_mean))\n                to_exclude.append(feature)\n    return to_exclude","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_exclude = check_distribution(reduce_train, reduce_test, features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the previous processes, we drop a few features, but I believe it is ok to keep those features. One thing more important is to transform train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = list(set(features) - set(to_exclude))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(new_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_id = list(reduce_train['installation_id'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce_train.loc[reduce_train['installation_id'] == unique_id[0], :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debug for \"LightGBMError: Do not support special JSON characters in feature name.\"\ndef change_json(df):\n    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  params =  {'num_leaves': 61,  \n#            'min_child_weight': 0.03454472573214212,\n#            'feature_fraction': 0.3797454081646243,\n#            'bagging_fraction': 0.4181193142567742,\n#            'min_data_in_leaf': 96,  \n#            'objective': 'regression',\n#            \"metric\": 'rmse',\n#            'learning_rate': 0.1, \n#            \"boosting_type\": \"gbdt\",\n#            \"bagging_seed\": 11,\n#            \"verbosity\": -1,\n#            'reg_alpha': 0.3899927210061127,\n#            'reg_lambda': 0.6485237330340494,\n#            'random_state': 46,\n#            'num_threads': 16,\n#            'lambda_l1': 1,  \n#            'lambda_l2': 1,\n#            'n_estimators': 8000,\n#            'early_stopping': 150\n#     }\ndef run_lgb_regression(reduce_train, reduce_test, useful_features, n_splits, depth, params):\n    loss_scores = []\n    useful_features = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in useful_features]\n    reduce_train = change_json(reduce_train)\n    reduce_test = change_json(reduce_test)\n    feature_importance = {}\n    kf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n    oof_predict = np.zeros((len(reduce_train), ))\n    y_pred = np.zeros((len(reduce_test), ))\n    for fold, (train_index, test_index) in enumerate(kf.split(reduce_train, reduce_train[target])):\n        print('Fold: {}'.format(fold + 1))\n        X_train = reduce_train[useful_features].iloc[train_index]\n        X_val = reduce_train[useful_features].iloc[test_index]\n        y_train = reduce_train[target].iloc[train_index]\n        y_val = reduce_train[target].iloc[test_index]\n        \n        train_set = lgb.Dataset(X_train, y_train, categorical_feature = categoricals)\n        val_set = lgb.Dataset(X_val, y_val, categorical_feature = categoricals)\n        \n        model = lgb.train(params, train_set, num_boost_round = params['n_estimators'], valid_sets = [train_set, val_set],\n                         early_stopping_rounds = params['early_stopping'])\n        oof_predict[test_index] = model.predict(X_val)\n        y_pred += model.predict(reduce_test[useful_features]) / n_splits\n        #_, loss_score, _ = eval_qwk_lgb_regr(reduce_train.loc[test_index, target], oof_predict) \n        #loss_scores.append(loss_score)\n        #print('The cohen_kappa score for folder_{} is: {}'.format(fold + 1, loss_score))\n        \n        feature_importance['fold_{}'.format(fold + 1)] = model.feature_importance()\n        print(feature_importance['fold_{}'.format(fold + 1)][:5])\n        \n    return y_pred, oof_predict, feature_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred, oof_predict, feature_importance = run_lgb_regression(reduce_train, reduce_test, new_features, 5, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eval_qwk_lgb_regr(reduce_train[target], oof_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_feature_importance(feature_importance, features):\n    feature_imp = pd.DataFrame(zip(feature_importance, features), columns=['Value','Feature'])\n    plt.figure(figsize=(20, 500))\n    sns.barplot(x=\"Value\", y=\"Feature\", data = feature_imp.sort_values(by = \"Value\", ascending = False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_important_features(feature_importance, feature):\n    feature_imp = pd.DataFrame(zip(feature_importance, features), columns=['Value','Feature'])\n    feature_imp = feature_imp.sort_values(by = 'Value', ascending = False)\n    return feature_imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* * > ### check the top n important features in 5 folders"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_top_features(top_n, feature_imp_fold_list):\n    set1 = set(list(feature_imp_fold_list[0].head(top_n)['Feature'].values))\n    set2 = set(list(feature_imp_fold_list[1].head(top_n)['Feature'].values))\n    set3 = set(list(feature_imp_fold_list[2].head(top_n)['Feature'].values))\n    set4 = set(list(feature_imp_fold_list[3].head(top_n)['Feature'].values))\n    set5 = set(list(feature_imp_fold_list[3].head(top_n)['Feature'].values))\n    top_features = set.intersection(set1, set2, set3, set4, set5)\n    return top_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top_features = check_top_features(500, feature_imp_fold_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(top_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I want to explore what happens if I use the transformed data(new_reduce_train and new_reduce test) rather than the original data"},{"metadata":{},"cell_type":"markdown","source":"## Using Bayesian Optimization to detect the best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(reduce_train, reduce_test, useful_features, n_splits, num_leaves, max_depth, min_child_weight, feature_fraction, lambda_l1, lambda_l2, \n          bagging_fraction, min_data_in_leaf, learning_rate, reg_alpha, reg_lambda, n_estimators):\n     \n        params =  {'num_leaves': int(num_leaves),  \n            'max_depth' : int(max_depth),\n           'min_child_weight': min_child_weight,\n           'feature_fraction': feature_fraction,\n           'bagging_fraction': bagging_fraction,\n           'min_data_in_leaf': int(min_data_in_leaf), \n           'objective': 'regression',\n           \"metric\": 'rmse',\n           'learning_rate': learning_rate, \n           \"boosting_type\": \"gbdt\",\n           \"bagging_seed\": 11,\n           \"verbosity\": -1,\n           'reg_alpha': reg_alpha,\n           'reg_lambda': reg_lambda,\n           'random_state': 46,\n           'num_threads': 16,\n           'lambda_l1': lambda_l1,  \n           'lambda_l2': lambda_l2, \n           'n_estimators': int(n_estimators),\n           'early_stopping': 150\n    }\n        def run_lgb(reduce_train, reduce_test, useful_features, n_splits = n_splits):\n            #useful_features.remove('installation_id')\n            rmse_score_list = []\n            useful_features = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in useful_features]\n            reduce_train = change_json(reduce_train)\n            reduce_test = change_json(reduce_test)\n            kf = StratifiedKFold(n_splits = n_splits, random_state = 42, shuffle = True)\n            oof_predict = np.zeros((len(reduce_train), ))\n            y_pred = np.zeros((len(reduce_test), ))\n            for fold, (train_index, test_index) in enumerate(kf.split(reduce_train, reduce_train[target])):\n                X_train = reduce_train[useful_features].iloc[train_index]\n                X_val = reduce_train[useful_features].iloc[test_index]\n                y_train = reduce_train[target].iloc[train_index]\n                y_val = reduce_train[target].iloc[test_index]\n                train_set = lgb.Dataset(X_train, y_train, categorical_feature = categoricals)\n                val_set = lgb.Dataset(X_val, y_val, categorical_feature = categoricals)\n                lgb_model = lgb.train(params, train_set, num_boost_round = params['n_estimators'], valid_sets = [train_set, val_set],\n                             early_stopping_rounds = params['early_stopping'])\n                val_predict = lgb_model.predict(X_val)\n                rmse_score = np.sqrt(mean_squared_error(val_predict, y_val))\n                rmse_score_list.append(rmse_score)\n            return -np.mean(rmse_score_list)\n        \n        return run_lgb(reduce_train, reduce_test, useful_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\npartial_model = partial(model, reduce_train, reduce_test, new_features, n_splits = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bounds_LGB = {\n    'num_leaves' : (50, 100),\n    'max_depth': (8, 30),\n    'min_child_weight' : (0.01, 0.6),\n    'min_data_in_leaf' : (80, 120),\n    'feature_fraction' : (0.1, 0.8),\n    'lambda_l1': (0, 10),\n    'lambda_l2': (0, 10),\n    'bagging_fraction': (0.2, 1),\n    'learning_rate': (0.01, 0.8),\n    'reg_alpha' : (0.1 , 5), \n    'reg_lambda' : (0.1, 5),\n    'n_estimators' : (5000,8000)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuning reduce_train and reduce_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# init_points = 16\n# n_iter = 16\n# LGB_BO = BayesianOptimization(partial_model, bounds_LGB, random_state=1029)\n# with warnings.catch_warnings():\n#     warnings.filterwarnings('ignore')\n#     LGB_BO.maximize(init_points = init_points, n_iter = n_iter, acq='ei', alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_LGB_BO_params = LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGB_BO.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_LGB_BO_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use the selected parameters to cross validation and find the best features"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayesian_params =  {'num_leaves': 50,  \n            'max_depth' : 30,\n           'min_child_weight': 0.01,\n           'feature_fraction': 0.8,\n           'bagging_fraction': 0.2,\n           'min_data_in_leaf': 80, \n           'objective': 'regression',\n           \"metric\": 'rmse',\n           'learning_rate': 0.01, \n           \"boosting_type\": \"gbdt\",\n           \"bagging_seed\": 11,\n           \"verbosity\": -1,\n           'reg_alpha': 50,\n           'reg_lambda': 0.1,\n           'random_state': 46,\n           'num_threads': 16,\n           'lambda_l1': 10,  \n           'lambda_l2': 0, \n           'n_estimators': 5149,\n           'early_stopping': 150\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_bayes, oof_predict_bayes, feature_importance_bayes = run_lgb_regression(reduce_train, reduce_test, new_features, 5, 10, bayesian_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_predict_bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_qwk_lgb_regr(reduce_train[target], oof_predict_bayes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp_fold_1 = get_important_features(feature_importance_bayes['fold_1'], new_features)\nfeature_imp_fold_2 = get_important_features(feature_importance_bayes['fold_2'], new_features)\nfeature_imp_fold_3 = get_important_features(feature_importance_bayes['fold_3'], new_features)\nfeature_imp_fold_4 = get_important_features(feature_importance_bayes['fold_4'], new_features)\nfeature_imp_fold_5 = get_important_features(feature_importance_bayes['fold_5'], new_features)\nfeature_imp_fold_list = [feature_imp_fold_1, feature_imp_fold_2, feature_imp_fold_3, feature_imp_fold_4, feature_imp_fold_5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_feature_imp(feature_imp_fold_list):\n    feature_imp_fold_1 = feature_imp_fold_list[0].set_index('Feature')\n    feature_imp_fold_2 = feature_imp_fold_list[1].set_index('Feature')\n    feature_imp_fold_3 = feature_imp_fold_list[2].set_index('Feature')\n    feature_imp_fold_4 = feature_imp_fold_list[3].set_index('Feature')\n    feature_imp_fold_5 = feature_imp_fold_list[4].set_index('Feature')\n    df1 = pd.merge(feature_imp_fold_1, feature_imp_fold_2, how = 'inner', left_index = True, right_index = True)\n    df2 = df1.merge(feature_imp_fold_3, how = 'inner', left_index = True, right_index = True)\n    df3 = df2.merge(feature_imp_fold_4, how = 'inner', left_index = True, right_index = True)\n    final_df = df3.merge(feature_imp_fold_5, how = 'inner', left_index = True, right_index = True)\n    final_df.columns = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n    final_df['average'] = final_df[['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']].mean(axis = 1)\n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_from_all_folders = merge_feature_imp(feature_imp_fold_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_from_all_folders = feature_importance_from_all_folders.sort_values('average', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = feature_importance_from_all_folders.loc[feature_importance_from_all_folders['average'] > 5, :].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = list(top_features.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'session_title' not in top_features:\n    top_features.append('session_title')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the top features, we run lightgbm and tune the parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#partial_model_top_features = partial(model, reduce_train, reduce_test, top_features, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# init_points = 16\n# n_iter = 16\n# LGB_BO_top_features = BayesianOptimization(partial_model_top_features, bounds_LGB, random_state = 1029)\n# with warnings.catch_warnings():\n#     warnings.filterwarnings('ignore')\n#     LGB_BO_top_features.maximize(init_points = init_points, n_iter = n_iter, acq='ei', alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_LGB_BO_top_features_params = LGB_BO_top_features.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGB_BO_top_features.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_LGB_BO_top_features_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayesian_params_top_features =  {'num_leaves': 64,  \n            'max_depth' : 17,\n           'min_child_weight': 0.39533312446497537,\n           'feature_fraction': 0.5373674618462821,\n           'bagging_fraction': 0.2357059531074505,\n           'min_data_in_leaf': 112, \n           'objective': 'regression',\n           \"metric\": 'rmse',\n           'learning_rate': 0.018024583616814218, \n           \"boosting_type\": \"gbdt\",\n           \"bagging_seed\": 11,\n           \"verbosity\": -1,\n           'reg_alpha': 1.6071134326080774,\n           'reg_lambda': 1.6430470013389429,\n           'random_state': 46,\n           'num_threads': 16,\n           'lambda_l1': 3.0475007160079546,  \n           'lambda_l2': 4.476200330834915, \n           'n_estimators': 5126,\n           'early_stopping': 150\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_bayes_top_features, oof_predict_bayes_top_features, feature_importance_bayes_top_features = run_lgb_regression(reduce_train, reduce_test, \n                                                                                             top_features, 5, 10, bayesian_params_top_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_predict_bayes_top_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_qwk_lgb_regr(reduce_train[target], oof_predict_bayes_top_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reg_to_cat(y_regress):\n    dist = Counter(reduce_train['accuracy_group'])\n    for k in dist:\n        dist[k] /= len(reduce_train)\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_regress, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_regress)))\n    return y_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg_to_cat(y_pred_bayes_top_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = y_pred.astype(int)\nsample_submission.to_csv('./submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}