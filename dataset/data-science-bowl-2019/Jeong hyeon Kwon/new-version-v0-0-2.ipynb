{"cells":[{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:10:38.024137Z","start_time":"2020-01-15T11:10:37.961134Z"},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime as dt\nfrom tqdm import tqdm_notebook, tqdm\nfrom tqdm._tqdm_notebook import tqdm_notebook\nimport time\nimport warnings\nwarnings.filterwarnings(action='ignore')\nfrom collections import OrderedDict, defaultdict\nimport subprocess\nimport json\nimport csv\nimport gc\nfrom itertools import chain, islice\nimport itertools\nimport os\n\n\nfrom typing import Dict #dict형식 \nfrom collections import Counter #동일한값의 자료가 몇개인지 파악하는데 사용\n\n#import missingno as msno\n#한글깨짐방지\nplt.rc('font',family='Malgun Gothic')\nplt.rcParams['axes.unicode_minus'] = False\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML('<style>.container {width:100% !important; }</style>'))\n\npd.options.display.max_columns = 900","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data load"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:48:26.616894Z","start_time":"2020-01-15T11:47:27.373505Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# #CSV\n# path = './001.data/'\n# train_df = pd.read_csv(path + 'train.csv')\n# test_df = pd.read_csv(path + 'test.csv')\n# train_labels_df = pd.read_csv(path +'train_labels.csv')\n# sample_submission = pd.read_csv(path +'sample_submission.csv')\n# train_df.shape, test_df.shape,train_labels_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/data-science-bowl-2019/'\ntrain_df = pd.read_csv(path+'train.csv')\ntest_df = pd.read_csv(path+'test.csv')\ntrain_labels_df = pd.read_csv(path+'train_labels.csv')\nsample_submission = pd.read_csv(path+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# user func"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:34.004339Z","start_time":"2020-01-15T11:11:33.995339Z"},"trusted":false},"cell_type":"code","source":"def info_df(data):\n    '''\n    data의 type, null_count, null_rate를 알려주는 함수 \n    추가로 unique한 데이터의 수를 알려줌 (속도가 느리므로 필요시 주석처리 필요)\n    '''\n    info_df = pd.DataFrame({\"type\":data.dtypes,\n                            'null_count':data.isnull().sum(),\n                           'null_rate':data.isnull().sum()/data.isnull().count() * 100})\n    \n#     cols = data.columns.values\n    \n#     uni_count =[]\n#     for col in cols:\n#         uni_count.append(len(data[col].unique()))\n#     info_df['uni_count'] = uni_count\n    \n    return info_df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:43:00.254227Z","start_time":"2020-01-15T11:43:00.236226Z"},"trusted":false},"cell_type":"code","source":"#feature importance\ndef model_fi(model,origin_df):\n    '''\n    모델의 feature_importance를 확인하여, origin_df의 컬럼명에 따른 DF생성\n    importance의 비율 및 누적합 또한 산출   \n    '''    \n    fi_df = pd.DataFrame({'importance': model.feature_importances_},index = origin_df.columns)\n    fi_df = fi_df.sort_values(by ='importance', ascending=False)\n    fi_df['importance'] = round(fi_df['importance'],4)\n    fi_df['rate'] = (fi_df.importance/fi_df.importance.sum()*100)\n    fi_df['rate_cumsum'] = fi_df.rate.cumsum()\n    \n    return fi_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### cut_train 재이해\n* 기존코드 짧게 가능한듯 \n* 마지막 Assessment 이후처리가 필요한가? 어차피 for문 돌릴거라면 굳이? 할필요없다 (왜냐면 오래걸리니까)"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.25393Z","start_time":"2020-01-15T01:12:01.957856Z"}},"cell_type":"markdown","source":"#유니크한값만\nuni_inst = train_labels_df.installation_id.unique()\n#들어오는거 안들어오는거\ninlabel = train_df[train_df.installation_id.isin(list(uni_inst))]\ninlabel = train_df[~train_df.installation_id.isin(list(uni_inst))]"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.42994Z","start_time":"2020-01-15T01:12:03.25593Z"}},"cell_type":"markdown","source":"#들어오지않는것중 승리코드가 있는 경우는 없다\noutlabel[(outlabel.type == 'Assessment')&(outlabel.event_code == 4110)].shape, outlabel[(outlabel.type == 'Assessment')&(outlabel.event_code == 4100)].shape"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:34.028341Z","start_time":"2020-01-15T11:11:34.02034Z"},"trusted":false},"cell_type":"code","source":"# data_mid = pd.DataFrame()\n# for ins_id, user_sample in tqdm_notebook(train_s.iloc[:57].groupby('installation_id', sort = False)):\n#     as_indexs = user_sample.loc[(user_sample.type.isin(['Assessment']))&\n#                                 (user_sample.accuracy_group.notnull())].index.values\n#     last_index = as_indexs[-1]\n# #     last_index = user_sample.index.max()\n#     sess_df = pd.DataFrame()\n#     #user_sample.drop(columns='accuracy_group', axis=1, inplace=True)\n#     for as_index in as_indexs:\n#         #iloc를 쓰면 as_index에 +1 \n#         #loc를 쓰면 as_index그대로 사용\n#         session = user_sample.iloc[n:as_index+1,:]\n#         session_df = session.groupby('installation_id').sum().reset_index(drop=True)\n#         sess_df = pd.concat([sess_df, session_df])\n#     data_mid = pd.concat([data_mid, sess_df])\n#     n = last_index+1\n# data_mid = data_mid.reset_index(drop=True)\n# data_fin = pd.concat([data_fin, data_mid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 아이디어"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:13:04.738447Z","start_time":"2020-01-15T01:13:04.727446Z"}},"cell_type":"markdown","source":"# day gap \nfirst_day = sameple_df['timestamp'].iloc[0]\n\n#game_session단위에서 \nlast_day = session['timestamp'].iloc[-1]\nday_gap = (last_day-first_day).days"},{"metadata":{},"cell_type":"markdown","source":"### 튜닝중"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:34.112346Z","start_time":"2020-01-15T11:11:34.031341Z"},"trusted":false},"cell_type":"code","source":"#함수빌려오기 \ndef encode_title(train, test, train_labels):\n    \n    #train in train_labels\n    uni_inst = train_labels.installation_id.unique()\n    train = train[train.installation_id.isin(list(uni_inst))]\n    \n    # title_event_code \n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    \n    # title, event_code, event_id, world\n    list_of_title = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    list_of_worlds = list(set(train['world'].unique()))   \n    \n    # create a dictionary numerating the titles\n    title_labels = dict(zip(list_of_title, np.arange(len(list_of_title)))) \n    labels_title = dict(zip(np.arange(len(list_of_title)), list_of_title)) \n    world_labels = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    labels_world = dict(zip(np.arange(len(list_of_worlds)),list_of_worlds))\n    assess_titles = list(train_df[train_df['type'] == 'Assessment']['title'].unique())\n    \n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(title_labels)\n    test['title'] = test['title'].map(title_labels)\n    train['world'] = train['world'].map(world_labels)\n    test['world'] = test['world'].map(world_labels)\n    train_labels['title'] = train_labels['title'].map(title_labels)\n    \n    # win_code\n    #'Bird Measurer (Assessment)' is win_code is 4110 \n    win_code = dict(zip(title_labels.values(), (4100*np.ones(len(title_labels))).astype('int')))  \n    win_code[title_labels['Bird Measurer (Assessment)']] = 4110\n    \n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, all_title_event_code, list_of_title, list_of_event_code, list_of_event_id, list_of_worlds, title_labels, labels_title ,world_labels, labels_world, assess_titles, win_code","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.588402Z","start_time":"2020-01-15T11:11:34.113346Z"},"trusted":false},"cell_type":"code","source":"train_df, test_df, train_labels_df, all_title_event_code, list_of_title, list_of_event_code, list_of_event_id, list_of_worlds, title_labels, labels_title ,world_labels, labels_world, assess_titles, win_code = encode_title(train_df, test_df, train_labels_df)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.594403Z","start_time":"2020-01-15T11:11:52.590402Z"},"trusted":false},"cell_type":"code","source":"labels_world","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### step 등등 추가하기"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.695408Z","start_time":"2020-01-15T11:11:52.596403Z"},"trusted":false},"cell_type":"code","source":"def get_key(string, key):\n    import re\n    repattern = re.compile(f'(?<=\"{key}\":)' + '[A-z0-9\\\"\\'\\[\\],]+' + \"(?=[\\},])\")\n\n    try:\n        value = repattern.search(string).group()\n    except:\n        value = np.NaN\n    \n    return value\n\ndef add_col(data):\n#     #duration (확인이 어려운부분이 있어서 생략)\n#     data['duration'] =data.event_data.apply(lambda x : get_key(x,\"duration\")).astype(float)\n#     #Scrub-A-Dub 2030은 np.nan부여\n#     data.loc[(data.event_code.isin([2030]))&(data.title.isin([title_labels['Scrub-A-Dub']])),'duration'] = np.nan\n#     #2050 임시 저장 \n#     saved = data.loc[(data.event_code.isin([2050]))&(data.title.isin([title_labels['Scrub-A-Dub']])),'duration']\n#     #2030 제외 np.nan부여\n#     data.loc[data.event_code != 2030,'duration'] = np.nan \n#     data.loc[saved.index.values,'duration'] = saved\n#     data.duration.fillna(0, inplace=True)\n    start = dt.now()\n    #step\n    key_list = [\"level\", \"round\", \"stage_number\", \"round_number\"]\n    for key in key_list:\n        data[key] = data.event_data.apply(lambda x : get_key(x,key))\n        \n    #step으로 통합  \n    data['step'] = data['round'].values\n    data.at[data['step'].isnull(),'step'] = data.loc[data['step'].isnull(),'stage_number']\n    data.at[data['step'].isnull(),'step'] = data.loc[data['step'].isnull(),'round_number']\n    data.at[data['level'].notnull(),'step'] = data.loc[data['level'].notnull(),'level']\n    data.drop(key_list,axis=1,inplace = True)\n    data.step.fillna(0, inplace=True)\n    data['step'] = data['step'].astype(int)\n    \n    #Assessment 제외\n    label_list = []\n    for title in assess_titles:\n        label_list.append(title_labels[title])\n    data.loc[data.title.isin(label_list),'step'] = 0\n    \n    step_max = data.groupby('game_session')['step'].max().reset_index()\n    data.drop('step',axis= 1,inplace = True)\n    data = pd.merge(data,step_max)\n    \n    end1 = dt.now()\n    print(\"step 소요시간\",round((end1 - start).total_seconds()/60,2),\"분\")\n        \n    #miss\n    data[\"correct\"] = data.event_data.apply(lambda x : get_key(x,\"correct\"))\n    data.loc[data.event_code != 4020,\"correct\"] = np.nan\n    data['cor_t'] = 0\n    data['cor_f'] = 0\n    data.loc[data.correct.isin([\"true\"]), 'cor_t'] = 1\n    data.loc[data.correct.isin([\"false\"]), 'cor_f'] = 1\n    data.drop('correct',axis=1,inplace = True)\n    \n    end2 = dt.now()\n    print(\"miss 소요시간\",round((end2 - end1).total_seconds()/60,2),\"분\")\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### step 고려사항\n* 1. 한Game_Session에 max(step)이 들어가는게 맞다\n* 2. 단 게임에 따라 max가 정해져 있는 경우가 있고 아닌 경우가 있다 파악 필요\n* 3. 각 게임별로 step이 다른데 이걸 하나도 뭉치기도 애매하고 하자니 품이 많이 들어가는데 그정도 가치가 있는지 모르겠어서 일단 패스"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.707409Z","start_time":"2020-01-15T11:11:52.696408Z"},"trusted":false},"cell_type":"code","source":"#일단 패스 못해먹겟다\n# test_df = add_col(test_df)\n# train_df = add_col(train_df)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.71941Z","start_time":"2020-01-15T11:11:52.708409Z"},"scrolled":true,"trusted":false},"cell_type":"code","source":"# ori_sample[ori_sample.title == 18]['step'].describe()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.73041Z","start_time":"2020-01-15T11:11:52.72041Z"},"trusted":false},"cell_type":"code","source":"# #title별로 묶어서 진행\n# step_title = dict(zip(step_title,[0]* len(step_title)))\n\n# # step_title[session['title']] session['step'][0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.742411Z","start_time":"2020-01-15T11:11:52.73141Z"},"scrolled":true,"trusted":false},"cell_type":"code","source":"# # step이 있는 타이틀 총 12개 (assessment 제외)\n# for i in a[a>0].index.values[:]:\n#     print(i, activities_labels[i])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.757412Z","start_time":"2020-01-15T11:11:52.743411Z"},"trusted":false},"cell_type":"code","source":"# ori_sample[ori_sample.title == 18]['step'].plot.box()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.770413Z","start_time":"2020-01-15T11:11:52.758412Z"},"trusted":false},"cell_type":"code","source":"# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 수정중"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.784413Z","start_time":"2020-01-15T11:11:52.771413Z"},"trusted":false},"cell_type":"code","source":"title_count: Dict[str, int] = {eve: 0 for eve in labels_title.values()} ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.798414Z","start_time":"2020-01-15T11:11:52.785414Z"},"scrolled":true,"trusted":false},"cell_type":"code","source":"labels_title","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.809415Z","start_time":"2020-01-15T11:11:52.799414Z"},"trusted":false},"cell_type":"code","source":"world_count : Dict[str, int]  = {eve: 0 for eve in labels_world.values()}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:52.827416Z","start_time":"2020-01-15T11:11:52.810415Z"},"trusted":false},"cell_type":"code","source":"world_count","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:53.543457Z","start_time":"2020-01-15T11:11:52.828416Z"},"trusted":false},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in labels_title.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    world_count : Dict[str, int]  = {eve: 0 for eve in labels_world.values()}\n        \n    # last features\n    sessions_count = 0\n        \n    #add feature\n    step =[]\n    cor_t =0\n    cor_f = 0\n    day_gap =0\n    \n    #first_day of installation_id\n    first_day = user_sample.iloc[0,2]\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = labels_title[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        # 타입이 Assessment이면서 test_set이 True 또는 길이가 1보다 클때\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            #승리코드에 해당하는 부분 출력 (여러번일수 있으니 데이터 프레임으로)\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            #시도에 대한 결과값을 각각 보존\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features.update(world_count.copy())\n            features['installation_session_count'] = sessions_count\n            \n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count),\n                               ('va_title_world',world_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            features['game_session'] = session['game_session'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n            \n            \n            #마지막시간에서 처음시간 빼고 초로 변환 후 durations에 전달\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            day_gap = (session.iloc[0, 2] - first_day).days\n            features['day_gap'] = day_gap\n            \n            # the accurace is the all time wins divided by the all time attempts\n            #누적 어큐레시 카운트가 0이상일때만 계산\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            #어큐레시 직접계산\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            #누적값생성\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            #어큐레시 그룹누적\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            #누적 엑션수 즉 누적 row수\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            # 테스트셋의 경우 모든 피쳐를 사용하고 train의 경우에는 4100,4110가 있어야함 아니면 pass\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = labels_title[k]\n                    if col == 'world':\n                        x = labels_world[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        world_count = update_counters(world_count, 'world')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:11:53.565458Z","start_time":"2020-01-15T11:11:53.545457Z"},"trusted":false},"cell_type":"code","source":"#위에서 정의된 get_data가 get_train_and_test함수에서 사용됨\n#이함수는 installation 단위의 연산이 이뤄짐\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for ins_id, user_sample in tqdm_notebook(train.groupby('installation_id', sort = False),total = 3614):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm_notebook(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:15:47.036812Z","start_time":"2020-01-15T11:11:53.566458Z"},"trusted":false},"cell_type":"code","source":"reduce_train, reduce_test, categoricals = get_train_and_test(train_df, test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#제이슨오류방지\nreduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\nreduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:15:47.385832Z","start_time":"2020-01-15T11:15:47.038812Z"},"trusted":false},"cell_type":"code","source":"reduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:37:00.696661Z","start_time":"2020-01-15T11:37:00.606656Z"},"trusted":false},"cell_type":"code","source":"def add_feature2(data):\n    '''\n    현재에 넣기위해 다소 수정함 \n    load했다면 숫자변수 문자로 수정해야함\n    \n    test_df가 있어야 작동\n    #1.RAR(Right_action_rate) :잘한학습과 잘못한 학습의 비율\n        * event_code 4020은 잘한 학습이며, 이는 correct: True/False로 나타남\n        * event_code 4035 및 4070은 학습과정에서 잘못된 곳을 누르거나 drop한 경우\n        * 4020_count/ (4035,4070_count)\n    # 2. acum_winning(누적_성공률)\n        * 3020/3021은 실패/성공시 나오는 message (Game/Assessment에서만 발동)\n        * 3021_count/(3020,3021_count)\n    # 3.same_world_rate(동일 world 학습률) :last_Assessment와 동일한 월드 횟수\n        * last_title에 따른 last_world 추출 : last_world\n        * installation_id당 전체 플레이한 title수 : total_title\n        * 각 isntallation_id당 동일 world 플레이한 비율 : same_world_rate\n        * installation_id의 last_world play횟수 / total_title \n    # 4.월드_type별 달성률\n    '''\n    \n    # 1.RAR(Right_action_rate)\n    data['RAR'] = data[4020]/(data[4035] + data[4070]+data[4020])\n    data.RAR.fillna(0, inplace=True)\n    \n    # 2. acum_winning(누적_성공률)\n    data['acum_winning'] = data[3021]/(data[3020]+data[3021])\n    data.acum_winning.fillna(0, inplace=True)\n    \n#     # 3. same_world_rate(동일 world 학습률)\n    # world_title을 합쳐놓은 df\n    world_title = test_ori[['title','world']].drop_duplicates()\n    world_title['title'] = world_title['title'].map(title_labels)\n    \n    data['last_world'] = np.nan\n    for title in list(set(data.session_title)):   \n        assessment_world = dict(world_title.values)\n        data.loc[data.session_title == title,'last_world'] = assessment_world[title]\n   \n    # total_title\n    data['total_title'] = data['MAGMAPEAK'] +data['CRYSTALCAVES'] +data['TREETOPCITY']\n    # same_world_rate \n    same_world_rate = []\n    for i in range(len(data)):\n        name = data['last_world'].iloc[i]\n        rate = data[name].iloc[i]/data['total_title'].iloc[i]\n        same_world_rate.append(rate)\n    data['same_world_rate'] = same_world_rate\n    \n    # 4.월드_type별 달성률\n    #target은 각월드별 type이 tn과 일치하는 title명을 array로 받음\n    #따라서 world_type = plat한 타이틀 수 / type별 총 타이틀수 \n    world_name = ['CRYSTALCAVES','MAGMAPEAK','TREETOPCITY']\n    type_name = ['Activity','Clip','Game','Assessment']\n    \n    title_type = test_ori[['world','title','type']].drop_duplicates('title')\n    #월드_type 형태로 컬럼생성\n    for wn in world_name:\n        for tn in type_name:\n            target = title_type.loc[(title_type['world'] == wn)&(title_type['type']== tn),'title'].values\n            data[wn+\"_\"+tn] = (data[target]>0).apply(sum,axis=1)/len(data[target].columns)\n            \n    data.drop(['last_world','total_title'], axis=1, inplace=True)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:37:17.451619Z","start_time":"2020-01-15T11:37:07.303039Z"},"trusted":false},"cell_type":"code","source":"test_ori = pd.read_csv(path + 'test.csv')\nreduce_train = add_feature2(reduce_train)\nreduce_test= add_feature2(reduce_test)\ndel test_ori","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:37:24.216006Z","start_time":"2020-01-15T11:37:24.211006Z"},"trusted":false},"cell_type":"code","source":"reduce_train.shape, reduce_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 기본모델"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:40:02.296048Z","start_time":"2020-01-15T11:40:02.227044Z"},"trusted":false},"cell_type":"code","source":"m_y = reduce_train.accuracy_group\nm_X = reduce_train.drop([\"accuracy_group\",\"game_session\",\"installation_id\"], axis = 1)\nm_test_X = reduce_test.drop([\"accuracy_group\",\"game_session\",\"installation_id\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:40:12.394626Z","start_time":"2020-01-15T11:40:12.391625Z"},"trusted":false},"cell_type":"code","source":"import lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:43:37.883379Z","start_time":"2020-01-15T11:43:32.393065Z"},"trusted":false},"cell_type":"code","source":"default_lgbmr = lgbm.LGBMRegressor()\ndefault_lgbmr.fit(m_X, m_y)\nm_pred = default_lgbmr.predict(m_X)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:43:47.785945Z","start_time":"2020-01-15T11:43:47.772945Z"},"trusted":false},"cell_type":"code","source":"model_fi(default_lgbmr,m_X).head(20)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:43:57.778517Z","start_time":"2020-01-15T11:43:57.636509Z"},"scrolled":true,"trusted":false},"cell_type":"code","source":"dist = Counter(reduce_train['accuracy_group'])\nfor k in dist:\n    dist[k] /= len(reduce_train)\nreduce_train['accuracy_group'].hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(m_pred, acum * 100)\nprint(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, m_pred)))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:44:08.075106Z","start_time":"2020-01-15T11:44:08.065105Z"},"trusted":false},"cell_type":"code","source":"pd.Series(final_pred).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:44:18.104679Z","start_time":"2020-01-15T11:44:18.056677Z"},"trusted":false},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(final_pred,reduce_train['accuracy_group'],weights=\"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### test_data(제출용)"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:44:27.971244Z","start_time":"2020-01-15T11:44:27.813235Z"},"trusted":false},"cell_type":"code","source":"r_m_pred = default_lgbmr.predict(m_test_X)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:44:38.56985Z","start_time":"2020-01-15T11:44:38.39684Z"},"trusted":false},"cell_type":"code","source":"dist = Counter(reduce_train['accuracy_group'])\nfor k in dist:\n    dist[k] /= len(reduce_train)\nreduce_train['accuracy_group'].hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(r_m_pred, acum * 100)\nprint(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, r_m_pred)))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:44:49.112453Z","start_time":"2020-01-15T11:44:49.105453Z"},"trusted":false},"cell_type":"code","source":"pd.Series(final_pred).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T11:48:36.893481Z","start_time":"2020-01-15T11:48:36.87648Z"},"trusted":false},"cell_type":"code","source":"sample_submission['accuracy_group'] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check1 마지막 assessment는 들어갔는가?\n* 결론 : 들어가지 않았다. 왜냐하면 test셋의 마지막 assessment에 대한 정보가 없기 떄문  \n--> 아마 이로 인해서 overfitting이 났을것으로 추정(이미 답을알고 있기 때문에)\n\n* 추정과정 : '0006a69f'을 대상으로 처음 game_session으로 확인   \n--> 처음 : 647건, 마지막 : 2568건 확인"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.441941Z","start_time":"2020-01-15T01:11:03.798Z"},"trusted":false},"cell_type":"code","source":"train[train.installation_id == '0006a69f']['game_session'].nunique()\ntt = train[train.installation_id == '0006a69f']\n\ntt.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.442941Z","start_time":"2020-01-15T01:11:03.802Z"},"trusted":false},"cell_type":"code","source":"#Assessment인거만 추출\nonly_ass = pd.DataFrame(tt[tt.type == 'Assessment'][['game_session','title']].drop_duplicates())\nonly_ass","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.442941Z","start_time":"2020-01-15T01:11:03.804Z"},"trusted":false},"cell_type":"code","source":"last_ass_index = tt[tt.game_session == '901acc108f55a5a1'].iloc[-1:].index[0]\nfirst_ass_index = tt[tt.game_session == '901acc108f55a5a1'].iloc[:1].index[0]\ntt.loc[:first_ass_index-1].shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.443941Z","start_time":"2020-01-15T01:11:03.805Z"},"trusted":false},"cell_type":"code","source":"last_ass_index = tt[tt.game_session == 'a9ef3ecb3d1acc6a'].iloc[-1:].index[0]\nfirst_ass_index = tt[tt.game_session == 'a9ef3ecb3d1acc6a'].iloc[:1].index[0]\ntt.loc[:first_ass_index-1].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1개 installation_id에서 확인"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.444941Z","start_time":"2020-01-15T01:11:03.807Z"},"trusted":false},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n        \n    # last features\n    sessions_count = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        # 타입이 Assessment이면서 test_set이 True 또는 길이가 1보다 클때\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            #승리코드에 해당하는 부분 출력 (여러번일수 있으니 데이터 프레임으로)\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            #시도에 대한 결과값을 각각 보존\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features['installation_session_count'] = sessions_count\n            \n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n                 \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n            #마지막시간에서 처음시간 빼고 초로 변환 후 durations에 전달\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            #누적 어큐레시 카운트가 0이상일때만 계산\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            #어큐레시 직접계산\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            #누적값생성\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            #어큐레시 그룹누적\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            #누적 엑션수 즉 누적 row수\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            # 테스트셋의 경우 모든 피쳐를 사용하고 train의 경우에는 4100,4110가 있어야함 아니면 pass\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.445941Z","start_time":"2020-01-15T01:11:03.809Z"},"trusted":false},"cell_type":"code","source":"one = train[train.installation_id == '0001e90f']\ntwo = train[train.installation_id == '0006a69f']\nsample = pd.concat([one,two])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.446941Z","start_time":"2020-01-15T01:11:03.811Z"},"trusted":false},"cell_type":"code","source":"two.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.447941Z","start_time":"2020-01-15T01:11:03.813Z"},"trusted":false},"cell_type":"code","source":"compiled_train = []\ncompiled_test = []\nfor i, (ins_id, user_sample) in tqdm(enumerate(sample.groupby('installation_id', sort = False)), total = 17000):\n    compiled_train += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-15T01:12:03.447941Z","start_time":"2020-01-15T01:11:03.818Z"},"trusted":false},"cell_type":"code","source":"compiled_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":531,"position":{"height":"553px","left":"1465px","right":"20px","top":"106px","width":"359px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"block","window_display":false}},"nbformat":4,"nbformat_minor":1}