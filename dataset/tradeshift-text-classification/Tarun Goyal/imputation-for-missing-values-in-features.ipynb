{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading data. Takes time to load due to data size.\n\ntrain = pd.read_csv('../input/train.csv/train.csv')\ntest = pd.read_csv('../input/test.csv/test.csv')\nprint (train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging datasets to apply common transformations\n\ntrain['train_identifier'] = 1\ntest['train_identifier'] = 0\ncombined = pd.concat([train, test], ignore_index=True)\nprint (combined.shape)\n\n# Deleting redundant dataframes\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify and remove columns from dataset which are categorical in nature and have more than 100 categories --> that is\n# probably useless for our predictions\n\ncombined_cols = combined.columns\nredundant_cols = []\n\nfor col in combined.columns:\n    unique_values = len(set(combined[col]))\n    if isinstance(combined[col][0], str) and unique_values >= 100:\n#         print (col, combined[col][0], unique_values)\n        redundant_cols.append(col)\n\nprint (redundant_cols)\n\ncombined.drop(redundant_cols, axis=1, inplace=True)\nprint (combined.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputation of missing values\n\n# (a) Filling columns with single dominant category (>= 85%) with \"MODE\"\n\ncols_with_monopoly = ['x1', 'x2', 'x11', 'x13', 'x14', 'x25', 'x32', 'x33', 'x42', 'x44', 'x45', 'x56', 'x62', 'x63', 'x72',\n                      'x74', 'x75', 'x86', 'x92', 'x93', 'x102', 'x104', 'x105', 'x116', 'x127', 'x129', 'x141']\nfor col in cols_with_monopoly:\n    combined[col].fillna(value='NO', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (b) Imputing missing values of categorical features using continuous features\n\n# Creating a list of predictors\n\ncat_features = []\nfor col in combined.columns:\n    if isinstance(combined[col][0], str):\n        cat_features.append(col)\nprint ('# of cat features: ', len(cat_features))\n\ncols_with_missing_values = ['x10', 'x12', 'x24', 'x26', 'x41', 'x43', 'x55', 'x57', 'x71', 'x73', 'x85', 'x87', 'x101', 'x103',\n                            'x115', 'x117', 'x126', 'x128', 'x130', 'x140', 'x142']\n\nreduced_cat_features = [col for col in cat_features if col not in cols_with_missing_values]\nprint ('# of reduced cat features: ', len(reduced_cat_features))\n\ncombined = pd.get_dummies(combined, prefix=reduced_cat_features, columns=reduced_cat_features)\nprint (len(combined.columns))\npredictors = [col for col in combined.columns if col not in cols_with_missing_values]\npredictors.remove('id')\npredictors.remove('train_identifier')\nprint ('Predictors: ', len(predictors), predictors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing missing values using classifier (hyperparameters not tuned)\n# Downloading resultant dataframe for further use\n\nfrom sklearn.linear_model import SGDClassifier\n\nclassifier = SGDClassifier(verbose=0, n_jobs=8, random_state=9)\n\ndf = combined.copy(deep=True)\n\nfor col in cols_with_missing_values:\n    print ('Imputation starts for: ', col, df.shape)\n    df_train = df[df[col].notnull()]\n    df_test = df[~ df[col].notnull()]\n    print (df_train.shape, df_test.shape)\n    model = classifier.fit(df_train[predictors], df_train[col])\n    df_test[col] = model.predict(df_test[predictors])\n    df = pd.concat([df_train, df_test])\n\nprint ('Imputation completed')\n\n# df.to_pickle('data_frame_with_imputed_values.pickle')\n# print ('Download completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}