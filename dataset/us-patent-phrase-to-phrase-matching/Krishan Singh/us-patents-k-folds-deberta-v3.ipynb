{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<ul>\n<li>The idea is simple to train deberta-v3-large on kfolds and then generating predictions using the model trained on each fold, so we train total <b>6 models</b> for each fold. </li>\n<li>Then the preditions were calibrated using thresholds</li>\n</ul>","metadata":{"id":"XIcQZgTtkXPR"}},{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece\n!pip install datasets","metadata":{"id":"smckRF-Nnpxu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import & Set & Def & Load","metadata":{"id":"SSnWVM22nktG"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\nfrom transformers import TrainingArguments,Trainer\n\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T05:44:40.272658Z","iopub.execute_input":"2022-05-30T05:44:40.273066Z","iopub.status.idle":"2022-05-30T05:44:43.083818Z","shell.execute_reply.started":"2022-05-30T05:44:40.272993Z","shell.execute_reply":"2022-05-30T05:44:43.083076Z"},"id":"Z9ghXNo_nktH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTo load and save pretrained model and tokenizer\n\"\"\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large', num_labels=1)\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')","metadata":{"id":"7yqJKPlzcarx","outputId":"1cf2ceec-ec90-43e5-9c1c-cfcc0199d219"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Ij3HOazyc2MB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n  def __init__(self,text,label,tokenizer):\n    self.sentence=text\n    self.label=label\n    self.tokenizer=tokenizer\n\n  def __len__(self):\n    return len(self.sentence)\n  \n  def __getitem__(self,idx):\n    inp_tokens=self.tokenizer.encode_plus(self.sentence[idx], \n                                          padding=\"max_length\", \n                                          add_special_tokens=True,\n                                          max_length=35, \n                                          truncation=True)\n    inp_id=inp_tokens.input_ids\n    inp_mask=inp_tokens.attention_mask\n    inp_type_ids=inp_tokens.token_type_ids\n    labels=self.label[idx]\n\n    return {\n        \"input_ids\":torch.tensor(inp_id, dtype=torch.long),\n        \"attention_mask\":torch.tensor(inp_mask, dtype=torch.long),\n        \"token_type_ids\":torch.tensor(inp_type_ids, dtype=torch.long),\n        \"label\":torch.tensor(labels, dtype=torch.float)\n    }","metadata":{"id":"p5BjhoPapbyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Data for training","metadata":{"id":"8tFYPuuVlasX"}},{"cell_type":"code","source":"cpc_texts = torch.load(\"../input/folddump/cpc_texts.pth\")\ntitles = pd.read_csv('../input/upppm/titles.csv')","metadata":{"id":"KY7wTN1Zp5Wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ndf = df.merge(titles, left_on='context', right_on='code')\n\n\ndf.reset_index(inplace=True)\ndf = df.merge(titles, left_on='context', right_on='code')\ndf.sort_values(by='index', inplace=True)\ndf.drop(columns='index', inplace=True)\n\ndf['context_text'] = df['context'].map(cpc_texts)\n","metadata":{"id":"fF-gE1PXpx8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll need to combine the context, anchor, and target together somehow. There's not much research as to the best way to do this, so we may need to iterate a bit. To start with, we'll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator **[SEP]** token to tell it:\n\n","metadata":{"id":"eMYjeG8TQafV"}},{"cell_type":"code","source":"df['inputs'] = df.context_text + '[SEP]' + df.anchor + '[SEP]' + df.target\n\ndf['inputs'] = df['inputs'].apply(str.lower)\ndf.rename(columns = {'score': 'label'}, inplace=True)\ndf.reset_index(drop=True, inplace=True)","metadata":{"id":"SbXR3cZIQbKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pscores=[]","metadata":{"id":"boK5m_TfJizH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def corr(eval_pred): \n    logits, labels = eval_pred\n    logits = logits.reshape(-1)\n    pscores.append(np.corrcoef(logits, labels)[0][1])\n    return  {'pearson': np.corrcoef(logits, labels)[0][1]}","metadata":{"id":"7kbiAOoQ1eyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=6, random_state=0, shuffle=True)","metadata":{"id":"Hcwm9IzbQOxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"3a1EzjVoQlEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HuggingFace Transformers tends to be rather enthusiastic about spitting out lots of warnings, so let's quieten it down for our sanity:","metadata":{"id":"wmspKa3TQKQi"}},{"cell_type":"code","source":"import warnings,logging\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)","metadata":{"id":"qFesDIML01eV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"exD51Z7jcTAl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training on seperate files","metadata":{"id":"Ge2kpGqdc2sq"}},{"cell_type":"code","source":"lr,bs = 2e-5,64\nwd,epochs = 0.01,4","metadata":{"id":"uvkr6FvQltmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your model, use the AutoTokenizer, passing in your model name.\n\n","metadata":{"id":"A-qVKa34QRIz"}},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/My docs/patents_bert')","metadata":{"id":"4itKu_N4dopC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=1\n\ntrain_df = pd.read_csv(f'../input/kfold-data/kfold/df_train{i}.csv')\nval_df = pd.read_csv(f'../input/kfold-data/kfold/df_val{i}.csv')","metadata":{"id":"O-TPSXeVc6mR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first generating fold csv's and then training them one by one manually\ntrain_dataset = TrainDataset(train_df['inputs'].values, train_df['label'].values, tokenizer)\nval_dataset = TrainDataset(val_df['inputs'].values, val_df['label'].values, tokenizer)\n\ntrain_dataloader=DataLoader(train_dataset,\n                            batch_size=2*bs,\n                            shuffle=True,\n                            num_workers=2,\n                          pin_memory=True, collate_fn=lambda x: x)\n\nval_dataloader=DataLoader(val_dataset,\n                            batch_size=2*bs,\n                            shuffle=False,\n                            num_workers=2,\n                          pin_memory=True, collate_fn=lambda x: x)\n\n# model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/My docs/patents_bert', num_labels=1)\n\nfor (dataTrain, dataVal) in zip(train_dataloader, val_dataloader):\n\n  args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, per_device_train_batch_size=bs, \n                           num_train_epochs=epochs, weight_decay=wd, report_to='none')\n  \n  trainer = Trainer(model, args, train_dataset=dataTrain)\n  trainer.train()\ntrainer.save_model(f'out_fold{i}')","metadata":{"id":"P_d3UzD1c6pl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.nanmean(pscores)","metadata":{"id":"4EnVPserc6wP","outputId":"b1fb0975-855b-4f9f-d8ef-1ca860404cd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"nhMDU5FubwUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To train on every fold at once","metadata":{"id":"_YwBMR1UeICt"}},{"cell_type":"code","source":"#to generate folds and train and save models at once (this process doesn't fit in the memory)\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n\ni=1\n\nfor train_index, val_index in kf.split(df[['inputs', 'label']]):\n  train_dataset = TrainDataset(df['inputs'].iloc[train_index].values, df['label'].iloc[train_index].values, tokenizer)\n  val_dataset = TrainDataset(df['inputs'].iloc[val_index].values, df['label'].iloc[val_index].values, tokenizer)\n\n  train_dataloader=DataLoader(train_dataset,\n                              batch_size=bs,\n                              shuffle=True,\n                              num_workers=2,\n                            pin_memory=True, collate_fn=lambda x: x)\n  \n  val_dataloader=DataLoader(val_dataset,\n                              batch_size=bs,\n                              shuffle=False,\n                              num_workers=2,\n                            pin_memory=True, collate_fn=lambda x: x)\n\n  model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large', num_labels=1)\n\n  for (dataTrain, dataVal) in zip(train_dataloader, val_dataloader):\n\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n      evaluation_strategy=\"epoch\", per_device_train_batch_size=8, per_device_eval_batch_size=2*8,\n      num_train_epochs=epochs, weight_decay=wd, report_to='none')\n    \n    trainer = Trainer(model, args, train_dataset=dataTrain, eval_dataset=dataVal , compute_metrics=corr)\n    trainer.train()\n  trainer.save_model(f'out_fold{i}')\n  i += 1","metadata":{"id":"cTwiIn4ltNLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.nanmean(pscores)","metadata":{"id":"RYD8N5aVtNHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{"id":"lE5Yr3cPvu2J"}},{"cell_type":"code","source":"def valid_fn(valid_loader, model, device):\n    model.eval()\n    preds = []\n    labels = []\n    \n    for step, batch in enumerate(valid_loader):\n        input_ids, token_type_ids, attention_mask = [i.to(device) for i in batch]\n    \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask, token_type_ids)\n        \n        preds.append(y_preds.to('cpu').numpy())\n    \n    predictions = np.concatenate(preds)\n    \n    return predictions\n\n\nmin_max_scaler = MinMaxScaler()\n\ndef upd_outputs(data, is_trim=True, is_minmax=True, is_reshape=True):\n    \"\"\"\\o/\"\"\"\n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T05:44:45.746847Z","iopub.execute_input":"2022-05-30T05:44:45.74729Z","iopub.status.idle":"2022-05-30T05:44:45.764168Z","shell.execute_reply.started":"2022-05-30T05:44:45.747252Z","shell.execute_reply":"2022-05-30T05:44:45.763287Z"},"id":"uJs83TaKnktJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv\ntitles = pd.read_csv('../input/upppm/titles.csv')\n\ntest_df.reset_index(inplace=True)\ntest_df = test_df.merge(titles, left_on='context', right_on='code')\ntest_df.sort_values(by='index', inplace=True)\ntest_df.drop(columns='index', inplace=True)\n\ncpc_texts = torch.load(\"../input/folddump/cpc_texts.pth\")\n\ntest_df['context_text'] = test_df['context'].map(cpc_texts)\ntest_df['inputs'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]'  + test_df['context_text']\ntest_df['inputs'] = test_df['inputs'].apply(str.lower)\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:44:46.563518Z","iopub.execute_input":"2022-05-30T05:44:46.56593Z","iopub.status.idle":"2022-05-30T05:44:47.458414Z","shell.execute_reply.started":"2022-05-30T05:44:46.565864Z","shell.execute_reply":"2022-05-30T05:44:47.457772Z"},"id":"YT96S1qYnktL","outputId":"e32fe8d9-e812-4ded-d765-9e04e2acc620","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Extract & Update Predictions","metadata":{"id":"yAPlQCM-nktL"}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_input_length):\n        self.text = df['inputs'].values.astype(str)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = self.text[item]\n        \n        inputs = self.tokenizer(inputs,\n                    max_length=self.max_input_length,\n                    padding='max_length',\n                    truncation=True)\n        \n        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)","metadata":{"id":"k9xYsqaoB_Rs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(valid_loader, model, device):\n    model.eval()\n    preds = []\n    labels = []\n    \n    for step, batch in enumerate(valid_loader):\n        input_ids, token_type_ids, attention_mask = [i for i in batch]\n    \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask, token_type_ids)\n        \n        preds.append(np.array(y_preds['logits'].flatten().to('cpu').numpy()))\n    print(preds)\n    predictions = np.concatenate(preds)\n    \n    return predictions","metadata":{"id":"AzoMyW_fCrAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\ntokenizer = AutoTokenizer.from_pretrained('../input/debertav3kfold/deberta_v3_large-20220602T055706Z-001/deberta_v3_large')\n\nte_dataset = TestDataset(test_df, tokenizer, 35)\n\nte_dataloader = DataLoader(te_dataset,\n                          batch_size=32, shuffle=False,\n                          num_workers = 2,\n                          pin_memory=True, drop_last=False)\n\n\nfor fold in tqdm(range(1, 7)):\n    \n    fold_path = f\"../input/debertav3kfold/deberta_V3_kfold/deberta_V3_kfold/out_fold{fold}\"\n    \n    model = AutoModelForSequenceClassification.from_pretrained(fold_path, num_labels=1)\n    \n    prediction = valid_fn(te_dataloader, model)\n\n    predictions.append(prediction)","metadata":{"id":"1xcmwA2tnktN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"folds:\", len(predictions))\nprint(\"rows: \", len(predictions[0]))\nprint(\"score:\", predictions[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:06.700863Z","iopub.execute_input":"2022-05-27T17:14:06.70152Z","iopub.status.idle":"2022-05-27T17:14:06.709023Z","shell.execute_reply.started":"2022-05-27T17:14:06.70148Z","shell.execute_reply":"2022-05-27T17:14:06.708032Z"},"id":"GdOnxnygnktP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_predictions = 14","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:07.934439Z","iopub.execute_input":"2022-05-27T17:14:07.934956Z","iopub.status.idle":"2022-05-27T17:14:07.939048Z","shell.execute_reply.started":"2022-05-27T17:14:07.934917Z","shell.execute_reply":"2022-05-27T17:14:07.937942Z"},"id":"Q9x5dbTZnktP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first fold\npredictions[0][:n_predictions]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:08.260613Z","iopub.execute_input":"2022-05-27T17:14:08.261231Z","iopub.status.idle":"2022-05-27T17:14:08.268679Z","shell.execute_reply.started":"2022-05-27T17:14:08.26117Z","shell.execute_reply":"2022-05-27T17:14:08.267626Z"},"id":"yU1P8_e4nktP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(*upd_outputs(predictions[0].reshape(-1,1), is_trim=False)[:n_predictions])\n# print(*upd_outputs(predictions[0].reshape(-1,1), is_minmax=False)[:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:08.545521Z","iopub.execute_input":"2022-05-27T17:14:08.545792Z","iopub.status.idle":"2022-05-27T17:14:08.549344Z","shell.execute_reply.started":"2022-05-27T17:14:08.545762Z","shell.execute_reply":"2022-05-27T17:14:08.54851Z"},"id":"NvXshP3vnktP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.where(x<=0, 0, x) .. >> min_max.fit_transform(x) >> x.reshape(-1)\nupd_predictions = [upd_outputs(x.reshape(-1,1), is_trim=False) for x in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:09.123479Z","iopub.execute_input":"2022-05-27T17:14:09.124242Z","iopub.status.idle":"2022-05-27T17:14:09.130748Z","shell.execute_reply.started":"2022-05-27T17:14:09.124187Z","shell.execute_reply":"2022-05-27T17:14:09.129616Z"},"id":"cRdeJplinktP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(*upd_predictions[0][:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:09.591567Z","iopub.execute_input":"2022-05-27T17:14:09.593853Z","iopub.status.idle":"2022-05-27T17:14:09.600421Z","shell.execute_reply.started":"2022-05-27T17:14:09.593789Z","shell.execute_reply":"2022-05-27T17:14:09.599377Z"},"id":"Pyq8s_QsnktQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Additional & Final Predictions","metadata":{"id":"hoH6cNnsnktQ"}},{"cell_type":"code","source":"origin_predictions = upd_predictions.copy()  # 5. Visualization","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:12.082788Z","iopub.execute_input":"2022-05-27T17:14:12.0831Z","iopub.status.idle":"2022-05-27T17:14:12.086938Z","shell.execute_reply.started":"2022-05-27T17:14:12.083067Z","shell.execute_reply":"2022-05-27T17:14:12.086228Z"},"id":"3X9Ds4F_nktQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === add np.median ===\nadd_preds = []\nfor x in zip(*upd_predictions):\n    add_preds.append(np.median(x, axis=0))\n    \nupd_predictions.append(add_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:12.830431Z","iopub.execute_input":"2022-05-27T17:14:12.831013Z","iopub.status.idle":"2022-05-27T17:14:12.840204Z","shell.execute_reply.started":"2022-05-27T17:14:12.830973Z","shell.execute_reply":"2022-05-27T17:14:12.839234Z"},"id":"8jSkcEeTnktQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === add np.mean ===\nadd_preds = []\nfor x in zip(*upd_predictions):\n    add_preds.append(np.mean(x, axis=0))\n    \nupd_predictions.append(add_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:13.15483Z","iopub.execute_input":"2022-05-27T17:14:13.155511Z","iopub.status.idle":"2022-05-27T17:14:13.161002Z","shell.execute_reply.started":"2022-05-27T17:14:13.155473Z","shell.execute_reply":"2022-05-27T17:14:13.160189Z"},"id":"BmoW7nl1nktR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = np.mean(upd_predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:13.543802Z","iopub.execute_input":"2022-05-27T17:14:13.544328Z","iopub.status.idle":"2022-05-27T17:14:13.548706Z","shell.execute_reply.started":"2022-05-27T17:14:13.544288Z","shell.execute_reply":"2022-05-27T17:14:13.547659Z"},"id":"SKeV2DaLnktR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(*final_predictions[:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:14:15.153992Z","iopub.execute_input":"2022-05-27T17:14:15.154554Z","iopub.status.idle":"2022-05-27T17:14:15.161524Z","shell.execute_reply.started":"2022-05-27T17:14:15.154515Z","shell.execute_reply":"2022-05-27T17:14:15.160522Z"},"id":"iqdUFJppnktR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(*final_predictions[:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:41:41.633424Z","iopub.execute_input":"2022-05-27T16:41:41.633663Z","iopub.status.idle":"2022-05-27T16:41:41.644978Z","shell.execute_reply.started":"2022-05-27T16:41:41.633636Z","shell.execute_reply":"2022-05-27T16:41:41.644236Z"},"id":"IzRciP42nktR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create & Calibrate Submissions","metadata":{"id":"dOBKEVIanktR"}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_df['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:41:41.646198Z","iopub.execute_input":"2022-05-27T16:41:41.646443Z","iopub.status.idle":"2022-05-27T16:41:41.664823Z","shell.execute_reply.started":"2022-05-27T16:41:41.646416Z","shell.execute_reply":"2022-05-27T16:41:41.664105Z"},"id":"7ZAJus9cnktR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:41:41.69362Z","iopub.execute_input":"2022-05-27T16:41:41.693947Z","iopub.status.idle":"2022-05-27T16:41:41.703875Z","shell.execute_reply.started":"2022-05-27T16:41:41.693901Z","shell.execute_reply":"2022-05-27T16:41:41.702919Z"},"id":"kSX5ScPonktS","trusted":true},"execution_count":null,"outputs":[]}]}