{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T05:17:24.891744Z","iopub.execute_input":"2022-05-22T05:17:24.892609Z","iopub.status.idle":"2022-05-22T05:17:24.925541Z","shell.execute_reply.started":"2022-05-22T05:17:24.8925Z","shell.execute_reply":"2022-05-22T05:17:24.924848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom gensim.parsing.preprocessing import preprocess_string\nimport pandas as pd\nimport time\nimport pdb\n\nfrom gensim import corpora\nfrom gensim.models import LdaModel\nfrom gensim.models.coherencemodel import CoherenceModel\nimport matplotlib.pyplot as plt\nfrom gensim.models import LdaModel\nimport pyLDAvis.gensim               #pyLDAvis.gensim_models\nfrom scipy.spatial import distance\n\n#nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:09.693838Z","iopub.execute_input":"2022-05-22T05:21:09.694156Z","iopub.status.idle":"2022-05-22T05:21:51.866974Z","shell.execute_reply.started":"2022-05-22T05:21:09.694125Z","shell.execute_reply":"2022-05-22T05:21:51.866026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ntest = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\ntitle = pd.read_csv('/kaggle/input/cpc-codes/titles.csv')\nsubmission = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:51.869211Z","iopub.execute_input":"2022-05-22T05:21:51.869967Z","iopub.status.idle":"2022-05-22T05:21:52.829004Z","shell.execute_reply.started":"2022-05-22T05:21:51.869915Z","shell.execute_reply":"2022-05-22T05:21:52.828138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_len = len(train)\ntest_len = len(test)\nprint(train_len, test_len)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:52.830114Z","iopub.execute_input":"2022-05-22T05:21:52.830367Z","iopub.status.idle":"2022-05-22T05:21:52.835932Z","shell.execute_reply.started":"2022-05-22T05:21:52.830324Z","shell.execute_reply":"2022-05-22T05:21:52.834893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"frames = [train, test]\nall = pd.concat(frames, keys=['train', 'test'])\nall","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:52.838419Z","iopub.execute_input":"2022-05-22T05:21:52.838745Z","iopub.status.idle":"2022-05-22T05:21:52.884245Z","shell.execute_reply.started":"2022-05-22T05:21:52.838698Z","shell.execute_reply":"2022-05-22T05:21:52.883368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = all.loc['test']\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:52.885399Z","iopub.execute_input":"2022-05-22T05:21:52.885622Z","iopub.status.idle":"2022-05-22T05:21:52.907712Z","shell.execute_reply.started":"2022-05-22T05:21:52.885589Z","shell.execute_reply":"2022-05-22T05:21:52.906766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_LEFT_JOIN = pd.merge(all, title, left_on='context', right_on='code', how='left')\ndf_LEFT_JOIN","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:52.90915Z","iopub.execute_input":"2022-05-22T05:21:52.909714Z","iopub.status.idle":"2022-05-22T05:21:53.071108Z","shell.execute_reply.started":"2022-05-22T05:21:52.909659Z","shell.execute_reply":"2022-05-22T05:21:53.070223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_LEFT_JOIN.drop(['code','section','class','subclass','group','main_group'], axis=1)\ndata = data[['id','anchor','target','context','title','score']]\ndata","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.07253Z","iopub.execute_input":"2022-05-22T05:21:53.072843Z","iopub.status.idle":"2022-05-22T05:21:53.111827Z","shell.execute_reply.started":"2022-05-22T05:21:53.072792Z","shell.execute_reply":"2022-05-22T05:21:53.111053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(d):\n    d = str(d)\n    pattern = r'[^a-zA-Z\\'\\s]'\n    text = re.sub(pattern, '', d)\n    text = text.lower()\n    #text = [word for word in text if len(word)>2]\n    return ' '.join([w for w in text.split() if len(w) > 3])# 3이하인거 버리기\n\ndef preprocessing(d):\n    return preprocess_string(d)\n\ndef data_load(data):\n    anc = data['anchor']\n    tar = data['target']\n\n    anc_df = pd.DataFrame({'text':anc})\n    tar_df = pd.DataFrame({'text':tar})\n    text = [anc_df, tar_df]\n    text_data = pd.concat(text)\n    tokenized_text = text_data['text'].apply(preprocessing)\n    tokenized_text = tokenized_text.to_list()\n    return tokenized_text \n\ndef make_topictable_per_doc(ldamodel, corpus):\n    topic_table = pd.DataFrame()\n    for i, topic_list in enumerate(ldamodel[corpus]):\n        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n        for j, (topic_num, prop_topic) in enumerate(doc):\n            if j == 0:  \n                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n            else:\n                break\n    return(topic_table)\n\ndef create_output(topictable, topicnum):\n    total_topic_list=list()\n    \n    for topic in range(len(topictable[\"각 토픽의 비중\"])):\n        topic_dict=dict()\n        tmp_list = list()\n        for k in range(len(topictable[\"각 토픽의 비중\"][topic])):\n            topic_dict[f'topic_{int(topictable[\"각 토픽의 비중\"][topic][k][0])}'] = float(topictable[\"각 토픽의 비중\"][topic][k][1])\n            tmp_list.append(int(topictable[\"각 토픽의 비중\"][topic][k][0]))\n\n        num_list = [i for i in range(topicnum)]\n        num_list = [i for i in num_list if i not in tmp_list]   ## 있는 토픽 제거\n\n        for n in num_list:\n            topic_dict[f'topic_{n}'] = 0.0      ## 확률 0인 토픽 넣어줌\n        sorted_dict = sorted(topic_dict.items())\n        total_topic_list.append(dict(sorted_dict))\n    return total_topic_list\n        \n#     with open(f\"./output1_to10.json\", 'w') as topic_file:\n#         json.dump(total_topic_list, topic_file, indent='\\t')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.113165Z","iopub.execute_input":"2022-05-22T05:21:53.113396Z","iopub.status.idle":"2022-05-22T05:21:53.132323Z","shell.execute_reply.started":"2022-05-22T05:21:53.113368Z","shell.execute_reply":"2022-05-22T05:21:53.13141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['title'] = data['title'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.133626Z","iopub.execute_input":"2022-05-22T05:21:53.133864Z","iopub.status.idle":"2022-05-22T05:21:53.355326Z","shell.execute_reply.started":"2022-05-22T05:21:53.133833Z","shell.execute_reply":"2022-05-22T05:21:53.354374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"origin_data = data\norigin_data","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.358672Z","iopub.execute_input":"2022-05-22T05:21:53.359023Z","iopub.status.idle":"2022-05-22T05:21:53.377373Z","shell.execute_reply.started":"2022-05-22T05:21:53.358979Z","shell.execute_reply":"2022-05-22T05:21:53.376267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['anchor'] = data['anchor'] + \" \" + data['title']\n# data['target'] = data['target'] + \" \" + data[\"title\"]\n\ndata","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.379295Z","iopub.execute_input":"2022-05-22T05:21:53.37963Z","iopub.status.idle":"2022-05-22T05:21:53.425611Z","shell.execute_reply.started":"2022-05-22T05:21:53.379587Z","shell.execute_reply":"2022-05-22T05:21:53.424757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LDA Modeling","metadata":{}},{"cell_type":"code","source":"doc_token = data_load(data)\nLDA_new_list = doc_token\n\n## Modeling\ndictionary = corpora.Dictionary(doc_token)\ncorpus = [dictionary.doc2bow(text) for text in doc_token] #문서를 bag-of-words 형태로 바꾼것\n\nNUM_TOPICS = 7\nNUM_PASSES = 30\nlda_model =LdaModel(corpus, num_topics = NUM_TOPICS, id2word = dictionary, passes = NUM_PASSES)\ntopics = lda_model.print_topics(num_words = 10)\n    \nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\npyLDAvis.save_html(vis, 'LDAvis_t12_1.html')\n\ntopictable = make_topictable_per_doc(lda_model, corpus)\ntopictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\ntopictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n\ntotal_dist=create_output(topictable, NUM_TOPICS)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:21:53.427401Z","iopub.execute_input":"2022-05-22T05:21:53.427696Z","iopub.status.idle":"2022-05-22T05:32:02.207128Z","shell.execute_reply.started":"2022-05-22T05:21:53.427655Z","shell.execute_reply":"2022-05-22T05:32:02.205998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LDA -> JS distance","metadata":{}},{"cell_type":"code","source":"topictable[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:02.209131Z","iopub.execute_input":"2022-05-22T05:32:02.209975Z","iopub.status.idle":"2022-05-22T05:32:02.233737Z","shell.execute_reply.started":"2022-05-22T05:32:02.209923Z","shell.execute_reply":"2022-05-22T05:32:02.232836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for topic in topics:\n    print(topic)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:02.235484Z","iopub.execute_input":"2022-05-22T05:32:02.236068Z","iopub.status.idle":"2022-05-22T05:32:02.242832Z","shell.execute_reply.started":"2022-05-22T05:32:02.236021Z","shell.execute_reply":"2022-05-22T05:32:02.24179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_dist","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:02.246453Z","iopub.execute_input":"2022-05-22T05:32:02.246772Z","iopub.status.idle":"2022-05-22T05:32:02.534974Z","shell.execute_reply.started":"2022-05-22T05:32:02.246743Z","shell.execute_reply":"2022-05-22T05:32:02.534337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_len = train_len + test_len\nanc_top = total_dist[:total_len]\ntar_top = total_dist[total_len:]\nprint(len(total_dist), len(anc_top), len(tar_top))\ndist_list = list()\nfor i in range(total_len):\n    dist = distance.jensenshannon(list(anc_top[i].values()), list(tar_top[i].values()))\n    dist_list.append(dist)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:02.536429Z","iopub.execute_input":"2022-05-22T05:32:02.536814Z","iopub.status.idle":"2022-05-22T05:32:04.104118Z","shell.execute_reply.started":"2022-05-22T05:32:02.536767Z","shell.execute_reply":"2022-05-22T05:32:04.103223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dist = dist_list[-test_len:]\nlen(test_dist)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:04.105373Z","iopub.execute_input":"2022-05-22T05:32:04.105624Z","iopub.status.idle":"2022-05-22T05:32:04.112642Z","shell.execute_reply.started":"2022-05-22T05:32:04.105592Z","shell.execute_reply":"2022-05-22T05:32:04.111619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dist","metadata":{"execution":{"iopub.status.busy":"2022-05-22T05:32:04.114644Z","iopub.execute_input":"2022-05-22T05:32:04.115044Z","iopub.status.idle":"2022-05-22T05:32:04.125862Z","shell.execute_reply.started":"2022-05-22T05:32:04.114999Z","shell.execute_reply":"2022-05-22T05:32:04.124993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame({'score':test_dist})\ntest_df.fillna(0, inplace=True)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T06:02:07.859063Z","iopub.execute_input":"2022-05-22T06:02:07.859798Z","iopub.status.idle":"2022-05-22T06:02:07.887921Z","shell.execute_reply.started":"2022-05-22T06:02:07.859745Z","shell.execute_reply":"2022-05-22T06:02:07.887091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reverse_score(val):\n    return (1-val)\ntest_df['score'] = test_df['score'].apply(reverse_score)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T06:02:15.156129Z","iopub.execute_input":"2022-05-22T06:02:15.156438Z","iopub.status.idle":"2022-05-22T06:02:15.170114Z","shell.execute_reply.started":"2022-05-22T06:02:15.156407Z","shell.execute_reply":"2022-05-22T06:02:15.169243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_std = scaler.fit_transform(test_df)\nnorm_df = pd.DataFrame(df_std, columns = ['score'])\nsubmission['score'] = norm_df['score']\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-22T06:02:56.057766Z","iopub.execute_input":"2022-05-22T06:02:56.058075Z","iopub.status.idle":"2022-05-22T06:02:56.080708Z","shell.execute_reply.started":"2022-05-22T06:02:56.058039Z","shell.execute_reply":"2022-05-22T06:02:56.079694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T06:03:22.251924Z","iopub.execute_input":"2022-05-22T06:03:22.252389Z","iopub.status.idle":"2022-05-22T06:03:22.259589Z","shell.execute_reply.started":"2022-05-22T06:03:22.252354Z","shell.execute_reply":"2022-05-22T06:03:22.258828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}