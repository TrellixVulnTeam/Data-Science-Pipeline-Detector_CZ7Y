{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:08:48.87729Z","iopub.execute_input":"2022-06-13T13:08:48.877739Z","iopub.status.idle":"2022-06-13T13:08:48.915374Z","shell.execute_reply.started":"2022-06-13T13:08:48.877654Z","shell.execute_reply":"2022-06-13T13:08:48.914576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References for this notbook as:\nhttps://iq.opengenus.org/sentence-semantic-similarity-bert/\nhttps://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/\nhttps://mccormickml.com/2019/07/22/BERT-fine-tuning/\nhttps://skimai.com/fine-tuning-bert-for-sentiment-analysis/\nhttps://keras.io/examples/nlp/semantic_similarity_with_bert/","metadata":{}},{"cell_type":"code","source":"# libraries \n!pip install transformers\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport tensorflow as tf\nimport torch\nfrom transformers import BertTokenizer, TFBertModel","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:08:56.736975Z","iopub.execute_input":"2022-06-13T13:08:56.737552Z","iopub.status.idle":"2022-06-13T13:09:34.155795Z","shell.execute_reply.started":"2022-06-13T13:08:56.737512Z","shell.execute_reply":"2022-06-13T13:09:34.155082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking GPU availability in order to operate on it, otherwise on CPU. \nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0) )\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:39.825971Z","iopub.execute_input":"2022-06-13T13:09:39.826246Z","iopub.status.idle":"2022-06-13T13:09:39.892426Z","shell.execute_reply.started":"2022-06-13T13:09:39.826207Z","shell.execute_reply":"2022-06-13T13:09:39.891403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nPATH = '/kaggle/input/bertbaseuncasedv'\n'''\nBertTokenizer.from_pretrained(\"bert-base-uncased\").save_pretrained(PATH)\nTFBertModel.from_pretrained(\"bert-base-uncased\").save_pretrained(PATH)\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input/bertbaseuncasedv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:42.916467Z","iopub.execute_input":"2022-06-13T13:09:42.916745Z","iopub.status.idle":"2022-06-13T13:09:42.923612Z","shell.execute_reply.started":"2022-06-13T13:09:42.916709Z","shell.execute_reply":"2022-06-13T13:09:42.92292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading training dataset\ntrain_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:47.647221Z","iopub.execute_input":"2022-06-13T13:09:47.647532Z","iopub.status.idle":"2022-06-13T13:09:47.76023Z","shell.execute_reply.started":"2022-06-13T13:09:47.647497Z","shell.execute_reply":"2022-06-13T13:09:47.759482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration on input length of the phrase, bactch_size and epochs applied for compiling the model\nmax_length = 128\nbatch_size = 32\nepochs = 10\n\n# checking for any missing rows in training dataset, then drop those NAN if there are\nprint(\"number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:50.681587Z","iopub.execute_input":"2022-06-13T13:09:50.681905Z","iopub.status.idle":"2022-06-13T13:09:50.746292Z","shell.execute_reply.started":"2022-06-13T13:09:50.681869Z","shell.execute_reply":"2022-06-13T13:09:50.745505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the distribution of our training and test targets.\ntrain_df.context.values\n#test_df.context.values\nprint(\"Train Target Distribution\")\nprint(train_df.context.value_counts())\n#print('\\n\\n')\n#print(\"Test Target Distribution\")\n#print(test_df.context.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:53.649739Z","iopub.execute_input":"2022-06-13T13:09:53.650011Z","iopub.status.idle":"2022-06-13T13:09:53.669653Z","shell.execute_reply.started":"2022-06-13T13:09:53.649982Z","shell.execute_reply":"2022-06-13T13:09:53.668887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grouping the cpc codes into classes of A,B,C,D,E,F,G,H,Y according to CPC classification version 2021.05","metadata":{}},{"cell_type":"code","source":"# CPC labels-configuration\n\n\nA_CPC_labels = [\"A01\", \"A01B\", \"A01C\",\"A01D\", \"A01F\", \"A01F\",\"A01G\",\"A01H\",\"A01J\", \"A01K\", \"A01L\", \"A01M\", \"A01N\", \n                \"A21\", \"A21B\", \"A21C\",\"A21D\", \n                \"A22\", \"A22B\",\"A22C\",\"A022D\",\n                \"A23\", \"A23B\", \"A23C\", \"A23D\",\"A23F\", \"A23G\", \"A23J\",\"A23K\",\"A23L\",\"A23N\", \"A23P\", \"A23V\", \"A23Y\", \n                \"A24\", \"A24B\", \"A24C\", \"A24D\",\"A24F\",\n                \"A41\", \"A41B\", \"A41C\", \"A41D\",\"A41F\", \"A41G\", \"A41J\",\n                \"A42\", \"A42B\", \"A42C\",\n                \"A43\", \"A43B\", \"A43C\", \"A43D\",\"A43F\",\n                \"A44\", \"A44B\", \"A44C\", \"A44D\",\n                \"A45\", \"A45B\", \"A45C\", \"A45D\",\"A45F\",\n                \"A46\", \"A46B\", \"A46D\",\n                \"A47\", \"A47B\", \"A47C\", \"A47D\",\"A47F\", \"A47G\", \"A47H\", \"A47J\", \"A47K\", \"A47L\",\n                \"A61\", \"A61B\", \"A61C\", \"A61D\",\"A61F\", \"A61G\", \"A61H\", \"A61J\", \"A61K\", \"A61L\", \"A61M\", \"A61N\", \"A61P\", \"A61Q\",\n                \"A62\", \"A62B\", \"A62C\", \"A62D\",\n                \"A63\", \"A63B\", \"A63C\", \"A63D\",\"A63F\", \"A63G\", \"A63H\", \"A63J\", \"A63K\",\n                \"A99\", \"A99Z\"]\n\nB_CPC_labels = [\"B01\", \"B01B\", \"B01D\",\"B01F\", \"B01J\", \"B01L\", \n                \"B02\", \"B02B\", \"B02C\",\n                \"B03\", \"B03B\", \"B03C\", \"B03D\",\n                \"B04\", \"B04B\", \"B04C\",\n                \"B05\", \"B05B\", \"B05C\", \"B05D\",\n                \"B06\", \"B06B\",\n                \"B07\", \"B07B\", \"B07C\",\n                \"B08\", \"B08B\",\n                \"B09\", \"B09B\", \"B09C\",\n                \"B21\", \"B21B\", \"B21C\", \"B21D\", \"B21F\", \"B21G\", \"B21H\", \"B21J\",\"B21K\", \"B21L\",  \n                \"B22\", \"B22C\", \"B22D\", \"B22F\",\n                \"B23\", \"B23B\", \"B23C\", \"B23D\", \"B23F\", \"B23G\", \"B23H\", \"B21G\", \"B21H\", \"B21J\",\"B21K\",\n                \"B24\", \"B24B\", \"B24C\", \"B24D\",\n                \"B25\", \"B25B\", \"B25C\", \"B25D\", \"B25F\", \"B25G\", \"B25H\", \"B25J\",\n                \"B26\", \"B26C\", \"B26D\", \"B26F\",\n                \"B27\", \"B27B\", \"B27C\", \"B27D\", \"B27F\", \"B27G\", \"B27H\", \"B27J\",\"B27K\", \"B27L\", \"B27M\", \"B27N\",\n                \"B28\", \"B28B\", \"B28C\", \"B28D\",\n                \"B29\", \"B29B\", \"B29C\", \"B29D\", \"B29K\", \"B29L\",\n                \"B30\", \"B30B\",\n                \"B31\", \"B31B\", \"B31C\", \"B31D\", \"B31F\",\n                \"B32\", \"B32B\",\n                \"B33\", \"B33Y\",\n                \"B41\", \"B41B\", \"B41C\", \"B41D\", \"B41F\", \"B41G\", \"B41J\", \"B41K\", \"B41L\", \"B41M\", \"B41N\", \"B41P\",\n                \"B42\", \"B42B\", \"B42C\", \"B42D\", \"B42F\", \"B42P\",\n                \"B43\", \"B43K\", \"B43L\", \"B43M\",\n                \"B44\", \"B44B\", \"B44C\", \"B44D\", \"B44F\",\n                \"B60\", \"B60B\", \"B60C\", \"B60D\", \"B60F\", \"B60G\", \"B60H\", \"B60J\", \"B60K\", \"B60L\", \"B60M\", \"B60N\", \"B60P\", \"B60Q\", \"B60R\", \"B60S\", \"B60T\", \"B60V\", \"B60W\", \"B60Y\",\n                \"B61\", \"B61B\", \"B61C\", \"B61D\", \"B61F\", \"B61G\", \"B61H\", \"B61J\", \"B61K\", \"B61L\",\n                \"B62\", \"B62B\", \"B62C\", \"B62D\", \"B62H\", \"B62J\", \"B62K\", \"B62L\", \"B62M\",\n                \"B63\", \"B63B\", \"B63C\", \"B63G\", \"B63H\", \"B63J\",\n                \"B64\", \"B64B\", \"B64C\", \"B64D\", \"B64F\", \"B64G\",\n                \"B65\", \"B65B\", \"B65C\", \"B65D\", \"B65F\", \"B65G\", \"B65H\", \n                \"B66\", \"B66B\", \"B66C\", \"B66D\", \"B66F\",\n                \"B67\", \"B67B\", \"B67C\", \"B67D\",\n                \"B67\", \"B68B\", \"B68C\", \"B68F\", \"B68G\",\n                \"B81\", \"B81B\", \"B81C\",\n                \"B82\", \"B82B\", \"B82Y\",\n                \"B99\", \"B99Z\"]\n#B_CPC_labels = list(itertools.chain(B1_CPC_labels, B2_CPC_labels))\n\nC_CPC_labels = [\"C01\", \"C01B\", \"C01C\", \"C01D\", \"C01F\", \"C01G\", \"C01P\", \n                \"C02\", \"C02F\",\n                \"C03\", \"C03B\", \"C03C\",\n                \"C04\", \"C04B\",\n                \"C05\", \"C05B\", \"C05C\", \"C05D\", \"C05F\", \"C05G\",\n                \"C06\", \"C06B\", \"C06C\", \"C06D\", \"C06F\",\n                \"C07\", \"C07B\", \"C07C\", \"C07D\", \"C07F\", \"C07G\", \"C07H\", \"C07J\", \"C07K\",\n                \"C08\", \"C08B\", \"C08C\", \"C08F\", \"C08G\", \"C08H\", \"C08J\", \"C08K\", \"C08L\",\n                \"C09\", \"C09B\", \"C09C\", \"C09D\", \"C09F\", \"C09G\", \"C09H\", \"C09J\", \"C09K\",\n                \"C10\", \"C10B\", \"C10C\", \"C10F\", \"C10G\", \"C10H\", \"C10J\", \"C10K\", \"C10L\", \"C10M\", \"C10N\",\n                \"C11\", \"C11B\", \"C11C\", \"C11D\",\n                \"C12\", \"C12C\", \"C12F\", \"C12G\", \"C12H\", \"C12J\", \"C12L\", \"C12M\", \"C12N\", \"C12P\", \"C12Q\", \"C12R\", \"C12Y\",\n                \"C13\", \"C13B\", \"C13K\",\n                \"C14\", \"C14B\", \"C14C\",\n                \"C21\", \"C21B\", \"C21C\", \"C21D\",\n                \"C22\", \"C22B\", \"C22C\", \"C22F\",\n                \"C23\", \"C23C\", \"C23D\", \"C23F\", \"C23G\",\n                \"C25\", \"C25B\", \"C25C\", \"C25D\", \"C25F\",\n                \"C30\", \"C30B\",\n                \"C40\", \"C40B\",\n                \"C99\", \"C99Z\"]\n\nD_CPC_labels = [\"D01\", \"D01B\", \"D01C\", \"D01D\", \"D01F\", \"D01G\", \"D01H\",\n                \"D02\", \"D02G\", \"D02H\", \"D02J\",\n                \"D03\", \"D03C\", \"D03D\", \"D03J\",\n                \"D04\", \"D04B\", \"D04C\", \"D04D\", \"D04G\", \"D04H\",\n                \"D05\", \"D05B\", \"D05C\", \"D05D\",\n                \"D06\", \"D06B\", \"D06C\", \"D06F\", \"D06G\", \"D06H\", \"D06F\", \"D06L\", \"D06M\", \"D06N\", \"D06P\", \"D06Q\", \n                \"D07\", \"D07B\", \n                \"D10\", \"D10B\", \n                \"D21\", \"D21B\", \"D21C\", \"D21D\", \"D21F\", \"D21G\", \"D21H\", \"D21J\",\n                \"D99\", \"D99Z\" ]\n\n\nE_CPC_labels = [\"E01\", \"E01B\", \"E01C\", \"E01D\", \"E01F\", \"E01H\",\n                \"E02\", \"E02B\", \"E02C\", \"E02D\", \"E02F\",\n                \"E03\", \"E03B\", \"E03C\", \"E03D\", \"E03F\",\n                \"E04\", \"E04B\", \"E04C\", \"E04D\", \"E04F\", \"E04G\", \"E04H\",\n                \"E05\", \"E05B\", \"E05C\", \"E05D\", \"E05F\", \"E05G\", \"E05Y\",\n                \"E06\", \"E06B\", \"E06C\",\n                \"E21\", \"E21B\", \"E21C\", \"E21D\", \"E21F\",\n                \"E99\", \"E99Z\"]\n\n\nF_CPC_labels = [\"F01\", \"F01B\", \"F01C\", \"F01D\", \"F01K\", \"F01L\", \"F01M\", \"F01N\", \"F01P\", \n                \"F02\", \"F02B\", \"F02C\", \"F02D\", \"F02F\", \"F02G\", \"F02K\", \"F02M\", \"F02N\", \"F02P\",\n                \"F03\", \"F03B\", \"F03C\", \"F03D\", \"F03G\", \"F03H\",\n                \"F04\", \"F04B\", \"F04C\", \"F04D\", \"F04F\", \n                \"F05\", \"F05B\", \"F05C\", \"F05D\",\n                \"F15\", \"F15B\", \"F15C\", \"F15D\",\n                \"F16\", \"F16B\", \"F16C\", \"F16D\", \"F16F\", \"F16G\", \"F16H\", \"F16J\", \"F16K\", \"F16L\", \"F16M\", \"F16N\", \"F16P\", \"F16S\", \"F16T\", \n                \"F17\", \"F17B\", \"F17C\", \"F17D\",\n                \"F21\", \"F21H\", \"F21K\", \"F21L\", \"F21S\", \"F21V\", \"F21W\", \"F21Y\", \n                \"F22\", \"F22B\", \"F22D\", \"F22G\",\n                \"F23\", \"F23B\", \"F23C\", \"F23D\", \"F23G\", \"F23H\", \"F23J\", \"F23K\", \"F23L\", \"F23M\", \"F23N\", \"F23Q\", \"F23R\",\n                \"F24\", \"F24B\", \"F24C\", \"F24D\", \"F24F\", \"F24H\", \"F24S\", \"F24T\", \"F24V\", \n                \"F25\", \"F25B\", \"F25C\", \"F25D\", \"F25J\", \n                \"F26\", \"F26B\",\n                \"F27\", \"F27B\", \"F27D\", \"F27M\",\n                \"F28\", \"F28B\", \"F28C\", \"F28D\", \"F28F\", \"F28G\",\n                \"F41\", \"F41A\", \"F41B\", \"F41C\", \"F41F\", \"F41G\", \"F41H\", \"F41J\", \n                \"F42\", \"F42B\", \"F42C\", \"F42D\", \n                \"F99\", \"F99Z\"]\n\nG_CPC_labels = [\"G01\", \"G01B\", \"G01C\", \"G01D\", \"G01F\", \"G01G\", \"G01H\", \"G01J\", \"G01K\", \"G01L\", \"G01M\", \"G01N\", \"G01P\", \"G01Q\", \"G01R\", \"G01S\", \"G01T\", \"G01V\", \"G01W\",  \n                \"G02\", \"G02B\", \"G02C\", \"G02F\", \n                \"G03\", \"G03B\", \"G03C\", \"G03D\", \"G03F\", \"G03G\", \"G03H\", \n                \"G04\", \"G04B\", \"G04C\", \"G04D\", \"G04F\", \"G04G\", \"G04R\", \n                \"G05\", \"G05B\", \"G05D\", \"G05F\", \"G05G\", \n                \"G06\", \"G06C\", \"G06D\", \"G06E\", \"G06F\", \"G06G\", \"G06J\", \"G06K\", \"G06M\", \"G06N\", \"G06Q\", \"G06T\", \n                \"G07\", \"G07B\", \"G07C\", \"G07D\", \"G07F\", \"G07G\",\n                \"G08\", \"G08B\", \"G08C\", \"G08G\", \n                \"G09\", \"G09B\", \"G09C\", \"G09D\", \"G09F\", \"G09G\", \n                \"G10\", \"G10B\", \"G10C\", \"G10D\", \"G10F\", \"G10G\", \"G10H\", \"G10K\", \"G10L\", \n                \"G11\", \"G11B\", \"G11C\", \n                \"G12\", \"G12B\",\n                \"G16\", \"G16B\", \"G16C\", \"G16H\", \"G16Y\", \n                \"G21\", \"G21B\", \"G21C\", \"G21D\", \"G21F\", \"G21G\", \"G21H\", \"G21J\", \"G21K\",   \n                \"G99\", \"G99Z\"]\n\nH_CPC_labels = [\"H01\", \"H01B\", \"H01C\", \"H01F\", \"H01G\", \"H01H\", \"H01J\", \"H01K\", \"H01L\", \"H01M\", \"H01P\", \"H01Q\", \"H01R\", \"H01S\", \"H01T\", \n                \"H02\", \"H02B\", \"H02G\", \"H02H\", \"H02J\", \"H02K\", \"H02M\", \"H02N\", \"H02P\", \"H02S\", \n                \"H03\", \"H03B\", \"H03C\", \"H03D\", \"H03F\", \"H03G\", \"H03H\", \"H03J\", \"H03K\", \"H03L\", \"H03M\", \n                \"H04\", \"H04B\", \"H04H\", \"H04J\", \"H04K\", \"H04L\", \"H04M\", \"H04N\", \"H04Q\", \"H04R\", \"H04S\", \"H04T\", \"H04W\", \n                \"H05\", \"H05B\", \"H05C\", \"H05F\", \"H05G\", \"H05H\", \"H05K\", \n                \"H99\", \"H99Z\"]\n\nY_CPC_labels = [\"Y02\", \"Y02A\", \"Y02B\", \"Y02C\", \"Y02D\", \"Y02E\", \"Y02P\", \"Y02T\", \"Y02W\", \n                \"Y04\", \"Y04S\", \n                \"Y10\", \"Y10S\", \"Y10T\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:09:57.216978Z","iopub.execute_input":"2022-06-13T13:09:57.217263Z","iopub.status.idle":"2022-06-13T13:09:57.256041Z","shell.execute_reply.started":"2022-06-13T13:09:57.217228Z","shell.execute_reply":"2022-06-13T13:09:57.255197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a CPC function used in One-hot encode training, validation, and test labels \n\n\ndef CPC_func(cpc):\n    \n    cpc_A = 1\n    cpc_B = 2\n    cpc_C = 3\n    cpc_D = 4\n    cpc_E = 5\n    cpc_F = 6\n    cpc_G = 7\n    cpc_H = 8\n    cpc_Y = 9\n    \n    if cpc in A_CPC_labels:\n        return cpc_A\n  \n    if cpc in B_CPC_labels:\n        return cpc_B\n    \n    if cpc in C_CPC_labels:\n        return cpc_C\n  \n    if cpc in D_CPC_labels:\n        return cpc_D\n    \n    if cpc in E_CPC_labels:\n        return cpc_E\n  \n    if cpc in F_CPC_labels:\n        return cpc_F\n    \n    if cpc in G_CPC_labels:\n        return cpc_G\n  \n    if cpc in H_CPC_labels:\n        return cpc_H\n    \n    if cpc in Y_CPC_labels:\n        return cpc_Y\n    \n#train_df = shuffle(train_df)\n#train_df.head(-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:02.212709Z","iopub.execute_input":"2022-06-13T13:10:02.212986Z","iopub.status.idle":"2022-06-13T13:10:02.22006Z","shell.execute_reply.started":"2022-06-13T13:10:02.212958Z","shell.execute_reply":"2022-06-13T13:10:02.219113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define labels for checking similarities later\nlabels = [\"cpc_A\", \"cpc_B\",\"cpc_C\", \"cpc_D\", \"cpc_E\", \"cpc_F\", \"cpc_G\", \"cpc_H\", \"cpc_Y\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:05.797623Z","iopub.execute_input":"2022-06-13T13:10:05.798146Z","iopub.status.idle":"2022-06-13T13:10:05.802135Z","shell.execute_reply.started":"2022-06-13T13:10:05.798108Z","shell.execute_reply":"2022-06-13T13:10:05.801347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train_ds into 90% train and 10% val datasets\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data by a percentage\ntrain_df, val_df = train_test_split(train_df, train_size=0.9, test_size=0.1, shuffle=True)\n#print(train_data)\n\n#val_df = val_df.copy()\nprint(\"Shape of new dataframes - {} , {}\".format(train_df.shape, val_df.shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:08.069679Z","iopub.execute_input":"2022-06-13T13:10:08.069935Z","iopub.status.idle":"2022-06-13T13:10:08.179924Z","shell.execute_reply.started":"2022-06-13T13:10:08.069907Z","shell.execute_reply":"2022-06-13T13:10:08.179184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode training, validation labels\n\ntrain_df[\"label\"] = train_df[\"context\"].apply(CPC_func)\n#train_df.to_int()\ny_train = to_categorical(train_df.label-1, num_classes=9, dtype =\"uint8\")\nprint(train_df)\nprint('\\n')\nprint(y_train)\nval_df[\"label\"] = val_df[\"context\"].apply(CPC_func)\n#print(val_df)\n#print(df_val.isnull().sum())\ny_val = to_categorical(val_df.label-1, num_classes = 9, dtype =\"uint8\")\n#print(y_val)\n#print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:11.404089Z","iopub.execute_input":"2022-06-13T13:10:11.40476Z","iopub.status.idle":"2022-06-13T13:10:11.64629Z","shell.execute_reply.started":"2022-06-13T13:10:11.404721Z","shell.execute_reply":"2022-06-13T13:10:11.645136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using tf.keras.utils.Sequence and BertTokenizer+\"bert-base-uncased\" as data genrator\n# for encoding\n\n#from transformers import BertTokenizer\n\nclass BertDataGenerator(tf.keras.utils.Sequence):\n    \n   \n    def __init__(\n        self,\n        phrase_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.phrase_pairs = phrase_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        \n        # BertTokenizer to encode the text using base-base-uncased pretrained model.\n        # \n        self.tokenizer = BertTokenizer.from_pretrained(PATH, local_files_only=True, do_lower_case=True)\n        self.indexes = np.arange(len(self.phrase_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # number of batches per epoch.\n        return len(self.phrase_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        phrase_pairs = self.phrase_pairs[indexes]\n        '''\n        #*** added in trying to turn sharding off\n        phrase_pairs = tf.data.Dataset.from_tensors((phrase_pairs)) \n        phrase_pairs = phrase_pairs.batch(batch_size)\n        options = tf.data.Options()\n        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n        phrase_pairs = phrase_pairs.with_options(options)\n        #************************* end of sharding turn off*****\n        '''\n        # both the sentences are encoded together and separated by [SEP] token\n        # using BertTokenizer's batch_encode_plus \n        encoded = self.tokenizer.batch_encode_plus(\n            phrase_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n        \n        \n        # \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:16.518301Z","iopub.execute_input":"2022-06-13T13:10:16.5186Z","iopub.status.idle":"2022-06-13T13:10:16.530273Z","shell.execute_reply.started":"2022-06-13T13:10:16.518569Z","shell.execute_reply":"2022-06-13T13:10:16.529558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building the model","metadata":{}},{"cell_type":"code","source":"#Build the model\n#\n# Using tf.distribute.MirroredStrategy+transformers.TFBertModel with \"bert-base-uncased\" for the Bert-model.\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # input ids \n    input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    \n    # Attention masks \n    input_masks = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_masks\")\n    \n    # Token type ids \n    input_token_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\")\n    \n    # turning off auto-sharding \n    \n    # BERT with pretrained of 'bert-base-uncased' model.\n    bert_model = TFBertModel.from_pretrained(PATH, local_files_only=True)\n    \n    bert_model.trainable = True\n\n    # last hidden state\n    embeddings = bert_model(input_ids, attention_mask=input_masks, token_type_ids=input_token_ids)[0]\n    \n    #The pooler_output\n    pooled_output = bert_model(input_ids, attention_mask=input_masks, token_type_ids=input_token_ids)[1]\n    \n    # \n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.1, recurrent_dropout=0.2, return_sequences=True))(embeddings)\n    \n    # pooling approach of average and Max on bi_lstm.\n    avg_outp = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_outp = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat_outp = tf.keras.layers.concatenate([avg_outp, max_outp])\n    output = tf.keras.layers.Dropout(0.3)(concat_outp)\n    output = Dense(64, activation = 'relu')(output) # chaged from 128 to 64\n    output = tf.keras.layers.Dropout(0.1)(output) # added in\n    outp = Dense(9, activation=\"softmax\")(output)\n    \n    model = tf.keras.Model(\n        inputs=[input_ids, input_masks, input_token_ids], outputs=outp)\n\n    model.compile(\n        optimizer= Adam(learning_rate=2e-5, epsilon=1e-08), # changed from 1e-5 to 2e-5, deleted clipnorm=1.0\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"])\n    \n\n    \nprint(f\"Strategy: {strategy}\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:21.968044Z","iopub.execute_input":"2022-06-13T13:10:21.968466Z","iopub.status.idle":"2022-06-13T13:10:41.317064Z","shell.execute_reply.started":"2022-06-13T13:10:21.968423Z","shell.execute_reply":"2022-06-13T13:10:41.316388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create train and validation data generators\ntrain_phrase_pairs = train_df[[\"anchor\", \"target\"]].values.astype(\"str\")\n'''\ntrain_phrase_pairs = tf.data.Dataset.from_tensors((train_phrase_pairs)) \ntrain_phrase_pairs = train_phrase_pairs.batch(batch_size)\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\ntrain_phrase_pairs = train_phrase_pairs.with_options(options)\n'''\ntrain_data = BertDataGenerator(\n    train_phrase_pairs,\n    y_train,\n    batch_size=batch_size,\n    shuffle=False,)\n\nval_phrase_pairs = val_df[[\"anchor\", \"target\"]].values.astype(\"str\")\n'''\nval_phrase_pairs = tf.data.Dataset.from_tensors((val_phrase_pairs)) \nval_phrase_pairs = val_phrase_pairs.batch(batch_size)\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\nval_phrase_pairs = val_phrase_pairs.with_options(options)\n'''\nvalid_data = BertDataGenerator(\n    val_phrase_pairs,\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:44.89438Z","iopub.execute_input":"2022-06-13T13:10:44.894816Z","iopub.status.idle":"2022-06-13T13:10:45.00875Z","shell.execute_reply.started":"2022-06-13T13:10:44.894782Z","shell.execute_reply":"2022-06-13T13:10:45.008055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model \n#\nhistory = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:10:48.295052Z","iopub.execute_input":"2022-06-13T13:10:48.295317Z","iopub.status.idle":"2022-06-13T13:11:02.754321Z","shell.execute_reply.started":"2022-06-13T13:10:48.295286Z","shell.execute_reply":"2022-06-13T13:11:02.752869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing the loss and accuracy of training and validation data\n\n#loss    \ntrain_metric = history.history['loss']\nval_metric = history.history['val_' + 'loss']\nepo = range(1, epochs + 1)\nplt.plot(epo, train_metric, 'bo', label='Train ' + 'loss')\nplt.plot(epo, val_metric, 'ro', label='Validation ' + 'loss')\nplt.xlabel('Epoch number')\nplt.ylabel('loss')\nplt.title('Training and Validation ' + 'loss')\nplt.legend()\nplt.show()\n\n#accuracy\ntrain_metric = history.history['accuracy']\nval_metric = history.history['val_' + 'accuracy']\nepo = range(1, epochs + 1)\nplt.plot(epo, train_metric, 'bo', label='Train ' + 'acc')\nplt.plot(epo, val_metric, 'ro', label='Validation ' + 'acc')\nplt.xlabel('Epoch number')\nplt.ylabel('acc')\nplt.title('Training and Validation ' + 'acurracy')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading test dataset\n\ntest_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\n#test_df.shape\n\ntest_df[\"label\"] = test_df[\"context\"].apply(CPC_func)\n#print(test_df)\n#print(test_df['label'].isnull().sum())\ny_test = to_categorical(test_df.label-1, num_classes=9, dtype =\"uint8\")\n#print(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate model on the test set\n#\n\ntest_data = BertDataGenerator(\n    test_df[[\"anchor\", \"target\"]].values.astype(\"str\"),\n    y_test,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.evaluate(test_data, verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inference for checking the similarities on two CPC phrases according to CPC classes of A, B, ..., H, Y\n#\n\n\ndef similarity_eval(test_ds):\n    #*********************trying to turn sharding off******************************\n    #phrase_pairs = test_ds[[\"anchor\", \"target\"]].values.astype(\"str\")\n    test_phrase_pairs = test_ds[[\"anchor\", \"target\"]].values.astype(\"str\")\n    \n    # unable to turn sharding off \n    '''\n    test_phrase_pairs = tf.data.Dataset.from_tensor_slices((test_phrase_pairs)) \n    options = tf.data.Options()\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n    test_phrase_pairs = test_phrase_pairs.with_options(options)\n    '''\n    test_data = BertDataGenerator(\n        test_phrase_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,)\n    print(test_data)\n    \n    #length = len(test_phrase_pairs)\n    proba_pred = []\n    pred_lbl = []\n    y_test_mx = []\n    index = np.arange(len(test_phrase_pairs))\n    \n    for i in index:\n        y_test_max = np.argmax(y_test[i]) # used later in calculating Pearson's CC\n        y_test_mx.append(y_test_max) # used later in calculating Pearson's CC\n        \n        proba = model.predict(test_data[i])[0]\n        #print(proba)\n        #proba_cy = proba.copy()\n        idx = np.argmax(proba)\n        proba = f\"{proba[idx]: .2f}\"\n        pred = labels[idx]    \n        proba_pred.append(proba)\n        pred_lbl.append(pred)\n        \n        results = pd.DataFrame({'pred_label': pred_lbl, 'proba_pred':proba_pred, 'y_test_max':y_test_mx})\n        #print(results)\n    return results\n\n#predicting on test dataset\nresult = similarity_eval(test_df)\nprint(result.pred_label + result.proba_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for calculating Pearson's Correlation coefficient\nimport math\n\n# calculates the mean\ndef mean(x):\n    sum = 0.0\n    for xi in x:\n         sum += xi\n    return sum / len(x) \n\n# calculates the sample standard deviation\ndef StandardDeviation(x):\n    sumv = 0.0\n    for xi in x:\n         sumv += (xi - mean(x))**2\n    return math.sqrt(sumv/(len(x)-1))\n\n# Note this pearson function does not calculate sum(x*y)/(n-1), \n# just calculate the products of xi, yi for each pair(x,y) \ndef pearson(x,y):\n    scorex = []\n    scorey = []\n    n = len(x)\n    for xi, yj in zip(x,y): \n        scorex.append((xi - mean(x))/StandardDeviation(x))\n        \n        scorey.append((yj - mean(y))/StandardDeviation(y))\n        \n        results = (np.array(scorex)*np.array(scorey))#/(n-1)\n    #print(results)\n       \n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from scipy.stats import pearsonr\n\n#def pred_score(y_true, y_pred):\npredictions = result['proba_pred'].astype('float32')\n#predictions = np.array(predictions).flatten()\n    \n#print(predictions)\nprint('\\n')\ny_true = result['y_test_max'].astype('float32') \n    \n#print(y_true) # = y_test_max.append()\n#pred_true = test_df['label'].astype('float32') \n    #pred_true = y_test_max.astype('float32') # one-hot code data\n    #pred_true = pred_true.flatten()# np.array(pred_true).flatten()\n    #print(pred_true)\n\npearson_corr = abs(pearson(y_true, predictions))\npearson_cc = np.reshape(pearson_corr, -1)\n#pearson_corr = num / den #pearsoncr(y_true, y_pred_i)\nscore_pd = pd.DataFrame(pearson_cc, columns = ['pred_score'])\n\n#generating the score for phrases_pairs using np.select\nconditions = [(score_pd['pred_score']  > 0) & (score_pd['pred_score'] <=0.10),\n              (score_pd['pred_score'] > 0.10 ) & (score_pd['pred_score'] <= 0.25 ),\n              (score_pd['pred_score'] > 0.25 ) & (score_pd['pred_score'] <= 0.50 ),\n              (score_pd['pred_score'] > 0.50 ) & (score_pd['pred_score'] <= 0.75 ),\n              (score_pd['pred_score'] > 0.75 ) & (score_pd['pred_score'] <= 1.00 ),\n              (score_pd['pred_score'] > 1 ) \n             ]\nscore_est = ['0', '0.25', '0.5', '0.75', '1.00', '1.25']\nscore_pd['score'] = np.select(conditions, score_est)\nscore_pd.score.astype('float32')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'Id': test_df['id'], 'score':score_pd.score})\n\n#print(submission)\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is fun to learn as well as to improve ML's skill","metadata":{}}]}