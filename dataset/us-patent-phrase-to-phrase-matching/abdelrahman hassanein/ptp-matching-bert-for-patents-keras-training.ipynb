{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer, AutoModel\n\n\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict\n\nimport tensorflow as tf\nfrom transformers import DataCollatorWithPadding\nfrom transformers import TFAutoModelForSequenceClassification\n\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_path = '../input/us-patent-phrase-to-phrase-matching/train.csv'\ntest_csv_path = '../input/us-patent-phrase-to-phrase-matching/test.csv'\n\nmodel_path = '../input/bert-for-patents/bert-for-patents'\nmodel_name = 'anferico/bert-for-patents'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load train file into df\ntrain= pd.read_csv(train_csv_path)\ntest= pd.read_csv(test_csv_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manage dataset","metadata":{}},{"cell_type":"code","source":"class Dataset:\n    \n    def __init__(self, train_dataframe: pd.DataFrame, random_state: int):\n        \n        self.train_dataframe = train_dataframe\n        self.random_state = random_state\n        \n        # Get indecis of df\n        self.idxs = np.arange(len(self.train_dataframe))\n\n        \n    def Create_CV_folds(self, number_folds: int):\n        \n        self.number_folds = number_folds\n        # Initialize stra-kfold object\n        cv = StratifiedGroupKFold(n_splits=self.number_folds)\n        \n        # shuffle train dataframe\n        self.train_dataframe = self.train_dataframe.sample(frac=1, random_state=self.random_state)\n        \n        # cast score to int--> required for StratifiedGroupKFold.split()\n        scores = (self.train_dataframe.score*100).astype(int)\n        \n        # Generate folds \n        folds = list(cv.split(self.idxs, scores, self.train_dataframe.anchor))\n        \n        return folds\n    \n    def get_tokenized_dataset(self, model_path: str):\n        \n        self.model_path = model_path\n        \n        # Initialize tokenizer\n        self.tokz = AutoTokenizer.from_pretrained(self.model_path)\n\n        # Define tokenization func needed by dataset.map()\n        def tok_func(x): return self.tokz(x[\"inputs\"])\n        \n        # create input column\n        self.train_dataframe['inputs'] = self.train_dataframe.context + self.tokz.sep_token + self.train_dataframe.anchor + self.tokz.sep_token + self.train_dataframe.target\n        \n        # Initialize HF dataset object\n        ds = datasets.Dataset.from_pandas(self.train_dataframe).rename_column('score', 'label')\n        \n        # Creating tokenized dataset \n        inps = \"anchor\",\"target\",\"context\"\n        tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id'))\n        \n        return tok_ds\n        \n    def _getfold(self, folds: list,  fold_num: int, tok_ds: datasets.Dataset):\n\n        train,val = folds[fold_num]\n        return DatasetDict({\"train\":tok_ds.select(train), \"test\": tok_ds.select(val)})\n    \n    \n    def get_tf_datasets(self, folds: list, fold_num: int, batch_size: int, tok_ds: datasets.Dataset):\n        \n        data_collator = DataCollatorWithPadding(tokenizer=self.tokz, return_tensors=\"tf\")\n        \n        self._dataset= self._getfold(folds, fold_num, tok_ds)\n        \n        tf_train_dataset = self._dataset[\"train\"].to_tf_dataset(\n            columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n            label_cols=[\"labels\"],\n            shuffle=True,\n            collate_fn=data_collator,\n            batch_size=batch_size,\n        )\n        \n        tf_validation_dataset = self._dataset[\"test\"].to_tf_dataset(\n            columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n            label_cols=[\"labels\"],\n            shuffle=False,\n            collate_fn=data_collator,\n            batch_size=batch_size,\n        )\n        \n        return tf_train_dataset, tf_validation_dataset\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize dataset object\ndataset= Dataset(train, random_state= 42)\n\n# Create validation folds\nfolds= dataset.Create_CV_folds(number_folds= 4)\n\n# create tokenized dataset\ntoc_ds= dataset.get_tokenized_dataset(model_path= model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"number_folds=4\nbatch_size = 64\nnum_epochs = 5\n\nfor fold_num in range(number_folds):\n    # Create tensorflow datasets\n    tf_train_dataset, tf_validation_dataset= dataset.get_tf_datasets(folds, fold_num= fold_num, batch_size=batch_size, tok_ds =toc_ds)\n    \n    # initialize schedular\n    num_train_steps = len(tf_train_dataset) * num_epochs\n    lr_scheduler = PolynomialDecay(\n        initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n    )\n    \n    # Initialzie optimizer\n    opt = Adam(learning_rate=lr_scheduler)\n\n    # Setting callbacks\n    callback_es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                               patience=2,\n                                               baseline=None,\n                                               min_delta = 0.002,\n                                               mode='min', \n                                               verbose=1,\n                                               restore_best_weights=True)\n\n    callback_save = tf.keras.callbacks.ModelCheckpoint(\n        f'./bert-for-patents{fold_num}.h5', monitor='val_loss', \n        verbose=1, save_best_only=True,\n        save_weights_only=True, mode='min', \n        save_freq='epoch')\n\n    \n    # Initialize and compile model\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, from_pt=True)\n    model.compile(optimizer=opt, loss='mse', metrics='mse')\n    \n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_num} ...')\n    \n    history = model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=10, callbacks=[callback_es, callback_save])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}