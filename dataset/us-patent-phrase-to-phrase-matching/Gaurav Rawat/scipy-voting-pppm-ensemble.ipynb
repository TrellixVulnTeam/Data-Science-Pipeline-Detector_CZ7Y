{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\nimport json\nimport numpy as np\n\nINPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"id":"fa3b873b","papermill":{"duration":0.041313,"end_time":"2022-03-22T09:40:01.526545","exception":false,"start_time":"2022-03-22T09:40:01.485232","status":"completed"},"tags":[],"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:05:45.500148Z","iopub.execute_input":"2022-06-20T15:05:45.500525Z","iopub.status.idle":"2022-06-20T15:05:45.506838Z","shell.execute_reply.started":"2022-06-20T15:05:45.500482Z","shell.execute_reply":"2022-06-20T15:05:45.505483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble PPME","metadata":{}},{"cell_type":"markdown","source":"# CFG","metadata":{"id":"1d0c4430","papermill":{"duration":0.024609,"end_time":"2022-03-22T09:40:01.576366","exception":false,"start_time":"2022-03-22T09:40:01.551757","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG_t:\n    num_workers=4\n    path=\"../input/patent-v3-tx/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-deb-v3_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    lowercase=False\n    task=\"classification\"\n    ds = \"baseline\"\n    lb = 0.83430\n    wt=1\n\n# MDRpts    \nclass CFG_mdrpts:\n    num_workers=4\n    path=\"../input/patent-v3-mdrpts/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-deb-v3_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=False\n    task=\"classification\"\n    ds = \"baseline\"\n    lb = 0.8365\n    wt=1.2\n\n# Concat    \nclass CFG_concat:\n    num_workers=4\n    path=\"../input/patent-v3-concat/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-deb-v3_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=False    \n    task=\"classification\"\n    ds = \"baseline\" \n    lb= 0.834301\n    wt=1\n\n# Patentbert\nclass CFG_pbert:\n    num_workers=4\n    path=\"../input/ppme-pubbert-t/\"\n    config_path=path+'config.pth'\n    model=\"../input/bert-for-patents/bert-for-patents\"\n    model_file=\"anferico-bert-for-patents-patentbert_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=94\n    seed=42\n    n_fold=4\n    eps=1e-6\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True    \n    task=\"classification\"\n    ds = \"phrase\"\n    lb= 0.8260\n    wt=1/3\n\nclass CFG_pb_awp:\n    num_workers=4\n    path=\"../input/pub-an-awp/\"\n    config_path=path+'config.pth'\n    model=\"../input/bert-for-patents/bert-for-patents\"\n    model_file=\"anferico-bert-for-patents-pb_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=117\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True    \n    task=\"classification\"\n    ds = \"baseline\" \n    lb= 0.8256\n    wt=1/3 \n\n# mse loss\nclass CFG_reg:\n    num_workers=4\n    path=\"../input/ppme-cls-mse/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-attn-sig_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=1654283144\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True\n    eps=1e-6\n    task=\"regression\"\n    ds = \"baseline\"\n    lb = 0.8348\n    wt=1.05 \n    \nclass CFG_coco:\n    num_workers=4\n    path=\"../input/ppme-coco-mean/\"\n    config_path=path+'config.pth'\n    model=\"../input/cocolmlargeweight\"\n    model_file=\"microsoft-cocolm-large-coco-mean_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.05\n    target_size=1\n    max_len=170\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-7\n    task=\"regression\"\n    ds=\"baseline\"\n    lb = 0.825601\n    wt=1/5\n    \nclass CFG_coco_v2:\n    num_workers=4\n    path=\"../input/ppme-coco-v2/\"\n    config_path=path+'config.pth'\n    model=\"../input/cocolmlargeweight\"\n    model_file=\"microsoft-cocolm-large-coco_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.15\n    target_size=1\n    max_len=170\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"regression\"\n    ds=\"baseline\"    \n    lb=0.8307\n    wt=1.02\n    \nclass CFG_coco_BCE:\n    num_workers=4\n    path=\"../input/ppme-coco-bce/\"\n    config_path=path+'config.pth'\n    model=\"../input/cocolmlargeweight\"\n    model_file=\"microsoft-cocolm-large-coco-mean-v2_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.15\n    target_size=1\n    max_len=170\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\"    \n    lb=0.8332\n    wt=1.02    \n    \n# mse loss Guang \nclass CFG_BERT_SIMPlE:\n    num_workers=4\n    path=\"../input/uspatentbertlargesimple/\"\n    config_path=path+'config.pth'\n    model=\"../input/bert-for-patents/bert-for-patents\"\n    #model=\"../input/uspatentbertlargesimple\"\n    model_file = 'anferico_bert-for-patents_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\"\n    lb= 0.82501\n    wt=1/3\n\nclass CFG_DEB_SIMPLE:\n    num_workers=4\n    path=\"../input/us-patent-deberta-simple/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file = 'microsoft_deberta-v3-large_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\"    \n    lb= 0.8389\n    wt=1.4\n\nclass CFG_BERT_MEAN:\n    num_workers=4\n    path=\"../input/uspatentbertmean/\"\n    config_path=path+'config.pth'\n    model=\"../input/uspatentbertlargecomplex\"\n    model_file = 'anferico_bert-for-patents_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    num_workers = 2\n    task=\"guang\"\n    ds = \"guangds\"     \n    lb= 0.8239\n    wt=1/4\n\nclass CFG_DEB_MEAN:\n    num_workers=4\n    path=\"../input/uspatent-deb-mean-v2/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file =  'microsoft_deberta-v3-large_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\"  \n    lb= 0.8387\n    wt=1.2\n\nclass CFG_BERT_LG_COMPLEX:\n    num_workers=4\n    path=\"../input/uspatent-deberta-v3-large-complex/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file =  'microsoft_deberta-v3-large_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\"   \n    lb= 0.837\n    wt=1.1\n    \nclass CFG_XL_MEAN:\n    num_workers=4\n    path=\"../input/uspatentxlmeanv2/xl-complex/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v2-xlarge\"\n    model_file =  'microsoft_deberta-v2-xlarge_best'\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\" \n    lb= 0.8159\n    wt=1/8\n\nclass CFG_XL_ELC:\n    num_workers=4\n    path=\"../input/uspatent-elc-mean/\"\n    config_path=path+'config.pth'\n    model=\"../input/electra-large-discriminator\"\n    model_file =  '/google_electra-large-discriminator_best'\n    grad_checkpoint=False\n    batch_size=64 \n    weight_decay = 0.01\n    learning_rate = 2e-5\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\"\n    lb= 0.8257\n    wt=1/5\n\nclass CFG_FT_MEAN:\n    num_workers=4\n    path=\"../input/uspatent-tf-mean/\"\n    config_path=path+'config.pth'\n    model=\"../input/funneltransformerlarge\"\n    model_file =  'funnel-transformer_large_best'\n    grad_checkpoint=False\n    batch_size=64 \n    weight_decay = 0.01\n    learning_rate = 2e-5\n    target_size=1\n    max_len=130\n    seed=42\n    n_fold=4\n    num_workers = 2\n    trn_fold=[0, 1, 2, 3]   \n    task=\"guang\"\n    ds = \"guangds\" \n    lb= 0.828\n    wt=1/2.9\n\n# Xiaoyi\nclass CFG_BCE_ATTN_PL:\n    num_workers=4\n    path=\"../input/bce-v3-attn-pool/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-deb-v3_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\"\n    lb= 0.8352\n    wt= 1.1\n\nclass CFG_BCE_mean_v2:\n    num_workers=4\n    path=\"../input/bce-v3-mean-v2/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-deb-v3_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\" \n    lb= -1\n    \nclass CFG_FT_ATTN:\n    num_workers=4\n    path=\"../input/ppme-fn-cl/\"\n    config_path=path+'config.pth'\n    model=\"../input/funneltransformerlarge\"\n    model_file=\"funnel-transformer-large-funnel-lg_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.15\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\"    \n    lb= 0.8273\n    wt=1/3\n    \nclass CFG_DEBV3_ATTN:\n    num_workers=4\n    path=\"../input/mse-attn-pool/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-v3-large_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"regression\"\n    ds=\"baseline\"\n    lb= 0.8352\n    wt=1.2\n\nclass CFG_MSE_S_X:\n    num_workers=4\n    path=\"../input/mse-v3-simple/\"\n    config_path=path+'config.pth'\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    model_file=\"microsoft-deberta-v3-large-v3-large_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"regression\"\n    ds=\"baseline\" \n    lb = 0.8336\n    wt=1.1\n\n    \nclass CFG_SIMSCE:\n    num_workers=4\n    path=\"../input/ppme-cse/\"\n    config_path=path+'config.pth'\n    model=\"../input/robl-sig-rev/simcse-bert-for-patent\"\n    model_file=\"Yanhao-simcse-bert-for-patent-simsce_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\" \n    lb = 0.8090\n    wt=1/19\n\nclass CFG_ELEC_ATTN:\n    num_workers=4\n    path=\"../input/mse-electra-attn/\"\n    config_path=path+'config.pth'\n    model=\"../input/electra-large-discriminator\"\n    model_file=\"google-electra-large-discriminator-electra-large_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"regression\"\n    ds=\"baseline\" \n    lb = 0.8311 \n    wt=1.09\n\nclass CFG_SIMSCE_X:\n    num_workers=4\n    path=\"../input/bcesimsceattn/\"\n    config_path=path+'config.pth'\n    model=\"../input/robl-sig-rev/simcse-bert-for-patent\"\n    model_file=\"Yanhao-simcse-bert-for-patent-simsce_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True     \n    eps=1e-6\n    task=\"classification\"\n    ds=\"baseline\" \n    lb = 0.8131\n    wt=1/7\n    \nclass CFG_BCE_BERT_X:\n    num_workers=4\n    path=\"../input/bce-bert-attn/\"\n    config_path=path+'config.pth'\n    model=\"../input/bert-for-patents/bert-for-patents\"\n    model_file=\"anferico-bert-for-patents-patents-1_fold\"\n    grad_checkpoint=False\n    batch_size=64\n    fc_dropout=0.2\n    target_size=1\n    max_len=94\n    seed=42\n    n_fold=4\n    eps=1e-6\n    trn_fold=[0, 1, 2, 3]   \n    lowercase=True    \n    task=\"classification\"\n    ds = \"baseline\"\n    lb= 0.8180\n    wt=1/2\n    \nMODELS = { \n         }\n#All available models | Removing CFG_coco\nall_configs =  all_configs =  [CFG_mdrpts,CFG_concat,CFG_t,CFG_pbert,#CFG_coco, \n                                CFG_coco_v2,CFG_coco_BCE,CFG_BERT_LG_COMPLEX,\n                                CFG_BERT_SIMPlE,CFG_DEB_SIMPLE,#CFG_BERT_MEAN,\n                                CFG_DEB_MEAN,CFG_XL_ELC,CFG_XL_MEAN,\n                                CFG_FT_MEAN,CFG_reg,#CFG_BCE_mean_v2,\n                                CFG_BCE_ATTN_PL,CFG_pb_awp, CFG_FT_ATTN,CFG_FT_MEAN, #CFG_DEBV3_ATTN,\n                                CFG_MSE_S_X,#CFG_SIMSCE\n                                CFG_ELEC_ATTN,CFG_SIMSCE_X,CFG_BCE_BERT_X\n                               ]  \nconfigs = all_configs\ntop_mods = {}\nnon_g = {} \nw= None\nfor i in range(len(configs)):\n    locals()[f\"CFG_{i}\"] = configs[i] \n    MODELS[configs[i]().__class__.__name__ ] = configs[i]\n    lc = locals()[f\"CFG_{i}\"] \n    if lc.lb > -1.0:\n        top_mods[lc.lb] = lc\n    if lc.task !=  \"guang\":\n        non_g[configs[i]().__class__.__name__ ] = configs[i]\n    print(f\"Added config index [{configs[i]().__class__.__name__}] [{i}] {lc.path} LB [{lc.lb}]\")\n\nw = None\nprint(\"Weights Used \",w)    \n\nSELECT_XG = False\nif SELECT_XG:\n    for key,cfg in  configs.items():\n        locals()[key] = cfg\n        MODELS[key] = cfg  ","metadata":{"id":"48dd82bb","papermill":{"duration":0.033949,"end_time":"2022-03-22T09:40:01.634977","exception":false,"start_time":"2022-03-22T09:40:01.601028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-20T15:05:45.637127Z","iopub.execute_input":"2022-06-20T15:05:45.637616Z","iopub.status.idle":"2022-06-20T15:05:45.731027Z","shell.execute_reply.started":"2022-06-20T15:05:45.637576Z","shell.execute_reply":"2022-06-20T15:05:45.729748Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONSTANTS","metadata":{}},{"cell_type":"code","source":"################## CONSTANTS  ##################   \nN_JOBS = 2\nFOLDS = 4\nDEBUG = False\nCOMPUTE_CV = True\nFOLD_SCORE = False\nFIND_BEST_MODELS = False\nOPTIMIZE = True\nSCIPY_OPTIMIZE = False\nADD_POSTPROCESSING = False\nAPPLY_SCALAR = False\n\nENSEMBLE_STRATEGY = \"regression\"#\"regression\"#\"regression\"#\"weighted\" # \"average\"\ntrain_file = f\"../input/folds-dump-the-two-paths/train_folds_mstrat_{FOLDS}.csv\"\nprint(f\"\"\"\\n sorted by lb {  [ val().__class__.__name__ for key, val in sorted(top_mods.items(), reverse=True)] } \\n scores\n      {[key for key, val in sorted(top_mods.items(), reverse=True)] }\"\"\")\nno_of_models = len(configs)\nprint(f\"\\n Ensembling models [{no_of_models}] with folds [{FOLDS}]\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:05:45.734584Z","iopub.execute_input":"2022-06-20T15:05:45.735388Z","iopub.status.idle":"2022-06-20T15:05:45.749601Z","shell.execute_reply.started":"2022-06-20T15:05:45.735332Z","shell.execute_reply":"2022-06-20T15:05:45.748283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library 📚","metadata":{"id":"f2ed8ef2","papermill":{"duration":0.038261,"end_time":"2022-03-22T09:40:10.626926","exception":false,"start_time":"2022-03-22T09:40:10.588665","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#  Add Coco Lm libs\nimport sys\nsys.path.insert(0,\"../input/cocolm-large-libs/huggingface\")\n# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nfrom joblib import Parallel, delayed\nimport threading\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers --quiet')\nos.system('pip uninstall -y tokenizers --quiet')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers --quiet')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers --quiet')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\n# Ensemble regressors\nfrom sklearn.linear_model import Ridge, LinearRegression, ElasticNet, SGDRegressor, HuberRegressor, BayesianRidge\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"executionInfo":{"elapsed":20123,"status":"ok","timestamp":1644920080956,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"35916341","outputId":"06fa0ab8-a380-4f54-a98d-b7015b79d9e2","papermill":{"duration":26.143536,"end_time":"2022-03-22T09:40:36.798853","exception":false,"start_time":"2022-03-22T09:40:10.655317","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:05:45.752133Z","iopub.execute_input":"2022-06-20T15:05:45.753261Z","iopub.status.idle":"2022-06-20T15:06:14.878794Z","shell.execute_reply.started":"2022-06-20T15:05:45.753207Z","shell.execute_reply":"2022-06-20T15:06:14.877734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\ndef get_result(oof_df,col ='pred'):\n    labels = oof_df['score'].values\n    preds = oof_df[col].values\n    score = get_score(labels, preds)\n    return score","metadata":{"id":"d5c0ccc6","papermill":{"duration":0.21551,"end_time":"2022-03-22T09:40:37.116848","exception":false,"start_time":"2022-03-22T09:40:36.901338","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:06:14.881011Z","iopub.execute_input":"2022-06-20T15:06:14.881389Z","iopub.status.idle":"2022-06-20T15:06:14.897151Z","shell.execute_reply.started":"2022-06-20T15:06:14.88134Z","shell.execute_reply":"2022-06-20T15:06:14.895686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv(train_file)\n\ndef create_fold_oof(fold):\n    print(f\"Processing fold [{fold}] Thread Id [{threading.currentThread().ident}] \\n\")\n    _oof_df = train[train.fold == fold].reset_index(drop=True) \n    for i,config in MODELS.items():\n        #config = globals()[f\"CFG_{i}\"]\n        predictions = torch.load(f\"{config.path}{config.model_file}{fold}{'' if config.task == 'guang' else '_best'}.pth\", \n                                 map_location=torch.device('cpu'))['predictions']  \n        MMscaler = MinMaxScaler()\n        if config.task == \"guang\" or config.task == \"regression\" and APPLY_SCALAR:\n            _oof_df[f\"pred_{i}\"]=  MMscaler.fit_transform(predictions.reshape(-1,1)).reshape(-1) #if APPLY_SCALAR else predictions\n        else:   \n            _oof_df[f\"pred_{i}\"]= MMscaler.fit_transform(predictions.reshape(-1,1)).reshape(-1) if APPLY_SCALAR else predictions\n        del predictions;gc.collect()    \n    return _oof_df\n\ndef create_oof():\n    oof_df = pd.DataFrame()\n    results = Parallel(n_jobs=N_JOBS, backend=\"multiprocessing\")(\n        delayed(create_fold_oof)(fold) for fold in range(FOLDS) )\n    for result in results:\n        oof_df = pd.concat([oof_df, result])  \n    del results;gc.collect()    \n    return oof_df\n\noof = pd.DataFrame()\nif ENSEMBLE_STRATEGY == \"regression\" or ENSEMBLE_STRATEGY == \"SGDRegressor\" or COMPUTE_CV :\n    oof = create_oof() \n    display(oof[[col for col in oof.columns if col.startswith('pred') or col == 'fold'\n     or col == \"score\"]].head())","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:06:14.903298Z","iopub.execute_input":"2022-06-20T15:06:14.903711Z","iopub.status.idle":"2022-06-20T15:17:54.792325Z","shell.execute_reply.started":"2022-06-20T15:06:14.903661Z","shell.execute_reply":"2022-06-20T15:17:54.790011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thresholds tuning","metadata":{}},{"cell_type":"code","source":"thresholds_dict = {\n'0': 0.02,\n'.25': (0.24, 0.26),\n'.50': (0.49, 0.51),\n'.75': (0.74, 0.76),\n'1': 0.98\n}\n    \ndef _upd_score_between(data, thresholds, value):\n    \"\"\"\\o/\"\"\"\n    mask_th = data.between(*thresholds, inclusive='both')\n    data[mask_th] = value\n\n\ndef upd_score(data, th_dict=None):\n    \"\"\"\\o/\"\"\"\n    if isinstance(data, pd.Series):\n        result = data.copy()\n    else:\n        return data\n\n    if not th_dict:        \n        th_dict = {\n            '0': 0.02,\n            '.25': (0.24, 0.26),\n            '.50': (0.49, 0.51),\n            '.75': (0.74, 0.76),\n            '1': 0.98\n        }\n\n    if isinstance(th_dict, dict):    \n        th0 = th_dict.get('0')\n        th25 = th_dict.get('.25')\n        th50 = th_dict.get('.50')\n        th75 = th_dict.get('.75')\n        th100 = th_dict.get('1')\n    else:\n        return data\n    \n    if th0:\n        if isinstance(th0, float):\n            th0 = (result.min(), th0)\n        \n        if isinstance(th0, tuple):\n            _upd_score_between(result, th0, 0)\n    \n    if th25 and isinstance(th25, tuple):\n        _upd_score_between(result, th25, 0.25)\n\n    if th50 and isinstance(th50, tuple):\n        _upd_score_between(result, th50, 0.50)\n            \n    if th75 and isinstance(th75, tuple):\n        _upd_score_between(result, th75, 0.75)\n            \n    if th100:\n        if isinstance(th100, float):\n            th100 = (th100, result.max())\n        \n        if isinstance(th100, tuple):\n            _upd_score_between(result, th100, 1)\n\n    return result","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:54.795628Z","iopub.execute_input":"2022-06-20T15:17:54.796003Z","iopub.status.idle":"2022-06-20T15:17:54.820762Z","shell.execute_reply.started":"2022-06-20T15:17:54.795951Z","shell.execute_reply":"2022-06-20T15:17:54.819381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler \ndef get_fold_oof(fold,features_cols,w):\n    #print(f\"Calc CV for fold [{fold}] Thread Id [{threading.currentThread().ident}] \\n\")\n    oof =  globals()[\"oof\"]\n    _oof_df = oof[oof.fold == fold].reset_index(drop=True) \n    preds = np.zeros(len(_oof_df))\n    index = 0\n    for i  in features_cols: \n        predictions = _oof_df[i]  \n        preds += predictions * w[index]\n        index +=1\n    _oof_df['pred'] = preds\n    if FOLD_SCORE:\n        get_result(_oof_df)\n        print(f\"Fold [{fold}] Score is [{get_result(_oof_df)}]\") \n    return _oof_df\n\ndef checkensemble_oof(w=[1/no_of_models for i in range(no_of_models)],features_cols =[col for col in oof.columns if col.startswith('pred')]\n                      , FOLD_SCORE = False):\n    \"\"\" Ensemble OOF defaults to average weights\"\"\"\n    oof_df = pd.DataFrame()\n    results = Parallel(n_jobs=N_JOBS, backend=\"multiprocessing\")(\n    delayed(get_fold_oof)(fold,features_cols,w) for fold in range(FOLDS) )\n    for result in results:\n        oof_df = pd.concat([oof_df, result]) \n    oof_df = oof_df.reset_index(drop=True)   \n    score = get_result(oof_df)\n    #print(f\"Ensemble Score [weighted] is [{get_result(oof_df)}]\")\n    del oof_df; gc.collect()\n    return score\n\ndef ensemble_oof(w=[1/no_of_models for i in range(no_of_models)],\n                 features_cols =[col for col in oof.columns if col.startswith('pred')]\n                      , FOLD_SCORE = False, display_oof = False):\n    \"\"\" Ensemble OOF defaults to average weights\"\"\" \n    predictions = np.zeros(oof['score'].to_numpy().shape)\n    index = 0     \n    oof_df = oof.copy() \n    for i in features_cols: \n        predictions += oof_df[i] * w[index]\n        index += 1 \n    MinMaxScala  = MinMaxScaler()\n    oof_df['pred'] = predictions \n    if APPLY_SCALAR:\n        oof_df['pred'] = MinMaxScala.fit_transform(oof_df['pred'].values.reshape(-1, 1))#upd_score(predictions, thresholds_dict) \n    score = get_result(oof_df) \n    if display_oof:\n        display(oof_df[['pred']+[col for col in oof.columns if col.startswith('pred') or col == 'pred' or col == \"score\"]])\n        del oof_df; gc.collect()\n    return score","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:54.824147Z","iopub.execute_input":"2022-06-20T15:17:54.824407Z","iopub.status.idle":"2022-06-20T15:17:54.849053Z","shell.execute_reply.started":"2022-06-20T15:17:54.824361Z","shell.execute_reply":"2022-06-20T15:17:54.847665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate CV ⚖️","metadata":{}},{"cell_type":"code","source":"# Get cv for each model\nif COMPUTE_CV:\n    scores = {}\n    for i,config in MODELS.items(): \n        col = f\"pred_{i}\"\n        scr = get_result(oof[['score',col]],col)\n        scores[scr] = i\n        config.cv = scr\n    print(json.dumps(dict(sorted(scores.items())), indent=4))    ","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:54.850951Z","iopub.execute_input":"2022-06-20T15:17:54.851573Z","iopub.status.idle":"2022-06-20T15:17:54.969316Z","shell.execute_reply.started":"2022-06-20T15:17:54.851525Z","shell.execute_reply":"2022-06-20T15:17:54.967628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scipy 👨🏼‍🔬","metadata":{}},{"cell_type":"code","source":"#%%time\nfrom scipy import optimize\ndef scipy_opt(no_mods,features_cols=[f\"pred_{key}\" for key in MODELS.keys()]):  \n    def scorer_sc(w): \n        _w = []\n        # use def LB and normal weights \n        #_w = w*df_wts \n        loss  = ensemble_oof(w,features_cols) \n        gc.collect()\n        return loss*-1\n\n    tol = 1e-10\n    #total = sum ([m.cv for nm,m in models.items()]) \n    #init_weights = [ m.cv/(total) for nm,m in models.items() ]\n    # Average Weights as Default\n    init_weights = [ 1/(no_mods) for i in range(no_mods) ]\n    print(f\"Number of models {no_mods} Leaderboard Weights {init_weights} \\n\")\n    result = optimize.minimize(scorer_sc,\n                                    init_weights,\n                                    #constraints=({'type': 'eq','fun': lambda w: 1-sum(w),'jac': lambda x: [1] * len(x)}),\n                                    method= 'Nelder-Mead',#'Nelder-Mead', #'SLSQP',\n                                    #bounds=[(0.0, 1.0)] * len(model_list),\n                                    #options = {'ftol':1e-10},\n                                    #tol=tol\n                                ) \n    w = result['x']\n    print(f'Optimum weights = {w} [{sum(w)}]\\n') \n    print('With CV =', result['fun']) \n    return -1*result['fun'],w ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:54.976298Z","iopub.execute_input":"2022-06-20T15:17:54.976796Z","iopub.status.idle":"2022-06-20T15:17:54.999867Z","shell.execute_reply.started":"2022-06-20T15:17:54.97674Z","shell.execute_reply":"2022-06-20T15:17:54.998513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta model trainer 👟","metadata":{}},{"cell_type":"code","source":"def meta_model_trainer(name,reg,folds,_X,_Y,test,features,\n                       validate_only = False,\n                       earlyStoppingRounds=None,\n                       TARGET_VAR=\"score\"):\n    scores = []\n    final_valid_predictions = []\n    final_valid_labels = []\n    final_preds = None\n    final_test_predictions = []\n    for i in  range(folds):\n        model = reg(i)\n        X_train = _X[_X.fold != i].reset_index(drop=True)\n        X_valid = _X[_X.fold == i].reset_index(drop=True)\n        train_data = X_train[features]\n        val_data = X_valid[features] \n        train_labels, val_labels = X_train[TARGET_VAR].to_numpy(), X_valid[TARGET_VAR].to_numpy()\n        if earlyStoppingRounds:\n            model.fit(train_data, train_labels,early_stopping_rounds=earlyStoppingRounds,eval_set=[(val_data, val_labels)],verbose=False)\n        else:\n            model.fit(train_data, train_labels)\n        if not validate_only:\n            test_preds = model.predict(test[features])\n            final_test_predictions.append(test_preds)\n        preds_valid = model.predict(val_data) \n        final_valid_labels.append(val_labels)\n        if ADD_POSTPROCESSING:\n            preds_valid = np.where(preds_valid<=0, 0, preds_valid)\n            preds_valid = np.where(preds_valid>=1, 1, preds_valid)\n        final_valid_predictions.append(preds_valid)\n        foldStat = get_score(val_labels,preds_valid) \n        scores.append(foldStat) \n        #print(f\"Fold {i} score {foldStat}\")\n    final_Score = get_score(np.concatenate(final_valid_labels), np.concatenate(final_valid_predictions)) \n    if not validate_only:\n        final_preds = np.mean(np.column_stack(final_test_predictions), axis=1)\n    else:\n        print(f\"Ensemble Score [{name}] :\",final_Score) \n    return final_preds,final_Score","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:55.006484Z","iopub.execute_input":"2022-06-20T15:17:55.007008Z","iopub.status.idle":"2022-06-20T15:17:55.034479Z","shell.execute_reply.started":"2022-06-20T15:17:55.006956Z","shell.execute_reply":"2022-06-20T15:17:55.033157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test CV OOF via Regressors\ndef get_regressor():\n    gbr = GradientBoostingRegressor(random_state=42)\n    huber= HuberRegressor(epsilon=1.35)\n    hgbr = HistGradientBoostingRegressor(random_state=42)\n    return VotingRegressor([('lr', Ridge()), ('rf', LinearRegression()),('r3', hgbr) \n                           # ,('r4', huber) \n                           ])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:17:55.036637Z","iopub.execute_input":"2022-06-20T15:17:55.037079Z","iopub.status.idle":"2022-06-20T15:17:55.049498Z","shell.execute_reply.started":"2022-06-20T15:17:55.03703Z","shell.execute_reply":"2022-06-20T15:17:55.047937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Group Models  ","metadata":{}},{"cell_type":"code","source":"MODELS_GRP = {}\nfor key,mod in MODELS.items():\n    if mod.model in MODELS_GRP.keys():\n        MODELS_GRP[mod.model] += [mod]\n    else:\n        MODELS_GRP[mod.model] = [mod]\n        \nMODELS_GRP","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:17:55.05954Z","iopub.execute_input":"2022-06-20T15:17:55.060093Z","iopub.status.idle":"2022-06-20T15:17:55.073401Z","shell.execute_reply.started":"2022-06-20T15:17:55.060053Z","shell.execute_reply":"2022-06-20T15:17:55.070566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof=oof.drop([col for col in  oof.columns if col.startswith('r_pred') ],axis=1)\nif OPTIMIZE:\n    i = 1 \n    for grp,mods in MODELS_GRP.items():\n        fc = [f\"pred_{mod().__class__.__name__}\" for mod in mods]\n        feature_cols = [col for col in oof.columns if col in fc]\n        pred,score = meta_model_trainer(\"Regressor\",lambda fold: get_regressor(),FOLDS,\n                           _X=oof,\n                           _Y=oof[[\"score\"]],\n                           test = oof,\n                           features = feature_cols,\n                           validate_only = False)\n        oof[f\"r_pred_{os.path.split(grp)[1]}\"]= pred\n        print(f\"Checking ridge ensemble {i} {grp} {feature_cols} {score}\")\n        i+=1\n      \n    reg_cols = [col for col in  oof.columns if col.startswith('r_pred') ]    \n#oof.head()    ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:17:55.074834Z","iopub.execute_input":"2022-06-20T15:17:55.075493Z","iopub.status.idle":"2022-06-20T15:18:15.272428Z","shell.execute_reply.started":"2022-06-20T15:17:55.075443Z","shell.execute_reply":"2022-06-20T15:18:15.269151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# scipy for above models\nif OPTIMIZE:\n    score,w =scipy_opt(len(reg_cols), [col for col in  oof.columns if col.startswith('r_pred') ] ) \nw","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:18:15.274168Z","iopub.execute_input":"2022-06-20T15:18:15.274777Z","iopub.status.idle":"2022-06-20T15:21:05.391626Z","shell.execute_reply.started":"2022-06-20T15:18:15.274729Z","shell.execute_reply":"2022-06-20T15:21:05.390451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n# Check Ensemble OOF \nif COMPUTE_CV : \n    score = ensemble_oof(w,reg_cols,display_oof=True) \n    print(f\"Average/Weighted cv [{score}] with weights {w}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:05.393698Z","iopub.execute_input":"2022-06-20T15:21:05.39407Z","iopub.status.idle":"2022-06-20T15:21:05.789489Z","shell.execute_reply.started":"2022-06-20T15:21:05.394023Z","shell.execute_reply":"2022-06-20T15:21:05.788538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"id":"cb3d8e1e","papermill":{"duration":0.032614,"end_time":"2022-03-22T09:40:37.184739","exception":false,"start_time":"2022-03-22T09:40:37.152125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ==================================================== \ntest = pd.read_csv(INPUT_DIR+'test.csv')\ntest = pd.read_csv(train_file).sample(frac=0.1) if DEBUG else test \nsubmission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\nprint(f\"test.shape: {test.shape}\")\nprint(f\"submission.shape: {submission.shape}\")\n# ====================================================\n# CPC Data\n# ====================================================\ncpc_texts = torch.load(\"../input/folds-dump-the-two-paths/cpc_texts.pth\")\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n","metadata":{"executionInfo":{"elapsed":2627,"status":"ok","timestamp":1644920084001,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"bef012d3","outputId":"d4d60dbc-510c-4f34-8d64-dd1d88c4808c","papermill":{"duration":0.154829,"end_time":"2022-03-22T09:40:37.374453","exception":false,"start_time":"2022-03-22T09:40:37.219624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-20T15:21:05.812049Z","iopub.execute_input":"2022-06-20T15:21:05.812459Z","iopub.status.idle":"2022-06-20T15:21:05.860348Z","shell.execute_reply.started":"2022-06-20T15:21:05.812404Z","shell.execute_reply":"2022-06-20T15:21:05.859131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer(s)","metadata":{"id":"918a28aa","papermill":{"duration":0.039494,"end_time":"2022-03-22T09:40:39.374931","exception":false,"start_time":"2022-03-22T09:40:39.335437","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Dataset(s)","metadata":{"id":"14da40cf","papermill":{"duration":0.04897,"end_time":"2022-03-22T09:40:44.706931","exception":false,"start_time":"2022-03-22T09:40:44.657961","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cocolm.modeling_cocolm import COCOLMModel\nfrom cocolm.configuration_cocolm import COCOLMConfig\nfrom cocolm.tokenization_cocolm import COCOLMTokenizer\nfor i in range(no_of_models):\n    cfg = globals()[f\"CFG_{i}\"]\n    if \"coco\" in cfg.model:\n        print(f\"Tokenizer for {cfg.model}\")\n        cfg.tokenizer = COCOLMTokenizer.from_pretrained(cfg.model)\n    else:\n        cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model)","metadata":{"papermill":{"duration":5.198604,"end_time":"2022-03-22T09:40:44.612849","exception":false,"start_time":"2022-03-22T09:40:39.414245","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:05.862027Z","iopub.execute_input":"2022-06-20T15:21:05.862582Z","iopub.status.idle":"2022-06-20T15:21:16.368211Z","shell.execute_reply.started":"2022-06-20T15:21:05.862534Z","shell.execute_reply":"2022-06-20T15:21:16.367046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    if \"cocolm\" in cfg.model:\n      inputs = cfg.tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,  \n            max_length=cfg.max_len,\n        ) \n      input_ids  = inputs[\"input_ids\"]\n      if \"token_type_ids\" in inputs:\n        token_type_ids = inputs[\"token_type_ids\"]\n      else:\n        token_type_ids = []\n      pad_token_id = 0\n      # Zero-pad up to the sequence length.\n      attention_mask = [1] * len(input_ids) \n      padding_length = cfg.max_len - len(input_ids) \n      #print(padding_length,inputs.items())\n      input_ids = input_ids + ([pad_token_id] * padding_length) \n      attention_mask = attention_mask + ([0 ] * padding_length)\n      if len(token_type_ids) == 0:\n          padding_length = cfg.max_len\n      token_type_ids = token_type_ids +  ([0] * padding_length)\n      inputs[\"input_ids\"] = input_ids\n      if \"token_type_ids\" in inputs:\n        inputs[\"token_type_ids\"] = token_type_ids\n      inputs[\"attention_mask\"] = attention_mask\n    else:\n      inputs = cfg.tokenizer(text,\n                            add_special_tokens=True,\n                            max_length=cfg.max_len,\n                            padding=\"max_length\",\n                            return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\ndef prepare_input_coco(cfg, anchor, target, context_text):\n    tokenizer = cfg.tokenizer\n    _anchor = tokenizer.encode(anchor, add_special_tokens=False)\n    _target = tokenizer.encode(target, add_special_tokens=False)\n    _context_text = tokenizer.encode(context_text, add_special_tokens=False)\n    inputs = {}\n    input_ids = [tokenizer.cls_token_id] + _anchor + [tokenizer.sep_token_id] + \\\n        _target + [tokenizer.sep_token_id] + \\\n        _context_text + [tokenizer.sep_token_id]\n    # print(input_ids)\n    pad_token_id = 0\n    # Zero-pad up to the sequence length.\n    attention_mask = [1] * len(input_ids)\n    padding_length = cfg.max_len - len(input_ids)\n    input_ids = input_ids + ([pad_token_id] * padding_length)\n    attention_mask = attention_mask + ([0] * padding_length)\n    #token_type_ids = token_type_ids +  ([0] * padding_length)\n    inputs[\"input_ids\"] = input_ids\n    inputs[\"attention_mask\"] = attention_mask\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\nclass TestDataset_v2(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        if cfg.lowercase:\n            df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes ==\n                                                    object].apply(lambda col: col.str.lower())\n        self._anchors = df[\"anchor\"].values\n        self._targets = df[\"target\"].values\n        self._contexts = df[\"context_text\"].values\n        self.texts = df['text'].values \n        self._sep_token = f\"{cfg.tokenizer.sep_token}\"\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        anchor = self._anchors[idx]\n        target = self._targets[idx]\n        context = self._contexts[idx]\n        if \"cocolm\" in self.cfg.model:\n            inputs = prepare_input_coco(self.cfg, anchor, target, context)\n        else:\n            input_text = self._sep_token.join([anchor, target, context])\n            inputs = prepare_input(self.cfg, input_text) \n        return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        if cfg.lowercase:\n            df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]' + df['context_text'].apply(str.lower)\n        else:\n            df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]' + df['context_text']\n            \n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\nclass TestDataset_spc(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        if cfg.lowercase:\n            df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]'  + df['context_text'].apply(str.lower)\n        else:\n            df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]'  + df['context_text']\n        if cfg.lowercase:\n            df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].apply(lambda col: col.str.lower())    \n        self._anchors = df[\"anchor\"].values\n        self._targets = df[\"target\"].values\n        self._contexts = df[\"context\"].values             \n        self.texts = df['text'].values\n        self._sep_token = f\" {cfg.tokenizer.sep_token} \"\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        anchor = self._anchors[idx]\n        target = self._targets[idx]\n        context = self._contexts[idx]      \n        input_text = self._sep_token.join([anchor, target, context])\n        inputs = prepare_input(self.cfg, input_text)\n        return inputs\n    \nclass PhraseDataset(Dataset):\n    def __init__(self, cfg, df):\n        super().__init__()\n\n        self._task = cfg.task \n        self.cfg = cfg\n        if cfg.lowercase:\n            df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].apply(lambda col: col.str.lower())\n        self._anchors = df[\"anchor\"].values\n        self._targets = df[\"target\"].values\n        self._contexts = df[\"context\"].values  \n        self._label_dtype = torch.float if self._task == \"regression\" else torch.long\n\n        self.tokenizer = cfg.tokenizer\n\n        self._max_len = cfg.max_len\n        self._sep_token = f\" {cfg.tokenizer.sep_token} \"\n\n    def __len__(self):\n        return len(self._anchors)\n\n    def __getitem__(self, idx):\n        anchor = self._anchors[idx]\n        target = self._targets[idx]\n        context = self._contexts[idx] \n        input_text = self._sep_token.join([anchor, target, context])\n        \n        if self._task == \"regression\":\n            tokenizer_output = self.tokenizer(input_text,\n                                  add_special_tokens=True,\n                                  truncation=True,\n                                  padding=\"max_length\",\n                                  max_length=self._max_len,\n                                  return_token_type_ids=True,\n                                  return_attention_mask=True,\n                                  return_tensors=\"pt\")\n            return {\n                    \"input_ids\": tokenizer_output[\"input_ids\"].squeeze(),\n                    \"attention_mask\": tokenizer_output[\"attention_mask\"].squeeze(),\n                    \"token_type_ids\": tokenizer_output[\"token_type_ids\"].squeeze() \n                }        \n        else:\n            inputs = prepare_input(self.cfg, input_text) \n            return {\n                \"input_ids\": inputs \n                }       \n        \nclass TestDatasetGuang(Dataset):\n    def __init__(self, df, tokenizer, max_input_length):\n        # default to lowercase for Guang models\n        df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].apply(lambda col: col.str.lower())\n        self.text = df['text'].values.astype(str)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = self.text[item]\n        inputs = self.tokenizer(inputs,\n                    max_length=self.max_input_length,\n                    padding='max_length',\n                    truncation=True )\n        return torch.as_tensor(inputs['input_ids'], dtype=torch.long),\\\n               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long),\\\n               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)        ","metadata":{"id":"9f791a19","papermill":{"duration":0.055528,"end_time":"2022-03-22T09:40:52.072178","exception":false,"start_time":"2022-03-22T09:40:52.01665","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.370417Z","iopub.execute_input":"2022-06-20T15:21:16.370832Z","iopub.status.idle":"2022-06-20T15:21:16.421885Z","shell.execute_reply.started":"2022-06-20T15:21:16.37077Z","shell.execute_reply":"2022-06-20T15:21:16.420605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model(s)","metadata":{"id":"e04d6363","papermill":{"duration":0.044161,"end_time":"2022-03-22T09:40:52.262022","exception":false,"start_time":"2022-03-22T09:40:52.217861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.config.update({ \"output_hidden_states\":True,\"layer_norm_eps\": self.cfg.eps, \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"id":"4c5bab44","papermill":{"duration":0.066203,"end_time":"2022-03-22T09:40:52.37203","exception":false,"start_time":"2022-03-22T09:40:52.305827","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.4244Z","iopub.execute_input":"2022-06-20T15:21:16.425185Z","iopub.status.idle":"2022-06-20T15:21:16.444953Z","shell.execute_reply.started":"2022-06-20T15:21:16.425131Z","shell.execute_reply":"2022-06-20T15:21:16.443709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionPool(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n\n        self.attention = nn.Sequential(\n            nn.Linear(in_dim, in_dim),\n            nn.LayerNorm(in_dim),\n            nn.GELU(),\n            nn.Linear(in_dim, 1),\n        )\n\n    def forward(self, x, mask):\n        w = self.attention(x).float() #\n        w[mask==0]=float('-inf')\n        w = torch.softmax(w,1)\n        x = torch.sum(w * x, dim=1)\n        return x\n\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel_att_pool(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if \"cocolm\" in cfg.model:\n          self.config = COCOLMConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        elif config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n\n        if \"cocolm\" in cfg.model:\n          self.model = COCOLMModel.from_pretrained(cfg.model, config=self.config) \n        else:\n          self.model = AutoModel.from_pretrained(cfg.model, config=self.config)  \n\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.score = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.score)\n        self.pool = AttentionPool(self.config.hidden_size) \n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, inputs):\n        outputs = self.model(**inputs) \n        last_hidden_states = outputs[0] \n        x = self.pool(last_hidden_states, inputs[\"attention_mask\"] )\n        x = self.fc_dropout(x)\n        score = self.score(x)\n        return score","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.446459Z","iopub.execute_input":"2022-06-20T15:21:16.446776Z","iopub.status.idle":"2022-06-20T15:21:16.472462Z","shell.execute_reply.started":"2022-06-20T15:21:16.446729Z","shell.execute_reply":"2022-06-20T15:21:16.471179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tr head model","metadata":{}},{"cell_type":"code","source":"class TransformerHead(nn.Module):\n    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n        super().__init__()\n\n        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_features,\n                                                                                          nhead=nhead),\n                                                 num_layers=num_layers)\n        self.row_fc = nn.Linear(in_features, 1)\n        self.out_features = max_length\n\n    def forward(self, x):\n        out = self.transformer(x)\n        out = self.row_fc(out).squeeze(-1)\n        return out\n\n\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel_TransformerHead(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})   \n        self.feature_extractor = AutoModelForTokenClassification.from_pretrained(self.cfg.model)\n        in_features = self.feature_extractor.classifier.in_features\n        self.attention = TransformerHead(in_features=in_features, max_length=133, num_layers=1, nhead=8, num_targets=1)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.attention.out_features, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        feature = self.attention(last_hidden_states)\n        \n        return feature\n\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        #print(feature.shape)\n        output = self.fc(self.fc_dropout(feature))\n        return output        ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.474642Z","iopub.execute_input":"2022-06-20T15:21:16.475155Z","iopub.status.idle":"2022-06-20T15:21:16.499228Z","shell.execute_reply.started":"2022-06-20T15:21:16.475106Z","shell.execute_reply":"2022-06-20T15:21:16.497864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# multi drpt model","metadata":{}},{"cell_type":"code","source":"class CustomModel_mdrpt(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n\n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        #self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n        self.fc_dropout_0 = nn.Dropout(0.1)\n        self.fc_dropout_1 = nn.Dropout(cfg.fc_dropout)\n        self.fc_dropout_2 = nn.Dropout(0.3)\n        self.fc_dropout_3 = nn.Dropout(0.4)\n        self.fc_dropout_4 = nn.Dropout(0.5)\n        self.layers = self.config.num_hidden_layers\n        print(f\"Number of layers in model [{cfg.model}] is [{self.layers}]\")\n\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        #output = self.fc(self.fc_dropout(feature))\n        output_0 = self.fc(self.fc_dropout_0(feature))\n        output_1 = self.fc(self.fc_dropout_1(feature))\n        output_2 = self.fc(self.fc_dropout_2(feature))\n        output_3 = self.fc(self.fc_dropout_3(feature))\n        output_4 = self.fc(self.fc_dropout_4(feature))\n        output =  (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n        return output","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.5035Z","iopub.execute_input":"2022-06-20T15:21:16.504151Z","iopub.status.idle":"2022-06-20T15:21:16.527199Z","shell.execute_reply.started":"2022-06-20T15:21:16.504098Z","shell.execute_reply":"2022-06-20T15:21:16.525825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# concat pooling model","metadata":{}},{"cell_type":"code","source":"# Concat attention pooling based model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n\nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel_Concat(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size*8, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention = AttentionHead(self.config.hidden_size*4)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        all_hidden_states = torch.stack(outputs.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.attention(cat_over_last_layers)\n        return torch.cat([head_logits, cls_pooling], -1)\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.529704Z","iopub.execute_input":"2022-06-20T15:21:16.53017Z","iopub.status.idle":"2022-06-20T15:21:16.559036Z","shell.execute_reply.started":"2022-06-20T15:21:16.530053Z","shell.execute_reply":"2022-06-20T15:21:16.557726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## coco","metadata":{}},{"cell_type":"code","source":"from cocolm.modeling_cocolm import COCOLMModel\nfrom cocolm.configuration_cocolm import COCOLMConfig\nfrom cocolm.tokenization_cocolm import COCOLMTokenizer\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel_Mean_G(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if \"coco\" in cfg.model:\n          self.config = COCOLMConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        elif config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n\n        if \"coco\" in cfg.model:\n          self.model = COCOLMModel.from_pretrained(cfg.model, config=self.config) \n        else:\n          self.model = AutoModel.from_pretrained(cfg.model, config=self.config) \n\n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        pooled_output =torch.mean(last_hidden_states, dim=1) \n        return pooled_output    \n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.562325Z","iopub.execute_input":"2022-06-20T15:21:16.563283Z","iopub.status.idle":"2022-06-20T15:21:16.584861Z","shell.execute_reply.started":"2022-06-20T15:21:16.563227Z","shell.execute_reply":"2022-06-20T15:21:16.583601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regressor","metadata":{}},{"cell_type":"code","source":"class CustomModel_REGR(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.model(input_ids, attention_mask, token_type_ids)\n        last_hidden_states = outputs[0]\n        pooled_output = outputs[0].mean(dim=1) \n        return pooled_output   \n        # feature = torch.mean(last_hidden_states, 1)\n        #weights = self.attention(last_hidden_states)\n        #feature = torch.sum(weights * last_hidden_states, dim=1)\n        #return feature\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        feature = self.feature(input_ids, attention_mask, token_type_ids)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.588481Z","iopub.execute_input":"2022-06-20T15:21:16.588791Z","iopub.status.idle":"2022-06-20T15:21:16.609803Z","shell.execute_reply.started":"2022-06-20T15:21:16.588741Z","shell.execute_reply":"2022-06-20T15:21:16.608512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cls Pool","metadata":{}},{"cell_type":"code","source":"class CustomModel_CLS_POOL(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n            \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.config.update({ \"output_hidden_states\":True,\n                            \"layer_norm_eps\": self.cfg.eps,\n                            \"hidden_dropout_prob\": 0.0,\n                            \"attention_probs_dropout_prob \": 0.0})  \n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)  \n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.model(input_ids, attention_mask, token_type_ids)\n        last_hidden_state = outputs[0]\n        # Last layer CLS model\n        cls_embeddings = last_hidden_state[:, 0]\n        return cls_embeddings    \n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        feature = self.feature(input_ids, attention_mask, token_type_ids)\n        logits = self.fc(feature)\n        preds = logits\n        return preds","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.611766Z","iopub.execute_input":"2022-06-20T15:21:16.612593Z","iopub.status.idle":"2022-06-20T15:21:16.632851Z","shell.execute_reply.started":"2022-06-20T15:21:16.612537Z","shell.execute_reply":"2022-06-20T15:21:16.631458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Concat new","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel_Concat_New(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        self.config.update({ \"output_hidden_states\":True,\n            \"layer_norm_eps\": self.cfg.eps,\n            \"hidden_dropout_prob\": 0.0,\n            \"attention_probs_dropout_prob \": 0.0}) \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        if cfg.grad_checkpoint:    \n            self.model.gradient_checkpointing_enable()   \n        self.layers_to_concat = cfg.layers_to_concat    \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size* cfg.layers_to_concat*2 , self.cfg.target_size)\n        self._init_weights(self.fc) \n        self.attention = AttentionHead(self.config.hidden_size* cfg.layers_to_concat) \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        all_hidden_states = torch.stack(outputs.hidden_states)\n        layers_cat = tuple([ all_hidden_states[-i] for i in range(self.layers_to_concat)])\n        cat_over_last_layers = torch.cat(layers_cat,-1 ) \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.attention(cat_over_last_layers)\n        return torch.cat([head_logits, cls_pooling], -1)\n        #return cls_pooling\n    \n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.635105Z","iopub.execute_input":"2022-06-20T15:21:16.635768Z","iopub.status.idle":"2022-06-20T15:21:16.662829Z","shell.execute_reply.started":"2022-06-20T15:21:16.635706Z","shell.execute_reply":"2022-06-20T15:21:16.661523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel_Mean_V2(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if \"cocolm\" in cfg.model:\n          self.config = COCOLMConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        elif config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        self.config.update({ \"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob \": 0.0})\n\n        if \"cocolm\" in cfg.model:\n          self.model = COCOLMModel.from_pretrained(cfg.model, config=self.config) \n        else:\n          self.model = AutoModel.from_pretrained(cfg.model, config=self.config) \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(\n                mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(\n                mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(\n            -1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        return output\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.664624Z","iopub.execute_input":"2022-06-20T15:21:16.665007Z","iopub.status.idle":"2022-06-20T15:21:16.690169Z","shell.execute_reply.started":"2022-06-20T15:21:16.664952Z","shell.execute_reply":"2022-06-20T15:21:16.687815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Guang Models","metadata":{}},{"cell_type":"code","source":"def init_params(module_lst):\n    for module in module_lst:\n        for param in module.parameters():\n            if param.dim() > 1:\n                torch.nn.init.xavier_uniform_(param)\n    return\n\nclass Custom_Bert(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        print(model_path)\n\n        config = AutoConfig.from_pretrained(model_path)\n        config.update({\"output_hidden_states\":True,\n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.base = AutoModel.from_config(config=config)  \n        \n        dim = config.hidden_size\n        \n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n        \n        n_weights = 24\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n            \n        self.attention = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size),            \n            nn.Tanh(),\n            nn.Linear(config.hidden_size, 1),\n            nn.Softmax(dim=1)\n        ) \n        self.cls = nn.Sequential(\n            nn.Linear(dim,1)\n        )\n        init_params([self.cls,self.attention])\n        \n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        base_output = self.base(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                               token_type_ids=token_type_ids )\n        \n        cls_outputs = torch.stack(\n            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n        )\n        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n    \n        logits = torch.mean(\n            torch.stack(\n                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n                dim=0,\n            ),\n            dim=0,\n        )\n        \n        output = self.cls(logits)\n        if labels is None:\n            return output\n        \n        else:\n            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n        \n        \nclass Custom_Bert_Simple(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        base_output = self.base(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                               token_type_ids=token_type_ids )\n\n        output = base_output[0]\n        if labels is None:\n            return output\n        \n        else:\n            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n        \nclass Custom_Bert_Mean(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(model_path)\n        config.output_hidden_states=True\n        self.base = AutoModel.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        base_output = self.base(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                               token_type_ids=token_type_ids )\n\n        \n        output = base_output.hidden_states[-1]\n        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n        if labels is None:\n            return output\n        \n        else:\n            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.692466Z","iopub.execute_input":"2022-06-20T15:21:16.692936Z","iopub.status.idle":"2022-06-20T15:21:16.72659Z","shell.execute_reply.started":"2022-06-20T15:21:16.692885Z","shell.execute_reply":"2022-06-20T15:21:16.725434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mixer","metadata":{"id":"deee9675","papermill":{"duration":0.044158,"end_time":"2022-03-22T09:40:52.460401","exception":false,"start_time":"2022-03-22T09:40:52.416243","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device, cfg):\n    preds = []\n    model.eval()\n    model.to(device) \n    for step, batch in enumerate(test_loader): \n        if cfg.ds == \"baseline\": # Old baseline dataset\n            inputs = batch\n            for k, v in inputs.items():\n                inputs[k] = v.to(device) \n        elif cfg.task == \"regression\":\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)        \n        else:\n            inputs = batch[\"input_ids\"]\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)             \n        with torch.no_grad():\n            if cfg.task == \"classification\" or cfg.ds == \"baseline\":\n                y_preds = model(inputs)\n            else:\n                y_preds = model(input_ids, attention_mask, token_type_ids)\n        if cfg.task == \"classification\":\n            preds.append(y_preds.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\ndef valid_fn(valid_loader, model, device):\n    model.eval()\n    preds = []\n    labels = []\n    for step, batch in enumerate(valid_loader):\n        input_ids, token_type_ids, attention_mask = [i.to(device) for i in batch]\n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask, token_type_ids)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-20T15:21:16.728928Z","iopub.execute_input":"2022-06-20T15:21:16.729576Z","iopub.status.idle":"2022-06-20T15:21:16.746994Z","shell.execute_reply.started":"2022-06-20T15:21:16.729525Z","shell.execute_reply":"2022-06-20T15:21:16.745442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model archs and datasets per model\nCFG_mdrpts.arch = CustomModel_mdrpt\nCFG_mdrpts.dataset = TestDataset\nCFG_concat.arch = CustomModel_Concat\nCFG_concat.dataset = TestDataset\nCFG_t.arch = CustomModel_TransformerHead\nCFG_t.dataset = TestDataset\nCFG_pbert.arch = CustomModel\nCFG_pbert.dataset = PhraseDataset\nCFG_coco.arch = CustomModel_Mean_G#CustomModel_Concat_New\nCFG_coco.dataset = TestDataset_spc\nCFG_coco_v2.arch = CustomModel_Mean_V2#CustomModel_Concat_New\nCFG_coco_v2.dataset = TestDataset_v2\n# Guang\n#CFG_5.arch =Custom_Bert\n#CFG_5.dataset = TestDatasetGuang\nCFG_BERT_LG_COMPLEX.arch =Custom_Bert\nCFG_BERT_LG_COMPLEX.dataset = TestDatasetGuang\nCFG_BERT_SIMPlE.arch =Custom_Bert_Simple\nCFG_BERT_SIMPlE.dataset = TestDatasetGuang\nCFG_BERT_SIMPlE.model=\"../input/uspatentbertlargesimple\"\n#model=\"../input/uspatentbertlargesimple\"\nCFG_DEB_SIMPLE.arch =Custom_Bert_Simple\nCFG_DEB_SIMPLE.dataset = TestDatasetGuang\nCFG_BERT_MEAN.arch =Custom_Bert_Mean\nCFG_BERT_MEAN.dataset = TestDatasetGuang\nCFG_DEB_MEAN.arch =Custom_Bert_Mean\nCFG_DEB_MEAN.dataset = TestDatasetGuang\nCFG_XL_ELC.arch =Custom_Bert_Mean\nCFG_XL_ELC.dataset = TestDatasetGuang\nCFG_FT_MEAN.arch =Custom_Bert_Mean\nCFG_FT_MEAN.dataset = TestDatasetGuang\nCFG_XL_MEAN.arch =Custom_Bert_Mean\nCFG_XL_MEAN.dataset = TestDatasetGuang\n# G reg\nCFG_reg.arch =CustomModel\nCFG_reg.dataset = TestDataset\nCFG_pb_awp.arch = CustomModel_Mean_V2\nCFG_pb_awp.dataset = TestDataset_v2\n\n# X\nCFG_BCE_mean_v2.arch =CustomModel_Mean_V2\nCFG_BCE_mean_v2.dataset = TestDataset_v2\nCFG_BCE_ATTN_PL.arch =CustomModel_att_pool\nCFG_BCE_ATTN_PL.dataset = TestDataset_v2\nCFG_FT_ATTN.arch =CustomModel_att_pool\nCFG_FT_ATTN.dataset = TestDataset_v2\nCFG_DEBV3_ATTN.arch =CustomModel_att_pool\nCFG_DEBV3_ATTN.dataset = TestDataset_v2\nCFG_MSE_S_X.arch =CustomModel\nCFG_MSE_S_X.dataset = TestDataset_v2\nCFG_SIMSCE.arch =CustomModel_att_pool\nCFG_SIMSCE.dataset = TestDataset_v2\nCFG_ELEC_ATTN.arch =CustomModel_att_pool\nCFG_ELEC_ATTN.dataset = TestDataset_v2\nCFG_SIMSCE_X.arch =CustomModel_att_pool\nCFG_SIMSCE_X.dataset = TestDataset_v2\nCFG_BCE_BERT_X.arch =CustomModel_att_pool\nCFG_BCE_BERT_X.dataset = TestDataset_v2\nCFG_coco_BCE.arch =CustomModel_Mean_V2\nCFG_coco_BCE.dataset = TestDataset_v2\n###\n\nfeature_cols = [col for col in oof.columns if col in [f\"pred_{col}\" for col in MODELS.keys()]]\nprint(f\"Using \\n Models => {MODELS.keys()} \\n Features => {feature_cols} \\n\")\n## Infer function\ndef predict_one_model(config):\n    \n    if config.task == \"guang\":\n        test_dataset = config.dataset(test, config.tokenizer, config.max_len)\n        test_loader = DataLoader(test_dataset,\n                                      batch_size=config.batch_size * 2,\n                                      shuffle=False,\n                                      num_workers=config.num_workers, pin_memory=True, drop_last=False)\n    else:        \n        test_dataset = config.dataset(config, test)\n        test_loader = DataLoader(test_dataset,\n                                 batch_size=config.batch_size* 2,\n                                 shuffle=False,\n                                 num_workers=config.num_workers, pin_memory=True, drop_last=False)\n    predictions = []\n    MMscaler = MinMaxScaler()\n    for fold in config.trn_fold:\n        #print(f\"Inferring for model [{config.model_file}] fold [{fold}]\")\n        \n        if config.task == \"guang\":\n            model = config.arch(config.model)\n            state = torch.load(f\"{config.path}{config.model_file}{fold}.pth\")\n            model.load_state_dict(state['model']) \n            model.to('cuda')\n            prediction =  valid_fn(test_loader, model, 'cuda')\n            if APPLY_SCALAR:\n                prediction = prediction.reshape(-1)\n                predictions.append(MMscaler.fit_transform(prediction.reshape(-1,1)).reshape(-1))\n            else:\n                prediction = prediction.reshape(-1)\n                predictions.append(MMscaler.fit_transform(prediction.reshape(-1,1)).reshape(-1))\n        else:\n            model = config.arch(config, pretrained=False)\n            state = torch.load(config.path+f\"{config.model_file}{fold}_best.pth\",\n                               map_location=torch.device('cuda'))\n            model.load_state_dict(state['model']) \n            prediction = inference_fn(test_loader, model, device, config)\n            if cfg.task == \"classification\":\n                predictions.append(prediction)\n            else:\n                predictions.append(MMscaler.fit_transform(prediction.reshape(-1,1)).reshape(-1))\n        del model, state, prediction; gc.collect()\n        torch.cuda.empty_cache()\n    predictions = np.mean(predictions, axis=0)\n    #print(predictions.shape)\n    del test_dataset, test_loader; gc.collect()\n    return predictions.reshape(-1,1)\n\ndef ensemble(w=[1/no_of_models for i in range(no_of_models)], weighted = False):\n    preds = np.empty((len(test),1)) if weighted else []\n    #print(preds.shape)\n    #for i in range(no_of_models):\n    for i,config in tqdm(MODELS.items()):\n        #config = globals()[f\"CFG_{i}\"]\n        print(f\"=> Inferring for model [{config.model_file}] index [{i}]\")\n        pred = predict_one_model(config)\n        submission[f\"pred_{i}\"] = pred\n        if weighted:\n            print(\"Using weighted strategy\")\n            pred = pred*w[i]\n            preds += pred\n        else:\n            preds.append(pred)\n    return  preds if weighted else np.mean(preds, axis=0)\n\npredictions = ensemble(weighted = False if ENSEMBLE_STRATEGY == \"weighted\" else False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T15:21:16.750744Z","iopub.execute_input":"2022-06-20T15:21:16.751395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge 📈 And Weighted 🏋","metadata":{}},{"cell_type":"code","source":"# Do Ridge on similar Models\ni=0\nfor grp,mods in MODELS_GRP.items():\n    fc = [f\"pred_{mod().__class__.__name__}\" for mod in mods]\n    feature_cols = [col for col in oof.columns if col in fc]\n    pred,score = meta_model_trainer(\"Regressor\",lambda fold: get_regressor(),FOLDS,\n                       _X=oof,\n                       _Y=oof[[\"score\"]],\n                       test = submission,\n                       features = feature_cols )\n    submission[f\"r_pred_{os.path.split(grp)[1]}\"]= pred\n    print(f\"Checking ridge ensemble {i} {grp} {feature_cols} {score}\")\n    i+=1\nprint(f\"****** CV [{score}] ******\")\n\n\n# Ensemble now with predefined Weights\nmodel_cols = [col for col in  submission.columns if col.startswith('r_pred') ]\nprint(f\"{len(model_cols)} Weights used {w}\\n\")    \npredictions = np.zeros(submission['score'].to_numpy().shape)\ni = 0 \nfor col in model_cols:\n    print(f\"Blending model [{col}] with weight [{w[i]}]\")\n    predictions += submission[col]*w[i]\n    i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Add Post processing \nif ADD_POSTPROCESSING:\n    predictions = np.where(predictions<=0, 0, predictions)\n    predictions = np.where(predictions>=1, 1, predictions)\n    \nprint(f\"******** Used [{ENSEMBLE_STRATEGY}] Strategy ********\\n\")\n\nsubmission['score'] = predictions\n# Apply min max scaling to rationalize results\nMinMaxScala  = MinMaxScaler()\nsubmission['score'] = predictions\nsubmission['score'] = MinMaxScala.fit_transform(submission['score'].values.reshape(-1, 1))#upd_score(predictions, thresholds_dict) \n\ndisplay(submission.head())\nsubmission[['id', 'score']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}