{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nimport csv\nimport os\nfrom spacy import displacy\ntry:\n    import stylecloud\nexcept:\n    !pip install stylecloud\n    import stylecloud\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\n\nimport xgboost as xgb\nimport lightgbm as lgb\ntry:\n    import fasttreeshap\nexcept:\n    !pip install fasttreeshap\n    import fasttreeshap\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T19:15:52.44496Z","iopub.execute_input":"2022-03-25T19:15:52.445213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DS_DIR='../input/us-patent-phrase-to-phrase-matching'\n\ntest = pd.read_csv(DS_DIR +'/test.csv')\ntest.info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(DS_DIR +'/train.csv')\ntrain.info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns = ['id','anchor','target', 'context'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(train.columns)\nfor i in cols:\n    train[i] = train[i].apply(pd.to_numeric)\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = train['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = list(map(int, y_true.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns='score', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(train, y_true, stratify=y_true, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    \n    B =(C/C.sum(axis=0))\n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"green\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source of data: https://archive.ics.uci.edu/ml/datasets/superconductivty+data\nDS_DIR='../input/us-patent-phrase-to-phrase-matching'\n\ndata = pd.read_csv(DS_DIR +'/train.csv', engine = \"python\")\ntrain, test = train_test_split(data, test_size = 0.5, random_state = 0)\nlabel_train = train[\"score\"]\nlabel_test = train[\"score\"]\ntrain = train.iloc[:, :-1]\ntest = test.iloc[:, :-1]\nprint(\"Training data has {} rows and {} columns.\".format(train.shape[0], train.shape[1])) \nprint(\"Testing data has {} rows and {} columns.\".format(test.shape[0], test.shape[1])) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(y = np.unique(train[\"anchor\"]),\nx = [list(train[\"anchor\"]).count(i) for i in np.unique(train[\"anchor\"])] , \n            color = np.unique(train[\"anchor\"]),\n             color_continuous_scale=\"Emrld\", \n             orientation='h',\n             width=800, \n             height=4000) \nfig.update_xaxes(title=\"Anchor\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Anchor Type Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        template=\"plotly_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(y = np.unique(train[\"target\"]),\nx = [list(train[\"target\"]).count(i) for i in np.unique(train[\"target\"])] , \n            color = np.unique(train[\"target\"]),\n             color_continuous_scale=\"Emrld\", \n             orientation='h',\n             width=800, \n             height=4000) \nfig.update_xaxes(title=\"Target\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Target  Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        template=\"plotly_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(y = np.unique(train[\"context\"]),\nx = [list(train[\"context\"]).count(i) for i in np.unique(train[\"context\"])] , \n            color = np.unique(train[\"context\"]),\n             color_continuous_scale=\"Emrld\", \n             orientation='h',\n             width=800, \n             height=4000) \nfig.update_xaxes(title=\"Context\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Context  Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        template=\"plotly_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['anchor_word_count'] = train.anchor.str.split().str.len()\ntrain['target_word_count'] = train.target.str.split().str.len()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame= train,x = \"anchor_word_count\",  marginal=\"violin\",nbins = 50 )\nfig.update_layout(template=\"plotly_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame= train,x = \"target_word_count\",  marginal=\"violin\",nbins = 50 )\nfig.update_layout(template=\"plotly_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef random_color():\n        rand = lambda: random.randint(1, 255)\n        return '#%02X%02X%02X' % (rand(), rand(), rand())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_n_grams(text,ngram=1):\n    words=[word for word in text.split()]\n    temp=zip(*[words[i:] for i in range(0,ngram)])\n    ans=[' '.join(ngram) for ngram in temp]\n    return ans\n\n# UNIGRAM\ncounts=defaultdict(int)\nfor text in train['anchor']:\n    for word in generate_n_grams(text):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in UNIGRAM ANALYSIS - Anchor\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# BIGRAM\ncounts=defaultdict(int)\nfor text in train['anchor']:\n    for word in generate_n_grams(text, ngram=2):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in BIGRAM ANALYSIS - Anchor \", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# TRIGRAM\ncounts=defaultdict(int)\nfor text in train['anchor']:\n    for word in generate_n_grams(text, ngram=3):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1,color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in TRIGRAM ANALYSIS - Anchor\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNIGRAM\ncounts=defaultdict(int)\nfor text in train['target']:\n    for word in generate_n_grams(text):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in UNIGRAM ANALYSIS - Target\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# BIGRAM\ncounts=defaultdict(int)\nfor text in train['target']:\n    for word in generate_n_grams(text, ngram=2):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in BIGRAM ANALYSIS - Target\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n\n# TRIGRAM\ncounts=defaultdict(int)\nfor text in train['target']:\n    for word in generate_n_grams(text, ngram=3):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in TRIGRAM ANALYSIS - Target\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNIGRAM\ncounts=defaultdict(int)\nfor text in train['context']:\n    for word in generate_n_grams(text):\n        counts[word]+=1\n    \ndf=pd.DataFrame(sorted(counts.items(),key=lambda x:x[1],reverse=True))\npd1=df[0][:50]\npd2=df[1][:50]\n\nplt.figure(1,figsize=(30,30))\nsns.barplot(pd2,pd1, color=random_color())\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Words in dataframe\", fontsize=20)\nplt.title(\"Top 50 words in UNIGRAM ANALYSIS - CONTEXT\", fontsize=30)\nplt.tick_params(axis='both', labelsize=20)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\n\nsns.histplot(x='target_word_count', data=train, hue='anchor', bins=50, palette='rainbow')\nplt.title('Distribution of target_word_count in Training Data', fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wordcloud\nwordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=80, max_words=5000,\n                      width = 600, height = 400,\n                      background_color='gray').generate(' '.join(txt for txt in train[\"target\"]))\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wordcloud\nwordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=80, max_words=5000,\n                      width = 600, height = 400,\n                      background_color='blue').generate(' '.join(txt for txt in train[\"target\"]))\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code credit - https://www.kaggle.com/code/hasanbasriakcay/patent-phrase-matching-eda-fe-baseline\n\nfrom IPython.core.display import HTML\ndef value_counts_all(df, columns):\n    pd.set_option('display.max_rows', 50)\n    table_list = []\n    for col in columns:\n        table_list.append(pd.DataFrame(df[col].value_counts()))\n    return HTML(\n        f\"<table><tr> {''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list])} </tr></table>\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value_counts_all(train, ['anchor', 'target', 'context'])#, 'score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value_counts_all(test, ['anchor', 'target', 'context'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_dict = {\n    'A': 'Human Necessities',\n    'B': 'Operations and Transport',\n    'C': 'Chemistry and Metallurgy',\n    'D': 'Textiles',\n    'E': 'Fixed Constructions',\n    'F': 'Mechanical Engineering',\n    'G': 'Physics',\n    'H': 'Electricity',\n    'Y': 'Emerging Cross-Sectional Technologies'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes_df = pd.read_csv(\"../input/cpc-codes/titles.csv\")\ncpc_codes_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code source - https://www.kaggle.com/code/hasanbasriakcay/patent-phrase-matching-eda-fe-baseline \n\ndef create_feature(df, cpc_codes_df):\n    import fuzzywuzzy\n    from fuzzywuzzy import fuzz\n    from fuzzywuzzy import process\n    \n    df['section'] = df['context'].str[:1]\n    df['class'] = df['context'].str[1:]\n    \n    df['anchor_len'] = df['anchor'].apply(lambda x: len(x.split(' ')))\n    df['target_len'] = df['target'].apply(lambda x: len(x.split(' ')))\n    \n    pattern = '[0-9]'\n    mask = df['anchor'].str.contains(pattern, na=False)\n    df['num_anchor'] = mask\n    mask = df['target'].str.contains(pattern, na=False)\n    df['num_target'] = mask\n    \n    df['context_desc'] = df['context'].map(cpc_codes_df.set_index('code')['title']).str.lower()\n    \n    fuzzy_anchor_target_scores = []\n    fuzzy_anchor_context_scores = []\n    fuzzy_taget_context_scores = []\n    for index, row in df.iterrows():\n        fuzzy_anchor_target_scores.append(fuzz.ratio(row['anchor'], row['target']))\n        fuzzy_anchor_context_scores.append(fuzz.ratio(row['anchor'], row['context_desc']))\n        fuzzy_taget_context_scores.append(fuzz.ratio(row['context_desc'], row['target']))\n    df['fuzzy_at_score'] = fuzzy_anchor_target_scores\n    df['fuzzy_ac_score'] = fuzzy_anchor_context_scores\n    df['fuzzy_tc_score'] = fuzzy_taget_context_scores\n    df['fuzzy_c_score'] = df['fuzzy_ac_score'] + df['fuzzy_tc_score']\n    df['fuzzy_total'] = df['fuzzy_at_score'] + df['fuzzy_c_score']\n    \n    df.drop(['context', 'fuzzy_ac_score', 'fuzzy_tc_score'], 1, inplace=True)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train = create_feature(train.copy(), cpc_codes_df)\nnew_test = create_feature(test.copy(), cpc_codes_df)\nnew_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.countplot(data=new_train, y='section', ax=ax, orientation='horizontal')\nax.set_yticklabels([context_dict['A'], context_dict['C'], context_dict['F'], context_dict['H'], context_dict['B'], \n                    context_dict['D'], context_dict['E'], context_dict['G']], rotation=0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 32))\n\nsns.countplot(data=new_train, y='class', ax=ax, orientation='horizontal')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig2, ax = plt.subplots(figsize=(16, 24))\ng = sns.jointplot(data=new_train, x=\"target_len\", y=\"fuzzy_total\", ax=ax)\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\ng.plot_marginals(sns.rugplot, color=\"r\", height=-.15, clip_on=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nfrom nltk.corpus import stopwords\n\n# Reference - https://www.kaggle.com/kapakudaibergenov/stylecloud/notebook\nconcat_data = ' '.join([i for i in train.anchor.astype(str)])\nstylecloud.gen_stylecloud(text=concat_data,\n                          icon_name='fas fa-book',\n                          palette='cartocolors.qualitative.Bold_6',\n                          background_color='black',\n                          gradient='horizontal',\n                          size=1024)\n\n\nImage(filename=\"./stylecloud.png\", width=1024, height=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference - https://www.kaggle.com/kapakudaibergenov/stylecloud/notebook\nconcat_data = ' '.join([i for i in train.target.astype(str)])\nstylecloud2.gen_stylecloud(text=concat_data,\n                          icon_name='fas fa-film',\n                          palette='cartocolors.qualitative.Prism_10',\n                          background_color='blue',\n                          gradient='horizontal',\n                          size=2048)\n\n\nImage(filename=\"./stylecloud2.png\", width=2048, height=2048)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}