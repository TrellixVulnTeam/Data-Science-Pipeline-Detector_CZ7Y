{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Dear Reader, please note that the EDA for this notebook was written in Jupyter Notebook and as such some features (particularly graphs) may not work on kaggle as intended - I have marked the respective parts and generally recommend downloading and opening in Jupyter Notebook to be able to follow my thoughts behind some graphs ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T16:55:01.334738Z","iopub.execute_input":"2022-05-02T16:55:01.335453Z","iopub.status.idle":"2022-05-02T16:55:01.342207Z","shell.execute_reply.started":"2022-05-02T16:55:01.335409Z","shell.execute_reply":"2022-05-02T16:55:01.340763Z"}}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") #can get annoying and visually distracting","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:58:36.458697Z","iopub.execute_input":"2022-06-20T04:58:36.459119Z","iopub.status.idle":"2022-06-20T04:58:36.462938Z","shell.execute_reply.started":"2022-06-20T04:58:36.459085Z","shell.execute_reply":"2022-06-20T04:58:36.462217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\ndef ecdf(data):\n    #select and sort data for the x_axis\n    x_axis = np.sort(data)\n    #aranges the x-values evenly spaced along the y-axis (5000 evenly spaced points): start = 1, stop is len(data)+1 / len(data) -> this goes from basically 0 to basically 1\n    ##having the x-values evenly spaced later allows interpretations that are \"kind of like quantiles\" -> Y% of data is below X\n    y_axis = np.arange(1, len(data)+1)/len(data)\n    #return allows the variables to be assigned to multiple variables when function is being called\n    return x_axis, y_axis\n\nimport pyLDAvis\nimport pyLDAvis.gensim #for kaggle\n#import pyLDAvis.gensim_models #for Jupyter\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport gensim\nfrom gensim import corpora\nimport en_core_web_sm\nimport re\nimport spacy\nfrom wordcloud import WordCloud\nfrom transformers import pipeline\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer, DataCollatorWithPadding\nfrom datasets import load_dataset, Dataset, DatasetDict\n\nseed_value = 2\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\nimport tensorflow as tf\ntf.random.set_seed(seed_value)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-20T04:58:36.531781Z","iopub.execute_input":"2022-06-20T04:58:36.532122Z","iopub.status.idle":"2022-06-20T04:58:50.742922Z","shell.execute_reply.started":"2022-06-20T04:58:36.532093Z","shell.execute_reply":"2022-06-20T04:58:50.742056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading data directly in kaggle\ndf = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\ntest = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\n\nprint(f\"train data shape: {df.shape}; test data shape: {test.shape}\")\n#notably, the test data very short and contains no output feature","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-20T04:58:50.744573Z","iopub.execute_input":"2022-06-20T04:58:50.745037Z","iopub.status.idle":"2022-06-20T04:58:50.83979Z","shell.execute_reply.started":"2022-06-20T04:58:50.745005Z","shell.execute_reply":"2022-06-20T04:58:50.839024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Breakdown of features:\n1. ID: unique identifier  - won't be used\n2. anchor: first phrase\n3. target: second phrase\n4. context: CPC Classification Number - scoring similarity within these groups (https://en.wikipedia.org/wiki/Cooperative_Patent_Classification)\n5. score: similarity score = outcome variable\n      * 1.0 = very close; 0.75 = close; 0.5 synonyms with different meaning; 0.25 = somewhat related; 0.0 = unrelated\n     ","metadata":{}},{"cell_type":"markdown","source":"# Goal:\npredict the score as value of similarity between anchor and target within each context\n\n\n-> While we want to score the similarity between anchor and target, the context can heavily impact this similarity! \n\nIn result, all columns of the data set (except ID) need to be explored","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"df.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-20T04:58:50.840879Z","iopub.execute_input":"2022-06-20T04:58:50.841128Z","iopub.status.idle":"2022-06-20T04:58:50.860416Z","shell.execute_reply.started":"2022-06-20T04:58:50.841093Z","shell.execute_reply":"2022-06-20T04:58:50.859705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Are all IDs unique identifiers? (because you never know)\nprint(f\"{len(np.unique(df.id))} out of {df.shape[0]} samples are unique\")\n#the length of unique values matches the train shape; there are no duplicates in the dataset\n\n#unique values per feature (not including ID)\nvals = [len(np.unique(df.anchor)), len(np.unique(df.target)), len(np.unique(df.context))]\nsns.barplot(x = [\"anchor\", \"target\", \"context\"], y = vals);\n#notably, although anchor and target are heavily related by meaning, the unique values vary greatly. \n#However, ~7000 target values seem to be identical, given that there are 36473 unique entries in the df.","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-20T04:58:50.862446Z","iopub.execute_input":"2022-06-20T04:58:50.862781Z","iopub.status.idle":"2022-06-20T04:58:51.2251Z","shell.execute_reply.started":"2022-06-20T04:58:50.862741Z","shell.execute_reply":"2022-06-20T04:58:51.224193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature: Anchor","metadata":{}},{"cell_type":"code","source":"#How often to anchors occur? \ndf.anchor.value_counts().reset_index().describe().T\n#The 733 anchors, appear on average 50 times; however, they at least appear once (duh) and 152 times at most","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:13.775331Z","iopub.execute_input":"2022-06-18T17:44:13.775732Z","iopub.status.idle":"2022-06-18T17:44:13.803458Z","shell.execute_reply.started":"2022-06-18T17:44:13.775694Z","shell.execute_reply":"2022-06-18T17:44:13.802661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tip: double clicking the plot will increase readability.\nsns.set(font_scale = 0.5)\nfig, ax =plt.subplots(figsize = (65,30))\nsns.countplot(x = df.anchor, order = df.anchor.value_counts().index, ax = ax, color = \"b\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);\nax.axhline(df.anchor.value_counts().reset_index().describe().loc[\"25%\"][0], color = \"r\", label = \"25% percentile\")\nax.axhline(df.anchor.value_counts().reset_index().describe().loc[\"50%\"][0], color = \"orange\", label = \"50% percentile\")\nax.axhline(df.anchor.value_counts().reset_index().describe().loc[\"75%\"][0], color = \"r\", label = \"75% percentile\")\nplt.title(\"Counts of Anchors\", fontsize = 40)\nplt.legend(fontsize=40)\n#there are many values that are above the 3rd quartile and below the first quartile","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:13.80571Z","iopub.execute_input":"2022-06-18T17:44:13.806194Z","iopub.status.idle":"2022-06-18T17:44:42.566551Z","shell.execute_reply.started":"2022-06-18T17:44:13.806149Z","shell.execute_reply":"2022-06-18T17:44:42.565933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = ecdf(df.anchor.value_counts())\nplt.plot(x, y, marker = \".\", linestyle =\"None\")\nplt.xlabel(\"Anchor occurences\");\n#We can see that that the anchor occurences are pretty unbalanced","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:42.567536Z","iopub.execute_input":"2022-06-18T17:44:42.567807Z","iopub.status.idle":"2022-06-18T17:44:42.823535Z","shell.execute_reply.started":"2022-06-18T17:44:42.567752Z","shell.execute_reply":"2022-06-18T17:44:42.822744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (30,10))\nsns.set(font_scale = 0.9)\nsymbols = []\nfor i in df.anchor:\n    symbols.append(len(i))\n\nsns.countplot(x = symbols, color = \"b\", ax = ax[0])\nax[0].set_title(\"Number of letters in Anchors\");\n#the number of symbols in the anchor are normally distributed\n#the most values are in the range of 12-19 letters\n\nword_count = []\nfor i in df.anchor:\n    word_count.append(len(i.split()))\n\nsns.countplot(x = word_count, color = \"b\", ax = ax[1])\nax[1].set_title(\"Number of words in Anchors\");\n#the anchors contain 1-5 words; most of them contain 2\n#this may be relevant for truncation and padding on the modelling process","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:42.824908Z","iopub.execute_input":"2022-06-18T17:44:42.825229Z","iopub.status.idle":"2022-06-18T17:44:43.836355Z","shell.execute_reply.started":"2022-06-18T17:44:42.825192Z","shell.execute_reply":"2022-06-18T17:44:43.835661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target","metadata":{}},{"cell_type":"code","source":"df.target.value_counts().reset_index().describe().T\n#there are a lot more unique values in the target and most of them only appear once\n#however, some of them appear up to 24 times; on average targets appear 1.24 times\n\n#this may also mean that it is hard to train (and also overfit) the models for specific targets!","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:43.837466Z","iopub.execute_input":"2022-06-18T17:44:43.838916Z","iopub.status.idle":"2022-06-18T17:44:43.873433Z","shell.execute_reply.started":"2022-06-18T17:44:43.838873Z","shell.execute_reply":"2022-06-18T17:44:43.872597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking numbers in anchor feature\n#Code from: https://www.kaggle.com/code/remekkinas/eda-and-feature-engineering/notebook\n\npattern = '[0-9]'\nmask = df['anchor'].str.contains(pattern, na=False)\ndf['nun_anchor'] = mask\ndf[mask]['anchor'].value_counts()\n#5 anchors contain numbers\n#generally these names are rather cryptic","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:43.876643Z","iopub.execute_input":"2022-06-18T17:44:43.87686Z","iopub.status.idle":"2022-06-18T17:44:43.915871Z","shell.execute_reply.started":"2022-06-18T17:44:43.876836Z","shell.execute_reply":"2022-06-18T17:44:43.915201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.anchor == \"conh2\"]\n#there is a lot of domain knowledge necessary here","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:43.917034Z","iopub.execute_input":"2022-06-18T17:44:43.917554Z","iopub.status.idle":"2022-06-18T17:44:43.941972Z","shell.execute_reply.started":"2022-06-18T17:44:43.91751Z","shell.execute_reply":"2022-06-18T17:44:43.941305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (30,10))\nsns.set(font_scale = 0.9)\n\nsymbols = []\nfor i in df.target:\n    symbols.append(len(i))\n\nsns.countplot(x = symbols, color = \"b\", ax = ax[0])\nax[0].set_title(\"Number of letters in Anchors\");\n#the number of symbols in the target are (beautifully) normal distributed\n\nsns.set(font_scale = 0.75)\nword_count = []\nfor i in df.target:\n    word_count.append(len(i.split()))\n\nsns.countplot(x = word_count, color = \"b\", ax = ax[1])\nax[1].set_title(\"Number of words in Anchors\");\n#the targets contain 1-15 words; most of them contain 1 to 3 words\n#some of the anchors are very long (15 words / 98 symbols)\n\n#this will be relevant for modelling later on\n#but given that these words are very rare, we can easily truncate them without hard feelings","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:43.943367Z","iopub.execute_input":"2022-06-18T17:44:43.943674Z","iopub.status.idle":"2022-06-18T17:44:45.964475Z","shell.execute_reply.started":"2022-06-18T17:44:43.943637Z","shell.execute_reply":"2022-06-18T17:44:45.963686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Context","metadata":{}},{"cell_type":"code","source":"#Dropping the int of the context to cluster on general category (called gen_cat)\ndf[\"gen_cat\"] = 0\nfor index in df.index:\n    df[\"gen_cat\"].iloc[index] = df.context.iloc[index][0]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:58:51.227182Z","iopub.execute_input":"2022-06-20T04:58:51.227446Z","iopub.status.idle":"2022-06-20T04:58:59.266133Z","shell.execute_reply.started":"2022-06-20T04:58:51.227411Z","shell.execute_reply":"2022-06-20T04:58:59.265238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = df.context.value_counts().reset_index().describe().T\npd.concat([context, df.gen_cat.value_counts().reset_index().describe().T])\n#there are 106 different context codes from 8 overall categories\n#the context appear at least 18 times, while the general categories appear at least 1279 times\n#the most common context appears 2186 times and the most common general category appeast 8019 times\n\n#this could potentialy impact overfitting on certain contexts / categories","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:53.972621Z","iopub.execute_input":"2022-06-18T17:44:53.972897Z","iopub.status.idle":"2022-06-18T17:44:54.006243Z","shell.execute_reply.started":"2022-06-18T17:44:53.972864Z","shell.execute_reply":"2022-06-18T17:44:54.005424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking numbers in target feature\n#Code from: https://www.kaggle.com/code/remekkinas/eda-and-feature-engineering/notebook\n\npattern = '[0-9]'\nmask = df['target'].str.contains(pattern, na=False)\ndf['num_target'] = mask\ndf[mask]['target'].value_counts()\n#there are more values in target containing numbers, but they are always less frequent.","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:54.007718Z","iopub.execute_input":"2022-06-18T17:44:54.00801Z","iopub.status.idle":"2022-06-18T17:44:54.050405Z","shell.execute_reply.started":"2022-06-18T17:44:54.007973Z","shell.execute_reply":"2022-06-18T17:44:54.049719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.target == \"h2o product\"]\n#this should have a higher score in my opinion.\n#0.5 implies synonyms with the different meaning, I disagree on this score :)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:54.051811Z","iopub.execute_input":"2022-06-18T17:44:54.052292Z","iopub.status.idle":"2022-06-18T17:44:54.069138Z","shell.execute_reply.started":"2022-06-18T17:44:54.052254Z","shell.execute_reply":"2022-06-18T17:44:54.0682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tip: double clicking the plot will increase readability.\nsns.set(font_scale = 1.5)\nfig, ax =plt.subplots(figsize = (65,30))\nsns.countplot(x = df.context, order = df.context.value_counts().index, ax = ax, color = \"b\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);\nax.axhline(df.context.value_counts().reset_index().describe().loc[\"25%\"][0], color = \"r\", linewidth = 3, label = \"25% percentile\")\nax.axhline(df.context.value_counts().reset_index().describe().loc[\"50%\"][0], color = \"orange\",linewidth = 3, label = \"50% percentile\")\nax.axhline(df.context.value_counts().reset_index().describe().loc[\"75%\"][0], color = \"r\", linewidth = 3, label = \"75% percentile\")\nplt.title(\"Counts of Context\", fontsize = 40)\nplt.legend(fontsize=40)\n#there are several values for context which heavily outweigh most other values","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:54.070632Z","iopub.execute_input":"2022-06-18T17:44:54.071187Z","iopub.status.idle":"2022-06-18T17:44:57.120342Z","shell.execute_reply.started":"2022-06-18T17:44:54.071152Z","shell.execute_reply":"2022-06-18T17:44:57.119664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tip: double clicking the plot will increase readability.\nsns.set(font_scale = 1.5)\nfig, ax =plt.subplots(figsize = (25,10))\nsns.countplot(x = df.gen_cat, order = df.gen_cat.value_counts().index, ax = ax, color = \"b\")\nax.set_xticklabels(ax.get_xticklabels());\nax.axhline(df.gen_cat.value_counts().reset_index().describe().loc[\"25%\"][0], color = \"r\", linewidth = 3, label = \"25% percentile\")\nax.axhline(df.gen_cat.value_counts().reset_index().describe().loc[\"50%\"][0], color = \"orange\",linewidth = 3, label = \"50% percentile\")\nax.axhline(df.gen_cat.value_counts().reset_index().describe().loc[\"75%\"][0], color = \"r\", linewidth = 3, label = \"75% percentile\")\nplt.title(\"Counts of general Categories\", fontsize = 25)\nplt.legend(fontsize=15)\n#unlike the individual contexts, the general contexts are more balanced\n#However, there is only little context for the general categories E & D","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:57.121617Z","iopub.execute_input":"2022-06-18T17:44:57.122326Z","iopub.status.idle":"2022-06-18T17:44:57.537251Z","shell.execute_reply.started":"2022-06-18T17:44:57.122282Z","shell.execute_reply":"2022-06-18T17:44:57.536591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#since there are many more anchors in the anchor-count plot than in the context-count plot, we know that some contexts\n#have multiple anchors; at the same time: multiple contexts can also have the same anchor!\nprint(df[df.anchor == \"activating position\"].context.nunique(), df[df.anchor == \"activating position\"].gen_cat.nunique())\ndf[df.anchor == \"activating position\"]\n#this example shows that some anchors are shared among contexts (in this case 3 different contexts in 3 different general categories)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:57.538631Z","iopub.execute_input":"2022-06-18T17:44:57.539096Z","iopub.status.idle":"2022-06-18T17:44:57.577628Z","shell.execute_reply.started":"2022-06-18T17:44:57.539058Z","shell.execute_reply":"2022-06-18T17:44:57.576972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How many unique contexts are given in train?\nnp.unique(df.context), f\"{len(np.unique(df.context))} unique values\"","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:57.579028Z","iopub.execute_input":"2022-06-18T17:44:57.579251Z","iopub.status.idle":"2022-06-18T17:44:57.648623Z","shell.execute_reply.started":"2022-06-18T17:44:57.579219Z","shell.execute_reply":"2022-06-18T17:44:57.647678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How many unique contexts are given in test?\nnp.unique(test.context), f\"{len(np.unique(test.context))} unique values\"\n#all the labels from test are included in train \n\n#notably, there are many context values given in the training data, which are not contained in the test data\n#However, this does not mean, that the final kaggle resut will not contain the missing 77 values!","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:57.651417Z","iopub.execute_input":"2022-06-18T17:44:57.651629Z","iopub.status.idle":"2022-06-18T17:44:57.657541Z","shell.execute_reply.started":"2022-06-18T17:44:57.651605Z","shell.execute_reply":"2022-06-18T17:44:57.656736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Closer look at the contexts which only have a few entries\ndf[df.context == \"F26\"]\n#it will maybe be hard to train models on this little data, however, the target words are very similar without much variation.\n#is there a way to arbitrarily increase the combinations for these contexts?","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:57.659023Z","iopub.execute_input":"2022-06-18T17:44:57.659559Z","iopub.status.idle":"2022-06-18T17:44:57.683185Z","shell.execute_reply.started":"2022-06-18T17:44:57.659524Z","shell.execute_reply":"2022-06-18T17:44:57.68235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Closer look at the contexts which only have a few entries\ndf[df.context == \"A62\"]\n#some of these word combinations seem wildly different.\n#also, some of these word combinations seem again ambigiously placed: \n#matel phase -> metal of material = 0.5\n#metal phase -> metal material = 0.25\n\n#with this context containing many anchors, it may make less sense to include context in the anchor for these samples\n#some of these anchors are particularly rare in this context","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:44:57.684784Z","iopub.execute_input":"2022-06-18T17:44:57.685259Z","iopub.status.idle":"2022-06-18T17:44:57.709298Z","shell.execute_reply.started":"2022-06-18T17:44:57.685223Z","shell.execute_reply":"2022-06-18T17:44:57.708523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df[\"gen_cat\"].unique())\n#we would expect B, E, F, G and H to be close to another! (just from general domains)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:57.710617Z","iopub.execute_input":"2022-06-18T17:44:57.711628Z","iopub.status.idle":"2022-06-18T17:44:57.721353Z","shell.execute_reply.started":"2022-06-18T17:44:57.711593Z","shell.execute_reply":"2022-06-18T17:44:57.720359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    A: Human Necessities\n    B: Operations and Transport\n    C: Chemistry and Metallurgy\n    D: Textiles\n    E: Fixed Constructions\n    F: Mechanical Engineering\n    G: Physics\n    H: Electricity\n    Y: Emerging Cross-Sectional Technologies","metadata":{}},{"cell_type":"code","source":"#Wordcloud per (general) context (most frequent words per context)\nwc_a = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"A\"].target))\nwc_b = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"B\"].target))\nwc_c = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"C\"].target))\nwc_d = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"D\"].target))\nwc_e = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"E\"].target))\nwc_f = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"F\"].target))\nwc_g = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"G\"].target))\nwc_h = WordCloud(width = 800, height = 400, background_color=\"white\").generate(\" \".join(target for target in df[df.gen_cat == \"H\"].target))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:44:57.722467Z","iopub.execute_input":"2022-06-18T17:44:57.723194Z","iopub.status.idle":"2022-06-18T17:45:04.279916Z","shell.execute_reply.started":"2022-06-18T17:44:57.723156Z","shell.execute_reply":"2022-06-18T17:45:04.279166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the wordclouds\nfig = plt.figure(figsize = (40,40))\nims = [[wc_a, \"Wordcloud: Context A\"],\n       [wc_b, \"Wordcloud: Context B\"],\n       [wc_c, \"Wordcloud: Context C\"],\n       [wc_d, \"Wordcloud: Context D\"],\n       [wc_e, \"Wordcloud: Context E\"],\n       [wc_f, \"Wordcloud: Context F\"],\n       [wc_g, \"Wordcloud: Context G\"],\n       [wc_h, \"Wordcloud: Context H\"]]\n\nfor a, b in enumerate(ims):\n    fig.add_subplot(4,2, a+1)\n    plt.imshow(b[0], interpolation='bilinear')\n    plt.title(b[1], fontsize = 30)\n    plt.axis(\"off\")\n    \n#Double-clicking may increase readability :) \n#Lets quickly look at the first things we can notice:\n    #Looking at the wordcloud, we see the word \"device\" being common in context A, B, E, G, H\n    #Context B and D both have the word \"layer\" as common occurence\n    #Context B, D, E and F all have the word \"water\" as common occurence\n    #Context A, B, E and F all have the word \"member\" as common occurence\n    #Context B and C both have the word \"metal\" as common occurence\n#In result, none of the wordclouds are fully disconnected from the others\n    #C seems \"the most disconnected\"","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:04.281274Z","iopub.execute_input":"2022-06-18T17:45:04.281528Z","iopub.status.idle":"2022-06-18T17:45:08.151617Z","shell.execute_reply.started":"2022-06-18T17:45:04.281492Z","shell.execute_reply":"2022-06-18T17:45:08.15077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lengths of target per context\ndf[\"target_length\"] = 0\nfor i in df.index:\n    df.target_length.iloc[i] = len(df.target.iloc[i].split())\n\nsns.boxplot(x = \"target_length\", y = \"gen_cat\", data = df, color = \"b\")\nplt.xticks([1,2,3,4,5, 10, 15]);\n#most context categories are in the area of 2-3 words for target\n#C has the relative-most longest targets\n#C and D have the relative-most shortest targets","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:08.153004Z","iopub.execute_input":"2022-06-18T17:45:08.153452Z","iopub.status.idle":"2022-06-18T17:45:19.987559Z","shell.execute_reply.started":"2022-06-18T17:45:08.15341Z","shell.execute_reply":"2022-06-18T17:45:19.986805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15,60))\nsns.boxplot(x = \"target_length\", y = \"context\", data = df, hue = \"gen_cat\")\n#interestingly some contexts (such as C07 and C08) are very short but also have the strongest outliers\n#we can see that the sub categories' context-length are often similar within their categories","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-18T17:45:19.993696Z","iopub.execute_input":"2022-06-18T17:45:19.994175Z","iopub.status.idle":"2022-06-18T17:45:23.778254Z","shell.execute_reply.started":"2022-06-18T17:45:19.99413Z","shell.execute_reply":"2022-06-18T17:45:23.777603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at these word lengths, lets have a look at the scores they receive\n#(because maybe they have a terrible score just because of the lengths)\ndf[df.target_length >= 6].head(25)\n#index 3402 seems particularly fun (also has a very high score)\n#again we can see that two basically identical lines have a different context nummer (7341, 7369)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:23.779471Z","iopub.execute_input":"2022-06-18T17:45:23.78015Z","iopub.status.idle":"2022-06-18T17:45:23.802261Z","shell.execute_reply.started":"2022-06-18T17:45:23.780106Z","shell.execute_reply":"2022-06-18T17:45:23.801623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.target_length >= 6].boxplot(column = \"score\", by = \"target_length\")\n#it seems like longer targets will not be able to receive full score","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:23.803315Z","iopub.execute_input":"2022-06-18T17:45:23.803561Z","iopub.status.idle":"2022-06-18T17:45:24.116471Z","shell.execute_reply.started":"2022-06-18T17:45:23.803527Z","shell.execute_reply":"2022-06-18T17:45:24.11582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df.target_length >= 6) & (df.score == 1)]\n#the only case of a perfect score with a long target has a very long anchor itself (so its only 2 words longer)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:24.117702Z","iopub.execute_input":"2022-06-18T17:45:24.118111Z","iopub.status.idle":"2022-06-18T17:45:24.133591Z","shell.execute_reply.started":"2022-06-18T17:45:24.118074Z","shell.execute_reply":"2022-06-18T17:45:24.132637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Maybe instead of looking at absolute lengths, we should look at relative lengths compared to the anchor\ndf[\"length_diff\"] = 0\nfor i in df.index:\n    df.length_diff.iloc[i] = df[\"target_length\"].iloc[i] - len(df.anchor.iloc[i].split())\n    \ndf.boxplot(column = \"score\", by = \"length_diff\")\n#it seems like a length difference of more than 3 and lower than -2 will not allow a perfect score\n#while it seems that the target being way shorter than the anchor is generally bad for score\n#the target being longer than the anchor seems to generally have a positive impact\n\n#these findings need to be looked at with some respect, though, given that there are only few data points, on which this data is based on\n# Accordingly, this may be completely different for unknown test data","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:24.135092Z","iopub.execute_input":"2022-06-18T17:45:24.135453Z","iopub.status.idle":"2022-06-18T17:45:37.402271Z","shell.execute_reply.started":"2022-06-18T17:45:24.135413Z","shell.execute_reply":"2022-06-18T17:45:37.401598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale = 1)\nsns.boxplot(x = df.score)\n#scores of 1 are so rare that they are considered outliers","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:37.403519Z","iopub.execute_input":"2022-06-18T17:45:37.404188Z","iopub.status.idle":"2022-06-18T17:45:37.625895Z","shell.execute_reply.started":"2022-06-18T17:45:37.404148Z","shell.execute_reply":"2022-06-18T17:45:37.625194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(x = df.score, bins = 5)\nplt.xticks([0.0, 0.25, np.mean(df.score), 0.5, 0.75, 1.0]);\nplt.axvline(np.mean(df.score), color = \"red\", label = \"mean\")\nplt.legend()\nplt.title(\"Hitsogramm of Score\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:37.627007Z","iopub.execute_input":"2022-06-18T17:45:37.628656Z","iopub.status.idle":"2022-06-18T17:45:37.901292Z","shell.execute_reply.started":"2022-06-18T17:45:37.628623Z","shell.execute_reply":"2022-06-18T17:45:37.900582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Which entries have a score of 1?\ndf[df.score == 1].head(15)\n#it seems like patents with the same anchor and target have sometimes different context (B65 & G06; A41 & B23)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:37.902589Z","iopub.execute_input":"2022-06-18T17:45:37.90303Z","iopub.status.idle":"2022-06-18T17:45:37.924298Z","shell.execute_reply.started":"2022-06-18T17:45:37.902991Z","shell.execute_reply":"2022-06-18T17:45:37.923646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How many are there per context group?\ncontext_counts = df[df.score == 1].groupby(\"context\").id.count().reset_index().sort_values(\"id\", ascending = False)\ncontext_counts.T\n#100 contexts have have perfect scores (out of 106)\n#however, 9 of them only have one perfect score; which basically allows no training for perfect synonyms","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:37.925546Z","iopub.execute_input":"2022-06-18T17:45:37.925812Z","iopub.status.idle":"2022-06-18T17:45:37.955982Z","shell.execute_reply.started":"2022-06-18T17:45:37.925777Z","shell.execute_reply":"2022-06-18T17:45:37.955284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking at an example of a context with only one perfect score (out of 70 entries)\nprint(f\"there are {df[df.context == 'A22'].shape[0]} samples for this context\")\ndf[df.context == \"A22\"].head(20)\n#maybe turning word groups into syllables will help in prediction\n#alternatively, it probably makes sense to reduce key words in to their parts for abbreviations\n#such as electromagnectic -> electro magnetic -> em  ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:37.957362Z","iopub.execute_input":"2022-06-18T17:45:37.957812Z","iopub.status.idle":"2022-06-18T17:45:37.988357Z","shell.execute_reply.started":"2022-06-18T17:45:37.957777Z","shell.execute_reply":"2022-06-18T17:45:37.987432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating this dataframe for a stacked barchart is tidious but mostly copy-paste\nscores_plot = df[df.score == 0].groupby([\"context\"]).id.count().reset_index()\nscores_plot.columns = [\"context\",\"count_score_0\"]\nscores_plot = scores_plot.merge(df[df.score == 0.25].groupby([\"context\"]).id.count().reset_index(), on = \"context\")\nscores_plot = scores_plot.merge(df[df.score == 0.50].groupby([\"context\"]).id.count().reset_index(), on = \"context\")\nscores_plot = scores_plot.merge(df[df.score == 0.75].groupby([\"context\"]).id.count().reset_index(), on = \"context\")\nscores_plot = scores_plot.merge(df[df.score == 1].groupby([\"context\"]).id.count().reset_index(), on = \"context\")\nscores_plot = scores_plot.merge(df.groupby(\"context\").id.count().reset_index(), on = \"context\")\nscores_plot.columns = [\"context\", \"count: score 0.0\", \"count: score 0.25\", \"count: score 0.50\", \"count: score 0.75\", \"count: score 1.0\", \"overall\"]\nscores_plot = scores_plot.sort_values(\"overall\", ascending = False).set_index(\"context\")\nscores_plot.drop(columns = [\"overall\"], inplace = True)\n\n#Creating the stacked barchart for scores\nfig, ax =plt.subplots(figsize = (65,30))\nscores_plot.plot(kind = \"bar\", stacked = True, ax = ax)\nplt.legend(fontsize = 40)\n#This plot underlines how rare perfect scores are and how very common 0.25 and 0.5 are as score.","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:37.990127Z","iopub.execute_input":"2022-06-18T17:45:37.990409Z","iopub.status.idle":"2022-06-18T17:45:41.741329Z","shell.execute_reply.started":"2022-06-18T17:45:37.990348Z","shell.execute_reply":"2022-06-18T17:45:41.740635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perfect_scores = df[df.score == 1].groupby(\"context\").id.count().reset_index().sort_values(\"id\", ascending = False)\n\n#tip: double clicking the plot will increase readability.\nsns.set(font_scale = 1.5)\nfig, ax =plt.subplots(figsize = (65,30))\nsns.barplot(x = \"context\", y =\"id\", data = perfect_scores, ax = ax, color = \"b\")\nsns.barplot(x = \"context\", y =\"id\", data = perfect_scores, ax = ax, color = \"b\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);\nax.axhline(perfect_scores.describe().loc[\"25%\"][0], color = \"r\", linewidth = 3, label = \"25% percentile\")\nax.axhline(perfect_scores.describe().loc[\"50%\"][0], color = \"orange\",linewidth = 3, label = \"50% percentile\")\nax.axhline(perfect_scores.describe().loc[\"75%\"][0], color = \"r\", linewidth = 3, label = \"75% percentile\")\nplt.title(\"Counts of perfect scores per context\", fontsize = 40)\nplt.ylabel(\"count\")\nplt.legend(fontsize=40);\n#again, some contexts are heavily outweighing the other contexts\n#However, the order of perfect scores is not identical to the order of overall counts per context","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:41.742603Z","iopub.execute_input":"2022-06-18T17:45:41.742993Z","iopub.status.idle":"2022-06-18T17:45:45.563218Z","shell.execute_reply.started":"2022-06-18T17:45:41.742957Z","shell.execute_reply":"2022-06-18T17:45:45.562519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Which entries have a score of 0?\ndf[df.score == 0].head(25)\n#some of these seem unjustified scored low: abatement- rent abatement; abatement- tax abatement","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-18T17:45:45.564468Z","iopub.execute_input":"2022-06-18T17:45:45.564852Z","iopub.status.idle":"2022-06-18T17:45:45.589374Z","shell.execute_reply.started":"2022-06-18T17:45:45.564815Z","shell.execute_reply":"2022-06-18T17:45:45.588578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.score == 0.75].head(25)\n#stopwords matter! (last two lines) -> if you kick them out, the target and anchor would be identical","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:45.59075Z","iopub.execute_input":"2022-06-18T17:45:45.591025Z","iopub.status.idle":"2022-06-18T17:45:45.614284Z","shell.execute_reply.started":"2022-06-18T17:45:45.59099Z","shell.execute_reply":"2022-06-18T17:45:45.613643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similiarities\nFurther explore on the ideas that were first shown in the wordclouds","metadata":{}},{"cell_type":"code","source":"#This thing will take a hot minute but will help for word clouds and clustering\nnlp = en_core_web_sm.load()\n#Lemmatize the data \ndata_lem = []\nfor i in list(df.target): \n    lemma = nlp(i)\n    data_lem.append(\" \".join([word.lemma_ for word in lemma]))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:45:45.615322Z","iopub.execute_input":"2022-06-18T17:45:45.615591Z","iopub.status.idle":"2022-06-18T17:48:27.64738Z","shell.execute_reply.started":"2022-06-18T17:45:45.615554Z","shell.execute_reply":"2022-06-18T17:48:27.646658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dictionary and bag of words from the data\ntokens = [[word for word in data.split()] for data in data_lem]\ndictionary = corpora.Dictionary(tokens)\ndoc_term_matrix = [dictionary.doc2bow(patent) for patent in tokens]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:48:27.648624Z","iopub.execute_input":"2022-06-18T17:48:27.648886Z","iopub.status.idle":"2022-06-18T17:48:28.131678Z","shell.execute_reply.started":"2022-06-18T17:48:27.648852Z","shell.execute_reply":"2022-06-18T17:48:28.130946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiate the gensim LDA model for pyLDAvis (also will take a short while)\nLDA = gensim.models.ldamodel.LdaModel\nldamodel = LDA(corpus = doc_term_matrix,\n               id2word = dictionary,\n               num_topics = len(list(df[\"gen_cat\"].unique())), \n               #it might make sense to explore how many ACTUALLY different topics there are based on the targets (probably less than 8)\n               random_state = 0,\n               chunksize = 2000,\n               passes = 50, \n               iterations = 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:48:28.132979Z","iopub.execute_input":"2022-06-18T17:48:28.133211Z","iopub.status.idle":"2022-06-18T17:51:16.081341Z","shell.execute_reply.started":"2022-06-18T17:48:28.133174Z","shell.execute_reply":"2022-06-18T17:51:16.08046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check coherence (high = good) and perplexity (low = good)\nfrom gensim.models import CoherenceModel\ncoherence_model = CoherenceModel(model = ldamodel, texts = tokens, dictionary = dictionary, coherence = \"c_v\")\nldamodel.log_perplexity(doc_term_matrix, total_docs = df.shape[0]), coherence_model.get_coherence()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:16.082948Z","iopub.execute_input":"2022-06-18T17:51:16.083346Z","iopub.status.idle":"2022-06-18T17:51:21.726737Z","shell.execute_reply.started":"2022-06-18T17:51:16.083308Z","shell.execute_reply":"2022-06-18T17:51:21.725915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looks a lot better on white background ;)\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary) #for Kaggle\n#vis = pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, dictionary) for Jupyter\nvis\n\n#we can see that stopwords are often in the most salient terms. \n#However, since the targets are very short, it doesnt make sense to remove them, since they sometimes reduce overall score","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:21.728194Z","iopub.execute_input":"2022-06-18T17:51:21.728447Z","iopub.status.idle":"2022-06-18T17:51:26.381447Z","shell.execute_reply.started":"2022-06-18T17:51:21.728412Z","shell.execute_reply":"2022-06-18T17:51:26.380624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, lets replicate the results with sk-learn (which shows the cluster less \"beautiful\")\n#Sklearn is a great alternative, because we can see how the groups are actually located\n\n#Vectorize data\nidf = TfidfVectorizer(min_df = 0.001) \n#0.001 will reduce computing time (a lot) and increase variance ratio on the first 3 PCs\ntext_idf = idf.fit_transform(df.target).toarray()\ny = list(df[\"gen_cat\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:26.38336Z","iopub.execute_input":"2022-06-18T17:51:26.384025Z","iopub.status.idle":"2022-06-18T17:51:26.750892Z","shell.execute_reply.started":"2022-06-18T17:51:26.383975Z","shell.execute_reply":"2022-06-18T17:51:26.749938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fit classifier (may take a while)\nclf = LinearDiscriminantAnalysis()\nX_r2 = clf.fit(text_idf, y).transform(text_idf)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:26.752624Z","iopub.execute_input":"2022-06-18T17:51:26.752958Z","iopub.status.idle":"2022-06-18T17:51:30.6898Z","shell.execute_reply.started":"2022-06-18T17:51:26.752916Z","shell.execute_reply":"2022-06-18T17:51:30.688765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the first 3 components explain 70% of variance\nclf.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:30.691283Z","iopub.execute_input":"2022-06-18T17:51:30.691536Z","iopub.status.idle":"2022-06-18T17:51:30.705514Z","shell.execute_reply.started":"2022-06-18T17:51:30.691501Z","shell.execute_reply":"2022-06-18T17:51:30.704823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_col = {\"A\":\"blue\",\n          \"B\":\"green\",\n          \"C\":\"black\",\n          \"D\":\"red\",\n          \"E\":\"yellow\",\n          \"F\":\"purple\",\n          \"G\":\"brown\",\n          \"H\":\"orange\"}\ndf[\"colours\"] = df[\"gen_cat\"].map(map_col)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:30.708858Z","iopub.execute_input":"2022-06-18T17:51:30.711923Z","iopub.status.idle":"2022-06-18T17:51:30.747336Z","shell.execute_reply.started":"2022-06-18T17:51:30.711876Z","shell.execute_reply":"2022-06-18T17:51:30.746607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this plot was created to be opened in jupyter notebook (to have an interactive 3D Chart and being able to see the clusters better)\n#%matplotlib notebook #activate this in jupyter\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection = '3d')\n\n\nx = X_r2[:,0]\ny = X_r2[:,1]\nz = X_r2[:,2]\n\nax.scatter(x,y,z, c = df[\"colours\"], marker = \".\")\n\ncol_a = mpatches.Patch(color='blue', label='A / Human Necessities')\ncol_b = mpatches.Patch(color='green', label='B / Operations and Transport')\ncol_c = mpatches.Patch(color='black', label='C / Chemistry and Metallurgy')\ncol_d = mpatches.Patch(color='red', label='D / Textiles')\ncol_e = mpatches.Patch(color='yellow', label='E / Fixed Constructs')\ncol_f = mpatches.Patch(color='purple', label='F / Mechanical Engineering')\ncol_g = mpatches.Patch(color='brown', label='G / Physics')\ncol_h = mpatches.Patch(color='orange', label='H / Electricity')\nhandles=[col_a, col_b, col_c, col_d, col_e, col_f, col_g, col_h]\nplt.legend(handles=handles, loc = \"upper right\", fontsize = 8);\n\n#we expected B, E, F, G and H to be close to another! (green, yellow, purple, brown, orange); just by topic names\n#this means, everything but: black, blue, red\n#However, we can see that only black is clustered apart (and still some outliers fall into other clusters)\n#interestingly, purple seems somewhat separated as well.","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:51:30.751698Z","iopub.execute_input":"2022-06-18T17:51:30.753952Z","iopub.status.idle":"2022-06-18T17:51:32.307483Z","shell.execute_reply.started":"2022-06-18T17:51:30.753896Z","shell.execute_reply":"2022-06-18T17:51:32.30681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summarizing EDA & Preprocessing:\n - Some anchors are shared among several contexts (and general categories)\n - Most context contain several different anchors\n - Some contexts are heavily outweighing others in overall occurence (heavily right-skewed)\n - In general, the contexts of the categories D & E are under-represented in the data\n - The proportion of scores are more or less similar around all contexts\n - Most of the contexts in which we want to predict the scores are similar in regards to words used and words lengths\n \n \n - It does not make sense to remove stop words or short words, since they actually impact the score (\"accept information -> accept this information\" = 0.75)\n - Abbreviations are a thing in the dataset (e.g., Electromagnetic = em; Water = h2o) -> It might makes sense to find a model for domain specific abbreviations (also for possibly unknown categories & abbreviations in the test set)\n     -> BUT: abreviations also penalize score!\n - Synonyms are often not as heavily penalized as abreviations - a good synonym finder will be helpful\n     -> generally, the penalization of synonyms seems to be sometimes weird (e.g., absorbant properties and absorbant characteristics is a perfect match at one point (id: 621b048d70aa8867) but absorption characteristics an inperfect match (0.75) at another point (id: e6f92889099fd908)) -> maybe lemmatization will mess up these relationships were they are considered \"inperfect\" because there are two small misallignments","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"#Cstom callback to return pearson correlation of val set after every epoch; maybe only predict on parts to enhance speed\nclass callback_pearson(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.Y_val = np.array(val_ds[\"label\"]).reshape(1,-1)\n    def on_epoch_end(self, epoch, logs):\n        X_val_preds = self.model.predict(tf_validation_dataset)[\"logits\"].reshape(1,-1)\n        pearson_corr = np.corrcoef(X_val_preds, self.Y_val)\n        print(\"pearson r on the validation set =\", pearson_corr[0][1])\n        logs[\"val_corr\"] = pearson_corr[0][1]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:07.763227Z","iopub.execute_input":"2022-06-20T04:59:07.763523Z","iopub.status.idle":"2022-06-20T04:59:07.769449Z","shell.execute_reply.started":"2022-06-20T04:59:07.76349Z","shell.execute_reply":"2022-06-20T04:59:07.768739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#increased train data from 70% to 75% \n#notably, several other notebooks (e.g., https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster#Improving-the-model)\n#have achieved higher correlation with less training; this may be due to a higher training size.\n\n#Creating validation set; again, copied mostly from: https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster\n#this random shuffling is done because the hidden test data does not overlap with the known training data (so it's random)\nanchors = df.anchor.unique()\nnp.random.shuffle(anchors)\n\n#gets the anchors \nval_prop = 0.25 #this was commonly used in other notebooks\nval_sz = int(len(anchors)*val_prop)\nval_anchors = anchors[:val_sz]\n\n#decide on validation indices and train indices\nis_val = np.isin(df.anchor, val_anchors)\nidxs = np.arange(len(df))\nval_idxs = idxs[ is_val]\ntrn_idxs = idxs[~is_val]\nlen(val_idxs),len(trn_idxs) #print length of validation and train set","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:07.880546Z","iopub.execute_input":"2022-06-20T04:59:07.880883Z","iopub.status.idle":"2022-06-20T04:59:08.09947Z","shell.execute_reply.started":"2022-06-20T04:59:07.880851Z","shell.execute_reply":"2022-06-20T04:59:08.098781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#With using the context instead of full name, I achieved a score of 0.815 on large -> lets try with more world clues\n#created this map (as above) from wikipedia\n\n#Hopefully with this extra information it will work better\ngen_cat_map = {\"A\": \"Human Necessities\",\n    \"B\": \"Operations and Transport\",\n    \"C\": \"Chemistry and Metallurgy\",\n    \"D\": \"Textiles\",\n    \"E\": \"Fixed Constructions\",\n    \"F\": \"Mechanical Engineering\",\n    \"G\": \"Physics\",\n    \"H\": \"Electricity\",\n    \"Y\": \"Emerging Cross-Sectional Technologies\"}\n\ndf[\"full\"] = df.gen_cat.map(gen_cat_map)\ntest[\"gen_cat\"] = 0\nfor index in test.index:\n    test[\"gen_cat\"].iloc[index] = test.context.iloc[index][0]\n\ntest[\"full\"] = test.gen_cat.map(gen_cat_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:08.191942Z","iopub.execute_input":"2022-06-20T04:59:08.192166Z","iopub.status.idle":"2022-06-20T04:59:08.2111Z","shell.execute_reply.started":"2022-06-20T04:59:08.19214Z","shell.execute_reply":"2022-06-20T04:59:08.210432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv(\"../input/cpc-codes/titles.csv\")\ncpc = cpc[[\"code\",\"title\"]]\ncpc.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:09.163404Z","iopub.execute_input":"2022-06-20T04:59:09.163959Z","iopub.status.idle":"2022-06-20T04:59:09.850846Z","shell.execute_reply.started":"2022-06-20T04:59:09.163918Z","shell.execute_reply":"2022-06-20T04:59:09.850129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.merge(cpc, left_on = \"context\", right_on = \"code\", how = \"left\")\ntest = test.merge(cpc, left_on = \"context\", right_on = \"code\", how = \"left\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:10.10471Z","iopub.execute_input":"2022-06-20T04:59:10.105191Z","iopub.status.idle":"2022-06-20T04:59:10.264868Z","shell.execute_reply.started":"2022-06-20T04:59:10.105153Z","shell.execute_reply":"2022-06-20T04:59:10.264135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"title\"] = df[\"title\"].str.lower()\ndf[\"full\"] = df[\"full\"].str.lower() #using lower seems to be a best practice \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:10.588537Z","iopub.execute_input":"2022-06-20T04:59:10.58931Z","iopub.status.idle":"2022-06-20T04:59:10.6421Z","shell.execute_reply.started":"2022-06-20T04:59:10.589266Z","shell.execute_reply":"2022-06-20T04:59:10.641255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"title\"] = test[\"title\"].str.lower()\ntest[\"full\"] = test[\"full\"].str.lower() #using lower seems to be a best practice \ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:11.726262Z","iopub.execute_input":"2022-06-20T04:59:11.726967Z","iopub.status.idle":"2022-06-20T04:59:11.744421Z","shell.execute_reply.started":"2022-06-20T04:59:11.726928Z","shell.execute_reply":"2022-06-20T04:59:11.740803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting","metadata":{}},{"cell_type":"markdown","source":"## DeBERTa small","metadata":{}},{"cell_type":"markdown","source":"Things that influenced finetuning DeBERTa small:\n- using context instead of sep token reduced score by 0.005\n- smaller batches did not change score, but increased fitting time\n- increasing train / test split from 0.3 to 0.25 increased score by 0.005\n- higher learning rate (1e-4 instead of 5e-5) drastically reduced score, by 0.05\n\nBest score achieved with DeBERTa small (did not go back with further optimisations afterwards): 0.781","metadata":{}},{"cell_type":"markdown","source":"## DeBERTa base","metadata":{}},{"cell_type":"markdown","source":"Things that influenced finetuning DeBERTa base\n- created warm up ratio -> increased score by 0.02\n- used recommended hypertuning parameters -> had no impact","metadata":{}},{"cell_type":"markdown","source":"# Electra -> Val Score: 0.8264","metadata":{}},{"cell_type":"markdown","source":"Things that influenced Electra:\n- changed input to include \"full\" instead of context -> boosted from 0.805 to 0.815\n- changed input to include \"context\" and \"title\" instead of \"full\" + dynamic padding -> boosted to 0.820\n- reduced warmup a little (0.15 instead of 0.1) -> boosted to 0.825\n- had to enable shuffle = False on fit and set random states to get consistent results; else model often died\n- had to also put Electra on first position so it runs with less problems (before it was third)\n\nIn general, the consense in many discussions seems to be that Electra is a rather hard to train model","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") #can get annoying and visually distracting","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:14.406268Z","iopub.execute_input":"2022-06-20T04:59:14.406789Z","iopub.status.idle":"2022-06-20T04:59:14.411381Z","shell.execute_reply.started":"2022-06-20T04:59:14.406749Z","shell.execute_reply":"2022-06-20T04:59:14.410223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"electra = \"../input/google-electra-large-discriminator\"\n#ELECTRA is a new method for self-supervised language representation learning. \n#It can be used to pre-train transformer networks using relatively little compute. \n#ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens \n#generated by another neural network, similar to the discriminator of a GAN. \n#TLDR; google did something cool and it works\n\ntokenizer = AutoTokenizer.from_pretrained(electra)\nsep = tokenizer.sep_token #is \"[SEP]\"\ndf[\"inputs\"] = df.context + sep + df.title + sep + df.anchor + sep + df.target + sep \ndf1 = df[[\"inputs\",\"score\"]]\n\n\ndef tok_func(x): return tokenizer(x[\"inputs\"])\n\n#create ds from dataframe\nds = Dataset.from_pandas(df).rename_column('score', 'label')\n#split into seperate sections\nnew_ds = DatasetDict({\"train\":ds.select(trn_idxs),\n             \"val\": ds.select(val_idxs)})\n#split into seperate ds\ntrain_ds = new_ds[\"train\"]\nval_ds  = new_ds[\"val\"]\n#tokenize\ntok_train = train_ds.map(tok_func,batched = True) \ntok_val = val_ds.map(tok_func, batched = True)\n\nfrom transformers import DataCollatorWithPadding\n#dynamic padding just decreased score\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding = True, return_tensors=\"tf\")\ntf_train_dataset = tok_train.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=16,\n)\ntf_validation_dataset = tok_val.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=16,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:14.648133Z","iopub.execute_input":"2022-06-20T04:59:14.648412Z","iopub.status.idle":"2022-06-20T04:59:31.387097Z","shell.execute_reply.started":"2022-06-20T04:59:14.648379Z","shell.execute_reply":"2022-06-20T04:59:31.38632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#electra was trained on google colab:\n#https://colab.research.google.com/drive/1whTtxMpZ0BGO-TdhVCW54prJ5FEK0MuC?authuser=1#scrollTo=in7NUwNkcRuV","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:31.388942Z","iopub.execute_input":"2022-06-20T04:59:31.389179Z","iopub.status.idle":"2022-06-20T04:59:31.393264Z","shell.execute_reply.started":"2022-06-20T04:59:31.389145Z","shell.execute_reply":"2022-06-20T04:59:31.39251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\nmodel_elec = TFAutoModelForSequenceClassification.from_pretrained(electra, num_labels=1, from_pt = True) #importantly, we are loading from pytorch as nobody likes TF\nmodel_elec.load_weights(\"../input/valentin-electra-patents-finetuned/tf-electra-large-finetuned.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:31.394869Z","iopub.execute_input":"2022-06-20T04:59:31.395125Z","iopub.status.idle":"2022-06-20T04:59:56.721236Z","shell.execute_reply.started":"2022-06-20T04:59:31.395089Z","shell.execute_reply":"2022-06-20T04:59:56.720475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets also analyse the error\npreds_elec = model_elec.predict(tf_validation_dataset)\n#quick reality check if the model took best model or last model\nnp.corrcoef(np.array(val_ds[\"label\"]).reshape(1,-1), preds_elec[\"logits\"].reshape(1,-1))[0][1]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:59:56.72336Z","iopub.execute_input":"2022-06-20T04:59:56.723594Z","iopub.status.idle":"2022-06-20T05:01:03.037826Z","shell.execute_reply.started":"2022-06-20T04:59:56.72356Z","shell.execute_reply":"2022-06-20T05:01:03.037061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l = df.iloc[val_idxs]\nval_df_l[\"pred_elec\"] = np.array(preds_elec[\"logits\"]).reshape(1,-1)[0]\nval_df_l[\"diff_elec\"] = val_df_l.score - val_df_l.pred_elec\nval_df_l[\"good_pred_elec\"] = np.abs(val_df_l[\"diff_elec\"]) < 0.125\n\nsns.histplot(x = \"diff_elec\", data = val_df_l, bins = 100)\nplt.axvline(0.125, color = \"r\")\nplt.axvline(-0.125, color = \"r\")\n#the red lines are the number of \"correct predictions\"\n\n#number of \"wrong predictions\"\nprint(\"accuracy\", 1 - val_df_l[(val_df_l[\"diff_elec\"] >= 0.125) | (val_df_l[\"diff_elec\"] <= -0.125)].shape[0] / val_df_l.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:03.039162Z","iopub.execute_input":"2022-06-20T05:01:03.040553Z","iopub.status.idle":"2022-06-20T05:01:03.45685Z","shell.execute_reply.started":"2022-06-20T05:01:03.040498Z","shell.execute_reply":"2022-06-20T05:01:03.456172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add noise to score to have a proper scatterplot\nnoise = 0.075 * np.random.randn(val_df_l.shape[0])\nval_df_l[\"score_noised\"] = val_df_l.score + noise\nplt.scatter(val_df_l.pred_elec, val_df_l.score_noised, color = \"b\", marker = \".\")\n#we should shave of values under 0 and over1\n#the gap between 0.9 and 1.0 seems very itneresting","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:03.457967Z","iopub.execute_input":"2022-06-20T05:01:03.458671Z","iopub.status.idle":"2022-06-20T05:01:03.688855Z","shell.execute_reply.started":"2022-06-20T05:01:03.458629Z","shell.execute_reply":"2022-06-20T05:01:03.68818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create test predictions\n #prepare input; mostly copied from https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster\ntokenizer = AutoTokenizer.from_pretrained(electra)\nsep = tokenizer.sep_token #is \"[SEP]\" #also try different seperators later for performance\ntest[\"inputs\"] = test.context + sep + test.title + sep + test.anchor + sep + test.target + sep\neval_ds = Dataset.from_pandas(test)\n#tokenize\ntok_eval = eval_ds.map(tok_func, batched = True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_test_dataset = tok_eval.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=1,\n)\n\npreds_test_elec = model_elec.predict(tf_test_dataset)\nelec_pred = preds_test_elec[\"logits\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:03.690102Z","iopub.execute_input":"2022-06-20T05:01:03.69179Z","iopub.status.idle":"2022-06-20T05:01:10.478321Z","shell.execute_reply.started":"2022-06-20T05:01:03.691749Z","shell.execute_reply":"2022-06-20T05:01:10.477549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Large -> Val Score: 0.8321","metadata":{"execution":{"iopub.status.busy":"2022-06-07T09:49:54.427269Z","iopub.execute_input":"2022-06-07T09:49:54.427637Z","iopub.status.idle":"2022-06-07T09:49:54.432431Z","shell.execute_reply.started":"2022-06-07T09:49:54.427581Z","shell.execute_reply":"2022-06-07T09:49:54.431389Z"}}},{"cell_type":"markdown","source":"Things that influenced DeBERTA large\n- short input (context + anchor + target) -> put score at 0.805\n- changed inputs to context + title + dynamic padding -> put score at 0.815\n- truncation and fixed padding reduced by score by .015\n- changed inputs to full instead, turned dynamic padding off -> put score to 0.829\n- changed dynamic padding on -> score to .832\n- tried out custom seperator tokens with the format [\"context\"] -> reduced score to 0.77\n- doesnt run on anything higher than batchsize 16\n- higher learning rate reduced score; no warmup reduced score; ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:10.479719Z","iopub.execute_input":"2022-06-20T05:01:10.479987Z","iopub.status.idle":"2022-06-20T05:01:10.486136Z","shell.execute_reply.started":"2022-06-20T05:01:10.479953Z","shell.execute_reply":"2022-06-20T05:01:10.485387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model was trained on collab: \n#https://colab.research.google.com/drive/1vxAQCJM3hPdUp0DrrlHF2vg9gcIFEfVC?authuser=2#scrollTo=8b0wmHGsxQMC","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:10.487666Z","iopub.execute_input":"2022-06-20T05:01:10.487972Z","iopub.status.idle":"2022-06-20T05:01:10.493175Z","shell.execute_reply.started":"2022-06-20T05:01:10.48792Z","shell.execute_reply":"2022-06-20T05:01:10.492476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Different model: try deberta large instead of small\ndeberta = \"../input/deberta-v3-large/deberta-v3-large\"\n#DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder.\n#With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n#our V3 version significantly improves the model performance on downstream tasks. \n#TLDR; its the hot shit!\n\ntokenizer = AutoTokenizer.from_pretrained(deberta)\nsep = tokenizer.sep_token #is \"[SEP]\"\n#recommendation for building on deberta: [CLS] A [SEP] B [SEP] #this came out at 0.8192\ndf[\"inputs\"] = df.full + sep + df.anchor + sep + df.target + sep \n#deberta large performs better on full than on title + context\ndf1 = df[[\"inputs\",\"score\"]]\n\n#Function to apply tokenizer\n#maxlen = 17 #cls + 4 for context + sep + 4 for anchor + sep + 6 for target\n#i tested on a boxplot; this will truncate 387 out of the 27346 inputs (~1.4%)\ndef tok_func(x): return tokenizer(x[\"inputs\"])#.batch_encode_plus(x[\"inputs\"], max_length = maxlen, padding = \"max_length\", truncation = True) #reduced score","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:10.496938Z","iopub.execute_input":"2022-06-20T05:01:10.497183Z","iopub.status.idle":"2022-06-20T05:01:11.228312Z","shell.execute_reply.started":"2022-06-20T05:01:10.497159Z","shell.execute_reply":"2022-06-20T05:01:11.227572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Down to 16 for RAM purposes\n#create ds from dataframe\nds = Dataset.from_pandas(df).rename_column('score', 'label')\n#split into seperate sections\nnew_ds = DatasetDict({\"train\":ds.select(trn_idxs),\n             \"val\": ds.select(val_idxs)})\n#split into seperate ds\ntrain_ds = new_ds[\"train\"]\nval_ds  = new_ds[\"val\"]\n#tokenize\ntok_train = train_ds.map(tok_func,batched = True) #I overwrite the earlier datasets for RAM and not-having-to-change-my-code purposes\ntok_val = val_ds.map(tok_func, batched = True)\n\nfrom transformers import DataCollatorWithPadding\n#changed padding = True along the way -> dynamic padding (was supposed to reduce time, but did effectively nothing), if anything: score decreased a little\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = tok_train.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=16,\n)\ntf_validation_dataset = tok_val.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=16,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:11.229435Z","iopub.execute_input":"2022-06-20T05:01:11.230139Z","iopub.status.idle":"2022-06-20T05:01:21.366263Z","shell.execute_reply.started":"2022-06-20T05:01:11.2301Z","shell.execute_reply":"2022-06-20T05:01:21.365489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\nmodel_l = TFAutoModelForSequenceClassification.from_pretrained(deberta, num_labels=1)\nmodel_l.load_weights(\"../input/valentin-debertalarge-patents-finetuned/tf-deberta-large-finetuned.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:21.367546Z","iopub.execute_input":"2022-06-20T05:01:21.367782Z","iopub.status.idle":"2022-06-20T05:01:58.048014Z","shell.execute_reply.started":"2022-06-20T05:01:21.367749Z","shell.execute_reply":"2022-06-20T05:01:58.047196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets also analyse the error\npreds_l = model_l.predict(tf_validation_dataset)\n#quick reality check if the model took best model or last model\nnp.corrcoef(np.array(val_ds[\"label\"]).reshape(1,-1), preds_l[\"logits\"].reshape(1,-1))[0][1]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:01:58.049714Z","iopub.execute_input":"2022-06-20T05:01:58.050014Z","iopub.status.idle":"2022-06-20T05:03:44.310946Z","shell.execute_reply.started":"2022-06-20T05:01:58.049973Z","shell.execute_reply":"2022-06-20T05:03:44.310164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l[\"pred\"] = np.array(preds_l[\"logits\"]).reshape(1,-1)[0]\nval_df_l[\"diff\"] = val_df_l.score - val_df_l.pred\nval_df_l[\"good_pred\"] = np.abs(val_df_l[\"diff\"]) < 0.125","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:44.312612Z","iopub.execute_input":"2022-06-20T05:03:44.313151Z","iopub.status.idle":"2022-06-20T05:03:44.32271Z","shell.execute_reply.started":"2022-06-20T05:03:44.313109Z","shell.execute_reply":"2022-06-20T05:03:44.321861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(x = \"diff\", data = val_df_l, bins = 100)\nplt.axvline(0.125, color = \"r\")\nplt.axvline(-0.125, color = \"r\")\n#the red lines are the number of \"correct predictions\"\n\n#number of \"wrong predictions\"\nprint(\"accuracy\", 1 - val_df_l[(val_df_l[\"diff\"] >= 0.125) | (val_df_l[\"diff\"] <= -0.125)].shape[0] / val_df_l.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:44.32393Z","iopub.execute_input":"2022-06-20T05:03:44.324307Z","iopub.status.idle":"2022-06-20T05:03:44.731872Z","shell.execute_reply.started":"2022-06-20T05:03:44.324267Z","shell.execute_reply":"2022-06-20T05:03:44.731174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l.groupby(\"gen_cat\")[\"diff\",\"good_pred\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:44.733174Z","iopub.execute_input":"2022-06-20T05:03:44.733592Z","iopub.status.idle":"2022-06-20T05:03:44.750733Z","shell.execute_reply.started":"2022-06-20T05:03:44.733554Z","shell.execute_reply":"2022-06-20T05:03:44.74976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(val_df_l.pred, val_df_l.score_noised, color = \"b\", marker = \".\")\n#the gap between 0.9 and 1.0 seems very interesting but goes hand in hand with our findings from the EDA","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:44.752156Z","iopub.execute_input":"2022-06-20T05:03:44.752439Z","iopub.status.idle":"2022-06-20T05:03:44.973831Z","shell.execute_reply.started":"2022-06-20T05:03:44.752401Z","shell.execute_reply":"2022-06-20T05:03:44.97316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove values lower than 0 and higher than 1\nval_df_l[\"preds_lim\"]  = val_df_l.pred.apply(lambda x: 0 if x < 0 else x)\nval_df_l[\"preds_lim\"]  = val_df_l.pred.apply(lambda x: 1 if x > 1 else x)\n\n#map to nearest value\nval_df_l.reset_index(inplace = True)\nval_df_l[\"preds_map\"] = 0\nfor i in val_df_l.index:\n    if val_df_l.pred.iloc[i] < 0.125: val_df_l.preds_map.iloc[i] = 0\n    elif val_df_l.pred.iloc[i] < 0.375: val_df_l.preds_map.iloc[i] = 0.25\n    elif val_df_l.pred.iloc[i] < 0.625: val_df_l.preds_map.iloc[i] = 0.50\n    elif val_df_l.pred.iloc[i] < 0.875: val_df_l.preds_map.iloc[i] = 0.75\n    elif val_df_l.pred.iloc[i] >= 0.875: val_df_l.preds_map.iloc[i] = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:44.975025Z","iopub.execute_input":"2022-06-20T05:03:44.975777Z","iopub.status.idle":"2022-06-20T05:03:46.541401Z","shell.execute_reply.started":"2022-06-20T05:03:44.975739Z","shell.execute_reply":"2022-06-20T05:03:46.540605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:46.542744Z","iopub.execute_input":"2022-06-20T05:03:46.542984Z","iopub.status.idle":"2022-06-20T05:03:46.564312Z","shell.execute_reply.started":"2022-06-20T05:03:46.542951Z","shell.execute_reply":"2022-06-20T05:03:46.563401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\n normal: {np.corrcoef(np.array(val_ds[\"label\"]), val_df_l[\"pred\"])[0][1]}\n with limits: {np.corrcoef(np.array(val_ds[\"label\"]), val_df_l[\"preds_lim\"])[0][1]}\n with mapping: {np.corrcoef(np.array(val_ds[\"label\"]), val_df_l[\"preds_map\"])[0][1]}\n \"\"\")\n#normal has best score","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:46.565788Z","iopub.execute_input":"2022-06-20T05:03:46.566268Z","iopub.status.idle":"2022-06-20T05:03:46.763562Z","shell.execute_reply.started":"2022-06-20T05:03:46.566154Z","shell.execute_reply":"2022-06-20T05:03:46.761992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create test predictions\n#prepare input; mostly copied from https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster\ntokenizer = AutoTokenizer.from_pretrained(deberta)\nsep = tokenizer.sep_token #is \"[SEP]\" #also try different seperators later for performance\ntest[\"inputs\"] = test.full + sep + test.anchor + sep + test.target + sep \neval_ds = Dataset.from_pandas(test)\n#tokenize\ntok_eval = eval_ds.map(tok_func, batched = True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_test_dataset = tok_eval.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=1,\n)\n\npreds_train_l = model_l.predict(tf_test_dataset)\ndeberta_pred = preds_train_l[\"logits\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:03:46.765104Z","iopub.execute_input":"2022-06-20T05:03:46.765434Z","iopub.status.idle":"2022-06-20T05:04:12.455555Z","shell.execute_reply.started":"2022-06-20T05:03:46.765391Z","shell.execute_reply":"2022-06-20T05:04:12.454759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:12.458752Z","iopub.execute_input":"2022-06-20T05:04:12.458964Z","iopub.status.idle":"2022-06-20T05:04:12.46795Z","shell.execute_reply.started":"2022-06-20T05:04:12.458939Z","shell.execute_reply":"2022-06-20T05:04:12.467245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patent BERT -> Val Score: 0.8205","metadata":{}},{"cell_type":"markdown","source":"Things that influenced patent Bert\n- Rarely had model dying \n- dynamic padding reduced score by .004 with short input (context + anchor + target)\n- score was better with full + anchor + target (~.005 better than short input)\n- score was best with context + title + anchor + target (~.01 better than short input) & dynamic padding\n- model was more consistent with batch size = 32","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:12.470887Z","iopub.execute_input":"2022-06-20T05:04:12.471403Z","iopub.status.idle":"2022-06-20T05:04:12.475978Z","shell.execute_reply.started":"2022-06-20T05:04:12.471327Z","shell.execute_reply":"2022-06-20T05:04:12.475106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patentbert = \"../input/bert-for-patents/bert-for-patents\"\n#BERT for Patents is a model trained by Google on 100M+ patents (not just US patents). It is based on BERTLARGE.\n\ntokenizer = AutoTokenizer.from_pretrained(patentbert)\nsep = tokenizer.sep_token #is \"[SEP]\"\n#df[\"inputs\"] = df.full + sep + df.anchor + sep + df.target + sep #using the translated gen_cat (\"full\") came out at ~0.814\ndf[\"inputs\"] = df.context + sep + df.title + sep + df.anchor + sep + df.target + sep \n#df[\"inputs\"] = df.context + sep + df.anchor + sep + df.target + sep #same shape as deberta\ndf1 = df[[\"inputs\",\"score\"]]\n\n\ndef tok_func(x): return tokenizer(x[\"inputs\"])\n\n#create ds from dataframe\nds = Dataset.from_pandas(df).rename_column('score', 'label')\n#split into seperate sections\nnew_ds = DatasetDict({\"train\":ds.select(trn_idxs),\n             \"val\": ds.select(val_idxs)})\n#split into seperate ds\ntrain_ds = new_ds[\"train\"]\nval_ds  = new_ds[\"val\"]\n#tokenize\ntok_train = train_ds.map(tok_func,batched = True) \ntok_val = val_ds.map(tok_func, batched = True)\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding = True, return_tensors=\"tf\")\ntf_train_dataset = tok_train.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=32\n)\ntf_validation_dataset = tok_val.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:12.477768Z","iopub.execute_input":"2022-06-20T05:04:12.478126Z","iopub.status.idle":"2022-06-20T05:04:18.136551Z","shell.execute_reply.started":"2022-06-20T05:04:12.478089Z","shell.execute_reply":"2022-06-20T05:04:18.135829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training of patent bert happened on collab: \n#https://colab.research.google.com/drive/1-bQ6bmojllp5bNMQb182KiM8f6QWpwvp?authuser=1#scrollTo=GggDEa5dXIAj","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:18.138058Z","iopub.execute_input":"2022-06-20T05:04:18.138271Z","iopub.status.idle":"2022-06-20T05:04:18.142844Z","shell.execute_reply.started":"2022-06-20T05:04:18.138241Z","shell.execute_reply":"2022-06-20T05:04:18.141788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\nmodel_pb = TFAutoModelForSequenceClassification.from_pretrained(patentbert, num_labels=1, from_pt = True) #importantly, we are loading from pytorch as nobody likes TF\nmodel_pb.load_weights(\"../input/valentin-patent-bert-finetuned/tf-patent-bert-finetuned(1).h5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:18.144846Z","iopub.execute_input":"2022-06-20T05:04:18.145541Z","iopub.status.idle":"2022-06-20T05:04:44.339577Z","shell.execute_reply.started":"2022-06-20T05:04:18.1455Z","shell.execute_reply":"2022-06-20T05:04:44.338756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets also analyse the error\npreds_pb = model_pb.predict(tf_validation_dataset)\n#quick reality check if the model took best model or last model\nnp.corrcoef(np.array(val_ds[\"label\"]).reshape(1,-1), preds_pb[\"logits\"].reshape(1,-1))[0][1]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:04:44.341233Z","iopub.execute_input":"2022-06-20T05:04:44.341522Z","iopub.status.idle":"2022-06-20T05:05:36.191967Z","shell.execute_reply.started":"2022-06-20T05:04:44.341486Z","shell.execute_reply":"2022-06-20T05:05:36.191231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l[\"preds_pb\"] = np.array(preds_pb[\"logits\"]).reshape(1,-1)[0]\nval_df_l[\"diff_pb\"] = val_df_l.score - val_df_l.preds_pb\nval_df_l[\"good_pred_pb\"] = np.abs(val_df_l[\"diff_pb\"]) < 0.125\n\nsns.histplot(x = \"diff_pb\", data = val_df_l, bins = 100)\nplt.axvline(0.125, color = \"r\")\nplt.axvline(-0.125, color = \"r\")\n#the red lines are the number of \"correct predictions\"\n\n#number of \"wrong predictions\"\nprint(\"accuracy\", 1 - val_df_l[(val_df_l[\"diff_pb\"] >= 0.125) | (val_df_l[\"diff_pb\"] <= -0.125)].shape[0] / val_df_l.shape[0])\n#accuracy is around the same as deberta large","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:36.195999Z","iopub.execute_input":"2022-06-20T05:05:36.196199Z","iopub.status.idle":"2022-06-20T05:05:36.605987Z","shell.execute_reply.started":"2022-06-20T05:05:36.196173Z","shell.execute_reply":"2022-06-20T05:05:36.605336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting predictions vs noised scored\nplt.scatter(val_df_l.preds_pb, val_df_l.score_noised, color = \"b\", marker = \".\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:36.607189Z","iopub.execute_input":"2022-06-20T05:05:36.607531Z","iopub.status.idle":"2022-06-20T05:05:36.829678Z","shell.execute_reply.started":"2022-06-20T05:05:36.607491Z","shell.execute_reply":"2022-06-20T05:05:36.829005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df_l.groupby(\"gen_cat\")[\"diff_pb\",\"good_pred_pb\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:36.83086Z","iopub.execute_input":"2022-06-20T05:05:36.831183Z","iopub.status.idle":"2022-06-20T05:05:36.846897Z","shell.execute_reply.started":"2022-06-20T05:05:36.831142Z","shell.execute_reply":"2022-06-20T05:05:36.846212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create test predictions\n#prepare input; mostly copied from https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster\ntokenizer = AutoTokenizer.from_pretrained(patentbert)\nsep = tokenizer.sep_token #is \"[SEP]\" #also try different seperators later for performance\ntest[\"inputs\"] = test.context + sep + test.title + sep + test.anchor + sep + test.target + sep\neval_ds = Dataset.from_pandas(test)\n#tokenize\ntok_eval = eval_ds.map(tok_func, batched = True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_test_dataset = tok_eval.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=1,\n)\n\npreds_test_pb = model_pb.predict(tf_test_dataset)\npb_pred = preds_test_pb[\"logits\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:36.848187Z","iopub.execute_input":"2022-06-20T05:05:36.848497Z","iopub.status.idle":"2022-06-20T05:05:43.888179Z","shell.execute_reply.started":"2022-06-20T05:05:36.848461Z","shell.execute_reply":"2022-06-20T05:05:43.887394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compare quote of in-range predictions for each model\nval_df_l.groupby(\"gen_cat\")[\"good_pred\",\"good_pred_elec\",\"good_pred_pb\"].mean()\n#the bert actually performs best on E; while it is hanging behind on all other \n#electra outperforms deberta on almost all categories in regards to accuracy","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:43.889764Z","iopub.execute_input":"2022-06-20T05:05:43.890006Z","iopub.status.idle":"2022-06-20T05:05:43.906914Z","shell.execute_reply.started":"2022-06-20T05:05:43.889972Z","shell.execute_reply":"2022-06-20T05:05:43.906171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gridsearch for the best ratio of models\nparts = np.arange(0, 1.005, 0.005)\ncomb_preds = []\nfor ratio_pb in parts:\n    comb_preds.append([ratio_pb, np.corrcoef((ratio_pb * preds_pb[\"logits\"] + (1 - ratio_pb) * preds_elec[\"logits\"]).reshape(-1),val_ds[\"label\"])[0][1]])\n\n#Plot the score of ratio\ncomb = pd.DataFrame(comb_preds, columns = [\"RatioPatentBert\", \"correlation\"])\nsns.lineplot(x = \"RatioPatentBert\", y = \"correlation\", data = comb)\nplt.xlabel(\"Ratio of PatentBert model (electra ratio = 1 - ratio of PB)\");\n#in combination the two models are really a lot better!\n#also around 0.5 seems to be the sweet spot","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:43.908199Z","iopub.execute_input":"2022-06-20T05:05:43.908496Z","iopub.status.idle":"2022-06-20T05:05:58.206848Z","shell.execute_reply.started":"2022-06-20T05:05:43.90846Z","shell.execute_reply":"2022-06-20T05:05:58.20616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gridsearch for the best ratio of models\nparts = np.arange(0, 1.005, 0.005)\ncomb_preds = []\nfor ratio_pb in parts:\n    comb_preds.append([ratio_pb, np.corrcoef((ratio_pb * preds_pb[\"logits\"] + (1 - ratio_pb) * preds_l[\"logits\"]).reshape(-1),val_ds[\"label\"])[0][1]])\n\n#Plot the score of ratio\ncomb = pd.DataFrame(comb_preds, columns = [\"RatioPatentBert\", \"correlation\"])\nsns.lineplot(x = \"RatioPatentBert\", y = \"correlation\", data = comb)\nplt.xlabel(\"Ratio of PatentBert model (DeBERTa ratio = 1 - ratio of PB)\");\n#in combination the two models are really a lot better!\n#also around 0.5 seems to be the sweet spot","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:05:58.208086Z","iopub.execute_input":"2022-06-20T05:05:58.209761Z","iopub.status.idle":"2022-06-20T05:06:11.715784Z","shell.execute_reply.started":"2022-06-20T05:05:58.209728Z","shell.execute_reply":"2022-06-20T05:06:11.715082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gridsearch for the best ratio of models\nparts = np.arange(0, 1.005, 0.005)\ncomb_preds = []\nfor ratio_elec in parts:\n    comb_preds.append([ratio_elec, np.corrcoef((ratio_elec * preds_elec[\"logits\"] + (1 - ratio_elec) * preds_l[\"logits\"]).reshape(-1),val_ds[\"label\"])[0][1]])\n\n#Plot the score of ratio\ncomb = pd.DataFrame(comb_preds, columns = [\"RatioElectra\", \"correlation\"])\nsns.lineplot(x = \"RatioElectra\", y = \"correlation\", data = comb)\nplt.xlabel(\"Ratio of Electra model (DeBERTa ratio = 1 - ratio of PB)\");\n#in combination the two models are really a lot better!\n#also around 0.5 seems to be the sweet spot","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:20:03.283889Z","iopub.execute_input":"2022-06-20T05:20:03.284174Z","iopub.status.idle":"2022-06-20T05:20:16.891875Z","shell.execute_reply.started":"2022-06-20T05:20:03.284145Z","shell.execute_reply":"2022-06-20T05:20:16.891164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GridSearch again\n#for 3models, we just search for ratio of best model and do (1-ratio) / 2the other two models \nparts = np.arange(0, 1.005, 0.005)\ncomb_preds = []\nfor ratio_db in parts:\n    comb_preds.append([ratio_db, np.corrcoef((((1-ratio_db)/2) * preds_pb[\"logits\"].reshape(-1) + ((1-ratio_db)/2) * preds_elec[\"logits\"].reshape(-1) + ratio_db * preds_l[\"logits\"].reshape(-1)),val_ds[\"label\"])[0][1]])\n\ncomb = pd.DataFrame(comb_preds, columns = [\"RatioDeberta\", \"correlation\"])\nbest_ratio_deberta = round(max(comb.correlation),3)\nprint(f\"maximum val score was reached on a ratio of {round(comb.RatioDeberta.iloc[comb.correlation.idxmax()],3)} with a score of {best_ratio_deberta}\")\nsns.lineplot(x = \"RatioDeberta\", y = \"correlation\", data = comb)\nplt.xlabel(\"Ratio of Deberta-large model (electra & patent bert ratio = (1 - ratio of PB)/2)\");\n#the model of 3 outperforms every model of 2","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:06:11.716974Z","iopub.execute_input":"2022-06-20T05:06:11.717212Z","iopub.status.idle":"2022-06-20T05:06:25.704105Z","shell.execute_reply.started":"2022-06-20T05:06:11.717178Z","shell.execute_reply":"2022-06-20T05:06:25.703395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for simplicity sake we will do a 3/3/4 split; with deberta having the highest portion\nensemble_val = np.corrcoef((0.3* preds_pb[\"logits\"].reshape(-1) + 0.3 * preds_elec[\"logits\"].reshape(-1) + 0.4 * preds_l[\"logits\"].reshape(-1)),val_ds[\"label\"])[0][1]\nensemble_val","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:20:39.640444Z","iopub.execute_input":"2022-06-20T05:20:39.640713Z","iopub.status.idle":"2022-06-20T05:20:39.715793Z","shell.execute_reply.started":"2022-06-20T05:20:39.640683Z","shell.execute_reply":"2022-06-20T05:20:39.715028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How is the distribution for this?\n#for graphical purposes; lets plot all 4 next to another\nval_df_l[\"ensemble\"] = (0.3 * preds_pb[\"logits\"] + 0.3 * preds_elec[\"logits\"] + 0.4 * preds_l[\"logits\"]).reshape(-1)\n\nfig, ax = plt.subplots(1,4, figsize = (20,4))\nax[0].scatter(val_df_l.pred_elec, val_df_l.score_noised, color = \"b\", marker = \".\")\nax[0].set_title(\"Predictions vs. Label - Electra\")\nax[1].scatter(val_df_l.pred, val_df_l.score_noised, color = \"b\", marker = \".\")\nax[1].set_title(\"Predictions vs. Label - DeBERTa Large\")\nax[2].scatter(val_df_l.preds_pb, val_df_l.score_noised, color = \"b\", marker = \".\")\nax[2].set_title(\"Predictions vs. Label - Patent Bert\")\nax[3].scatter(val_df_l.ensemble, val_df_l.score_noised, color = \"y\", marker = \".\")\nax[3].set_title(\"Predictions vs. Label - Ensemble\")\n\n#we can see that the ensembling forces the outliers of single models further into the middle\n#in general the paterns are very similar","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:20:39.886693Z","iopub.execute_input":"2022-06-20T05:20:39.886931Z","iopub.status.idle":"2022-06-20T05:20:41.127809Z","shell.execute_reply.started":"2022-06-20T05:20:39.886903Z","shell.execute_reply":"2022-06-20T05:20:41.127109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example data for validation set\npreds = pd.DataFrame((val_df_l.gen_cat, val_df_l.pred_elec, val_df_l.pred, val_df_l.preds_pb, val_df_l.ensemble, val_df_l.score)).T\npreds.columns = [\"category\",\"electra\",\"deberta\", \"bert\", \"ensemble\", \"score\"]\npreds.head(25)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:20:50.803397Z","iopub.execute_input":"2022-06-20T05:20:50.803673Z","iopub.status.idle":"2022-06-20T05:20:51.193015Z","shell.execute_reply.started":"2022-06-20T05:20:50.803642Z","shell.execute_reply":"2022-06-20T05:20:51.192256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats = preds.category.unique()\ndeb_corr_cats = []\npb_corr_cats = []\nelec_corr_cats = []\navg_corr_cats = []\nfor category in cats:\n    deb_corr_cats.append(np.corrcoef(list(preds[preds.category == category].deberta), list(preds[preds.category == category].score))[0][1])\n    pb_corr_cats.append(np.corrcoef(list(preds[preds.category == category].bert), list(preds[preds.category == category].score))[0][1])\n    elec_corr_cats.append(np.corrcoef(list(preds[preds.category == category].electra), list(preds[preds.category == category].score))[0][1])\n    avg_corr_cats.append(np.corrcoef(list(preds[preds.category == category].ensemble), list(preds[preds.category == category].score))[0][1])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:21:00.596884Z","iopub.execute_input":"2022-06-20T05:21:00.597159Z","iopub.status.idle":"2022-06-20T05:21:00.752072Z","shell.execute_reply.started":"2022-06-20T05:21:00.597128Z","shell.execute_reply":"2022-06-20T05:21:00.751281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs = pd.DataFrame(preds.category.unique(), columns = [\"category\"])\ncorrs[\"correlation_deberta\"] = deb_corr_cats\ncorrs[\"correlation_pb\"] = pb_corr_cats\ncorrs[\"correlation_electra\"] = elec_corr_cats\ncorrs[\"avg_corr\"] = avg_corr_cats\ncorrs.sort_values(\"category\")\n#the ensemble outperforms every individual model along all categories\n\n#strong categories: B, D, E, F\n#Week categories: A, C, G, H","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:28:21.590252Z","iopub.execute_input":"2022-06-20T05:28:21.590814Z","iopub.status.idle":"2022-06-20T05:28:21.609865Z","shell.execute_reply.started":"2022-06-20T05:28:21.590772Z","shell.execute_reply":"2022-06-20T05:28:21.609093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs = corrs.merge(test.groupby(\"gen_cat\").id.count().reset_index(), left_on = \"category\", right_on = \"gen_cat\").drop(columns = [\"gen_cat\"])\ncorrs\n#our weeker categories make up around 53% of the dataset\n#crossvalidation would probably help with this","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:28:44.758937Z","iopub.execute_input":"2022-06-20T05:28:44.759329Z","iopub.status.idle":"2022-06-20T05:28:44.772124Z","shell.execute_reply.started":"2022-06-20T05:28:44.759292Z","shell.execute_reply":"2022-06-20T05:28:44.771408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs[\"weighted\"] = corrs.avg_corr * corrs.id / sum(corrs.id)\nsum(corrs.weighted) #we would expect a lb score around this \n\n#Note from future Valentin: Since we performed worse than this, it seems like the categories are among the categories which were harder to classify\n#As shown in the EDA several contexts are in validation but not in test; the ones in test may be harder to classify","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:30:46.715283Z","iopub.execute_input":"2022-06-20T05:30:46.716056Z","iopub.status.idle":"2022-06-20T05:30:46.726166Z","shell.execute_reply.started":"2022-06-20T05:30:46.716009Z","shell.execute_reply":"2022-06-20T05:30:46.722687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"#print all predictions and the ensembled score in a dataframe\npred_ensemble = 0.3 * elec_pred.reshape(-1) + 0.4 * deberta_pred.reshape(-1) + 0.3 * preds_test_pb[\"logits\"].reshape(-1)\npreds = pd.DataFrame((elec_pred.reshape(-1), deberta_pred.reshape(-1), preds_test_pb[\"logits\"].reshape(-1), pred_ensemble)).T\npreds.columns = [\"electra\",\"deberta\", \"bert\", \"ensemble\"]\npreds.head(36)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:06:26.839236Z","iopub.execute_input":"2022-06-20T05:06:26.839499Z","iopub.status.idle":"2022-06-20T05:06:26.861883Z","shell.execute_reply.started":"2022-06-20T05:06:26.839462Z","shell.execute_reply":"2022-06-20T05:06:26.861212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsub[\"score\"] = pred_ensemble\nsub[[\"id\",\"score\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:06:26.863154Z","iopub.execute_input":"2022-06-20T05:06:26.863431Z","iopub.status.idle":"2022-06-20T05:06:26.878312Z","shell.execute_reply.started":"2022-06-20T05:06:26.863395Z","shell.execute_reply":"2022-06-20T05:06:26.877689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Open To-Do's that would influence the performance but are out of scope\n- **Cross Validation** (was commonly used on the fit and resulted in significantly better scores)\n    - sadly, this would mean training the whole models 3+ times, which would heavily increase GPU runtime\n    - there are workarounds, but sadly, the time was not available for me\n- weight decay (somehow doesnt seem to be a thing with Keras; would have to use PyTorch / TrainerAPI for it)\n    - i tried with AdamW from tensorflow but it didn't work with the setup\n- trying more models; see how well an ensemble of 4 or 5 would perform; current model selection was not analysed ","metadata":{}},{"cell_type":"markdown","source":"# What I have learned from this Challenge\n- PyTorch seems to be way more popular than Keras, but PyTorch models can also be run from Keras\n- Batching the inputs matters for RAM (very important on Kaggle / Collab); this can be \"tricked\" with gradient accumulation (which is not a thing in keras apparently)\n- Fine tuning includes a lot of things apart from Hyperparameters; such as deciding on input formats; sep-tokens etc.\n- there is a multitude of fine-tunable models for the problem; eventhough it doesnt seem like it at first\n- Custom metrics in Keras are a thing (but are hard to get to work with normal Keras functions)\n- **Ensemble models** are a thing (and often outperform its strongest individual models)\n    - it seems like even though the individual models perform at different levels, simply averaging them will give the best result\n- Managing GPU usage time is critical in a kaggle competition\n\n- Warmup ratio matters a lot\n- Playing around with inputs matters a lot\n- Sometimes the order of the models in which they are computed seem to matter! (must be some hardware / GPU thing)\n- Submissions to Kaggle are painful: it is so hard to debug kaggle code before submitting and often submissions are wasted or fail on the last lines of code due to little changes\n    - from my submissions (33):\n    - 1 Submission for solely EDA \n    - 3 versions that actually worked as intended\n    - 20 crashed submissions to figure out how to submit on kaggle\n    - 4 included models that did not train properly\n    - 3 were just saves to clean up code\n    - 2 were missclicks that were submitted when they werent ready\n\n- just dont spend 4 hours on every submission and train the model once on colab -> save weight -> upload to kaggle -> restore weights\n    ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}