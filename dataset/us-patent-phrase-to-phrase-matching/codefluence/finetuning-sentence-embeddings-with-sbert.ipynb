{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Finetuning sentence embeddings with SBERT\n\nInstead of a cross-encoder, Sentence-BERT (SBERT) uses a siamese network, two encoders that yield independent embeddings (but semantically comparable) for each of the input sentences.\n\nPaper: https://arxiv.org/abs/1908.10084  \nA number of pre-trained SBERT models are available: https://www.sbert.net/docs/pretrained_models.html\n\nMy attempt to fine-tune the SBERT pre-trained model with the data of this competition is to replicate the siamese network structure with two instances of the model and train it with pairs of anchor and target strings. The scores guide what the cosine similarity should be for each pair, moving the embeddings to the patent space.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage('../input/patent-img/cosim.png', width = 600, height = 250)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:43:37.888264Z","iopub.execute_input":"2022-04-29T22:43:37.889009Z","iopub.status.idle":"2022-04-29T22:43:37.898524Z","shell.execute_reply.started":"2022-04-29T22:43:37.888932Z","shell.execute_reply":"2022-04-29T22:43:37.897762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel\n\n\nDATA_PATH_TRAIN = '../input/us-patent-phrase-to-phrase-matching/train.csv'\nDATA_PATH_TEST  = '../input/us-patent-phrase-to-phrase-matching/test.csv'\n\nTOKENIZER_DIR_LOAD = '../input/patent-tokenizer/'\nTOKENIZER_DIR_SAVE = './tokenizer'\n\nMODEL_DIR_LOAD = '../input/patent-models/'\nMODEL_DIR_SAVE = './models'\n\nMODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\nHUGGING_FACE_CACHE = './hugging_face_cache'\n\n\nIS_INFERENCE_TIME = True","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:43:37.90037Z","iopub.execute_input":"2022-04-29T22:43:37.900669Z","iopub.status.idle":"2022-04-29T22:43:37.91034Z","shell.execute_reply.started":"2022-04-29T22:43:37.900629Z","shell.execute_reply":"2022-04-29T22:43:37.909326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatentDataModule(pl.LightningDataModule):\n\n    def __init__(self, data_path, fold):\n\n        super(PatentDataModule, self).__init__()\n\n        print('reading data...')\n        data = pd.read_csv(data_path)\n\n        dict_code_descs = dict( A = 'Human Necessities', \n                                B = 'Operations and Transport', \n                                C = 'Chemistry and Metallurgy', \n                                D = 'Textiles', \n                                E = 'Fixed Constructions', \n                                F = 'Mechanical Engineering', \n                                G = 'Physics', \n                                H = 'Electricity', \n                                Y = 'Emerging Cross-Sectional Technologies')\n\n        data['cpc_code'] = 'cpc' + data.context.str.lower()\n        data['cpc_desc'] = data.context.apply(lambda x: dict_code_descs[x[0]].lower())\n\n        data.anchor = data.anchor + ' ' + data.cpc_code + ' ' + data.cpc_desc\n        data.target = data.target + ' ' + data.cpc_code + ' ' + data.cpc_desc\n\n        max_length = max(map(lambda x: len(x.split(' ')) +  1, pd.concat((data.anchor, data.target))))\n        print('max_length:',max_length)\n\n        if IS_INFERENCE_TIME:\n            \n            tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR_LOAD)\n        else:\n            \n            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HUGGING_FACE_CACHE)\n            tokenizer.add_tokens(list(pd.unique('cpc' + pd.read_csv(DATA_PATH_TRAIN).context.str.lower())))\n            tokenizer.save_pretrained(TOKENIZER_DIR_SAVE)\n\n        if not 'score' in data.columns:\n            # for test data\n            data['score'] = np.nan\n\n        train = []\n        val = []\n        i = 0\n\n        if fold >= 0:\n\n            for _, group in data.groupby('anchor'):\n\n                if i % 5 == fold:\n                    val.append(group)\n                else:\n                    train.append(group)\n\n                i += 1\n        else:\n            val.append(data)\n\n        if len(train) > 0:\n\n            train = pd.concat(train)\n            self.train_ds = PatentDataSet(train.anchor, train.target, train.score, tokenizer, max_length)\n\n        if len(val) > 0:\n\n            val = pd.concat(val)\n            self.val_ds = PatentDataSet(val.anchor, val.target, val.score, tokenizer, max_length)\n\n    def train_dataloader(self):\n\n        return DataLoader(self.train_ds, shuffle=True, batch_size=128)\n\n    def val_dataloader(self):\n\n        return DataLoader(self.val_ds, shuffle=False, batch_size=512)\n\n\nclass PatentDataSet(Dataset):\n    \n    def __init__(self, anchor, target, score, tokenizer, max_length):\n\n        super(PatentDataSet, self).__init__()\n\n        self.score = score.to_numpy(dtype=np.float32)\n\n        print('tokenization of data...')\n\n        bert_anchor = tokenizer(    list(anchor),\n                                    padding = \"max_length\",\n                                    max_length = max_length,\n                                    truncation = True,\n                                    return_tensors=\"pt\")\n\n        self.anchor = bert_anchor['input_ids']\n        self.anchor_mask = bert_anchor['attention_mask']\n\n        bert_target = tokenizer(    list(target),\n                                    padding = \"max_length\",\n                                    max_length = max_length,\n                                    truncation = True,\n                                    return_tensors=\"pt\")\n\n        self.target = bert_target['input_ids']\n        self.target_mask = bert_target['attention_mask']\n\n    def __len__(self):\n\n        return len(self.score)\n\n    def __getitem__(self, idx):\n\n        return self.anchor[idx], self.anchor_mask[idx], self.target[idx], self.target_mask[idx], self.score[idx]\n\n\nclass PatentTransfomer(LightningModule):\n\n    def __init__(self):\n\n        super(PatentTransfomer, self).__init__()\n\n        if IS_INFERENCE_TIME:\n\n            config = AutoConfig.from_pretrained(MODEL_DIR_LOAD)\n\n            self.transformer_anchor = AutoModel.from_config(config)\n            self.transformer_target = AutoModel.from_config(config) \n\n        else:\n            self.transformer_anchor = AutoModel.from_pretrained(MODEL_NAME, cache_dir=HUGGING_FACE_CACHE)\n            self.transformer_target = AutoModel.from_pretrained(MODEL_NAME, cache_dir=HUGGING_FACE_CACHE)\n            \n            tokenizer_length = len(AutoTokenizer.from_pretrained(TOKENIZER_DIR_LOAD))\n            self.transformer_anchor.resize_token_embeddings(tokenizer_length)\n            self.transformer_target.resize_token_embeddings(tokenizer_length)\n\n            # Weights are not saved, only the config file to load the layers during inference.\n            # Weights will be saved / loaded with pytorch lightning instead.\n            self.transformer_target.save_pretrained(MODEL_DIR_SAVE,state_dict={})\n\n        # freezing first layers parameters?\n        # for i in range(6):\n        #     self.transformer_anchor.encoder.layer[i].requires_grad_(False)\n        #     self.transformer_target.encoder.layer[i].requires_grad_(False)\n \n    def mean_pooling(self, model_output, attention_mask):\n\n        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def forward(self, anchor, anchor_mask, target, target_mask):\n\n        if False and IS_INFERENCE_TIME:\n            embeddings_anchor = self.transformer_target(anchor, anchor_mask)\n        else:\n            embeddings_anchor = self.transformer_anchor(anchor, anchor_mask)\n\n        embeddings_target = self.transformer_target(target, target_mask)\n\n        return F.cosine_similarity( self.mean_pooling(embeddings_anchor, anchor_mask),\n                                    self.mean_pooling(embeddings_target, target_mask))\n\n    def training_step(self, train_batch, batch_idx):\n\n        return self.process_batch(*train_batch, 'train')\n\n    def validation_step(self, val_batch, batch_idx):\n        \n        with torch.no_grad():\n            self.process_batch(*val_batch, 'val')\n\n    def process_batch(self, anchor, anchor_mask, target, target_mask, target_scores, prefix):\n\n        cos_similarity = self.forward(anchor, anchor_mask, target, target_mask)\n\n        loss = - self.pearson_corr(cos_similarity, target_scores)\n\n        self.log(prefix+'_loss', loss.item())\n\n        return loss\n\n    def pearson_corr(self, x, y):\n\n        cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n        return cos(x - x.mean(dim=0, keepdim=True), y - y.mean(dim=0, keepdim=True))\n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n        sccheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, factor=1/5, verbose=True, min_lr=1e-7)\n        return [optimizer], {'scheduler': sccheduler, 'monitor': 'val_loss'}","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:43:37.912068Z","iopub.execute_input":"2022-04-29T22:43:37.912846Z","iopub.status.idle":"2022-04-29T22:43:37.952855Z","shell.execute_reply.started":"2022-04-29T22:43:37.912802Z","shell.execute_reply":"2022-04-29T22:43:37.95176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# num_folds = 5\nnum_folds = 1\n\nif IS_INFERENCE_TIME:\n\n    data = PatentDataModule(DATA_PATH_TEST, -1)\n\n    test_scores = np.zeros(len(data.val_ds), dtype=np.float32)\n\n    for fold in range(num_folds):\n\n        print('loading model for fold {} and scoring...'.format(fold))\n        model = PatentTransfomer.load_from_checkpoint(  '{}sbert_{}.ckpt'.format(MODEL_DIR_LOAD,fold))\n\n        model.eval()\n        model.to(device)\n\n        fold_scores = []\n\n        for batch in DataLoader(data.val_ds, shuffle=False, batch_size=128):\n            \n            fold_scores.append(model(   batch[0].to(device),\n                                        batch[1].to(device),\n                                        batch[2].to(device),\n                                        batch[3].to(device)\n                                    ).detach().cpu().numpy())\n\n        test_scores = test_scores + np.concatenate(fold_scores)\n\n    test_scores = test_scores / num_folds\n\n    print('submitting...')\n    test_df = pd.read_csv(DATA_PATH_TEST)\n    sub_df = pd.DataFrame(data={\n        \"id\": test_df[\"id\"],\n        \"score\": test_scores\n    })\n\n    sub_df.to_csv(\"submission.csv\", index=False)\n    # print(sub_df)\n\nelse:\n\n    if not os.path.exists(TOKENIZER_DIR_SAVE):\n        os.makedirs(TOKENIZER_DIR_SAVE)\n\n    if not os.path.exists(MODEL_DIR_SAVE):\n        os.makedirs(MODEL_DIR_SAVE)\n\n    if not os.path.exists(HUGGING_FACE_CACHE):\n        os.makedirs(HUGGING_FACE_CACHE)\n\n    for fold in range(num_folds):\n\n        print('training model for fold {}...'.format(fold))\n\n        checkpoint_callback = ModelCheckpoint(\n            dirpath=MODEL_DIR_SAVE,\n            filename='sbert_{}'.format(fold),\n            save_top_k=1,\n            monitor='val_loss',\n            mode='min',\n            verbose=True\n        )\n\n        early_stop_callback = EarlyStopping(\n            patience=5,\n            monitor='val_loss',\n            mode='min',\n            verbose=True\n        )\n\n        trainer = pl.Trainer(   logger=pl_loggers.TensorBoardLogger('./logs/'),\n                                gpus=1,\n                                max_epochs=10,\n                                callbacks=[early_stop_callback,checkpoint_callback] )\n\n        data = PatentDataModule(DATA_PATH_TRAIN, fold)\n        model = PatentTransfomer()\n        trainer.fit(model, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:43:38.137159Z","iopub.execute_input":"2022-04-29T22:43:38.137572Z","iopub.status.idle":"2022-04-29T22:43:54.770106Z","shell.execute_reply.started":"2022-04-29T22:43:38.137523Z","shell.execute_reply":"2022-04-29T22:43:54.76912Z"},"trusted":true},"execution_count":null,"outputs":[]}]}