{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# US Patent Phrase","metadata":{}},{"cell_type":"markdown","source":"This notebook is inspired by https://www.kaggle.com/code/vad13irt/uspppm-baseline-inference. Huge thanks to Vadim Irtlach.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:53.424016Z","iopub.execute_input":"2022-06-11T17:11:53.424825Z","iopub.status.idle":"2022-06-11T17:11:55.484152Z","shell.execute_reply.started":"2022-06-11T17:11:53.42478Z","shell.execute_reply":"2022-06-11T17:11:55.482939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration, Timer, Averager\nfrom torch_components.utils import seed_everything, get_batch, load_checkpoint\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.model_selection import StratifiedKFold\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport random\nimport os\nimport shutil\nimport gc\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEBUG = False\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"EXPERIMENT_NAME\"] = \"none\"\n\n        \nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-11T17:11:55.488639Z","iopub.execute_input":"2022-06-11T17:11:55.488965Z","iopub.status.idle":"2022-06-11T17:11:55.500517Z","shell.execute_reply.started":"2022-06-11T17:11:55.488928Z","shell.execute_reply":"2022-06-11T17:11:55.499469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pathes = Configuration(train=\"../input/us-patent-phrase-to-phrase-matching/train.csv\", \n                       test=\"../input/us-patent-phrase-to-phrase-matching/test.csv\",\n                       sample_submission=\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\",\n                       cpc_codes=\"../input/cpc-codes/titles.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.501859Z","iopub.execute_input":"2022-06-11T17:11:55.502091Z","iopub.status.idle":"2022-06-11T17:11:55.519263Z","shell.execute_reply.started":"2022-06-11T17:11:55.502063Z","shell.execute_reply":"2022-06-11T17:11:55.518353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"config = Configuration(seed=42,\n                       max_length=72,\n                       batch_size=24,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=4,  \n                       verbose=250,\n                       device=DEVICE,\n                       amp=True, \n                       input_directory=\"../input/us-bert-for-patents-anchor-change\",\n                       debug=True)\n\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.522211Z","iopub.execute_input":"2022-06-11T17:11:55.522946Z","iopub.status.idle":"2022-06-11T17:11:55.535904Z","shell.execute_reply.started":"2022-06-11T17:11:55.522893Z","shell.execute_reply":"2022-06-11T17:11:55.534985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def create_submission(ids, predictions, path=\"submission.csv\"):\n    submission = pd.DataFrame({\n        \"id\": ids,\n        \"score\": predictions,\n    })\n    \n    submission.to_csv(path, index=False)\n    return submission\n\ndef prediction_loop(loader, \n                    model, \n                    device=\"cpu\", \n                    amp=False, \n                    verbose=1, \n                    time_format=\"{hours}:{minutes}:{seconds}\", \n                    logger=\"print\"):\n    \n    if device is not None:\n        model.to(device)\n    \n    model.eval()\n    outputs = []\n    timer = Timer(time_format)\n    steps = len(loader)\n    \n    if logger == \"tqdm\":\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n        loader.set_description_str(\"[Prediction]\")\n    \n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_outputs = prediction_step(batch=batch, model=model, device=device)\n                \n            outputs.extend(batch_outputs.to(\"cpu\").numpy())\n            \n            if logger == \"print\":\n                if step % verbose == 0 or step == steps:\n                    elapsed, remain = timer(step/steps)\n\n                    print(f\"[Prediction] \"\n                          f\"{step}/{steps} - \"\n                          f\"remain: {remain}\")\n            \n    outputs = torch.tensor(outputs)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.53726Z","iopub.execute_input":"2022-06-11T17:11:55.537515Z","iopub.status.idle":"2022-06-11T17:11:55.553156Z","shell.execute_reply.started":"2022-06-11T17:11:55.537477Z","shell.execute_reply":"2022-06-11T17:11:55.552394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_step(batch, model, device=\"cpu\"):\n    input_ids, attention_mask = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    \n    outputs = model(input_ids, attention_mask)\n    \n    return outputs.sigmoid().squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.554079Z","iopub.execute_input":"2022-06-11T17:11:55.554316Z","iopub.status.idle":"2022-06-11T17:11:55.57184Z","shell.execute_reply.started":"2022-06-11T17:11:55.554287Z","shell.execute_reply":"2022-06-11T17:11:55.571004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"print(pd.read_csv(pathes.train))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.573353Z","iopub.execute_input":"2022-06-11T17:11:55.573703Z","iopub.status.idle":"2022-06-11T17:11:55.688429Z","shell.execute_reply.started":"2022-06-11T17:11:55.573662Z","shell.execute_reply":"2022-06-11T17:11:55.687395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(pathes.cpc_codes)\n\npath = pathes.train if DEBUG else pathes.test \ntest = pd.read_csv(path)\ntest = test.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\ntest_ids = test[\"id\"].values\n\nsample_submission = pd.read_csv(pathes.sample_submission)\n\nif config.debug:\n    display(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:55.68986Z","iopub.execute_input":"2022-06-11T17:11:55.690177Z","iopub.status.idle":"2022-06-11T17:11:56.71287Z","shell.execute_reply.started":"2022-06-11T17:11:55.690136Z","shell.execute_reply":"2022-06-11T17:11:56.711948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_texts = torch.load(\"../input/foldsdump/cpc_texts.pth\")\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.714231Z","iopub.execute_input":"2022-06-11T17:11:56.714471Z","iopub.status.idle":"2022-06-11T17:11:56.72863Z","shell.execute_reply.started":"2022-06-11T17:11:56.714439Z","shell.execute_reply":"2022-06-11T17:11:56.727347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.731996Z","iopub.execute_input":"2022-06-11T17:11:56.732524Z","iopub.status.idle":"2022-06-11T17:11:56.770387Z","shell.execute_reply.started":"2022-06-11T17:11:56.732489Z","shell.execute_reply":"2022-06-11T17:11:56.769666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer_path = os.path.join(config.input_directory, \"tokenizer/\")\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.772237Z","iopub.execute_input":"2022-06-11T17:11:56.77297Z","iopub.status.idle":"2022-06-11T17:11:56.86072Z","shell.execute_reply.started":"2022-06-11T17:11:56.772919Z","shell.execute_reply":"2022-06-11T17:11:56.859676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.862123Z","iopub.execute_input":"2022-06-11T17:11:56.864364Z","iopub.status.idle":"2022-06-11T17:11:56.880356Z","shell.execute_reply.started":"2022-06-11T17:11:56.864324Z","shell.execute_reply":"2022-06-11T17:11:56.879539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n        def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.884325Z","iopub.execute_input":"2022-06-11T17:11:56.884843Z","iopub.status.idle":"2022-06-11T17:11:56.898113Z","shell.execute_reply.started":"2022-06-11T17:11:56.884793Z","shell.execute_reply":"2022-06-11T17:11:56.897223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.90177Z","iopub.execute_input":"2022-06-11T17:11:56.902122Z","iopub.status.idle":"2022-06-11T17:11:56.913465Z","shell.execute_reply.started":"2022-06-11T17:11:56.902084Z","shell.execute_reply":"2022-06-11T17:11:56.91281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"class TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_input_length):\n        self.text = df['text'].values.astype(str)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = self.text[item]\n        \n        inputs = self.tokenizer(inputs,\n                    max_length=self.max_input_length,\n                    padding='max_length',\n                    truncation=True )\n        return torch.as_tensor(inputs['input_ids'], dtype=torch.long),\\\n               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long),\\\n               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.914406Z","iopub.execute_input":"2022-06-11T17:11:56.91514Z","iopub.status.idle":"2022-06-11T17:11:56.931331Z","shell.execute_reply.started":"2022-06-11T17:11:56.915102Z","shell.execute_reply":"2022-06-11T17:11:56.930397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.933261Z","iopub.execute_input":"2022-06-11T17:11:56.933881Z","iopub.status.idle":"2022-06-11T17:11:56.9784Z","shell.execute_reply.started":"2022-06-11T17:11:56.933834Z","shell.execute_reply":"2022-06-11T17:11:56.977583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collator = Collator(return_targets=False, tokenizer=tokenizer, max_length=config.max_length)\n\ntest_dataset = Dataset(texts=test[\"text\"].values, \n                       pair_texts=test[\"target\"].values,\n                       contexts=test[\"title\"].values,\n                       max_length=config.max_length,\n                       sep=tokenizer.sep_token,\n                       tokenizer=tokenizer)\n    \ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=config.batch_size*2, \n                         num_workers=config.num_workers,\n                         pin_memory=config.pin_memory,\n                         collate_fn=collator,\n                         shuffle=False, \n                         drop_last=False)\n\nprint(f\"Test Samples: {len(test_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.97991Z","iopub.execute_input":"2022-06-11T17:11:56.980366Z","iopub.status.idle":"2022-06-11T17:11:56.990844Z","shell.execute_reply.started":"2022-06-11T17:11:56.980325Z","shell.execute_reply":"2022-06-11T17:11:56.9898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"../input/us-bert-for-patents-anchor-change\", config_path=None, config_updates={}, reinitialization_layers=0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n                \n                \n        self.reinit_layers(n=reinitialization_layers, layers=self.model.encoder.layer, std=self.config.initializer_range)\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n    \n    \n    def reinit_layers(self, layers, n=0, std=0.02):\n        if n > 0:\n            for layer in layers[-n:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=std)\n            \n            print(f\"Reinitializated last {n} layers.\")\n                \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:56.992616Z","iopub.execute_input":"2022-06-11T17:11:56.992924Z","iopub.status.idle":"2022-06-11T17:11:57.011047Z","shell.execute_reply.started":"2022-06-11T17:11:56.992885Z","shell.execute_reply":"2022-06-11T17:11:57.009961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"oof_predictions = []\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold [{fold}/{config.folds}]\")\n    \n    fold_directory = os.path.join(config.input_directory, f\"fold_{fold}/\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    checkpoint_path = os.path.join(checkpoints_directory, \"checkpoint.pth\")\n    \n    model = Model(config_path=model_config_path)\n    \n    fold_checkpoint = load_checkpoint(path=checkpoint_path, \n                                      model=model, \n                                      strict=True, \n                                      ignore_warnings=True)\n    \n    \n    print(f\"Loaded checkpoint from '{checkpoint_path}'.\")\n    \n    fold_predictions = prediction_loop(loader=test_loader, \n                                       model=model, \n                                       amp=config.amp, \n                                       device=config.device)\n    \n    oof_predictions.append(fold_predictions.numpy())\n    \n    del model, fold_checkpoint, fold_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*3)\n    \noof_predictions = np.array(oof_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:11:57.012465Z","iopub.execute_input":"2022-06-11T17:11:57.012823Z","iopub.status.idle":"2022-06-11T17:15:59.723693Z","shell.execute_reply.started":"2022-06-11T17:11:57.012786Z","shell.execute_reply":"2022-06-11T17:15:59.722795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = np.mean(oof_predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:15:59.726901Z","iopub.execute_input":"2022-06-11T17:15:59.727324Z","iopub.status.idle":"2022-06-11T17:15:59.734638Z","shell.execute_reply.started":"2022-06-11T17:15:59.727284Z","shell.execute_reply":"2022-06-11T17:15:59.733131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_submission(ids=test_ids, predictions=test_predictions, path=\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:15:59.736342Z","iopub.execute_input":"2022-06-11T17:15:59.736745Z","iopub.status.idle":"2022-06-11T17:15:59.773883Z","shell.execute_reply.started":"2022-06-11T17:15:59.736683Z","shell.execute_reply":"2022-06-11T17:15:59.773136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}