{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Will soon be uploading the datasets as well. For furthur improvement might add deberta base model and bert for patents. Any other suggestion/feedback/critic will be appreciated. Thanks Vadim Irtlach for his valuable notebooks. Be sure to check him out.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration, Timer, Averager\nfrom torch_components.utils import seed_everything, get_batch, load_checkpoint\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.model_selection import StratifiedKFold\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport random\nimport os\nimport shutil\nimport gc\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEBUG = False\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"EXPERIMENT_NAME\"] = \"none\"\n\n        \nwarnings.simplefilter(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Small","metadata":{}},{"cell_type":"code","source":"pathes = Configuration(train=\"../input/us-patent-phrase-to-phrase-matching/train.csv\", \n                       test=\"../input/us-patent-phrase-to-phrase-matching/test.csv\",\n                       sample_submission=\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\",\n                       cpc_codes=\"../input/cpc-codes/titles.csv\")\nConfiguration\nconfig = Configuration(seed=42,\n                       max_length=72,\n                       batch_size=24,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=4,  \n                       verbose=250,\n                       device=DEVICE,\n                       amp=True, \n                       input_directory=\"../input/deberta-v3-small-anchor-change\",\n                       debug=True)\n\nseed_everything(config.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission(ids, predictions, path=\"submission.csv\"):\n    submission = pd.DataFrame({\n        \"id\": ids,\n        \"score\": predictions,\n    })\n    \n    submission.to_csv(path, index=False)\n    return submission\n\ndef prediction_loop(loader, \n                    model, \n                    device=\"cpu\", \n                    amp=False, \n                    verbose=1, \n                    time_format=\"{hours}:{minutes}:{seconds}\", \n                    logger=\"print\"):\n    \n    if device is not None:\n        model.to(device)\n    \n    model.eval()\n    outputs = []\n    timer = Timer(time_format)\n    steps = len(loader)\n    \n    if logger == \"tqdm\":\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n        loader.set_description_str(\"[Prediction]\")\n    \n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_outputs = prediction_step(batch=batch, model=model, device=device)\n                \n            outputs.extend(batch_outputs.to(\"cpu\").numpy())\n            \n            if logger == \"print\":\n                if step % verbose == 0 or step == steps:\n                    elapsed, remain = timer(step/steps)\n\n                    print(f\"[Prediction] \"\n                          f\"{step}/{steps} - \"\n                          f\"remain: {remain}\")\n            \n    outputs = torch.tensor(outputs)\n    return outputs\ndef prediction_step(batch, model, device=\"cpu\"):\n    input_ids, attention_mask = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    \n    outputs = model(input_ids, attention_mask)\n    \n    return outputs.sigmoid().squeeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(pd.read_csv(pathes.train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(pathes.cpc_codes)\n\npath = pathes.train if DEBUG else pathes.test \ntest = pd.read_csv(path)\ntest = test.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\ntest_ids = test[\"id\"].values\n\nsample_submission = pd.read_csv(pathes.sample_submission)\n\nif config.debug:\n    display(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_texts = torch.load(\"../input/foldsdump/cpc_texts.pth\")\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_path = os.path.join(config.input_directory, \"tokenizer/\")\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collator = Collator(return_targets=False, tokenizer=tokenizer, max_length=config.max_length)\n\ntest_dataset = Dataset(texts=test[\"text\"].values, \n                       pair_texts=test[\"target\"].values,\n                       contexts=test[\"title\"].values,\n                       max_length=config.max_length,\n                       sep=tokenizer.sep_token,\n                       tokenizer=tokenizer)\n    \ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=config.batch_size*2, \n                         num_workers=config.num_workers,\n                         pin_memory=config.pin_memory,\n                         collate_fn=collator,\n                         shuffle=False, \n                         drop_last=False)\n\nprint(f\"Test Samples: {len(test_dataset)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"../input/deberta-small-real\", config_path=None, config_updates={}, reinitialization_layers=0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n                \n                \n        self.reinit_layers(n=reinitialization_layers, layers=self.model.encoder.layer, std=self.config.initializer_range)\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n    \n    \n    def reinit_layers(self, layers, n=0, std=0.02):\n        if n > 0:\n            for layer in layers[-n:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=std)\n            \n            print(f\"Reinitializated last {n} layers.\")\n                \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        return outputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noof_predictions = []\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold [{fold}/{config.folds}]\")\n    \n    fold_directory = os.path.join(config.input_directory, f\"fold_{fold}/\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    checkpoint_path = os.path.join(checkpoints_directory, \"checkpoint.pth\")\n    \n    model = Model(config_path=model_config_path)\n    \n    fold_checkpoint = load_checkpoint(path=checkpoint_path, \n                                      model=model, \n                                      strict=True, \n                                      ignore_warnings=True)\n    \n    \n    print(f\"Loaded checkpoint from '{checkpoint_path}'.\")\n    \n    fold_predictions = prediction_loop(loader=test_loader, \n                                       model=model, \n                                       amp=config.amp, \n                                       device=config.device)\n    \n    oof_predictions.append(fold_predictions.numpy())\n    \n    del model, fold_checkpoint, fold_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*3)\n    \noof_predictions = np.array(oof_predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_predictions = np.mean(oof_predictions, axis=0)\n\nsmall_preds=pd.concat([pd.DataFrame(test_ids),pd.DataFrame(small_predictions)],keys=[\"ids\",\"score\"],axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LARGE TURN","metadata":{}},{"cell_type":"code","source":"config = Configuration(seed=42,\n                       max_length=72,\n                       batch_size=24,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=4,  \n                       verbose=250,\n                       device=DEVICE,\n                       amp=True, \n                       input_directory=\"../input/large-anchor-change\",\n                       debug=True)\n\nseed_everything(config.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(pathes.cpc_codes)\n\npath = pathes.train if DEBUG else pathes.test \ntest = pd.read_csv(path)\ntest = test.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\ntest_ids = test[\"id\"].values\n\nsample_submission = pd.read_csv(pathes.sample_submission)\n\nif config.debug:\n    display(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_texts = torch.load(\"../input/foldsdump/cpc_texts.pth\")\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_path = os.path.join(config.input_directory, \"tokenizer/\")\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collator = Collator(return_targets=False, tokenizer=tokenizer, max_length=config.max_length)\n\ntest_dataset = Dataset(texts=test[\"text\"].values, \n                       pair_texts=test[\"target\"].values,\n                       contexts=test[\"title\"].values,\n                       max_length=config.max_length,\n                       sep=tokenizer.sep_token,\n                       tokenizer=tokenizer)\n    \ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=config.batch_size*2, \n                         num_workers=config.num_workers,\n                         pin_memory=config.pin_memory,\n                         collate_fn=collator,\n                         shuffle=False, \n                         drop_last=False)\n\nprint(f\"Test Samples: {len(test_dataset)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_predictions = []\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold [{fold}/{config.folds}]\")\n    \n    fold_directory = os.path.join(config.input_directory, f\"fold_{fold}/\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    checkpoint_path = os.path.join(checkpoints_directory, \"checkpoint.pth\")\n    \n    model = Model(config_path=model_config_path)\n    \n    fold_checkpoint = load_checkpoint(path=checkpoint_path, \n                                      model=model, \n                                      strict=True, \n                                      ignore_warnings=True)\n    \n    \n    print(f\"Loaded checkpoint from '{checkpoint_path}'.\")\n    \n    fold_predictions = prediction_loop(loader=test_loader, \n                                       model=model, \n                                       amp=config.amp, \n                                       device=config.device)\n    \n    oof_predictions.append(fold_predictions.numpy())\n    \n    del model, fold_checkpoint, fold_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*3)\n    \noof_predictions = np.array(oof_predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"large_predictions = np.mean(oof_predictions, axis=0)\n\nlarge_preds=pd.concat([pd.DataFrame(test_ids),pd.DataFrame(large_predictions)],keys=[\"ids\",\"score\"],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_max_scaler=MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_preds.score=pd.DataFrame(min_max_scaler.fit_transform(small_preds.score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"large_preds.score=pd.DataFrame(min_max_scaler.fit_transform(large_preds.score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = pd.concat(\n    [small_preds, large_preds],\n    keys=['deberta small', 'deberta large'],\n    axis=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mean = pd.DataFrame({\n    'deberta small': small_preds.mean(axis=1),\n    'deberta large': large_preds.mean(axis=1)\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_ = [0.25, 0.75]\nfinal_predictions = all_mean.mul(weights_).sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}