{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![here](https://miro.medium.com/max/1400/1*ziDWjvLlK9acS0SJYHB2Fw.png)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport torch\nimport gensim\nimport scipy\nfrom scipy import spatial\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-20T16:45:24.747194Z","iopub.execute_input":"2022-04-20T16:45:24.747693Z","iopub.status.idle":"2022-04-20T16:45:24.753367Z","shell.execute_reply.started":"2022-04-20T16:45:24.747644Z","shell.execute_reply":"2022-04-20T16:45:24.752588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#jaccard-similarity\">1.&nbsp;&nbsp;&nbsp;&nbsp;Jaccard Similarity</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#cosine-similarity\">2.&nbsp;&nbsp;&nbsp;&nbsp;Cosine Similarity</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#word-mover-distance\">3.&nbsp;&nbsp;&nbsp;&nbsp;Word Mover Distance</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#variational-autoencoder\">4.&nbsp;&nbsp;&nbsp;&nbsp;Variational AutoEncoder</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#pre-trained-sentence-encoder\">5.&nbsp;&nbsp;&nbsp;&nbsp;Pre-trained Sentence Encoder</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#siamese-manhattan-lstm\">6.&nbsp;&nbsp;&nbsp;&nbsp;Siamese Manhattan LSTM (MaLSTM)</a></h3>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"jaccard-similarity\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"jaccard-siilarity\">1.&nbsp;&nbsp;JACCARD SIMILARITY&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p>Jaccard similarity is length of intersection of two sets divided by length of union of two sets. This measurement refers to number of common words in both sets.More number of common words means both objects should be similarity.<p/><p>But we must apply some preprocessing before using jaccard and keep that sentence in form array containing tokens of that sentence</p>\n","metadata":{}},{"cell_type":"code","source":"def jaccard(array1, array2):\n    intersection = set(array1).intersection(set(array2))\n    union = set(array1).union(set(array2))\n    return len(intersection) / len(union)\n\ndef preprocess(text):\n    array = []\n    text = re.sub(\"[^a-zA-z]\", \" \",text)\n    text = text.lower()\n    text = word_tokenize(text)\n    for word in text:\n        if word not in stop_words:\n            array.append(word)\n    return array\n\n#example\nsent1 = \"AI is our friend and it has been friendly\"\nsent2 = \"AI and humans have always been friendly\"\n\nstop_words = set(stopwords.words(\"english\"))\narray1 = preprocess(sent1) \narray2 = preprocess(sent2)\njaccard(array1,array2)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T15:57:48.892715Z","iopub.execute_input":"2022-04-20T15:57:48.893034Z","iopub.status.idle":"2022-04-20T15:57:48.924445Z","shell.execute_reply.started":"2022-04-20T15:57:48.892992Z","shell.execute_reply":"2022-04-20T15:57:48.923679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let's apply jaccard similarity on our dataset</p>","metadata":{}},{"cell_type":"code","source":"def stemming(array):\n    new_array = []\n    for word in array:\n        new_array.append(stemmer.stem(word))\n    return new_array\n\ndef lemma(array):\n    new_array = []\n    for word in array:\n        new_array.append(lemmatizer.stem(word))\n    return new_array\nstemmer = PorterStemmer()\ndf= pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\n\ndf[\"pr_anchor\"] = df[\"anchor\"].apply(preprocess)\ndf[\"pr_target\"] = df[\"target\"].apply(preprocess)\n\ndf[\"pr_stem_anchor\"] = df[\"pr_anchor\"].apply(stemming)\ndf[\"pr_stem_target\"] = df[\"pr_target\"].apply(stemming)\n\n\ndf[\"jaccard_score\"] = df.apply(lambda x: jaccard(x[\"pr_stem_anchor\"], x[\"pr_stem_target\"]), 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T15:57:48.92586Z","iopub.execute_input":"2022-04-20T15:57:48.926087Z","iopub.status.idle":"2022-04-20T15:57:58.373862Z","shell.execute_reply.started":"2022-04-20T15:57:48.926061Z","shell.execute_reply":"2022-04-20T15:57:58.373034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>But there is some problem</p>\n1.what if two sentence has same meaning but don't have any common words.\n\n        For example :\n        sent1 = \"President greets the press in Chicago\"\n        sent2 = \"Obama speaks in Illinois\"\n        \n2.What if two sentence has common words but don't have any similarity\n\n        For example:\n        sent1 = \"Play the record\"\n        sent2 = \"Record the play\"","metadata":{}},{"cell_type":"code","source":"#Let's see jaccard score for case 1. \nsent1 = \"President greets the press in Chicago\"\nsent2 = \"Obama speaks in Illinois\"\n\narray1 = preprocess(sent1)\narray2 = preprocess(sent2)\nprint(\"jccard_score for case one = \",jaccard(array1, array2))\n\n#Let's see jaccard score for case 2. \nsent3 = \"Play the record\"\nsent4 = \"Record the play\"\n\narray3 = preprocess(sent3)\narray4 = preprocess(sent4)\nprint(\"jccard_score for case two = \",jaccard(array1, array2))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T15:57:58.375886Z","iopub.execute_input":"2022-04-20T15:57:58.376699Z","iopub.status.idle":"2022-04-20T15:57:58.38432Z","shell.execute_reply.started":"2022-04-20T15:57:58.376652Z","shell.execute_reply":"2022-04-20T15:57:58.383256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>see for our first case score is 0.0 . This is terrible distance even two sentences have similar meaning. and score for second case is this is terrible too even two sentence has no similar meaning. In our case one jaccard simlilarity niether able to capture lexial similarity nor semantic similarity. This is because jaccard similary work like <b>\"more common words more similarity no common words no similarity\"</b></p>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"cosine-similarity\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"cosine-similarity\">2.&nbsp;&nbsp;Cosine Similarity&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p>cosine similarity is the cosine angle between two vector.</p>","metadata":{}},{"cell_type":"markdown","source":"![here](https://miro.medium.com/max/426/1*5J8YlnfnZlzFobQC9cGk-w.png)","metadata":{}},{"cell_type":"markdown","source":"<p>Mathematically speaking cosine similarity is the measure of similarity between two non-zero vector in <b>inner-product space</b> which measure the cosine angle between them.The cosine of 0ยบ is 1 and it is less than 1 for any angle between (0,ฯ] radians.Two vectors with same orientation has similarity 1, two vector oriented at 90ยบ has similarity 0 and two vectors with 180ยบ has similarity of -1 independent of their magnitude</p>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"pretrained-embedding-cosine-similarity\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"pretrained-embedding-cosine-similarity\">2.1&nbsp;&nbsp;Pretrained Embedding (Glove) + Cosine Similarity&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"code","source":"glove_file_path = \"../input/glove-embeddings/glove.6B.300d.txt\"\nembeddings_index = {}\nwith open(glove_file_path, \"r\") as f:\n    for line in f:\n        values = line.rstrip().rsplit(\" \")\n        word = values[0]\n        coef = np.asarray(values[1:], dtype=\"float\")\n        embeddings_index[word] = coef","metadata":{"execution":{"iopub.status.busy":"2022-04-20T15:58:48.75683Z","iopub.execute_input":"2022-04-20T15:58:48.757122Z","iopub.status.idle":"2022-04-20T15:59:19.582783Z","shell.execute_reply.started":"2022-04-20T15:58:48.757078Z","shell.execute_reply":"2022-04-20T15:59:19.581731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_similarity(array1, array2):\n    try:\n        vector1 = np.mean([embeddings_index[word] for word in array1])\n        vector2 = np.mean([embeddings_index[word] for word in array2])\n    except KeyError:\n        return 0.0\n    \n    cosine = spatial.distance.cosine(vector1, vector2)\n    return 1 - cosine","metadata":{"execution":{"iopub.status.busy":"2022-04-20T15:59:24.993497Z","iopub.execute_input":"2022-04-20T15:59:24.994114Z","iopub.status.idle":"2022-04-20T15:59:25.000254Z","shell.execute_reply.started":"2022-04-20T15:59:24.994059Z","shell.execute_reply":"2022-04-20T15:59:24.99949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"pr_anchor\"] = df[\"anchor\"].apply(preprocess)\ndf[\"pr_target\"] = df[\"target\"].apply(preprocess)\ndf[\"glove_cosine\"] = df.apply(lambda x: cosine_similarity(x[\"pr_anchor\"], x[\"pr_target\"]), 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:00:11.444429Z","iopub.execute_input":"2022-04-20T16:00:11.445048Z","iopub.status.idle":"2022-04-20T16:00:20.908638Z","shell.execute_reply.started":"2022-04-20T16:00:11.445005Z","shell.execute_reply":"2022-04-20T16:00:20.907992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"smooth-inverse-frequency\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"smooth-inverse-frequency\">2.2&nbsp;&nbsp;Smooth Inverse Frequency&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p>Taking average of the word embeddings in sentence tends to give too much weight to the irrelevant word. Smooth Inverse frequency try to solve this problem by following way</p>\n<p><b>1.Weighted Average</b></p>\n<p>This take weighted average of every word embedding by a/(a + p(w)) where a is ususally 1e-3 and p(w) is the freaquency of that word in that sentence.</p>\n<p><b>2.Commomn Component removal</b></p>\n<p>This computes principal component of resulting set of senetence embeddings. And subtract it from those sentence embedding</p>\n<p> SIF downgrades unimportant words such as \"just\",\"but\" etc. and keep the information which contributes most to the semantic of the sentence. But we are not going to apply smooth inverse frequency because in our data we dont have big sentences.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"word-mover-distance\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"word-mover-distance\">3.&nbsp;&nbsp;Word Mover's Distance&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p><h2>Earth Mover Distance</h2></p>\n<p>&nbsp;</p>\n<p>The core part of WMD is earth mover distance (EMD).So fisrt i want to share idea of EMD.</p>\n<p>EMD solve transportation problem. For instance we have m and n while m and n derives set of suppliers and warehouse. The target is going to minimize transportation cost such that shipping all goods from m to n.Given that there are contrains:</p>\n<p>1.only allow transportation from m to n. Not allowing transporattion from n to m.</p>\n<p>2.Total number of sending cargoes doesn't exceed total capacity of m.</p>\n<p>3.Total number of recieving cargoes doesn't exceed total capacity of n.</p>\n<p>4.Total number of transportation is the minimum between total cargoes in m and total cargoes in n.</p>\n<p>&nbsp;</p>\n<p><h4>The denotations are :</h4></p>\n<p>* p: Set of origin</p>\n<p>* q: Set of destination</p>\n<p>* f(i, j): flow from i to j</p>\n<p>* m: number of origin</p>\n<p>* n: number of destination</p>\n<p>* w(i, j): Number of cargoes transport from i to j</p>\n<p>&nbsp;</p>\n<img src=\"https://miro.medium.com/max/558/1*T1ExbOo5LooayRrIcpPrbA.png\" />\n<p>&nbsp;</p>\n<p><h2>Word Mover's Distance</h2></p>\n<p>&nbsp;</p>\n<p>WMD is designed to overcame synonym problem</p>\n<p>The typical example is</p>\n<p>sent1: \"Obama speaks to the media in Illinois\"</p>\n<p>sent2: \"The president greets the press in Chicago\"</p>\n<p>Except the stopwords there is no common words among two sentence but both of them taking about same topic(at the time).</p>\n<img src=\"https://miro.medium.com/max/890/1*nTWAm46JMYWXpHVsS9MA5w.png\" />\n<p>&nbsp;</p>\n<p>Retrive vectors from any pretrained embedding that can be glove,fasttext, word2vec or custom embedding. After that it is using nBOW to represent weight or importance. It allows words to transfer from sentence1 to sentence2 because alogorithm doesn't know \"obama\" would transfer to president. At the end it will choose the minimum transportation cost to transfer each word from sent1 to sent2. </p>","metadata":{}},{"cell_type":"code","source":"#Let's see how WMD works on our cases\nfasttext_file_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nfasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_file_path)\nscore1 = fasttext_model.wmdistance(array1,array2)\nprint(score1)\nscore2 = fasttext_model.wmdistance(array3,array4)\nprint(score2)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:02:02.341605Z","iopub.execute_input":"2022-04-20T16:02:02.342397Z","iopub.status.idle":"2022-04-20T16:07:46.63289Z","shell.execute_reply.started":"2022-04-20T16:02:02.342363Z","shell.execute_reply":"2022-04-20T16:07:46.632029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's apply WMD on our dataset\ndef wmd(array1,array2):\n    score = fasttext_model.wmdistance(array1,array2)\n    return score\n\ndf[\"WMD\"] = df.apply(lambda x: wmd(x[\"pr_anchor\"],x[\"pr_target\"]), 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:18:07.364646Z","iopub.execute_input":"2022-04-20T16:18:07.365183Z","iopub.status.idle":"2022-04-20T16:18:25.489941Z","shell.execute_reply.started":"2022-04-20T16:18:07.365144Z","shell.execute_reply":"2022-04-20T16:18:25.489046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"variational-autoencoder\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"variational-autoencoder\">4.&nbsp;&nbsp;Variational AutoEncoder&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p><b>What are Autoencoders?</b></p>\n<p>Autoencoders are a special type of neural network architecture in which output is same as input. Autoencoders are trained in an unsupervised manner in order to learn the extremly low level representations of the input data. These low level features are then deformed by decoder to project actual data. An autoencoder is a regression task where network is asked to predict it's input. These network has tight bottleneck of few neuron in middle forcing them to create effective represention of that compress input that can use by decoder to reproduce orignal input.</p>\n<p>&nbsp;</p>\n<img src=\"https://miro.medium.com/proxy/1*HIBRgSV2ePFtCSOF_lgPCQ.png\">\n<p>&nbsp;</p>\n<p>An autoencoder comprise of three main component:</p>\n<p><b>โข Encoder Architecture :</b> The encoder architecture is series of layers with decreseing number of nodes and ultimately reduce to latent view representation</p>\n<p><b>โข Latent View Representation:</b> The Latent view represent lowest level space in which input is reduced and information is preserved</p>\n<p><b>โข Decoder Architecture :</b> The decoder architecture is series of layers with increasing number of nodes and ultimately outputs similer(almost) input.</p>\nFor more in depth infomation you can read this <a href=\"https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases/notebook\" target=\"_top\">article</a>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"pre-trained-sentence-encoder\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"pre-trained-sentence-encoder\">5.&nbsp;&nbsp;Pre-trained Sentence Encoders&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p>The universal sentence encoders encode text into high dimensional vectors that cn be used for text classification, semantic similarity, clustering and other natural language task.</p>\n<p>Pretrained sentence encoders aim to play same role as word2vec and Glove but for senetence embeddings: The embeddings they produce can be used in variety of application, such as text classification, paraphrase detection, etc. Typically they have trained on a range of supervised and unsupervised task, in order to capture as much universal semantic information as possible.</p>\n<p>&nbsp;</p>\n<img src=\"https://miro.medium.com/max/700/0*sYubQH60abTnSnAq.png\">\n<p>&nbsp;</p>\n<p>Several such encoders are available :</p>\n<p><b>โข Infersent(Facebook Research)</b>: BiLSTM with max pooling, trained on SNLI dataset, 570k, English sentence pairs labelled with one of three categories: entailment, contradiction or neutral.</p>\n<p><b>โข Google Sentence Encoder</b>: a simpler Deep Averaging Network where input embeddings for words and bigrams are avarged together and passed through a feed-forward deep neural network.</p>","metadata":{}},{"cell_type":"code","source":"cd ../input/infersent-models","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:20:57.511171Z","iopub.execute_input":"2022-04-20T16:20:57.511683Z","iopub.status.idle":"2022-04-20T16:20:57.518647Z","shell.execute_reply.started":"2022-04-20T16:20:57.511646Z","shell.execute_reply":"2022-04-20T16:20:57.51789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from models import InferSent\n\ndef cosine_similarity(text1,text2):\n    try :\n        array1 = model.encode([text1])[0]\n        array2 = model.encode([text2])[0]\n        cosine = scipy.spatial.distance.cosine(array1,array2)\n    except KeyError:\n        return 0.50\n    return 1 - cosine\n\n\nmodel_version = 1\nmodel_path = f\"../infersent1-2/infersent{model_version}.pkl\"\nparams_model = {\n    \"bsize\":64,\n    \"word_emb_dim\":300,\n    \"enc_lstm_dim\":2048,\n    \"pool_type\":\"max\",\n    \"dpout_model\":0.0,\n    \"version\":model_version\n}\n\nmodel = InferSent(params_model)\nmodel.load_state_dict(torch.load(model_path))\n\nuse_cuda = False\nmodel = model.cuda() if use_cuda else model\n\nglove_path = \"../glove-embeddings/glove.6B.300d.txt\"\nmodel.set_w2v_path(glove_path)\nmodel.build_vocab_k_words(K=100000)\n\ndf[\"infersent_cosine\"] = df.apply(lambda x: cosine_similarity(x[\"anchor\"],x[\"target\"]), 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:45:47.830995Z","iopub.execute_input":"2022-04-20T16:45:47.831473Z","iopub.status.idle":"2022-04-20T16:46:08.338624Z","shell.execute_reply.started":"2022-04-20T16:45:47.831424Z","shell.execute_reply":"2022-04-20T16:46:08.337151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"siamese-manhattan-lstm\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"siamese-manhattan-lstm\">6.&nbsp;&nbsp;Siamese Manhattan LSTM (MaLSTM)&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<p>Siamese networks are networks that have <b>two or more identical sub-networks in them</b>.Siamese network seems to <b>perform well on similarity tasks</b> and have been used for task like sentence semantic similarity , recoonizing forged signatures and many more.</p>\n<p>The name MaLSTM  and SiameseLSTM might leave and impression that there are some kind of new LSTM unit proposed, but that not the case.</p>\n<p><b>Siamese</b> is the name of the general model architecture where the model consist of two identical subnetworks that compute some kine of representation vectors for two inputs and a distance measure is used to compute a score to estimate the similarity or difference of the inputs. In the attached figure the LSTMa and LSTMb share parameters(weights) and have identical structure. The idea itself is not new and goes back to 1994.</p>\n<p><b>MaLSTM</b> (ManhattanLSTM) just refers to the fact that they chose to use Manhattan distance to compare the final hidden state of two standard LSTM layers. Alternatives like cosine and Euclidean distance can also be used but the auther states that \"Manhattan distance slighty outperforms other reasonable alternatives such as cosine similarity\".</p>\n<p>&nbsp;</p>\n<img src=\"https://miro.medium.com/max/602/0*_km0_7zWh8crsoc-\">\n<p>&nbsp;</p>\n<p>&nbsp;</p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}