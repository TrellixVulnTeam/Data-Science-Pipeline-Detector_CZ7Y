{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pkgutil\ncheck_module = True if pkgutil.find_loader(\"hydra\") else False\nif not check_module:\n    import subprocess\n    subprocess.run('pip uninstall -y transformers'.split())\n    subprocess.run('python -m pip install --no-index --find-links=../input/uspppm-pip-wheels transformers'.split())\n    subprocess.run('python -m pip install --no-index --find-links=../input/uspppm-pip-wheels datasets'.split())\n    subprocess.run('python -m pip install --no-index --find-links=../input/uspppm-pip-wheels sentencepiece'.split())\n    subprocess.run('python -m pip install --no-index --find-links=../input/uspppm-pip-wheels hydra-core'.split())\n    subprocess.run('python -m pip install --no-index --find-links=../input/uspppm-pip-wheels slackclient'.split())\nelse:\n    print(\"Environment is already setup\")\n\ndel check_module","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T09:43:09.662212Z","iopub.execute_input":"2022-06-20T09:43:09.662504Z","iopub.status.idle":"2022-06-20T09:43:09.678882Z","shell.execute_reply.started":"2022-06-20T09:43:09.66243Z","shell.execute_reply":"2022-06-20T09:43:09.67813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"import os\n\nDEBUG = False\nBATCH_SIZE = 64\nN_FOLDS = 5\n\n##############\n# ROOT PATHS\n##############\nDATA_DIR = \"/kaggle/input/us-patent-phrase-to-phrase-matching/\"\nINPUT_DIR = \"/kaggle/input/uspppm-data\"\nCODE_DIR = \"/kaggle/input/uspppm-source-code\"","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:43:09.680232Z","iopub.execute_input":"2022-06-20T09:43:09.680596Z","iopub.status.idle":"2022-06-20T09:43:09.687754Z","shell.execute_reply.started":"2022-06-20T09:43:09.680463Z","shell.execute_reply":"2022-06-20T09:43:09.686898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport gc\ngc.enable()\nimport sys\nimport importlib\nimport logging\nimport warnings\nfrom pprint import pprint\nfrom typing import Dict\nfrom omegaconf import OmegaConf\n\nsys.path.append(\"../input/uspppm-source-code\")\nsys.path.append(\"../input/uspppm-source-code/src\")\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pprint import pprint\nimport transformers\nimport datasets\nimport tokenizers\nfrom datasets import Dataset\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n)\n\n# from source-code\nfrom data.cpc_texts import get_cpc_texts\nfrom data.dataset import tokenize_func, prepare_data, tokenize_func_transformer_head, create_folds\nfrom extensions.scoring import pearsonr, post_process_predictions, scale_predictions\nfrom utils import seed_everything\n\nfrom src.cocolm.configuration_cocolm import COCOLMConfig\nfrom src.cocolm.tokenization_cocolm import COCOLMTokenizer\nfrom src.modeling.models.cocolm.cocolm import COCOLMForSequenceClassification\n\n# setup\nseed_everything(42)\n\n# log some stuff\nprint(f\"-> transformers version: {transformers.__version__}\")\nprint(f\"-> datasets version: {datasets.__version__}\")\nprint(f\"-> tokenizers version: {tokenizers.__version__}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T09:43:09.899222Z","iopub.execute_input":"2022-06-20T09:43:09.899783Z","iopub.status.idle":"2022-06-20T09:43:18.303419Z","shell.execute_reply.started":"2022-06-20T09:43:09.899742Z","shell.execute_reply":"2022-06-20T09:43:18.302581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read OOF files","metadata":{}},{"cell_type":"code","source":"import glob\ncheckpoint_root_dirs = [f\"../input/{root_dir}\" for root_dir in os.listdir(\"../input\") if \"checkpoints\" in root_dir]\n\n# refactor, inefficient and hacky code!\nCKPTS = []\nfor ckpt_dir in checkpoint_root_dirs:\n    for dire in os.listdir(ckpt_dir):\n        if \".\" not in dire:\n             CKPTS.append(f\"{ckpt_dir}/{dire}\")\nCKPTS = np.sort(CKPTS)\n# CKPTS = [ck for ck in CKPTS if \"coco\" not in ck]\nprint(f\"We have {len(CKPTS)} experiments for ensemble\\n\")\npprint([ckpt.split('/')[-1] for ckpt in CKPTS])\n\n# reading oof files\nOOF = {f\"{f.split('/')[-1]}\": f\"{f}/{f.split('/')[-1]}_oof.csv\" for f in CKPTS}\nOOF_CSV = [pd.read_csv(v) for _, v in OOF.items()]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:43:18.305537Z","iopub.execute_input":"2022-06-20T09:43:18.306268Z","iopub.status.idle":"2022-06-20T09:43:21.807441Z","shell.execute_reply.started":"2022-06-20T09:43:18.306226Z","shell.execute_reply":"2022-06-20T09:43:21.806558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.zeros((len(OOF_CSV[0]),len(OOF)))\nfor k in range(len(OOF)):\n    x[:,k] = scale_predictions(OOF_CSV[k]['preds'].to_numpy())\n    \nTRUE = OOF_CSV[0]['score'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:43:21.808834Z","iopub.execute_input":"2022-06-20T09:43:21.809084Z","iopub.status.idle":"2022-06-20T09:43:21.914713Z","shell.execute_reply.started":"2022-06-20T09:43:21.809051Z","shell.execute_reply":"2022-06-20T09:43:21.913779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_scores = []\nfor k in range(x.shape[1]):\n    pscore = pearsonr(OOF_CSV[0]['score'].to_numpy(),x[:,k])\n    all_scores.append(pscore)\n    print('Model %i has OOF Pearsonr = %.4f'%(k,pscore))\n    \nm = [np.argmax(all_scores)]; w = []","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:43:21.917059Z","iopub.execute_input":"2022-06-20T09:43:21.917426Z","iopub.status.idle":"2022-06-20T09:43:22.025015Z","shell.execute_reply.started":"2022-06-20T09:43:21.917359Z","shell.execute_reply":"2022-06-20T09:43:22.024103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hill Climbing Ensemble on OOF","metadata":{}},{"cell_type":"code","source":"old = np.max(all_scores); \n\nRES = 200\nPATIENCE = 10\nTOL = 0.0003\nDUPLICATES = False\n\nprint('Ensemble Pearsonr = %.5f by beginning with model %i'%(old,m[0]))\nprint()\n\nfor kk in range(len(OOF)):\n    \n    # BUILD CURRENT ENSEMBLE\n    md = x[:,m[0]]\n    for i,k in enumerate(m[1:]):\n        md = w[i]*x[:,k] + (1-w[i])*md\n        \n    # FIND MODEL TO ADD\n    mx = 0; mx_k = 0; mx_w = 0\n    print('Searching for best model to add... ')\n    \n    # TRY ADDING EACH MODEL\n    for k in range(x.shape[1]):\n        print(k,', ',end='')\n        if not DUPLICATES and (k in m): continue\n            \n        # EVALUATE ADDING MODEL K WITH WEIGHTS W\n        bst_j = 0; bst = 0; ct = 0\n        for j in range(RES):\n            tmp = j/RES*x[:,k] + (1-j/RES)*md\n            pear_score = pearsonr(TRUE,tmp)\n            if pear_score>bst:\n                bst = pear_score\n                bst_j = j/RES\n            else: ct += 1\n            if ct>PATIENCE: break\n        if bst>mx:\n            mx = bst\n            mx_k = k\n            mx_w = bst_j\n            \n    # STOP IF INCREASE IS LESS THAN TOL\n    inc = mx-old\n    if inc<=TOL: \n        print(); print('No increase. Stopping.')\n        break\n        \n    # DISPLAY RESULTS\n    print();\n    print('Ensemble Pearsonr = %.4f after adding model %i with weight %.3f. Increase of %.4f'%(mx,mx_k,mx_w,inc))\n    print()\n    \n    old = mx; m.append(mx_k); w.append(mx_w)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T09:43:22.026331Z","iopub.execute_input":"2022-06-20T09:43:22.026635Z","iopub.status.idle":"2022-06-20T09:43:38.724353Z","shell.execute_reply.started":"2022-06-20T09:43:22.026604Z","shell.execute_reply":"2022-06-20T09:43:38.72361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'We are using models (total {len(m)})',m)\nprint('with weights',w)\nprint(f\"with tolerance of {TOL}\")\nprint('and achieve ensemble pearsonr = %.5f'%old)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:43:38.725932Z","iopub.execute_input":"2022-06-20T09:43:38.726188Z","iopub.status.idle":"2022-06-20T09:43:38.734987Z","shell.execute_reply.started":"2022-06-20T09:43:38.726152Z","shell.execute_reply":"2022-06-20T09:43:38.73424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_weights = dict()\nfor i, s in enumerate(m):\n    if i == 0:\n        m_weights.update({s: None})\n    else:\n        m_weights.update({s:w[i-1]})\n\nprint(\"dictionary storing the index and weight of each oof/experiment\")\nm_weights # has all model's indices along with their weights","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:47:49.810143Z","iopub.execute_input":"2022-06-20T09:47:49.810859Z","iopub.status.idle":"2022-06-20T09:47:49.826497Z","shell.execute_reply.started":"2022-06-20T09:47:49.810804Z","shell.execute_reply":"2022-06-20T09:47:49.825029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"md = x[:,m[0]]  # highest cv experiment preds\nfor mod, weight in m_weights.items():\n    if weight is not None:\n        md = weight * x[:, mod] + (1-weight) * md\nprint(f\"Ensemble CV Score: {np.round(pearsonr(md, TRUE), 5)}\\n\\n\")\nplt.hist(md,bins=100)\nplt.title('Ensemble OOF predictions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:47:50.364364Z","iopub.execute_input":"2022-06-20T09:47:50.364953Z","iopub.status.idle":"2022-06-20T09:47:50.734952Z","shell.execute_reply.started":"2022-06-20T09:47:50.36491Z","shell.execute_reply":"2022-06-20T09:47:50.734244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INFER_EXPS = {}\nfor i, (k,v) in enumerate(OOF.items()):\n    if i in m:\n        INFER_EXPS.update({k:i})\n\nINFER_EXPS_NAMES = [k for k, v in INFER_EXPS.items()]\n\nprint(f\"Inferencing selected experiments to generate submission file for ensemble (total {len(INFER_EXPS_NAMES)}):\\n\")\npprint(INFER_EXPS)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:47:50.736692Z","iopub.execute_input":"2022-06-20T09:47:50.737171Z","iopub.status.idle":"2022-06-20T09:47:50.745624Z","shell.execute_reply.started":"2022-06-20T09:47:50.737131Z","shell.execute_reply":"2022-06-20T09:47:50.744908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"# read data\nsubmission = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n\nif DEBUG:\n    print(\"Running in DEBUG mode\")\n    test_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))[:10000]\n\ndisplay(test_df.head())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T09:47:51.031212Z","iopub.execute_input":"2022-06-20T09:47:51.031955Z","iopub.status.idle":"2022-06-20T09:47:51.05912Z","shell.execute_reply.started":"2022-06-20T09:47:51.031913Z","shell.execute_reply":"2022-06-20T09:47:51.058288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper funcs","metadata":{}},{"cell_type":"code","source":"# function to return key for any value\ndef get_key(dicti, val):\n    for key, value in dicti.items():\n         if val == value:\n            return key\n \n    return \"key doesn't exist\"\n\ndef infer_one_experiment(\n    checkpoint_dir: str,\n    test_df: pd.DataFrame,\n    batch_size: int = 64,\n    n_folds: int = 5,\n):\n    predictions = []\n    for fold in range(n_folds):\n        ckpt_dir = os.path.join(checkpoint_dir, f\"fold_{fold}\")\n        cfg = OmegaConf.load(os.path.join(ckpt_dir, \"experiment_config.yaml\"))\n\n\n        # tokenizer\n        if \"cocolm\" in cfg.model.model_name:\n            tokenizer = COCOLMTokenizer.from_pretrained(ckpt_dir)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(ckpt_dir)\n        \n        # determine whether to train transformer head type model or not\n        # if True then tokenization is max_length and dynamic padding is disabled\n        if \"TransformerHead\" in cfg.model.class_name:\n            transformer_head_trainer = True\n        else:\n            transformer_head_trainer = False\n        \n        test_df_prep = prepare_data(df=test_df,\n                              tokenizer=tokenizer,\n                              cpc_scheme_xml_dir=os.path.join(INPUT_DIR, \"CPCSchemeXML202105\"),\n                              cpc_title_list_dir=os.path.join(INPUT_DIR, \"CPCTitleList202202\"),\n                              use_custom_seperator=cfg.data.use_custom_seperator)\n        test_ds = Dataset.from_pandas(test_df_prep)\n\n        if transformer_head_trainer:\n            print(\">>> Transformer head trainer\")\n            # max_length padding\n            tokenized_ds = test_ds.map(\n                lambda x: tokenize_func_transformer_head(\n                    x, tokenizer=tokenizer, max_length=133\n                ),\n                batched=True,\n            )\n        else:\n            tokenized_ds = test_ds.map(\n                lambda x: tokenize_func(\n                    x,\n                    tokenizer=tokenizer,\n                ),\n                batched=True,\n            )\n        \n        if fold == 0:\n            print(\"\\nSample tokenized text:\")\n            print(tokenizer.decode(tokenized_ds[0][\"input_ids\"]) + \"\\n\")\n\n        print(f\"-> Loading checkpoint from {ckpt_dir}\")\n\n        # init model\n        if \"cocolm\" in cfg.model.model_name:\n            config = COCOLMConfig.from_pretrained(ckpt_dir)\n        else:\n            config = AutoConfig.from_pretrained(ckpt_dir, num_labels=1)        \n        # gets the appropiate class of the model defined in `cfg.model.class_name` in hydra configuration\n        # from `src.modeling.models` package\n        if \"AutoModel\" in cfg.model.class_name:\n            print(f\"-> Loading AutoModelForSequenceClassification class\")\n            model = AutoModelForSequenceClassification.from_pretrained(\n                ckpt_dir, config=config\n            )\n\n        # \"\"\"\n        # Model class name should contain `*General`\n        # This class supports \n        # - different losses (mse, bce, pearson) with appropiate post-process\n        # - multi_sample dropout\n        # - attention_pool\n        # \"\"\"\n\n        elif \"General\" in cfg.model.class_name:\n            if \"cocolm\" in ckpt_dir:\n                ckpt_dir = f\"{ckpt_dir}/pytorch_model.bin\"\n            print(\"-> Loading general class\")\n            MODEL_CLASS = getattr(importlib.import_module(\"modeling\"), cfg.model.class_name)\n            model = MODEL_CLASS.from_pretrained(\n                ckpt_dir,\n                config=config,\n                loss_type=cfg.model.loss_type,\n                multi_sample_dropout=cfg.model.multi_sample_dropout,\n                attention_pool=cfg.model.attention_pool,\n            )\n        else:\n            if \"cocolm\" in ckpt_dir:\n                ckpt_dir = f\"{ckpt_dir}/pytorch_model.bin\"\n            print(\"-> Loading class with loss_type key\")\n            MODEL_CLASS = getattr(importlib.import_module(\"modeling\"), cfg.model.class_name)\n            model = MODEL_CLASS.from_pretrained(\n                ckpt_dir,\n                config=config,\n                loss_type=cfg.model.loss_type\n            )\n\n        trainer_args = TrainingArguments(\n            output_dir=\"/kaggle/working/trainer_out\",\n            per_device_eval_batch_size=batch_size,\n        )\n        # init Trainer\n        trainer = Trainer(\n            model=model,\n            args=trainer_args,\n            # dynamic padding within batch\n            data_collator=None if transformer_head_trainer else DataCollatorWithPadding(tokenizer),\n            tokenizer=tokenizer,\n        )\n\n        # infer on test dataset\n        logits, _, _ = trainer.predict(tokenized_ds)\n        predictions.append(scale_predictions(post_process_predictions(logits.reshape(-1), cfg.model.loss_type)))\n        \n        # clean-up\n        del trainer, model, trainer_args, config, logits, tokenized_ds, cfg, tokenizer, test_ds\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    \n    exp_sub_df = pd.DataFrame()\n    exp_sub_df['id'] = submission['id'].to_numpy()\n    exp_sub_df[\"score\"] = np.mean(predictions, axis=0)\n    save_name = checkpoint_dir.split(\"/\")[-1]\n    exp_sub_df.to_csv(f\"{save_name}_sub.csv\", index=False)\n    \n    del exp_sub_df, save_name\n    gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T09:47:51.458485Z","iopub.execute_input":"2022-06-20T09:47:51.458998Z","iopub.status.idle":"2022-06-20T09:47:51.481934Z","shell.execute_reply.started":"2022-06-20T09:47:51.458961Z","shell.execute_reply":"2022-06-20T09:47:51.481096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# DISABLE PROGRESS BARS AND WARNINGS\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\ndatasets.disable_progress_bar()\n\nprint(\"Inferencing following experiments:\\n\")\npprint(INFER_EXPS_NAMES)\nprint(\"\\n\\n\")\n\n# INFER ALL EXPERIMENTS SELECTED TO MAXIMIZE CV SCORE\nfor exp, oof_path in OOF.items():\n    if exp in INFER_EXPS_NAMES:\n        root_dir = oof_path.split('/')[2]\n        \n        print(\"*\" * 10 + f\" Experiment {exp} \" + \"*\" * 10)\n        infer_one_experiment(\n            checkpoint_dir=os.path.join(\"../input\", root_dir, exp),\n            test_df=test_df,\n            batch_size=BATCH_SIZE,\n            n_folds=N_FOLDS,\n        )\n\n        print(\"=\" * 100)\n        print()\n        gc.collect()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T09:47:52.466625Z","iopub.execute_input":"2022-06-20T09:47:52.46722Z","iopub.status.idle":"2022-06-20T09:48:00.099832Z","shell.execute_reply.started":"2022-06-20T09:47:52.467181Z","shell.execute_reply":"2022-06-20T09:48:00.09812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"INFER_EXPS_REV = {v:k for k,v in INFER_EXPS.items()}\nINFER_EXPS_REV","metadata":{"execution":{"iopub.status.busy":"2022-06-20T09:48:00.100741Z","iopub.status.idle":"2022-06-20T09:48:00.101029Z","shell.execute_reply.started":"2022-06-20T09:48:00.10088Z","shell.execute_reply":"2022-06-20T09:48:00.100897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_cv_exp_name = INFER_EXPS_REV[get_key(m_weights, None)]\nhigh_cv_preds = scale_predictions(pd.read_csv(OOF[high_cv_exp_name])['preds'].to_numpy())\nhigh_inf_preds = scale_predictions(pd.read_csv(f\"/kaggle/working/{high_cv_exp_name}_sub.csv\")['score'].to_numpy())\nfor mod_idx, weight in m_weights.items():\n    if weight is not None:\n        exp_name = INFER_EXPS_REV[mod_idx]\n        oof_preds = scale_predictions(pd.read_csv(OOF[exp_name])['preds'].to_numpy())\n        inf_preds = scale_predictions(pd.read_csv(f\"/kaggle/working/{exp_name}_sub.csv\")['score'].to_numpy())\n        high_cv_preds = weight * oof_preds + (1-weight) * high_cv_preds\n        high_inf_preds = weight * inf_preds + (1-weight) * high_inf_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_cv_preds.shape, high_inf_preds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SANITY CHECK CV (if all weights were being assigned properly)\nnp.round(pearsonr(high_cv_preds, TRUE), 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(high_cv_preds,bins=100)\nplt.title('Ensemble OOF predictions')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(high_inf_preds)\nplt.title('Ensemble predictions')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DEBUG:\n    # create submission\n    submission[\"score\"] = high_inf_preds\n    submission.to_csv(\"submission.csv\", index=False)\n    print(submission.dtypes)\n    display(submission.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/trainer_out\", ignore_errors=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}