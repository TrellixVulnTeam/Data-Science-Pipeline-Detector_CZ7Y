{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this notebook we attempt a solution as a baseline \n\n### We will be using BERT as the model and computing the cosine similarity\n### between the anchor and text without considering the context","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy.linalg import norm\nfrom scipy.stats import pearsonr\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-25T19:50:53.747878Z","iopub.execute_input":"2022-05-25T19:50:53.748315Z","iopub.status.idle":"2022-05-25T19:50:53.760557Z","shell.execute_reply.started":"2022-05-25T19:50:53.748276Z","shell.execute_reply":"2022-05-25T19:50:53.75927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:50:54.475776Z","iopub.execute_input":"2022-05-25T19:50:54.477577Z","iopub.status.idle":"2022-05-25T19:50:55.228433Z","shell.execute_reply.started":"2022-05-25T19:50:54.47752Z","shell.execute_reply":"2022-05-25T19:50:55.227345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:50:56.847899Z","iopub.execute_input":"2022-05-25T19:50:56.848531Z","iopub.status.idle":"2022-05-25T19:51:13.462758Z","shell.execute_reply.started":"2022-05-25T19:50:56.848495Z","shell.execute_reply":"2022-05-25T19:51:13.461626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wnl = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.466776Z","iopub.execute_input":"2022-05-25T19:51:13.467032Z","iopub.status.idle":"2022-05-25T19:51:13.479022Z","shell.execute_reply.started":"2022-05-25T19:51:13.467001Z","shell.execute_reply":"2022-05-25T19:51:13.477819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv\").drop(columns=[\"id\"])\ntest_df = pd.read_csv(\"/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv\").drop(columns=[\"id\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.480826Z","iopub.execute_input":"2022-05-25T19:51:13.481949Z","iopub.status.idle":"2022-05-25T19:51:13.59357Z","shell.execute_reply.started":"2022-05-25T19:51:13.481902Z","shell.execute_reply":"2022-05-25T19:51:13.592602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.595961Z","iopub.execute_input":"2022-05-25T19:51:13.596244Z","iopub.status.idle":"2022-05-25T19:51:13.617832Z","shell.execute_reply.started":"2022-05-25T19:51:13.596204Z","shell.execute_reply":"2022-05-25T19:51:13.616883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.619424Z","iopub.execute_input":"2022-05-25T19:51:13.619909Z","iopub.status.idle":"2022-05-25T19:51:13.633018Z","shell.execute_reply.started":"2022-05-25T19:51:13.619864Z","shell.execute_reply":"2022-05-25T19:51:13.631722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"anchor\"].nunique(), train_df[\"context\"].nunique(), train_df[\"target\"].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.636888Z","iopub.execute_input":"2022-05-25T19:51:13.637521Z","iopub.status.idle":"2022-05-25T19:51:13.670357Z","shell.execute_reply.started":"2022-05-25T19:51:13.637483Z","shell.execute_reply":"2022-05-25T19:51:13.669351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions:","metadata":{}},{"cell_type":"code","source":"def clean_text(corpus, remove_stop_words = True):\n    '''\n    Function to clean a given corpus - lower the words, strip of the spaces, remove stopwords and lemmatize the corpus\n    Args:\n        corpus: the text to be cleaned\n        remove_stop_words: whether to remove stopwords\n    Returns:\n        filtered_sentence: cleaned corpus\n    '''\n    corpus = corpus.lower().strip()\n    word_tokens = word_tokenize(corpus)\n    if remove_stop_words:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus)) if i not in stop_words])\n    else:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus))])\n    return filtered_sentence\n\ndef cosine(a,b):\n    '''\n    Function to calculate cosine similarity of two vectors\n    Args:\n        a,b: vectors to calculate cosine between\n    Returns:\n        cosine similarity of the given vectors\n    '''\n    return np.dot(a,b)/(norm(a)*norm(b))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.671945Z","iopub.execute_input":"2022-05-25T19:51:13.672753Z","iopub.status.idle":"2022-05-25T19:51:13.684783Z","shell.execute_reply.started":"2022-05-25T19:51:13.672708Z","shell.execute_reply":"2022-05-25T19:51:13.683836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"anchor\"] = train_df[\"anchor\"].apply(lambda x: clean_text(x,False))\ntrain_df[\"target\"] = train_df[\"target\"].apply(lambda x: clean_text(x,False))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:51:13.688554Z","iopub.execute_input":"2022-05-25T19:51:13.688831Z","iopub.status.idle":"2022-05-25T19:52:00.97195Z","shell.execute_reply.started":"2022-05-25T19:51:13.688789Z","shell.execute_reply":"2022-05-25T19:52:00.970933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n\nmodelPath = \"/kaggle/working/bert-base\"\n\nmodel.save(modelPath)\n# model = SentenceTransformer(modelPath)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:52:00.975924Z","iopub.execute_input":"2022-05-25T19:52:00.976912Z","iopub.status.idle":"2022-05-25T19:52:38.441131Z","shell.execute_reply.started":"2022-05-25T19:52:00.97685Z","shell.execute_reply":"2022-05-25T19:52:38.440136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anchors = train_df[\"anchor\"].to_list()\nanchor_embed = model.encode(anchors,show_progress_bar=True, batch_size=128)##explore normalize embeddings param","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:52:38.443088Z","iopub.execute_input":"2022-05-25T19:52:38.44339Z","iopub.status.idle":"2022-05-25T19:52:55.356546Z","shell.execute_reply.started":"2022-05-25T19:52:38.443349Z","shell.execute_reply":"2022-05-25T19:52:55.355468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train_df[\"target\"].to_list()\ntarget_embed = model.encode(targets,show_progress_bar=True, batch_size=128)##explore normalize embeddings param","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:52:55.358347Z","iopub.execute_input":"2022-05-25T19:52:55.358961Z","iopub.status.idle":"2022-05-25T19:53:06.672446Z","shell.execute_reply.started":"2022-05-25T19:52:55.358913Z","shell.execute_reply":"2022-05-25T19:53:06.671452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sims = [cosine(i[0],i[1]) for i in zip(anchor_embed,target_embed)]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:53:40.885627Z","iopub.execute_input":"2022-05-25T19:53:40.885945Z","iopub.status.idle":"2022-05-25T19:53:41.654794Z","shell.execute_reply.started":"2022-05-25T19:53:40.885911Z","shell.execute_reply":"2022-05-25T19:53:41.653805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_val = max(sims)\nmin_val = min(sims)\nsim_norm = (sims-min_val)/(max_val-min_val)\nsim_norm = np.floor(sim_norm*4)/4","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:53:43.406364Z","iopub.execute_input":"2022-05-25T19:53:43.407201Z","iopub.status.idle":"2022-05-25T19:53:43.418883Z","shell.execute_reply.started":"2022-05-25T19:53:43.407143Z","shell.execute_reply":"2022-05-25T19:53:43.417737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.array(train_df[\"score\"].to_list())","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:53:47.266072Z","iopub.execute_input":"2022-05-25T19:53:47.266868Z","iopub.status.idle":"2022-05-25T19:53:47.2769Z","shell.execute_reply.started":"2022-05-25T19:53:47.266832Z","shell.execute_reply":"2022-05-25T19:53:47.275671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr,_ = pearsonr(y,sim_norm)\nprint(corr)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:54:09.146925Z","iopub.execute_input":"2022-05-25T19:54:09.147242Z","iopub.status.idle":"2022-05-25T19:54:09.157145Z","shell.execute_reply.started":"2022-05-25T19:54:09.14721Z","shell.execute_reply":"2022-05-25T19:54:09.155813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Although the obtained correlation isn't that good, but its good enough for a baseline solution \n## without context and build up from this point","metadata":{}},{"cell_type":"markdown","source":"### Following lines are to get the sentence-transfromer package as a github repo","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/UKPLab/sentence-transformers.git","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:45:56.37424Z","iopub.execute_input":"2022-05-25T19:45:56.374615Z","iopub.status.idle":"2022-05-25T19:45:59.66906Z","shell.execute_reply.started":"2022-05-25T19:45:56.374576Z","shell.execute_reply":"2022-05-25T19:45:59.667778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:46:42.951458Z","iopub.execute_input":"2022-05-25T19:46:42.951799Z","iopub.status.idle":"2022-05-25T19:46:42.958335Z","shell.execute_reply.started":"2022-05-25T19:46:42.951758Z","shell.execute_reply":"2022-05-25T19:46:42.957722Z"},"trusted":true},"execution_count":null,"outputs":[]}]}