{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train- DeBERTa with context multi-class\n\nIn this noteboook we fine tune pre-trained DeBERTa model for sequence classification problem\n\nWe take into consideration the context also in this notebook","metadata":{}},{"cell_type":"code","source":"# Import relevant modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy.linalg import norm\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\nimport torch\nfrom transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW\n# from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T10:00:32.545006Z","iopub.execute_input":"2022-05-29T10:00:32.545461Z","iopub.status.idle":"2022-05-29T10:00:40.176781Z","shell.execute_reply.started":"2022-05-29T10:00:32.545352Z","shell.execute_reply":"2022-05-29T10:00:40.175931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wnl = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:40.17834Z","iopub.execute_input":"2022-05-29T10:00:40.178965Z","iopub.status.idle":"2022-05-29T10:00:40.190013Z","shell.execute_reply.started":"2022-05-29T10:00:40.178928Z","shell.execute_reply":"2022-05-29T10:00:40.189201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(corpus, remove_stop_words = True):\n    '''\n    Function to clean a given corpus - lower the words, strip of the spaces, remove stopwords and lemmatize the corpus\n    Args:\n        corpus: the text to be cleaned\n        remove_stop_words: whether to remove stopwords\n    Returns:\n        filtered_sentence: cleaned corpus\n    '''\n    corpus = corpus.lower().strip()\n    word_tokens = word_tokenize(corpus)\n    if remove_stop_words:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus)) if i not in stop_words])\n    else:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus))])\n    return filtered_sentence\n\ndef cosine(a,b):\n    '''\n    Function to calculate cosine similarity of two vectors\n    Args:\n        a,b: vectors to calculate cosine between\n    Returns:\n        cosine similarity of the given vectors\n    '''\n    return np.dot(a,b)/(norm(a)*norm(b))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:40.191173Z","iopub.execute_input":"2022-05-29T10:00:40.192241Z","iopub.status.idle":"2022-05-29T10:00:40.203873Z","shell.execute_reply.started":"2022-05-29T10:00:40.192205Z","shell.execute_reply":"2022-05-29T10:00:40.202951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code_df = pd.read_csv(\"/kaggle/input/cpc-codes/titles.csv\")[[\"code\",\"title\"]]\ntrain_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:40.205929Z","iopub.execute_input":"2022-05-29T10:00:40.206461Z","iopub.status.idle":"2022-05-29T10:00:41.001973Z","shell.execute_reply.started":"2022-05-29T10:00:40.206425Z","shell.execute_reply":"2022-05-29T10:00:41.001023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:41.003333Z","iopub.execute_input":"2022-05-29T10:00:41.003742Z","iopub.status.idle":"2022-05-29T10:00:41.023945Z","shell.execute_reply.started":"2022-05-29T10:00:41.003702Z","shell.execute_reply":"2022-05-29T10:00:41.02303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:41.025214Z","iopub.execute_input":"2022-05-29T10:00:41.026102Z","iopub.status.idle":"2022-05-29T10:00:41.03613Z","shell.execute_reply.started":"2022-05-29T10:00:41.026066Z","shell.execute_reply":"2022-05-29T10:00:41.035308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.merge(train_df, code_df, left_on=\"context\",right_on=\"code\",how=\"left\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:41.037751Z","iopub.execute_input":"2022-05-29T10:00:41.038085Z","iopub.status.idle":"2022-05-29T10:00:41.149719Z","shell.execute_reply.started":"2022-05-29T10:00:41.038059Z","shell.execute_reply":"2022-05-29T10:00:41.148891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the text\ntrain_df[\"anchor\"] = train_df[\"anchor\"].apply(lambda x: clean_text(x,False))\ntrain_df[\"target\"] = train_df[\"target\"].apply(lambda x: clean_text(x,False))\ntrain_df[\"title\"] = train_df[\"title\"].apply(lambda x: clean_text(x,False))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:00:41.150987Z","iopub.execute_input":"2022-05-29T10:00:41.151505Z","iopub.status.idle":"2022-05-29T10:01:49.424011Z","shell.execute_reply.started":"2022-05-29T10:00:41.15146Z","shell.execute_reply":"2022-05-29T10:01:49.423129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating the anchor, target and context\ntrain_df[\"text\"] = train_df.apply(lambda x: x[\"anchor\"]+' [SEP] '+x[\"title\"]+' [SEP] '+x[\"target\"],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:01:49.42526Z","iopub.execute_input":"2022-05-29T10:01:49.425647Z","iopub.status.idle":"2022-05-29T10:01:50.220025Z","shell.execute_reply.started":"2022-05-29T10:01:49.425609Z","shell.execute_reply":"2022-05-29T10:01:50.21911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = 5","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:01:50.223084Z","iopub.execute_input":"2022-05-29T10:01:50.223524Z","iopub.status.idle":"2022-05-29T10:01:50.227863Z","shell.execute_reply.started":"2022-05-29T10:01:50.223478Z","shell.execute_reply":"2022-05-29T10:01:50.226955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the pretrained DeBERTa model\ntokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\nmodel = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\",num_labels=num_labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:01:50.229576Z","iopub.execute_input":"2022-05-29T10:01:50.230009Z","iopub.status.idle":"2022-05-29T10:02:11.367521Z","shell.execute_reply.started":"2022-05-29T10:01:50.22997Z","shell.execute_reply":"2022-05-29T10:02:11.366422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(\"Model loaded\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:02:11.36951Z","iopub.execute_input":"2022-05-29T10:02:11.370248Z","iopub.status.idle":"2022-05-29T10:02:16.26335Z","shell.execute_reply.started":"2022-05-29T10:02:11.370206Z","shell.execute_reply":"2022-05-29T10:02:16.262524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.batch_encode_plus(train_df[\"text\"].tolist(), truncation=False,return_tensors=\"pt\",padding=True)['input_ids']\nY = train_df[\"score\"].apply(lambda x: int(x*4)).tolist()\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:20.395626Z","iopub.execute_input":"2022-05-29T10:12:20.39619Z","iopub.status.idle":"2022-05-29T10:12:31.676515Z","shell.execute_reply.started":"2022-05-29T10:12:20.396155Z","shell.execute_reply":"2022-05-29T10:12:31.67569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:31.678297Z","iopub.execute_input":"2022-05-29T10:12:31.678684Z","iopub.status.idle":"2022-05-29T10:12:31.682676Z","shell.execute_reply.started":"2022-05-29T10:12:31.678649Z","shell.execute_reply":"2022-05-29T10:12:31.681783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs = torch.tensor(X_train, dtype=torch.int)\ntrain_labels = torch.tensor(Y_train, dtype=torch.float32)\nvalidation_inputs = torch.tensor(X_test, dtype=torch.int)\nvalidation_labels = torch.tensor(Y_test, dtype=torch.float32)\nprint(train_labels.shape,X_train.shape)\nprint(validation_labels.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:31.68408Z","iopub.execute_input":"2022-05-29T10:12:31.684705Z","iopub.status.idle":"2022-05-29T10:12:31.703056Z","shell.execute_reply.started":"2022-05-29T10:12:31.684668Z","shell.execute_reply":"2022-05-29T10:12:31.702382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(train_inputs, train_labels )\nvalidation_data = TensorDataset(validation_inputs, validation_labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:36.217277Z","iopub.execute_input":"2022-05-29T10:12:36.218099Z","iopub.status.idle":"2022-05-29T10:12:36.228895Z","shell.execute_reply.started":"2022-05-29T10:12:36.218058Z","shell.execute_reply":"2022-05-29T10:12:36.228022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sampler = SequentialSampler(train_data)\ntrain_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_loader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:37.667211Z","iopub.execute_input":"2022-05-29T10:12:37.667801Z","iopub.status.idle":"2022-05-29T10:12:37.673093Z","shell.execute_reply.started":"2022-05-29T10:12:37.667765Z","shell.execute_reply":"2022-05-29T10:12:37.672381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 5e-5, \n                  eps = 1e-8\n                )","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:12:38.142505Z","iopub.execute_input":"2022-05-29T10:12:38.143213Z","iopub.status.idle":"2022-05-29T10:12:38.155985Z","shell.execute_reply.started":"2022-05-29T10:12:38.143176Z","shell.execute_reply":"2022-05-29T10:12:38.155185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nloss_values = []\nepochs = 1\n\ntrain_len = len(train_loader)\neval_len = len(validation_loader)\n\nfor epoch in range(epochs):\n    print(f\"Epoch: {epoch+1}\")\n    total_loss = 0\n    model.train()\n    for step, batch in enumerate(train_loader):\n        if step % 50 == 0 and not step == 0:\n            print(f\"Step {step} loss: \",total_loss/(step*batch_size))\n        input_ids = batch[0].to(device)\n        labels = batch[1].to(device)\n\n        model.zero_grad()        \n        outputs = model(input_ids, labels=labels)  \n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n    avg_train_loss = total_loss / train_len      \n    loss_values.append(avg_train_loss)\n    print(f\"Train loss {avg_train_loss}\")\n    \n    model.eval()\n    \n    # Predicting the classes for validation data\n    i = 0\n    y_pred = []\n    while i<len(validation_inputs):\n        y_ = np.argmax(model(validation_inputs[i:i+16].to(device))[0].detach().to('cpu').numpy(),axis=1).tolist()\n        i+=16\n        y_pred.extend(y_)\n        \n    # Calculating correlation on the validation data\n    y1 = validation_labels.detach().to('cpu').numpy()/4\n    y2 = np.array([i/4 for i in y_pred])\n    corr,_ = pearsonr(y1,y2)\n\n    print(\"Validation Score: {0:.2f}\".format(corr))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:23:58.603412Z","iopub.execute_input":"2022-05-29T10:23:58.604096Z","iopub.status.idle":"2022-05-29T10:31:01.874635Z","shell.execute_reply.started":"2022-05-29T10:23:58.60406Z","shell.execute_reply":"2022-05-29T10:31:01.873389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This score is not that good\n\nBut this is an improvement over past 0.46 without the use of context","metadata":{}}]}