{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inferring predictions using the DeBERTa model with sinle class classification\n\nRefer to the training notebook:\n\nhttps://www.kaggle.com/code/bhavesjain/train-deberta-single-class","metadata":{}},{"cell_type":"code","source":"# Import relevant modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy.linalg import norm\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nimport datasets\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW, AutoModel, AutoConfig, AutoTokenizer\nfrom transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-07T13:12:03.351554Z","iopub.execute_input":"2022-06-07T13:12:03.351962Z","iopub.status.idle":"2022-06-07T13:12:11.271379Z","shell.execute_reply.started":"2022-06-07T13:12:03.35188Z","shell.execute_reply":"2022-06-07T13:12:11.270585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatizer and prerocessing functions","metadata":{}},{"cell_type":"code","source":"wnl = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:11:30.544733Z","iopub.execute_input":"2022-06-07T10:11:30.547121Z","iopub.status.idle":"2022-06-07T10:11:30.558806Z","shell.execute_reply.started":"2022-06-07T10:11:30.547083Z","shell.execute_reply":"2022-06-07T10:11:30.558025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(corpus, remove_stop_words = True):\n    '''\n    Function to clean a given corpus - lower the words, strip of the spaces, remove stopwords and lemmatize the corpus\n    Args:\n        corpus: the text to be cleaned\n        remove_stop_words: whether to remove stopwords\n    Returns:\n        filtered_sentence: cleaned corpus\n    '''\n    corpus = corpus.lower().strip()\n    word_tokens = word_tokenize(corpus)\n    if remove_stop_words:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus)) if i not in stop_words])\n    else:\n        filtered_sentence = \" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(corpus))])\n    return filtered_sentence\n\ndef cosine(a,b):\n    '''\n    Function to calculate cosine similarity of two vectors\n    Args:\n        a,b: vectors to calculate cosine between\n    Returns:\n        cosine similarity of the given vectors\n    '''\n    return np.dot(a,b)/(norm(a)*norm(b))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:11:30.56374Z","iopub.execute_input":"2022-06-07T10:11:30.566402Z","iopub.status.idle":"2022-06-07T10:11:30.581049Z","shell.execute_reply.started":"2022-06-07T10:11:30.566361Z","shell.execute_reply":"2022-06-07T10:11:30.579867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading and preprocessing the data","metadata":{}},{"cell_type":"code","source":"code_df = pd.read_csv(\"/kaggle/input/cpc-codes/titles.csv\")[[\"code\",\"title\"]]\ntest_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\ntest_df = pd.merge(test_df, code_df, left_on=\"context\",right_on=\"code\",how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:11:31.76882Z","iopub.execute_input":"2022-06-07T10:11:31.769228Z","iopub.status.idle":"2022-06-07T10:11:32.615697Z","shell.execute_reply.started":"2022-06-07T10:11:31.769197Z","shell.execute_reply":"2022-06-07T10:11:32.614907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the text\ntest_df[\"anchor\"] = test_df[\"anchor\"].apply(lambda x: clean_text(x,False))\ntest_df[\"target\"] = test_df[\"target\"].apply(lambda x: clean_text(x,False))\ntest_df[\"title\"] = test_df[\"title\"].apply(lambda x: clean_text(x,False))\n\n# Concatenating the anchor, target and context\ntest_df[\"text\"] = test_df.apply(lambda x: x[\"anchor\"]+' [SEP] '+x[\"title\"]+' [SEP] '+x[\"target\"],axis=1)\ntest_df = test_df.drop(columns = [\"anchor\", \"target\", \"context\", \"code\", \"title\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:11:33.329791Z","iopub.execute_input":"2022-06-07T10:11:33.330765Z","iopub.status.idle":"2022-06-07T10:11:35.673342Z","shell.execute_reply.started":"2022-06-07T10:11:33.330714Z","shell.execute_reply":"2022-06-07T10:11:35.672498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the model and tokenzier","metadata":{}},{"cell_type":"code","source":"model_path = \"/kaggle/input/debertabase/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:13:54.747767Z","iopub.execute_input":"2022-06-07T10:13:54.748121Z","iopub.status.idle":"2022-06-07T10:13:54.75221Z","shell.execute_reply.started":"2022-06-07T10:13:54.74809Z","shell.execute_reply":"2022-06-07T10:13:54.751454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = DebertaForSequenceClassification.from_pretrained(model_path,num_labels=1)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(\"Model loaded\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:13:55.128843Z","iopub.execute_input":"2022-06-07T10:13:55.129211Z","iopub.status.idle":"2022-06-07T10:14:07.104911Z","shell.execute_reply.started":"2022-06-07T10:13:55.129183Z","shell.execute_reply":"2022-06-07T10:14:07.103973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the test inputs","metadata":{}},{"cell_type":"code","source":"X = test_tokenizer.batch_encode_plus(test_df[\"text\"].tolist(), truncation=True,return_tensors=\"pt\",padding=True)['input_ids']\ntest_inputs = torch.tensor(X, dtype=torch.int)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:14:07.154204Z","iopub.execute_input":"2022-06-07T10:14:07.154894Z","iopub.status.idle":"2022-06-07T10:14:07.175019Z","shell.execute_reply.started":"2022-06-07T10:14:07.154862Z","shell.execute_reply":"2022-06-07T10:14:07.173966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating predictions","metadata":{}},{"cell_type":"code","source":"batch_size = 32\ni = 0\ny_pred = []\n\nwhile i<len(test_df):\n    outputs = model(test_inputs[i:i+batch_size].to(device))[0].detach().to('cpu').numpy()\n    i+=batch_size\n#     print(outputs)\n    y_pred.extend([i[0] for i in outputs])","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:14:09.184442Z","iopub.execute_input":"2022-06-07T10:14:09.184926Z","iopub.status.idle":"2022-06-07T10:14:10.146089Z","shell.execute_reply.started":"2022-06-07T10:14:09.184883Z","shell.execute_reply":"2022-06-07T10:14:10.145306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"score\"] = y_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:14:11.582352Z","iopub.execute_input":"2022-06-07T10:14:11.583076Z","iopub.status.idle":"2022-06-07T10:14:11.589282Z","shell.execute_reply.started":"2022-06-07T10:14:11.583031Z","shell.execute_reply":"2022-06-07T10:14:11.58737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df.drop(columns=[\"text\"])\nsubmission_df.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:14:12.023545Z","iopub.execute_input":"2022-06-07T10:14:12.024298Z","iopub.status.idle":"2022-06-07T10:14:12.034784Z","shell.execute_reply.started":"2022-06-07T10:14:12.024235Z","shell.execute_reply":"2022-06-07T10:14:12.033519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:14:12.348009Z","iopub.execute_input":"2022-06-07T10:14:12.348385Z","iopub.status.idle":"2022-06-07T10:14:12.368863Z","shell.execute_reply.started":"2022-06-07T10:14:12.348354Z","shell.execute_reply":"2022-06-07T10:14:12.367947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}