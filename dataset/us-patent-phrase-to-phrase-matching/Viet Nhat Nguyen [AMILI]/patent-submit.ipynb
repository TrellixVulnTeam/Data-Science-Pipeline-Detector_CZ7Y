{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This problem is similar to Natural language inference, but on patent phrases instead of sentences. It is a hard problem as the dataset is relatively small while to determine semantic similarity between phrases we need to encode information about domain context. Fortunately we have a pretrained BERT model trained on 100M+ patents released by Google [https://cloud.google.com/blog/products/ai-machine-learning/how-ai-improves-patent-analysis](https://cloud.google.com/blog/products/ai-machine-learning/how-ai-improves-patent-analysis). With this model, we can cast our problem to a classification problem where the model needs to predict among 5 classes corresponding to 5 levels of matching and the final score is the weighted average over 5 levels. Input to the model is simply the concatenation of [Cooperative Patent Classification, anchor, target].","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:32:16.458145Z","iopub.execute_input":"2022-04-17T14:32:16.459204Z","iopub.status.idle":"2022-04-17T14:32:16.46465Z","shell.execute_reply.started":"2022-04-17T14:32:16.459157Z","shell.execute_reply":"2022-04-17T14:32:16.46368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained('../input/patent-phrase-matching/patent_phrase/checkpoint-1026', num_labels=5, local_files_only=False)\n\ntokenizer = AutoTokenizer.from_pretrained('../input/patent-phrase-matching/patent_phrase/checkpoint-1026')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:02:57.107234Z","iopub.execute_input":"2022-04-17T15:02:57.108154Z","iopub.status.idle":"2022-04-17T15:03:14.331585Z","shell.execute_reply.started":"2022-04-17T15:02:57.108104Z","shell.execute_reply":"2022-04-17T15:03:14.330633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:32:29.82551Z","iopub.execute_input":"2022-04-17T14:32:29.825915Z","iopub.status.idle":"2022-04-17T14:32:29.83325Z","shell.execute_reply.started":"2022-04-17T14:32:29.825865Z","shell.execute_reply":"2022-04-17T14:32:29.832169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_row(row, test=False):\n    ret = tokenizer(row['context'][0] + ' ' + row['anchor'], row['target'])\n    if not test:\n        ret['label'] = np.digitize(row['score'], bins=np.linspace(0, 1, 5)) - 1\n    \n    return ret\n\ntest_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\ntest_data = [encode_row(row, test=True) for _, row in test_df.iterrows()]\ntestset = MyDataset(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:32:29.835019Z","iopub.execute_input":"2022-04-17T14:32:29.835936Z","iopub.status.idle":"2022-04-17T14:32:29.868936Z","shell.execute_reply.started":"2022-04-17T14:32:29.835883Z","shell.execute_reply":"2022-04-17T14:32:29.867943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model,tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:32:29.871072Z","iopub.execute_input":"2022-04-17T14:32:29.872865Z","iopub.status.idle":"2022-04-17T14:32:29.907453Z","shell.execute_reply.started":"2022-04-17T14:32:29.872813Z","shell.execute_reply":"2022-04-17T14:32:29.906641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = trainer.predict(testset)\n\nprob = np.exp(outputs.predictions)\nprob = prob / np.sum(prob, axis=1, keepdims=True)\npred = prob * np.linspace(0, 1, 5)\npred = np.sum(pred, axis=1)\n\nsubmit = pd.DataFrame({'id': test_df['id'], 'score': pred})\nsubmit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:32:29.908916Z","iopub.execute_input":"2022-04-17T14:32:29.909804Z","iopub.status.idle":"2022-04-17T14:32:33.408749Z","shell.execute_reply.started":"2022-04-17T14:32:29.909756Z","shell.execute_reply":"2022-04-17T14:32:33.408007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}