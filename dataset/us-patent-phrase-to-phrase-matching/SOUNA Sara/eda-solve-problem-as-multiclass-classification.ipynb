{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Content:**\n* [Data Description](#section-one)\n* [Load all dependencies we need](#section-two)\n* [EDA](#section-three)\n    - [General](#one)\n    - [Most common words in anchors](#two)\n    - [Most common words in targets](#three)\n    - [WordClouds](#foor)\n* [Create 5 folds cross validation](#folds)\n* [Supervised Learning](#supervised-learning)\n    - [Hyperparameters](#h)\n    - [Seed Everything](#s)\n    - [Dataset](#d)\n    - [Model](#m)\n    - [Utils + Train Function](#u)\n    - [Train](#t)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Data Description\n\nIn this dataset, you are presented pairs of phrases (an anchor and a target phrase) and asked to rate how similar they are on a scale from 0 (not at all similar) to 1 (identical in meaning). This challenge differs from a standard semantic similarity task in that similarity has been scored here within a patent's context, specifically its CPC classification (version 2021.05), which indicates the subject to which the patent relates. For example, while the phrases \"bird\" and \"Cape Cod\" may have low semantic similarity in normal language, the likeness of their meaning is much closer if considered in the context of \"house\".\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Load all dependencies we need","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom plotly import graph_objs as go\nfrom collections import Counter\nimport plotly.express as px\nimport seaborn as sns\n\n### dependencies we need to creat 5 folds cross validation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n### dependencies we need to do supervised learning (Multi class classification)\nimport torch\nimport torch.nn as nn\nfrom scipy import stats\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport scipy as sp\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T01:22:10.214394Z","iopub.execute_input":"2022-05-27T01:22:10.215574Z","iopub.status.idle":"2022-05-27T01:22:18.857632Z","shell.execute_reply.started":"2022-05-27T01:22:10.21552Z","shell.execute_reply":"2022-05-27T01:22:18.856909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train             = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ntest              = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:23.739491Z","iopub.execute_input":"2022-05-27T01:22:23.740194Z","iopub.status.idle":"2022-05-27T01:22:23.840255Z","shell.execute_reply.started":"2022-05-27T01:22:23.740158Z","shell.execute_reply":"2022-05-27T01:22:23.839598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# EDA\n\n<a id=\"subsection-one\"></a>\n**General**","metadata":{}},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:24.387514Z","iopub.execute_input":"2022-05-27T01:22:24.38825Z","iopub.status.idle":"2022-05-27T01:22:24.394549Z","shell.execute_reply.started":"2022-05-27T01:22:24.388206Z","shell.execute_reply":"2022-05-27T01:22:24.393349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So We have 36473 samples in the train set and 36 samples in the test set\n","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:24.868322Z","iopub.execute_input":"2022-05-27T01:22:24.869588Z","iopub.status.idle":"2022-05-27T01:22:24.9047Z","shell.execute_reply.started":"2022-05-27T01:22:24.869532Z","shell.execute_reply":"2022-05-27T01:22:24.90371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:25.078786Z","iopub.execute_input":"2022-05-27T01:22:25.079387Z","iopub.status.idle":"2022-05-27T01:22:25.090543Z","shell.execute_reply.started":"2022-05-27T01:22:25.079342Z","shell.execute_reply":"2022-05-27T01:22:25.089603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null Values in the test set and train set.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:25.539626Z","iopub.execute_input":"2022-05-27T01:22:25.540063Z","iopub.status.idle":"2022-05-27T01:22:25.560347Z","shell.execute_reply.started":"2022-05-27T01:22:25.540027Z","shell.execute_reply":"2022-05-27T01:22:25.559605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at the distribution of score in the train set","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='score',data=train)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:26.00156Z","iopub.execute_input":"2022-05-27T01:22:26.002217Z","iopub.status.idle":"2022-05-27T01:22:26.217039Z","shell.execute_reply.started":"2022-05-27T01:22:26.00218Z","shell.execute_reply":"2022-05-27T01:22:26.216268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's draw a Funnel-Chart for better visualization","metadata":{}},{"cell_type":"code","source":"temp = train.groupby('score').count()['target'].reset_index().sort_values(by='target',ascending=False)\nfig = go.Figure(go.Funnelarea(\n    text =temp.score,\n    values = temp.target,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Score Distribution\"}\n    ))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:26.431827Z","iopub.execute_input":"2022-05-27T01:22:26.432368Z","iopub.status.idle":"2022-05-27T01:22:26.552517Z","shell.execute_reply.started":"2022-05-27T01:22:26.432336Z","shell.execute_reply":"2022-05-27T01:22:26.551458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of uniques values in ANCHOR column: {train.anchor.nunique()} in train set\")\nprint(f\"Number of uniques values in TARGET column: {train.target.nunique()} in train set\")\nprint(f\"Number of uniques values in CONTEXT column: {train.context.nunique()} in train set\")\n\n\nprint(f\"Number of uniques values in ANCHOR column: {test.anchor.nunique()} in test set\")\nprint(f\"Number of uniques values in TARGET column: {test.target.nunique()} in test set\")\nprint(f\"Number of uniques values in CONTEXT column: {test.context.nunique()} in test set\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:26.688411Z","iopub.execute_input":"2022-05-27T01:22:26.68916Z","iopub.status.idle":"2022-05-27T01:22:26.709586Z","shell.execute_reply.started":"2022-05-27T01:22:26.689121Z","shell.execute_reply":"2022-05-27T01:22:26.708484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What do we currently Know About our Data:\n\nBefore starting let's look at some things that we already know about the data and will help us in gaining more new insights:\n* The data is imbalance we have just 3.16% of samples that have score 1, and 33.7 of samples with score 0.5. To solve this problem, we can augment the data containing the number of small samples to be the same as the largest class (score=0.5)\n* We have a lot of duplicated anchors(733) compared to targets(29340) in the train set.\n* Thanks to this discussion:https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/315220 We know that the anchors present in the test set are not included in the training data\n","metadata":{}},{"cell_type":"markdown","source":"TOP 20 anchors values","metadata":{}},{"cell_type":"code","source":"train.anchor.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:27.359613Z","iopub.execute_input":"2022-05-27T01:22:27.360589Z","iopub.status.idle":"2022-05-27T01:22:27.370773Z","shell.execute_reply.started":"2022-05-27T01:22:27.360532Z","shell.execute_reply":"2022-05-27T01:22:27.369784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TOP 20 targets values","metadata":{}},{"cell_type":"code","source":"train.target.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:27.810501Z","iopub.execute_input":"2022-05-27T01:22:27.811154Z","iopub.status.idle":"2022-05-27T01:22:27.833457Z","shell.execute_reply.started":"2022-05-27T01:22:27.811099Z","shell.execute_reply":"2022-05-27T01:22:27.832712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TOP 20 context values","metadata":{}},{"cell_type":"code","source":"train.context.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:28.401319Z","iopub.execute_input":"2022-05-27T01:22:28.402013Z","iopub.status.idle":"2022-05-27T01:22:28.412006Z","shell.execute_reply.started":"2022-05-27T01:22:28.401974Z","shell.execute_reply":"2022-05-27T01:22:28.411192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"two\"></a>\n**Most Common words in anchor**","metadata":{}},{"cell_type":"code","source":"train['temp_list'] = train['anchor'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:28.788532Z","iopub.execute_input":"2022-05-27T01:22:28.789016Z","iopub.status.idle":"2022-05-27T01:22:29.0914Z","shell.execute_reply.started":"2022-05-27T01:22:28.788975Z","shell.execute_reply":"2022-05-27T01:22:29.090735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oops we forgot to remove stopwors, but there is not much, so I guess there is no need","metadata":{}},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in anchor', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:29.318352Z","iopub.execute_input":"2022-05-27T01:22:29.319263Z","iopub.status.idle":"2022-05-27T01:22:30.307579Z","shell.execute_reply.started":"2022-05-27T01:22:29.319223Z","shell.execute_reply":"2022-05-27T01:22:30.306609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words in anchor')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:30.30904Z","iopub.execute_input":"2022-05-27T01:22:30.309462Z","iopub.status.idle":"2022-05-27T01:22:30.39044Z","shell.execute_reply.started":"2022-05-27T01:22:30.309429Z","shell.execute_reply":"2022-05-27T01:22:30.389292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n**Most Common words in target**\n","metadata":{}},{"cell_type":"code","source":"train['temp_list'] = train['target'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:30.568535Z","iopub.execute_input":"2022-05-27T01:22:30.569364Z","iopub.status.idle":"2022-05-27T01:22:30.639068Z","shell.execute_reply.started":"2022-05-27T01:22:30.569325Z","shell.execute_reply":"2022-05-27T01:22:30.637973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in target', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:30.979629Z","iopub.execute_input":"2022-05-27T01:22:30.980269Z","iopub.status.idle":"2022-05-27T01:22:31.108077Z","shell.execute_reply.started":"2022-05-27T01:22:30.980231Z","shell.execute_reply":"2022-05-27T01:22:31.107453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words in target')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:31.55361Z","iopub.execute_input":"2022-05-27T01:22:31.5547Z","iopub.status.idle":"2022-05-27T01:22:31.615297Z","shell.execute_reply.started":"2022-05-27T01:22:31.554652Z","shell.execute_reply":"2022-05-27T01:22:31.614311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that a lot of most common words in anchor and target are the same as 'source', 'system', 'surface', and 'member',which was obvious bc we are asked to rate how similar anchor and target.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"foor\"></a>\n\n**It's Time For WordClouds**\n\nWe will be building wordclouds in the following order:\n\n* WordCloud of anchors\n* WordCloud of targets","metadata":{}},{"cell_type":"code","source":"def generate_wordCloud(text, color, title, title_size):\n    wordcloud = WordCloud(background_color=color,\n                    min_font_size = 5,\n                    random_state = 42,\n                    width=400, \n                    height=200)\n    wordcloud.generate(str(text))\n\n    plt.imshow(wordcloud);\n    plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                              'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:32.748505Z","iopub.execute_input":"2022-05-27T01:22:32.748928Z","iopub.status.idle":"2022-05-27T01:22:32.755381Z","shell.execute_reply.started":"2022-05-27T01:22:32.748893Z","shell.execute_reply":"2022-05-27T01:22:32.754256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WORDCLOUD OF ANCHORS¶**\n\nWe Have already visualized our Most Common anchors words ,but Wordclouds Provide us much more clarity","metadata":{}},{"cell_type":"code","source":"generate_wordCloud(train['anchor'],color='black',title_size=15,title=\"WordCloud of Anchors\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:33.994209Z","iopub.execute_input":"2022-05-27T01:22:33.994593Z","iopub.status.idle":"2022-05-27T01:22:34.205467Z","shell.execute_reply.started":"2022-05-27T01:22:33.994561Z","shell.execute_reply":"2022-05-27T01:22:34.204218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WORDCLOUD OF TARGETS¶**\n\nWe Have already visualized our Most Common targets words ,but Wordclouds Provide us much more clarity","metadata":{}},{"cell_type":"code","source":"generate_wordCloud(train['target'],color='black',title_size=15,title=\"WordCloud of Targets\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:22:35.142409Z","iopub.execute_input":"2022-05-27T01:22:35.1435Z","iopub.status.idle":"2022-05-27T01:22:35.3695Z","shell.execute_reply.started":"2022-05-27T01:22:35.143449Z","shell.execute_reply":"2022-05-27T01:22:35.36837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n\n# Create folds","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ndf['score_map'] = df['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n\nencoder = LabelEncoder()\ndf['anchor_map'] = encoder.fit_transform(df['anchor'])\n\nkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (_, valid_index) in enumerate(kf.split(df, df['score_map'], groups=df['anchor_map'])):\n    df.loc[valid_index, 'fold'] = int(n)\n\ndf['fold'] = df['fold'].astype(int)\ndf.to_csv('folds.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:23:08.738119Z","iopub.execute_input":"2022-05-27T01:23:08.738503Z","iopub.status.idle":"2022-05-27T01:23:09.272515Z","shell.execute_reply.started":"2022-05-27T01:23:08.738472Z","shell.execute_reply":"2022-05-27T01:23:09.271602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"supervised-learning\"></a>\n# supervised learning (Multi class classification)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"h\"></a>\n**Hyperparameters**","metadata":{}},{"cell_type":"code","source":"class args:\n    model = \"anferico/bert-for-patents\"\n    max_len = 32\n    accumulation_steps = 1\n    batch_size = 64\n    epochs = 5\n    learning_rate = 2e-5","metadata":{"execution":{"iopub.status.busy":"2022-05-27T01:23:29.174351Z","iopub.execute_input":"2022-05-27T01:23:29.174951Z","iopub.status.idle":"2022-05-27T01:23:29.179725Z","shell.execute_reply.started":"2022-05-27T01:23:29.174907Z","shell.execute_reply":"2022-05-27T01:23:29.178842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"s\"></a>\n\n**seed everything**","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    #random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    #np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"d\"></a>\n**Dataset**","metadata":{}},{"cell_type":"code","source":"class PhraseDataset:\n    def __init__(self, anchor, target, context, score, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.score = score\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item]\n        context = self.context[item]\n        target = self.target[item]\n        score = self.score[item]\n        \n        if score == 0.0  : score = [1,0,0,0,0]\n        if score == 0.25 : score = [0,1,0,0,0]\n        if score == 0.5  : score = [0,0,1,0,0]\n        if score == 0.75 : score = [0,0,0,1,0]\n        if score == 1.0  : score = [0,0,0,0,1]\n\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        attention_mask = encoded_text[\"attention_mask\"]\n        token_type_ids = encoded_text[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"score\": torch.tensor(score, dtype=torch.float32),\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"m\"></a>\n**Model**","metadata":{}},{"cell_type":"code","source":"class PhraseModel(nn.Module):\n    def __init__(self, model_name, learning_rate, num_train_steps, steps_per_epoch):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.steps_per_epoch = steps_per_epoch\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 5,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 5)\n\n    def forward(self, ids, mask, token_type_ids, score):\n        transformer_out = self.transformer(ids, mask, token_type_ids)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"u\"></a>\n**Utils and train finction**","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(state, filename): \n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)\n    return score[0]\n\ndef check_acc(loader, model, loss):\n    model.eval()\n    loop       = tqdm(loader)\n    total_loss = 0\n    total_p    = 0\n    with torch.no_grad():\n        for batch_idx, inputs in enumerate(loop):\n\n            ids            = inputs['ids'].to('cuda')\n            mask           = inputs['mask'].to('cuda')\n            token_type_ids = inputs['token_type_ids'].to('cuda')\n            score          = inputs['score'].to('cuda')\n            \n            output         = model(ids, mask, token_type_ids, score)\n            total_loss    += loss(score, output)      \n            total_p       += get_score((torch.argmax(score, dim=1) / 4).cpu(), (torch.argmax(output, dim=1) / 4).cpu())\n    print(total_loss/len(loader), total_p/len(loader))\n    return total_p/len(loader)\n    model.train()\n\n    \ndef train_fn(loader, model, optimizer, loss_fn):\n    loop = tqdm(loader)\n    \n    for batch_idx, inputs in enumerate(loop):\n    \n        ids            = inputs['ids'].to('cuda')\n        mask           = inputs['mask'].to('cuda')\n        token_type_ids = inputs['token_type_ids'].to('cuda')\n        score          = inputs['score'].to('cuda')\n  \n        optimizer.zero_grad()\n        output = model(ids, mask, token_type_ids, score)\n        loss   = loss_fn(output.squeeze(), score.squeeze())\n\n        loss.backward()\n        optimizer.step()\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t\"></a>\n**Train**","metadata":{}},{"cell_type":"code","source":"for fold_ in range(5):\n    print('#############################' + str(fold_))\n    df = pd.read_csv(\"./folds.csv\")\n\n    context_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n    }\n\n    df.context = df.context.apply(lambda x: context_mapping[x[0]])\n\n    train_df = df[df[\"fold\"] != fold_].reset_index(drop=True)\n    valid_df = df[df[\"fold\"] == fold_].reset_index(drop=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n    train_dataset = PhraseDataset(\n        anchor=train_df.anchor.values,\n        target=train_df.target.values,\n        context=train_df.context.values,\n        score=train_df.score.values,\n        tokenizer=tokenizer,\n        max_len=args.max_len,\n    )\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=args.batch_size,\n        shuffle=True,\n    )\n    valid_dataset = PhraseDataset(\n        anchor=valid_df.anchor.values,\n        target=valid_df.target.values,\n        context=valid_df.context.values,\n        score=valid_df.score.values,\n        tokenizer=tokenizer,\n        max_len=args.max_len,\n    )\n    val_loader = DataLoader(\n        valid_dataset,\n        batch_size=args.batch_size,\n        shuffle=False,\n    )\n\n    num_train_steps = int(len(train_dataset) / args.batch_size / args.accumulation_steps * args.epochs)\n    steps_per_epoch = len(train_dataset) / args.batch_size\n\n    model = PhraseModel(\n        model_name      = args.model,\n        learning_rate   = args.learning_rate,\n        num_train_steps = num_train_steps,\n        steps_per_epoch = steps_per_epoch,\n    ).to('cuda')\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) \n    loss_fn   = nn.BCEWithLogitsLoss()\n    \n    check_acc(val_loader, model, loss_fn)\n    loss = 0\n    for epoch in range(args.epochs):\n        print(epoch)\n        train_fn(train_loader, model, optimizer, loss_fn)\n        loss_val = check_acc(val_loader, model, loss_fn)\n        \n        if loss_val > loss:\n            loss = loss_val\n\n            checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\":  optimizer.state_dict(),\n            }\n\n            save_checkpoint(checkpoint, filename='my_checkpoint.pth.tar'+ str(fold_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}