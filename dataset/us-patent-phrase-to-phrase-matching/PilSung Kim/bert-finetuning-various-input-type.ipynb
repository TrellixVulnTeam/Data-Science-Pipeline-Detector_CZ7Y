{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-20T10:18:20.667188Z","iopub.execute_input":"2022-05-20T10:18:20.667488Z","iopub.status.idle":"2022-05-20T10:18:20.751511Z","shell.execute_reply.started":"2022-05-20T10:18:20.667455Z","shell.execute_reply":"2022-05-20T10:18:20.750497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ncpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntrain_df = pd.merge(train_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T11:00:33.453844Z","iopub.execute_input":"2022-05-20T11:00:33.454139Z","iopub.status.idle":"2022-05-20T11:00:34.212729Z","shell.execute_reply.started":"2022-05-20T11:00:33.454106Z","shell.execute_reply":"2022-05-20T11:00:34.211547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nimport string\n\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset, SequentialSampler, RandomSampler\n\ndef normalize_text(s):\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n    \n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\ntrain_df['title'] = train_df['title'].apply(lambda x: normalize_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-20T11:00:34.408845Z","iopub.execute_input":"2022-05-20T11:00:34.409835Z","iopub.status.idle":"2022-05-20T11:00:34.760643Z","shell.execute_reply.started":"2022-05-20T11:00:34.409789Z","shell.execute_reply":"2022-05-20T11:00:34.759415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Various input type","metadata":{}},{"cell_type":"code","source":"# input_type = 0    # [CLS] target [SEP] anchor title\n# input_type = 1    # [CLS] target title [SEP] anchor title\n# input_type = 2    # [CLS] target [SEP] anchor [SEP] title\n# input_type = 3    # [CLS] target anchor [SEP] title \n\n\nmodel_name = ''\nif input_type == 0:\n    train_df['sen1'] = train_df['target'].astype('str') + ' [SEP] '+ train_df['anchor'].astype('str') + ' ' + train_df['title'].astype('str')\n    model_name = 'type0'\nelif input_type == 1:\n    train_df['sen1'] = train_df['target'].astype('str') + ' ' + train_df['title'].astype('str') + ' [SEP] ' + train_df['anchor'].astype('str') + ' ' + train_df['title'].astype('str')\n    model_name = 'type1'\nelif input_type == 2:\n    train_df['sen1'] = train_df['target'].astype('str')+' [SEP] '+ train_df['anchor'].astype('str') + ' [SEP] ' + train_df['title'].astype('str')\n    model_name = 'type2'\nelif input_type == 3:\n    train_df['sen1'] = train_df['target'].astype('str')+' ' + train_df['anchor'].astype('str') + ' [SEP] ' + train_df['title'].astype('str')\n    model_name = 'type3'\nelse:\n    raise Exception(\"Error\")\n\n# train_df = train_df.drop(['anchor','context','title'],axis=1)\ntrain_df = train_df.drop(['anchor','context','title','target'],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:23.390381Z","iopub.execute_input":"2022-05-20T10:18:23.390662Z","iopub.status.idle":"2022-05-20T10:18:23.436372Z","shell.execute_reply.started":"2022-05-20T10:18:23.390601Z","shell.execute_reply":"2022-05-20T10:18:23.435381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# x_train,x_test,y_train,y_test = train_test_split(train_df[['target','sen1']],train_df['score'],random_state=1234,test_size=0.1)\nx_train,x_test,y_train,y_test = train_test_split(train_df['sen1'],train_df['score'],random_state=1234,test_size=0.1)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:24.727362Z","iopub.execute_input":"2022-05-20T10:18:24.727675Z","iopub.status.idle":"2022-05-20T10:18:24.744731Z","shell.execute_reply.started":"2022-05-20T10:18:24.727634Z","shell.execute_reply":"2022-05-20T10:18:24.743492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn.utils.clip_grad import clip_grad_norm\n\n\ntorch.cuda.is_available()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:25.466762Z","iopub.execute_input":"2022-05-20T10:18:25.467023Z","iopub.status.idle":"2022-05-20T10:18:25.677186Z","shell.execute_reply.started":"2022-05-20T10:18:25.466991Z","shell.execute_reply":"2022-05-20T10:18:25.676136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self,bert_path):\n        super(BERT,self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.AutoModel.from_pretrained(self.bert_path)\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(1024,1),\n            nn.Sigmoid()\n        )\n    def forward(self,ids,mask,token_type_ids):\n        out = self.bert(input_ids=ids,attention_mask=mask,token_type_ids=token_type_ids,return_dict=True)\n        pooler_output = out.get('pooler_output')\n        cls_out = self.fc_layer(pooler_output)\n        return cls_out","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:26.065706Z","iopub.execute_input":"2022-05-20T10:18:26.065981Z","iopub.status.idle":"2022-05-20T10:18:26.073871Z","shell.execute_reply.started":"2022-05-20T10:18:26.06595Z","shell.execute_reply":"2022-05-20T10:18:26.072403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train_dataset:\n#     def __init__(self,text1,text2,label,tokenizer,max_len):\n#         self.text1=text1\n#         self.text2=text2\n#         self.label=label\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n\n    def __init__(self,text1,label,tokenizer,max_len):\n        self.text1=text1\n        self.label=label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n#         text_2 = str(self.text2[idx])\n        label = self.label[idx]\n        \n        inputs = self.tokenizer(\n#             text_1,\n#             text_2,\n            text_1, # ex, \"hi [sep] hello [sep] nice to meet you'\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\": torch.tensor(label,dtype=torch.float),\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:26.991487Z","iopub.execute_input":"2022-05-20T10:18:26.991815Z","iopub.status.idle":"2022-05-20T10:18:27.003989Z","shell.execute_reply.started":"2022-05-20T10:18:26.991775Z","shell.execute_reply":"2022-05-20T10:18:27.002475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len=128\ntrain_batch_size = 16\nepochs=2\nbert_path = '../input/bert-for-patents/bert-for-patents'\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(bert_path)\n\n# Training dataset prep\n\n# train_text1 = list(x_train['target'].values)\n# train_text2 = list(x_train['sen1'].values)\ntrain_text1 = list(x_train.values)\ntrain_label = list(y_train.values)\n\ntrain_dataset = Train_dataset(\n    text1 = train_text1,\n#     text2 = train_text2,\n    label = train_label,\n    tokenizer=tokenizer ,\n    max_len=max_len\n)\n\ntrain_data_loader = DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)\n\n# validation dataset prep\n# val_text1 = list(x_test['target'].values)\n# val_text2 = list(x_test['sen1'].values)\nval_text1 = list(x_test.values)\nval_label = list(y_test.values)\n\nvalid_dataset = Train_dataset(\n    text1 = val_text1,\n#     text2 = val_text2,\n    label = val_label,\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\nvalid_data_loader = DataLoader(valid_dataset,batch_size=train_batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:27.781032Z","iopub.execute_input":"2022-05-20T10:18:27.78134Z","iopub.status.idle":"2022-05-20T10:18:27.879606Z","shell.execute_reply.started":"2022-05-20T10:18:27.781289Z","shell.execute_reply":"2022-05-20T10:18:27.878665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, scheduler, loss_function, epochs,train_dataloader, device, clip_value=2):\n    model.train()\n    for epoch in range(epochs):\n        best_loss = []\n        for step, batch in enumerate(train_dataloader): \n            batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n            batch_token_type_ids = batch['token_type_ids'].to(device)\n            model.zero_grad()\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n            loss = loss_function(outputs.squeeze(),batch_labels.squeeze())\n            best_loss.append(loss)\n            loss.backward()\n            clip_grad_norm(model.parameters(), clip_value)\n            optimizer.step()\n            scheduler.step()\n\n        loss2 = sum(best_loss)/len(best_loss)\n        print(f'Epoch : {epoch} ,Train loss : {loss2}')\n                \n    return model\n\ndef compute_metrics(predictions, labels):\n\n    predictions = predictions.reshape(len(predictions))\n    predictions = predictions.cpu().clone().numpy()\n    labels = labels.cpu().clone().numpy()\n    return  np.corrcoef(predictions, labels)[0][1]\n\ndef evaluate(model,loss_function,test_dataloader,device):\n    model.eval()\n    test_loss, test_pear = [], []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        loss = loss_function(outputs, batch_labels)\n        test_loss.append(loss.item())\n        pearson = compute_metrics(outputs, batch_labels)\n        test_pear.append(pearson.item())\n    return test_loss, test_pear","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:28.530238Z","iopub.execute_input":"2022-05-20T10:18:28.530735Z","iopub.status.idle":"2022-05-20T10:18:28.547937Z","shell.execute_reply.started":"2022-05-20T10:18:28.5307Z","shell.execute_reply":"2022-05-20T10:18:28.546807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_steps = len(train_data_loader) * epochs\n\nmodel = BERT(bert_path).to(device)\n\noptimizer = transformers.AdamW(model.parameters(),lr=3e-5,eps=1e-8)\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n)\n\nloss_function = nn.MSELoss()\n\n\nmodel = train(model, optimizer, scheduler, loss_function, epochs,train_data_loader, device)\n\n\nloss1,pear_ = evaluate(model,loss_function,valid_data_loader,device)\n\nloss = sum(loss1)/len(loss1)\npear = sum(pear_)/len(pear_)\nprint(f\"eval mean result : loss {loss}, pearson {pear}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:18:29.54451Z","iopub.execute_input":"2022-05-20T10:18:29.544871Z","iopub.status.idle":"2022-05-20T10:44:15.77039Z","shell.execute_reply.started":"2022-05-20T10:18:29.544837Z","shell.execute_reply":"2022-05-20T10:44:15.76932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss1,pear_ = evaluate(model,loss_function,valid_data_loader,device)\n\nloss = sum(loss1)/len(loss1)\npear = sum(pear_)/len(pear_)\nprint(f\"eval mean result : loss {loss}, pearson {pear}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:44:15.772495Z","iopub.execute_input":"2022-05-20T10:44:15.77288Z","iopub.status.idle":"2022-05-20T10:45:09.359986Z","shell.execute_reply.started":"2022-05-20T10:44:15.772823Z","shell.execute_reply":"2022-05-20T10:45:09.357849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'type0'\nif not os.path.exists(f'./my_bert'):\n    os.mkdir(f'./my_bert')\ntorch.save(model.state_dict(),f'./my_bert'+'/'+model_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:09.36191Z","iopub.execute_input":"2022-05-20T10:45:09.362253Z","iopub.status.idle":"2022-05-20T10:45:12.82081Z","shell.execute_reply.started":"2022-05-20T10:45:09.362193Z","shell.execute_reply":"2022-05-20T10:45:12.819797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\n- follow evaluate function\n- create dataset class","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:12.824626Z","iopub.execute_input":"2022-05-20T10:45:12.82498Z","iopub.status.idle":"2022-05-20T10:45:13.293709Z","shell.execute_reply.started":"2022-05-20T10:45:12.824934Z","shell.execute_reply":"2022-05-20T10:45:13.29252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\ncpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntest_df = pd.merge(test_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\ntest_df['title'] = test_df['title'].apply(lambda x: normalize_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:13.2952Z","iopub.execute_input":"2022-05-20T10:45:13.296024Z","iopub.status.idle":"2022-05-20T10:45:14.183289Z","shell.execute_reply.started":"2022-05-20T10:45:13.295968Z","shell.execute_reply":"2022-05-20T10:45:14.182262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['sen1'] = test_df['anchor'].astype('str')+' '+test_df['title'].astype('str')\ntest_df = test_df.drop(['anchor','context','title'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:14.185011Z","iopub.execute_input":"2022-05-20T10:45:14.18533Z","iopub.status.idle":"2022-05-20T10:45:14.196106Z","shell.execute_reply.started":"2022-05-20T10:45:14.185269Z","shell.execute_reply":"2022-05-20T10:45:14.195124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test_datset:\n    def __init__(self,text1,text2,idf,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.idf = idf\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        idf = self.idf[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"idf\": idf\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:14.19767Z","iopub.execute_input":"2022-05-20T10:45:14.198187Z","iopub.status.idle":"2022-05-20T10:45:14.632826Z","shell.execute_reply.started":"2022-05-20T10:45:14.19814Z","shell.execute_reply":"2022-05-20T10:45:14.631795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text1 = list(test_df['target'].values)\ntest_text2 = list(test_df['sen1'].values)\n\ntest_dataset = Test_datset(\n    text1 = test_text1,\n    text2 = test_text2,\n    idf=list(test_df['id'].values),\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\ntest_data_loader = DataLoader(test_dataset,batch_size=64,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:14.636078Z","iopub.execute_input":"2022-05-20T10:45:14.636317Z","iopub.status.idle":"2022-05-20T10:45:14.982104Z","shell.execute_reply.started":"2022-05-20T10:45:14.636286Z","shell.execute_reply":"2022-05-20T10:45:14.98099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model,test_dataloader,device):\n    model.eval()\n    result = []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks = batch['ids'].to(device), batch['mask'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        out = [i[0] for i in outputs.cpu().detach().numpy()]\n        batch_idf = batch['idf']\n        temp = [[i,j] for i,j in zip(batch_idf,out)]\n        result.extend(temp)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:14.983645Z","iopub.execute_input":"2022-05-20T10:45:14.984078Z","iopub.status.idle":"2022-05-20T10:45:15.246007Z","shell.execute_reply.started":"2022-05-20T10:45:14.984017Z","shell.execute_reply":"2022-05-20T10:45:15.244987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = my_model(bert_path).to(device)\nmodel.load_state_dict(torch.load('my_bert'))\n\nfinal_res = predict(model,test_data_loader,device)\nsample = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nsubmit_csv = pd.DataFrame(final_res,columns=['id','score'])\nsubmit_csv.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T10:45:15.250408Z","iopub.execute_input":"2022-05-20T10:45:15.251035Z","iopub.status.idle":"2022-05-20T10:45:16.377514Z","shell.execute_reply.started":"2022-05-20T10:45:15.251Z","shell.execute_reply":"2022-05-20T10:45:16.375939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}