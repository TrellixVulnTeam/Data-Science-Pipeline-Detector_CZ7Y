{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np, pandas as pd, time, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras, os\nimport scipy\nfrom keras.layers import Flatten, Dense\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.stats import pearsonr\nimport random\n\nimport transformers\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoTokenizer, TFAutoModel\n\ntransformers.logging.set_verbosity_error()\n\nMAX_LEN = 50\nLEARNING_RATE = 5e-5\nMODEL_PATH = '../input/deberta-v3-large/deberta-v3-large'\n    \n\n\n#dataloads\ndf_train = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ndf_test = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\ndf_submission_template = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\ndf_cpc = pd.read_csv(\"../input/cpc-codes/titles.csv\")\n\n\n#adaptive LR\ndef schedule(epoch, lr):\n    if epoch == 0:\n        return lr * 0.25\n    else:\n        return lr * (0.75**epoch)\n        \n \n        \n#pearson \ndef correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = keras.backend.mean(x)\n    my = keras.backend.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = keras.backend.sum(tf.multiply(xm,ym))\n    r_den = keras.backend.sqrt(tf.multiply(keras.backend.sum(keras.backend.square(xm)), keras.backend.sum(keras.backend.square(ym))))\n    r = r_num / r_den\n\n    r = keras.backend.maximum(keras.backend.minimum(r, 1.0), -1.0)\n    return 1 - keras.backend.square(r)\n\n\n#pearson2\nclass callback_pearson(tf.keras.callbacks.Callback):\n    def __init__(self, val_data):\n        self.X_val, self.Y_val = val_data\n    def on_epoch_end(self, epoch, logs):\n        X_val_preds = self.model.predict(self.X_val)\n        pearson_corr = pearsonr(X_val_preds.ravel(), self.Y_val)\n        print(\"pearsonr_val (from log) =\", pearson_corr[0])\n        logs[\"val_pearsonr\"] = pearson_corr[0]\n\n\n    \n        \n#tokenizer\ndef tokenize_data(word_list, max_len):\n    tokens = tokenizer(\n        [(w[0] + \" \" + w[2], w[1]) for w in word_list],\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True,\n        return_tensors=\"np\",\n        verbose=False\n    )\n    \n    # minimize memory usage with datatypes\n    token_output = {'input_ids': tokens['input_ids'].astype('int32'),\n                    'attention_mask': tokens['attention_mask'].astype('int32'),\n                    'token_type_ids': tokens['token_type_ids'].astype('int32')}\n    return token_output\n\n \ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n#CPC Load\ndf_cpc['title'] = df_cpc['title'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()\n\n#train data\ndf_train_merged = df_train.merge(df_cpc[['code','title']], left_on='context',right_on='code', how='left').drop(['code', 'context'], axis=1)\nanchor_target_title = df_train_merged[[\"anchor\", \"target\", \"title\"]].values.tolist()\ntoken_output_train = tokenize_data(anchor_target_title, MAX_LEN)\ntrainX = [token_output_train['input_ids'], token_output_train['attention_mask'], token_output_train['token_type_ids']]\ntrainY = df_train_merged['score'].values\n#test data\ndf_test_merged = df_test.merge(df_cpc[['code','title']], left_on='context',right_on='code', how='left').drop(['code', 'context'], axis=1)\nanchor_target_title = df_test_merged[[\"anchor\", \"target\", \"title\"]].values.tolist()\ntoken_output_test = tokenize_data(anchor_target_title, MAX_LEN)\ntestX = [token_output_test['input_ids'], token_output_test['attention_mask'], token_output_test['token_type_ids']]\n\n#lazy validation\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor train_index, test_index in kf.split(trainY):\n    valX = [trainX[0][test_index], trainX[1][test_index], trainX[2][test_index]]\n    valY = trainY[test_index]\n    trainX[0] = np.delete(trainX[0], test_index, 0)\n    trainX[1] = np.delete(trainX[1], test_index, 0)\n    trainX[2] = np.delete(trainX[2], test_index, 0)\n    trainY = np.delete(trainY, test_index)\n    break\n\nprint(valX[1].shape)\nval_combined = ((valX[0],valX[1], valX[2]), valY.ravel())\n\n\n\n\n#model\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)   # always seed your experiments\nInit = keras.initializers.GlorotUniform(seed=0)\n\ncallback_learningrate = tf.keras.callbacks.LearningRateScheduler(schedule)\ncallback_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_pearsonr', patience=2,mode='max',restore_best_weights=True)\ncallback_checkpoint = tf.keras.callbacks.ModelCheckpoint('tf_model.h5', monitor='val_pearsonr', save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n\ninput_ids = tf.keras.layers.Input(\n    shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = tf.keras.layers.Input(\n    shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\ntoken_type_ids = tf.keras.layers.Input(\n    shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n\ndeberta_model = TFAutoModel.from_pretrained(MODEL_PATH, trainable=True)\ndeberta_model_output = deberta_model(\n    input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    \navg_pool = tf.keras.layers.GlobalAveragePooling1D()(deberta_model_output.last_hidden_state)\ndropout = tf.keras.layers.Dropout(0.3)(avg_pool)\noutput = tf.keras.layers.Dense(1, activation=\"linear\", name=\"output\")(dropout)\n\nmodel = tf.keras.models.Model(\n    inputs=[input_ids, attention_mask, token_type_ids], outputs=output)\n\nmodel.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n    loss='mse')\n\nmodel.summary()   \n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=20,\n    callbacks = [callback_learningrate, callback_pearson(val_combined), callback_earlystop, callback_checkpoint],\n    batch_size=16,\n    shuffle=True,\n    validation_data=val_combined)\n    \n    \n\npred = model.predict(testX)\ndf_submission_template['score'] = np.clip(pred, 0, 1)\ndf_submission_template.to_csv(\"submission.csv\", index=False)\n\n\n\n\n\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T13:58:25.699544Z","iopub.execute_input":"2022-05-02T13:58:25.700377Z","iopub.status.idle":"2022-05-02T14:06:04.93287Z","shell.execute_reply.started":"2022-05-02T13:58:25.700247Z","shell.execute_reply":"2022-05-02T14:06:04.930446Z"},"trusted":true},"execution_count":null,"outputs":[]}]}