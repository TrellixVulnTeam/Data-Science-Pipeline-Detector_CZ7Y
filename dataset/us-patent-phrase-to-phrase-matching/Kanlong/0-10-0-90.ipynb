{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('begin')\n\nINPUT_DIR = '/kaggle/input/us-patent-phrase-to-phrase-matching/'\n# INPUT_DIR = ''\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-11T08:14:33.936015Z","iopub.execute_input":"2022-06-11T08:14:33.936824Z","iopub.status.idle":"2022-06-11T08:14:41.063363Z","shell.execute_reply.started":"2022-06-11T08:14:33.936736Z","shell.execute_reply":"2022-06-11T08:14:41.062052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        super(Config, self).__init__()\n        self.DEBUG = False\n        self.SEED = 42\n        self.MODEL_TYPE = 'electra_large'\n        self.MODEL_PATH = 'google/electra-large-discriminator'\n        self.BATCH_SIZE = 8\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.LR = 1e-5\n        self.N_WARMUP = 0\n        self.EPOCHS = 5\n\nCONFIG = Config()\n\n# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\ndef get_logger(filename=OUTPUT_DIR + 'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=CONFIG.SEED)\n\n\n# ====================================================\n# Data Loading\n# ====================================================\nif CONFIG.DEBUG:\n    train = pd.read_csv(INPUT_DIR+'train.csv', nrows=1000)\nelse:\n    train = pd.read_csv(INPUT_DIR + 'train.csv')\ntest = pd.read_csv(INPUT_DIR+'test.csv')\nsubmission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\nprint(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")\nprint(f\"submission.shape: {submission.shape}\")\n\n# ====================================================\n# CPC Data\n# ====================================================\ndef get_cpc_texts():\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt', encoding='utf-8') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results\n\ncpc_texts = get_cpc_texts()\ntorch.save(cpc_texts, OUTPUT_DIR+\"cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntest['context_text'] = test['context'].map(cpc_texts)\ntrain['context_text'] = train['context_text'].map(lambda s : s.lower())\ntest['context_text'] = test['context_text'].map(lambda s : s.lower())\n\ntrain['text'] = 'anchor:' + train['anchor'] + '[SEP]' + 'target:' + train['target'] + '[SEP]' + 'context:' + train['context_text']\ntest['text'] = 'anchor:' + test['anchor'] + '[SEP]' + 'target:' + test['target'] + '[SEP]' + 'context:' + test['context_text']","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:14:41.065275Z","iopub.execute_input":"2022-06-11T08:14:41.065922Z","iopub.status.idle":"2022-06-11T08:14:42.004223Z","shell.execute_reply.started":"2022-06-11T08:14:41.065885Z","shell.execute_reply":"2022-06-11T08:14:42.003441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/deberta-v3-large/deberta-v3-large'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions1 = 0\npredictions2 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/deberta-large-cv/deberta_v3_large_BCE_fold_{fold}.pt'))\n    predictions1 += val_fn(model, valid_dl, mode = 'BCE')/4\n    model.load_state_dict(torch.load(f'../input/deberta-large-cv/deberta_v3_large_MSE_fold_{fold}.pt'))\n    predictions2 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:14:42.005365Z","iopub.execute_input":"2022-06-11T08:14:42.005711Z","iopub.status.idle":"2022-06-11T08:17:08.290734Z","shell.execute_reply.started":"2022-06-11T08:14:42.005679Z","shell.execute_reply":"2022-06-11T08:17:08.289978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/bert-for-patents'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions3 = 0\npredictions4 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/bert-cv/bert_for_patents_BCE_fold_{fold}.pt'))\n    predictions3 += val_fn(model, valid_dl, mode = 'BCE')/4\n    model.load_state_dict(torch.load(f'../input/bert-cv/bert_for_patents_MSE_fold_{fold}.pt'))\n    predictions4 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:19:21.711608Z","iopub.execute_input":"2022-06-11T08:19:21.711973Z","iopub.status.idle":"2022-06-11T08:21:22.836702Z","shell.execute_reply.started":"2022-06-11T08:19:21.711943Z","shell.execute_reply":"2022-06-11T08:21:22.835806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/deberta-v3-base/deberta-v3-base'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions5 = 0\npredictions6 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/deberta-base-cv/deberta_v3_base_BCE_fold_{fold}.pt'))\n    predictions5 += val_fn(model, valid_dl, mode = 'BCE')/4\n    model.load_state_dict(torch.load(f'../input/deberta-base-cv/deberta_v3_base_MSE_fold_{fold}.pt'))\n    predictions6 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:28:26.6069Z","iopub.execute_input":"2022-06-11T08:28:26.607296Z","iopub.status.idle":"2022-06-11T08:29:30.611801Z","shell.execute_reply.started":"2022-06-11T08:28:26.607266Z","shell.execute_reply":"2022-06-11T08:29:30.611112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/deberta-v3-small/deberta-v3-small'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions7 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/deberta-small-cv/deberta_v3_small_MSE_fold_{fold}.pt'))\n    predictions7 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:33:08.355982Z","iopub.execute_input":"2022-06-11T08:33:08.356506Z","iopub.status.idle":"2022-06-11T08:33:41.093268Z","shell.execute_reply.started":"2022-06-11T08:33:08.356471Z","shell.execute_reply":"2022-06-11T08:33:41.092394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/ernie-large'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions8 = 0\npredictions9 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/ernie-large-cv/ernie_2.0_large_BCE_fold_{fold}.pt'))\n    predictions8 += val_fn(model, valid_dl, mode = 'BCE')/4\n    model.load_state_dict(torch.load(f'../input/ernie-large-cv/ernie_2.0_large_MSE_fold_{fold}.pt'))\n    predictions9 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:35:51.636089Z","iopub.execute_input":"2022-06-11T08:35:51.636473Z","iopub.status.idle":"2022-06-11T08:36:33.127876Z","shell.execute_reply.started":"2022-06-11T08:35:51.636443Z","shell.execute_reply":"2022-06-11T08:36:33.127041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/funnel-large'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions10 = 0\npredictions11 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/funnel-cv/funnel_large_BCE_fold_{fold}.pt'))\n    predictions10 += val_fn(model, valid_dl, mode = 'BCE')/4\n    model.load_state_dict(torch.load(f'../input/funnel-cv/funnel_large_MSE_fold_{fold}.pt'))\n    predictions11 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions12 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/roberta-large-cv/roberta_large_BCE_fold_{fold}.pt'))\n    predictions12 += val_fn(model, valid_dl, mode = 'BCE')/4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCONFIG.MODEL_PATH = '../input/electra-large-discriminator'\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_PATH)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCONFIG.TOKENIZER = tokenizer\n\n# ====================================================\n# Define max_len\n# ====================================================\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCONFIG.MAX_LENGTH = max(lengths_dict['anchor']) + max(lengths_dict['target']) \\\n              + max(lengths_dict['context_text']) + 4  # CLS + SEP + SEP + SEP\nLOGGER.info(f\"max_len: {CONFIG.MAX_LENGTH}\")\n\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text'].values.tolist()\n        self.tokenizer = CONFIG.TOKENIZER\n        self.max_length = CONFIG.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }\n\n\n\nclass Model(nn.Module):\n    def __init__(self, training_mode = 'MSE'):\n        super(Model, self).__init__()\n\n        self.bert = AutoModelForSequenceClassification.from_pretrained(CONFIG.MODEL_PATH, num_labels=1)\n\n        if training_mode == 'MSE':\n            self.loss = nn.MSELoss(reduction=\"mean\")\n        else:\n            self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, input_ids, attention_mask, targets = None):\n        hidden = self.bert(input_ids=input_ids,\n                           attention_mask=attention_mask)\n        loss = 0\n        if targets is not None:\n            loss = self.loss(hidden.logits.reshape(-1,1), targets.reshape(-1,1))\n            return hidden.logits, loss\n\n        return hidden.logits, loss\n\ndef val_fn(model, valid_dataloader, mode = 'MSE'):\n    val_loss = 0\n    model.eval()\n    preds = []\n    for step, batch in tqdm(enumerate(valid_dataloader),\n                            total=len(valid_dataloader),\n                            desc='validing'):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        with torch.no_grad():\n            y_preds, loss = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            if mode == 'MSE':\n                preds.append(y_preds.reshape(-1,).to('cpu').numpy())\n            else:\n                preds.append(y_preds.reshape(-1,).sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\nvalid_ds = TrainDataset(test)\nvalid_dl = DataLoader(valid_ds, batch_size=CONFIG.BATCH_SIZE*10)\ntorch.manual_seed(CONFIG.SEED)\n\npredictions13 = 0\nfor fold in range(4):\n    model = Model()\n    model = model.to(device)\n    model.load_state_dict(torch.load(f'../input/electra-large-cv/electra_large_MSE_fold_{fold}.pt'))\n    predictions13 += val_fn(model, valid_dl, mode = 'MSE')/4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred  = 0.07809745*predictions1 \npred += 0.12919135*predictions2\npred += 0.19441621*predictions3 \npred += 0.09985616*predictions4 \npred += 0.01198952*predictions5 \npred += 0.01414211*predictions6 \n# pred += 0.00263129*predictions7 \npred += 0.05200046*predictions8 \npred += 0.01081633*predictions9 \npred += 0.08326132*predictions10\npred += 0.04504323*predictions11 \n# pred += 0.01514399*predictions12 \npred += 0.10742007*predictions13\n\nfinal_predictions_1 = pred\n\n'''print(pred[:5])\n\nsubmission['score'] = pred\nsubmission[['id', 'score']].to_csv('submission.csv', index=False)'''","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:46:06.470281Z","iopub.execute_input":"2022-06-11T08:46:06.470634Z","iopub.status.idle":"2022-06-11T08:46:06.485405Z","shell.execute_reply.started":"2022-06-11T08:46:06.470605Z","shell.execute_reply":"2022-06-11T08:46:06.484389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pred 2 CV:0.8839","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\nimport sys\nINPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:34:48.816026Z","iopub.execute_input":"2022-06-15T08:34:48.816405Z","iopub.status.idle":"2022-06-15T08:34:48.821327Z","shell.execute_reply.started":"2022-06-15T08:34:48.816368Z","shell.execute_reply":"2022-06-15T08:34:48.820501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport time\nimport random\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as sp\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\n\nfrom transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForTokenClassification\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:34:48.927828Z","iopub.execute_input":"2022-06-15T08:34:48.928598Z","iopub.status.idle":"2022-06-15T08:35:12.059513Z","shell.execute_reply.started":"2022-06-15T08:34:48.928566Z","shell.execute_reply":"2022-06-15T08:35:12.058639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.06148Z","iopub.execute_input":"2022-06-15T08:35:12.061856Z","iopub.status.idle":"2022-06-15T08:35:12.070894Z","shell.execute_reply.started":"2022-06-15T08:35:12.061819Z","shell.execute_reply":"2022-06-15T08:35:12.070126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv(INPUT_DIR+'test.csv')\nsubmission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\nprint(f\"test.shape: {test.shape}\")\nprint(f\"submission.shape: {submission.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.07234Z","iopub.execute_input":"2022-06-15T08:35:12.072928Z","iopub.status.idle":"2022-06-15T08:35:12.092388Z","shell.execute_reply.started":"2022-06-15T08:35:12.07289Z","shell.execute_reply":"2022-06-15T08:35:12.091609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CPC Data\n# ====================================================\ncpc_texts = torch.load('../input/pppm-debertav3large-baseline/cpc_texts.pth')\ntest['context_text'] = test['context'].map(cpc_texts)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.094843Z","iopub.execute_input":"2022-06-15T08:35:12.095781Z","iopub.status.idle":"2022-06-15T08:35:12.103825Z","shell.execute_reply.started":"2022-06-15T08:35:12.095742Z","shell.execute_reply":"2022-06-15T08:35:12.103009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.105199Z","iopub.execute_input":"2022-06-15T08:35:12.106066Z","iopub.status.idle":"2022-06-15T08:35:12.112612Z","shell.execute_reply.started":"2022-06-15T08:35:12.106028Z","shell.execute_reply":"2022-06-15T08:35:12.111846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.114017Z","iopub.execute_input":"2022-06-15T08:35:12.114826Z","iopub.status.idle":"2022-06-15T08:35:12.123469Z","shell.execute_reply.started":"2022-06-15T08:35:12.114788Z","shell.execute_reply":"2022-06-15T08:35:12.122434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.124901Z","iopub.execute_input":"2022-06-15T08:35:12.125341Z","iopub.status.idle":"2022-06-15T08:35:12.135484Z","shell.execute_reply.started":"2022-06-15T08:35:12.125304Z","shell.execute_reply":"2022-06-15T08:35:12.134603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference(MSE)\n# ====================================================\ndef inference_fn_mse(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.137043Z","iopub.execute_input":"2022-06-15T08:35:12.137472Z","iopub.status.idle":"2022-06-15T08:35:12.14457Z","shell.execute_reply.started":"2022-06-15T08:35:12.137428Z","shell.execute_reply":"2022-06-15T08:35:12.143611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.145938Z","iopub.execute_input":"2022-06-15T08:35:12.146524Z","iopub.status.idle":"2022-06-15T08:35:12.162784Z","shell.execute_reply.started":"2022-06-15T08:35:12.146486Z","shell.execute_reply":"2022-06-15T08:35:12.161787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerHead(nn.Module):\n    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n        super().__init__()\n\n        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_features,\n                                                                                          nhead=nhead),\n                                                 num_layers=num_layers)\n        self.row_fc = nn.Linear(in_features, 1)\n        self.out_features = max_length\n\n    def forward(self, x):\n        out = self.transformer(x)\n        out = self.row_fc(out).squeeze(-1)\n        return out\n\n\n# ====================================================\n# Model\n# ====================================================\nclass Th(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n\n        self.feature_extractor = AutoModelForTokenClassification.from_pretrained(cfg.model)\n        in_features = self.feature_extractor.classifier.in_features\n        self.attention = TransformerHead(in_features=in_features, max_length=cfg.max_len, num_layers=1, nhead=8, num_targets=1)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.attention.out_features, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self._init_weights(self.attention)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        feature = self.attention(last_hidden_states)\n\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        # print(feature.shape)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.166403Z","iopub.execute_input":"2022-06-15T08:35:12.166904Z","iopub.status.idle":"2022-06-15T08:35:12.183938Z","shell.execute_reply.started":"2022-06-15T08:35:12.166868Z","shell.execute_reply":"2022-06-15T08:35:12.183184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model(MSE)\n# ====================================================\nclass MSEModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            '''if num == 0:\n                self.model = AutoModel.from_pretrained(cfg.model0, config=self.config)\n            if num == 1:\n                self.model = AutoModel.from_pretrained(cfg.model1, config=self.config)\n            if num == 2:\n                self.model = AutoModel.from_pretrained(cfg.model2, config=self.config)\n            if num == 3:\n                self.model = AutoModel.from_pretrained(cfg.model3, config=self.config)'''\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # 取出mask\n        attention_mask = inputs['attention_mask']\n        attention_mask = attention_mask.unsqueeze(2)\n        # 将padding置为0\n        feature = last_hidden_states * attention_mask\n        # 平均只包含非padding部分\n        feature = torch.sum(feature, 1) / torch.sum(attention_mask, dim=1)\n        return feature\n\n\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.185256Z","iopub.execute_input":"2022-06-15T08:35:12.18578Z","iopub.status.idle":"2022-06-15T08:35:12.203885Z","shell.execute_reply.started":"2022-06-15T08:35:12.185742Z","shell.execute_reply":"2022-06-15T08:35:12.203087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass ELECTRAModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n\n        out = self.layer_norm1(out)\n        output = self.fc(out)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.205345Z","iopub.execute_input":"2022-06-15T08:35:12.20574Z","iopub.status.idle":"2022-06-15T08:35:12.223236Z","shell.execute_reply.started":"2022-06-15T08:35:12.205705Z","shell.execute_reply":"2022-06-15T08:35:12.222402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass LUKEModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = LukeConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = LukeModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = LukeModel(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.224524Z","iopub.execute_input":"2022-06-15T08:35:12.224893Z","iopub.status.idle":"2022-06-15T08:35:12.236373Z","shell.execute_reply.started":"2022-06-15T08:35:12.224858Z","shell.execute_reply":"2022-06-15T08:35:12.23557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass BARTModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = BartConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = BartModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = BartModel(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:35:12.238281Z","iopub.execute_input":"2022-06-15T08:35:12.238559Z","iopub.status.idle":"2022-06-15T08:35:12.249071Z","shell.execute_reply.started":"2022-06-15T08:35:12.238535Z","shell.execute_reply":"2022-06-15T08:35:12.247827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.deberta-v3-small-4folds transformer head","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmdebertasmalltransformer-head08440/USPPPM-deberta-small-transformer_head-0.8440/outputconfig.pth'\n    model=\"../input/deberta-v3-small/deberta-v3-small\"\n    tokenizer_path='../input/uspppmdebertaltransformer-head08624/outputtokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:27:50.40693Z","iopub.execute_input":"2022-06-15T08:27:50.407735Z","iopub.status.idle":"2022-06-15T08:27:50.413342Z","shell.execute_reply.started":"2022-06-15T08:27:50.407691Z","shell.execute_reply":"2022-06-15T08:27:50.412551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:27:50.861997Z","iopub.execute_input":"2022-06-15T08:27:50.862579Z","iopub.status.idle":"2022-06-15T08:27:51.544976Z","shell.execute_reply.started":"2022-06-15T08:27:50.862543Z","shell.execute_reply":"2022-06-15T08:27:51.544154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = Th(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f\"../input/uspppmdebertasmalltransformer-head08440/USPPPM-deberta-small-transformer_head-0.8440/output-tmp-Allen-Pycharm-input-deberta-v3-small_fold{fold}_best.pth\")\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:27:51.547082Z","iopub.execute_input":"2022-06-15T08:27:51.547645Z","iopub.status.idle":"2022-06-15T08:27:59.384185Z","shell.execute_reply.started":"2022-06-15T08:27:51.547602Z","shell.execute_reply":"2022-06-15T08:27:59.3827Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.bert for patents 4folds","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmbertforpatents/bert-for-patents/outputconfig.pth'\n    tokenizer_path='../input/uspppmbertforpatents/bert-for-patents/outputtokenizer/'\n    #model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=117\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:28:56.422298Z","iopub.execute_input":"2022-06-15T08:28:56.423135Z","iopub.status.idle":"2022-06-15T08:28:56.429746Z","shell.execute_reply.started":"2022-06-15T08:28:56.423094Z","shell.execute_reply":"2022-06-15T08:28:56.428796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:28:56.860699Z","iopub.execute_input":"2022-06-15T08:28:56.861067Z","iopub.status.idle":"2022-06-15T08:28:56.896065Z","shell.execute_reply.started":"2022-06-15T08:28:56.861038Z","shell.execute_reply":"2022-06-15T08:28:56.895156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f\"../input/uspppmbertforpatents/bert-for-patents/output-tmp-pycharm_project_926-input-bert-for-patents_fold{fold}_best.pth\")\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred3 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:28:57.481113Z","iopub.execute_input":"2022-06-15T08:28:57.481865Z","iopub.status.idle":"2022-06-15T08:29:07.004559Z","shell.execute_reply.started":"2022-06-15T08:28:57.481827Z","shell.execute_reply":"2022-06-15T08:29:07.003111Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.Deberta large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmdebertav3large4foldscv08612/uspppm-deberta-large-4folds-0.8612/outputconfig.pth'\n    #model=\"microsoft/deberta-v3-large\"\n    tokenizer_path='../input/uspppmdebertaltransformer-head08624/outputtokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:29:28.943793Z","iopub.execute_input":"2022-06-15T08:29:28.944188Z","iopub.status.idle":"2022-06-15T08:29:28.949681Z","shell.execute_reply.started":"2022-06-15T08:29:28.944152Z","shell.execute_reply":"2022-06-15T08:29:28.948805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:29:29.363658Z","iopub.execute_input":"2022-06-15T08:29:29.364388Z","iopub.status.idle":"2022-06-15T08:29:30.042694Z","shell.execute_reply.started":"2022-06-15T08:29:29.36435Z","shell.execute_reply":"2022-06-15T08:29:30.041862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmdebertav3large4foldscv08612/uspppm-deberta-large-4folds-0.8612/output-tmp-Allen-Pycharm-input-deberta-v3-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred4 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:29:30.044322Z","iopub.execute_input":"2022-06-15T08:29:30.044718Z","iopub.status.idle":"2022-06-15T08:30:03.272112Z","shell.execute_reply.started":"2022-06-15T08:29:30.044681Z","shell.execute_reply":"2022-06-15T08:30:03.270824Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Deberta-large-transformer_head","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmdebertaltransformer-head08624/outputconfig.pth'\n    model='../input/deberta-v3-large/deberta-v3-large'\n    tokenizer_path='../input/uspppmdebertaltransformer-head08624/outputtokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:30:27.291252Z","iopub.execute_input":"2022-06-15T08:30:27.291626Z","iopub.status.idle":"2022-06-15T08:30:27.297675Z","shell.execute_reply.started":"2022-06-15T08:30:27.291591Z","shell.execute_reply":"2022-06-15T08:30:27.296512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:30:27.586623Z","iopub.execute_input":"2022-06-15T08:30:27.586966Z","iopub.status.idle":"2022-06-15T08:30:28.263585Z","shell.execute_reply.started":"2022-06-15T08:30:27.58692Z","shell.execute_reply":"2022-06-15T08:30:28.26277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = Th(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmdebertaltransformer-head08624/USPPPM-deberta-large-transformer_head-0.8624/output-tmp-Allen-Pycharm-input-deberta-v3-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred6 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:30:28.26494Z","iopub.execute_input":"2022-06-15T08:30:28.265312Z","iopub.status.idle":"2022-06-15T08:31:33.273337Z","shell.execute_reply.started":"2022-06-15T08:30:28.265276Z","shell.execute_reply":"2022-06-15T08:31:33.272216Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.deberta-base-transformer_head-0.8503","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmdebertabasetransformer-head08503/USPPPM-deberta-base-transformer_head-0.8503/outputconfig.pth'\n    model='../input/deberta-v3-base/deberta-v3-base'\n    tokenizer_path='../input/uspppmdebertaltransformer-head08624/outputtokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:31:38.41232Z","iopub.execute_input":"2022-06-15T08:31:38.412838Z","iopub.status.idle":"2022-06-15T08:31:38.419181Z","shell.execute_reply.started":"2022-06-15T08:31:38.412801Z","shell.execute_reply":"2022-06-15T08:31:38.418232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:31:38.860827Z","iopub.execute_input":"2022-06-15T08:31:38.861567Z","iopub.status.idle":"2022-06-15T08:31:39.551999Z","shell.execute_reply.started":"2022-06-15T08:31:38.861532Z","shell.execute_reply":"2022-06-15T08:31:39.551196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = Th(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmdebertabasetransformer-head08503/USPPPM-deberta-base-transformer_head-0.8503/output-tmp-Allen-Pycharm-input-deberta-v3-base_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred7 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:31:57.697964Z","iopub.execute_input":"2022-06-15T08:31:57.69882Z","iopub.status.idle":"2022-06-15T08:32:28.618983Z","shell.execute_reply.started":"2022-06-15T08:31:57.698783Z","shell.execute_reply":"2022-06-15T08:32:28.617792Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8.bert for patents Transformer_head-0.8493","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/uspppmberttransformer-head08493/USPPPM-bert-for-patents-transformer_head--0.8493/outputconfig.pth\"\n    config_path='../input/uspppmberttransformer-head08493/USPPPM-bert-for-patents-transformer_head--0.8493/outputconfig.pth'\n    model='../input/bert-for-patents'\n    tokenizer_path='../input/uspppmberttransformer-head08493/USPPPM-bert-for-patents-transformer_head--0.8493/outputtokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=117\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:36:52.216497Z","iopub.execute_input":"2022-06-15T08:36:52.216852Z","iopub.status.idle":"2022-06-15T08:36:52.222651Z","shell.execute_reply.started":"2022-06-15T08:36:52.216822Z","shell.execute_reply":"2022-06-15T08:36:52.22153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:36:53.222121Z","iopub.execute_input":"2022-06-15T08:36:53.222915Z","iopub.status.idle":"2022-06-15T08:36:53.257477Z","shell.execute_reply.started":"2022-06-15T08:36:53.222876Z","shell.execute_reply":"2022-06-15T08:36:53.256668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = Th(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmberttransformer-head08493/USPPPM-bert-for-patents-transformer_head--0.8493/output-tmp-Allen-Pycharm-input-bert-for-patents_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred8 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:36:54.036812Z","iopub.execute_input":"2022-06-15T08:36:54.037608Z","iopub.status.idle":"2022-06-15T08:37:56.708772Z","shell.execute_reply.started":"2022-06-15T08:36:54.037571Z","shell.execute_reply":"2022-06-15T08:37:56.705904Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10.Roberta large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmrobertalarge08406/roberta/config.pth'\n    #model=\"microsoft/deberta-v3-large\"\n    tokenizer_path='../input/uspppmrobertalarge08406/roberta/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:02.064073Z","iopub.execute_input":"2022-06-15T08:38:02.064895Z","iopub.status.idle":"2022-06-15T08:38:02.070444Z","shell.execute_reply.started":"2022-06-15T08:38:02.064853Z","shell.execute_reply":"2022-06-15T08:38:02.069618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:02.292714Z","iopub.execute_input":"2022-06-15T08:38:02.293374Z","iopub.status.idle":"2022-06-15T08:38:02.522118Z","shell.execute_reply.started":"2022-06-15T08:38:02.293335Z","shell.execute_reply":"2022-06-15T08:38:02.521321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmrobertalarge08406/roberta/-tmp-Allen-input-roberta-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred10 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:02.564805Z","iopub.execute_input":"2022-06-15T08:38:02.56513Z","iopub.status.idle":"2022-06-15T08:38:28.815419Z","shell.execute_reply.started":"2022-06-15T08:38:02.565103Z","shell.execute_reply":"2022-06-15T08:38:28.81211Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11.mnli-deberta-large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmmnlidebertalarge08504/USPPPM-mnli-deberta-large/config.pth'\n    #model=\"microsoft/deberta-v3-large\"\n    tokenizer_path='../input/uspppmmnlidebertalarge08504/USPPPM-mnli-deberta-large/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:32.139274Z","iopub.execute_input":"2022-06-15T08:38:32.139716Z","iopub.status.idle":"2022-06-15T08:38:32.146412Z","shell.execute_reply.started":"2022-06-15T08:38:32.139635Z","shell.execute_reply":"2022-06-15T08:38:32.145328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:32.38139Z","iopub.execute_input":"2022-06-15T08:38:32.381754Z","iopub.status.idle":"2022-06-15T08:38:32.555333Z","shell.execute_reply.started":"2022-06-15T08:38:32.381723Z","shell.execute_reply":"2022-06-15T08:38:32.554524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmmnlidebertalarge08504/USPPPM-mnli-deberta-large/-root-autodl-tmp-input-deberta-large-mnli_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred11 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:38:32.600485Z","iopub.execute_input":"2022-06-15T08:38:32.602318Z","iopub.status.idle":"2022-06-15T08:39:03.560333Z","shell.execute_reply.started":"2022-06-15T08:38:32.602287Z","shell.execute_reply":"2022-06-15T08:39:03.558817Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12.roberta-base","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmrobertabase08063/USPPPM-roberta-base/config.pth'\n    #model=\"microsoft/deberta-v3-large\"\n    tokenizer_path='../input/uspppmrobertabase08063/USPPPM-roberta-base/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:39:11.355796Z","iopub.execute_input":"2022-06-15T08:39:11.356308Z","iopub.status.idle":"2022-06-15T08:39:11.361931Z","shell.execute_reply.started":"2022-06-15T08:39:11.356261Z","shell.execute_reply":"2022-06-15T08:39:11.361133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:39:11.473629Z","iopub.execute_input":"2022-06-15T08:39:11.474211Z","iopub.status.idle":"2022-06-15T08:39:11.67256Z","shell.execute_reply.started":"2022-06-15T08:39:11.474179Z","shell.execute_reply":"2022-06-15T08:39:11.671745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmrobertabase08063/USPPPM-roberta-base/-root-autodl-tmp-input-roberta-base_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred12 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:39:11.674016Z","iopub.execute_input":"2022-06-15T08:39:11.674368Z","iopub.status.idle":"2022-06-15T08:39:22.83515Z","shell.execute_reply.started":"2022-06-15T08:39:11.674333Z","shell.execute_reply":"2022-06-15T08:39:22.833801Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13.deberta-mnli-th","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmmnlith08477/USPPPM-mnli-th-0.8477/config.pth'\n    model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmmnlith08477/USPPPM-mnli-th-0.8477/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:41:32.488252Z","iopub.execute_input":"2022-06-15T08:41:32.488683Z","iopub.status.idle":"2022-06-15T08:41:32.494562Z","shell.execute_reply.started":"2022-06-15T08:41:32.488645Z","shell.execute_reply":"2022-06-15T08:41:32.493482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:41:32.707588Z","iopub.execute_input":"2022-06-15T08:41:32.707874Z","iopub.status.idle":"2022-06-15T08:41:32.801384Z","shell.execute_reply.started":"2022-06-15T08:41:32.707849Z","shell.execute_reply":"2022-06-15T08:41:32.800569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = Th(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmmnlith08477/USPPPM-mnli-th-0.8477/-root-autodl-tmp-input-deberta-large-mnli_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred13 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:41:33.485412Z","iopub.execute_input":"2022-06-15T08:41:33.485759Z","iopub.status.idle":"2022-06-15T08:42:45.875565Z","shell.execute_reply.started":"2022-06-15T08:41:33.48573Z","shell.execute_reply":"2022-06-15T08:42:45.873744Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 16. MSE Deberta Large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmmsedebertav3large/MSE-deberta-large-0.8457/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmmsedebertav3large/MSE-deberta-large-0.8457/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:42:50.424285Z","iopub.execute_input":"2022-06-15T08:42:50.424685Z","iopub.status.idle":"2022-06-15T08:42:50.431016Z","shell.execute_reply.started":"2022-06-15T08:42:50.424648Z","shell.execute_reply":"2022-06-15T08:42:50.430026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:42:50.540805Z","iopub.execute_input":"2022-06-15T08:42:50.54154Z","iopub.status.idle":"2022-06-15T08:42:51.302817Z","shell.execute_reply.started":"2022-06-15T08:42:50.541507Z","shell.execute_reply":"2022-06-15T08:42:51.301997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = MSEModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmmsedebertav3large/MSE-deberta-large-0.8457/-tmp-Allen-input-deberta-v3-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn_mse(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred16 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:43:01.05431Z","iopub.execute_input":"2022-06-15T08:43:01.054681Z","iopub.status.idle":"2022-06-15T08:43:33.54357Z","shell.execute_reply.started":"2022-06-15T08:43:01.054652Z","shell.execute_reply":"2022-06-15T08:43:33.542104Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 18.patentSBERTa","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmpatentsberta08313/patentSBERTa/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmpatentsberta08313/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:28.41535Z","iopub.execute_input":"2022-06-15T08:45:28.415704Z","iopub.status.idle":"2022-06-15T08:45:28.420722Z","shell.execute_reply.started":"2022-06-15T08:45:28.415675Z","shell.execute_reply":"2022-06-15T08:45:28.419978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:28.577189Z","iopub.execute_input":"2022-06-15T08:45:28.577816Z","iopub.status.idle":"2022-06-15T08:45:28.64681Z","shell.execute_reply.started":"2022-06-15T08:45:28.577784Z","shell.execute_reply":"2022-06-15T08:45:28.645998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmpatentsberta08313/patentSBERTa/-root-autodl-tmp-input-PatentSBERTa_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred18 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:40.211472Z","iopub.execute_input":"2022-06-15T08:45:40.21182Z","iopub.status.idle":"2022-06-15T08:45:45.381497Z","shell.execute_reply.started":"2022-06-15T08:45:40.211791Z","shell.execute_reply":"2022-06-15T08:45:45.378325Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 19.Electra","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmelectralarge/electra-large-0.8452/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmelectralarge/electra-large-0.8452/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:48.515612Z","iopub.execute_input":"2022-06-15T08:45:48.516013Z","iopub.status.idle":"2022-06-15T08:45:48.521844Z","shell.execute_reply.started":"2022-06-15T08:45:48.515964Z","shell.execute_reply":"2022-06-15T08:45:48.520834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:48.627418Z","iopub.execute_input":"2022-06-15T08:45:48.627873Z","iopub.status.idle":"2022-06-15T08:45:48.700075Z","shell.execute_reply.started":"2022-06-15T08:45:48.627841Z","shell.execute_reply":"2022-06-15T08:45:48.699241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ELECTRAModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmelectralarge/electra-large-0.8452/-root-autodl-tmp-input-electra-large_fold{fold}_best.pth',\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred19 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:45:48.802182Z","iopub.execute_input":"2022-06-15T08:45:48.802579Z","iopub.status.idle":"2022-06-15T08:46:13.073845Z","shell.execute_reply.started":"2022-06-15T08:45:48.802549Z","shell.execute_reply":"2022-06-15T08:46:13.072314Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20. ernie","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmernielarge/ernie-large-ml=125/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmernielarge/ernie-large-ml=125/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:46:18.280893Z","iopub.execute_input":"2022-06-15T08:46:18.281721Z","iopub.status.idle":"2022-06-15T08:46:18.28743Z","shell.execute_reply.started":"2022-06-15T08:46:18.281674Z","shell.execute_reply":"2022-06-15T08:46:18.286456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:46:18.449167Z","iopub.execute_input":"2022-06-15T08:46:18.449467Z","iopub.status.idle":"2022-06-15T08:46:18.534105Z","shell.execute_reply.started":"2022-06-15T08:46:18.449442Z","shell.execute_reply":"2022-06-15T08:46:18.533328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ELECTRAModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmernielarge/ernie-large-ml=125/-root-autodl-tmp-input-ernie-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred20 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:46:38.302244Z","iopub.execute_input":"2022-06-15T08:46:38.303084Z","iopub.status.idle":"2022-06-15T08:46:51.348792Z","shell.execute_reply.started":"2022-06-15T08:46:38.303047Z","shell.execute_reply":"2022-06-15T08:46:51.347494Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 21.funnel","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmfunnel/funnel-ml=125/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmfunnel/funnel-ml=125/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:02.474066Z","iopub.execute_input":"2022-06-15T08:47:02.474865Z","iopub.status.idle":"2022-06-15T08:47:02.480338Z","shell.execute_reply.started":"2022-06-15T08:47:02.474823Z","shell.execute_reply":"2022-06-15T08:47:02.479289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:02.57653Z","iopub.execute_input":"2022-06-15T08:47:02.576936Z","iopub.status.idle":"2022-06-15T08:47:02.663734Z","shell.execute_reply.started":"2022-06-15T08:47:02.576904Z","shell.execute_reply":"2022-06-15T08:47:02.663001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ELECTRAModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmfunnel/funnel-ml=125/-root-autodl-tmp-input-funnel_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred21 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:10.411509Z","iopub.execute_input":"2022-06-15T08:47:10.411935Z","iopub.status.idle":"2022-06-15T08:47:39.469743Z","shell.execute_reply.started":"2022-06-15T08:47:10.411899Z","shell.execute_reply":"2022-06-15T08:47:39.468433Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 22.luke","metadata":{}},{"cell_type":"code","source":"from transformers import LukeTokenizer, LukeModel, LukeConfig\n\n# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmluke/luke-large-ml=175/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmluke/luke-large-ml=175/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:43.023606Z","iopub.execute_input":"2022-06-15T08:47:43.024002Z","iopub.status.idle":"2022-06-15T08:47:43.039666Z","shell.execute_reply.started":"2022-06-15T08:47:43.023941Z","shell.execute_reply":"2022-06-15T08:47:43.038875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = LukeTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:43.186364Z","iopub.execute_input":"2022-06-15T08:47:43.186654Z","iopub.status.idle":"2022-06-15T08:47:44.216329Z","shell.execute_reply.started":"2022-06-15T08:47:43.186628Z","shell.execute_reply":"2022-06-15T08:47:44.215264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = LUKEModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmluke/luke-large-ml=175/studio-ousia-luke-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred22 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:47:44.221014Z","iopub.execute_input":"2022-06-15T08:47:44.224162Z","iopub.status.idle":"2022-06-15T08:48:49.345004Z","shell.execute_reply.started":"2022-06-15T08:47:44.224115Z","shell.execute_reply":"2022-06-15T08:48:49.343624Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 23.bart large","metadata":{}},{"cell_type":"code","source":"from transformers import BartTokenizer, BartModel, BartConfig\n\n# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    config_path='../input/uspppmbartlarge/bart-ml=174/config.pth'\n    #model=\"../input/microsoft-deberta-large-mnli\"\n    tokenizer_path='../input/uspppmbartlarge/bart-ml=174/tokenizer'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=174\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:48:53.074827Z","iopub.execute_input":"2022-06-15T08:48:53.075541Z","iopub.status.idle":"2022-06-15T08:48:53.087003Z","shell.execute_reply.started":"2022-06-15T08:48:53.075499Z","shell.execute_reply":"2022-06-15T08:48:53.086177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.tokenizer = BartTokenizer.from_pretrained(CFG.tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:48:53.17877Z","iopub.execute_input":"2022-06-15T08:48:53.179139Z","iopub.status.idle":"2022-06-15T08:48:53.321976Z","shell.execute_reply.started":"2022-06-15T08:48:53.179107Z","shell.execute_reply":"2022-06-15T08:48:53.321158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = BARTModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/uspppmbartlarge/bart-ml=174/-root-autodl-tmp-input-bart-large_fold{fold}_best.pth')\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npred23 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:48:53.360407Z","iopub.execute_input":"2022-06-15T08:48:53.360747Z","iopub.status.idle":"2022-06-15T08:49:27.784571Z","shell.execute_reply.started":"2022-06-15T08:48:53.360719Z","shell.execute_reply":"2022-06-15T08:49:27.783424Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pred2 Ensemble","metadata":{}},{"cell_type":"code","source":"w1 = 0.03605261 # deberta-small-transformer-head\n#w2 = 0.01577283 # deberta-base\nw3 = 0.18962274 # bert for patents\nw4 = 0.11517471 # deberta-large\n#w5 = 0.01215114 # deberta-xsmall transformer_head\nw6 = 0.13769397 # deberta-large-transformer_head\nw7 = 0.01575894 # deberta-base-transformer_head\nw8 = 0.08336175 # bert for patents transformer_head\n#w9 = 0.01323456 # roberta-large-th\nw10 = 0.02831863 # roberta-large\nw11 = 0.05526383 # mnli-deberta-large\nw12 = 0.00992199 # roberta-base\nw13 = 0.02017179 # mnli-th\n#w14 = -0.03159355 # mse-deberta-small\n#w15 = 0.02173322 # mse-deberta-base\nw16 = 0.03500124 # mse-deberta-large\n#w17 = 0.00915298 # mse-bert\nw18 = 0.0057002 # patentSBERTa\nw19 = 0.06964565 # electra\nw20 = 0.03321846\nw21 = 0.07020563\nw22 = 0.02133232\nw23 = 0.01152879\n'''\n0.8838650207947062\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nMMscaler = MinMaxScaler()\n\npred1_mm = MMscaler.fit_transform(pred1.reshape(-1,1)).reshape(-1)\n#pred2_mm = MMscaler.fit_transform(pred2.reshape(-1,1)).reshape(-1)\npred3_mm = MMscaler.fit_transform(pred3.reshape(-1,1)).reshape(-1)\npred4_mm = MMscaler.fit_transform(pred4.reshape(-1,1)).reshape(-1)\n#pred5_mm = MMscaler.fit_transform(pred5.reshape(-1,1)).reshape(-1)\npred6_mm = MMscaler.fit_transform(pred6.reshape(-1,1)).reshape(-1)\npred7_mm = MMscaler.fit_transform(pred7.reshape(-1,1)).reshape(-1)\npred8_mm = MMscaler.fit_transform(pred8.reshape(-1,1)).reshape(-1)\n#pred9_mm = MMscaler.fit_transform(pred9.reshape(-1,1)).reshape(-1)\npred10_mm = MMscaler.fit_transform(pred10.reshape(-1,1)).reshape(-1)\npred11_mm = MMscaler.fit_transform(pred11.reshape(-1,1)).reshape(-1)\npred12_mm = MMscaler.fit_transform(pred12.reshape(-1,1)).reshape(-1)\npred13_mm = MMscaler.fit_transform(pred13.reshape(-1,1)).reshape(-1)\n#pred14_mm = MMscaler.fit_transform(pred14.reshape(-1,1)).reshape(-1)\n#pred15_mm = MMscaler.fit_transform(pred15.reshape(-1,1)).reshape(-1)\npred16_mm = MMscaler.fit_transform(pred16.reshape(-1,1)).reshape(-1)\n#pred17_mm = MMscaler.fit_transform(pred17.reshape(-1,1)).reshape(-1)\npred18_mm = MMscaler.fit_transform(pred18.reshape(-1,1)).reshape(-1)\npred19_mm = MMscaler.fit_transform(pred19.reshape(-1,1)).reshape(-1)\npred20_mm = MMscaler.fit_transform(pred20.reshape(-1,1)).reshape(-1)\npred21_mm = MMscaler.fit_transform(pred21.reshape(-1,1)).reshape(-1)\npred22_mm = MMscaler.fit_transform(pred22.reshape(-1,1)).reshape(-1)\npred23_mm = MMscaler.fit_transform(pred23.reshape(-1,1)).reshape(-1)\n\n\nfinal_predictions_2 =  pred1_mm * w1 + pred3_mm * w3 + pred4_mm * w4 + pred6_mm * w6 + pred7_mm * w7 + pred8_mm * w8 + pred10_mm * w10 + pred11_mm * w11 + pred12_mm * w12 + pred13_mm * w13 + pred16_mm * w16 + pred18_mm * w18 + pred19_mm * w19 + pred20_mm * w20 + pred21_mm * w21 + pred22_mm * w22 + pred23_mm * w23","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a1 = 0.10\na2 = 0.90\n\nfinal_predictions = a1 * final_predictions_1 + a2 * final_predictions_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"PATH = '../input/us-patent-phrase-to-phrase-matching'\nsub = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['score'] = final_predictions\nsub.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{},"execution_count":null,"outputs":[]}]}