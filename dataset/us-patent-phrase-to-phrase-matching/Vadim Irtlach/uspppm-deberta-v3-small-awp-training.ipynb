{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nGuide: link to guide<br>\n\nInference: link to inference","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:36.268172Z","iopub.execute_input":"2022-05-10T17:59:36.268779Z","iopub.status.idle":"2022-05-10T17:59:39.821838Z","shell.execute_reply.started":"2022-05-10T17:59:36.268663Z","shell.execute_reply":"2022-05-10T17:59:39.82101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\nsys.path.append(\"../input/mixout-github-code/mixout\")\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import GradScaler, autocast\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration, Timer, Averager\nfrom torch_components.callbacks import EarlyStopping, ModelCheckpoint\nfrom torch_components.utils import seed_everything, get_lr, get_optimizer, get_scheduler\nfrom torch_components.import_utils import wandb_run_exists\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom mixout import MixLinear, Mixout\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display\nfrom datetime import timedelta\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport wandb\nimport os\nimport shutil\nimport gc\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"EXPERIMENT_NAME\"] = \"microsoft/deberta-v3-small awp (eps=1e-2)\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nWANDB = True\nDEBUG = True\nIS_KAGGLE_ENVIRONMENT = True\n\n\nif IS_KAGGLE_ENVIRONMENT:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n\nif WANDB:\n    os.environ[\"WANDB_PROJECT\"] = \"uspppm\"\n    os.environ[\"WANDB_ENTITY\"] = \"uspppm\"\n    os.environ[\"WANDB_SILENT\"] = \"true\"\n    \n    \n    if IS_KAGGLE_ENVIRONMENT:\n        wandb_secret_name = \"wandb_api_key\"\n        wandb_key = user_secrets.get_secret(wandb_secret_name)\n    else:\n        wandb_key = ... # set your personal Weights & Biases API key/token.\n        \n    wandb.login(key=wandb_key)\n    \n    \nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T17:59:39.825677Z","iopub.execute_input":"2022-05-10T17:59:39.825918Z","iopub.status.idle":"2022-05-10T17:59:46.65928Z","shell.execute_reply.started":"2022-05-10T17:59:39.825891Z","shell.execute_reply":"2022-05-10T17:59:46.658494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"config = Configuration(model=dict(model_path=\"microsoft/deberta-v3-small\"),\n                       optimizer=dict(name=\"AdamW\", parameters=dict(lr=1e-5, weight_decay=0.0)),\n                       scheduler=dict(name=\"get_cosine_with_hard_restarts_schedule_with_warmup\", parameters=dict(num_cycles=2, last_epoch=-1)),\n                       warmup=0.1,\n                       scheduling_after=\"step\",\n                       seed=42,\n                       max_length=75,\n                       batch_size=32,\n                       epochs=10,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=5,\n                       validation_steps=200, \n                       gradient_accumulation_steps=1,\n                       gradient_norm=1.0,\n                       gradient_scaling=True,\n                       delta=1e-4,\n                       verbose=200,\n                       save_model=False,\n                       device=DEVICE,\n                       output_directory=\"./\",\n                       cv_monitor_value=\"pearson\",\n                       amp=True, \n                       debug=True,\n                       decimals=4)\n\n\nconfig.seed = seed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.660762Z","iopub.execute_input":"2022-05-10T17:59:46.661014Z","iopub.status.idle":"2022-05-10T17:59:46.673747Z","shell.execute_reply.started":"2022-05-10T17:59:46.660967Z","shell.execute_reply":"2022-05-10T17:59:46.673083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def make_directory(directory, overwriting=False):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    else:\n        if overwriting:\n            shutil.rmtree(directory)\n            os.mkdir(directory)\n\n            \ndef create_folds(data_frame, targets, groups, folds=5, seed=42, shuffle=True, fold_column=\"fold\"):\n    cv_strategy = StratifiedGroupKFold(n_splits=folds, random_state=seed, shuffle=shuffle)\n    folds = cv_strategy.split(X=data_frame, y=targets, groups=groups)\n    for fold, (train_indexes, validation_indexes) in enumerate(folds):\n        data_frame.loc[validation_indexes, fold_column] =  int(fold+1)\n        \n    data_frame[fold_column] = data_frame[fold_column].astype(int)\n    \n    return data_frame","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.675367Z","iopub.execute_input":"2022-05-10T17:59:46.678851Z","iopub.status.idle":"2022-05-10T17:59:46.686641Z","shell.execute_reply.started":"2022-05-10T17:59:46.678823Z","shell.execute_reply":"2022-05-10T17:59:46.685925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_loop(train_loader, \n                  model,\n                  optimizer,\n                  scheduler=None,\n                  scheduling_after=\"step\",\n                  epochs=1,\n                  validation_loader=None, \n                  gradient_accumulation_steps=1, \n                  gradient_scaling=False,\n                  gradient_norm=1,\n                  validation_steps=100, \n                  amp=False,\n                  recalculate_metrics_at_end=True, \n                  return_validation_outputs=True,\n                  debug=True, \n                  verbose=1, \n                  device=\"cpu\", \n                  time_format=\"{hours}:{minutes}:{seconds}\", \n                  logger=[\"print\", \"wandb\"], \n                  decimals=4):\n    \n    training_steps = len(train_loader) * epochs\n    \n    if isinstance(validation_steps, float):\n        validation_steps = int(training_steps * validation_steps)\n    elif validation_steps == \"epoch\":\n        validation_steps = len(train_loader)\n    \n    scaler = GradScaler() if gradient_scaling else None\n    \n    if debug:\n        print(f\"Auto Mixed Precision: {amp}\")\n        print(f\"Gradient norm: {gradient_norm}\")\n        print(f\"Gradient scaling: {gradient_scaling}\")\n        print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n        print(f\"Validation steps: {validation_steps}\")\n        print(f\"Device: {device}\")\n        print()\n        \n    if wandb_run_exists() and \"wandb\" in logger:\n        print(f\"Weights & Biases Run: {wandb.run.get_url()}\", end=\"\\n\"*2)\n        \n    passed_steps = 1\n    train_loss, train_metrics = Averager(), Averager()\n    best_validation_loss, best_validation_metrics, best_validation_outputs = None, None, None\n    \n    if device is not None:\n        model.to(device)\n    \n    model.zero_grad()\n    total_time = timedelta(seconds=0)\n    for epoch in range(1, epochs+1):\n        if \"tqdm\" in logger:\n            train_loader = tqdm(iterable=train_loader, \n                                total=len(train_loader),\n                                colour=\"#000\",\n                                bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n            train_loader.set_description_str(f\"Epoch {epoch}/{epochs}\")\n        \n        if \"print\" in logger:\n            print(f\"\\nEpoch {epoch}/{epochs}\", end=\"\\n\"*2)\n            \n        epoch_train_loss, epoch_train_metrics = Averager(), Averager()\n            \n        timer = Timer(time_format)\n        steps = len(train_loader)\n        for step, batch in enumerate(train_loader, 1):\n            batch_size = len(batch)\n            batch_loss, batch_metrics = training_step(batch=batch, \n                                                      model=model, \n                                                      optimizer=optimizer,\n                                                      gradient_norm=gradient_norm,\n                                                      gradient_accumulation_steps=gradient_accumulation_steps, \n                                                      amp=amp, \n                                                      scaler=scaler, \n                                                      device=device, \n                                                      overall_loss=epoch_train_loss.average, \n                                                      overall_metrics=epoch_train_metrics.average,\n                                                      step=passed_steps, \n                                                      epoch=epoch)\n            \n            train_loss.update(batch_loss, n=batch_size)\n            epoch_train_loss.update(batch_loss, n=batch_size)\n            train_metrics.update(batch_metrics, n=batch_size)\n            epoch_train_metrics.update(batch_metrics, n=batch_size)\n            \n            if (step % gradient_accumulation_steps) == 0 or step == steps:\n                optimization_step(model=model, optimizer=optimizer, scaler=scaler)\n                \n\n            lr = get_lr(optimizer, only_last=True)\n            if scheduling_after == \"step\":\n                scheduling_step(scheduler, loop=\"training\")\n            \n                \n            logs = {\"train/loss\": train_loss.average, \n                    \"train/loss vs batch\": batch_loss, \n                    \"train/loss vs epoch\": epoch_train_loss.average,\n                    \"lr\": lr}\n            \n            for metric in batch_metrics:\n                metric = metric.strip().lower()\n                logs.update({f\"train/{metric}\": train_metrics.average[metric], \n                             f\"train/{metric} vs batch\": batch_metrics[metric], \n                             f\"train/{metric} vs epoch\": epoch_train_metrics.average[metric]})\n                \n            if wandb_run_exists() and \"wandb\" in logger:\n                wandb.log(logs, step=passed_steps) \n            \n            if \"tqdm\" in logger:\n                train_loader.set_postfix_str(f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                                             f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n            if \"print\" in logger:\n                 if step % verbose == 0 or step == steps:\n                    elapsed, remain = timer(step/steps)\n                    print(f\"{step}/{steps} - \"\n                          f\"remain: {remain} - \"\n                          f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                          f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)} - \"\n                          f\"lr: {lr}\")\n                    \n            \n            if validation_loader is not None:\n                if (passed_steps % validation_steps) == 0:\n                    if step > validation_steps: print()\n                        \n                    validation_loss, validation_metrics, validation_outputs = validation_loop(loader=validation_loader, \n                                                                                              model=model, \n                                                                                              gradient_accumulation_steps=gradient_accumulation_steps,\n                                                                                              amp=amp, \n                                                                                              return_outputs=True, \n                                                                                              verbose=verbose, \n                                                                                              recalculate_metrics_at_end=True, \n                                                                                              device=device, \n                                                                                              logger=logger)\n                    \n                    \n                    \n                    logs = {\"validation/loss\": validation_loss, \n                            \"train/loss vs validation steps\": epoch_train_loss.average}\n    \n                    for metric, value in validation_metrics.items():\n                        metric = metric.strip().lower()\n                        logs.update({f\"validation/{metric}\": value, \n                                     f\"train/{metric} vs validation steps\": epoch_train_metrics.average[metric]})\n                    \n                    if wandb_run_exists() and \"wandb\" in logger:\n                        wandb.log(logs, step=passed_steps)\n                    \n                    is_checkpoint_saved = model_checkpointing(loss=validation_loss, \n                                                              metrics=validation_metrics,\n                                                              model=model, \n                                                              optimizer=optimizer, \n                                                              scheduler=scheduler, \n                                                              step=passed_steps, \n                                                              best_loss=best_validation_loss, \n                                                              best_metrics=validation_metrics)\n                    \n                    if is_checkpoint_saved:\n                        best_validation_loss = validation_loss\n                        best_validation_metrics = validation_metrics\n                        best_validation_outputs = validation_outputs\n                    \n                    \n                    scheduling_step(scheduler, loss=validation_loss, loop=\"validation\")\n                    \n                    print()\n            \n            passed_steps += 1\n        \n        if scheduling_after == \"epoch\":\n            scheduling_step(scheduler, loop=\"training\")\n            \n        if \"tqdm\" in logger and \"print\" not in logger:\n            elapsed, remain = timer(1/1)\n        \n        epoch_elapsed_seconds = timer.elapsed_time.total_seconds()\n        total_time += timedelta(seconds=epoch_elapsed_seconds)\n        \n        if wandb_run_exists() and \"wandb\" in logger:\n            wandb.log({\"epoch\": epoch}, step=passed_steps)\n        \n        if \"tqdm\" in logger:\n            train_loader.close()\n            \n        print(f\"\\nTraining loss: {epoch_train_loss.average:.{decimals}}{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n        if validation_loader is not None:\n            print(f\"Validation loss: {best_validation_loss:.{decimals}}{format_metrics(best_validation_metrics, decimals=decimals)}\")\n            \n        print(f\"Total time: {Timer.format_time(total_time, time_format=time_format)}\")\n        \n        \n    \n    if validation_loader is not None:\n        if return_validation_outputs:\n            return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics, best_validation_outputs)\n\n        return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics)\n\n    return (epoch_train_loss.average, epoch_train_metrics.average)\n    \n\n    \ndef validation_loop(loader, \n                    model, \n                    gradient_accumulation_steps=1,\n                    amp=False, \n                    return_outputs=True, \n                    recalculate_metrics_at_end=True, \n                    verbose=1, \n                    device=\"cpu\", \n                    time_format=\"{hours}:{minutes}:{seconds}\",\n                    logger=[\"print\"], \n                    decimals=4):\n    \n    model.eval()\n    loss, metrics = Averager(), Averager()\n    timer = Timer(time_format)\n    outputs, targets = [], []\n    steps = len(loader)\n    \n    if \"tqdm\" in logger:\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n        loader.set_description_str(\"[Validation]\")\n    \n    is_targets = False\n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_loss, batch_outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n                \n                batch_loss /= gradient_accumulation_steps\n                loss.update(batch_loss.item(), n=len(batch))\n                \n                batch_targets = get_targets(batch)\n                batch_metrics = calculate_metrics(predictions=batch_outputs, targets=batch_targets, device=device)\n                metrics.update(batch_metrics, n=len(batch))\n                \n                if batch_targets is not None:\n                    if isinstance(batch_targets, dict):\n                        targets.append(batch_targets)\n                    else:\n                        targets.extend(batch_targets.to(\"cpu\").tolist())\n                        \n                    is_targets = True\n                \n                outputs.extend(batch_outputs.to(\"cpu\").tolist())\n                \n                \n                if step == steps and recalculate_metrics_at_end and is_targets:\n                    outputs = torch.tensor(outputs)\n                    targets = torch.tensor(targets)\n                        \n                    metrics = Averager(calculate_metrics(predictions=outputs, targets=targets))\n                \n                if \"tqdm\" in logger:\n                    loader.set_postfix_str(f\"loss: {loss.average:.{decimals}}\"\n                                           f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                \n                if \"print\" in logger:\n                    if step % verbose == 0 or step == steps:\n                        elapsed, remain = timer(step/steps)\n\n                        print(f\"[Validation] \"\n                              f\"{step}/{steps} - \"\n                              f\"remain: {remain} - \"\n                              f\"loss: {loss.average:.{decimals}}\"\n                              f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                    \n    if not recalculate_metrics_at_end: \n        outputs = torch.tensor(outputs)\n        \n    if \"tqdm\" in logger:\n        loader.close()\n        \n    return (loss.average, metrics.average, outputs) if return_outputs else (loss.average, metrics.average)\n\n\ndef format_metrics(metrics, sep=\" - \", add_sep_to_start=True, decimals=4):\n    if metrics != {}:\n        string = sep.join([f\"{k.strip().lower()}: {v:.{decimals}}\" for k, v in metrics.items()])\n        return sep + string if add_sep_to_start else string \n    \n    return \"\"\n\n    \ndef training_step(batch, \n                  model, \n                  optimizer, \n                  gradient_norm=1.0, \n                  amp=False, \n                  gradient_accumulation_steps=1, \n                  scaler=None, \n                  device=\"cpu\", \n                  overall_loss=None, \n                  overall_metrics=None, \n                  step=None, \n                  epoch=None):\n    \n    model.train()\n    with autocast(enabled=amp):\n        loss, outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n        targets = get_targets(batch)\n        metrics = calculate_metrics(predictions=outputs, targets=targets, device=device)\n        \n        loss /= gradient_accumulation_steps\n        loss = backward_step(loss=loss, optimizer=optimizer, scaler=scaler)\n        \n        adversarial_loss = adversarial_step(batch=batch, \n                                            model=model, \n                                            device=device, \n                                            loss=overall_loss, \n                                            metrics=overall_metrics, \n                                            step=step, \n                                            epoch=epoch)\n        \n        if adversarial_loss is not None:\n            adversarial_loss = backward_step(loss=adversarial_loss, optimizer=optimizer, scaler=scaler)\n            \n    if gradient_norm > 0:\n        if scaler is not None:\n            scaler.unscale_(optimizer)\n                            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_norm)\n        \n    return loss.detach(), metrics\n\ndef backward_step(loss, optimizer, scaler=None):\n    if scaler is not None:\n        scaler.scale(loss).backward()\n    else:\n        loss.backward()\n        \n    return loss\n        \n\ndef optimization_step(model, optimizer, scaler=None):                        \n    if scaler is not None:\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        optimizer.step()\n        \n    model.zero_grad()\n        \n\ndef scheduling_step(scheduler=None, loss=None, loop=\"training\"):\n    if scheduler is not None:\n        if loop == \"validation\":\n            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(loss)\n        else:\n            if not isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step()\n\n                \ndef adversarial_step(batch, \n                     model, \n                     device=\"cpu\", \n                     loss=None, \n                     metrics=None, \n                     step=None, \n                     epoch=None):\n    \n    pass\n\n                \n    \ndef calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    raise NotImplementedError(f\"`calculate_loss` function is not implemented.\")\n                \ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    return dict()\n\ndef get_targets(batch):\n    return []\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    return True","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.688519Z","iopub.execute_input":"2022-05-10T17:59:46.689071Z","iopub.status.idle":"2022-05-10T17:59:46.74669Z","shell.execute_reply.started":"2022-05-10T17:59:46.689012Z","shell.execute_reply":"2022-05-10T17:59:46.746072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    input_ids, attention_mask, targets = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    targets = targets.to(device).float()\n    \n    outputs = model(input_ids, attention_mask)\n    outputs = outputs.sigmoid().squeeze(dim=-1)\n    loss = F.mse_loss(outputs, targets, reduction=\"mean\")\n    \n    return (loss, outputs) if return_outputs else loss\n\n\ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    predictions = predictions.sigmoid().detach().view(-1).to(\"cpu\").float().numpy()\n    targets = targets.view(-1).to(\"cpu\").float().numpy()\n    \n    return dict(pearson=scipy.stats.pearsonr(predictions, targets)[0])\n\n\ndef get_targets(batch):\n    *_, targets = batch\n    return targets\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    is_saved_checkpoint = model_checkpoint(value=metrics[\"pearson\"], \n                                           model=model, \n                                           optimizer=optimizer, \n                                           scheduler=scheduler, \n                                           step=step)\n    return is_saved_checkpoint\n\n\ndef adversarial_step(batch, \n                     model, \n                     device=\"cpu\", \n                     loss=None, \n                     metrics=None, \n                     step=None, \n                     epoch=None):\n    \n    loss = awp_model.attack_backward(batch, epoch=epoch)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.747661Z","iopub.execute_input":"2022-05-10T17:59:46.747892Z","iopub.status.idle":"2022-05-10T17:59:46.761923Z","shell.execute_reply.started":"2022-05-10T17:59:46.747852Z","shell.execute_reply":"2022-05-10T17:59:46.761099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.764115Z","iopub.execute_input":"2022-05-10T17:59:46.764644Z","iopub.status.idle":"2022-05-10T17:59:46.777214Z","shell.execute_reply.started":"2022-05-10T17:59:46.764607Z","shell.execute_reply":"2022-05-10T17:59:46.77642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n    \n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   #truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.778478Z","iopub.execute_input":"2022-05-10T17:59:46.778906Z","iopub.status.idle":"2022-05-10T17:59:46.789345Z","shell.execute_reply.started":"2022-05-10T17:59:46.77887Z","shell.execute_reply":"2022-05-10T17:59:46.788531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AWP","metadata":{}},{"cell_type":"code","source":"class AWP:\n    def __init__(\n        self,\n        model,\n        adv_param=\"weight\",\n        adv_lr=1,\n        adv_eps=0.2,\n        start_epoch=0,\n        adv_step=1,\n        device=\"cpu\",\n    ):\n        self.model = model\n        self.adv_param = adv_param\n        self.adv_lr = adv_lr\n        self.adv_eps = adv_eps\n        self.start_epoch = start_epoch\n        self.adv_step = adv_step\n        self.backup = {}\n        self.backup_eps = {}\n        self.device = device\n\n    def attack_backward(self, batch, epoch=0):\n        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n            return None\n        \n        loss = 0\n        self.save() \n        for i in range(self.adv_step):\n            self.attack_step() \n            loss += calculate_loss(model=self.model, batch=batch, device=self.device, return_outputs=False)\n            self.model.zero_grad()\n            \n        self.restore()\n        \n        return loss\n        \n        \n\n    def attack_step(self):\n        e = 1e-6\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                norm1 = torch.norm(param.grad)\n                norm2 = torch.norm(param.data.detach())\n                if norm1 != 0 and not torch.isnan(norm1):\n                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n                    param.data.add_(r_at)\n                    param.data = torch.min(\n                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n                    )\n                # param.data.clamp_(*self.backup_eps[name])\n\n    def save(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                if name not in self.backup:\n                    self.backup[name] = param.data.clone()\n                    grad_eps = self.adv_eps * param.abs().detach()\n                    self.backup_eps[name] = (\n                        self.backup[name] - grad_eps,\n                        self.backup[name] + grad_eps,\n                    )\n\n    def restore(self,):\n        for name, param in self.model.named_parameters():\n            if name in self.backup:\n                param.data = self.backup[name]\n        self.backup = {}\n        self.backup_eps = {}","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.790687Z","iopub.execute_input":"2022-05-10T17:59:46.79151Z","iopub.status.idle":"2022-05-10T17:59:46.808032Z","shell.execute_reply.started":"2022-05-10T17:59:46.791465Z","shell.execute_reply":"2022-05-10T17:59:46.8072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"microsoft/deberta-base\", config_path=None, config_updates={}, reinitialization_layers=0, mixout=0.0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        if mixout > 0:\n            for module in self.model.modules():\n                for name, submodule in module.named_children():\n                    if isinstance(submodule, nn.Dropout):\n                        module.p = 0.0\n                    if isinstance(submodule, nn.Linear):\n                        target_state_dict = submodule.state_dict()\n                        bias = True if submodule.bias is not None else False\n                        \n                        new_module = MixLinear(in_features=submodule.in_features, \n                                               out_features=submodule.out_features, \n                                               bias=bias, \n                                               target=target_state_dict[\"weight\"], \n                                               p=mixout)\n                        \n                        new_module.load_state_dict(target_state_dict)\n                        setattr(module, name, new_module)\n                \n            print(f\"Initialized Mixout (p={mixout}) Regularization\")\n        \n        if reinitialization_layers > 0:\n            layers = ...\n            for layer in layers[-reinitialization_layers:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=self.config.initializer_range)\n            \n            print(f\"Reinitializated last {n} layers.\")\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n            \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.81219Z","iopub.execute_input":"2022-05-10T17:59:46.812482Z","iopub.status.idle":"2022-05-10T17:59:46.828753Z","shell.execute_reply.started":"2022-05-10T17:59:46.812454Z","shell.execute_reply":"2022-05-10T17:59:46.827883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/us-patent-phrase-to-phrase-matching/train.csv\"\ntest_path = \"../input/us-patent-phrase-to-phrase-matching/test.csv\"\nsample_submission_path = \"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\"\ncpc_codes_path = \"../input/cpc-codes/titles.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.830013Z","iopub.execute_input":"2022-05-10T17:59:46.830514Z","iopub.status.idle":"2022-05-10T17:59:46.840445Z","shell.execute_reply.started":"2022-05-10T17:59:46.83048Z","shell.execute_reply":"2022-05-10T17:59:46.839709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(cpc_codes_path)\ntrain = pd.read_csv(train_path)\ntrain = train.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\n\nif DEBUG:\n    display(train)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:46.842264Z","iopub.execute_input":"2022-05-10T17:59:46.843031Z","iopub.status.idle":"2022-05-10T17:59:47.738446Z","shell.execute_reply.started":"2022-05-10T17:59:46.842958Z","shell.execute_reply":"2022-05-10T17:59:47.73769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation split","metadata":{}},{"cell_type":"code","source":"train[\"score_bin\"] = pd.cut(train[\"score\"], bins=5, labels=False)\n\ntrain = create_folds(data_frame=train, \n                     targets=train[\"score_bin\"].values,\n                     groups=train[\"anchor\"].values,\n                     folds=config.folds, \n                     seed=config.seed, \n                     shuffle=True)\n\nif DEBUG:\n    folds_samples_count = train.groupby('fold').size()\n    display(folds_samples_count)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:47.739853Z","iopub.execute_input":"2022-05-10T17:59:47.74027Z","iopub.status.idle":"2022-05-10T17:59:48.213454Z","shell.execute_reply.started":"2022-05-10T17:59:47.74023Z","shell.execute_reply":"2022-05-10T17:59:48.212616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.model.model_path)\ntokenizer_path = os.path.join(config.output_directory, \"tokenizer/\")\ntokenizer_files = tokenizer.save_pretrained(tokenizer_path)\n\nif DEBUG:\n    print(f\"Tokenizer: {tokenizer_files}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:48.214704Z","iopub.execute_input":"2022-05-10T17:59:48.215355Z","iopub.status.idle":"2022-05-10T17:59:55.406625Z","shell.execute_reply.started":"2022-05-10T17:59:48.215314Z","shell.execute_reply":"2022-05-10T17:59:55.405753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation","metadata":{}},{"cell_type":"code","source":"if WANDB:\n    experiment_name = os.environ.get(\"EXPERIMENT_NAME\")\n    group = experiment_name if experiment_name != \"none\" else wandb.util.generate_id()\n\ncv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold {fold}/{config.folds}\", end=\"\\n\"*2)\n    fold_directory = os.path.join(config.output_directory, f\"fold_{fold}\")    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    make_directory(checkpoints_directory)\n    \n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[\"fold\"].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[\"anchor\"].values, \n                            pair_texts=train_fold[\"target\"].values,\n                            contexts=train_fold[\"title\"].values,\n                            targets=train_fold[\"score\"].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    \n    validation_fold = train[train[\"fold\"].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[\"anchor\"].values, \n                                 pair_texts=validation_fold[\"target\"].values,\n                                 contexts=validation_fold[\"title\"].values,\n                                 targets=validation_fold[\"score\"].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=False, \n                                   drop_last=False)\n    \n    print(f\"Validation samples: {len(validation_dataset)}\")\n    \n    model = Model(**config.model)\n    awp_model = AWP(model=model, \n                    adv_eps=1e-2, \n                    device=config.device)\n    \n    if not os.path.exists(model_config_path): \n        model.config.to_json_file(model_config_path)\n    \n    model_parameters = model.parameters()\n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    training_steps = len(train_loader) * config.epochs\n    \n    if \"scheduler\" in config:\n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(\"warmup\", 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=\"max\", \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=\"checkpoint.pth\", \n                                       num_candidates=1)\n\n\n    if WANDB:\n        wandb.init(group=group, name=f\"fold_{fold}\", config=config)\n    \n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=[\"print\", \"wandb\"], \n                                                                                                           decimals=config.decimals)\n    \n    if WANDB:\n        wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f\"Model's path: {model_path}\")\n    \n    validation_fold[\"prediction\"] = validation_outputs.to(\"cpu\").numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n    \n    cv_monitor_value = validation_loss if config.cv_monitor_value == \"loss\" else validation_metrics[config.cv_monitor_value]\n    cv_scores.append(cv_monitor_value)\n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*5)\n    \n    \ncv_scores = np.array(cv_scores)\nprint(f\"CV scores: {cv_scores.round(config.decimals)}\")\nprint(f\"CV mean: {cv_scores.mean():.{config.decimals}}\")\nprint(f\"CV std: {cv_scores.std():.{config.decimals}}\")\n\noof_data_frame.to_pickle(\"oof.pkl\")\nnp.save(\"cv_scores.npy\", cv_scores)\nconfiguration_path = config.to_json(\"configuration.json\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:55.408394Z","iopub.execute_input":"2022-05-10T17:59:55.408883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}