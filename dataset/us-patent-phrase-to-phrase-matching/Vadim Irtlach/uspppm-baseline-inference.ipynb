{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration, Timer, Averager\nfrom torch_components.utils import seed_everything, get_batch, load_checkpoint\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.model_selection import StratifiedKFold\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport random\nimport os\nimport shutil\nimport gc\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEBUG = False\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"EXPERIMENT_NAME\"] = \"none\"\n\n        \nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T11:51:44.151913Z","iopub.execute_input":"2022-04-22T11:51:44.152173Z","iopub.status.idle":"2022-04-22T11:51:44.162958Z","shell.execute_reply.started":"2022-04-22T11:51:44.152144Z","shell.execute_reply":"2022-04-22T11:51:44.161799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pathes = Configuration(train=\"../input/us-patent-phrase-to-phrase-matching/train.csv\", \n                       test=\"../input/us-patent-phrase-to-phrase-matching/test.csv\",\n                       sample_submission=\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\",\n                       cpc_codes=\"../input/cpc-codes/titles.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.248653Z","iopub.execute_input":"2022-04-22T11:51:44.248929Z","iopub.status.idle":"2022-04-22T11:51:44.252404Z","shell.execute_reply.started":"2022-04-22T11:51:44.248902Z","shell.execute_reply":"2022-04-22T11:51:44.251776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"config = Configuration(seed=42,\n                       max_length=60,\n                       batch_size=32,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=5,  \n                       verbose=250,\n                       device=DEVICE,\n                       amp=True, \n                       input_directory=\"../input/uspppm-baseline-training\",\n                       debug=True)\n\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.279632Z","iopub.execute_input":"2022-04-22T11:51:44.279905Z","iopub.status.idle":"2022-04-22T11:51:44.287168Z","shell.execute_reply.started":"2022-04-22T11:51:44.279878Z","shell.execute_reply":"2022-04-22T11:51:44.286243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def create_submission(ids, predictions, path=\"submission.csv\"):\n    submission = pd.DataFrame({\n        \"id\": ids,\n        \"score\": predictions,\n    })\n    \n    submission.to_csv(path, index=False)\n    return submission\n\ndef prediction_loop(loader, \n                    model, \n                    device=\"cpu\", \n                    amp=False, \n                    verbose=1, \n                    time_format=\"{hours}:{minutes}:{seconds}\", \n                    logger=\"print\"):\n    \n    if device is not None:\n        model.to(device)\n    \n    model.eval()\n    outputs = []\n    timer = Timer(time_format)\n    steps = len(loader)\n    \n    if logger == \"tqdm\":\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n        loader.set_description_str(\"[Prediction]\")\n    \n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_outputs = prediction_step(batch=batch, model=model, device=device)\n                \n            outputs.extend(batch_outputs.to(\"cpu\").numpy())\n            \n            if logger == \"print\":\n                if step % verbose == 0 or step == steps:\n                    elapsed, remain = timer(step/steps)\n\n                    print(f\"[Prediction] \"\n                          f\"{step}/{steps} - \"\n                          f\"remain: {remain}\")\n            \n    outputs = torch.tensor(outputs)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.18637Z","iopub.execute_input":"2022-04-22T11:51:44.186778Z","iopub.status.idle":"2022-04-22T11:51:44.194865Z","shell.execute_reply.started":"2022-04-22T11:51:44.18675Z","shell.execute_reply":"2022-04-22T11:51:44.19383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_step(batch, model, device=\"cpu\"):\n    input_ids, attention_mask = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    \n    outputs = model(input_ids, attention_mask)\n    \n    return outputs.sigmoid().squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.218146Z","iopub.execute_input":"2022-04-22T11:51:44.218437Z","iopub.status.idle":"2022-04-22T11:51:44.222961Z","shell.execute_reply.started":"2022-04-22T11:51:44.218396Z","shell.execute_reply":"2022-04-22T11:51:44.222172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"cpc_codes = pd.read_csv(pathes.cpc_codes)\n\npath = pathes.train if DEBUG else pathes.test \ntest = pd.read_csv(path)\ntest = test.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\ntest_ids = test[\"id\"].values\n\nsample_submission = pd.read_csv(pathes.sample_submission)\n\nif config.debug:\n    display(test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.314415Z","iopub.execute_input":"2022-04-22T11:51:44.314846Z","iopub.status.idle":"2022-04-22T11:51:44.869572Z","shell.execute_reply.started":"2022-04-22T11:51:44.314817Z","shell.execute_reply":"2022-04-22T11:51:44.868744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer_path = os.path.join(config.input_directory, \"tokenizer/\")\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.871347Z","iopub.execute_input":"2022-04-22T11:51:44.871614Z","iopub.status.idle":"2022-04-22T11:51:44.897475Z","shell.execute_reply.started":"2022-04-22T11:51:44.87158Z","shell.execute_reply":"2022-04-22T11:51:44.896827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.898844Z","iopub.execute_input":"2022-04-22T11:51:44.899311Z","iopub.status.idle":"2022-04-22T11:51:44.910894Z","shell.execute_reply.started":"2022-04-22T11:51:44.899273Z","shell.execute_reply":"2022-04-22T11:51:44.909943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.913078Z","iopub.execute_input":"2022-04-22T11:51:44.91358Z","iopub.status.idle":"2022-04-22T11:51:44.924237Z","shell.execute_reply.started":"2022-04-22T11:51:44.913539Z","shell.execute_reply":"2022-04-22T11:51:44.923465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collator = Collator(return_targets=False, tokenizer=tokenizer, max_length=config.max_length)\n\ntest_dataset = Dataset(texts=test[\"anchor\"].values, \n                       pair_texts=test[\"target\"].values,\n                       contexts=test[\"title\"].values,\n                       max_length=config.max_length,\n                       sep=tokenizer.sep_token,\n                       tokenizer=tokenizer)\n    \ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=config.batch_size*2, \n                         num_workers=config.num_workers,\n                         pin_memory=config.pin_memory,\n                         collate_fn=collator,\n                         shuffle=False, \n                         drop_last=False)\n\nprint(f\"Test Samples: {len(test_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.925568Z","iopub.execute_input":"2022-04-22T11:51:44.926055Z","iopub.status.idle":"2022-04-22T11:51:44.944857Z","shell.execute_reply.started":"2022-04-22T11:51:44.926018Z","shell.execute_reply":"2022-04-22T11:51:44.943946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"microsoft/deberta-base\", config_path=None, config_updates={}, reinitialization_layers=0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n                \n                \n        #self.reinit_layers(n=reinitialization_layers, layers=self.model.encoder.layer, std=self.config.initializer_range)\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n    \n    \n    def reinit_layers(self, layers, n=0, std=0.02):\n        if n > 0:\n            for layer in layers[-n:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=std)\n            \n            print(f\"Reinitializated last {n} layers.\")\n                \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.946206Z","iopub.execute_input":"2022-04-22T11:51:44.946704Z","iopub.status.idle":"2022-04-22T11:51:44.961621Z","shell.execute_reply.started":"2022-04-22T11:51:44.946666Z","shell.execute_reply":"2022-04-22T11:51:44.960849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"oof_predictions = []\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold [{fold}/{config.folds}]\")\n    \n    fold_directory = os.path.join(config.input_directory, f\"fold_{fold}/\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    checkpoint_path = os.path.join(checkpoints_directory, \"checkpoint.pth\")\n    \n    model = Model(config_path=model_config_path)\n    \n    fold_checkpoint = load_checkpoint(path=checkpoint_path, \n                                      model=model, \n                                      strict=True, \n                                      ignore_warnings=True)\n    \n    \n    print(f\"Loaded checkpoint from '{checkpoint_path}'.\")\n    \n    fold_predictions = prediction_loop(loader=test_loader, \n                                       model=model, \n                                       amp=config.amp, \n                                       device=config.device)\n    \n    oof_predictions.append(fold_predictions.numpy())\n    \n    del model, fold_checkpoint, fold_predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*3)\n    \noof_predictions = np.array(oof_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:44.964662Z","iopub.execute_input":"2022-04-22T11:51:44.965256Z","iopub.status.idle":"2022-04-22T11:51:55.520095Z","shell.execute_reply.started":"2022-04-22T11:51:44.965215Z","shell.execute_reply":"2022-04-22T11:51:55.519283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = np.mean(oof_predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:55.521826Z","iopub.execute_input":"2022-04-22T11:51:55.522335Z","iopub.status.idle":"2022-04-22T11:51:55.527357Z","shell.execute_reply.started":"2022-04-22T11:51:55.522293Z","shell.execute_reply":"2022-04-22T11:51:55.526133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_submission(ids=test_ids, predictions=test_predictions, path=\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T11:51:55.528758Z","iopub.execute_input":"2022-04-22T11:51:55.529211Z","iopub.status.idle":"2022-04-22T11:51:55.549752Z","shell.execute_reply.started":"2022-04-22T11:51:55.529169Z","shell.execute_reply":"2022-04-22T11:51:55.548981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}