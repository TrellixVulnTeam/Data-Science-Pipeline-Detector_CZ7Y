{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nGuide: Baseline Guide<br>\nInference: USPPPM: DeBERTa V3 Small [Inference]","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:19.36287Z","iopub.execute_input":"2022-05-30T09:23:19.363275Z","iopub.status.idle":"2022-05-30T09:23:24.10783Z","shell.execute_reply.started":"2022-05-30T09:23:19.363189Z","shell.execute_reply":"2022-05-30T09:23:24.106627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\nsys.path.append(\"../input/mixout-github-code/mixout\")\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.checkpoint import checkpoint\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration as Config, Timer, Averager\nfrom torch_components.callbacks import EarlyStopping, ModelCheckpoint\nfrom torch_components.utils import seed_everything, get_lr, get_optimizer, get_scheduler\nfrom torch_components.import_utils import wandb_run_exists\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom mixout import MixLinear, Mixout\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display\nfrom datetime import timedelta\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport wandb\nimport os\nimport shutil\nimport gc\nfrom kaggle_secrets import UserSecretsClient\n\n\nos.environ[\"EXPERIMENT_NAME\"] = \"microsoft/deberta-v3-small\"\n\nEXPERIMENT_NAME = os.environ.get(\"EXPERIMENT_NAME\")\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nWANDB = False\nDEBUG = True\nUSER_SECRETS = UserSecretsClient()\n\n\nif WANDB:\n    os.environ[\"WANDB_PROJECT\"] = \"uspppm\"\n    os.environ[\"WANDB_ENTITY\"] = \"uspppm\"\n    os.environ[\"WANDB_SILENT\"] = \"true\"\n    \n    wandb_secret_name = \"wandb_api_key\"\n    wandb_key = USER_SECRETS.get_secret(wandb_secret_name)\n    \n    EXPERIMENT_NAME = EXPERIMENT_NAME if EXPERIMENT_NAME != \"none\" else wandb.util.generate_id()\n    wandb.login(key=wandb_key)\n    \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T09:23:24.112253Z","iopub.execute_input":"2022-05-30T09:23:24.112769Z","iopub.status.idle":"2022-05-30T09:23:29.454065Z","shell.execute_reply.started":"2022-05-30T09:23:24.112719Z","shell.execute_reply":"2022-05-30T09:23:29.452923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"config = Config(model=dict(model_path=\"microsoft/deberta-v3-small\"),\n                optimizer=dict(name=\"AdamW\", parameters=dict(lr=1e-5, weight_decay=0.0)),\n                scheduler=dict(name=\"get_cosine_with_hard_restarts_schedule_with_warmup\", \n                               parameters=dict(num_cycles=2, last_epoch=-1)),\n                warmup=0.1,\n                scheduling_after=\"step\",\n                seed=42,\n                max_length=75,\n                batch_size=32,\n                epochs=10,\n                num_workers=4,\n                pin_memory=True,\n                folds=5,\n                validation_steps=200, \n                gradient_accumulation_steps=1,\n                gradient_norm=1.0,\n                gradient_scaling=True,\n                delta=1e-4,\n                verbose=100,\n                save_model=False,\n                device=DEVICE,\n                input_directory=\"./\",\n                output_directory=\"./\",\n                cv_monitor_value=\"pearson\",\n                amp=True, \n                debug=True,\n                decimals=4)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.45755Z","iopub.execute_input":"2022-05-30T09:23:29.458262Z","iopub.status.idle":"2022-05-30T09:23:29.46826Z","shell.execute_reply.started":"2022-05-30T09:23:29.458196Z","shell.execute_reply":"2022-05-30T09:23:29.467123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.seed = seed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.471307Z","iopub.execute_input":"2022-05-30T09:23:29.47166Z","iopub.status.idle":"2022-05-30T09:23:29.492537Z","shell.execute_reply.started":"2022-05-30T09:23:29.471613Z","shell.execute_reply":"2022-05-30T09:23:29.491605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def make_directory(directory, overwriting=False):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    else:\n        if overwriting:\n            shutil.rmtree(directory)\n            os.mkdir(directory)\n\n            \ndef create_folds(data_frame, targets, groups, folds=5, seed=42, shuffle=True, fold_column=\"fold\"):\n    cv_strategy = StratifiedGroupKFold(n_splits=folds, random_state=seed, shuffle=shuffle)\n    folds = cv_strategy.split(X=data_frame, y=targets, groups=groups)\n    for fold, (train_indexes, validation_indexes) in enumerate(folds):\n        data_frame.loc[validation_indexes, fold_column] =  int(fold+1)\n        \n    data_frame[fold_column] = data_frame[fold_column].astype(int)\n    \n    return data_frame","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.495923Z","iopub.execute_input":"2022-05-30T09:23:29.496558Z","iopub.status.idle":"2022-05-30T09:23:29.506426Z","shell.execute_reply.started":"2022-05-30T09:23:29.496512Z","shell.execute_reply":"2022-05-30T09:23:29.505073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_loop(train_loader, \n                  model,\n                  optimizer,\n                  scheduler=None,\n                  scheduling_after=\"step\",\n                  epochs=1,\n                  validation_loader=None, \n                  gradient_accumulation_steps=1, \n                  gradient_scaling=False,\n                  gradient_norm=1,\n                  validation_steps=\"epoch\", \n                  amp=False,\n                  recalculate_metrics_at_end=True, \n                  return_validation_outputs=True,\n                  debug=True, \n                  teacher_model=None,\n                  pseudo_loader=None,\n                  verbose=100, \n                  device=\"cpu\", \n                  time_format=\"{hours}:{minutes}:{seconds}\", \n                  logger=[\"print\", \"wandb\"], \n                  decimals=4):\n    \n    training_steps = len(train_loader) * epochs\n    \n    if isinstance(validation_steps, float):\n        validation_steps = int(training_steps * validation_steps)\n    elif validation_steps == \"epoch\":\n        validation_steps = len(train_loader)\n    \n    if debug:\n        print(f\"Epochs: {epochs}\")\n        print(f\"Auto Mixed Precision: {amp}\")\n        print(f\"Gradient norm: {gradient_norm}\")\n        print(f\"Gradient scaling: {gradient_scaling}\")\n        print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n        print(f\"Validation steps: {validation_steps}\")\n        print(f\"Device: {device}\")\n        print()\n        \n    if wandb_run_exists() and \"wandb\" in logger:\n        print(f\"Weights & Biases Run: {wandb.run.get_url()}\", end=\"\\n\"*2)\n        \n    passed_steps = 1\n    train_loss, train_metrics = Averager(), Averager()\n    scaler = GradScaler() if gradient_scaling else None\n    best_validation_loss, best_validation_metrics, best_validation_outputs = None, None, None\n    total_time = timedelta(seconds=0)\n    \n    if device is not None: \n        model.to(device)\n        \n        if teacher_model is not None: teacher_model.to(device)\n    \n    for epoch in range(1, epochs+1):\n        if \"tqdm\" in logger:\n            bar_format = \"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\"\n            train_loader = tqdm(iterable=train_loader, \n                                total=len(train_loader),\n                                colour=\"#000\",\n                                bar_format=bar_format)\n            \n            train_loader.set_description_str(f\"Epoch {epoch}/{epochs}\")\n        \n        if \"print\" in logger:\n            print(f\"\\nEpoch {epoch}/{epochs}\", end=\"\\n\"*2)\n            \n        epoch_train_loss, epoch_train_metrics = Averager(), Averager()\n        timer = Timer(time_format)\n        steps = len(train_loader)    \n        \n        model.zero_grad()\n        for step, batch in enumerate(train_loader, 1):\n            batch_size = train_loader.batch_size\n            \n            step_timer =  Timer(time_format)\n            pseudo_batch = next(iter(pseudo_loader)) if pseudo_loader is not None else None\n            batch_loss, batch_metrics = training_step(batch=batch, \n                                                      model=model, \n                                                      optimizer=optimizer,\n                                                      gradient_norm=gradient_norm,\n                                                      gradient_accumulation_steps=gradient_accumulation_steps, \n                                                      amp=amp, \n                                                      scaler=scaler, \n                                                      device=device, \n                                                      overall_loss=epoch_train_loss.average, \n                                                      overall_metrics=epoch_train_metrics.average,\n                                                      step=passed_steps, \n                                                      epoch=epoch, \n                                                      teacher_model=teacher_model,\n                                                      pseudo_batch=pseudo_batch)\n            \n            lr_key = \"lr\"\n            lr = get_lr(optimizer, only_last=True, key=lr_key)\n            \n            if step % gradient_accumulation_steps == 0:\n                optimization_step(model=model, optimizer=optimizer, scaler=scaler)\n    \n                if scheduling_after == \"step\":\n                    scheduling_step(scheduler, loop=\"training\")\n            \n            elapsed, remain = step_timer(1/1)\n            step_seconds = step_timer.elapsed_time.total_seconds()\n            sample_seconds = step_seconds / batch_size\n            \n            if wandb_run_exists() and \"wandb\" in logger:\n                logs = {\"train/seconds vs step\": step_seconds, \n                        \"train/seconds vs sample\": sample_seconds}\n                \n                wandb.log(logs, step=passed_steps)\n            \n            train_loss.update(batch_loss, n=batch_size)\n            epoch_train_loss.update(batch_loss, n=batch_size)\n            train_metrics.update(batch_metrics, n=batch_size)\n            epoch_train_metrics.update(batch_metrics, n=batch_size)\n            \n            \n            logs = {\"train/loss\": train_loss.average, \n                    \"train/loss vs batch\": batch_loss, \n                    \"train/loss vs epoch\": epoch_train_loss.average,\n                    \"lr\": lr}\n            \n            for metric in batch_metrics:\n                logs.update({f\"train/{metric}\": train_metrics.average[metric], \n                             f\"train/{metric} vs batch\": batch_metrics[metric], \n                             f\"train/{metric} vs epoch\": epoch_train_metrics.average[metric]})\n                \n            if wandb_run_exists() and \"wandb\" in logger:\n                wandb.log(logs, step=passed_steps) \n            \n            if \"tqdm\" in logger:\n                train_loader.set_postfix_str(f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                                             f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n            if \"print\" in logger:\n                 if step % verbose == 0 or step == steps and verbose > 0:\n                    elapsed, remain = timer(step/steps)\n                    print(f\"{step}/{steps} - \"\n                          f\"remain: {remain} - \"\n                          f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                          f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)} - \"\n                          f\"lr: {lr}\")\n                    \n            \n            if validation_loader is not None:\n                if (passed_steps % validation_steps) == 0:\n                    if step > validation_steps: print()\n                    validation_loop_steps = len(validation_loader)\n                    validation_batch_size = validation_loader.batch_size\n                    \n                    validation_timer =  Timer(time_format)\n                    validation_loss, validation_metrics, validation_outputs = validation_loop(loader=validation_loader, \n                                                                                              model=model,\n                                                                                              gradient_accumulation_steps=gradient_accumulation_steps,\n                                                                                              amp=amp, \n                                                                                              return_outputs=True, \n                                                                                              verbose=verbose, \n                                                                                              recalculate_metrics_at_end=True, \n                                                                                              device=device, \n                                                                                              logger=logger)\n                    \n                    \n                    elapsed, remain = validation_timer(1/1)\n                    validation_seconds = validation_timer.elapsed_time.total_seconds()\n                    validation_step_seconds = validation_seconds / validation_loop_steps\n                    validation_sample_seconds = validation_step_seconds / validation_batch_size\n            \n                    if wandb_run_exists() and \"wandb\" in logger:\n                        logs = {\"validation/seconds vs step\": validation_step_seconds, \n                                \"validation/seconds vs sample\": validation_sample_seconds}\n                \n                        wandb.log(logs, step=passed_steps)\n                    \n                    \n                    logs = {\"validation/loss\": validation_loss, \n                            \"train/loss vs validation steps\": epoch_train_loss.average}\n    \n                    for metric, value in validation_metrics.items():\n                        logs.update({f\"validation/{metric}\": value, \n                                     f\"train/{metric} vs validation steps\": epoch_train_metrics.average[metric]})\n                    \n                    if wandb_run_exists() and \"wandb\" in logger:\n                        wandb.log(logs, step=passed_steps)\n                    \n                    is_checkpoint_saved = model_checkpointing(loss=validation_loss, \n                                                              metrics=validation_metrics,\n                                                              model=model, \n                                                              optimizer=optimizer, \n                                                              scheduler=scheduler, \n                                                              step=passed_steps, \n                                                              best_loss=best_validation_loss, \n                                                              best_metrics=validation_metrics)\n                    \n                    if is_checkpoint_saved:\n                        best_validation_loss = validation_loss\n                        best_validation_metrics = validation_metrics\n                        best_validation_outputs = validation_outputs\n                        \n                    scheduling_step(scheduler, loss=validation_loss, loop=\"validation\")\n                    print()\n            \n            passed_steps += 1\n        \n        if scheduling_after == \"epoch\":\n            scheduling_step(scheduler, loop=\"training\")\n        \n        on_epoch_end(model=model, \n                     step=passed_steps, \n                     epoch=epoch)\n        \n        if \"tqdm\" in logger and \"print\" not in logger:\n            elapsed, remain = timer(1/1)\n        \n        epoch_elapsed_seconds = timer.elapsed_time.total_seconds()\n        total_time += timedelta(seconds=epoch_elapsed_seconds)\n        \n        if wandb_run_exists() and \"wandb\" in logger:\n            wandb.log({\"epoch\": epoch}, step=passed_steps)\n        \n        if \"tqdm\" in logger: train_loader.close()\n            \n        print(f\"\\nTraining loss: {epoch_train_loss.average:.{decimals}}\"\n              f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n        \n        if validation_loader is not None:\n            print(f\"Validation loss: {best_validation_loss:.{decimals}}\"\n                  f\"{format_metrics(best_validation_metrics, decimals=decimals)}\")\n        \n        total_time_string = Timer.format_time(total_time, time_format=time_format)\n        print(f\"Total time: {total_time_string}\")\n    \n    if validation_loader is not None:\n        if return_validation_outputs:\n            return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics, best_validation_outputs)\n        \n        return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics)\n\n    return (epoch_train_loss.average, epoch_train_metrics.average)\n        \ndef validation_loop(loader, \n                    model, \n                    gradient_accumulation_steps=1,\n                    amp=False, \n                    return_outputs=True, \n                    recalculate_metrics_at_end=True, \n                    verbose=1, \n                    device=\"cpu\", \n                    time_format=\"{hours}:{minutes}:{seconds}\",\n                    logger=[\"print\"], \n                    decimals=4):\n    \n    model.eval()\n    loss, metrics = Averager(), Averager()\n    timer = Timer(time_format)\n    outputs, targets = [], []\n    steps = len(loader)\n    \n    if \"tqdm\" in logger:\n        bar_format = \"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\"\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=bar_format)\n            \n        loader.set_description_str(\"[Validation]\")\n    \n    is_targets = False\n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_loss, batch_outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n                \n                batch_loss /= gradient_accumulation_steps\n                loss.update(batch_loss.item(), n=len(batch))\n                \n                batch_targets = get_targets(batch)\n                batch_metrics = calculate_metrics(predictions=batch_outputs, targets=batch_targets, device=device)\n                metrics.update(batch_metrics, n=len(batch))\n                \n                if batch_targets is not None:\n                    if isinstance(batch_targets, dict):\n                        targets.append(batch_targets)\n                    else:\n                        targets.extend(batch_targets.to(\"cpu\").tolist())\n                        \n                    is_targets = True\n                \n                outputs.extend(batch_outputs.to(\"cpu\").tolist())\n                \n                if step == steps and recalculate_metrics_at_end and is_targets:\n                    outputs = torch.tensor(outputs)\n                    targets = torch.tensor(targets)\n                        \n                    metrics = Averager(calculate_metrics(predictions=outputs, targets=targets))\n                \n                if \"tqdm\" in logger:\n                    loader.set_postfix_str(f\"loss: {loss.average:.{decimals}}\"\n                                           f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                \n                if \"print\" in logger:\n                    if step % verbose == 0 or step == steps and verbose > 0:\n                        elapsed, remain = timer(step/steps)\n\n                        print(f\"[Validation] \"\n                              f\"{step}/{steps} - \"\n                              f\"remain: {remain} - \"\n                              f\"loss: {loss.average:.{decimals}}\"\n                              f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                    \n    if not recalculate_metrics_at_end: \n        outputs = torch.tensor(outputs)\n        \n    if \"tqdm\" in logger:\n        loader.close()\n        \n    return (loss.average, metrics.average, outputs) if return_outputs else (loss.average, metrics.average)\n\n\ndef format_metrics(metrics, sep=\" - \", add_sep_to_start=True, decimals=4):\n    if metrics != {}:\n        string = sep.join([f\"{k}: {v:.{decimals}}\" for k, v in metrics.items()])\n        return sep + string if add_sep_to_start else string \n    \n    return \"\"\n\n    \ndef training_step(batch, \n                  model, \n                  optimizer, \n                  gradient_norm=1.0, \n                  amp=False, \n                  gradient_accumulation_steps=1, \n                  scaler=None, \n                  device=\"cpu\", \n                  overall_loss=None, \n                  overall_metrics=None, \n                  step=None, \n                  epoch=None,\n                  teacher_model=None,\n                  pseudo_batch=None):\n    \n    model.train()\n    with autocast(enabled=amp):\n        loss, outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n        targets = get_targets(batch)\n        metrics = calculate_metrics(predictions=outputs, targets=targets, device=device)\n        \n        loss /= gradient_accumulation_steps\n        loss = backward_step(loss=loss, optimizer=optimizer, scaler=scaler)\n        \n        adversarial_loss = adversarial_step(batch=batch, \n                                            model=model, \n                                            device=device, \n                                            loss=overall_loss, \n                                            metrics=overall_metrics, \n                                            step=step, \n                                            epoch=epoch)\n        \n        if adversarial_loss is not None:\n            adversarial_loss = backward_step(loss=adversarial_loss, optimizer=optimizer, scaler=scaler)\n        \n        if pseudo_batch is not None and teacher_model is not None:\n            pseudo_loss = pseudo_labeling_step(batch=batch,\n                                               pseudo_batch=pseudo_batch,\n                                               model=model, \n                                               teacher_model=teacher_model, \n                                               loss=loss, \n                                               metrics=metrics,\n                                               step=step, \n                                               epoch=epoch, \n                                               device=device)\n        \n            if pseudo_loss is not None:\n                pseudo_loss = backward_step(loss=pseudo_loss, optimizer=optimizer, scaler=scaler)\n            \n    if gradient_norm > 0:\n        if scaler is not None:\n            scaler.unscale_(optimizer)\n                            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_norm)\n        \n    return loss.detach(), metrics\n\ndef backward_step(loss, optimizer, scaler=None):\n    if scaler is not None:\n        scaler.scale(loss).backward()\n    else:\n        loss.backward()\n        \n    return loss\n        \n\ndef optimization_step(model, optimizer, scaler=None):                        \n    if scaler is not None:\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        optimizer.step()\n        \n    model.zero_grad()\n        \n\ndef scheduling_step(scheduler=None, loss=None, loop=\"training\"):\n    if scheduler is not None:\n        if loop == \"validation\":\n            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(loss)\n        else:\n            if not isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step()\n\n                \ndef adversarial_step(batch, \n                     model, \n                     device=\"cpu\", \n                     loss=None, \n                     metrics=None, \n                     step=None, \n                     epoch=None):\n    pass\n\n                \n    \ndef calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    raise NotImplementedError(f\"`calculate_loss` function is not implemented.\")\n                \ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    return dict()\n\ndef get_targets(batch):\n    return []\n\n\ndef on_epoch_end(model=None, step=None, epoch=None):\n    pass\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    return True\n\n\ndef pseudo_labeling_step(batch, \n                         pseudo_batch, \n                         model, \n                         teacher_model, \n                         loss=None, \n                         metrics=None, \n                         step=None, \n                         epoch=None, \n                         device=\"cpu\"):\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.510743Z","iopub.execute_input":"2022-05-30T09:23:29.510981Z","iopub.status.idle":"2022-05-30T09:23:29.597641Z","shell.execute_reply.started":"2022-05-30T09:23:29.510952Z","shell.execute_reply":"2022-05-30T09:23:29.596614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    input_ids, attention_mask, targets = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    targets = targets.to(device).float()\n    \n    outputs = model(input_ids, attention_mask)\n    outputs = outputs.sigmoid().squeeze(dim=-1)\n    loss = F.mse_loss(outputs, targets, reduction=\"mean\")\n    \n    return (loss, outputs) if return_outputs else loss\n\n\ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    predictions = predictions.sigmoid().detach().view(-1).to(\"cpu\").float().numpy()\n    targets = targets.view(-1).to(\"cpu\").float().numpy()\n    \n    return dict(pearson=scipy.stats.pearsonr(predictions, targets)[0])\n\n\ndef get_targets(batch):\n    *_, targets = batch\n    return targets\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    is_saved_checkpoint = model_checkpoint(value=metrics[\"pearson\"], \n                                           model=model, \n                                           optimizer=optimizer, \n                                           scheduler=scheduler, \n                                           step=step)\n    return is_saved_checkpoint","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.599876Z","iopub.execute_input":"2022-05-30T09:23:29.60013Z","iopub.status.idle":"2022-05-30T09:23:29.614525Z","shell.execute_reply.started":"2022-05-30T09:23:29.600089Z","shell.execute_reply":"2022-05-30T09:23:29.612169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.616355Z","iopub.execute_input":"2022-05-30T09:23:29.616794Z","iopub.status.idle":"2022-05-30T09:23:29.63355Z","shell.execute_reply.started":"2022-05-30T09:23:29.616747Z","shell.execute_reply":"2022-05-30T09:23:29.632501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n    \n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   #truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.635605Z","iopub.execute_input":"2022-05-30T09:23:29.636004Z","iopub.status.idle":"2022-05-30T09:23:29.651177Z","shell.execute_reply.started":"2022-05-30T09:23:29.635946Z","shell.execute_reply":"2022-05-30T09:23:29.650198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"microsoft/deberta-base\", config_path=None, config_updates={}, reinitialization_layers=0, mixout=0.0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        self.model.gradient_checkpointing_enable()\n        print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n        \n        if mixout > 0:\n            for module in self.model.modules():\n                for name, submodule in module.named_children():\n                    if isinstance(submodule, nn.Dropout):\n                        module.p = 0.0\n                    if isinstance(submodule, nn.Linear):\n                        target_state_dict = submodule.state_dict()\n                        bias = True if submodule.bias is not None else False\n                        \n                        new_module = MixLinear(in_features=submodule.in_features, \n                                               out_features=submodule.out_features, \n                                               bias=bias, \n                                               target=target_state_dict[\"weight\"], \n                                               p=mixout)\n                        \n                        new_module.load_state_dict(target_state_dict)\n                        setattr(module, name, new_module)\n                \n            print(f\"Initialized Mixout (p={mixout}) Regularization\")\n        \n        if reinitialization_layers > 0:\n            layers = ...\n            for layer in layers[-reinitialization_layers:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=self.config.initializer_range)\n            \n            print(f\"Reinitializated last {n} layers.\")\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n            \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.655261Z","iopub.execute_input":"2022-05-30T09:23:29.655681Z","iopub.status.idle":"2022-05-30T09:23:29.678777Z","shell.execute_reply.started":"2022-05-30T09:23:29.655568Z","shell.execute_reply":"2022-05-30T09:23:29.677766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/us-patent-phrase-to-phrase-matching/train.csv\"\ntest_path = \"../input/us-patent-phrase-to-phrase-matching/test.csv\"\nsample_submission_path = \"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\"\ncpc_codes_path = \"../input/cpc-codes/titles.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.680663Z","iopub.execute_input":"2022-05-30T09:23:29.681059Z","iopub.status.idle":"2022-05-30T09:23:29.691478Z","shell.execute_reply.started":"2022-05-30T09:23:29.681014Z","shell.execute_reply":"2022-05-30T09:23:29.690381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(cpc_codes_path)\ntrain = pd.read_csv(train_path)\ntrain = train.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\n\nif DEBUG:\n    display(train)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:29.693217Z","iopub.execute_input":"2022-05-30T09:23:29.693556Z","iopub.status.idle":"2022-05-30T09:23:30.70964Z","shell.execute_reply.started":"2022-05-30T09:23:29.693511Z","shell.execute_reply":"2022-05-30T09:23:30.70864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation split","metadata":{}},{"cell_type":"code","source":"train[\"score_bin\"] = pd.cut(train[\"score\"], bins=5, labels=False)\ntrain = create_folds(data_frame=train, \n                     targets=train[\"score_bin\"].values,\n                     groups=train[\"anchor\"].values,\n                     folds=config.folds, \n                     seed=config.seed, \n                     shuffle=True)\n\nif DEBUG:\n    folds_samples_count = train.groupby(\"fold\").size()\n    display(folds_samples_count)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:30.711132Z","iopub.execute_input":"2022-05-30T09:23:30.711692Z","iopub.status.idle":"2022-05-30T09:23:31.346007Z","shell.execute_reply.started":"2022-05-30T09:23:30.711629Z","shell.execute_reply":"2022-05-30T09:23:31.344854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.model.model_path)\ntokenizer_path = os.path.join(config.output_directory, \"tokenizer/\")\ntokenizer_files = tokenizer.save_pretrained(tokenizer_path)\n\nif DEBUG:\n    print(f\"Tokenizer: {tokenizer}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:31.348022Z","iopub.execute_input":"2022-05-30T09:23:31.348346Z","iopub.status.idle":"2022-05-30T09:23:38.648749Z","shell.execute_reply.started":"2022-05-30T09:23:31.348301Z","shell.execute_reply":"2022-05-30T09:23:38.647751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation","metadata":{}},{"cell_type":"code","source":"cv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold {fold}/{config.folds}\", end=\"\\n\"*2)\n    \n    fold_directory = os.path.join(config.output_directory, f\"fold_{fold}\")    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    make_directory(checkpoints_directory)\n    \n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[\"fold\"].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[\"anchor\"].values, \n                            pair_texts=train_fold[\"target\"].values,\n                            contexts=train_fold[\"title\"].values,\n                            targets=train_fold[\"score\"].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    \n    validation_fold = train[train[\"fold\"].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[\"anchor\"].values, \n                                 pair_texts=validation_fold[\"target\"].values,\n                                 contexts=validation_fold[\"title\"].values,\n                                 targets=validation_fold[\"score\"].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=False, \n                                   drop_last=False)\n    \n    print(f\"Validation samples: {len(validation_dataset)}\")\n    \n    \n    model = Model(**config.model)\n    model.config.to_json_file(model_config_path)\n    model_parameters = model.parameters()\n    \n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    if \"scheduler\" in config:\n        training_steps = len(train_loader) * config.epochs\n        training_steps = int(training_steps // config.gradient_accumulation_steps)\n        \n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(\"warmup\", 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=\"max\", \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=\"checkpoint.pth\", \n                                       num_candidates=1)\n\n\n    if WANDB: wandb.init(group=EXPERIMENT_NAME, name=f\"Fold {fold}\", config=config)\n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=[\"print\", \"wandb\"], \n                                                                                                           decimals=config.decimals)\n    \n    if WANDB: wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f\"Model's path: {model_path}\")\n    \n    validation_fold[\"prediction\"] = validation_outputs.to(\"cpu\").numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n        \n    cv_monitor_value = validation_loss if config.cv_monitor_value == \"loss\" else validation_metrics.get(config.cv_monitor_value, np.nan)\n    cv_scores.append(cv_monitor_value)\n    \n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(end=\"\\n\"*5)\n    \ncv_scores = np.array(cv_scores).round(config.decimals)\nnp.save(\"cv_scores.npy\", cv_scores)\noof_data_frame.to_pickle(\"oof.pkl\")\nconfiguration_path = config.to_json(\"configuration.json\")\n\nprint(f\"CV scores: {cv_scores}\")\nprint(f\"CV mean: {cv_scores.mean():.{config.decimals}}\")\nprint(f\"CV std: {cv_scores.std():.{config.decimals}}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:23:38.650469Z","iopub.execute_input":"2022-05-30T09:23:38.651268Z","iopub.status.idle":"2022-05-30T09:24:52.974253Z","shell.execute_reply.started":"2022-05-30T09:23:38.651223Z","shell.execute_reply":"2022-05-30T09:24:52.972657Z"},"trusted":true},"execution_count":null,"outputs":[]}]}