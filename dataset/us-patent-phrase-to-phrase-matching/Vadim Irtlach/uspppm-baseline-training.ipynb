{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip uninstall -q -y transformers","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:44.345202Z","iopub.execute_input":"2022-04-23T10:46:44.345798Z","iopub.status.idle":"2022-04-23T10:46:45.889496Z","shell.execute_reply.started":"2022-04-23T10:46:44.34576Z","shell.execute_reply":"2022-04-23T10:46:45.888609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\n\nimport transformers\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration, Timer, Averager\nfrom torch_components.callbacks import EarlyStopping, ModelCheckpoint\nfrom torch_components.utils import seed_everything, get_lr, get_optimizer, get_scheduler, get_batch\nfrom torch_components.import_utils import wandb_run_exists\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display\nfrom datetime import timedelta\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport wandb\nimport os\nimport shutil\nimport gc\nfrom kaggle_secrets import UserSecretsClient\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nWANDB = True\nDEBUG = False\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nos.environ[\"EXPERIMENT_NAME\"] = \"none\"\nos.environ[\"WANDB_PROJECT\"] = \"uspppm\"\nos.environ[\"WANDB_ENTITY\"] = \"uspppm\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\n    \nuser_secrets = UserSecretsClient()\n\nif WANDB:\n    wandb_secret_name = \"wandb_api_key\"\n    wandb_key = user_secrets.get_secret(wandb_secret_name)\n    wandb.login(key=wandb_key)\n    \nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T10:46:45.891664Z","iopub.execute_input":"2022-04-23T10:46:45.892051Z","iopub.status.idle":"2022-04-23T10:46:45.903773Z","shell.execute_reply.started":"2022-04-23T10:46:45.892012Z","shell.execute_reply":"2022-04-23T10:46:45.903088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"pathes = Configuration(train=\"../input/us-patent-phrase-to-phrase-matching/train.csv\", \n                       test=\"../input/us-patent-phrase-to-phrase-matching/test.csv\",\n                       sample_submission=\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\",\n                       cpc_codes=\"../input/cpc-codes/titles.csv\")\n\nconfig = Configuration(model=dict(model_path=\"distilbert-base-uncased\", reinitialization_layers=0), \n                       optimizer=dict(name=\"AdamW\", \n                                      parameters=dict(lr=2e-5, weight_decay=0.0)),\n                       \n                       scheduler=dict(name=\"get_cosine_with_hard_restarts_schedule_with_warmup\", \n                                      parameters=dict(num_cycles=2, last_epoch=-1)),\n                       warmup=0.1,\n                       scheduling_after=\"step\",\n                       seed=42,\n                       max_length=75,\n                       batch_size=32,\n                       epochs=2,\n                       num_workers=4,\n                       pin_memory=True,\n                       folds=5, \n                       validation_steps=500, \n                       gradient_accumulation_steps=1,\n                       gradient_norm=1.0,\n                       gradient_scaling=True,\n                       delta=1e-4,\n                       verbose=250,\n                       save_model=True,\n                       device=DEVICE,\n                       output_directory=\"./\",\n                       cv_monitor_value=\"pearson\",\n                       amp=True, \n                       debug=True)\n\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:45.905382Z","iopub.execute_input":"2022-04-23T10:46:45.905931Z","iopub.status.idle":"2022-04-23T10:46:45.921769Z","shell.execute_reply.started":"2022-04-23T10:46:45.905895Z","shell.execute_reply":"2022-04-23T10:46:45.920828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def make_directory(directory, overwriting=False):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    else:\n        if overwriting:\n            shutil.rmtree(directory)\n            os.mkdir(directory)\n\n            \ndef create_folds(data_frame, targets, groups, folds=5, seed=42, shuffle=True, fold_column=\"fold\"):\n    cv_strategy = StratifiedGroupKFold(n_splits=folds, random_state=seed, shuffle=shuffle)\n    folds = cv_strategy.split(X=data_frame, y=targets, groups=groups)\n    for fold, (train_indexes, validation_indexes) in enumerate(folds):\n        data_frame.loc[validation_indexes, fold_column] =  int(fold+1)\n        \n    data_frame[fold_column] = data_frame[fold_column].astype(int)\n    \n    return data_frame","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:45.924141Z","iopub.execute_input":"2022-04-23T10:46:45.924663Z","iopub.status.idle":"2022-04-23T10:46:45.934368Z","shell.execute_reply.started":"2022-04-23T10:46:45.924625Z","shell.execute_reply":"2022-04-23T10:46:45.93371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_loop(train_loader, \n                  model,\n                  optimizer,\n                  scheduler=None,\n                  scheduling_after=\"step\",\n                  epochs=1,\n                  validation_loader=None, \n                  gradient_accumulation_steps=1, \n                  gradient_scaling=False,\n                  gradient_norm=1,\n                  validation_steps=100, \n                  amp=False,\n                  recalculate_metrics_at_end=True, \n                  return_validation_outputs=True,\n                  debug=True, \n                  verbose=1, \n                  device=\"cpu\", \n                  time_format=\"{hours}:{minutes}:{seconds}\", \n                  logger=\"print\"):\n    \n    training_steps = len(train_loader) * epochs\n    \n    if isinstance(validation_steps, float):\n        validation_steps = int(training_steps * validation_steps)\n    elif validation_steps == \"epoch\":\n        validation_steps = len(train_loader)\n    \n    scaler = GradScaler() if gradient_scaling else None\n    \n    \n    if wandb_run_exists():\n        wandb.define_metric(\"train/loss vs epoch\", step_metric=\"epoch\")\n    \n    if debug:\n        print(f\"Auto Mixed Precision: {amp}\")\n        print(f\"Gradient norm: {gradient_norm}\")\n        print(f\"Gradient scaling: {gradient_scaling}\")\n        print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n        print(f\"Validation steps: {validation_steps}\")\n        print(f\"Device: {device}\")\n        print()\n        \n    \n    if wandb_run_exists():\n        print(f\"Weights & Biases Run: {wandb.run.get_url()}\", end=\"\\n\"*2)\n        \n    passed_steps = 1\n    train_loss, train_metrics = Averager(), Averager()\n    best_validation_loss, best_validation_metrics, best_validation_outputs = None, None, None\n    \n    if device is not None:\n        model.to(device)\n    \n    model.zero_grad()\n    total_time = timedelta(seconds=0)\n    for epoch in range(1, epochs+1):\n        if logger == \"tqdm\":\n            train_loader = tqdm(iterable=train_loader, \n                                total=len(train_loader),\n                                colour=\"#000\",\n                                bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n            train_loader.set_description_str(f\"Epoch {epoch}/{epochs}\")\n        else:\n            print(f\"\\nEpoch {epoch}/{epochs}\", end=\"\\n\"*2)\n            \n\n        timer = Timer(time_format)\n        epoch_train_loss, epoch_train_metrics = Averager(), Averager()\n        steps = len(train_loader)\n        for step, batch in enumerate(train_loader, 1):\n            batch_size = len(batch)\n            batch_loss, batch_metrics = training_step(batch=batch, \n                                                      model=model, \n                                                      optimizer=optimizer,\n                                                      gradient_norm=gradient_norm,\n                                                      gradient_accumulation_steps=gradient_accumulation_steps, \n                                                      amp=amp, \n                                                      scaler=scaler, \n                                                      device=device)\n            \n            train_loss.update(batch_loss, n=batch_size)\n            train_metrics.update(batch_metrics, n=batch_size)\n            epoch_train_loss.update(batch_loss, n=batch_size)\n            epoch_train_metrics.update(batch_metrics, n=batch_size)\n            \n            if (passed_steps % gradient_accumulation_steps) == 0:\n                optimization_step(model=model, optimizer=optimizer, scaler=scaler)\n                \n\n            lr = get_lr(optimizer, only_last=True)\n            if scheduling_after == \"step\":\n                scheduling_step(scheduler)\n            \n                \n            logs = {\"train/loss\": train_loss.average, \n                    \"train/loss vs batch\": batch_loss, \"lr\": lr}\n            \n            for metric in batch_metrics:\n                metric = metric.strip().lower()\n                logs.update({f\"train/{metric}\": train_metrics.average[metric], \n                             f\"train/{metric} vs batch\": batch_metrics[metric]})\n                \n            if wandb_run_exists():\n                wandb.log(logs, step=passed_steps) \n            \n            if logger == \"tqdm\":\n                train_loader.set_postfix_str(f\"loss: {epoch_train_loss.average:.4}\"\n                                             f\"{format_metrics(epoch_train_metrics.average)}\")\n            else:\n                 if step % verbose == 0 or step == steps:\n                    elapsed, remain = timer(step/steps)\n                    print(f\"{step}/{steps} - \"\n                          f\"remain: {remain} - \"\n                          f\"loss: {epoch_train_loss.average:.4}\"\n                          f\"{format_metrics(epoch_train_metrics.average)}\")\n                    \n            \n            if validation_loader is not None:\n                if (passed_steps % validation_steps) == 0:\n                    print()\n                    validation_loss, validation_metrics, validation_outputs = validation_loop(loader=validation_loader, \n                                                                                              model=model, \n                                                                                              gradient_accumulation_steps=gradient_accumulation_steps,\n                                                                                              amp=amp, \n                                                                                              return_outputs=True, \n                                                                                              verbose=verbose, \n                                                                                              recalculate_metrics_at_end=True, \n                                                                                              device=device, \n                                                                                              logger=logger)\n                    \n                    \n                    \n                    logs = {\"validation/loss\": validation_loss, \n                            \"train/loss vs validation steps\": train_loss.average}\n    \n                    for metric, value in validation_metrics.items():\n                        metric = metric.strip().lower()\n                        logs.update({f\"validation/{metric}\": value, \n                                     f\"train/{metric} vs validation steps\": train_metrics.average[metric]})\n                    \n                    if wandb_run_exists():\n                        wandb.log(logs, step=passed_steps)\n                    \n                    is_checkpoint_saved = model_checkpointing(loss=validation_loss, \n                                                              metrics=validation_metrics,\n                                                              model=model, \n                                                              optimizer=optimizer, \n                                                              scheduler=scheduler, \n                                                              step=passed_steps, \n                                                              previous_loss=best_validation_loss, \n                                                              previous_metrics=validation_metrics)\n                    \n                    if is_checkpoint_saved:\n                        best_validation_loss = validation_loss\n                        best_validation_metrics = validation_metrics\n                        best_validation_outputs = validation_outputs\n                    \n                    scheduling_step(scheduler, loss=validation_loss)\n                    \n                    print()\n            \n            passed_steps += 1\n        \n        if scheduling_after == \"epoch\":\n            scheduling_step(scheduler)\n        \n            \n        if logger == \"tqdm\":\n            elapsed, remain = timer(1/1)\n\n        epoch_elapsed_seconds = timer.elapsed_time.total_seconds()\n        total_time += timedelta(seconds=epoch_elapsed_seconds)\n        \n        \n        logs = {\"train/loss vs epoch\": epoch_train_loss.average, \n                \"epoch\": epoch}\n        \n        for metric, value in train_metrics.average.items():\n            metric = metric.strip().lower()\n            logs.update({f\"train/{metric} vs epoch\": value})\n            \n            if wandb_run_exists():\n                wandb.define_metric(f\"train/{metric} vs epoch\", step_metric=\"epoch\")\n        \n        if wandb_run_exists():\n            wandb.log(logs, step=passed_steps)\n            \n        if logger == \"tqdm\":\n            train_loader.close()\n\n    \n    if debug:\n        print(f\"\\nResults\", end=\"\\n\"*2)\n\n        print(f\"Training loss: {train_loss.average}{format_metrics(train_metrics.average)}\")\n        print(f\"Validation loss: {best_validation_loss}{format_metrics(best_validation_metrics)}\")\n        print(f\"Total time: {Timer.format_time(total_time, time_format=time_format)}\")\n    \n    if validation_loader is not None:\n        if return_validation_outputs:\n            return (train_loss.average, train_metrics.average), (best_validation_loss, best_validation_metrics, best_validation_outputs)\n\n        return (train_loss.average, train_metrics.average), (best_validation_loss, best_validation_metrics)\n\n    return (train_loss.average, train_metrics.average)\n    \n\n    \ndef validation_loop(loader, \n                    model, \n                    gradient_accumulation_steps=1,\n                    amp=False, \n                    return_outputs=True, \n                    recalculate_metrics_at_end=True, \n                    verbose=1, \n                    device=\"cpu\", \n                    time_format=\"{hours}:{minutes}:{seconds}\",\n                    logger=\"print\"):\n    \n    model.eval()\n    loss, metrics = Averager(), Averager()\n    timer = Timer(time_format)\n    outputs, targets = [], []\n    steps = len(loader)\n    \n    if logger == \"tqdm\":\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=\"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\")\n            \n        loader.set_description_str(\"[Validation]\")\n    \n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_loss, batch_outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n                \n                if gradient_accumulation_steps > 1:\n                    batch_loss /= gradient_accumulation_steps\n                \n                loss.update(batch_loss.item(), n=len(batch))\n                \n                batch_targets = get_targets(batch)\n                batch_metrics = calculate_metrics(predictions=batch_outputs, targets=batch_targets, device=device)\n                metrics.update(batch_metrics, n=len(batch))\n                \n                if isinstance(batch_targets, dict):\n                    targets.append(batch_targets)\n                else:\n                    targets.extend(batch_targets.to(\"cpu\").tolist())\n                \n                outputs.extend(batch_outputs.to(\"cpu\").tolist())\n                \n                \n                if step == steps and recalculate_metrics_at_end:\n                    outputs = torch.tensor(outputs)\n                    targets = torch.tensor(targets)\n                        \n                    metrics = Averager(calculate_metrics(predictions=outputs, targets=targets))\n                \n                if logger == \"tqdm\":\n                    loader.set_postfix_str(f\"loss: {loss.average:.4}\"\n                                           f\"{format_metrics(metrics.average)}\")\n                else:\n                    if step % verbose == 0 or step == steps:\n                        elapsed, remain = timer(step/steps)\n\n                        print(f\"[Validation] \"\n                              f\"{step}/{steps} - \"\n                              f\"remain: {remain} - \"\n                              f\"loss: {loss.average:.4}\"\n                              f\"{format_metrics(metrics.average)}\")\n                    \n    if not recalculate_metrics_at_end: \n        outputs = torch.tensor(outputs)\n        \n    if logger == \"tqdm\":\n        loader.close()\n        \n    return (loss.average, metrics.average, outputs) if return_outputs else (loss.average, metrics.average)\n\n\ndef format_metrics(metrics, sep=\" - \", add_sep_to_start=True):\n    if metrics != {}:\n        string = sep.join([f\"{k.strip().lower()}: {v:.4}\" for k, v in metrics.items()])\n        return sep + string if add_sep_to_start else string \n    else:\n        return \"\"\n\n    \ndef training_step(batch, \n                  model, \n                  optimizer, \n                  gradient_norm=1.0, \n                  amp=False, \n                  gradient_accumulation_steps=1, \n                  scaler=None, \n                  device=\"cpu\"):\n    \n    model.train()\n    with autocast(enabled=amp):\n        loss, outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n        targets = get_targets(batch)\n        metrics = calculate_metrics(predictions=outputs, targets=targets, device=device)\n        \n        if gradient_accumulation_steps > 1:\n            loss /= gradient_accumulation_steps\n        \n        if scaler is not None:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n            \n    if gradient_norm > 0:\n        if scaler is not None:\n            scaler.unscale_(optimizer)\n                            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_norm)\n        \n    return loss.detach(), metrics\n\n\ndef optimization_step(model, optimizer, scaler=None):                        \n    if scaler is not None:\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        optimizer.step()\n        \n    model.zero_grad()\n        \n\ndef scheduling_step(scheduler=None, loss=None):\n    if scheduler is not None:\n        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(loss)\n        else:\n            scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:45.936147Z","iopub.execute_input":"2022-04-23T10:46:45.936682Z","iopub.status.idle":"2022-04-23T10:46:45.987774Z","shell.execute_reply.started":"2022-04-23T10:46:45.936635Z","shell.execute_reply":"2022-04-23T10:46:45.987047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    input_ids, attention_mask, targets = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    targets = targets.to(device).float()\n    \n    outputs = model(input_ids, attention_mask)\n    outputs = outputs.sigmoid().squeeze(dim=-1)\n    loss = F.mse_loss(outputs, targets, reduction=\"mean\")\n        \n    return (loss, outputs) if return_outputs else loss\n\n\ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    predictions = predictions.sigmoid().detach().view(-1).to(\"cpu\").float().numpy()\n    targets = targets.view(-1).to(\"cpu\").float().numpy()\n    \n    return dict(pearson=scipy.stats.pearsonr(predictions, targets)[0])\n\n\ndef get_targets(batch):\n    *_, targets = batch\n    return targets\n\n\ndef model_checkpointing(loss, metrics, model, optimizer=None, scheduler=None, step=None, previous_loss=None, previous_metrics=None):\n    is_saved_checkpoint = model_checkpoint(value=loss, \n                                           model=model, \n                                           optimizer=optimizer, \n                                           scheduler=scheduler, \n                                           step=step)\n    return is_saved_checkpoint","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:45.991015Z","iopub.execute_input":"2022-04-23T10:46:45.991214Z","iopub.status.idle":"2022-04-23T10:46:46.003716Z","shell.execute_reply.started":"2022-04-23T10:46:45.99119Z","shell.execute_reply":"2022-04-23T10:46:46.002913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:46.006196Z","iopub.execute_input":"2022-04-23T10:46:46.006907Z","iopub.status.idle":"2022-04-23T10:46:46.018619Z","shell.execute_reply.started":"2022-04-23T10:46:46.006794Z","shell.execute_reply":"2022-04-23T10:46:46.017875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:46.019794Z","iopub.execute_input":"2022-04-23T10:46:46.020109Z","iopub.status.idle":"2022-04-23T10:46:46.031358Z","shell.execute_reply.started":"2022-04-23T10:46:46.020074Z","shell.execute_reply":"2022-04-23T10:46:46.030611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"microsoft/deberta-base\", config_path=None, config_updates={}, reinitialization_layers=0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n                \n                \n        #self.reinit_layers(n=reinitialization_layers, layers=self.model.encoder.layer, std=self.config.initializer_range)\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n    \n    \n    def reinit_layers(self, layers, n=0, std=0.02):\n        if n > 0:\n            for layer in layers[-n:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=std)\n            \n            print(f\"Reinitializated last {n} layers.\")\n                \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:46.032705Z","iopub.execute_input":"2022-04-23T10:46:46.033398Z","iopub.status.idle":"2022-04-23T10:46:46.048295Z","shell.execute_reply.started":"2022-04-23T10:46:46.033361Z","shell.execute_reply":"2022-04-23T10:46:46.047631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"cpc_codes = pd.read_csv(pathes.cpc_codes)\ntrain = pd.read_csv(pathes.train)\ntrain = train.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\n\nif DEBUG:\n    display(train)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:46.05111Z","iopub.execute_input":"2022-04-23T10:46:46.051409Z","iopub.status.idle":"2022-04-23T10:46:46.698977Z","shell.execute_reply.started":"2022-04-23T10:46:46.051372Z","shell.execute_reply":"2022-04-23T10:46:46.698196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation split","metadata":{}},{"cell_type":"code","source":"train[\"score_bin\"] = pd.cut(train[\"score\"], bins=5, labels=False)\n\ntrain = create_folds(data_frame=train, \n                     targets=train[\"score_bin\"].values,\n                     groups=train[\"anchor\"].values,\n                     folds=config.folds, \n                     seed=config.seed, \n                     shuffle=True)\n\nif config.debug:\n    folds_samples_count = train.groupby('fold').size()\n    display(folds_samples_count)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:46.700406Z","iopub.execute_input":"2022-04-23T10:46:46.700663Z","iopub.status.idle":"2022-04-23T10:46:47.136207Z","shell.execute_reply.started":"2022-04-23T10:46:46.700628Z","shell.execute_reply":"2022-04-23T10:46:47.1354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.model.model_path)\ntokenizer_path = os.path.join(config.output_directory, \"tokenizer/\")\ntokenizer_files = tokenizer.save_pretrained(tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:47.137674Z","iopub.execute_input":"2022-04-23T10:46:47.137932Z","iopub.status.idle":"2022-04-23T10:46:50.345253Z","shell.execute_reply.started":"2022-04-23T10:46:47.137898Z","shell.execute_reply":"2022-04-23T10:46:50.344487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation","metadata":{}},{"cell_type":"code","source":"if WANDB:\n    experiment_name = os.environ.get(\"EXPERIMENT_NAME\")\n    group = experiment_name if experiment_name != \"none\" else wandb.util.generate_id()\n\ncv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold {fold}/{config.folds}\", end=\"\\n\"*2)\n    fold_directory = os.path.join(config.output_directory, f\"fold_{fold}\")    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    make_directory(checkpoints_directory)\n    \n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[\"fold\"].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[\"anchor\"].values, \n                            pair_texts=train_fold[\"target\"].values,\n                            contexts=train_fold[\"title\"].values,\n                            targets=train_fold[\"score\"].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    \n    validation_fold = train[train[\"fold\"].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[\"anchor\"].values, \n                                 pair_texts=validation_fold[\"target\"].values,\n                                 contexts=validation_fold[\"title\"].values,\n                                 targets=validation_fold[\"score\"].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=True, \n                                   drop_last=False)\n    \n    print(f\"Validation samples: {len(validation_dataset)}\")\n    \n    model = Model(**config.model)\n    \n    if not os.path.exists(model_config_path): \n        model.config.to_json_file(model_config_path)\n    \n    model_parameters = model.parameters()\n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    training_steps = len(train_loader) * config.epochs\n    \n    if \"scheduler\" in config:\n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(\"warmup\", 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=\"min\", \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=\"checkpoint.pth\", \n                                       num_candidates=1)\n\n\n    if WANDB:\n        wandb.init(group=group, name=f\"fold_{fold}\", config=config)\n    \n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=\"tqdm\")\n    \n    if WANDB:\n        wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f\"Model's path: {model_path}\")\n    \n    validation_fold[\"prediction\"] = validation_outputs.to(\"cpu\").numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n    \n    cv_monitor_value = validation_loss if config.cv_monitor_value == \"loss\" else validation_metrics[config.cv_monitor_value]\n    cv_scores.append(cv_monitor_value)\n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=\"\\n\"*6)\n    \ncv_scores = np.array(cv_scores)\nprint(f\"CV scores: {cv_scores} \")\nprint(f\"CV mean: {cv_scores.mean()}\")\nprint(f\"CV std: {cv_scores.std()}\")\n\noof_data_frame.to_pickle(\"oof.pkl\")\nnp.save(\"cv_scores.npy\", cv_scores)\nconfiguration_path = config.to_json(\"configuration.json\")","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:46:50.34689Z","iopub.execute_input":"2022-04-23T10:46:50.347139Z","iopub.status.idle":"2022-04-23T10:50:09.101579Z","shell.execute_reply.started":"2022-04-23T10:46:50.347106Z","shell.execute_reply":"2022-04-23T10:50:09.100535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}