{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The goal of this notebook is to evaluate how well does the vocabulary of each language model represent the patent data. If a model's tokenizer doesn't contain a word, it will split it into subwords. Therefore, we can look at the distribution of the number of tokens per word. This might be a good signal for whether a model is suitable for this competition. \n\nI created the notebook following a suggestion from the user @hengck23.","metadata":{}},{"cell_type":"code","source":"! pip install -q transformers sentencepiece\n! git clone https://github.com/microsoft/COCO-LM.git\n! cp -r COCO-LM/huggingface/* .","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:18:57.214115Z","iopub.execute_input":"2022-05-27T14:18:57.214371Z","iopub.status.idle":"2022-05-27T14:19:09.498276Z","shell.execute_reply.started":"2022-05-27T14:18:57.214343Z","shell.execute_reply":"2022-05-27T14:19:09.497423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\n\nfrom transformers import AutoTokenizer\nimport transformers.utils.logging\n\ntransformers.utils.logging.disable_progress_bar()\n\nimport transformers\nfrom cocolm.tokenization_cocolm import COCOLMTokenizer\n\n\n\ndf_train = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\n\ndef plot_avg_tokens_per_word(df_train, model_name):\n    df_train = df_train.copy(deep=True)\n    \n    if 'cocolm' in model_name:\n        tokz = COCOLMTokenizer.from_pretrained(model_name)\n    elif 'roberta' in model_name:\n        tokz = transformers.AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n    else:\n        tokz = transformers.AutoTokenizer.from_pretrained(model_name)\n\n        \n    #df_train['num_toks'] = df_train.anchor.apply(lambda x: len(tokz.convert_ids_to_tokens(tokz.encode(x.split(' '), is_split_into_words=True)))-2)\n    df_train['num_toks'] = df_train.anchor.apply(lambda x: len(tokz.convert_ids_to_tokens(tokz.encode(x)))-2)\n    df_train['num_words'] = df_train.anchor.apply(lambda x: len(x.split(' ')))\n    df_train['tok_rate'] = df_train['num_toks'] / df_train['num_words']\n    df_train['tok_rate'].hist()\n\n    df_train['tok_rate'].hist()\n    plt.xlabel('Number of Tokens per Word')\n    plt.ylabel('Frequency')\n    plt.title(f'model: {model_name}')\n    \n    avg_num_toks_per_word = (df_train['num_toks'] / df_train['num_words']).mean()\n    return avg_num_toks_per_word\n\navg_tokens_per_word = {}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T14:24:31.164985Z","iopub.execute_input":"2022-05-27T14:24:31.165257Z","iopub.status.idle":"2022-05-27T14:24:31.254693Z","shell.execute_reply.started":"2022-05-27T14:24:31.165225Z","shell.execute_reply":"2022-05-27T14:24:31.253633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#avg_tokens_per_word['microsoft/cocolm-large'] = plot_avg_tokens_per_word(df_train,'microsoft/cocolm-large')\n#avg_tokens_per_word['bert-base-uncased'] = plot_avg_tokens_per_word(df_train, 'bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:24:34.576124Z","iopub.execute_input":"2022-05-27T14:24:34.576412Z","iopub.status.idle":"2022-05-27T14:24:37.054266Z","shell.execute_reply.started":"2022-05-27T14:24:34.576382Z","shell.execute_reply":"2022-05-27T14:24:37.053571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_tokens_per_word['anferico/bert-for-patents'] = plot_avg_tokens_per_word(df_train,'anferico/bert-for-patents')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T15:21:32.845696Z","iopub.execute_input":"2022-05-24T15:21:32.846074Z","iopub.status.idle":"2022-05-24T15:21:39.789962Z","shell.execute_reply.started":"2022-05-24T15:21:32.84603Z","shell.execute_reply":"2022-05-24T15:21:39.788919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_tokens_per_word['roberta-large'] = plot_avg_tokens_per_word(df_train,'roberta-large')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T15:21:39.791968Z","iopub.execute_input":"2022-05-24T15:21:39.793018Z","iopub.status.idle":"2022-05-24T15:21:46.558398Z","shell.execute_reply.started":"2022-05-24T15:21:39.792957Z","shell.execute_reply":"2022-05-24T15:21:46.557705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_tokens_per_word['microsoft/deberta-v3-small'] = plot_avg_tokens_per_word(df_train,'microsoft/deberta-v3-small')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T15:21:46.559695Z","iopub.execute_input":"2022-05-24T15:21:46.560364Z","iopub.status.idle":"2022-05-24T15:21:55.884092Z","shell.execute_reply.started":"2022-05-24T15:21:46.560331Z","shell.execute_reply":"2022-05-24T15:21:55.883362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_tokens_per_word['microsoft/deberta-v3-large'] = plot_avg_tokens_per_word(df_train,'microsoft/deberta-v3-large')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T15:21:55.885486Z","iopub.execute_input":"2022-05-24T15:21:55.885939Z","iopub.status.idle":"2022-05-24T15:22:05.196157Z","shell.execute_reply.started":"2022-05-24T15:21:55.885906Z","shell.execute_reply":"2022-05-24T15:22:05.194869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_tokens_per_word['allenai/scibert_scivocab_uncased'] = plot_avg_tokens_per_word(df_train,'allenai/scibert_scivocab_uncased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparison of model tokenizers by average number of tokens per word**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame.from_dict(avg_tokens_per_word, orient='index', columns=['avg_tokens_per_word']).sort_values(by='avg_tokens_per_word').plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T15:22:05.197377Z","iopub.execute_input":"2022-05-24T15:22:05.197612Z","iopub.status.idle":"2022-05-24T15:22:05.42991Z","shell.execute_reply.started":"2022-05-24T15:22:05.197584Z","shell.execute_reply":"2022-05-24T15:22:05.428926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's consider how having the target word in the model's vocabulary affects the predictions we need to make in the competition.","metadata":{}},{"cell_type":"code","source":"model_name = 'microsoft/deberta-v3-large'\ntokz = transformers.AutoTokenizer.from_pretrained(model_name) \ndf_oof = pd.read_pickle('../input/pppmdebertaoof/oof_df.pkl')\n\ndf_oof['toks'] = df_oof.target.apply(lambda x: tokz.convert_ids_to_tokens(tokz.encode(x.split(' '), is_split_into_words=True)))\ndf_oof['num_toks'] = df_oof.toks.apply(lambda x: len(x)-2)\ndf_oof['num_words'] = df_oof.target.apply(lambda x: len(x.split(' ')))\ndf_oof['tok_rate'] = df_oof['num_toks'] / df_oof['num_words']\ndf_oof['abs_err'] = np.abs((df_oof['score']-df_oof['score'].mean())/df_oof['score'].var() - (df_oof['pred']-df_oof['pred'].mean())/df_oof.pred.var() )\ndf_oof['tok_rate_adj'] = df_oof['tok_rate'].apply(lambda x: 2 if x > 1 else 1)\n\n\ndf_oof[df_oof.tok_rate_adj == 1].abs_err.hist(ax=plt.gca(), alpha=0.5, density=True, color='b', bins=50)\n#plt.hold(True)\ndf_oof[df_oof.tok_rate_adj == 2].abs_err.hist(ax=plt.gca(), alpha=0.5, density=True, color='r', bins=50)\nplt.xlabel('Standardized Error')\nplt.ylabel('Density')\nplt.legend(['One token per word', 'More than one token per word'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see from the plot that targets with one token per word have higher scores than targets with more than one token per word. Now let's look at the statistics:","metadata":{}},{"cell_type":"code","source":"df_summary = df_oof.groupby('tok_rate_adj').agg({'abs_err': ['mean', 'var', 'max', 'count']})#.plot(y=('abs_err', 'mean'), kind='bar')\ndf_summary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here tok_rate_adj=2 is the group of samples with more than one token per word, you can see that it has a higher average error and also higher error variance.","metadata":{}}]}