{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Iterate like a grandmaster","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets\n\nfrom torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\nfrom transformers import DataCollatorWithPadding  \nimport os\nimport logging.config\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom fastai.imports import *\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True \n    \ndef get_logger(filename='train'):    \n    logging.config.dictConfig({\n        'version': 1,\n        'disable_existing_loggers': True\n    })\n    \n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    \n    log_format = \"%(asctime)s | %(levelname)s | %(message)s\"\n    \n    handler1.setFormatter(Formatter(log_format))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(log_format))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n    \nseed_everything()\nif not 'LOGGER' in globals():\n    LOGGER = get_logger()\n\nlr,bs = 2e-5,32\nwd,epochs = 0.01,5\n\nmodel_nm = '../input/deberta-v3-large/deberta-v3-large'\n\ninps = \"anchor\",\"target\",\"context\"\n\ntest_data_path = '../input/us-patent-phrase-to-phrase-matching/test.csv'\ntrain_data_path = '../input/us-patent-phrase-to-phrase-matching/train.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:30:45.842406Z","iopub.execute_input":"2022-05-10T18:30:45.843224Z","iopub.status.idle":"2022-05-10T18:31:25.003131Z","shell.execute_reply.started":"2022-05-10T18:30:45.84318Z","shell.execute_reply":"2022-05-10T18:31:25.002012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Prep Methods","metadata":{}},{"cell_type":"code","source":"context_mapping = {\n    \"A\": \"Human Necessities\",\n    \"B\": \"Operations and Transport\",\n    \"C\": \"Chemistry and Metallurgy\",\n    \"D\": \"Textiles\",\n    \"E\": \"Fixed Constructions\",\n    \"F\": \"Mechanical Engineering\",\n    \"G\": \"Physics\",\n    \"H\": \"Electricity\",\n    \"Y\": \"Emerging Cross-Sectional Technologies\",\n}\n\ndef process_context(df):\n    cpc_codes_df = pd.read_csv('../input/cpc-codes/titles.csv')\n    \n    df = df.copy()\n    df['context_category'] = df.context.apply(lambda x: context_mapping.get(x[0], ''))\n    df = df.merge(cpc_codes_df, left_on='context', right_on='code', how='left')        \n    df.title = df.title.apply(lambda x: x.lower())\n    df.context_category = df.context_category.apply(lambda x: x.lower())\n    df = df.drop('code', axis=1)\n    df['class'] = df['class'].astype('int')    \n    return df\n\ndef get_data_df(df_path):\n    df = pd.read_csv(df_path)\n    df = process_context(df)\n    df['section'] = df.context.str[0]\n    sep = \" [s] \"\n    df['sectok'] = '[' + df.section + ']'\n    df['inputs'] = df.sectok + sep + df.context_category + sep + df.title + sep + df.anchor + sep + df.target\n    #df['inputs'] = df.inputs.str.lower()\n    return df\n\ndef get_train_test_split(df):\n    anchors = df.anchor.unique()\n    \n    np.random.shuffle(anchors)\n\n    val_prop = 0.25\n    val_sz = int(len(anchors)*val_prop)\n    val_anchors = anchors[:val_sz]\n\n    is_val = np.isin(df.anchor, val_anchors)\n    idxs = np.arange(len(df))\n    val_idxs = idxs[ is_val]\n    trn_idxs = idxs[~is_val]\n    return trn_idxs, val_idxs\n\ndef tok_func(x): \n    return tokz(x[\"inputs\"], padding=True, truncation=True)\n\ndef get_dds(df, trn_idxs=None, val_idxs=None):    \n    \n    ds = Dataset.from_pandas(df)\n    if 'score' in df.columns:    \n        ds = ds.rename_column('score', 'label')\n    tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('__index_level_0__', 'inputs','id','sectok', 'context_category', 'title', 'section', 'class', 'subclass', 'group', 'main_group'))\n    dataset_dict = {\"train\":tok_ds.select(trn_idxs), \"val\": tok_ds.select(val_idxs)} if trn_idxs is not None else {'test': tok_ds}\n    return DatasetDict(dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:38:30.302232Z","iopub.execute_input":"2022-05-10T18:38:30.302526Z","iopub.status.idle":"2022-05-10T18:38:30.347449Z","shell.execute_reply.started":"2022-05-10T18:38:30.302497Z","shell.execute_reply":"2022-05-10T18:38:30.345713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit and Inference Methods","metadata":{}},{"cell_type":"code","source":"def corr(data): \n    x, y = data\n    return {'pearson': np.corrcoef(np.squeeze(x), np.squeeze(y))[0][1]}\n\ndef get_model():\n    return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n\ndef get_tokz():\n    return AutoTokenizer.from_pretrained(model_nm)\n\ndef get_trainer(dds, model, tokz):\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy = \"epoch\", save_strategy = \"epoch\", logging_strategy= \"epoch\",\n        per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, optim=\"adamw_torch\", report_to='none', load_best_model_at_end=True,\n        metric_for_best_model=\"pearson\",\n        greater_is_better=True,\n        save_total_limit=1)    \n    \n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['val'],\n                   tokenizer=tokz, compute_metrics=corr)\n\ndef model_predict(dds, tokenizer):\n    test_data_loader = DataLoader(dds['test'], batch_size=32, collate_fn=DataCollatorWithPadding(tokenizer)) \n    data_iter = iter(test_data_loader)\n\n    all_preds = []\n    for inputs in data_iter:        \n        inputs = {k:v.to('cuda') for k,v in inputs.items()} # TODO: get device from config\n        with torch.no_grad():            \n            batch_preds = model(**inputs).logits.detach().cpu().tolist()  \n            all_preds += batch_preds\n\n    all_preds = np.array(all_preds).reshape(-1)\n    return all_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:32:57.417806Z","iopub.execute_input":"2022-05-10T18:32:57.418346Z","iopub.status.idle":"2022-05-10T18:32:57.434303Z","shell.execute_reply.started":"2022-05-10T18:32:57.418306Z","shell.execute_reply":"2022-05-10T18:32:57.430403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run it","metadata":{}},{"cell_type":"code","source":"train_df = get_data_df(train_data_path)\ntest_df = get_data_df(test_data_path)\n\ntokz = get_tokz()\nsectoks = list(train_df.sectok.unique())\ntokz.add_special_tokens({'additional_special_tokens': sectoks})\n\ntrn_idxs, val_idxs = get_train_test_split(train_df)\n\ndds = get_dds(train_df, trn_idxs, val_idxs)\ntest_dds = get_dds(test_df)\n\nmodel = get_model()\nmodel.resize_token_embeddings(len(tokz))\n    \ntrainer = get_trainer(dds, model=model,  tokz=tokz)\ntrainer.train()\n\nsubmission_preds = model_predict(test_dds, tokz)\n\nsubmission = datasets.Dataset.from_dict({\n    'id': pd.read_csv(test_data_path).id.values.tolist(),\n    'score': submission_preds,\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T00:59:14.706449Z","iopub.execute_input":"2022-04-18T00:59:14.708194Z","iopub.status.idle":"2022-04-18T01:02:00.69774Z","shell.execute_reply.started":"2022-04-18T00:59:14.708154Z","shell.execute_reply":"2022-04-18T01:02:00.69687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df\n\n#tokz = AutoTokenizer.from_pretrained('../input/condensed/outputs/checkpoint-4275')\n#test_df = get_data_df(test_data_path)\n#test_dds = get_dds(test_df)\n#model = AutoModelForSequenceClassification.from_pretrained('../input/condensed/outputs/checkpoint-4275', num_labels=1).to('cuda')\n#submission_preds = model_predict(test_dds, tokz)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:53:41.761669Z","iopub.execute_input":"2022-05-10T18:53:41.761962Z","iopub.status.idle":"2022-05-10T18:53:41.79512Z","shell.execute_reply.started":"2022-05-10T18:53:41.761933Z","shell.execute_reply":"2022-05-10T18:53:41.794044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent_missing = test_df.isnull().sum() * 100 / len(test_df)\nmissing_value_df = pd.DataFrame({'column_name': test_df.columns,\n                                 'percent_missing': percent_missing})\n\nmissing_value_df.sort_values('percent_missing', inplace=True)\nmissing_value_df","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:42:29.2681Z","iopub.execute_input":"2022-05-10T18:42:29.268394Z","iopub.status.idle":"2022-05-10T18:42:29.286515Z","shell.execute_reply.started":"2022-05-10T18:42:29.268361Z","shell.execute_reply":"2022-05-10T18:42:29.285451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}