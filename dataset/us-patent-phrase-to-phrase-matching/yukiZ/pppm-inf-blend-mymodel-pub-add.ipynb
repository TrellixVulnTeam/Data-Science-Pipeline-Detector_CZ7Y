{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInf\n</b></h1> \n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nLogs\n</b></h2> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nLibs\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport glob\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom dataclasses import dataclass\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T02:40:29.005004Z","iopub.execute_input":"2022-06-20T02:40:29.005375Z","iopub.status.idle":"2022-06-20T02:40:36.346659Z","shell.execute_reply.started":"2022-06-20T02:40:29.005278Z","shell.execute_reply":"2022-06-20T02:40:36.345942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True    \n    torch.backends.cudnn.benchmark = False\n\n    \ndef inference_fn(test_loader, model, device, is_sigmoid=True):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n        if is_sigmoid == True:\n            preds.append(output.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(output.to('cpu').numpy())\n\n    return np.concatenate(preds)    \n    \n\ndef upd_outputs(data, is_trim=False, is_minmax=False, is_reshape=False):\n    min_max_scaler = MinMaxScaler()\n    \n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T02:40:36.348292Z","iopub.execute_input":"2022-06-20T02:40:36.348545Z","iopub.status.idle":"2022-06-20T02:40:36.359262Z","shell.execute_reply.started":"2022-06-20T02:40:36.348515Z","shell.execute_reply":"2022-06-20T02:40:36.358555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.precision', 4)\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:36.360581Z","iopub.execute_input":"2022-06-20T02:40:36.361048Z","iopub.status.idle":"2022-06-20T02:40:36.428914Z","shell.execute_reply.started":"2022-06-20T02:40:36.361013Z","shell.execute_reply":"2022-06-20T02:40:36.428144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"competition_dir = \"../input/us-patent-phrase-to-phrase-matching/\"\n\nsubmission = pd.read_csv(competition_dir+'sample_submission.csv')\ntest_origin = pd.read_csv(competition_dir+'test.csv')\ntest_origin.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:36.431255Z","iopub.execute_input":"2022-06-20T02:40:36.431758Z","iopub.status.idle":"2022-06-20T02:40:36.46226Z","shell.execute_reply.started":"2022-06-20T02:40:36.431722Z","shell.execute_reply":"2022-06-20T02:40:36.461493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInference TorchModels\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-104MixFold-TRAIN-deberta-v3-large-ModelExtendv1-SplitScore-All\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-104mixfold-20220613115117/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:36.463595Z","iopub.execute_input":"2022-06-20T02:40:36.463856Z","iopub.status.idle":"2022-06-20T02:40:37.187968Z","shell.execute_reply.started":"2022-06-20T02:40:36.463824Z","shell.execute_reply":"2022-06-20T02:40:37.187217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:37.190919Z","iopub.execute_input":"2022-06-20T02:40:37.191131Z","iopub.status.idle":"2022-06-20T02:40:37.217152Z","shell.execute_reply.started":"2022-06-20T02:40:37.191107Z","shell.execute_reply":"2022-06-20T02:40:37.216473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:37.218348Z","iopub.execute_input":"2022-06-20T02:40:37.21866Z","iopub.status.idle":"2022-06-20T02:40:37.243586Z","shell.execute_reply.started":"2022-06-20T02:40:37.218624Z","shell.execute_reply":"2022-06-20T02:40:37.242809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_104Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_104Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_104Mix = [upd_outputs(x, is_reshape=True) for x in D_104Mix]\nD_104Mix = pd.DataFrame(D_104Mix).T\n\nD_104Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:40:37.245032Z","iopub.execute_input":"2022-06-20T02:40:37.245407Z","iopub.status.idle":"2022-06-20T02:42:58.8234Z","shell.execute_reply.started":"2022-06-20T02:40:37.245304Z","shell.execute_reply":"2022-06-20T02:42:58.822665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:42:58.824667Z","iopub.execute_input":"2022-06-20T02:42:58.82585Z","iopub.status.idle":"2022-06-20T02:42:58.990692Z","shell.execute_reply.started":"2022-06-20T02:42:58.825803Z","shell.execute_reply":"2022-06-20T02:42:58.989972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nChanged\n    D-20031MixFold-TRAIN-electra-large-discriminator-ModelExtendv1-SplitScore-s5-e10-f1-Copy14 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/electrav1/\"\n    config_path=path+'config.pth'\n    model=\"google/electra-large-discriminator\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(\"../input/pppm-dc-d-20031mixfold-20220615092815/tokenizer/\")\n\ncontext_mapping = torch.load(\"../input/pppm-dc-d-20031mixfold-20220615092815/cpc_texts.pth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:42:58.994492Z","iopub.execute_input":"2022-06-20T02:42:58.994752Z","iopub.status.idle":"2022-06-20T02:42:59.100714Z","shell.execute_reply.started":"2022-06-20T02:42:58.994719Z","shell.execute_reply":"2022-06-20T02:42:59.100018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:42:59.102024Z","iopub.execute_input":"2022-06-20T02:42:59.102255Z","iopub.status.idle":"2022-06-20T02:42:59.121591Z","shell.execute_reply.started":"2022-06-20T02:42:59.102223Z","shell.execute_reply":"2022-06-20T02:42:59.120654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:42:59.122899Z","iopub.execute_input":"2022-06-20T02:42:59.123385Z","iopub.status.idle":"2022-06-20T02:42:59.145819Z","shell.execute_reply.started":"2022-06-20T02:42:59.123346Z","shell.execute_reply":"2022-06-20T02:42:59.145107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_20031Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f'../input/electrav1/google-electra-large-discriminator_fold{fold}_best/google-electra-large-discriminator_fold{fold}_best.pth'\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_20031Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_20031Mix = [upd_outputs(x, is_reshape=True) for x in D_20031Mix]\nD_20031Mix = pd.DataFrame(D_20031Mix).T\n\nD_20031Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:42:59.147403Z","iopub.execute_input":"2022-06-20T02:42:59.148382Z","iopub.status.idle":"2022-06-20T02:44:15.441427Z","shell.execute_reply.started":"2022-06-20T02:42:59.148345Z","shell.execute_reply":"2022-06-20T02:44:15.440715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:15.443108Z","iopub.execute_input":"2022-06-20T02:44:15.443585Z","iopub.status.idle":"2022-06-20T02:44:15.597667Z","shell.execute_reply.started":"2022-06-20T02:44:15.443545Z","shell.execute_reply":"2022-06-20T02:44:15.596164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-1123MixFold-TRAIN-deberta-v3-base-ModelExtendv1-SplitScore-s5-e10 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-1123mixfold-20220614054320/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-base\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:15.639509Z","iopub.execute_input":"2022-06-20T02:44:15.639926Z","iopub.status.idle":"2022-06-20T02:44:16.358708Z","shell.execute_reply.started":"2022-06-20T02:44:15.639887Z","shell.execute_reply":"2022-06-20T02:44:16.357932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:16.360023Z","iopub.execute_input":"2022-06-20T02:44:16.361674Z","iopub.status.idle":"2022-06-20T02:44:16.384134Z","shell.execute_reply.started":"2022-06-20T02:44:16.361642Z","shell.execute_reply":"2022-06-20T02:44:16.383464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:16.385197Z","iopub.execute_input":"2022-06-20T02:44:16.385626Z","iopub.status.idle":"2022-06-20T02:44:16.405857Z","shell.execute_reply.started":"2022-06-20T02:44:16.385589Z","shell.execute_reply":"2022-06-20T02:44:16.405195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_1123Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_1123Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_1123Mix = [upd_outputs(x, is_reshape=True) for x in D_1123Mix]\nD_1123Mix = pd.DataFrame(D_1123Mix).T\n\nD_1123Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:16.407108Z","iopub.execute_input":"2022-06-20T02:44:16.407337Z","iopub.status.idle":"2022-06-20T02:45:11.468315Z","shell.execute_reply.started":"2022-06-20T02:44:16.407306Z","shell.execute_reply":"2022-06-20T02:45:11.467529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-1112MixFold-TRAIN-deberta-v3-large-ModelExtendv1-SplitAnchor-s5-e10\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-1112mixfold-20220614045637/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:11.469509Z","iopub.execute_input":"2022-06-20T02:45:11.469941Z","iopub.status.idle":"2022-06-20T02:45:12.222589Z","shell.execute_reply.started":"2022-06-20T02:45:11.4699Z","shell.execute_reply":"2022-06-20T02:45:12.221736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:12.224014Z","iopub.execute_input":"2022-06-20T02:45:12.224265Z","iopub.status.idle":"2022-06-20T02:45:12.245793Z","shell.execute_reply.started":"2022-06-20T02:45:12.224229Z","shell.execute_reply":"2022-06-20T02:45:12.244893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:12.246787Z","iopub.execute_input":"2022-06-20T02:45:12.247453Z","iopub.status.idle":"2022-06-20T02:45:12.271286Z","shell.execute_reply.started":"2022-06-20T02:45:12.247406Z","shell.execute_reply":"2022-06-20T02:45:12.270449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_1112Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_1112Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_1112Mix = [upd_outputs(x, is_reshape=True) for x in D_1112Mix]\nD_1112Mix = pd.DataFrame(D_1112Mix).T\n\nD_1112Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:12.272618Z","iopub.execute_input":"2022-06-20T02:45:12.273086Z","iopub.status.idle":"2022-06-20T02:47:20.603413Z","shell.execute_reply.started":"2022-06-20T02:45:12.273044Z","shell.execute_reply":"2022-06-20T02:47:20.602723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:47:20.604796Z","iopub.execute_input":"2022-06-20T02:47:20.605048Z","iopub.status.idle":"2022-06-20T02:47:20.753414Z","shell.execute_reply.started":"2022-06-20T02:47:20.605012Z","shell.execute_reply":"2022-06-20T02:47:20.752604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nG-100MixFold---LB:0.8243\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-g-100mixfold-20220617152024/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:47:20.754878Z","iopub.execute_input":"2022-06-20T02:47:20.755157Z","iopub.status.idle":"2022-06-20T02:47:20.941429Z","shell.execute_reply.started":"2022-06-20T02:47:20.755123Z","shell.execute_reply":"2022-06-20T02:47:20.940674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:47:20.943214Z","iopub.execute_input":"2022-06-20T02:47:20.943535Z","iopub.status.idle":"2022-06-20T02:47:20.964507Z","shell.execute_reply.started":"2022-06-20T02:47:20.943497Z","shell.execute_reply":"2022-06-20T02:47:20.963617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:47:20.965716Z","iopub.execute_input":"2022-06-20T02:47:20.966036Z","iopub.status.idle":"2022-06-20T02:47:20.990497Z","shell.execute_reply.started":"2022-06-20T02:47:20.966Z","shell.execute_reply":"2022-06-20T02:47:20.989803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G_100MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    G_100MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nG_100MixFold = [upd_outputs(x, is_reshape=True) for x in G_100MixFold]\nG_100MixFold = pd.DataFrame(G_100MixFold).T\n\nG_100MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:47:20.991697Z","iopub.execute_input":"2022-06-20T02:47:20.992013Z","iopub.status.idle":"2022-06-20T02:49:18.061844Z","shell.execute_reply.started":"2022-06-20T02:47:20.991977Z","shell.execute_reply":"2022-06-20T02:49:18.061095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:49:18.066846Z","iopub.execute_input":"2022-06-20T02:49:18.067052Z","iopub.status.idle":"2022-06-20T02:49:18.214462Z","shell.execute_reply.started":"2022-06-20T02:49:18.067025Z","shell.execute_reply":"2022-06-20T02:49:18.213746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nF-10xMixFold---0.84976\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-f-10xmixfold-20220618121926/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v2-xlarge\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:49:18.215841Z","iopub.execute_input":"2022-06-20T02:49:18.216315Z","iopub.status.idle":"2022-06-20T02:49:18.943219Z","shell.execute_reply.started":"2022-06-20T02:49:18.216278Z","shell.execute_reply":"2022-06-20T02:49:18.942438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:49:18.944669Z","iopub.execute_input":"2022-06-20T02:49:18.944962Z","iopub.status.idle":"2022-06-20T02:49:18.968474Z","shell.execute_reply.started":"2022-06-20T02:49:18.944914Z","shell.execute_reply":"2022-06-20T02:49:18.967722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:49:18.969702Z","iopub.execute_input":"2022-06-20T02:49:18.970359Z","iopub.status.idle":"2022-06-20T02:49:18.990864Z","shell.execute_reply.started":"2022-06-20T02:49:18.970321Z","shell.execute_reply":"2022-06-20T02:49:18.989994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_10xMixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    F_10xMixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nF_10xMixFold = [upd_outputs(x, is_reshape=True) for x in F_10xMixFold]\nF_10xMixFold = pd.DataFrame(F_10xMixFold).T\n\nF_10xMixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:49:18.992481Z","iopub.execute_input":"2022-06-20T02:49:18.99287Z","iopub.status.idle":"2022-06-20T02:53:29.040623Z","shell.execute_reply.started":"2022-06-20T02:49:18.992834Z","shell.execute_reply":"2022-06-20T02:53:29.039747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:53:29.042198Z","iopub.execute_input":"2022-06-20T02:53:29.042469Z","iopub.status.idle":"2022-06-20T02:53:29.197015Z","shell.execute_reply.started":"2022-06-20T02:53:29.042431Z","shell.execute_reply":"2022-06-20T02:53:29.19626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nI-100-TRAIN-albert-xxlarge-v2-ModelExtendAttention-SplitScore-s5-e10-f012345 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-i-100mixfold-20220618221837/\"\n    config_path=path+'config.pth'\n    model=\"albert-xxlarge-v2\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:53:29.198089Z","iopub.execute_input":"2022-06-20T02:53:29.198833Z","iopub.status.idle":"2022-06-20T02:53:29.311828Z","shell.execute_reply.started":"2022-06-20T02:53:29.198785Z","shell.execute_reply":"2022-06-20T02:53:29.311106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:53:29.313246Z","iopub.execute_input":"2022-06-20T02:53:29.31349Z","iopub.status.idle":"2022-06-20T02:53:29.331286Z","shell.execute_reply.started":"2022-06-20T02:53:29.313457Z","shell.execute_reply":"2022-06-20T02:53:29.330374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:53:29.332745Z","iopub.execute_input":"2022-06-20T02:53:29.33326Z","iopub.status.idle":"2022-06-20T02:53:29.354918Z","shell.execute_reply.started":"2022-06-20T02:53:29.333221Z","shell.execute_reply":"2022-06-20T02:53:29.354129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"I_100MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    I_100MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nI_100MixFold = [upd_outputs(x, is_reshape=True) for x in I_100MixFold]\nI_100MixFold = pd.DataFrame(I_100MixFold).T\n\nI_100MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:53:29.356233Z","iopub.execute_input":"2022-06-20T02:53:29.356495Z","iopub.status.idle":"2022-06-20T02:54:46.483505Z","shell.execute_reply.started":"2022-06-20T02:53:29.356462Z","shell.execute_reply":"2022-06-20T02:54:46.482753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:54:46.484977Z","iopub.execute_input":"2022-06-20T02:54:46.485238Z","iopub.status.idle":"2022-06-20T02:54:46.63839Z","shell.execute_reply.started":"2022-06-20T02:54:46.485201Z","shell.execute_reply":"2022-06-20T02:54:46.637647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nL-101-TRAIN-bert-for-patents-ModelExtendv1-SplitScore-s5 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-l-101mixfold-20220620013202/\"\n    config_path=path+'config.pth'\n    model=\"anferico/bert-for-patents\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:54:46.63998Z","iopub.execute_input":"2022-06-20T02:54:46.640347Z","iopub.status.idle":"2022-06-20T02:54:46.710817Z","shell.execute_reply.started":"2022-06-20T02:54:46.640306Z","shell.execute_reply":"2022-06-20T02:54:46.710087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:54:46.712158Z","iopub.execute_input":"2022-06-20T02:54:46.712408Z","iopub.status.idle":"2022-06-20T02:54:46.733735Z","shell.execute_reply.started":"2022-06-20T02:54:46.712374Z","shell.execute_reply":"2022-06-20T02:54:46.732976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:54:46.73471Z","iopub.execute_input":"2022-06-20T02:54:46.735148Z","iopub.status.idle":"2022-06-20T02:54:46.756262Z","shell.execute_reply.started":"2022-06-20T02:54:46.735112Z","shell.execute_reply":"2022-06-20T02:54:46.755627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L_101MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    L_101MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nL_101MixFold = [upd_outputs(x, is_reshape=True) for x in L_101MixFold]\nL_101MixFold = pd.DataFrame(L_101MixFold).T\n\nL_101MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:54:46.758127Z","iopub.execute_input":"2022-06-20T02:54:46.758327Z","iopub.status.idle":"2022-06-20T02:56:15.837791Z","shell.execute_reply.started":"2022-06-20T02:54:46.758304Z","shell.execute_reply":"2022-06-20T02:56:15.836716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:15.839175Z","iopub.execute_input":"2022-06-20T02:56:15.83976Z","iopub.status.idle":"2022-06-20T02:56:15.998428Z","shell.execute_reply.started":"2022-06-20T02:56:15.839711Z","shell.execute_reply":"2022-06-20T02:56:15.997605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInference TransformersModels\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nPubKernel\n</b></h2>\n\nhttps://www.kaggle.com/code/surilee/inference-bert-for-uspatents-deepshare/notebook\nLB:0.8392\n\nhttps://www.kaggle.com/code/renokan/2-deberta-1-roberta-analysis-and-using/notebook","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           truncation=True)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg        \n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.text[item])\n        \n        return inputs\n   \n    \nclass CustomModel(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, inputs):\n        output = self.base(**inputs)\n\n        return output[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:15.999995Z","iopub.execute_input":"2022-06-20T02:56:16.000671Z","iopub.status.idle":"2022-06-20T02:56:16.01204Z","shell.execute_reply.started":"2022-06-20T02:56:16.000632Z","shell.execute_reply":"2022-06-20T02:56:16.011332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:16.013417Z","iopub.execute_input":"2022-06-20T02:56:16.014308Z","iopub.status.idle":"2022-06-20T02:56:16.025195Z","shell.execute_reply.started":"2022-06-20T02:56:16.014269Z","shell.execute_reply":"2022-06-20T02:56:16.024388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_path='../input/deberta-v3-large/deberta-v3-large'\n    batch_size=CUSTOM_BATCH\n    num_workers=2\n    max_len=130\n    trn_fold=[0, 1, 2, 3]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n\ncontext_mapping = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:16.027089Z","iopub.execute_input":"2022-06-20T02:56:16.027268Z","iopub.status.idle":"2022-06-20T02:56:16.75491Z","shell.execute_reply.started":"2022-06-20T02:56:16.027246Z","shell.execute_reply":"2022-06-20T02:56:16.754166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest.reset_index(inplace=True)\ntest = test.merge(titles, left_on='context', right_on='code')\ntest.sort_values(by='index', inplace=True)\ntest.drop(columns='index', inplace=True)\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:16.756199Z","iopub.execute_input":"2022-06-20T02:56:16.756535Z","iopub.status.idle":"2022-06-20T02:56:17.508057Z","shell.execute_reply.started":"2022-06-20T02:56:16.756496Z","shell.execute_reply":"2022-06-20T02:56:17.507243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pub_deberta_predicts_1 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size, shuffle=False,\n                             num_workers=CFG.num_workers,\n                             pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = CustomModel(CFG.model_path)    \n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_dataloader, model, DEVICE, is_sigmoid=False)\n    \n    pub_deberta_predicts_1.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:56:17.509441Z","iopub.execute_input":"2022-06-20T02:56:17.509763Z","iopub.status.idle":"2022-06-20T02:58:10.346396Z","shell.execute_reply.started":"2022-06-20T02:56:17.50972Z","shell.execute_reply":"2022-06-20T02:58:10.345237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------- inference_fn([...], is_sigmoid=False)\npub_deberta_predicts_1 = [upd_outputs(x, is_minmax=True, is_reshape=True) for x in pub_deberta_predicts_1]\npub_deberta_predicts_1 = pd.DataFrame(pub_deberta_predicts_1).T\n\npub_deberta_predicts_1.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.347944Z","iopub.execute_input":"2022-06-20T02:58:10.34821Z","iopub.status.idle":"2022-06-20T02:58:10.387445Z","shell.execute_reply.started":"2022-06-20T02:58:10.34817Z","shell.execute_reply":"2022-06-20T02:58:10.386634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.388778Z","iopub.execute_input":"2022-06-20T02:58:10.389555Z","iopub.status.idle":"2022-06-20T02:58:10.623394Z","shell.execute_reply.started":"2022-06-20T02:58:10.389515Z","shell.execute_reply":"2022-06-20T02:58:10.622579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nEnsemble\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"IF_ENSEMBLE=True","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.624901Z","iopub.execute_input":"2022-06-20T02:58:10.625552Z","iopub.status.idle":"2022-06-20T02:58:10.633196Z","shell.execute_reply.started":"2022-06-20T02:58:10.625508Z","shell.execute_reply":"2022-06-20T02:58:10.632287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nNo Ensemble\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"# if not IF_ENSEMBLE:\n#     print(\"# ---------------------------------------------------- #\")\n#     print(\"# Prediction Single Model\")\n#     print(\"# ---------------------------------------------------- #\")\n    \n#     PREDICTION_1 = D_104Mix\n#     PREDICTION_KEY = 'D_104Mix'\n    \n#     # --------------------------------------------------------------- #\n#     all_predictions = pd.concat(\n#         [PREDICTION_1],\n#         keys=[PREDICTION_KEY],\n#         axis=1\n#     )\n\n#     all_predictions.head(10) \\\n#         .assign(mean=lambda x: x.mean(axis=1)) \\\n#             .style.background_gradient(cmap=cm, axis=1)\n    \n#     # --------------------------------------------------------------- #\n#     all_mean = pd.DataFrame({\n#         'PREDICTION_KEY': PREDICTION_1.mean(axis=1)\n#     })\n\n#     all_mean.head(10) \\\n#         .assign(mean=lambda x: x.mean(axis=1)) \\\n#             .style.highlight_max(axis=1, props=props_param)\n#     # --------------------------------------------------------------- #\n#     # === N1 ===\n#     weights_ = [1.00]\n#     final_predictions = all_mean.mul(weights_).sum(axis=1)\n\n#     # === N2 ===\n#     # final_predictions = all_mean.median(axis=1)\n#     # final_predictions = all_mean.mean(axis=1)\n\n#     # === N3 ===\n#     # final_predictions = all_predictions.mean(axis=1)\n\n#     # === N4 ===\n#     # combs = pd.DataFrame({\n#     #     'deberta_1': deberta_predicts_1.mean(axis=1),\n#     #     'deb_2+rob': (deberta_predicts_2.mean(axis=1) * 0.666) \\\n#     #                     + (roberta_predicts.mean(axis=1) * 0.333)\n#     # })\n#     # display(combs.head())\n#     # final_predictions = combs.median(axis=1)\n#     # final_predictions = combs.mean(axis=1)\n\n#     final_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.635414Z","iopub.execute_input":"2022-06-20T02:58:10.635763Z","iopub.status.idle":"2022-06-20T02:58:10.644832Z","shell.execute_reply.started":"2022-06-20T02:58:10.635727Z","shell.execute_reply":"2022-06-20T02:58:10.644097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nUse Ensemble\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    all_predictions = pd.concat(\n        [D_104Mix,\n         D_20031Mix,\n         D_1123Mix,\n         D_1112Mix,\n         pub_deberta_predicts_1,\n         G_100MixFold,\n         F_10xMixFold,\n         I_100MixFold,\n         L_101MixFold\n        ],\n        keys=['D_104Mix',\n              'D_20031Mix',\n              'D_1123Mix',\n              'D_1112Mix',\n              'pub_deberta_predicts_1',\n              'G_100MixFold',\n              'F_10xMixFold',\n              'I_100MixFold',\n              'L_101MixFold'\n             ],\n        axis=1\n    )\n\n    all_predictions.head(10) \\\n        .assign(mean=lambda x: x.mean(axis=1)) \\\n            .style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.646126Z","iopub.execute_input":"2022-06-20T02:58:10.646654Z","iopub.status.idle":"2022-06-20T02:58:10.675483Z","shell.execute_reply.started":"2022-06-20T02:58:10.646615Z","shell.execute_reply":"2022-06-20T02:58:10.674729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    all_mean = pd.DataFrame({\n        'D_104Mix': D_104Mix.mean(axis=1),\n        'D_20031Mix': D_20031Mix.mean(axis=1),\n        'D_1123Mix': D_1123Mix.mean(axis=1),\n        'D_1112Mix': D_1112Mix.mean(axis=1),\n        'pub_deberta_predicts_1': pub_deberta_predicts_1.mean(axis=1),\n        'G_100MixFold': G_100MixFold.mean(axis=1),\n        'F_10xMixFold': F_10xMixFold.mean(axis=1),\n        'I_100MixFold': I_100MixFold.mean(axis=1),\n        'L_101MixFold': L_101MixFold.mean(axis=1)\n    })\n\n    all_mean.head(10) \\\n        .assign(mean=lambda x: x.mean(axis=1)) \\\n            .style.highlight_max(axis=1, props=props_param)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.676926Z","iopub.execute_input":"2022-06-20T02:58:10.67727Z","iopub.status.idle":"2022-06-20T02:58:10.690702Z","shell.execute_reply.started":"2022-06-20T02:58:10.677236Z","shell.execute_reply":"2022-06-20T02:58:10.689616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    # === N1 ===\n    weights_ = [0.35, 0.16, 0.08, 0.05, 0.16, 0.05, 0.05, 0.05, 0.05]\n    final_predictions = all_mean.mul(weights_).sum(axis=1)\n\n    # === N2 ===\n    # final_predictions = all_mean.median(axis=1)\n    # final_predictions = all_mean.mean(axis=1)\n\n    # === N3 ===\n    # final_predictions = all_predictions.mean(axis=1)\n\n    # === N4 ===\n    # combs = pd.DataFrame({\n    #     'deberta_1': deberta_predicts_1.mean(axis=1),\n    #     'deb_2+rob': (deberta_predicts_2.mean(axis=1) * 0.666) \\\n    #                     + (roberta_predicts.mean(axis=1) * 0.333)\n    # })\n    # display(combs.head())\n    # final_predictions = combs.median(axis=1)\n    # final_predictions = combs.mean(axis=1)\n\n    final_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.692185Z","iopub.execute_input":"2022-06-20T02:58:10.693136Z","iopub.status.idle":"2022-06-20T02:58:10.701128Z","shell.execute_reply.started":"2022-06-20T02:58:10.693089Z","shell.execute_reply":"2022-06-20T02:58:10.700011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nSubmission\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_origin['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.702627Z","iopub.execute_input":"2022-06-20T02:58:10.703581Z","iopub.status.idle":"2022-06-20T02:58:10.720701Z","shell.execute_reply.started":"2022-06-20T02:58:10.703545Z","shell.execute_reply":"2022-06-20T02:58:10.719634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:58:10.722191Z","iopub.execute_input":"2022-06-20T02:58:10.722639Z","iopub.status.idle":"2022-06-20T02:58:10.73826Z","shell.execute_reply.started":"2022-06-20T02:58:10.722592Z","shell.execute_reply":"2022-06-20T02:58:10.737551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nEOF\n</b></h1> ","metadata":{}}]}