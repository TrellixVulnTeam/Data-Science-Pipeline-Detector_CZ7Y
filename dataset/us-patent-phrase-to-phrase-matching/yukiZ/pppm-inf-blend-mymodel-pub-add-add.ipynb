{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInf\n</b></h1> \n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nLogs\n</b></h2> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nLibs\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport glob\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom dataclasses import dataclass\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T18:15:13.643405Z","iopub.execute_input":"2022-06-20T18:15:13.644196Z","iopub.status.idle":"2022-06-20T18:15:19.173262Z","shell.execute_reply.started":"2022-06-20T18:15:13.644094Z","shell.execute_reply":"2022-06-20T18:15:19.172495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True    \n    torch.backends.cudnn.benchmark = False\n\n    \ndef inference_fn(test_loader, model, device, is_sigmoid=True):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n        if is_sigmoid == True:\n            preds.append(output.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(output.to('cpu').numpy())\n\n    return np.concatenate(preds)    \n    \n\ndef upd_outputs(data, is_trim=False, is_minmax=False, is_reshape=False):\n    min_max_scaler = MinMaxScaler()\n    \n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T18:15:19.176875Z","iopub.execute_input":"2022-06-20T18:15:19.17791Z","iopub.status.idle":"2022-06-20T18:15:19.188678Z","shell.execute_reply.started":"2022-06-20T18:15:19.177453Z","shell.execute_reply":"2022-06-20T18:15:19.187888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.precision', 4)\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:19.189894Z","iopub.execute_input":"2022-06-20T18:15:19.190135Z","iopub.status.idle":"2022-06-20T18:15:19.262488Z","shell.execute_reply.started":"2022-06-20T18:15:19.190101Z","shell.execute_reply":"2022-06-20T18:15:19.261608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"competition_dir = \"../input/us-patent-phrase-to-phrase-matching/\"\n\nsubmission = pd.read_csv(competition_dir+'sample_submission.csv')\ntest_origin = pd.read_csv(competition_dir+'test.csv')\ntest_origin.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:19.26494Z","iopub.execute_input":"2022-06-20T18:15:19.265574Z","iopub.status.idle":"2022-06-20T18:15:19.295102Z","shell.execute_reply.started":"2022-06-20T18:15:19.265536Z","shell.execute_reply":"2022-06-20T18:15:19.294443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInference TorchModels\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-104MixFold-TRAIN-deberta-v3-large-ModelExtendv1-SplitScore-All\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-104mixfold-20220613115117/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:19.296626Z","iopub.execute_input":"2022-06-20T18:15:19.29692Z","iopub.status.idle":"2022-06-20T18:15:20.070322Z","shell.execute_reply.started":"2022-06-20T18:15:19.296887Z","shell.execute_reply":"2022-06-20T18:15:20.06935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:20.071619Z","iopub.execute_input":"2022-06-20T18:15:20.072351Z","iopub.status.idle":"2022-06-20T18:15:20.12206Z","shell.execute_reply.started":"2022-06-20T18:15:20.07231Z","shell.execute_reply":"2022-06-20T18:15:20.121009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:20.123685Z","iopub.execute_input":"2022-06-20T18:15:20.124127Z","iopub.status.idle":"2022-06-20T18:15:20.156292Z","shell.execute_reply.started":"2022-06-20T18:15:20.124082Z","shell.execute_reply":"2022-06-20T18:15:20.155594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_104Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_104Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_104Mix = [upd_outputs(x, is_reshape=True) for x in D_104Mix]\nD_104Mix = pd.DataFrame(D_104Mix).T\n\nD_104Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:15:20.157535Z","iopub.execute_input":"2022-06-20T18:15:20.157906Z","iopub.status.idle":"2022-06-20T18:17:59.270693Z","shell.execute_reply.started":"2022-06-20T18:15:20.157871Z","shell.execute_reply":"2022-06-20T18:17:59.269877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:17:59.272183Z","iopub.execute_input":"2022-06-20T18:17:59.27246Z","iopub.status.idle":"2022-06-20T18:17:59.49678Z","shell.execute_reply.started":"2022-06-20T18:17:59.272422Z","shell.execute_reply":"2022-06-20T18:17:59.495456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nChanged\n    D-20031MixFold-TRAIN-electra-large-discriminator-ModelExtendv1-SplitScore-s5-e10-f1-Copy14 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/electrav1/\"\n    config_path=path+'config.pth'\n    model=\"google/electra-large-discriminator\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(\"../input/pppm-dc-d-20031mixfold-20220615092815/tokenizer/\")\n\ncontext_mapping = torch.load(\"../input/pppm-dc-d-20031mixfold-20220615092815/cpc_texts.pth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:17:59.502503Z","iopub.execute_input":"2022-06-20T18:17:59.502779Z","iopub.status.idle":"2022-06-20T18:17:59.621504Z","shell.execute_reply.started":"2022-06-20T18:17:59.502743Z","shell.execute_reply":"2022-06-20T18:17:59.620744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:17:59.625994Z","iopub.execute_input":"2022-06-20T18:17:59.626269Z","iopub.status.idle":"2022-06-20T18:17:59.656919Z","shell.execute_reply.started":"2022-06-20T18:17:59.626233Z","shell.execute_reply":"2022-06-20T18:17:59.656119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:17:59.661543Z","iopub.execute_input":"2022-06-20T18:17:59.664356Z","iopub.status.idle":"2022-06-20T18:17:59.687257Z","shell.execute_reply.started":"2022-06-20T18:17:59.664316Z","shell.execute_reply":"2022-06-20T18:17:59.686601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_20031Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f'../input/electrav1/google-electra-large-discriminator_fold{fold}_best/google-electra-large-discriminator_fold{fold}_best.pth'\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_20031Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_20031Mix = [upd_outputs(x, is_reshape=True) for x in D_20031Mix]\nD_20031Mix = pd.DataFrame(D_20031Mix).T\n\nD_20031Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:17:59.691094Z","iopub.execute_input":"2022-06-20T18:17:59.693006Z","iopub.status.idle":"2022-06-20T18:19:16.145577Z","shell.execute_reply.started":"2022-06-20T18:17:59.692968Z","shell.execute_reply":"2022-06-20T18:19:16.144724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:16.148439Z","iopub.execute_input":"2022-06-20T18:19:16.148666Z","iopub.status.idle":"2022-06-20T18:19:16.319834Z","shell.execute_reply.started":"2022-06-20T18:19:16.148632Z","shell.execute_reply":"2022-06-20T18:19:16.318921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-1123MixFold-TRAIN-deberta-v3-base-ModelExtendv1-SplitScore-s5-e10 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-1123mixfold-20220614054320/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-base\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:16.321615Z","iopub.execute_input":"2022-06-20T18:19:16.321987Z","iopub.status.idle":"2022-06-20T18:19:17.079673Z","shell.execute_reply.started":"2022-06-20T18:19:16.321849Z","shell.execute_reply":"2022-06-20T18:19:17.078904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:17.081234Z","iopub.execute_input":"2022-06-20T18:19:17.081499Z","iopub.status.idle":"2022-06-20T18:19:17.104138Z","shell.execute_reply.started":"2022-06-20T18:19:17.081464Z","shell.execute_reply":"2022-06-20T18:19:17.103062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:17.105452Z","iopub.execute_input":"2022-06-20T18:19:17.107259Z","iopub.status.idle":"2022-06-20T18:19:17.127309Z","shell.execute_reply.started":"2022-06-20T18:19:17.107228Z","shell.execute_reply":"2022-06-20T18:19:17.126601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_1123Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_1123Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_1123Mix = [upd_outputs(x, is_reshape=True) for x in D_1123Mix]\nD_1123Mix = pd.DataFrame(D_1123Mix).T\n\nD_1123Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:17.129221Z","iopub.execute_input":"2022-06-20T18:19:17.129453Z","iopub.status.idle":"2022-06-20T18:20:23.508026Z","shell.execute_reply.started":"2022-06-20T18:19:17.129402Z","shell.execute_reply":"2022-06-20T18:20:23.507319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nD-1112MixFold-TRAIN-deberta-v3-large-ModelExtendv1-SplitAnchor-s5-e10\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-d-1112mixfold-20220614045637/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:20:23.509211Z","iopub.execute_input":"2022-06-20T18:20:23.509433Z","iopub.status.idle":"2022-06-20T18:20:24.280441Z","shell.execute_reply.started":"2022-06-20T18:20:23.509406Z","shell.execute_reply":"2022-06-20T18:20:24.279707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:20:24.281878Z","iopub.execute_input":"2022-06-20T18:20:24.282162Z","iopub.status.idle":"2022-06-20T18:20:24.306523Z","shell.execute_reply.started":"2022-06-20T18:20:24.282117Z","shell.execute_reply":"2022-06-20T18:20:24.305737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:20:24.307476Z","iopub.execute_input":"2022-06-20T18:20:24.307675Z","iopub.status.idle":"2022-06-20T18:20:24.324187Z","shell.execute_reply.started":"2022-06-20T18:20:24.307639Z","shell.execute_reply":"2022-06-20T18:20:24.323301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_1112Mix = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    D_1112Mix.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nD_1112Mix = [upd_outputs(x, is_reshape=True) for x in D_1112Mix]\nD_1112Mix = pd.DataFrame(D_1112Mix).T\n\nD_1112Mix.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:20:24.325932Z","iopub.execute_input":"2022-06-20T18:20:24.326305Z","iopub.status.idle":"2022-06-20T18:22:56.761596Z","shell.execute_reply.started":"2022-06-20T18:20:24.326264Z","shell.execute_reply":"2022-06-20T18:22:56.76089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:22:56.7632Z","iopub.execute_input":"2022-06-20T18:22:56.763553Z","iopub.status.idle":"2022-06-20T18:22:56.931833Z","shell.execute_reply.started":"2022-06-20T18:22:56.763513Z","shell.execute_reply":"2022-06-20T18:22:56.930789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nG-100MixFold---LB:0.8243\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-g-100mixfold-20220617152024/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:22:56.93351Z","iopub.execute_input":"2022-06-20T18:22:56.933847Z","iopub.status.idle":"2022-06-20T18:22:57.139192Z","shell.execute_reply.started":"2022-06-20T18:22:56.933807Z","shell.execute_reply":"2022-06-20T18:22:57.138432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:22:57.140782Z","iopub.execute_input":"2022-06-20T18:22:57.141121Z","iopub.status.idle":"2022-06-20T18:22:57.160578Z","shell.execute_reply.started":"2022-06-20T18:22:57.141084Z","shell.execute_reply":"2022-06-20T18:22:57.159925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:22:57.161614Z","iopub.execute_input":"2022-06-20T18:22:57.162047Z","iopub.status.idle":"2022-06-20T18:22:57.186834Z","shell.execute_reply.started":"2022-06-20T18:22:57.162009Z","shell.execute_reply":"2022-06-20T18:22:57.186104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G_100MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    G_100MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nG_100MixFold = [upd_outputs(x, is_reshape=True) for x in G_100MixFold]\nG_100MixFold = pd.DataFrame(G_100MixFold).T\n\nG_100MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:22:57.188215Z","iopub.execute_input":"2022-06-20T18:22:57.189165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nF-10xMixFold---0.84976\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-f-10xmixfold-20220618121926/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v2-xlarge\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_10xMixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    F_10xMixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nF_10xMixFold = [upd_outputs(x, is_reshape=True) for x in F_10xMixFold]\nF_10xMixFold = pd.DataFrame(F_10xMixFold).T\n\nF_10xMixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nI-100-TRAIN-albert-xxlarge-v2-ModelExtendAttention-SplitScore-s5-e10-f012345 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-i-100mixfold-20220618221837/\"\n    config_path=path+'config.pth'\n    model=\"albert-xxlarge-v2\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"I_100MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    I_100MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nI_100MixFold = [upd_outputs(x, is_reshape=True) for x in I_100MixFold]\nI_100MixFold = pd.DataFrame(I_100MixFold).T\n\nI_100MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nL-101-TRAIN-bert-for-patents-ModelExtendv1-SplitScore-s5 \n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-l-101mixfold-20220620013202/\"\n    config_path=path+'config.pth'\n    model=\"anferico/bert-for-patents\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L_101MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    L_101MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nL_101MixFold = [upd_outputs(x, is_reshape=True) for x in L_101MixFold]\nL_101MixFold = pd.DataFrame(L_101MixFold).T\n\nL_101MixFold.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nN-200MixFold-TRAIN-albert-xxlarge-v1-ModelExtendAttention-SplitScore-s5-e10\n</b></h2>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-dc-n-200mixfold-20220620180707/\"\n    config_path=path+'config.pth'\n    model=\"albert-xxlarge-v1\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0,1,2,3,4]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n\nseed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndisplay(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_200MixFold = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    N_200MixFold.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nN_200MixFold = [upd_outputs(x, is_reshape=True) for x in N_200MixFold]\nN_200MixFold = pd.DataFrame(N_200MixFold).T\n\nN_200MixFold.head(10).style.background_gradient(cmap=cm, axis=1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInference TransformersModels\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nPubKernel\n</b></h2>\n\nhttps://www.kaggle.com/code/surilee/inference-bert-for-uspatents-deepshare/notebook\nLB:0.8392\n\nhttps://www.kaggle.com/code/renokan/2-deberta-1-roberta-analysis-and-using/notebook","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           truncation=True)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg        \n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.text[item])\n        \n        return inputs\n   \n    \nclass CustomModel(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, inputs):\n        output = self.base(**inputs)\n\n        return output[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_path='../input/deberta-v3-large/deberta-v3-large'\n    batch_size=CUSTOM_BATCH\n    num_workers=2\n    max_len=130\n    trn_fold=[0, 1, 2, 3]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n\ncontext_mapping = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest.reset_index(inplace=True)\ntest = test.merge(titles, left_on='context', right_on='code')\ntest.sort_values(by='index', inplace=True)\ntest.drop(columns='index', inplace=True)\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pub_deberta_predicts_1 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size, shuffle=False,\n                             num_workers=CFG.num_workers,\n                             pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = CustomModel(CFG.model_path)    \n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_dataloader, model, DEVICE, is_sigmoid=False)\n    \n    pub_deberta_predicts_1.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------- inference_fn([...], is_sigmoid=False)\npub_deberta_predicts_1 = [upd_outputs(x, is_minmax=True, is_reshape=True) for x in pub_deberta_predicts_1]\npub_deberta_predicts_1 = pd.DataFrame(pub_deberta_predicts_1).T\n\npub_deberta_predicts_1.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nEnsemble\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"IF_ENSEMBLE=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nNo Ensemble\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"# if not IF_ENSEMBLE:\n#     print(\"# ---------------------------------------------------- #\")\n#     print(\"# Prediction Single Model\")\n#     print(\"# ---------------------------------------------------- #\")\n    \n#     PREDICTION_1 = D_104Mix\n#     PREDICTION_KEY = 'D_104Mix'\n    \n#     # --------------------------------------------------------------- #\n#     all_predictions = pd.concat(\n#         [PREDICTION_1],\n#         keys=[PREDICTION_KEY],\n#         axis=1\n#     )\n\n#     all_predictions.head(10) \\\n#         .assign(mean=lambda x: x.mean(axis=1)) \\\n#             .style.background_gradient(cmap=cm, axis=1)\n    \n#     # --------------------------------------------------------------- #\n#     all_mean = pd.DataFrame({\n#         'PREDICTION_KEY': PREDICTION_1.mean(axis=1)\n#     })\n\n#     all_mean.head(10) \\\n#         .assign(mean=lambda x: x.mean(axis=1)) \\\n#             .style.highlight_max(axis=1, props=props_param)\n#     # --------------------------------------------------------------- #\n#     # === N1 ===\n#     weights_ = [1.00]\n#     final_predictions = all_mean.mul(weights_).sum(axis=1)\n\n#     # === N2 ===\n#     # final_predictions = all_mean.median(axis=1)\n#     # final_predictions = all_mean.mean(axis=1)\n\n#     # === N3 ===\n#     # final_predictions = all_predictions.mean(axis=1)\n\n#     # === N4 ===\n#     # combs = pd.DataFrame({\n#     #     'deberta_1': deberta_predicts_1.mean(axis=1),\n#     #     'deb_2+rob': (deberta_predicts_2.mean(axis=1) * 0.666) \\\n#     #                     + (roberta_predicts.mean(axis=1) * 0.333)\n#     # })\n#     # display(combs.head())\n#     # final_predictions = combs.median(axis=1)\n#     # final_predictions = combs.mean(axis=1)\n\n#     final_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nUse Ensemble\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    all_predictions = pd.concat(\n        [D_104Mix,\n         D_20031Mix,\n         D_1123Mix,\n         D_1112Mix,\n         pub_deberta_predicts_1,\n         G_100MixFold,\n         F_10xMixFold,\n         I_100MixFold,\n         L_101MixFold,\n         N_200MixFold\n        ],\n        keys=['D_104Mix',\n              'D_20031Mix',\n              'D_1123Mix',\n              'D_1112Mix',\n              'pub_deberta_predicts_1',\n              'G_100MixFold',\n              'F_10xMixFold',\n              'I_100MixFold',\n              'L_101MixFold',\n              'N_200MixFold'\n             ],\n        axis=1\n    )\n\n    all_predictions.head(10) \\\n        .assign(mean=lambda x: x.mean(axis=1)) \\\n            .style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    all_mean = pd.DataFrame({\n        'D_104Mix': D_104Mix.mean(axis=1),\n        'D_20031Mix': D_20031Mix.mean(axis=1),\n        'D_1123Mix': D_1123Mix.mean(axis=1),\n        'D_1112Mix': D_1112Mix.mean(axis=1),\n        'pub_deberta_predicts_1': pub_deberta_predicts_1.mean(axis=1),\n        'G_100MixFold': G_100MixFold.mean(axis=1),\n        'F_10xMixFold': F_10xMixFold.mean(axis=1),\n        'I_100MixFold': I_100MixFold.mean(axis=1),\n        'L_101MixFold': L_101MixFold.mean(axis=1),\n        'N_200MixFold': N_200MixFold.mean(axis=1)\n    })\n\n    all_mean.head(10) \\\n        .assign(mean=lambda x: x.mean(axis=1)) \\\n            .style.highlight_max(axis=1, props=props_param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IF_ENSEMBLE:\n    # === N1 ===\n    weights_ = [0.30, 0.15, 0.05, 0.06, 0.15, 0.06, 0.06, 0.06, 0.06, 0.05]\n    final_predictions = all_mean.mul(weights_).sum(axis=1)\n\n    # === N2 ===\n    # final_predictions = all_mean.median(axis=1)\n    # final_predictions = all_mean.mean(axis=1)\n\n    # === N3 ===\n    # final_predictions = all_predictions.mean(axis=1)\n\n    # === N4 ===\n    # combs = pd.DataFrame({\n    #     'deberta_1': deberta_predicts_1.mean(axis=1),\n    #     'deb_2+rob': (deberta_predicts_2.mean(axis=1) * 0.666) \\\n    #                     + (roberta_predicts.mean(axis=1) * 0.333)\n    # })\n    # display(combs.head())\n    # final_predictions = combs.median(axis=1)\n    # final_predictions = combs.mean(axis=1)\n\n    final_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nSubmission\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_origin['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nEOF\n</b></h1> ","metadata":{}}]}