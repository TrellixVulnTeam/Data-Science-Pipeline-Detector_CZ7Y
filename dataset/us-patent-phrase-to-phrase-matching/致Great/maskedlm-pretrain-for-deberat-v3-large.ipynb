{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"thanks to Tifo：https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/324216\n\npppm_abstract：https://www.kaggle.com/datasets/fankaixie/pppm-abstract","metadata":{}},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# author:quincy qiang\n\nimport os\nimport random\nimport warnings\nimport pandas as pd\n\nimport numpy as np\nimport torch\nfrom transformers import (AutoModelForMaskedLM,\n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nwarnings.filterwarnings('ignore')\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_torch(42)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Genegate Pretraining Corpus","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/pppm-abstract/pppm_abstract.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.dropna().reset_index(drop=True)\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('corpus.txt','w',encoding='utf-8') as f:\n    for ab in df['abstract']:\n        f.write(ab+'\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"\n\nmodel_name = 'microsoft/deberta-v3-large'\n\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('pretrained_models/microsoft/deberta-v3-large')\n\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"./corpus.txt\",  # mention train text file here\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"./corpus.txt\",  # mention valid text file here\n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\"pretrained_models/microsoft/deberta-v3-large-pretrain\",  # select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=8,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    evaluation_strategy='steps',\n    save_total_limit=2,\n    eval_steps=5000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    prediction_loss_only=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)\n\ntrainer.train()\ntrainer.save_model(f'pretrained_models/microsoft/deberta-v3-large')  \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\n|Step |Trainging Loss|Validation Loss\n5000,1.816200,1.677946\n10000,1.521900,1.444387\n15000,1.398000,1.323548\n20000,1.313400,1.239668\n25000,1.229300,1.177530\n30000,1.168100,1.124718\n35000,1.162000,1.083667\n40000,1.101400,1.045740\n45000,1.081000,1.023710\n\n```","metadata":{}},{"cell_type":"markdown","source":"Because I set the epoch to 10, the pre-training task takes a long time, which takes about ~15 hours. Here, the valid data can be reduced to shorten the time.","metadata":{}},{"cell_type":"markdown","source":"Here is pretrained models：https://www.kaggle.com/datasets/quincyqiang/deberta-v3-large-pretrain","metadata":{}},{"cell_type":"markdown","source":"## Conclusions","metadata":{}},{"cell_type":"markdown","source":"- Using the pre-training task for fine-tuning, the offline CV score becomes lower, CV Score: 0.8321-> 0.8081\n- In the discussion forum, I also saw other people have the same situations,can someone explain？:[The key idea?](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/324389)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}