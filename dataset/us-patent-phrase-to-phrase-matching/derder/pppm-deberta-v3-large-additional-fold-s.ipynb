{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### If this notebook is helpful, please upvote [the original version](https://www.kaggle.com/code/leehann/inference-bert-for-uspatents)! (score: 0.8392)\n\n### Version 2-8: \n\n```\n# === add np.median ===\n\nadd_preds = []\nfor x in zip(*upd_predictions):\n    add_preds.append(np.median(x, axis=0))\n    \nupd_predictions.append(add_preds)\n\n# === add np.mean ===\n\n[...]\n```\n\n### I am trying to improve my score: 0.8393\n\n### Version 9: \n\n```\ndef _upd_score_between(data, thresholds, value):\n    mask_th = data.between(*thresholds, inclusive='both')\n    data[mask_th] = value\n\n\ndef upd_score(data, th_dict=None):\n    [...]\n    if th0:\n        if isinstance(th0, float):\n            th0 = (result.min(), th0)\n        \n        if isinstance(th0, tuple):\n            _upd_score_between(result, th0, 0)\n    \n    if th25 and isinstance(th25, tuple):\n        _upd_score_between(result, th25, 0.25)\n\n    [...]\n```\n\n#### Calibrate scores (use thresholds)\n\n```\nthresholds_dict = {       # between(min_x, max_x, inclusive='both')\n    '0': 0.02,            # (min_x, max_x) or X -> (data.min(), X)\n    '.25': (0.24, 0.26),  # (min_x, max_x)\n    '.50': (0.49, 0.51),  # (min_x, max_x)\n    '.75': (0.74, 0.76),  # (min_x, max_x)\n    '1': 0.98             # (min_x, max_x) or X -> (x, data.max())\n}\n\nsubmission['score'] = upd_score(submission['score'], thresholds_dict)\n\n```","metadata":{}},{"cell_type":"markdown","source":"# 1. Import & Set & Def & Load","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\nimport os\nimport nltk\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem import WordNetLemmatizer\n#!pip install nlpre\n#from nlpre import titlecaps, dedash, identify_parenthetical_phrases\n#from nlpre import replace_acronyms, replace_from_dictionary\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n#nltk.download('wordnet')\n#nltk.download('stopwords')\nwtlem = WordNetLemmatizer()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-05T08:32:18.588246Z","iopub.execute_input":"2022-06-05T08:32:18.588606Z","iopub.status.idle":"2022-06-05T08:32:18.600009Z","shell.execute_reply.started":"2022-06-05T08:32:18.588566Z","shell.execute_reply":"2022-06-05T08:32:18.599011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**----------------- rule base -------------------------**","metadata":{}},{"cell_type":"code","source":"def take_word(word):\n    return str(word).split(\"\\'\")[1].split('.')[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.606201Z","iopub.execute_input":"2022-06-05T08:32:18.606559Z","iopub.status.idle":"2022-06-05T08:32:18.617538Z","shell.execute_reply.started":"2022-06-05T08:32:18.60652Z","shell.execute_reply":"2022-06-05T08:32:18.616292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def take_set(word):\n    return str(word).split(\"\\'\")[1]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.622379Z","iopub.execute_input":"2022-06-05T08:32:18.622962Z","iopub.status.idle":"2022-06-05T08:32:18.632976Z","shell.execute_reply.started":"2022-06-05T08:32:18.622918Z","shell.execute_reply":"2022-06-05T08:32:18.631883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_pos(word):\n            \n    word_set=wn.synsets(word)\n    \n    if len(word_set) == 0:\n        if len(word) >2:\n            if word[-2:] == 'al':\n                word=word[:-2]\n        word_set=wn.synsets(word)\n    #print(str(word)+' : '+str(word_set))\n    wordpos_list=[]\n    \n    for www in word_set:\n        www=str(www).split(\"\\'\")[1]\n        if word == www.split('.')[0]:\n            return [www.split('.')[1],www]\n    \n    if len(word_set) > 0:\n        www=str(word_set[0]).split(\"\\'\")[1]\n        return [www.split('.')[1],www]\n    else:\n        #print('error: '+word)\n        return ['X','derder']","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.635304Z","iopub.execute_input":"2022-06-05T08:32:18.636003Z","iopub.status.idle":"2022-06-05T08:32:18.647861Z","shell.execute_reply.started":"2022-06-05T08:32:18.635957Z","shell.execute_reply":"2022-06-05T08:32:18.646661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_preprocess(word):\n    T=word\n    word=word.lower()\n    word_list=word.split(' ')\n    word = ''\n    for temp_word in word_list:\n        if len(wn.synsets(temp_word)) > 0:\n            if temp_word not in stopwords.words():\n                temp_word=wtlem.lemmatize(temp_word, 'v')\n                temp_word=wtlem.lemmatize(temp_word, 'a')\n                temp_word=wtlem.lemmatize(temp_word, 'n')\n                temp_wordset=wn.synsets(temp_word)\n                if len(temp_wordset)>0:\n                    temp_wordset=temp_wordset[0]\n                    temp_wordsetroot=temp_wordset.root_hypernyms()\n                    if take_word(temp_wordsetroot[0]) != 'express':\n                        word=word+temp_word+' '\n                else:\n                    word=word+temp_word+' '\n    word=word[:-1]\n    if len(word) >0: \n        return word\n    else:\n        return T","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.657014Z","iopub.execute_input":"2022-06-05T08:32:18.657336Z","iopub.status.idle":"2022-06-05T08:32:18.675482Z","shell.execute_reply.started":"2022-06-05T08:32:18.657297Z","shell.execute_reply":"2022-06-05T08:32:18.67448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nounify(word,word_pos):\n    set_of_related_nouns = set()\n    #Synset('magnetically.r.01')\n#     if word_pos == 'n':\n#         for lemma in wn.lemmas(wn.morphy(word, wn.NOUN), pos=\"n\"):\n#             for related_form in lemma.derivationally_related_forms():\n#                 for synset in wn.synsets(related_form.name(), pos=wn.ADJ):\n#                     set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'a'))\n    \n    if word_pos == 'v':\n        for lemma in wn.lemmas(wn.morphy(word, wn.VERB), pos=\"v\"):\n            for related_form in lemma.derivationally_related_forms():\n                for synset in wn.synsets(related_form.name(), pos=wn.NOUN):\n                    set_of_related_nouns.add(synset)\n    \n    if word_pos == 'a':\n        for lemma in wn.lemmas(wn.morphy(word, wn.ADJ), pos=\"a\"):\n            for related_form in lemma.derivationally_related_forms():\n                for synset in wn.synsets(related_form.name(), pos=wn.NOUN):\n                    set_of_related_nouns.add(synset)\n                for synset in wn.synsets(related_form.name(), pos=wn.VERB):\n                    set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'v'))\n                    \n    if word_pos == 's':\n        for lemma in wn.lemmas(wn.morphy(word, wn.ADJ), pos=\"s\"):\n            for related_form in lemma.derivationally_related_forms():\n                for synset in wn.synsets(related_form.name(), pos=wn.NOUN):\n                    set_of_related_nouns.add(synset)\n                for synset in wn.synsets(related_form.name(), pos=wn.VERB):\n                    set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'v'))\n                for synset in wn.synsets(related_form.name(), pos=wn.ADJ):\n                    set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'a'))   \n                    \n    if word_pos == 'r':\n        for lemma in wn.lemmas(wn.morphy(word, wn.ADV), pos=\"r\"):\n            for related_form in lemma.derivationally_related_forms():\n                for synset in wn.synsets(related_form.name(), pos=wn.NOUN):\n                    set_of_related_nouns.add(synset)\n                for synset in wn.synsets(related_form.name(), pos=wn.VERB):\n                    set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'v'))\n                for synset in wn.synsets(related_form.name(), pos=wn.ADJ):\n                    set_of_related_nouns=set_of_related_nouns.union(nounify(take_word(synset),'a'))   \n                    \n    if word_pos != 'v' and word_pos != 'a' and word_pos != 's' and word_pos != 'r':\n        for data in wn.synsets(word):\n            set_of_related_nouns.add(data)\n\n    return set_of_related_nouns","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.677918Z","iopub.execute_input":"2022-06-05T08:32:18.679691Z","iopub.status.idle":"2022-06-05T08:32:18.713255Z","shell.execute_reply.started":"2022-06-05T08:32:18.679648Z","shell.execute_reply":"2022-06-05T08:32:18.712146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_word(word):\n    if len(word) >= 5:\n        if word[-2:] == 'ry':\n            word=word[:-2]+'te'\n        if word[-2:] == 'ly':\n            word=word[:-2]\n        if word[-2:] == 'er':\n            word=word[:-2]\n        if word[-2:] == 'or':\n            word=word[:-2]\n        if word[-2:] == 'al':\n            word=word[:-2]\n            \n        if word[-3:] == 'ial':\n            word=word[:-3]\n        if word[-3:] == 'ful':\n            word=word[:-3]\n            \n        if word[-2:] == 'ic':\n            word=word[:-2]\n           \n        if word[-3:] == 'ish':\n            word=word[:-3]\n          \n        if word[-3:] == 'ive':\n            word=word[:-3]\n           \n        if word[-5:] == 'ative':\n            word=word[:-5]\n          \n        if word[-4:] == 'able':\n            word=word[:-4]\n            \n        if word[-4:] == 'ible':\n            word=word[:-4]+'t'\n          \n        if word[-4:] == 'ical':\n            word=word[:-4]\n            \n        if word[-4:] == 'iess':\n            word=word[:-4]\n              \n            \n    return word","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.837507Z","iopub.execute_input":"2022-06-05T08:32:18.838532Z","iopub.status.idle":"2022-06-05T08:32:18.863846Z","shell.execute_reply.started":"2022-06-05T08:32:18.838488Z","shell.execute_reply":"2022-06-05T08:32:18.862886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_similarity_100(word1,word2):\n     \n        \n        \n        \n        \n    \n    ###############################################   # toolpath  __ tool paths\n    word11=word1.replace(' ','')\n    word22=word2.replace(' ','')\n    if len(word11) > len(word22):    \n        LL=len(word22)\n        other_LL=len(word11)\n    else:\n        LL=len(word11)\n        other_LL=len(word22)\n        \n    count=0        \n    for index in range(0,LL):\n        if word11[index] == word22[index]:\n            count=count+1\n        else:\n            break\n    if LL > 5:\n        if (LL-count) ==0 and (other_LL-count)<=4:\n            #print('>>>' + word1+'  '+word2)\n            return 1\n    ###############################################\n    word1_list=word1.split(' ')\n    word2_list=word2.split(' ')\n    \n    ############################### just change position\n    count=0\n    for ww1 in word1_list:\n        for ww2 in word2_list:\n            if ww1 == ww2:\n                count=count+1\n    \n    if count >= len(word1_list) and count >= len(word2_list) and count!=0:\n        return 1\n    ##############################\n    \n   \n    count=0\n    for anchorword in word1_list:\n        for targetword in word2_list:\n            anchorword=pre_word(anchorword)\n            targetword=pre_word(targetword)\n            #print('---------------------------')\n            #print('anchorword: '+anchorword)\n            #print('targetword: '+targetword)\n            if anchorword == '' or targetword =='':\n                continue\n            try:\n                anchor_pos=word_pos(anchorword)\n                target_pos=word_pos(targetword)\n            except:\n                print('error: '+anchorword+'   !!!  '+targetword)\n                print('error: '+str(anchor_pos)+'   !!!  '+str(target_pos))\n            \n           \n            if anchor_pos[0]!= 'X':\n                anchor_set=wn.synset(anchor_pos[1])\n            if target_pos[0]!='X':\n                target_set=wn.synset(target_pos[1])\n                \n                \n            #print(anchor_pos)\n            #print(target_pos)\n            #print('\\n')\n            \n            \n            anchor_nounset_extra=set({\"Synset('\"+anchorword+\"')\"})\n            target_nounset_extra=set({\"Synset('\"+targetword+\"')\"})\n            \n            anchor_pos=anchor_pos[0]\n            target_pos=target_pos[0]\n\n            anchor_nounset=set()\n            target_nounset=set()\n            anchor_nounset=nounify(anchorword,anchor_pos)\n            target_nounset=nounify(targetword,target_pos)\n            anchor_nounset=anchor_nounset.union(anchor_nounset_extra)\n            target_nounset=target_nounset.union(target_nounset_extra)   \n           \n            \n            \n            #print(anchor_nounset)\n            #print(target_nounset)\n            #print('\\n')\n            \n            select_anchor_token=[]\n            for anchor_token in anchor_nounset:\n                anchor_token=take_word(anchor_token)\n                if len(anchorword) > len(anchor_token):    \n                    LL=len(anchor_token)\n                else:\n                     LL=len(anchorword)\n\n                tempcount=0        \n                for index in range(0,LL):\n                    if anchorword[index] == anchor_token[index]:\n                        tempcount=tempcount+1\n                    else:\n                        break\n                if tempcount >=3:\n                    select_anchor_token.append(anchor_token)\n                    \n            \n            select_target_token=[]\n            for target_token in target_nounset:\n                target_token=take_word(target_token)\n                if len(targetword) > len(target_token):    \n                    LL=len(target_token)\n                else:\n                     LL=len(targetword)\n\n                tempcount=0        \n                for index in range(0,LL):\n                    if targetword[index] == target_token[index]:\n                        tempcount=tempcount+1\n                    else:\n                        break\n                if tempcount >=3:\n                    select_target_token.append(target_token)\n                    \n            check=0\n            for A_token in select_anchor_token:\n                for S_token in select_target_token:\n                    if A_token == S_token:\n                        check=1\n            \n            if check ==1:\n                 count=count+1\n            \n            \n \n                \n            #print('count = ' + str(count) )\n    if count >= len(word1_list)  and count >= len(word2_list) and count!=0:\n        return 1\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:18.871164Z","iopub.execute_input":"2022-06-05T08:32:18.871512Z","iopub.status.idle":"2022-06-05T08:32:18.918691Z","shell.execute_reply.started":"2022-06-05T08:32:18.871439Z","shell.execute_reply":"2022-06-05T08:32:18.916999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**--------------------------------------**","metadata":{}},{"cell_type":"code","source":"class CFG_DEB_SIMPLE:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = '../input/deberta-v3-large/deberta-v3-large'\n    batch_size = 24\n    num_workers = 2\n    num_fold = 4\n    max_input_length = 130","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-05T08:32:18.923147Z","iopub.execute_input":"2022-06-05T08:32:18.925588Z","iopub.status.idle":"2022-06-05T08:32:18.931765Z","shell.execute_reply.started":"2022-06-05T08:32:18.925547Z","shell.execute_reply":"2022-06-05T08:32:18.929511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_input_length):\n        self.text = df['text'].values.astype(str)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = self.text[item]\n        \n        inputs = self.tokenizer(inputs,\n                    max_length=self.max_input_length,\n                    padding='max_length',\n                    truncation=True)\n        \n        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\n    \n    \nclass Custom_Bert_Simple(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        base_output = self.base(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                token_type_ids=token_type_ids\n        )\n\n        return base_output[0]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-05T08:32:18.936745Z","iopub.execute_input":"2022-06-05T08:32:18.937521Z","iopub.status.idle":"2022-06-05T08:32:18.961797Z","shell.execute_reply.started":"2022-06-05T08:32:18.937474Z","shell.execute_reply":"2022-06-05T08:32:18.960778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(valid_loader, model, device):\n    model.eval()\n    preds = []\n    labels = []\n    \n    for step, batch in enumerate(valid_loader):\n        input_ids, token_type_ids, attention_mask = [i.to(device) for i in batch]\n    \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask, token_type_ids)\n        \n        preds.append(y_preds.to('cpu').numpy())\n    \n    predictions = np.concatenate(preds)\n    \n    return predictions\n\n\nmin_max_scaler = MinMaxScaler()\n\ndef upd_outputs(data, is_trim=True, is_minmax=True, is_reshape=True):\n    \"\"\"\\o/\"\"\"\n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n\n\ndef _upd_score_between(data, thresholds, value):\n    \"\"\"\\o/\"\"\"\n    mask_th = data.between(*thresholds, inclusive='both')\n    data[mask_th] = value\n\n\ndef upd_score(data, th_dict=None):\n    \"\"\"\\o/\"\"\"\n    if isinstance(data, pd.Series):\n        result = data.copy()\n    else:\n        return data\n\n    if not th_dict:        \n        th_dict = {\n            '0': 0.02,\n            '.25': (0.24, 0.26),\n            '.50': (0.49, 0.51),\n            '.75': (0.74, 0.76),\n            '1': 0.98\n        }\n\n    if isinstance(th_dict, dict):    \n        th0 = th_dict.get('0')\n        th25 = th_dict.get('.25')\n        th50 = th_dict.get('.50')\n        th75 = th_dict.get('.75')\n        th100 = th_dict.get('1')\n    else:\n        return data\n    \n    if th0:\n        if isinstance(th0, float):\n            th0 = (result.min(), th0)\n        \n        if isinstance(th0, tuple):\n            _upd_score_between(result, th0, 0)\n    \n    if th25 and isinstance(th25, tuple):\n        _upd_score_between(result, th25, 0.25)\n\n    if th50 and isinstance(th50, tuple):\n        _upd_score_between(result, th50, 0.50)\n            \n    if th75 and isinstance(th75, tuple):\n        _upd_score_between(result, th75, 0.75)\n            \n    if th100:\n        if isinstance(th100, float):\n            th100 = (th100, result.max())\n        \n        if isinstance(th100, tuple):\n            _upd_score_between(result, th100, 1)\n\n    return result","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-05T08:32:18.967676Z","iopub.execute_input":"2022-06-05T08:32:18.970644Z","iopub.status.idle":"2022-06-05T08:32:19.005455Z","shell.execute_reply.started":"2022-06-05T08:32:18.970592Z","shell.execute_reply":"2022-06-05T08:32:19.002475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(f\"{CFG_DEB_SIMPLE.input_path}test.csv\")\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\ntest_df = test_df.merge(titles, left_on='context', right_on='code')\n\ncpc_texts = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")\n\ntest_df['context_text'] = test_df['context'].map(cpc_texts)\ntest_df['text'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]'  + test_df['context_text']\ntest_df['text'] = test_df['text'].apply(str.lower)\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:19.011141Z","iopub.execute_input":"2022-06-05T08:32:19.011804Z","iopub.status.idle":"2022-06-05T08:32:19.718862Z","shell.execute_reply.started":"2022-06-05T08:32:19.011754Z","shell.execute_reply":"2022-06-05T08:32:19.717651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Extract & Update Predictions","metadata":{}},{"cell_type":"code","source":"tokenizer_deberta_v3 = AutoTokenizer.from_pretrained(CFG_DEB_SIMPLE.model_path)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-05T08:32:19.721184Z","iopub.execute_input":"2022-06-05T08:32:19.721561Z","iopub.status.idle":"2022-06-05T08:32:20.440138Z","shell.execute_reply.started":"2022-06-05T08:32:19.721515Z","shell.execute_reply":"2022-06-05T08:32:20.439121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nte_dataset = TestDataset(test_df, tokenizer_deberta_v3, CFG_DEB_SIMPLE.max_input_length)\nte_dataloader = DataLoader(te_dataset,\n                              batch_size=CFG_DEB_SIMPLE.batch_size, shuffle=False,\n                              num_workers=CFG_DEB_SIMPLE.num_workers,\n                              pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in tqdm(range(CFG_DEB_SIMPLE.num_fold)):\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = Custom_Bert_Simple(CFG_DEB_SIMPLE.model_path)\n    model.load_state_dict(torch.load(fold_path)['model'])\n    model.to('cuda')\n    \n    prediction = valid_fn(te_dataloader, model, 'cuda')\n    \n    predictions.append(prediction)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:32:20.441812Z","iopub.execute_input":"2022-06-05T08:32:20.44343Z","iopub.status.idle":"2022-06-05T08:33:13.283341Z","shell.execute_reply.started":"2022-06-05T08:32:20.443382Z","shell.execute_reply":"2022-06-05T08:33:13.280269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"folds:\", len(predictions))\nprint(\"rows: \", len(predictions[0]))\nprint(\"score:\", predictions[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.285407Z","iopub.execute_input":"2022-06-05T08:33:13.285652Z","iopub.status.idle":"2022-06-05T08:33:13.301313Z","shell.execute_reply.started":"2022-06-05T08:33:13.28562Z","shell.execute_reply":"2022-06-05T08:33:13.299848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_predictions = 14","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.306611Z","iopub.execute_input":"2022-06-05T08:33:13.307173Z","iopub.status.idle":"2022-06-05T08:33:13.315769Z","shell.execute_reply.started":"2022-06-05T08:33:13.307124Z","shell.execute_reply":"2022-06-05T08:33:13.314359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first fold\npredictions[0][:n_predictions]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.320403Z","iopub.execute_input":"2022-06-05T08:33:13.320727Z","iopub.status.idle":"2022-06-05T08:33:13.334286Z","shell.execute_reply.started":"2022-06-05T08:33:13.320684Z","shell.execute_reply":"2022-06-05T08:33:13.333025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(*upd_outputs(predictions[0], is_trim=False)[:n_predictions])\n# print(*upd_outputs(predictions[0], is_minmax=False)[:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.338069Z","iopub.execute_input":"2022-06-05T08:33:13.341467Z","iopub.status.idle":"2022-06-05T08:33:13.348236Z","shell.execute_reply.started":"2022-06-05T08:33:13.341334Z","shell.execute_reply":"2022-06-05T08:33:13.346926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.where(x<=0, 0, x) .. >> min_max.fit_transform(x) >> x.reshape(-1)\nupd_predictions = [upd_outputs(x, is_trim=False) for x in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.350421Z","iopub.execute_input":"2022-06-05T08:33:13.350694Z","iopub.status.idle":"2022-06-05T08:33:13.362005Z","shell.execute_reply.started":"2022-06-05T08:33:13.350664Z","shell.execute_reply":"2022-06-05T08:33:13.36051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(*upd_predictions[0][:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.364071Z","iopub.execute_input":"2022-06-05T08:33:13.364965Z","iopub.status.idle":"2022-06-05T08:33:13.376997Z","shell.execute_reply.started":"2022-06-05T08:33:13.364744Z","shell.execute_reply":"2022-06-05T08:33:13.375925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Additional & Final Predictions","metadata":{}},{"cell_type":"code","source":"origin_predictions = upd_predictions.copy()  # 5. Visualization","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.378741Z","iopub.execute_input":"2022-06-05T08:33:13.37906Z","iopub.status.idle":"2022-06-05T08:33:13.384609Z","shell.execute_reply.started":"2022-06-05T08:33:13.379017Z","shell.execute_reply":"2022-06-05T08:33:13.383355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === add np.median ===\nadd_preds = []\nfor x in zip(*upd_predictions):\n    add_preds.append(np.median(x, axis=0))\n    \nupd_predictions.append(add_preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.387217Z","iopub.execute_input":"2022-06-05T08:33:13.387964Z","iopub.status.idle":"2022-06-05T08:33:13.400576Z","shell.execute_reply.started":"2022-06-05T08:33:13.387887Z","shell.execute_reply":"2022-06-05T08:33:13.399554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === add np.mean ===\nadd_preds = []\nfor x in zip(*upd_predictions):\n    add_preds.append(np.mean(x, axis=0))\n    \nupd_predictions.append(add_preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.402781Z","iopub.execute_input":"2022-06-05T08:33:13.403133Z","iopub.status.idle":"2022-06-05T08:33:13.411886Z","shell.execute_reply.started":"2022-06-05T08:33:13.403087Z","shell.execute_reply":"2022-06-05T08:33:13.410868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = np.mean(upd_predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.413688Z","iopub.execute_input":"2022-06-05T08:33:13.414115Z","iopub.status.idle":"2022-06-05T08:33:13.422672Z","shell.execute_reply.started":"2022-06-05T08:33:13.41407Z","shell.execute_reply":"2022-06-05T08:33:13.421719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(*final_predictions[:n_predictions])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.424881Z","iopub.execute_input":"2022-06-05T08:33:13.425491Z","iopub.status.idle":"2022-06-05T08:33:13.43788Z","shell.execute_reply.started":"2022-06-05T08:33:13.425443Z","shell.execute_reply":"2022-06-05T08:33:13.436643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create & Calibrate Submissions","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_df['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.44012Z","iopub.execute_input":"2022-06-05T08:33:13.440462Z","iopub.status.idle":"2022-06-05T08:33:13.455781Z","shell.execute_reply.started":"2022-06-05T08:33:13.440418Z","shell.execute_reply":"2022-06-05T08:33:13.454688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_dict = {\n    '0': 0.125,\n    '.25': (0.125, 0.375),\n    '.50': (0.375, 0.625),\n    '.75': (0.625, 0.875),\n    '1': 0.875\n}\n\nsubmission['score'] = upd_score(submission['score'], thresholds_dict)\n\nsubmission.head(30)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.457489Z","iopub.execute_input":"2022-06-05T08:33:13.458464Z","iopub.status.idle":"2022-06-05T08:33:13.490657Z","shell.execute_reply.started":"2022-06-05T08:33:13.458416Z","shell.execute_reply":"2022-06-05T08:33:13.489295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_df['text'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]'  + test_df['context_text']\n#print(test_df['anchor']+ ' / ' +test_df['target']+' / '+test_df['id'])\nderder_id_list=[]\nfor anchor,target,testid in zip(test_df['anchor'],test_df['target'],test_df['id']): \n    #print(word)\n    #print(anchor + '  /  ' + target)\n    try:\n        if check_similarity_100(anchor,target) ==1:\n            derder_id_list.append(testid)\n            #print('>>>  ' + testid)\n    except:\n        pass\n\ntemp=submission['id']\nfor dd in derder_id_list:\n    submission.loc[submission.id == dd,'score']=1\n\nsubmission.head(30)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.493208Z","iopub.execute_input":"2022-06-05T08:33:13.493666Z","iopub.status.idle":"2022-06-05T08:33:13.567254Z","shell.execute_reply.started":"2022-06-05T08:33:13.493601Z","shell.execute_reply":"2022-06-05T08:33:13.566269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.568924Z","iopub.execute_input":"2022-06-05T08:33:13.569257Z","iopub.status.idle":"2022-06-05T08:33:13.579369Z","shell.execute_reply.started":"2022-06-05T08:33:13.569199Z","shell.execute_reply":"2022-06-05T08:33:13.578306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Visualization origin_predictions","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\npd.set_option('display.precision', 10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-05T08:33:13.582747Z","iopub.execute_input":"2022-06-05T08:33:13.583496Z","iopub.status.idle":"2022-06-05T08:33:13.590304Z","shell.execute_reply.started":"2022-06-05T08:33:13.583447Z","shell.execute_reply":"2022-06-05T08:33:13.589225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(origin_predictions).T.head(15)\ndf = df.rename_axis(columns='folds', index='rows')\n\ndf.style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.593841Z","iopub.execute_input":"2022-06-05T08:33:13.594326Z","iopub.status.idle":"2022-06-05T08:33:13.635025Z","shell.execute_reply.started":"2022-06-05T08:33:13.594267Z","shell.execute_reply":"2022-06-05T08:33:13.634076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.style.highlight_quantile(q_left=0.75, axis=1, color='green')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.636542Z","iopub.execute_input":"2022-06-05T08:33:13.637585Z","iopub.status.idle":"2022-06-05T08:33:13.660412Z","shell.execute_reply.started":"2022-06-05T08:33:13.637543Z","shell.execute_reply":"2022-06-05T08:33:13.659442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.assign(mean=lambda x: x.mean(axis=1)) \\\n    .style.highlight_max(axis=1, props=props_param)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.662167Z","iopub.execute_input":"2022-06-05T08:33:13.662809Z","iopub.status.idle":"2022-06-05T08:33:13.690155Z","shell.execute_reply.started":"2022-06-05T08:33:13.662764Z","shell.execute_reply":"2022-06-05T08:33:13.689255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sub(df.mean(axis=1), axis=0) \\\n    .style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:13.694713Z","iopub.execute_input":"2022-06-05T08:33:13.695421Z","iopub.status.idle":"2022-06-05T08:33:13.729529Z","shell.execute_reply.started":"2022-06-05T08:33:13.69539Z","shell.execute_reply":"2022-06-05T08:33:13.728597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}