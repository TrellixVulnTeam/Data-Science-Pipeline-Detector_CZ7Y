{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-24T03:35:23.830797Z","iopub.execute_input":"2022-05-24T03:35:23.831055Z","iopub.status.idle":"2022-05-24T03:35:23.836712Z","shell.execute_reply.started":"2022-05-24T03:35:23.831027Z","shell.execute_reply":"2022-05-24T03:35:23.835826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\ntitle_df = pd.read_csv(\"../input/cpc-codes/titles.csv\")\n\ntrain_df = train_df.merge(title_df, how='inner', left_on='context', right_on='code')\ntrain_df['text'] = train_df['anchor'] + '[SEP]' + train_df['target'] + '[SEP]' + train_df['title']\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:23.881265Z","iopub.execute_input":"2022-05-24T03:35:23.881763Z","iopub.status.idle":"2022-05-24T03:35:25.002813Z","shell.execute_reply.started":"2022-05-24T03:35:23.881708Z","shell.execute_reply":"2022-05-24T03:35:25.002051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# generate auxilary data","metadata":{}},{"cell_type":"code","source":"all_train_contexts = train_df.context.unique()\ncontext_map = {}\nfor _, row in title_df.iterrows():\n    code =row.code\n    title=row.title\n    context_map[code] = title\n    \n\ntitle_group_df = title_df[['code', 'title']].copy()\ntitle_group_df['context'] = title_group_df['code'].apply(lambda x: x[:3])\ntitle_group_df = title_group_df[(title_group_df['context'].apply(lambda x: len(x)==3)) &\n                                (title_group_df['code'] != title_group_df['context']) &\n                                (title_group_df['context'].isin(all_train_contexts))].copy()\n\ntitle_group_df.rename(columns={'title': \"pos_title\"}, inplace=True)\ntitle_group_df['title'] = title_group_df['context'].apply(lambda k: context_map[k])\ntitle_group_df['section'] = title_group_df.code.apply(lambda code: code[0])\ntitle_group_df = title_group_df.groupby('context').head(1000)\n\ntitle_group_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:25.004359Z","iopub.execute_input":"2022-05-24T03:35:25.004669Z","iopub.status.idle":"2022-05-24T03:35:42.511816Z","shell.execute_reply.started":"2022-05-24T03:35:25.004639Z","shell.execute_reply":"2022-05-24T03:35:42.510575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_context_map={}\n\nfor _,row in title_group_df.iterrows():\n    context=row.context\n    pos_title=row.pos_title\n    \n    if context not in all_context_map:\n        all_context_map[context]=[]\n    \n    all_context_map[context].append(pos_title)\nprint(len(all_context_map))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:42.51334Z","iopub.execute_input":"2022-05-24T03:35:42.513575Z","iopub.status.idle":"2022-05-24T03:35:48.04784Z","shell.execute_reply.started":"2022-05-24T03:35:42.513544Z","shell.execute_reply":"2022-05-24T03:35:48.046979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fold_map(row):\n    score = str(row.score)\n    section = row.section\n    return section+'-'+score\n\ntrain_df['fold_group'] = train_df.apply(get_fold_map, axis=1)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:48.049028Z","iopub.execute_input":"2022-05-24T03:35:48.049341Z","iopub.status.idle":"2022-05-24T03:35:49.132256Z","shell.execute_reply.started":"2022-05-24T03:35:48.049306Z","shell.execute_reply":"2022-05-24T03:35:49.131412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    batch_size=8\n    n_epochs = 1\n    model_name = \"microsoft/deberta-v3-large\"\n    max_len = 200\n    nfolds = 5\n    min_lr = 1e-6\n    max_lr = 5e-6\n    weight_decay=0.01\n    ACC_STEPS=2\n    eval_every = 800\n    print_every = 800","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:49.134488Z","iopub.execute_input":"2022-05-24T03:35:49.134743Z","iopub.status.idle":"2022-05-24T03:35:49.139954Z","shell.execute_reply.started":"2022-05-24T03:35:49.134698Z","shell.execute_reply":"2022-05-24T03:35:49.139023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:49.141438Z","iopub.execute_input":"2022-05-24T03:35:49.141981Z","iopub.status.idle":"2022-05-24T03:35:49.157151Z","shell.execute_reply.started":"2022-05-24T03:35:49.141945Z","shell.execute_reply":"2022-05-24T03:35:49.156558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\nmodel_config = AutoConfig.from_pretrained(CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:35:49.158338Z","iopub.execute_input":"2022-05-24T03:35:49.158787Z","iopub.status.idle":"2022-05-24T03:36:02.080578Z","shell.execute_reply.started":"2022-05-24T03:35:49.158746Z","shell.execute_reply":"2022-05-24T03:36:02.079829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer)\nprint()\nprint(model_config)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.081643Z","iopub.execute_input":"2022-05-24T03:36:02.082431Z","iopub.status.idle":"2022-05-24T03:36:02.090778Z","shell.execute_reply.started":"2022-05-24T03:36:02.082399Z","shell.execute_reply":"2022-05-24T03:36:02.089797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"title_group_df.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.092874Z","iopub.execute_input":"2022-05-24T03:36:02.093555Z","iopub.status.idle":"2022-05-24T03:36:02.106478Z","shell.execute_reply.started":"2022-05-24T03:36:02.093505Z","shell.execute_reply":"2022-05-24T03:36:02.105638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TitleGroupDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.all_context = list(set(df.context.values))\n    \n    def prepare_inputs(self, text):\n        inputs = tokenizer(text, add_special_tokens=True, max_length=CFG.max_len, padding='max_length', truncation=True)\n        for k,v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n        return inputs\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        title = row.title\n        pos_title = row.pos_title\n        section = row.section\n        context = row.context\n        neg_context = np.random.choice(self.all_context, 2)\n        if neg_context[0]!=context:\n            neg_context = neg_context[0]\n        else:\n            neg_context = neg_context[1]\n        neg_title = np.random.choice(all_context_map[ neg_context ])\n        \n        pos_text = title+\"[SEP]\"+pos_title\n        neg_text = title+\"[SEP]\"+neg_title\n        \n        xpos = self.prepare_inputs(pos_text)\n        xneg = self.prepare_inputs(neg_text)\n        \n        return (xpos, xneg)\n        \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.107778Z","iopub.execute_input":"2022-05-24T03:36:02.108497Z","iopub.status.idle":"2022-05-24T03:36:02.123397Z","shell.execute_reply.started":"2022-05-24T03:36:02.108452Z","shell.execute_reply":"2022-05-24T03:36:02.122555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatentDataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase='train'):\n        self.phase = phase\n        self.context = df.context.values\n        self.text = df.text.values\n        self.score = df.score.values\n    \n    def prepare_inputs(self, text):\n        inputs = tokenizer(text, add_special_tokens=True, max_length=CFG.max_len, padding='max_length', truncation=True)\n        for k,v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n        return inputs\n    \n    def __getitem__(self, idx):\n        text = self.text[idx]\n        score = self.score[idx]\n        \n        if self.phase == 'train':\n            score = score + np.random.uniform(-0.025, 0.025)\n            score = np.clip(score, 0.0, 1.0)\n        \n        inputs = self.prepare_inputs(text)\n        label = torch.tensor(score, dtype=torch.float32)\n        return (inputs, label)\n    \n    def __len__(self):\n        return len(self.text)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.124797Z","iopub.execute_input":"2022-05-24T03:36:02.125214Z","iopub.status.idle":"2022-05-24T03:36:02.140258Z","shell.execute_reply.started":"2022-05-24T03:36:02.125182Z","shell.execute_reply":"2022-05-24T03:36:02.139522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class BackboneModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(CFG.model_name)\n    def forward(self, inputs):\n        outputs = self.backbone(**inputs)\n        h = outputs.last_hidden_state[:, 0, :]\n        return h\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = BackboneModel()\n        self.auxilary_mlp = nn.Sequential(\n            nn.Dropout(0.15),\n            nn.Linear(model_config.hidden_size, 256),\n            nn.LeakyReLU(),\n            \n            nn.BatchNorm1d(256),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1)\n        )\n        self.mlp = nn.Sequential(\n            nn.Dropout(0.15),\n            nn.Linear(model_config.hidden_size, 256),\n            nn.LeakyReLU(),\n            \n            nn.BatchNorm1d(256),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1)\n        )\n    \n    \n    def get_auxilary_output(self, inputs):\n        h = self.backbone(inputs)\n        y = self.auxilary_mlp(h)\n        return y\n    \n    def forward(self, inputs):\n        h = self.backbone(inputs)\n        y = self.mlp(h)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.141428Z","iopub.execute_input":"2022-05-24T03:36:02.141883Z","iopub.status.idle":"2022-05-24T03:36:02.158201Z","shell.execute_reply.started":"2022-05-24T03:36:02.141836Z","shell.execute_reply":"2022-05-24T03:36:02.157429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loss","metadata":{}},{"cell_type":"code","source":"def compute_loss(y, yhat):\n    loss = torch.tensor(0.0, device=device)\n    loss = -y * torch.log( 1e-9 + yhat.sigmoid() ) - (1-y) * torch.log(1e-9 + 1 - yhat.sigmoid())\n    loss = loss.mean()\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.159703Z","iopub.execute_input":"2022-05-24T03:36:02.159974Z","iopub.status.idle":"2022-05-24T03:36:02.175102Z","shell.execute_reply.started":"2022-05-24T03:36:02.159945Z","shell.execute_reply":"2022-05-24T03:36:02.174394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# evaluate","metadata":{}},{"cell_type":"code","source":"def evaluate(val_dataloader, model):\n    print(\"evaluating...\")\n    ytrue=[]\n    ypreds=[]\n    \n    model.eval()\n    for it, (inputs, label) in enumerate(val_dataloader):\n        batch_max_seqlen = inputs['attention_mask'].sum(dim=-1).max()\n        for k,v in inputs.items():\n            v = v[:, :batch_max_seqlen]\n            inputs[k] = v.to(device)\n\n        label = label.tolist()\n        ytrue += label\n        with torch.no_grad():\n            yhat = model(inputs)\n            yhat = yhat.view(-1).sigmoid().cpu().tolist()\n            ypreds+=yhat\n    (corr, _) = pearsonr(ytrue, ypreds)\n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.179005Z","iopub.execute_input":"2022-05-24T03:36:02.179741Z","iopub.status.idle":"2022-05-24T03:36:02.189168Z","shell.execute_reply.started":"2022-05-24T03:36:02.179691Z","shell.execute_reply":"2022-05-24T03:36:02.188217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train epoch","metadata":{}},{"cell_type":"code","source":"def get_group_train_ops(lam_, group_iterator, model):\n    model.train()\n    #Iterate over auxilary dataset to get auxilary loss\n    loss2 = 0.0\n    num_iters = 3\n    for _ in range(num_iters):\n        try:\n            pos_samples, neg_samples = next(group_iterator)\n            batch_pos_max_seqlen = pos_samples['attention_mask'].sum(dim=-1).max()\n            for k,v in pos_samples.items():\n                v = v[:, :batch_pos_max_seqlen]\n                pos_samples[k] = v.to(device)\n            \n            ypos = model(pos_samples)\n            ypos = ypos.view(-1)\n            pos_loss = -0.9 * torch.log(1e-9 + ypos.sigmoid()) - 0.1 * torch.log(1e-9 + 1 - ypos.sigmoid())\n            pos_loss = lam_ * pos_loss.mean()\n            pos_loss = pos_loss/2/num_iters/ CFG.ACC_STEPS\n            pos_loss.backward()\n            \n            #Negsamples\n            batch_neg_max_seqlen = neg_samples['attention_mask'].sum(dim=-1).max()\n            for k,v in neg_samples.items():\n                v = v[:, :batch_neg_max_seqlen]\n                neg_samples[k] = v.to(device)\n            \n            yneg = model(neg_samples)\n            yneg = yneg.view(-1)\n            neg_loss = -0.1 * torch.log(1e-9 + yneg.sigmoid()) - 0.9 * torch.log(1e-9 + 1 - yneg.sigmoid())\n            neg_loss = lam_ * neg_loss.mean()\n            neg_loss = neg_loss/2/num_iters/ CFG.ACC_STEPS\n            neg_loss.backward()\n            \n            loss2 += (pos_loss.item() + neg_loss.item())\n        except:\n            pass\n    return loss2","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.19098Z","iopub.execute_input":"2022-05-24T03:36:02.191867Z","iopub.status.idle":"2022-05-24T03:36:02.205571Z","shell.execute_reply.started":"2022-05-24T03:36:02.191813Z","shell.execute_reply":"2022-05-24T03:36:02.204832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(fold_num, train_dataloader,val_dataloader,  optimizer, schedular, model):\n    epoch_loss=[]\n    epoch_aux_loss=[]\n    evals = []\n    \n    best_eval = None\n    num_iterations = len(train_dataloader)\n    \n    for e in range(CFG.n_epochs):\n        group_dataset = TitleGroupDataset(title_group_df)\n        group_loader = torch.utils.data.DataLoader(group_dataset,\n                                                   batch_size=CFG.batch_size,\n                                                   shuffle=True,\n                                                   drop_last=True)\n        group_iterator = iter(group_loader)\n        print(\"number of Group Iterations:\", len(group_loader))\n        if e==0:\n            lam_ = 0.1\n        elif e<=2:\n            lam_ = 0.5\n        else:\n            lam_ = 0.1\n        \n        model.zero_grad(set_to_none=True)\n        for it, (inputs, label) in enumerate(train_dataloader):\n            model.train()\n            \n            batch_max_seqlen = inputs['attention_mask'].sum(dim=-1).max()\n            for k,v in inputs.items():\n                v = v[:, :batch_max_seqlen]\n                inputs[k] = v.to(device)\n            label = label.to(device)\n\n            yhat = model(inputs)\n            yhat = yhat.view(-1)\n            \n            \n            loss1 = compute_loss(label, yhat)\n            loss1 = loss1 / CFG.ACC_STEPS\n            loss1.backward()\n            \n            loss2 = get_group_train_ops(lam_, group_iterator, model)\n            epoch_aux_loss.append(loss2)\n            \n            if (1+it)%CFG.ACC_STEPS==0  or (it == num_iterations-1):\n                optimizer.step()\n                schedular.step()\n                model.zero_grad(set_to_none=True)\n            \n            epoch_loss.append(loss1.item())\n            if it%CFG.print_every == 0:\n                print(\"iteration:{} | loss:{:.4f} | aux loss:{:.4f}\".format(it, np.mean(epoch_loss), np.mean(epoch_aux_loss)))\n\n            if (1+it)%CFG.eval_every == 0:\n                cur_eval = evaluate(val_dataloader, model)\n                evals.append(cur_eval)\n                if (best_eval is None) or (cur_eval > best_eval):\n                    best_eval = cur_eval\n                    torch.save(model, \"model_{}.pt\".format(fold_num))\n                print(\"eval:{:.4f} | best eval:{:.4f}\".format(cur_eval, best_eval))\n            \n        cur_eval = evaluate(val_dataloader, model)\n        evals.append(cur_eval)\n        if (best_eval is None) or (cur_eval > best_eval):\n            best_eval = cur_eval\n            torch.save(model, \"model_{}.pt\".format(fold_num))\n        \n        print(\"eval:{:.4f} | best eval:{:.4f}\".format(cur_eval, best_eval))\n        print()\n        print(\"-------------------\")\n        print(\"epoch:{} | loss:{:.4f} | auxloss:{:.4f}\".format(e, np.mean(epoch_loss), np.mean(epoch_aux_loss)))\n    \n    print()\n    print()\n    plt.title(\"epoch losses....\")\n    plt.plot(epoch_loss)\n    plt.show()\n    print()\n    \n    plt.title(\"epoch aux losses....\")\n    plt.plot(epoch_aux_loss)\n    plt.show()\n    print()\n    \n    print()\n    plt.title(\"evals\")\n    plt.plot(evals)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.207181Z","iopub.execute_input":"2022-05-24T03:36:02.207409Z","iopub.status.idle":"2022-05-24T03:36:02.228174Z","shell.execute_reply.started":"2022-05-24T03:36:02.207382Z","shell.execute_reply":"2022-05-24T03:36:02.227239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skfold = StratifiedKFold(n_splits=CFG.nfolds, random_state=44, shuffle=True)\n\nfor foldnum, (train_idx, val_idx) in enumerate(skfold.split(train_df, train_df.fold_group)):\n    fold_train_df = train_df.iloc[train_idx]\n    fold_val_df = train_df.iloc[val_idx]\n    \n    train_dataset = PatentDataset( fold_train_df, phase='train' )\n    val_dataset   = PatentDataset( fold_val_df , phase='val')\n    \n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size,\n                                                   shuffle=True, \n                                                   drop_last=False,\n                                                   pin_memory=True\n                                                  )\n    \n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.batch_size,\n                                                 shuffle=False, \n                                                 drop_last=False)\n\n    num_train_batches = len(train_dataloader)\n    print(\"number of train batches:\", len(train_dataloader))\n    print(\"number of val batches:\", len(val_dataloader))\n    print()\n    print(\"-----------------------------------------------\")\n    \n    #model = Model().to(device)\n    model  = torch.load(\"../input/uspatentdebertasectiongroupmodel/model_0.pt\", map_location=device)\n    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=CFG.weight_decay, lr=CFG.max_lr)\n    schedular = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                           T_max = 2 + (num_train_batches * CFG.n_epochs)//CFG.ACC_STEPS,\n                                                           eta_min  = CFG.min_lr)\n    \n    train_epoch(foldnum, train_dataloader, val_dataloader, optimizer, schedular, model)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:36:02.229446Z","iopub.execute_input":"2022-05-24T03:36:02.229675Z","iopub.status.idle":"2022-05-24T03:37:46.914932Z","shell.execute_reply.started":"2022-05-24T03:36:02.229647Z","shell.execute_reply":"2022-05-24T03:37:46.914174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}