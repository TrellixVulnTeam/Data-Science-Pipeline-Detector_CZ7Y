{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    <h1 align='center'>PyTorch Lightning ‚ö° Training Notebook: RoBERTa with KFolds and TPU Support üë®‚Äçüíª</h1>\n</div>\n\n<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n\n<p style='text-align: center'>\n    This notebook features PyTorch lightning fine-tuning of RoBERTa Large model from ü§ó transformers. Training is done in KFolds format and support for GPU/TPU is also provided.<br> Huge inspiration for this notebook was <a href=\"https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\">Y Nakama's training notebook</a>. I would encourage you to fork my notebook and play around with the models and other parameters.\n    <br>\n    To run the model on TPU, un-comment and run the below cell and change the <code>gpus=1</code> argument to <code>tpu_cores=1</code> or <code>tpu_cores=8</code> in the <code>Trainer</code> class.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3 style='color: #fc0362; font-size: 1.5em; font-weight: 300; font-size: 24px'>If you liked this notebook, kindly leave an upvote ‚¨ÜÔ∏è</h3>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" align=\"center\">    \n    <h2>1. Installation and Imports</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Installation necessary for TPU training\n# ! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# ! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-23T10:58:13.88516Z","iopub.execute_input":"2022-03-23T10:58:13.885533Z","iopub.status.idle":"2022-03-23T10:58:13.906299Z","shell.execute_reply.started":"2022-03-23T10:58:13.885411Z","shell.execute_reply":"2022-03-23T10:58:13.905667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%sh\npip install -q pytorch-lightning\npip install -q --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-23T10:58:18.580245Z","iopub.execute_input":"2022-03-23T10:58:18.58052Z","iopub.status.idle":"2022-03-23T10:58:38.368332Z","shell.execute_reply.started":"2022-03-23T10:58:18.58049Z","shell.execute_reply":"2022-03-23T10:58:38.367549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport transformers\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nimport os\nimport re\nimport json\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:00:40.073556Z","iopub.execute_input":"2022-03-23T11:00:40.074324Z","iopub.status.idle":"2022-03-23T11:00:47.446079Z","shell.execute_reply.started":"2022-03-23T11:00:40.074287Z","shell.execute_reply":"2022-03-23T11:00:47.445224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" align=\"center\">    \n    <h2>2. Functions, Variables, Configs and Preprocessing</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"table = {\n'A': 'Human Necessities',\n'B': 'Operations and Transport',\n'C': 'Chemistry and Metallurgy',\n'D': 'Textiles',\n'E': 'Fixed Constructions',\n'F': 'Mechanical Engineering',\n'G': 'Physics',\n'H': 'Electricity',\n'Y': 'Emerging Cross-Sectional Technologies'\n}\n\ndef pearson(prediction, ground_truth):\n    \"\"\"\n    Pearson Correlation Coefficient\n    \"\"\"\n    return np.corrcoef(prediction, ground_truth)[0][1]\n\ndef get_cpc_texts():\n    \"\"\"\n    Function taken from Y Nakama's notebook: \n    https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\n    \"\"\"\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:00.470278Z","iopub.execute_input":"2022-03-23T06:02:00.470794Z","iopub.status.idle":"2022-03-23T06:02:00.481349Z","shell.execute_reply.started":"2022-03-23T06:02:00.470754Z","shell.execute_reply":"2022-03-23T06:02:00.480371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    NB_EPOCHS = 5\n    LR = 2e-5\n    MAX_LEN = 128\n    N_SPLITS = 5\n    TRAIN_BS = 16\n    VALID_BS = 32\n    NUM_WORKERS = 4\n    MODEL_NAME = 'roberta-large'\n    TRAIN_FILE = '../input/us-patent-phrase-to-phrase-matching/train.csv'\n    TEST_FILE = '../input/us-patent-phrase-to-phrase-matching/test.csv'\n    TOKENIZER = transformers.RobertaTokenizer.from_pretrained('roberta-large')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:00.485067Z","iopub.execute_input":"2022-03-23T06:02:00.485564Z","iopub.status.idle":"2022-03-23T06:02:04.532779Z","shell.execute_reply.started":"2022-03-23T06:02:00.485524Z","shell.execute_reply":"2022-03-23T06:02:04.531968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_texts = get_cpc_texts()\ntrain_file = pd.read_csv(Config.TRAIN_FILE)\ntest_file = pd.read_csv(Config.TEST_FILE)\ntrain_file['context_text'] = train_file['context'].map(cpc_texts)\ntest_file['context_text'] = test_file['context'].map(cpc_texts)\n\ntrain_file['text'] = train_file['anchor'] + '[SEP]' + train_file['target'] + '[SEP]' + train_file['context_text']\ntest_file['text'] = test_file['anchor'] + '[SEP]' + test_file['target'] + '[SEP]' + test_file['context_text']\n\ntrain_file['score_map'] = train_file['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n\ntrain_file.to_csv(\"train.csv\", index=None)\ntest_file.to_csv(\"test.csv\", index=None)\n\ntrain_file.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:04.534178Z","iopub.execute_input":"2022-03-23T06:02:04.534565Z","iopub.status.idle":"2022-03-23T06:02:05.890628Z","shell.execute_reply.started":"2022-03-23T06:02:04.534516Z","shell.execute_reply":"2022-03-23T06:02:05.889808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" align=\"center\">    \n    <h2>3. Custom Dataset Class</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class PPPMDataset(Dataset):\n    def __init__(self, df, is_test=False):\n        self.is_test = is_test\n        self.texts = df['text'].values\n        if not self.is_test:\n            self.scores = df['score'].values\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        inputs = Config.TOKENIZER.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=Config.MAX_LEN,\n            pad_to_max_length=True\n        )\n        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        \n        if self.is_test:\n            return {\n                'ids': ids,\n                'mask': mask,\n            }\n        else:\n            targets = torch.tensor(self.scores[idx], dtype=torch.float)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'targets': targets\n            }","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:05.891878Z","iopub.execute_input":"2022-03-23T06:02:05.892243Z","iopub.status.idle":"2022-03-23T06:02:05.901543Z","shell.execute_reply.started":"2022-03-23T06:02:05.892203Z","shell.execute_reply":"2022-03-23T06:02:05.900489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" align=\"center\">    \n    <h2>4. Model class</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class Model(pl.LightningModule):\n    def __init__(self, train_df, valid_df) -> None:\n        super(Model, self).__init__()\n        self.model = transformers.RobertaModel.from_pretrained(Config.MODEL_NAME)\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(1024, 1)\n        self.all_targets = []\n        self.train_loss_fn = nn.BCEWithLogitsLoss()\n        self.valid_loss_fn = nn.BCEWithLogitsLoss()\n        \n        self.train_df = train_df\n        self.valid_df = valid_df\n        \n    def forward(self, ids, mask) -> torch.Tensor:\n        _, output = self.model(ids, attention_mask=mask, return_dict=False)\n        output = self.drop(output)\n        output = self.out(output)\n        return output\n    \n    def prepare_data(self) -> None:\n        # Make Training and Validation Datasets\n        self.training_set = PPPMDataset(\n            self.train_df\n        )\n\n        self.validation_set = PPPMDataset(\n            self.valid_df\n        )\n\n    def train_dataloader(self):\n        train_loader = DataLoader(\n            self.training_set,\n            batch_size=Config.TRAIN_BS,\n            shuffle=True,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=True\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        val_loader = DataLoader(\n            self.validation_set,\n            batch_size=Config.VALID_BS,\n            shuffle=False,\n            num_workers=Config.NUM_WORKERS,\n        )\n        return val_loader\n    \n    def training_step(self, batch, batch_idx):\n        ids = batch['ids']\n        mask = batch['mask']\n        targets = batch['targets']\n\n        outputs = self(ids=ids, mask=mask)\n\n        train_loss = self.train_loss_fn(outputs, targets.view(-1, 1))\n        return {'loss': train_loss}\n    \n    def validation_step(self, batch, batch_idx):\n        ids = batch['ids']\n        mask = batch['mask']\n        targets = batch['targets']\n\n        outputs = self(ids=ids, mask=mask)\n\n        self.all_targets.extend(targets.cpu().detach().numpy().tolist())\n        \n        valid_loss = self.valid_loss_fn(outputs, targets.view(-1, 1))\n        return {'val_loss': valid_loss}\n    \n    def validation_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        logs = {'val_loss': avg_loss}\n        \n        print(f\"val_loss: {avg_loss}\")\n        return {'avg_val_loss': avg_loss, 'log': logs}\n    \n    def configure_optimizers(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        return transformers.AdamW(optimizer_parameters, lr=Config.LR)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:05.902973Z","iopub.execute_input":"2022-03-23T06:02:05.903336Z","iopub.status.idle":"2022-03-23T06:02:05.921612Z","shell.execute_reply.started":"2022-03-23T06:02:05.903299Z","shell.execute_reply":"2022-03-23T06:02:05.920751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\" align=\"center\">    \n    <h2>5. Main training and Validation</h2>\n    \n    Keep in mind, I'm only training here for 1 fold and not for all 5 since it will take a lot of time and my GPU quota will be exhausted in just 1 notebook!\n</div>","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n        DEVICE = torch.device('cuda:0')\n    else:\n        print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n        DEVICE = torch.device('cpu')\n    \n    data = pd.read_csv(\"./train.csv\")\n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Do Kfolds training and cross validation\n    kf = StratifiedKFold(n_splits=Config.N_SPLITS)\n    nb_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, 'bins'] = pd.cut(data['score'], bins=nb_bins, labels=False)\n    \n    for fold, (train_idx, valid_idx) in enumerate(kf.split(X=data, y=data['bins'].values)):\n        if fold != 0:\n            continue\n        print(f\"\\nFold: {fold}\")\n        print(f\"{'-'*20}\\n\")\n        \n        train_data = data.loc[train_idx]\n        valid_data = data.loc[valid_idx]\n        \n        model = Model(train_data, valid_data)\n        trainer = pl.Trainer(max_epochs=Config.NB_EPOCHS, gpus=1)\n        trainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T06:02:05.922991Z","iopub.execute_input":"2022-03-23T06:02:05.923433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n<img src=\"https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle\">\n</center>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}