{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Idea\nThis notebooks explores how to utilize a model from Hugging Face for Feature Engineering. The idea is to extract the hidden state of a model and then feed this state into a simple classifier. ","metadata":{}},{"cell_type":"markdown","source":"# Setup\nThis competition is a little bit special as we mustn't utilize an internet connection during inference. For this reason we have to install pip packages that are not part of the standard kaggle image manually","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\nfrom fastai.imports import *\nimport os\nimport pyarrow\nimport pyarrow.dataset\nfrom sklearn import preprocessing\nfrom umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:17.131536Z","iopub.execute_input":"2022-05-29T17:02:17.131785Z","iopub.status.idle":"2022-05-29T17:02:35.461532Z","shell.execute_reply.started":"2022-05-29T17:02:17.131711Z","shell.execute_reply":"2022-05-29T17:02:35.460807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We mustn't use internet this is why we install the pip package from the input","metadata":{}},{"cell_type":"code","source":"!pip install datasets -q --no-index --find-links=file:///kaggle/input/hf-datasets/wheels","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:35.463321Z","iopub.execute_input":"2022-05-29T17:02:35.463581Z","iopub.status.idle":"2022-05-29T17:02:49.384261Z","shell.execute_reply.started":"2022-05-29T17:02:35.463547Z","shell.execute_reply":"2022-05-29T17:02:49.383283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nfrom datasets import load_dataset, Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:49.386216Z","iopub.execute_input":"2022-05-29T17:02:49.387027Z","iopub.status.idle":"2022-05-29T17:02:49.636007Z","shell.execute_reply.started":"2022-05-29T17:02:49.386983Z","shell.execute_reply":"2022-05-29T17:02:49.635266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_nm = '../input/debertav3small'\n    train_path='../input/us-patent-phrase-to-phrase-matching/train.csv'\n    test_path='../input/us-patent-phrase-to-phrase-matching/test.csv'\n    sample_submission='../input/us-patent-phrase-to-phrase-matching/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:49.638486Z","iopub.execute_input":"2022-05-29T17:02:49.638996Z","iopub.status.idle":"2022-05-29T17:02:49.644018Z","shell.execute_reply.started":"2022-05-29T17:02:49.638954Z","shell.execute_reply":"2022-05-29T17:02:49.643297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data and the pre-trained model","metadata":{}},{"cell_type":"code","source":"tokz = AutoTokenizer.from_pretrained(CFG.model_nm)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:49.646214Z","iopub.execute_input":"2022-05-29T17:02:49.646686Z","iopub.status.idle":"2022-05-29T17:02:50.35969Z","shell.execute_reply.started":"2022-05-29T17:02:49.646642Z","shell.execute_reply":"2022-05-29T17:02:50.358967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG.train_path)\ndf_test = pd.read_csv(CFG.test_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.360836Z","iopub.execute_input":"2022-05-29T17:02:50.361084Z","iopub.status.idle":"2022-05-29T17:02:50.447347Z","shell.execute_reply.started":"2022-05-29T17:02:50.36105Z","shell.execute_reply":"2022-05-29T17:02:50.446619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sep = tokz.sep_token","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.448809Z","iopub.execute_input":"2022-05-29T17:02:50.449056Z","iopub.status.idle":"2022-05-29T17:02:50.454068Z","shell.execute_reply.started":"2022-05-29T17:02:50.449023Z","shell.execute_reply":"2022-05-29T17:02:50.452951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing","metadata":{}},{"cell_type":"code","source":"df['section'] = df.context.str[0]\ndf_test['section'] = df_test.context.str[0]\ndf['inputs'] = df.context + sep + df.anchor + sep + df.target\ndf_test['inputs'] = df_test.context + sep + df_test.anchor + sep + df_test.target","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.455687Z","iopub.execute_input":"2022-05-29T17:02:50.456227Z","iopub.status.idle":"2022-05-29T17:02:50.52074Z","shell.execute_reply.started":"2022-05-29T17:02:50.456193Z","shell.execute_reply":"2022-05-29T17:02:50.520019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we create a Dataset for the Hugging Face Model - we rename the column as the models loaded from hugging face expect the target column to have the name label.","metadata":{}},{"cell_type":"code","source":"ds = Dataset.from_pandas(df).rename_column('score', 'label')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.523689Z","iopub.execute_input":"2022-05-29T17:02:50.523911Z","iopub.status.idle":"2022-05-29T17:02:50.550716Z","shell.execute_reply.started":"2022-05-29T17:02:50.523885Z","shell.execute_reply":"2022-05-29T17:02:50.550026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we won't have to rename the column as the evaluation Dataset has no target column","metadata":{}},{"cell_type":"code","source":"eval_ds = Dataset.from_pandas(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.553729Z","iopub.execute_input":"2022-05-29T17:02:50.55394Z","iopub.status.idle":"2022-05-29T17:02:50.562173Z","shell.execute_reply.started":"2022-05-29T17:02:50.553905Z","shell.execute_reply":"2022-05-29T17:02:50.561407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tokenize function takes as input a row and then tokenizes from this row the input column. The tokenized row is an array of numbers. This is the only format that can be fed into the model.","metadata":{}},{"cell_type":"code","source":"def tok_func(x): return tokz(x[\"inputs\"], padding=True, truncation=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.563614Z","iopub.execute_input":"2022-05-29T17:02:50.56391Z","iopub.status.idle":"2022-05-29T17:02:50.569358Z","shell.execute_reply.started":"2022-05-29T17:02:50.563872Z","shell.execute_reply":"2022-05-29T17:02:50.568471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inps = \"anchor\",\"target\",\"context\"\ntok_ds = ds.map(tok_func, batched=True, batch_size=None, remove_columns=inps+('inputs','id','section'))\neval_tok_ds = eval_ds.map(tok_func, batched=True, batch_size=None, remove_columns=inps+('inputs','id','section'))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:50.571206Z","iopub.execute_input":"2022-05-29T17:02:50.571482Z","iopub.status.idle":"2022-05-29T17:02:59.583651Z","shell.execute_reply.started":"2022-05-29T17:02:50.571447Z","shell.execute_reply":"2022-05-29T17:02:59.582976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cell we see that the map function created several new columns:\n* The attention_mask column tells the model on which part of the sentence to focus. This column is created because during tokenization all tokenized senteces are padded until they have the same length as the longest tokenized sentence. The attention mask tells the model on which part of the sentence to focus\n* The input_ids column represents the tokenized sentence\n* The label column represents the label for the tokeized sentence\n* The token_type_ids is of no interest I think","metadata":{}},{"cell_type":"code","source":"tok_ds","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:59.584913Z","iopub.execute_input":"2022-05-29T17:02:59.585308Z","iopub.status.idle":"2022-05-29T17:02:59.591971Z","shell.execute_reply.started":"2022-05-29T17:02:59.585271Z","shell.execute_reply":"2022-05-29T17:02:59.591276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds = tok_ds\neval_dds=eval_tok_ds","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:59.593295Z","iopub.execute_input":"2022-05-29T17:02:59.593761Z","iopub.status.idle":"2022-05-29T17:02:59.601005Z","shell.execute_reply.started":"2022-05-29T17:02:59.593723Z","shell.execute_reply":"2022-05-29T17:02:59.600097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(CFG.model_nm).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:02:59.602358Z","iopub.execute_input":"2022-05-29T17:02:59.602771Z","iopub.status.idle":"2022-05-29T17:03:08.657609Z","shell.execute_reply.started":"2022-05-29T17:02:59.602733Z","shell.execute_reply":"2022-05-29T17:03:08.656859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is the main idea of the notebook. Models from Hugging Face offer the function .last_hidden_state with which we can extract the last hidden state of a model and then utilize this as features for downstream models.","metadata":{}},{"cell_type":"code","source":"def extract_hidden_states(batch):\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokz.model_input_names}\n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:08.659067Z","iopub.execute_input":"2022-05-29T17:03:08.659333Z","iopub.status.idle":"2022-05-29T17:03:08.665387Z","shell.execute_reply.started":"2022-05-29T17:03:08.659298Z","shell.execute_reply":"2022-05-29T17:03:08.664691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\neval_dds.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:08.666681Z","iopub.execute_input":"2022-05-29T17:03:08.667218Z","iopub.status.idle":"2022-05-29T17:03:08.711274Z","shell.execute_reply.started":"2022-05-29T17:03:08.66718Z","shell.execute_reply":"2022-05-29T17:03:08.710329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds_hidden = dds.map(extract_hidden_states, batched=True)\neval_dds_hidden = eval_dds.map(extract_hidden_states, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:08.712755Z","iopub.execute_input":"2022-05-29T17:03:08.713277Z","iopub.status.idle":"2022-05-29T17:03:53.895565Z","shell.execute_reply.started":"2022-05-29T17:03:08.713176Z","shell.execute_reply":"2022-05-29T17:03:53.894746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nX_train = np.array(dds_hidden[\"hidden_state\"])\ny_train = np.array(dds_hidden[\"label\"])\nX_test = np.array(eval_dds_hidden[\"hidden_state\"])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:53.896971Z","iopub.execute_input":"2022-05-29T17:03:53.89724Z","iopub.status.idle":"2022-05-29T17:03:54.302152Z","shell.execute_reply.started":"2022-05-29T17:03:53.897203Z","shell.execute_reply":"2022-05-29T17:03:54.301438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that the original input was transformed into a 768 dimensional vector","metadata":{}},{"cell_type":"code","source":"X_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:54.3034Z","iopub.execute_input":"2022-05-29T17:03:54.306549Z","iopub.status.idle":"2022-05-29T17:03:54.312924Z","shell.execute_reply.started":"2022-05-29T17:03:54.306511Z","shell.execute_reply":"2022-05-29T17:03:54.312176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next cell we visualize the effect of transforming the input into a 768 dimensional vector. For this we utilize the umap algorithm. This algorithm maps the 768 dimensional vector into a 2 dimensional vector. After mapping all instances belong to the same class as before the mapping. This gives us an idea of how good the feature engineering process made our samples more distinguishable.","metadata":{}},{"cell_type":"code","source":"# Scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n# Initialize and fit UMAP\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n# Create a DataFrame of 2D embeddings\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\ndf_emb[\"label\"] = y_train\nfig, axes = plt.subplots(1, 5, figsize=(8,2))\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\"]\nlabels = df_emb[\"label\"].unique().astype(str)\nlabels.sort()\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {label}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n                   gridsize=20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:03:54.314505Z","iopub.execute_input":"2022-05-29T17:03:54.315376Z","iopub.status.idle":"2022-05-29T17:04:40.202428Z","shell.execute_reply.started":"2022-05-29T17:03:54.315332Z","shell.execute_reply":"2022-05-29T17:04:40.201577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see the pre-processing does not separate the input very well. Many samples that have a label of 0.25, 0.5 or 0.75 fall into the same region. You could try out another model that might separate the input samples more clearly.","metadata":{}},{"cell_type":"markdown","source":"# Downstream model","metadata":{}},{"cell_type":"markdown","source":"As Downstream model we utilize logistic regression as this model can be trained quite fast with input that has got many features.","metadata":{}},{"cell_type":"code","source":"lbl_enc_train = preprocessing.LabelEncoder()\ny_train = lbl_enc_train.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:04:40.20701Z","iopub.execute_input":"2022-05-29T17:04:40.207647Z","iopub.status.idle":"2022-05-29T17:04:40.219213Z","shell.execute_reply.started":"2022-05-29T17:04:40.207604Z","shell.execute_reply":"2022-05-29T17:04:40.218368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# We increase `max_iter` to guarantee convergence\nlr_clf = LogisticRegression(max_iter=3000)\nlr_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:04:40.224919Z","iopub.execute_input":"2022-05-29T17:04:40.225622Z","iopub.status.idle":"2022-05-29T17:09:12.671626Z","shell.execute_reply.started":"2022-05-29T17:04:40.225572Z","shell.execute_reply":"2022-05-29T17:09:12.67072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_prediction=lr_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:09:12.673145Z","iopub.execute_input":"2022-05-29T17:09:12.673411Z","iopub.status.idle":"2022-05-29T17:09:12.678676Z","shell.execute_reply.started":"2022-05-29T17:09:12.673374Z","shell.execute_reply":"2022-05-29T17:09:12.677422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:09:12.680299Z","iopub.execute_input":"2022-05-29T17:09:12.680609Z","iopub.status.idle":"2022-05-29T17:09:12.689799Z","shell.execute_reply.started":"2022-05-29T17:09:12.680566Z","shell.execute_reply":"2022-05-29T17:09:12.688826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_classifier=RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:09:12.69164Z","iopub.execute_input":"2022-05-29T17:09:12.691937Z","iopub.status.idle":"2022-05-29T17:10:56.624779Z","shell.execute_reply.started":"2022-05-29T17:09:12.691899Z","shell.execute_reply":"2022-05-29T17:10:56.624042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF_prediction=rf_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.626216Z","iopub.execute_input":"2022-05-29T17:10:56.626469Z","iopub.status.idle":"2022-05-29T17:10:56.64916Z","shell.execute_reply.started":"2022-05-29T17:10:56.626427Z","shell.execute_reply":"2022-05-29T17:10:56.648525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF_prediction=lbl_enc_train.inverse_transform(RF_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.652748Z","iopub.execute_input":"2022-05-29T17:10:56.653378Z","iopub.status.idle":"2022-05-29T17:10:56.657746Z","shell.execute_reply.started":"2022-05-29T17:10:56.653341Z","shell.execute_reply":"2022-05-29T17:10:56.657017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_prediction=lbl_enc_train.inverse_transform(LR_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.659309Z","iopub.execute_input":"2022-05-29T17:10:56.659641Z","iopub.status.idle":"2022-05-29T17:10:56.665792Z","shell.execute_reply.started":"2022-05-29T17:10:56.659605Z","shell.execute_reply":"2022-05-29T17:10:56.664994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1=.66\nw2=.33","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.667348Z","iopub.execute_input":"2022-05-29T17:10:56.667964Z","iopub.status.idle":"2022-05-29T17:10:56.67387Z","shell.execute_reply.started":"2022-05-29T17:10:56.667929Z","shell.execute_reply":"2022-05-29T17:10:56.673152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_prediction=w1*LR_prediction+w2*RF_prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.674755Z","iopub.execute_input":"2022-05-29T17:10:56.674945Z","iopub.status.idle":"2022-05-29T17:10:56.681516Z","shell.execute_reply.started":"2022-05-29T17:10:56.674916Z","shell.execute_reply":"2022-05-29T17:10:56.680656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:10:56.682835Z","iopub.execute_input":"2022-05-29T17:10:56.683936Z","iopub.status.idle":"2022-05-29T17:10:56.691949Z","shell.execute_reply.started":"2022-05-29T17:10:56.683872Z","shell.execute_reply":"2022-05-29T17:10:56.691022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(CFG.sample_submission)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:12:11.388683Z","iopub.execute_input":"2022-05-29T17:12:11.389436Z","iopub.status.idle":"2022-05-29T17:12:11.405153Z","shell.execute_reply.started":"2022-05-29T17:12:11.389394Z","shell.execute_reply":"2022-05-29T17:12:11.404351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['score'] = FINAL_prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:12:12.205504Z","iopub.execute_input":"2022-05-29T17:12:12.20621Z","iopub.status.idle":"2022-05-29T17:12:12.210605Z","shell.execute_reply.started":"2022-05-29T17:12:12.206171Z","shell.execute_reply":"2022-05-29T17:12:12.209721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T18:30:21.943155Z","iopub.execute_input":"2022-05-22T18:30:21.943662Z","iopub.status.idle":"2022-05-22T18:30:21.962176Z","shell.execute_reply.started":"2022-05-22T18:30:21.943621Z","shell.execute_reply":"2022-05-22T18:30:21.961519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}