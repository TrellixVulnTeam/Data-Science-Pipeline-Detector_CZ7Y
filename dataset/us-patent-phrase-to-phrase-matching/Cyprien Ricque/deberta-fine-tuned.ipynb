{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\nimport transformers\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nfrom sklearn import metrics\n","metadata":{"_cell_guid":"fe8e30c3-ad26-4ab9-999e-bb0441121929","_uuid":"350b6cf2-57dc-45ae-8ffa-b1055d66e67d","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\n\ncfg01 = dict(\n    fold_num=5,  # Unused\n    seed=123,\n    model='../input/deberta-v3-large/deberta-v3-large',\n    path='',\n    max_len=400,\n    train_bs=4,\n    valid_bs=24,\n    accumulate_grad_batches=4,\n    workers=2,\n    gradient_clip_val=1000,\n    learning_rate=2e-5,\n)\n\ndata_folder = '../input/'","metadata":{"tags":[],"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = cfg01\n\nseed_everything(cfg['seed'])\ntokenizer = AutoTokenizer.from_pretrained(cfg['model'])\nsep = tokenizer.sep_token","metadata":{"tags":[],"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{"_cell_guid":"fde46fa0-f0f2-46bf-a340-6a3a31f65407","_uuid":"76b3939a-2774-42d0-b708-35739c99ccb7","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_cpc_texts():\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    \n    for file_name in os.listdir(f'{data_folder}/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    \n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'{data_folder}/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        \n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results\n\ncpc_texts = get_cpc_texts()\n\ndf_train = pd.read_csv(f'{data_folder}/us-patent-phrase-to-phrase-matching/train.csv')\ndf_test = pd.read_csv(f'{data_folder}/us-patent-phrase-to-phrase-matching/test.csv')","metadata":{"tags":[],"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{"_cell_guid":"9060b538-4122-4fe2-8660-39eddaff3b0d","_uuid":"a87648a6-8d1a-4492-9abe-59c673bfdeb7","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df_train['type'] = 'train'\ndf_test['tyoe'] = 'test'\ndf_test['score'] = np.nan\n\ndf_all = pd.concat([df_test, df_train], axis=0)\n\ndf_all['context_text'] = df_all['context'].map(cpc_texts).apply(lambda x:x.lower())\ndf_all = df_all.join(df_all.groupby('anchor').target.agg(list).rename('ref'), on='anchor')\ndf_all['ref2'] = df_all.apply(lambda x:[i for i in x['ref'] if i != x['target']], axis=1)\ndf_all['ref2'] = df_all.ref2.apply(lambda x: ', '.join(sorted(list(set(x)), key=x.index)))\ndf_all['ref'] = df_all.ref.apply(lambda x:', '.join(sorted(list(set(x)), key=x.index)))\n\ndf_all = df_all.join(df_all.groupby(['anchor', 'context']).target.agg(list).rename('ref3'), on=['anchor', 'context'])\ndf_all['ref3'] = df_all.apply(lambda x: ', '.join([i for i in x['ref3'] if i != x['target']]), axis=1)\n\ndf_all = df_all.join(df_all.groupby('context').anchor.agg('unique').rename('anchor_list'), on='context')\ndf_all['anchor_list'] = df_all.apply(lambda x:', '.join([i for i in x['anchor_list'] if i != x['anchor']]), axis=1)\n\ndf_all['text1'] = df_all['anchor'] + sep + df_all['target'] + sep  + df_all['context_text']\ndf_all['text2'] = df_all['anchor'] + sep + df_all['target'] + sep  + df_all['context_text'] + sep  + df_all['ref']\ndf_all['text3'] = df_all['anchor'] + sep + df_all['target'] + sep  + df_all['context_text'] + sep  + df_all['ref2']\ndf_all['text4'] = df_all['anchor'] + sep + df_all['target'] + sep  + df_all['context_text'] + sep  + df_all['ref2'] + ', ' + df_all['anchor_list']\ndf_all['text5'] = df_all['anchor'] + sep + df_all['target'] + sep  + df_all['context_text'] + sep  + df_all['ref3']\ndf_all['text6'] = 'The similarity between anchor ' + df_all['anchor'] + ' and target ' + df_all['target'] + '. Context is ' + df_all['context_text'] + '. Candidates are ' + df_all['ref3']\n\n# Select the one used\ndf_all['input'] = df_all.text3\n\ndf_all.head(2)","metadata":{"_cell_guid":"49f4a5ce-9c43-4340-ab6a-17fa9ee62b16","_uuid":"94dd3a7b-2f7a-45c6-a7d9-eca657e58174","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['out'] = pd.get_dummies(df_all.score, prefix='score').agg(list, axis=1)\ndf_all[df_all.type == 'train']['out'].head(2)","metadata":{"_cell_guid":"63b2c966-b5a5-4f47-83ca-977fa326c80a","_uuid":"26b5616a-cd82-4b8b-85cd-66eb91c670a6","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_train = df_all[df_all.type == 'train'].copy()\ndf_test = df_all[df_all.type == 'test'].copy()\n\ndf_train, df_val = train_test_split(df_train, test_size=.20, shuffle=True, random_state=41)\ndf_train.shape, df_val.shape","metadata":{"_cell_guid":"6df86e04-9613-4671-8233-8bfbafbc4438","_uuid":"f766f365-475a-45b1-9746-e6c51cc41af3","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatentDataset(Dataset):\n    def __init__(self, tokenizer, dataset, max_length, export=False):\n        \"\"\"\n        Args:\n            tokenizer: The tokenizer to be used.\n            dataset: The dataset to be used.\n            max_length: The maximum length of the input.\n            export: This mode is designed for computing final results on a dataset that does not contain the target variable\n        \"\"\"\n        super(PatentDataset, self).__init__()\n        self.export = export\n        self.tokenizer = tokenizer\n        self.df = dataset\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n\n        inputs = self.tokenizer(\n            df_train.input.iloc[index],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation_strategy='max_length',\n            padding='max_length',\n            truncation=True, \n            return_tensors='pt'\n        )\n        ids = inputs[\"input_ids\"][0]\n        mask = inputs[\"attention_mask\"][0]\n\n        out = dict(\n            # raw=df_train.input.iloc[index],  # debug\n            ids=ids.to(torch.long),\n            mask=mask.to(torch.long),\n        )\n\n        if not self.export:\n            out['target'] = torch.tensor(self.df.score.iloc[index], dtype=torch.float)\n\n        return out\n\ntrain_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_train, max_length=cfg['max_len'])\nval_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_val, max_length=cfg['max_len'])\ntest_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_test, max_length=cfg['max_len'], export=True)","metadata":{"_cell_guid":"a38dd7ea-8113-4863-8f89-27d104d353eb","_uuid":"c256aea5-7dcc-4d60-9463-6650fa649967","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=cfg['train_bs'], num_workers=cfg['workers'])\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=cfg['valid_bs'], num_workers=cfg['workers'])\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=cfg['valid_bs'], num_workers=cfg['workers'], shuffle=False)","metadata":{"_cell_guid":"bee2e9ec-de11-41d2-a1ae-22c18a493c77","_uuid":"771bc62e-752c-4fb8-b960-18dc74f3c052","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"_cell_guid":"6a2ee762-e48c-4c53-9b2f-7409b0fad241","_uuid":"4ba8579a-74ac-4775-a708-4dc931d57dfc","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"from pytorch_lightning.utilities.types import STEP_OUTPUT\n\n\nclass Model(pl.LightningModule):\n    def __init__(self, num_classes, num_train_optimization_steps,\n                 deeper_layer_to_train=11,\n                 learning_rate=1e-5, warmup_steps=0, weight_decay=0.01, adam_epsilon=1e-08):\n        super(Model, self).__init__()\n        self.config = AutoConfig.from_pretrained(cfg['model'], output_hidden_states=True)\n        # pretrained_cfg.num_labels = 1\n        \n        self.pretrained_model = AutoModel.from_config(config=self.config)\n        # self.classifier = nn.Linear(self.pretrained_model.config.hidden_size * MAX_LENGTH, num_classes)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        # self.configure_trained_layers(deeper_layer_to_train)\n        self._init_weights(self.attention)\n        self.save_hyperparameters()\n        \n        # debug\n        self.total_true = np.array([])\n        self.total_pred = np.array([])\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def configure_trained_layers(self, deeper_layer_to_train, verbose=1):\n        requires_grad = False\n        print(f'deeper_layer_to_train: {deeper_layer_to_train}')\n        for param in self.pretrained_model.named_parameters():\n            if f'encoder.layer.{deeper_layer_to_train}' in param[0]:\n                requires_grad = True\n            param[1].requires_grad = requires_grad\n            if verbose == 2 or (verbose == 1 and requires_grad):\n                print(f'layer {param[0]} is {\"NOT \" if requires_grad is False else \"\"}trained.')\n\n    def forward(self, ids, mask):\n        out = self.pretrained_model(ids, attention_mask=mask, return_dict=False)\n        # out = torch.relu(self.classifier(out[0].view(out[0].size(0), -1)))\n        weights = self.attention(out[0])\n        feature = torch.sum(weights * out[0], dim=1)\n        return self.fc(feature).view(-1)\n\n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': self.hparams.weight_decay},\n            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        \n        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        print(f'current learning rate: {self.hparams.learning_rate}')\n        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.num_train_optimization_steps, verbose=True)\n        \n        scheduler = transformers.get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.num_train_optimization_steps\n        )\n        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n        \n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n        loss = self._common_step(batch, batch_idx, 'train')\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._common_step(batch, batch_idx, 'val')\n\n    def _common_step(self, batch, batch_idx, stage: str):\n        ids, label, mask = self._prepare_batch(batch)\n        output = self(ids=ids, mask=mask)\n\n        # loss = F.cross_entropy(output, label)\n        # loss = F.mse_loss(output, label)\n        loss = F.binary_cross_entropy_with_logits(output, label)\n        # y_true = torch.argmax(label, dim=-1).view(-1)\n        # y_pred = torch.argmax(output, dim=-1).view(-1)\n        \n        if stage == 'train':\n            self.total_true = np.concatenate([self.total_true, label.view(-1).clone().cpu().detach().numpy()])\n            self.total_pred = np.concatenate([self.total_pred, output.view(-1).clone().cpu().detach().numpy()])\n\n        if len(self.total_true) >= 1200 and stage == 'train':\n            pearson = np.round(sp.stats.pearsonr(self.total_true, self.total_pred)[0], 3)\n            print('batched_pearson', pearson)\n            self.log(f\"batched_pearson\", pearson, on_step=True, prog_bar=True)\n            self.total_true = np.array([])\n            self.total_pred = np.array([])\n        \n        y_true = torch.round(label.cpu().detach() * 4).to(int)\n        y_pred = torch.min(torch.max(torch.round(output.cpu().detach() * 4).to(int), torch.tensor(0)), torch.tensor(4))\n\n        self._log_metrics(loss, y_true, y_pred, label.cpu().detach(), stage)\n        return loss\n\n    def _log_metrics(self, loss, y_true, y_pred, raw_y_true, stage: str) -> None:\n        # Compute metrics\n        acc = (y_true == y_pred).float().mean()\n        pearson = np.round(sp.stats.pearsonr(y_true, y_pred)[0], 3)\n\n        # Log metrics\n        self.log(f\"Loss/{stage}\", loss, on_step=True, prog_bar=True)\n        self.log(f\"Accuracy/{stage}\", acc, on_step=True, prog_bar=True)\n        self.log(f\"Pearson/{stage}\", pearson, on_step=True, prog_bar=True)\n        \n        if stage == 'val':  # do classification report\n            classification_report = metrics.classification_report(y_true, y_pred, digits=2, output_dict=True)\n            macro_precision, macro_recall, macro_f1, _ = classification_report['macro avg'].values()\n            weighted_precision, weighted_recall, weighted_f1, _ = classification_report['weighted avg'].values()\n            self.log(f'Macro/{stage}', {'precision': macro_precision, 'recall': macro_recall, 'f1': macro_f1}, prog_bar=False, on_step=True)\n            self.log(f'Weighted/{stage}', {'precision': weighted_precision, 'recall': weighted_recall, 'f1': weighted_f1}, prog_bar=False, on_step=True)\n\n\n    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n        ids, _, mask = self._prepare_batch(batch, include_target=False)\n        output = self(ids=ids, mask=mask)\n        print(f'predict {output.shape}')\n        return torch.argmax(output, dim=-1)\n\n    def test_step(self, batch, batch_idx):\n        self._common_step(batch, batch_idx, 'test')\n\n    def _prepare_batch(self, batch, include_target=True):\n        ids = batch['ids']\n        mask = batch['mask']\n        if not include_target:\n            return ids, None, mask\n        label = batch['target']\n        return ids, label, mask\n","metadata":{"_cell_guid":"a86783d4-02a1-46ed-83f2-b3101d98f0ef","_uuid":"6d6ab46f-7b45-4561-aa6b-58b975316535","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\nfrom torch import nn\n\n# Add the WandbLogger to your PyTorch Lightning Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nimport logging\nfrom logging import WARNING\nlogging.basicConfig(level=WARNING)\n\n#import wandb\n#wandb.login(key='70c6395b4eb3a3d517ea0020904b1e5ae8c8ad0c')\n\n\nearly_stop_callback = EarlyStopping(monitor=\"Loss/val\", min_delta=1e-4, patience=3, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor(logging_interval='step')\n# wb_logger = WandbLogger(name=f\"{cfg['model']}_local\", save_dir='./logs/WB_logs', offline=False, project='PPPM', tags=[\"deberta_fine_tune\", cfg['model']], config=cfg)\ntb_logger = TensorBoardLogger(save_dir=\"./logs/TensorBoard_logs\", name=\"DeBERTa_fine-tuned\")\n\n\ntrainer = pl.Trainer(\n    accelerator='gpu',\n    gradient_clip_val=cfg['gradient_clip_val'],\n    auto_lr_find=False,\n    callbacks=[lr_logger, early_stop_callback],\n    #logger=[tb_logger, wb_logger],\n    logger=[tb_logger],\n    weights_summary=\"top\",\n    max_epochs=1,\n    accumulate_grad_batches=cfg['accumulate_grad_batches'],\n    precision=16,\n    amp_backend=\"native\"\n)\n\nhparams = dict(\n    num_classes=1, # df_train.score.unique().size,\n    deeper_layer_to_train=11,  # Not used\n    num_train_optimization_steps=len(train_dataloader),\n    learning_rate=cfg['learning_rate']\n)\n\ncheckpoint = None\nif checkpoint is not None:\n    model = Model.load_from_checkpoint(checkpoint, **hparams)\n    print(f'Checkpoint {checkpoint} loaded')\nelse:\n    model = Model(**hparams)\n\n\n# 70c6395b4eb3a3d517ea0020904b1e5ae8c8ad0c","metadata":{"_cell_guid":"0ed57f80-0c25-447f-9a4e-f5f9db288d80","_uuid":"f23ffb04-71d9-4528-8257-ec7b954c4242","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\nimport scipy as sp\nfrom scipy.stats import PearsonRConstantInputWarning\n\nwarnings.filterwarnings('ignore', category=PearsonRConstantInputWarning)","metadata":{"tags":[],"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.validate(model, val_dataloader)","metadata":{"_cell_guid":"5818b955-849f-4d6f-ba61-2219a8ee5f68","_uuid":"ed7172fe-fc58-4d9d-a6c4-f8978883bcc6","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n# 10.2go 12.9go","metadata":{"_cell_guid":"1ecc1b28-ffce-41f2-ab2e-5b37c130546d","_uuid":"ee7a4a8a-847a-4e12-bf49-5c8ed37057b3","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model, val_dataloader)","metadata":{"_cell_guid":"5818b955-849f-4d6f-ba61-2219a8ee5f68","_uuid":"ed7172fe-fc58-4d9d-a6c4-f8978883bcc6","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#results = trainer.predict(model, test_dataloader)\n\n#df_test['y_pred'] = np.concatenate(results)\n#df_test","metadata":{"_cell_guid":"c55b9750-f11f-4db6-9c5d-6ad7554d0c2e","_uuid":"ed881703-ade6-4c38-b8bc-c75bf157ed41","collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]}]}