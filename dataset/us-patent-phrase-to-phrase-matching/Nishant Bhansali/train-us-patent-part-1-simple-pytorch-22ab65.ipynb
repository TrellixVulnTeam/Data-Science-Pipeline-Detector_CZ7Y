{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import StratifiedKFold\nfrom itertools import chain","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-29T17:46:44.151265Z","iopub.execute_input":"2022-03-29T17:46:44.151534Z","iopub.status.idle":"2022-03-29T17:46:44.156217Z","shell.execute_reply.started":"2022-03-29T17:46:44.151505Z","shell.execute_reply":"2022-03-29T17:46:44.15537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    device = 'cuda'\n    model = \"anferico/bert-for-patents\"\n    tokenizer =  AutoTokenizer.from_pretrained(model)\n    max_len = 128\n    folds = 5\n    train_batch_size = 16\n    valid_batch_size = 16\n    epochs = 6\n    lr = 2e-5\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:47.709813Z","iopub.execute_input":"2022-03-29T17:32:47.710034Z","iopub.status.idle":"2022-03-29T17:32:52.357345Z","shell.execute_reply.started":"2022-03-29T17:32:47.710007Z","shell.execute_reply":"2022-03-29T17:32:52.356551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, text, targets, score):\n        self.text = text\n        self.targets = targets\n        self.score = score\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        text = self.text[item]\n        targets = self.targets[item]\n        score = self.score[item]\n        \n        encoded_text = config.tokenizer.encode_plus(text, targets, padding=\"max_length\",\n                                                    max_length=config.max_len, truncation=True,)\n        \n        return {\n            \"ids\": torch.tensor(encoded_text[\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(encoded_text[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoded_text[\"token_type_ids\"], dtype=torch.long),\n            \"score\": torch.tensor(score, dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:52.358762Z","iopub.execute_input":"2022-03-29T17:32:52.359064Z","iopub.status.idle":"2022-03-29T17:32:52.36755Z","shell.execute_reply.started":"2022-03-29T17:32:52.359027Z","shell.execute_reply":"2022-03-29T17:32:52.366874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engine","metadata":{}},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    train_loss = 0.0\n    for data in data_loader :\n        ids = data['ids'].to(device, dtype=torch.long)\n        mask = data['mask'].to(device, dtype=torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n        score = data['score'].to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(\n         ids=ids,\n         mask=mask,\n         token_type_ids=token_type_ids\n         )\n        \n        loss = nn.BCEWithLogitsLoss()(outputs, score.view(-1, 1))\n        loss.backward()\n        \n        optimizer.step()\n        scheduler.step()\n        \n        train_loss +=loss.item()\n    print(f'train BCE loss is {train_loss/len(data_loader)}')\n        \ndef valid_fn(data_loader, model, device):\n    model.eval()\n    val_loss = 0.0\n    final_score = []\n    final_outputs = []\n    with torch.no_grad():\n        for data in data_loader :\n            ids = data['ids'].to(device, dtype=torch.long)\n            mask = data['mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            score = data['score'].to(device, dtype=torch.float)\n\n            outputs = model(\n             ids=ids,\n             mask=mask,\n             token_type_ids=token_type_ids\n             )\n\n            val_loss += nn.BCEWithLogitsLoss()(outputs,score.view(-1, 1))\n            \n            score = (score.detach().cpu().numpy()).tolist()\n            outputs = (torch.sigmoid(outputs).detach().cpu().numpy()).tolist()\n            final_outputs.extend(outputs)\n            final_score.extend(score)\n            \n            \n    print(f\"valid BCE loss : {val_loss/len(data_loader)}\")\n    return final_outputs,final_score  \n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:52.369916Z","iopub.execute_input":"2022-03-29T17:32:52.370529Z","iopub.status.idle":"2022-03-29T17:32:52.384548Z","shell.execute_reply.started":"2022-03-29T17:32:52.370489Z","shell.execute_reply":"2022-03-29T17:32:52.383721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class PhraseModel(nn.Module): \n    def __init__(self):\n        super().__init__()\n        \n        model_config = AutoConfig.from_pretrained(config.model)  ## credits https://www.kaggle.com/code/abhishek/tez-training-phrase-matching\n        model_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(config.model, config=model_config)\n        self.dropout = nn.Dropout(model_config.hidden_dropout_prob)\n        self.output = nn.Linear(model_config.hidden_size, 1)\n        \n    def forward(self, ids, mask, token_type_ids):\n        transformer_out = self.transformer(ids, mask, token_type_ids)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:52.386379Z","iopub.execute_input":"2022-03-29T17:32:52.386655Z","iopub.status.idle":"2022-03-29T17:32:52.397225Z","shell.execute_reply.started":"2022-03-29T17:32:52.386619Z","shell.execute_reply":"2022-03-29T17:32:52.396457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# manage CSV file and folds","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\ncontext_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n    }\n\ndf.context = df.context.apply(lambda x: context_mapping[x[0]])\ndf[\"text\"] = df.context + \" \" + df.anchor\ndf = df.drop(columns = [\"context\", \"anchor\"])\n\n## folds from https://www.kaggle.com/code/abhishek/phrase-matching-folds\n\ndf['kfold'] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# bin targets\ndf.loc[:, \"bins\"] = pd.cut(\n        df[\"score\"], bins=5, labels=False\n    )\n\nkf = StratifiedKFold(n_splits=config.folds, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.bins.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold\n        \ndf = df.drop(\"bins\", axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:52.398367Z","iopub.execute_input":"2022-03-29T17:32:52.398587Z","iopub.status.idle":"2022-03-29T17:32:53.399428Z","shell.execute_reply.started":"2022-03-29T17:32:52.398557Z","shell.execute_reply":"2022-03-29T17:32:53.398667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:53.400577Z","iopub.execute_input":"2022-03-29T17:32:53.403123Z","iopub.status.idle":"2022-03-29T17:32:53.42328Z","shell.execute_reply.started":"2022-03-29T17:32:53.403093Z","shell.execute_reply":"2022-03-29T17:32:53.42246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"for fold in range(config.folds):\n    \n    model = PhraseModel()\n    model.to(config.device)\n    \n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    df_train = df_train.drop(columns = 'kfold')\n    df_valid = df_valid.drop(columns = 'kfold')\n    \n    train_dataset = CustomDataset(text = df_train.text.values, targets = df_train.target.values, score = df_train.score.values)\n    train_loader = DataLoader(train_dataset, batch_size = config.train_batch_size,shuffle=True)\n    \n    valid_dataset = CustomDataset(text = df_valid.text.values, targets = df_valid.target.values, score = df_valid.score.values)\n    valid_loader = DataLoader(valid_dataset, batch_size = config.valid_batch_size,shuffle=False)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    num_train_steps = int(len(df_train) / config.train_batch_size * config.epochs)\n\n    optimizer = torch.optim.AdamW(optimizer_parameters, lr=config.lr)\n    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_steps,)\n\n    best_pearson = -999\n\n    for epoch in range(config.epochs):\n        train_fn(train_loader, model, optimizer, config.device, scheduler)\n        outputs, labels = valid_fn(valid_loader, model, config.device)\n\n        pearson_score = np.corrcoef(list(chain.from_iterable(outputs)), labels)[0][1]\n\n        print(f\"pearson Score = {pearson_score}\")\n\n        if pearson_score > best_pearson:\n            torch.save(model.state_dict(), f'model-epoch{epoch}-fold-{fold}.pth')\n            best_pearson = pearson_score\n     \n    break    ","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:32:53.424576Z","iopub.execute_input":"2022-03-29T17:32:53.424904Z","iopub.status.idle":"2022-03-29T17:41:47.934026Z","shell.execute_reply.started":"2022-03-29T17:32:53.424866Z","shell.execute_reply":"2022-03-29T17:41:47.933104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = list(chain.from_iterable(outputs)) \npearson_score = np.corrcoef(outputs, labels)[0][1]\n\nprint(pearson_score)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T17:46:53.431378Z","iopub.execute_input":"2022-03-29T17:46:53.432121Z","iopub.status.idle":"2022-03-29T17:46:53.443145Z","shell.execute_reply.started":"2022-03-29T17:46:53.432074Z","shell.execute_reply":"2022-03-29T17:46:53.442367Z"},"trusted":true},"execution_count":null,"outputs":[]}]}