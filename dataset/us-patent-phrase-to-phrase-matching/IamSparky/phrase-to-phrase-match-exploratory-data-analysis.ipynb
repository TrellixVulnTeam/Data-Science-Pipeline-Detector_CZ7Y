{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://literacyideas.com/wp-content/uploads/2021/08/2_different_text_types-1024x768.png\"></center>\n\n<h1><center>üììDetecting Simalrity Score in between 2 phrasesüìì</center></h1>\n\n# 1. Introduction\n\n1. Through this notebook my aim is to do <b>exploratory data analysis</b> on the given small set of text data and derive some intution out of it.\n\n2. As this is a text tabular data so I will be trying to add some <b>NLP related framework</b> to generate some meaningful onformation out from the data.\n\n3. Finally, I will be trying to add the <b>visual intepretation</b> of the data that I infer from the given data table.\n\n### Adding the libraries required ‚Äãüìè‚Äãüìö‚Äã","metadata":{}},{"cell_type":"code","source":"# ---- Necessary Libraries for data analysis ----\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore') # ignoring warinings from the frameworks run in each cell ","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.825387Z","iopub.execute_input":"2022-05-30T12:26:45.825994Z","iopub.status.idle":"2022-05-30T12:26:45.830494Z","shell.execute_reply.started":"2022-05-30T12:26:45.825959Z","shell.execute_reply":"2022-05-30T12:26:45.829842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Importing the train.csv file emoji üóÑ","metadata":{}},{"cell_type":"code","source":"train_dataframe = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\n\n#---- seeing the first 10 rows and columns of the train dataframe -----\ntrain_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.840551Z","iopub.execute_input":"2022-05-30T12:26:45.841081Z","iopub.status.idle":"2022-05-30T12:26:45.914933Z","shell.execute_reply.started":"2022-05-30T12:26:45.841025Z","shell.execute_reply":"2022-05-30T12:26:45.914125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- checking the rows and columns in the data ---\ntrain_dataframe.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.91651Z","iopub.execute_input":"2022-05-30T12:26:45.916743Z","iopub.status.idle":"2022-05-30T12:26:45.92141Z","shell.execute_reply.started":"2022-05-30T12:26:45.916715Z","shell.execute_reply":"2022-05-30T12:26:45.92074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- checking null values in rows and columns of the dataframe ---\ntrain_dataframe.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.922309Z","iopub.execute_input":"2022-05-30T12:26:45.922731Z","iopub.status.idle":"2022-05-30T12:26:45.950404Z","shell.execute_reply.started":"2022-05-30T12:26:45.922697Z","shell.execute_reply":"2022-05-30T12:26:45.948728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p>üìå<b>Result</b>: In <code>train_dataframe</code> there are <b>Non Null 36473 rows & 5 columns</b> </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# 3. Univariate Analysis of the data üóÑ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Note: ‚òÄÔ∏è </b> \n<p>1. Next we are moving to check <b>score</b> column in <code>train_dataframe</code> as this is column which we are going to do predictions for. Thereby, checking on its uniform distribution. </p>\n    \n<p>2. Next it is being mentioned in data description that different vaules of score means different for the values existing in <i><b>anchor</b></i>, <i><b>target</b></i> & <i><b>context</b></i> columns :--></p> \n        * 1.0 - <b>Very close match</b>. This is typically an exact match except possibly for differences in conjugation, quantity (e.g. singular vs. plural), and addition or removal of stopwords (e.g. ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúor‚Äù).<br>\n        * 0.75 - <b>Close synonym</b>, e.g. ‚Äúmobile phone‚Äù vs. ‚Äúcellphone‚Äù. This also includes abbreviations, e.g. \"TCP\" -> \"transmission control protocol\".<br>\n        * 0.5 - <b>Synonyms which don‚Äôt have the same meaning</b> (same function, same properties). This includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.<br>\n        * 0.25 - <b>Somewhat related</b>, e.g. the two phrases are in the same high level domain but are not synonyms. This also includes antonyms.<br>\n        * 0.0 - <b>Unrelated</b>.<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"> ### ‚Ü™Ô∏è 3.1 Analysing the score column first","metadata":{}},{"cell_type":"code","source":"# --- making the score column categorical for doing better analysis\ndef score_chainging(value):\n    if value == 1.0:\n        return \"Very close match(Value = 1.0)\"\n    elif value == 0.75:\n        return \"Close synonym(Value = 0.75)\"\n    elif value == 0.5:\n        return \"Synonyms which don‚Äôt have the same meaning(Value = 0.5)\"\n    elif value == 0.25:\n        return \"Somewhat related(Value = 0.25)\"\n    else:\n        return \"Unrelated(Value = 0.0)\"\n    \ntrain_dataframe['new_score'] = train_dataframe['score'].apply(score_chainging)\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.952453Z","iopub.execute_input":"2022-05-30T12:26:45.953134Z","iopub.status.idle":"2022-05-30T12:26:45.978678Z","shell.execute_reply.started":"2022-05-30T12:26:45.953087Z","shell.execute_reply":"2022-05-30T12:26:45.977706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nsns.set(rc = {'figure.figsize':(23, 10)})\nsns.countplot(x=\"new_score\", data=train_dataframe, order = train_dataframe['new_score'].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:45.979923Z","iopub.execute_input":"2022-05-30T12:26:45.980222Z","iopub.status.idle":"2022-05-30T12:26:46.236513Z","shell.execute_reply.started":"2022-05-30T12:26:45.980182Z","shell.execute_reply":"2022-05-30T12:26:46.235778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataframe.new_score.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.238249Z","iopub.execute_input":"2022-05-30T12:26:46.238538Z","iopub.status.idle":"2022-05-30T12:26:46.247831Z","shell.execute_reply.started":"2022-05-30T12:26:46.238499Z","shell.execute_reply":"2022-05-30T12:26:46.24711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- making a pie chart ----\nlabels = 'Synonyms which don‚Äôt have the same meaning', 'Somewhat related', 'Unrelated', 'Close synonym', 'Very close match'\nsizes = [round(i/len(train_dataframe)*100) for i in [12300, 11519, 7471, 4029, 1154]]\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05)\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.249125Z","iopub.execute_input":"2022-05-30T12:26:46.249323Z","iopub.status.idle":"2022-05-30T12:26:46.483187Z","shell.execute_reply.started":"2022-05-30T12:26:46.249299Z","shell.execute_reply":"2022-05-30T12:26:46.482236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n<b>Note:‚òÄÔ∏è </b> \n<p>1. From the above Pie Chart I come to infer that <b>score</b> column with values <code>0.5</code> & <code>0.25</code> are almost evenly dsitributed</p>\n<p>2. Whereas the other values of the score column is unevenly distributed. </p>","metadata":{}},{"cell_type":"markdown","source":"> ### ‚Ü™Ô∏è 3.2 Now Analysing the Context column Next","metadata":{}},{"cell_type":"code","source":"# --- deriving a new context category column from th given dict\ndict = {\n    'A': 'Human Necessities',\n    'B': 'Operations and Transport',\n    'C': 'Chemistry and Metallurgy',\n    'D': 'Textiles',\n    'E': 'Fixed Constructions',\n    'F': 'Mechanical Engineering',\n    'G': 'Physics',\n    'H': 'Electricity',\n    'Y': 'Emerging Cross-Sectional Technologies'\n}\n\ntrain_dataframe['context_new'] = train_dataframe['context'].apply(lambda x: dict[x[0]])\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.484296Z","iopub.execute_input":"2022-05-30T12:26:46.48455Z","iopub.status.idle":"2022-05-30T12:26:46.508164Z","shell.execute_reply.started":"2022-05-30T12:26:46.484519Z","shell.execute_reply":"2022-05-30T12:26:46.507596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataframe['context_new'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.509218Z","iopub.execute_input":"2022-05-30T12:26:46.509536Z","iopub.status.idle":"2022-05-30T12:26:46.525823Z","shell.execute_reply.started":"2022-05-30T12:26:46.509508Z","shell.execute_reply":"2022-05-30T12:26:46.525223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc = {'figure.figsize':(23, 10)})\nsns.countplot(x=\"context_new\",\n              data=train_dataframe,\n              order = train_dataframe['context_new'].value_counts().index\n             )","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.527548Z","iopub.execute_input":"2022-05-30T12:26:46.527952Z","iopub.status.idle":"2022-05-30T12:26:46.813259Z","shell.execute_reply.started":"2022-05-30T12:26:46.527919Z","shell.execute_reply":"2022-05-30T12:26:46.812553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--- generating a pie chart out of it---\nlabels = 'Human Necessities', 'Chemistry and Metallurgy', 'Mechanical Engineering', 'Electricity', 'Operations and Transport', 'Textiles', 'Fixed Constructions', 'Physics'\nsizes = [round(i/len(train_dataframe)*100) for i in [4094, 5288, 4054, 6195, 8095, 1279, 1531, 6013]]\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:46.81433Z","iopub.execute_input":"2022-05-30T12:26:46.814663Z","iopub.status.idle":"2022-05-30T12:26:47.074456Z","shell.execute_reply.started":"2022-05-30T12:26:46.814635Z","shell.execute_reply":"2022-05-30T12:26:47.073652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n<b>Result:‚òÄÔ∏è </b> \n<p>1. Inference generated from the above pie chart is that the values for <code>Fixed Construction</code> and <code>Textiles</code> are same but their frequency is pretty less in comparison to other values</p>\n<p>2. Whereas the other values of the <b>context</b> column is almost being equally distributed. </p>","metadata":{}},{"cell_type":"markdown","source":"> ### ‚Ü™Ô∏è 3.3 Finally Analysing the anchor column ","metadata":{}},{"cell_type":"code","source":"train_dataframe['anchor'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:47.075755Z","iopub.execute_input":"2022-05-30T12:26:47.075978Z","iopub.status.idle":"2022-05-30T12:26:47.086589Z","shell.execute_reply.started":"2022-05-30T12:26:47.07595Z","shell.execute_reply":"2022-05-30T12:26:47.085596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Taking only the top 6 values in count from the train dataframe to analyze the anchor coloumn -----\ndf = train_dataframe[train_dataframe['anchor'].isin(['component composite coating', 'sheet supply roller', 'source voltage', 'perfluoroalkyl group', 'el display'])]\n\nsns.set(rc = {'figure.figsize':(23, 10)})\nsns.countplot(x=\"anchor\",\n              data= df,\n              order = df['anchor'].value_counts().index\n             )","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:47.087899Z","iopub.execute_input":"2022-05-30T12:26:47.088161Z","iopub.status.idle":"2022-05-30T12:26:47.338454Z","shell.execute_reply.started":"2022-05-30T12:26:47.088131Z","shell.execute_reply":"2022-05-30T12:26:47.337869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n<b>Result:‚òÄÔ∏è </b> \n<p>1. Inference generated from the diagram is that the frequency of <b>'component composite coating', 'sheet supply roller', 'source voltage', 'perfluoroalkyl group', 'el display'</b>values is pretty high in comparison to other values in the <code>train_dataframe[['anchor']]</code> dataframe</p>","metadata":{}},{"cell_type":"markdown","source":"# 4. Bivariate Analysis of the data üóÑ","metadata":{}},{"cell_type":"code","source":"sns.set(rc = {'figure.figsize':(23, 10)})\n\nsns.countplot(x=\"context_new\", hue=\"new_score\", data=train_dataframe)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:47.33939Z","iopub.execute_input":"2022-05-30T12:26:47.339872Z","iopub.status.idle":"2022-05-30T12:26:47.773716Z","shell.execute_reply.started":"2022-05-30T12:26:47.339838Z","shell.execute_reply":"2022-05-30T12:26:47.772799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n<b>Result:‚òÄÔ∏è </b> \n<p>On the overall note I can see that :</p>\n<p>1. <b>Electricity</b> value from <b><i>\"context_new\"</i></b> column contributes more towards <code><b>Synonyms which don‚Äôt have the same meaning(Value = 0.5)</b></code></p>\n<p>2. <b>Operations and Transport</b> value from <b><i>\"context_new\"</i></b> column contributes more towards <code><b>Close synonym(Value = 0.75)</b></code></p>\n<p>3. <b>Operations and Transport</b> value from <b><i>\"context_new\"</i></b> column contributes more towards <code><b>Somewhat realted(Value = 0.25)</b></code></p>\n<p>4. <b>Operations and Transport</b> value from <b><i>\"context_new\"</i></b> column contributes more towards <code><b>Unrelated(Value = 0.0)</b></code></p>\n<p>5. <b>Operations and Transport</b> value from <b><i>\"context_new\"</i></b> column contributes more towards <code><b>Very close match(Value = 1.0)</b></code></p>\n    \n<p>Finally, my overall analysis says that, from the diagram generated above I can see <b>Operations and Transport</b> value from <b><i>\"context_new\"</i></b> column is contributing more towards <code><b>score</b></code> value generation in comparison to other values present in the column</p>","metadata":{}},{"cell_type":"markdown","source":"# 5. Defining correlation between the data üóÑ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Note: ‚òÄÔ∏è </b> \n<p>We are going to generate 3 more numerical columns from <code>target</code> column in order to define how much this column is related in defining the <code>score</code> column.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"#calculating length of the target values\ntrain_dataframe['target_sentence_length_count'] = train_dataframe['target'].apply(lambda x: len(x))\n\n# calculating the noumber of characters in the sentence without space\ntrain_dataframe['target_characters_count_without_spaces'] = train_dataframe['target'].apply(lambda x: len(''.join(set(x.replace(' ', '')))))\n\n# calculating count of words in the values\ntrain_dataframe['target_word_count'] = train_dataframe['target'].apply(lambda x: len(x.split()))\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:26:47.775171Z","iopub.execute_input":"2022-05-30T12:26:47.775526Z","iopub.status.idle":"2022-05-30T12:26:47.893584Z","shell.execute_reply.started":"2022-05-30T12:26:47.775484Z","shell.execute_reply":"2022-05-30T12:26:47.892776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_df = train_dataframe[['target_sentence_length_count', 'target_characters_count_without_spaces', 'target_word_count', 'score']]\nsns.heatmap(correlation_df.corr(), cmap=\"YlGnBu\", annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:32:08.781809Z","iopub.execute_input":"2022-05-30T12:32:08.782366Z","iopub.status.idle":"2022-05-30T12:32:09.126781Z","shell.execute_reply.started":"2022-05-30T12:32:08.78233Z","shell.execute_reply":"2022-05-30T12:32:09.125975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n<b>Result:‚òÄÔ∏è </b> \n<p>The reason I plot the correlation because the new columns that we generated on that I need to see how much they are related to score column.</p>\n<p>We generally consider a strong correlation when the correlation value is more than or equal to 0.7</p>\n<p>In this case, the correlation of the new column with the dependent column score is relatively low so we can't use these columns for model generation</p>","metadata":{}},{"cell_type":"markdown","source":"## Rest Work in Progress.......\n### Working on different transformer models to genrate a prediction for the dependent data colimn SCORE.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}