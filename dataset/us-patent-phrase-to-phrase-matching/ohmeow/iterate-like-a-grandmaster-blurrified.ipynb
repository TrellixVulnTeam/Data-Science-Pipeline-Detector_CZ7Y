{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Iterate like a grandmaster ... blurrified!","metadata":{}},{"cell_type":"markdown","source":"Jeremy Howard recently released a wonderful [notebook](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/) on how to tackle the [U.S. Patent Phrase to Phrase Matching](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/) competition effectively with a focus on two key topics:\n\n1. Creating an effective validation set\n1. Iterating rapidly to find changes which improve results on the validation set.\n\nAs the author of the [blurr](https://ohmeow.github.io/blurr/) library, a fastai first framework for training huggingface transformers, I thought it might be fun to ***blurrify*** his notebook and demonstrate how one might work through this competition using it rather the Hugging Face Trainer API. Along the way I'll point out some **TIP**s for those looking to use blurr as well.\n\nSo without further ado, lets go!!!","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Install and imports","metadata":{}},{"cell_type":"code","source":"! pip install -q ohmeow-blurr==1.0.4","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:30.842427Z","iopub.execute_input":"2022-04-15T03:11:30.843117Z","iopub.status.idle":"2022-04-15T03:11:38.628541Z","shell.execute_reply.started":"2022-04-15T03:11:30.843021Z","shell.execute_reply":"2022-04-15T03:11:38.627521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will get you blurr, fastai, transformers, etc....  To keep things simple, I'm going to import all the data and modeling bits from blurr.","metadata":{}},{"cell_type":"code","source":"import gc\n\nfrom fastai.callback.all import *\nfrom fastai.data.block import CategoryBlock, ColReader, DataBlock, IndexSplitter, RegressionBlock\nfrom fastai.imports import *\nfrom fastai.learner import *\nfrom fastai.optimizer import Adam\nfrom fastai.metrics import *\nfrom fastai.torch_core import *\nfrom fastai.torch_imports import *\nfrom transformers import AutoModelForSequenceClassification, logging\n\nfrom blurr.text.data.core import TextBlock\nfrom blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\nfrom blurr.text.utils import get_hf_objects\nfrom blurr.utils import PreCalculatedCrossEntropyLoss, print_versions, set_seed\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:38.632083Z","iopub.execute_input":"2022-04-15T03:11:38.632338Z","iopub.status.idle":"2022-04-15T03:11:44.150625Z","shell.execute_reply.started":"2022-04-15T03:11:38.632307Z","shell.execute_reply":"2022-04-15T03:11:44.149844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before going through Jeremy's notebook, I didn't know I could even do the below and determine whether I'm running locally or on kaggle ... so I'm keeping this in here as well :)\n\n**TIP**: Define a variable that indicates whether you are running locally or on kagle.","metadata":{}},{"cell_type":"code","source":"is_kaggle = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:44.152086Z","iopub.execute_input":"2022-04-15T03:11:44.152336Z","iopub.status.idle":"2022-04-15T03:11:44.15753Z","shell.execute_reply.started":"2022-04-15T03:11:44.152302Z","shell.execute_reply":"2022-04-15T03:11:44.156746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Data and EDA","metadata":{"heading_collapsed":true}},{"cell_type":"markdown","source":"I have things setup a bit different locally, path wise, but other than that (and some formatting/naming differences) what follows here is from Jeremy's notebook. For all the EDA and resulting observations, see his notebook [here](https://www.kaggle.com/code/jhoward?scriptVersionId=92968513&cellId=7).\n\n**TIP**: Set your paths up to work when running locally or on kaggle.","metadata":{"hidden":true}},{"cell_type":"code","source":"if is_kaggle:\n    path = Path(\"../input/us-patent-phrase-to-phrase-matching\")\nelse:\n    path = Path(\"./data\")\n\npath.ls()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-04-15T03:11:44.159992Z","iopub.execute_input":"2022-04-15T03:11:44.160963Z","iopub.status.idle":"2022-04-15T03:11:44.176272Z","shell.execute_reply.started":"2022-04-15T03:11:44.16092Z","shell.execute_reply":"2022-04-15T03:11:44.175522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the training set:","metadata":{"hidden":true}},{"cell_type":"code","source":"train_df = pd.read_csv(path/\"train.csv\")\n\nprint(len(train_df))\ntrain_df.head()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-04-15T03:11:44.17778Z","iopub.execute_input":"2022-04-15T03:11:44.178196Z","iopub.status.idle":"2022-04-15T03:11:44.248373Z","shell.execute_reply.started":"2022-04-15T03:11:44.178158Z","shell.execute_reply":"2022-04-15T03:11:44.24755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...and the test set:","metadata":{"hidden":true}},{"cell_type":"code","source":"test_df = pd.read_csv(path/\"test.csv\")\n\nprint(len(test_df))\ntest_df.head()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-04-15T03:11:44.249883Z","iopub.execute_input":"2022-04-15T03:11:44.250144Z","iopub.status.idle":"2022-04-15T03:11:44.265927Z","shell.execute_reply.started":"2022-04-15T03:11:44.250106Z","shell.execute_reply":"2022-04-15T03:11:44.265148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['section'] = train_df.context.str[0]\ntrain_df.section.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:44.267037Z","iopub.execute_input":"2022-04-15T03:11:44.267369Z","iopub.status.idle":"2022-04-15T03:11:44.308061Z","shell.execute_reply.started":"2022-04-15T03:11:44.267331Z","shell.execute_reply":"2022-04-15T03:11:44.30734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TIP**: As you explore the counts and distributions, note your observations!","metadata":{}},{"cell_type":"markdown","source":"### Step 3: Get your Hugging Face objects\n\nTime to blurrify, and we'll begin to do so by using blurr's `get_hf_objects` method to get all the Hugging Face objects we need. We'll define our pretrained model checkpoint and some hyperparameters here as well.  Since we're structuring this as a regression problem, we need to tell the Hugging Face configuration object that we have only *one* label via the `config_kwargs` parameter.","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"microsoft/deberta-v3-small\" #\"distilroberta-base\"\nbatch_size = 128\nweight_decay = 0.01","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:44.309389Z","iopub.execute_input":"2022-04-15T03:11:44.309638Z","iopub.status.idle":"2022-04-15T03:11:44.313984Z","shell.execute_reply.started":"2022-04-15T03:11:44.309605Z","shell.execute_reply":"2022-04-15T03:11:44.313121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TIP**: For classification tasks, my go-to baseline architectures are roberta, deberta, and bart.\n\n**TIP**: To iterate quickly, choose a small or distilled model.","metadata":{}},{"cell_type":"code","source":"hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 1})","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:44.315721Z","iopub.execute_input":"2022-04-15T03:11:44.316093Z","iopub.status.idle":"2022-04-15T03:11:47.903953Z","shell.execute_reply.started":"2022-04-15T03:11:44.316054Z","shell.execute_reply":"2022-04-15T03:11:47.903076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4: Build your DataLoaders","metadata":{"heading_collapsed":true}},{"cell_type":"markdown","source":"#### 4a. Define a **good** validation set","metadata":{}},{"cell_type":"markdown","source":"We'll start by following the same strategy Jeremy went with in [defining our validation set](https://www.kaggle.com/code/jhoward?scriptVersionId=92968513&cellId=54). Once we got our training/validation indicies, we'll use the `IndexSplitter` splitter function to ensure our training and validation datasets get separated properly.\n\nFrom Jeremy's notebook: \"BTW, a lot of people do more complex stuff for creating their validation set, but with a dataset this large there's not much point. As you can see, the mean scores in the two groups are very similar despite just doing a random shuffle.\"\n\n**TIP**: A good validation set is critical to every ML solution. For more info, see [How to create good validation and test sets](https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html#How-to-create-good-validation-and-test-sets) in my blog post on lessons learned from chapter 1 of the fastbook, [\"Deep Learning for Coders with fastai & PyTorch\"](https://github.com/fastai/fastbook).","metadata":{"hidden":true}},{"cell_type":"code","source":"anchors = train_df.anchor.unique()\nnp.random.seed(42)\nnp.random.shuffle(anchors)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:47.907674Z","iopub.execute_input":"2022-04-15T03:11:47.907929Z","iopub.status.idle":"2022-04-15T03:11:47.916164Z","shell.execute_reply.started":"2022-04-15T03:11:47.907894Z","shell.execute_reply":"2022-04-15T03:11:47.915355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_prop = 0.25\nval_sz = int(len(anchors) * val_prop)\nval_anchors = anchors[:val_sz]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:47.917749Z","iopub.execute_input":"2022-04-15T03:11:47.918023Z","iopub.status.idle":"2022-04-15T03:11:47.923814Z","shell.execute_reply.started":"2022-04-15T03:11:47.917988Z","shell.execute_reply":"2022-04-15T03:11:47.922906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_val = np.isin(train_df.anchor, val_anchors)\n\nidxs = np.arange(len(train_df))\nval_idxs = idxs[ is_val]\ntrn_idxs = idxs[~is_val]\n\nlen(val_idxs),len(trn_idxs)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:47.925423Z","iopub.execute_input":"2022-04-15T03:11:47.92577Z","iopub.status.idle":"2022-04-15T03:11:48.134031Z","shell.execute_reply.started":"2022-04-15T03:11:47.92573Z","shell.execute_reply":"2022-04-15T03:11:48.133323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[trn_idxs].score.mean(), train_df.iloc[val_idxs].score.mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:48.135459Z","iopub.execute_input":"2022-04-15T03:11:48.135724Z","iopub.status.idle":"2022-04-15T03:11:48.152117Z","shell.execute_reply.started":"2022-04-15T03:11:48.135689Z","shell.execute_reply":"2022-04-15T03:11:48.151375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4b. Define your `DataBlock`","metadata":{}},{"cell_type":"markdown","source":"From Jeremy's notebook ... \"We'll need to combine the context, anchor, and target together somehow. There's not much research as to the best way to do this, so we may need to iterate a bit. To start with, we'll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator token to tell it.\"\n\n... and that is what I've done via the `build_inputs()` method.","metadata":{}},{"cell_type":"code","source":"set_seed(42)\nsep = f' {hf_tokenizer.sep_token} '\n\ndef build_inputs(example):\n    return f'{example[\"context\"]}{sep}{example[\"anchor\"]}{sep}{example[\"target\"]}'\n\nblocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), RegressionBlock)\ndblock = DataBlock(blocks=blocks, get_x=build_inputs, get_y=ColReader(\"score\"), splitter=IndexSplitter(val_idxs))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:48.153555Z","iopub.execute_input":"2022-04-15T03:11:48.15381Z","iopub.status.idle":"2022-04-15T03:11:48.164317Z","shell.execute_reply.started":"2022-04-15T03:11:48.153775Z","shell.execute_reply":"2022-04-15T03:11:48.163391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4c. Build your `DataLoaders`","metadata":{}},{"cell_type":"code","source":"dls = dblock.dataloaders(train_df, bs=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:11:48.165856Z","iopub.execute_input":"2022-04-15T03:11:48.16634Z","iopub.status.idle":"2022-04-15T03:12:21.007369Z","shell.execute_reply.started":"2022-04-15T03:11:48.1663Z","shell.execute_reply":"2022-04-15T03:12:21.006548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TIP**: Transformer models are very sensitive to how to prepare your inputs, and exploring different approaches are usually worthwhile.\n\n**TIP**: We'll use blurr's `set_seed()` method to ensure reproducibility (which is important as you iterate over different hyperparameters, explore different augmentation strategies, etc...). For a great discussion on how to do this in fastai, see the [\"[Solved] Reproducibility: Where is the randomness coming in?\"](https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/25) post on the forums.","metadata":{"hidden":true}},{"cell_type":"code","source":"dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:12:21.008964Z","iopub.execute_input":"2022-04-15T03:12:21.009206Z","iopub.status.idle":"2022-04-15T03:12:21.161454Z","shell.execute_reply.started":"2022-04-15T03:12:21.009171Z","shell.execute_reply":"2022-04-15T03:12:21.160768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tip**: Always take a look at your batches to make sure they look right and that you understand what they represent","metadata":{}},{"cell_type":"code","source":"b = dls.one_batch()\nlen(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-04-15T03:12:21.162765Z","iopub.execute_input":"2022-04-15T03:12:21.163174Z","iopub.status.idle":"2022-04-15T03:12:21.294533Z","shell.execute_reply.started":"2022-04-15T03:12:21.163135Z","shell.execute_reply":"2022-04-15T03:12:21.293693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:12:21.296034Z","iopub.execute_input":"2022-04-15T03:12:21.296294Z","iopub.status.idle":"2022-04-15T03:12:21.314232Z","shell.execute_reply.started":"2022-04-15T03:12:21.296259Z","shell.execute_reply":"2022-04-15T03:12:21.313434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Train\n\nSticking with blurr's mid-level API, let's define our `Learner`.  You can learn all about the blurr specific bits [here](https://ohmeow.github.io/blurr/text-modeling-core.html).\n\nFrom Jeremy's notebook: \"Let's now train our model! We'll need to specify a metric, which is the correlation coefficient.\"\n\n**TIP**: Your metric should reflect your objective, and in the case of kaggle comps they tell you exactly what that is.","metadata":{}},{"cell_type":"code","source":"set_seed(42)\n\nmodel = BaseModelWrapper(hf_model)\n\nlearn = Learner(\n    dls,\n    model,\n    opt_func=partial(Adam, wd=weight_decay),\n    loss_func=PreCalculatedCrossEntropyLoss(),\n    metrics=[PearsonCorrCoef()],\n    cbs=[BaseModelCallback],\n    splitter=blurr_splitter\n)\n\nlearn = learn.to_fp16()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:12:21.315302Z","iopub.execute_input":"2022-04-15T03:12:21.315518Z","iopub.status.idle":"2022-04-15T03:12:21.474741Z","shell.execute_reply.started":"2022-04-15T03:12:21.315469Z","shell.execute_reply":"2022-04-15T03:12:21.47399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find(start_lr=1e-9, suggest_funcs=[minimum, steep, valley, slide])","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:12:21.475906Z","iopub.execute_input":"2022-04-15T03:12:21.476243Z","iopub.status.idle":"2022-04-15T03:12:41.506091Z","shell.execute_reply.started":"2022-04-15T03:12:21.476205Z","shell.execute_reply":"2022-04-15T03:12:41.505337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using **1cycle** learning allows us to be a bit more aggressive with our learning rates.  Want to learn more? Check out the [\"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\" paper](https://arxiv.org/abs/1803.09820).\n\n**TIP**: Try different schedulers and see what works and what doesn't.","metadata":{}},{"cell_type":"code","source":"set_seed(42)\nlearn.fit_one_cycle(4, lr_max=slice(1e-5, 1e-2))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:12:41.507836Z","iopub.execute_input":"2022-04-15T03:12:41.508117Z","iopub.status.idle":"2022-04-15T03:16:26.109983Z","shell.execute_reply.started":"2022-04-15T03:12:41.508079Z","shell.execute_reply":"2022-04-15T03:16:26.109194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del learn\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:16:26.111565Z","iopub.execute_input":"2022-04-15T03:16:26.111834Z","iopub.status.idle":"2022-04-15T03:16:26.588177Z","shell.execute_reply.started":"2022-04-15T03:16:26.111797Z","shell.execute_reply":"2022-04-15T03:16:26.587292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Improving the model","metadata":{}},{"cell_type":"markdown","source":"As Jeremy says in his notebook, \"Iteration speed is critical, so we need to quickly be able to try different data processing and trainer parameters.\"\n\n... so lets create some helper methods to help us iterate more quickly. First, our `DataLoaders`:","metadata":{}},{"cell_type":"code","source":"def get_dls(df, get_x_func, batch_size, seed=42):\n    set_seed(seed)\n\n    blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), RegressionBlock)\n    dblock = DataBlock(blocks=blocks, get_x=get_x_func, get_y=ColReader(\"score\"), splitter=IndexSplitter(val_idxs))\n    \n    return dblock.dataloaders(df, bs=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:16:26.589995Z","iopub.execute_input":"2022-04-15T03:16:26.590702Z","iopub.status.idle":"2022-04-15T03:16:26.59754Z","shell.execute_reply.started":"2022-04-15T03:16:26.590658Z","shell.execute_reply":"2022-04-15T03:16:26.596864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...and also a function to create a `Learner`:","metadata":{}},{"cell_type":"code","source":"def get_learner(dls, hf_model, weight_decay=0.01, use_fp16=True, seed=42):\n    set_seed(42)\n\n    model = BaseModelWrapper(hf_model)\n\n    learn = Learner(\n        dls,\n        model,\n        opt_func=partial(Adam, wd=weight_decay),\n        loss_func=PreCalculatedCrossEntropyLoss(),\n        metrics=[PearsonCorrCoef()],\n        cbs=[BaseModelCallback],\n        splitter=blurr_splitter\n    )\n\n    if use_fp16:\n        learn = learn.to_fp16()\n        \n    return learn","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:16:26.598765Z","iopub.execute_input":"2022-04-15T03:16:26.59923Z","iopub.status.idle":"2022-04-15T03:16:26.606997Z","shell.execute_reply.started":"2022-04-15T03:16:26.599193Z","shell.execute_reply":"2022-04-15T03:16:26.606012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_inputs_with_sep(example, sep=\" - \", lower_case=False):\n    return f'{example[\"context\"]}{sep}{example[\"anchor\"]}{sep}{example[\"target\"]}'","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:16:26.608255Z","iopub.execute_input":"2022-04-15T03:16:26.608699Z","iopub.status.idle":"2022-04-15T03:16:26.615429Z","shell.execute_reply.started":"2022-04-15T03:16:26.608661Z","shell.execute_reply":"2022-04-15T03:16:26.614669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using our utility functions above, we can re-write your exploratory data prep/training loop as such:","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"microsoft/deberta-v3-small\"\nbatch_size = 128\nweight_decay = 0.01\nuse_fp16 = True\nseed = 42\n\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 1})\n\ndls = get_dls(train_df, get_x_func=partial(build_inputs_with_sep, sep=f' {hf_tokenizer.sep_token} '), batch_size=batch_size, seed=seed)\nlearn = get_learner(dls, hf_model, weight_decay=weight_decay, use_fp16=use_fp16, seed=seed)\n\nlearn.fit_one_cycle(4, lr_max=slice(1e-5, 1e-2))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:16:26.61652Z","iopub.execute_input":"2022-04-15T03:16:26.618619Z","iopub.status.idle":"2022-04-15T03:20:42.481242Z","shell.execute_reply.started":"2022-04-15T03:16:26.618579Z","shell.execute_reply":"2022-04-15T03:20:42.48041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TIP**: Make sure you instantiate fresh version of your Hugging Face objects each time.","metadata":{}},{"cell_type":"code","source":"try:\n    del learn\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:20:42.482972Z","iopub.execute_input":"2022-04-15T03:20:42.483844Z","iopub.status.idle":"2022-04-15T03:20:42.957764Z","shell.execute_reply.started":"2022-04-15T03:20:42.483801Z","shell.execute_reply":"2022-04-15T03:20:42.956973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's now try out some ideas...\n\nFrom Jeremy's notebook: \"**Perhaps using the special separator character isn't a good idea, and we should use something we create instead**. Let's see if that makes things better.\"","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"microsoft/deberta-v3-small\"\nbatch_size = 128\nweight_decay = 0.01\nuse_fp16 = True\nseed = 42\n\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 1})\n\ndls = get_dls(train_df, get_x_func=partial(build_inputs_with_sep, sep=\" [s] \"), batch_size=batch_size, seed=seed)\nlearn = get_learner(dls, hf_model, weight_decay=weight_decay, use_fp16=use_fp16, seed=seed)\n\nlearn.fit_one_cycle(4, lr_max=slice(1e-5, 1e-2))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:20:42.962716Z","iopub.execute_input":"2022-04-15T03:20:42.962932Z","iopub.status.idle":"2022-04-15T03:25:24.897304Z","shell.execute_reply.started":"2022-04-15T03:20:42.962905Z","shell.execute_reply":"2022-04-15T03:25:24.896531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del learn\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:25:24.899014Z","iopub.execute_input":"2022-04-15T03:25:24.89959Z","iopub.status.idle":"2022-04-15T03:25:25.451113Z","shell.execute_reply.started":"2022-04-15T03:25:24.899549Z","shell.execute_reply":"2022-04-15T03:25:25.450308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From Jeremy's notebook \"That's looking quite a bit better, so we'll keep that change. **Often changing to lowercase is helpful**. Let's see if that helps too.\"","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"microsoft/deberta-v3-small\"\nbatch_size = 128\nweight_decay = 0.01\nuse_fp16 = True\nseed = 42\n\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 1})\n\ndls = get_dls(train_df, get_x_func=partial(build_inputs_with_sep, sep=\" [s] \", lower_case=True), batch_size=batch_size, seed=seed)\nlearn = get_learner(dls, hf_model, weight_decay=weight_decay, use_fp16=use_fp16, seed=seed)\n\nlearn.fit_one_cycle(4, lr_max=slice(1e-5, 1e-2))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:25:25.452933Z","iopub.execute_input":"2022-04-15T03:25:25.453214Z","iopub.status.idle":"2022-04-15T03:30:07.744674Z","shell.execute_reply.started":"2022-04-15T03:25:25.453177Z","shell.execute_reply":"2022-04-15T03:30:07.743698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del learn\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:30:07.746689Z","iopub.execute_input":"2022-04-15T03:30:07.747337Z","iopub.status.idle":"2022-04-15T03:30:08.2797Z","shell.execute_reply.started":"2022-04-15T03:30:07.747286Z","shell.execute_reply":"2022-04-15T03:30:08.278738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From Jeremy's notebook: \"**What if we made the patent section a special token?** Then potentially the model might learn to recognize that different sections need to be handled in different ways. To do that, we'll use, e.g. `[A]` for section A. We'll then add those as special tokens.\"","metadata":{}},{"cell_type":"code","source":"train_df['sectok'] = f'[{train_df.section}]'\nsectoks = list(train_df.sectok.unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:30:08.281894Z","iopub.execute_input":"2022-04-15T03:30:08.282199Z","iopub.status.idle":"2022-04-15T03:30:08.306767Z","shell.execute_reply.started":"2022-04-15T03:30:08.282158Z","shell.execute_reply":"2022-04-15T03:30:08.306058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_inputs_with_sep_and_sectoks(example, sep=\" - \", lower_case=False):\n    return f'{example[\"sectok\"]}{sep}{example[\"context\"]}{sep}{example[\"anchor\"]}{sep}{example[\"target\"]}'","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:30:08.309338Z","iopub.execute_input":"2022-04-15T03:30:08.309997Z","iopub.status.idle":"2022-04-15T03:30:08.316276Z","shell.execute_reply.started":"2022-04-15T03:30:08.309962Z","shell.execute_reply":"2022-04-15T03:30:08.315337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"microsoft/deberta-v3-small\"\nbatch_size = 128\nweight_decay = 0.01\nuse_fp16 = True\nseed = 42\n\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 1})\n\n# After adding the new tokens, we need to resize the embedding matrix in the model and initialize the weights\nhf_tokenizer.add_special_tokens({'additional_special_tokens': sectoks})\nhf_model.resize_token_embeddings(len(hf_tokenizer))\n\nwith torch.no_grad():\n    hf_model.get_input_embeddings().weight[-len(hf_tokenizer), :] = torch.zeros([hf_model.config.hidden_size])\n\ndls = get_dls(train_df, get_x_func=partial(build_inputs_with_sep_and_sectoks, sep=\" [s] \", lower_case=True), batch_size=batch_size, seed=seed)\nlearn = get_learner(dls, hf_model, weight_decay=weight_decay, use_fp16=use_fp16, seed=seed)\n\nlearn.fit_one_cycle(4, lr_max=slice(1e-5, 1e-2))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:30:08.317962Z","iopub.execute_input":"2022-04-15T03:30:08.318436Z","iopub.status.idle":"2022-04-15T03:35:19.797342Z","shell.execute_reply.started":"2022-04-15T03:30:08.318394Z","shell.execute_reply":"2022-04-15T03:35:19.796367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Jeremy's list of things to try:\n\n- Try a model pretrained on legal vocabulary. E.g. how about [BERT for patents](https://huggingface.co/anferico/bert-for-patents)?\n- You'd likely get better results by using a sentence similarity model. Did you know that there's a [patent similarity model](https://huggingface.co/AI-Growth-Lab/PatentSBERTa) you could try?\n- You could also fine-tune any HuggingFace model using the full patent database (which is provided in BigQuery), before applying it to this dataset\n- Replace the patent context field with the description of that context provided by the patent office\n- ...and try out your own ideas too!\n\nWayde's list of things to try:\n\n- Experiment with other smaller models that typically work well for clssification tasks (e.g., roberta, deberta, and bart are my go tos)\n- Experiment with some bigger versions of those smaller models that worked well for you.\n- Be creative with your inputs; you can improve your results by adding special or regular tokens and/or structuring your inputs differently\n- Try using K-Fold or Stratified K-Fold cross validation and ensemble your results (see Jeremy's notebook for more info)\n- Once you have a decent set of hyperparameters working for you, you can use an optimization framework like Optuna and/or Weights & Biases to fine-tune your choices.\n- Read the papers related to the architectures you are using. Often you'll find recommended hyperparameter values and other important recommendations to training them well.\n\nAnd as Jeremy said, \"Before submitting a model, retrain it on the full dataset, rather than just the 75% training subset we've used here. Create a function like the ones above to make that easy for you!\"","metadata":{}},{"cell_type":"markdown","source":"## In conclusion\n\nI hope you've learned a little bit about training transformers with blurr, and may even be encouraged to give it a go on Kaggle or at work. If you enjoyed this notebook, **I would greatly appreciate an upvote**. \n\nPlease use the comments section below to ask any questions or share insights you may have on using blurr, fastai, and the transformers library to effectively train transformer models. For folks new to working with the Hugging Face transformers library with a particular interest in using fastai and blurr, I heartily recommend the study group hosted by Weights&Biases that I've been leading for the past few months.  You can watch the entire playlist [here](https://www.youtube.com/playlist?list=PLD80i8An1OEF8UOb9N9uSoidOGIMKW96t).\n\n\nAnd if you made it this far, thanks for reading all the way to the end :)\n\nYou can find me on tweeting at [@waydegilliam](https://twitter.com/waydegilliam) and blogging ML/Software development at [ohmeow.com](https://ohmeow.com/)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}