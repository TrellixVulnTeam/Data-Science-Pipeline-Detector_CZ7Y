{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Updates\n\n* Added cpc_codes context text to the input.\n* Changed optimizer back to Adam.\n* Used droupout.\n* Added Context tokens.\n* Used BCE loss function.","metadata":{}},{"cell_type":"markdown","source":"\n**Mostly adapted from my original work on keras.io:** [Semantic Similarity with BERT](https://keras.io/examples/nlp/semantic_similarity_with_bert/)\n\n**Inference notebook can be found here:** [US Phrase Matching: TF-Keras Inference](https://www.kaggle.com/code/mohamadmerchant/us-phrase-matching-tf-keras-inference)","metadata":{}},{"cell_type":"markdown","source":"**Main purpose of this notebook is to make faster and efficient experiments on large language models.**\n\n**Of course better results can be acieved by further tuning and training.**","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:22:38.452314Z","iopub.execute_input":"2022-03-31T16:22:38.452787Z","iopub.status.idle":"2022-03-31T16:22:38.46214Z","shell.execute_reply.started":"2022-03-31T16:22:38.452724Z","shell.execute_reply":"2022-03-31T16:22:38.460757Z"}}},{"cell_type":"code","source":"!pip install -U scikit-learn","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:42.110666Z","iopub.execute_input":"2022-06-12T08:26:42.11106Z","iopub.status.idle":"2022-06-12T08:26:57.153994Z","shell.execute_reply.started":"2022-06-12T08:26:42.110966Z","shell.execute_reply":"2022-06-12T08:26:57.15285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\n\nimport transformers\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:57.157702Z","iopub.execute_input":"2022-06-12T08:26:57.158081Z","iopub.status.idle":"2022-06-12T08:27:04.33307Z","shell.execute_reply.started":"2022-06-12T08:26:57.158036Z","shell.execute_reply":"2022-06-12T08:27:04.332048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Overview","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"../input/us-patent-phrase-to-phrase-matching/\"\n\ntrain = pd.read_csv(DATASET_PATH + \"train.csv\")\ntest = pd.read_csv(DATASET_PATH + \"test.csv\")\nsub = pd.read_csv(DATASET_PATH + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:04.334383Z","iopub.execute_input":"2022-06-12T08:27:04.334628Z","iopub.status.idle":"2022-06-12T08:27:04.450944Z","shell.execute_reply.started":"2022-06-12T08:27:04.3346Z","shell.execute_reply":"2022-06-12T08:27:04.449807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(\"../input/cpc-codes/titles.csv\")\ntrain = train.merge(cpc_codes, left_on='context', right_on='code', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:04.453011Z","iopub.execute_input":"2022-06-12T08:27:04.453624Z","iopub.status.idle":"2022-06-12T08:27:05.771519Z","shell.execute_reply.started":"2022-06-12T08:27:04.453587Z","shell.execute_reply":"2022-06-12T08:27:05.770463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the data\nprint(f\"Total train samples : {train.shape[0]}\")\nprint(f\"Total test samples: {sub.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:05.772828Z","iopub.execute_input":"2022-06-12T08:27:05.773081Z","iopub.status.idle":"2022-06-12T08:27:05.77938Z","shell.execute_reply.started":"2022-06-12T08:27:05.773054Z","shell.execute_reply":"2022-06-12T08:27:05.778224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train Score Distribution\")\ntrain['score'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:05.781054Z","iopub.execute_input":"2022-06-12T08:27:05.7813Z","iopub.status.idle":"2022-06-12T08:27:05.802724Z","shell.execute_reply.started":"2022-06-12T08:27:05.78127Z","shell.execute_reply":"2022-06-12T08:27:05.80184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup TPU","metadata":{}},{"cell_type":"code","source":"try:\n    # TPU config\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    auto = tf.data.experimental.AUTOTUNE\n    replicas = strategy.num_replicas_in_sync\n    print(f'TPU: {tpu.master()}')\nexcept:\n    strategy = tf.distribute.get_strategy()\n    auto = tf.data.experimental.AUTOTUNE\n    replicas = strategy.num_replicas_in_sync\n\n# XLA acceleartion\ntf.config.optimizer.set_jit(True)\nprint(f'Replicas: {replicas}')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:05.80463Z","iopub.execute_input":"2022-06-12T08:27:05.805499Z","iopub.status.idle":"2022-06-12T08:27:11.912632Z","shell.execute_reply.started":"2022-06-12T08:27:05.805458Z","shell.execute_reply":"2022-06-12T08:27:11.911534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config():\n    seed = 42\n    epochs = 4\n    num_folds = 4\n    max_length = 156\n    batch_size = 32\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    base_model = \"google/electra-large-discriminator\"\n    \n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n            \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:35:49.973699Z","iopub.execute_input":"2022-06-12T08:35:49.974083Z","iopub.status.idle":"2022-06-12T08:35:49.985298Z","shell.execute_reply.started":"2022-06-12T08:35:49.974044Z","shell.execute_reply":"2022-06-12T08:35:49.984192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Encoder","metadata":{}},{"cell_type":"code","source":"# Adding Context tokens to tokenizer.\n\ntrain['title'] = train['title'].str.lower()\ntrain['anchor'] = train['anchor'].str.lower()\ntrain['target'] = train['target'].str.lower()\n\n# Tokenizer.\ntokenizer = transformers.AutoTokenizer.from_pretrained(Config.base_model)\n\n# Context tokens. \ntrain['context_token'] = '[' + train.context + ']'\ntrain['sep_token'] = '[SEP]'\ncontext_tokens = list(train.context_token.unique())\ntokenizer.add_special_tokens({'additional_special_tokens': context_tokens})\n\n# Preparing input text for the model.\n# We are adding context_token before the context title\n# to let model learn the context of anchor and target.\ntrain['text'] = train['anchor'] + train['sep_token'] + train['target'] + \\\n                train['context_token'] + train['title']","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:35:58.608141Z","iopub.execute_input":"2022-06-12T08:35:58.608811Z","iopub.status.idle":"2022-06-12T08:35:59.838358Z","shell.execute_reply.started":"2022-06-12T08:35:58.608772Z","shell.execute_reply":"2022-06-12T08:35:59.837427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:27:16.003001Z","iopub.execute_input":"2022-06-12T08:27:16.003295Z","iopub.status.idle":"2022-06-12T08:27:16.031176Z","shell.execute_reply.started":"2022-06-12T08:27:16.003263Z","shell.execute_reply":"2022-06-12T08:27:16.030509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_text(text, \n                tokenizer,\n                max_length):\n    \n    # With tokenizer's batch_encode_plus batch of both the sentences are\n    # encoded together and separated by [SEP] token.\n    encoded = tokenizer.batch_encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=False,\n        return_offsets_mapping=False,\n        return_tensors=\"tf\",\n    )\n\n    # Convert batch of encoded features to numpy array.\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_masks\": attention_masks,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:36:07.921107Z","iopub.execute_input":"2022-06-12T08:36:07.921455Z","iopub.status.idle":"2022-06-12T08:36:07.92933Z","shell.execute_reply.started":"2022-06-12T08:36:07.921424Z","shell.execute_reply":"2022-06-12T08:36:07.928543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Competition Metrics","metadata":{}},{"cell_type":"code","source":"class Pearsonr(tf.keras.callbacks.Callback):\n    def __init__(self, val_data, y_val):\n        self.val_data = val_data\n        self.y_val = y_val\n    def on_epoch_end(self, epoch, logs):\n        val_preds = self.model.predict(self.val_data, verbose=0)\n        \n        val_pearsonr = stats.pearsonr(self.y_val, np.clip(val_preds.ravel(), 0, 1))[0]\n\n        print(f\"val_pearsonr: {val_pearsonr:.4f}\\n\")\n        logs[\"val_pearsonr\"] = val_pearsonr","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:36:30.32404Z","iopub.execute_input":"2022-06-12T08:36:30.324784Z","iopub.status.idle":"2022-06-12T08:36:30.331492Z","shell.execute_reply.started":"2022-06-12T08:36:30.324741Z","shell.execute_reply":"2022-06-12T08:36:30.330625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the model","metadata":{}},{"cell_type":"code","source":"def build_model(config, num_train_steps):\n    # Create the model under a distribution strategy scope.\n    with strategy.scope():\n        # Encoded token ids from BERT tokenizer.\n        input_ids = tf.keras.layers.Input(\n            shape=(config.max_length,), dtype=tf.int32, name=\"input_ids\"\n        )\n        # Attention masks indicates to the model which tokens should be attended to.\n        attention_masks = tf.keras.layers.Input(\n            shape=(config.max_length,), dtype=tf.int32, name=\"attention_masks\"\n        )\n\n        # Loading pretrained BERT model.\n        base_model = transformers.TFAutoModel.from_pretrained(config.base_model)\n\n        base_model_output = base_model(\n            input_ids, attention_mask=attention_masks\n        )\n        \n        last_hidden_state = base_model_output.last_hidden_state\n        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n#         cls_token = base_model_output.pooler_output\n        output = tf.keras.layers.Dense(1)(avg_pool)\n        \n        model = tf.keras.models.Model(\n            inputs=[input_ids, attention_masks], outputs=output\n        )\n\n        # Linear scheduler.\n        lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n            initial_learning_rate=config.learning_rate, \n            end_learning_rate=2e-6, \n            decay_steps=num_train_steps)\n        \n        model.compile(\n            optimizer = tf.keras.optimizers.Adam(lr_scheduler),\n            loss='mse'\n        )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:36:41.905066Z","iopub.execute_input":"2022-06-12T08:36:41.90542Z","iopub.status.idle":"2022-06-12T08:36:41.91596Z","shell.execute_reply.started":"2022-06-12T08:36:41.905389Z","shell.execute_reply":"2022-06-12T08:36:41.915074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Folds","metadata":{}},{"cell_type":"code","source":"def train_folds(train, config):\n    oof = np.zeros(len(train))\n    \n    train[\"score_bins\"] = pd.cut(train[\"score\"], bins=5, labels=False)\n    \n    skf = StratifiedGroupKFold(n_splits=config.num_folds, \n                      shuffle=True,\n                      random_state=config.seed)\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(train, \n                                                          train['score_bins'].values,\n                                                          train['anchor'].values)):\n        print(\"*\" * 25)\n        print(f\"Training fold: {fold+1}\")\n\n        train_df = train.loc[train_idx].reset_index(drop=True)\n        val_df = train.loc[val_idx].reset_index(drop=True)\n        \n        # Clear keras session.\n        K.clear_session()\n        \n        train_encoded =  encode_text(train_df[\"text\"].tolist(),\n                                     tokenizer=tokenizer,\n                                     max_length=config.max_length)\n        \n        val_encoded =  encode_text(val_df[\"text\"].tolist(),\n                                     tokenizer=tokenizer,\n                                     max_length=config.max_length)\n        # Dataloader.\n        train_data = tf.data.Dataset.from_tensor_slices((train_encoded, train_df['score'].tolist()))\n        val_data = tf.data.Dataset.from_tensor_slices((val_encoded, val_df['score'].tolist()))\n\n        train_data = (\n                        train_data\n                        .shuffle(1024)\n                        .batch(config.batch_size)\n                        .prefetch(tf.data.AUTOTUNE)\n                     )\n        \n        val_data = (\n                        val_data\n                        .batch(config.batch_size)\n                        .prefetch(tf.data.AUTOTUNE)\n                    )\n\n        # Callbacks.\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'model-{fold+1}.h5',\n                                                        monitor='val_loss',\n                                                        mode='min',\n                                                        save_best_only=True,\n                                                        save_weights_only=True,\n                                                        save_freq='epoch',\n                                                        verbose=1)\n        \n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                          mode='min',\n                                                          patience=3,\n                                                          verbose=1)\n        \n        pearsonr_callback = Pearsonr(val_data, val_df['score'].values)\n        num_train_steps = int(len(train_df) / config.batch_size * config.epochs)\n        \n        # Build and Train model.\n        model = build_model(config, num_train_steps)\n        history = model.fit(\n                        train_data,\n                        validation_data=val_data,\n                        epochs=config.epochs,\n                        callbacks=[checkpoint, \n                                   early_stopping, \n                                   pearsonr_callback],\n                        verbose=1\n                    )\n        \n        print('\\nLoading best model weights...')\n        model.load_weights(f'model-{fold+1}.h5')\n        \n        print('Predicting OOF...')\n        oof[val_idx] = model.predict(val_data,\n                                     batch_size=config.batch_size,\n                                     verbose=0).reshape(-1)\n        \n        \n        score = stats.pearsonr(val_df['score'].values, oof[val_idx])[0]\n        print(f'\\nFold {fold + 1}: OOF pearson_r: {score:.4f}')        \n        print(\"*\" * 25)\n        \n    score = stats.pearsonr(train['score'].values, oof)[0]\n    print(f'\\nOverall OOF pearson_r: {score:.4f}')\n    return oof","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:36:43.194935Z","iopub.execute_input":"2022-06-12T08:36:43.195304Z","iopub.status.idle":"2022-06-12T08:36:43.214854Z","shell.execute_reply.started":"2022-06-12T08:36:43.195268Z","shell.execute_reply":"2022-06-12T08:36:43.21414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = Config()\noof_preds = train_folds(train, config)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:36:44.054719Z","iopub.execute_input":"2022-06-12T08:36:44.055635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('oof.npy', oof_preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:35:31.721198Z","iopub.status.idle":"2022-06-12T08:35:31.721752Z","shell.execute_reply.started":"2022-06-12T08:35:31.721473Z","shell.execute_reply":"2022-06-12T08:35:31.721499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./five-folds/\n!cp -r ../us-patent-phrase-matching-models/* ./five-folds/*","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:35:31.723667Z","iopub.status.idle":"2022-06-12T08:35:31.724058Z","shell.execute_reply.started":"2022-06-12T08:35:31.723859Z","shell.execute_reply":"2022-06-12T08:35:31.723894Z"},"trusted":true},"execution_count":null,"outputs":[]}]}