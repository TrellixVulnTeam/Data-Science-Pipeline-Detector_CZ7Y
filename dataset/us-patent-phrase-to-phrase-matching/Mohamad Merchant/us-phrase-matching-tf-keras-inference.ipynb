{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Training notebook can be found here:** [US Phrase Matching: TF-Keras Train [TPU]](https://www.kaggle.com/mohamadmerchant/us-phrase-matching-tf-keras-train-tpu)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\n\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:21.812072Z","iopub.execute_input":"2022-04-19T13:09:21.812651Z","iopub.status.idle":"2022-04-19T13:09:30.958762Z","shell.execute_reply.started":"2022-04-19T13:09:21.812519Z","shell.execute_reply":"2022-04-19T13:09:30.957797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_PATH = \"../input/us-patent-phrase-to-phrase-matching/\"\n\ntest = pd.read_csv(DATASET_PATH + \"test.csv\")\ncpc_codes = pd.read_csv(\"../input/cpc-codes/titles.csv\")\n\ntest = test.merge(cpc_codes, left_on='context', right_on='code', how='left')\n\nsub = pd.read_csv(DATASET_PATH + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:30.961428Z","iopub.execute_input":"2022-04-19T13:09:30.96169Z","iopub.status.idle":"2022-04-19T13:09:31.89468Z","shell.execute_reply.started":"2022-04-19T13:09:30.961661Z","shell.execute_reply":"2022-04-19T13:09:31.893678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config():\n    seed = 42\n    num_folds = 5\n    max_length = 192\n    batch_size = 64\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    base_model = \"../input/bert-for-patents/bert-for-patents/\"\n    \n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n            \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:31.896299Z","iopub.execute_input":"2022-04-19T13:09:31.89665Z","iopub.status.idle":"2022-04-19T13:09:31.906539Z","shell.execute_reply.started":"2022-04-19T13:09:31.896583Z","shell.execute_reply":"2022-04-19T13:09:31.904527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding Context tokens to tokenizer.\n\ntest['title'] = test['title'].str.lower()\ntest['anchor'] = test['anchor'].str.lower()\ntest['target'] = test['target'].str.lower()\n\n# Tokenizer.\ntokenizer = transformers.AutoTokenizer.from_pretrained(Config.base_model)\n\n# Context tokens. \ntest['context_token'] = '[' + test.context + ']'\ntest['sep_token'] = '[SEP]'\ntest['cls_token'] = '[CLS]'\ncontext_tokens = list(test.context_token.unique())\ntokenizer.add_special_tokens({'additional_special_tokens': context_tokens})\n\n# Preparing input text for the model.\n# We are adding context_token before the context title\n# to let model learn the context of anchor and target.\ntest['text'] = test['cls_token'] + \\\n                    test['context_token'] + test['title'] + \\\n                    test['sep_token'] + test['anchor'] + \\\n                    test['sep_token'] + test['target'] + \\\n                test['sep_token']","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:31.908221Z","iopub.execute_input":"2022-04-19T13:09:31.908885Z","iopub.status.idle":"2022-04-19T13:09:32.460036Z","shell.execute_reply.started":"2022-04-19T13:09:31.908837Z","shell.execute_reply":"2022-04-19T13:09:32.459058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:32.463079Z","iopub.execute_input":"2022-04-19T13:09:32.463381Z","iopub.status.idle":"2022-04-19T13:09:32.494163Z","shell.execute_reply.started":"2022-04-19T13:09:32.463338Z","shell.execute_reply":"2022-04-19T13:09:32.493053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_text(text, \n                tokenizer,\n                max_length):\n    \n    # With tokenizer's batch_encode_plus batch of both the sentences are\n    # encoded together and separated by [SEP] token.\n    encoded = tokenizer.batch_encode_plus(\n        text,\n        add_special_tokens=False,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True,\n        return_tensors=\"tf\",\n    )\n    # Convert batch of encoded features to numpy array.\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_masks\": attention_masks,\n        \"token_type_ids\": token_type_ids\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:32.496008Z","iopub.execute_input":"2022-04-19T13:09:32.496368Z","iopub.status.idle":"2022-04-19T13:09:32.504936Z","shell.execute_reply.started":"2022-04-19T13:09:32.496326Z","shell.execute_reply":"2022-04-19T13:09:32.503275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the model","metadata":{}},{"cell_type":"code","source":"def build_model(config):\n    \n    # Create the model under a distribution strategy scope.\n    strategy = tf.distribute.MirroredStrategy()\n\n    # Create the model under a distribution strategy scope.\n    with strategy.scope():\n        # Encoded token ids from BERT tokenizer.\n        input_ids = tf.keras.layers.Input(\n            shape=(config.max_length,), dtype=tf.int32, name=\"input_ids\"\n        )\n        # Attention masks indicates to the model which tokens should be attended to.\n        attention_masks = tf.keras.layers.Input(\n            shape=(config.max_length,), dtype=tf.int32, name=\"attention_masks\"\n        )\n        # Token type ids are binary masks identifying different sequences in the model.\n        token_type_ids = tf.keras.layers.Input(\n            shape=(config.max_length,), dtype=tf.int32, name=\"token_type_ids\"\n        )\n        # Loading pretrained BERT model.\n        base_model = transformers.TFAutoModel.from_pretrained(config.base_model, from_pt=True)\n\n        base_model_output = base_model(\n            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n        )\n        \n        last_hidden_state = base_model_output.last_hidden_state\n        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n        dropout = tf.keras.layers.Dropout(0.3)(avg_pool)\n\n        output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n        \n        model = tf.keras.models.Model(\n            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n        )\n\n        model.compile(\n            optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n            loss=tf.keras.losses.BinaryCrossentropy()\n        )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:32.50732Z","iopub.execute_input":"2022-04-19T13:09:32.507691Z","iopub.status.idle":"2022-04-19T13:09:32.522852Z","shell.execute_reply.started":"2022-04-19T13:09:32.507632Z","shell.execute_reply":"2022-04-19T13:09:32.521648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict Folds","metadata":{}},{"cell_type":"code","source":"def predict_folds(test, config):\n    preds = []\n    \n    for fold in range(config.num_folds):\n        print(\"*\" * 25)\n        print(f\"Predicting fold: {fold+1}\")\n\n        # Clear keras session.\n        K.clear_session()\n        \n        test_encoded =  encode_text(test[\"text\"].tolist(),\n                                     tokenizer=tokenizer,\n                                     max_length=config.max_length)\n        # Dataloader.\n        test_data = tf.data.Dataset.from_tensor_slices((test_encoded))\n        \n        # Disable AutoShard.\n        options = tf.data.Options()\n        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n        test_data = test_data.with_options(options)\n\n        test_data = (\n                        test_data\n                        .batch(config.batch_size)\n                        .prefetch(tf.data.AUTOTUNE)\n                    )\n\n        # Build and Load the model.\n        model = build_model(config)\n        print('Loading best model weights...')\n        model.load_weights(f'../input/us-patent-phrase-matching-models/model-{fold+1}.h5')\n        \n        preds.append(\n                model.predict(test_data,\n                              batch_size=config.batch_size,\n                              verbose=1).reshape(-1)\n                    )\n        print(\"*\" * 25)\n\n    preds = np.mean(preds, axis=0)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:32.525153Z","iopub.execute_input":"2022-04-19T13:09:32.525565Z","iopub.status.idle":"2022-04-19T13:09:32.539658Z","shell.execute_reply.started":"2022-04-19T13:09:32.52552Z","shell.execute_reply":"2022-04-19T13:09:32.538493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = Config()\npreds = predict_folds(test, config)\nsub['score'] = preds\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:09:32.541876Z","iopub.execute_input":"2022-04-19T13:09:32.542347Z","iopub.status.idle":"2022-04-19T13:11:50.74198Z","shell.execute_reply.started":"2022-04-19T13:09:32.542271Z","shell.execute_reply":"2022-04-19T13:11:50.740827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:11:50.74406Z","iopub.execute_input":"2022-04-19T13:11:50.744429Z","iopub.status.idle":"2022-04-19T13:11:50.756072Z","shell.execute_reply.started":"2022-04-19T13:11:50.744355Z","shell.execute_reply":"2022-04-19T13:11:50.754998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}