{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook is to analyze what `Tez` do under the hood in the original notebook from @Abhishek Thakur \n\n(A little dispointed that I do not find any documentation for Tez library but thankfully the source code is clean and readable.)","metadata":{}},{"cell_type":"code","source":"# this cell is totally same as the original notebook, which is familiar Pytorch/Transformer routines\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport torch\n\nimport pandas as pd\nimport torch.nn as nn\n\nfrom scipy import stats\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n\nclass args:\n    model = \"../input/anferico-bert-for-patents/\"\n    max_len = 32\n    accumulation_steps = 1\n    batch_size = 64\n    epochs = 5\n    learning_rate = 2e-5\n    \nclass PhraseDataset:\n    def __init__(self, anchor, target, context, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item]\n        context = self.context[item]\n        target = self.target[item]\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        attention_mask = encoded_text[\"attention_mask\"]\n        token_type_ids = encoded_text[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n        }\n    \nclass PhraseModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model_name = model_name\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        transformer_out = self.transformer(ids, mask, token_type_ids)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        return output, 0, {}\n    \ndf = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\n\ncontext_mapping = {\n    \"A\": \"Human Necessities\",\n    \"B\": \"Operations and Transport\",\n    \"C\": \"Chemistry and Metallurgy\",\n    \"D\": \"Textiles\",\n    \"E\": \"Fixed Constructions\",\n    \"F\": \"Mechanical Engineering\",\n    \"G\": \"Physics\",\n    \"H\": \"Electricity\",\n    \"Y\": \"Emerging Cross-Sectional Technologies\",\n}\n\ndf.context = df.context.apply(lambda x: context_mapping[x[0]])\n\ntokenizer = AutoTokenizer.from_pretrained(args.model)\ntest_dataset = PhraseDataset(\n    anchor=df.anchor.values,\n    target=df.target.values,\n    context=df.context.values,\n    tokenizer=tokenizer,\n    max_len=args.max_len,\n)\n\nmodel = PhraseModel(model_name=args.model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-22T23:59:26.897179Z","iopub.execute_input":"2022-03-22T23:59:26.897942Z","iopub.status.idle":"2022-03-22T23:59:50.429686Z","shell.execute_reply.started":"2022-03-22T23:59:26.897841Z","shell.execute_reply":"2022-03-22T23:59:50.42903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing Tez-specific code","metadata":{}},{"cell_type":"code","source":"\n# `Tez` maintain two important attributes: Pytorch `self.model` and `self.config`\ntez_model = Tez(\n    model\n)\nprint(type(tez_model.model), tez_model.config)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:07:35.280085Z","iopub.execute_input":"2022-03-23T00:07:35.280538Z","iopub.status.idle":"2022-03-23T00:07:35.285753Z","shell.execute_reply.started":"2022-03-23T00:07:35.280502Z","shell.execute_reply":"2022-03-23T00:07:35.284703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the following lines do these things:\n# 1. set config: tez_model.config = ...\n# 2. set model device: model.to(torch.device(\"cuda:0\"))\n# 3. load model parameters: model.load_state_dict(torch.load(model_path, map_location='cuda'))\nmodel_path = \"../input/uspppm-tez-models/model_f0.bin\"\nconfig = TezConfig(test_batch_size=64, device=\"cuda\",)\ntez_model.load(model_path, weights_only=True, config=config)\nprint(tez_model.config)\n# One important thing I notice is that TezConfig contains many default argument settings, which we may want to take control\n","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:32:44.859862Z","iopub.execute_input":"2022-03-23T00:32:44.860175Z","iopub.status.idle":"2022-03-23T00:32:45.627273Z","shell.execute_reply.started":"2022-03-23T00:32:44.860141Z","shell.execute_reply":"2022-03-23T00:32:45.626383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally, it predicts (seem like to be a clean API)\n# it do the following things:\n# 1. make data loader: data_loader = DataLoader(dataset,  batch_size=batch_size, num_workers=-1, sampler=sampler, collate_fn=collate_fn, pin_memory=False,)\n# 2. simply call python torch model with input from data_loader\npreds_iter = tez_model.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:32:52.221007Z","iopub.execute_input":"2022-03-23T00:32:52.221728Z","iopub.status.idle":"2022-03-23T00:32:52.225908Z","shell.execute_reply.started":"2022-03-23T00:32:52.221688Z","shell.execute_reply":"2022-03-23T00:32:52.22495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Pure Pytorch","metadata":{}},{"cell_type":"code","source":"# here is pure pytorch code\nfrom torch.utils.data import DataLoader\nmodel.to(torch.device(\"cuda:0\"))\nmodel_path = \"../input/uspppm-tez-models/model_f0.bin\"\nmodel.load_state_dict(torch.load(model_path, map_location='cuda'))\ndef predict(model, dataset, batch_size):\n        data_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            num_workers=0,\n            sampler=None,\n            collate_fn=None,\n            pin_memory=False,\n        )\n\n        if model.training:\n            model.eval()\n\n        for data in data_loader:\n            with torch.no_grad():  \n                for key, value in data.items():\n                    data[key] = value.to(torch.device(\"cuda:0\"))\n                # here the PhraseModel.forward retures two extra parameters for Tez template, we can remove them\n                output, _, _ = model(**data)\n                output = output.cpu().detach().numpy()\n                yield output\n\ntest_batch_size=64\npreds_iter = predict(model, test_dataset, test_batch_size)\nfinal_preds = []\nfor preds in preds_iter:\n    preds[preds < 0] = 0\n    preds[preds > 1] = 1\n    final_preds.extend(preds.ravel().tolist())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:31:56.308825Z","iopub.execute_input":"2022-03-23T00:31:56.309104Z","iopub.status.idle":"2022-03-23T00:31:57.15952Z","shell.execute_reply.started":"2022-03-23T00:31:56.309067Z","shell.execute_reply":"2022-03-23T00:31:57.158825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsample_submission.score = final_preds\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:32:06.484531Z","iopub.execute_input":"2022-03-23T00:32:06.484785Z","iopub.status.idle":"2022-03-23T00:32:06.497242Z","shell.execute_reply.started":"2022-03-23T00:32:06.484756Z","shell.execute_reply":"2022-03-23T00:32:06.496398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head() # I have checked that the result is totally same as the original notebook","metadata":{"execution":{"iopub.status.busy":"2022-03-23T00:32:08.523692Z","iopub.execute_input":"2022-03-23T00:32:08.523939Z","iopub.status.idle":"2022-03-23T00:32:08.538657Z","shell.execute_reply.started":"2022-03-23T00:32:08.523909Z","shell.execute_reply":"2022-03-23T00:32:08.537997Z"},"trusted":true},"execution_count":null,"outputs":[]}]}