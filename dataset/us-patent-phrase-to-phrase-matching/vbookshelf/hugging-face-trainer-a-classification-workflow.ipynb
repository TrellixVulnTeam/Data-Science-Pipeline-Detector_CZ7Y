{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Hugging Face Trainer - A classification workflow\nby Marsh [ @vbookshelf ]<br>\n9 April 2022","metadata":{}},{"cell_type":"markdown","source":"## Overview","metadata":{}},{"cell_type":"markdown","source":"This task can be framed as a classification problem. We are given an anchor, target and a context. Based on the context we need to predict if the anchor and target are:\n- 1.0 - A very close match\n- 0.75 - Close synonyms\n- 0.5 - Synonyms which donâ€™t have the same meaning\n- 0.25 - Somewhat related\n- 0.0 - Unrelated\n\nThe scores (labels) in the training data are made up of 5 increments [0.0, 0.25, 0.5, 0.75, 1.0]. These increments can be mapped to five classes: [0, 1, 2, 3, 4]. In this notebook we will train a Bert for Patents classifier to predict these classes. After inference the predicted classes will be converted back to float increments.\n\nThis solution uses the Hugging Face trainer. One of the downsides of using this trainer on Kaggle is that it uses a lot of disk space. This causes notebooks to crash during training. I've included some notes that explain how to set TrainingArguments to reduce the amount of disk space that gets used.\n\nThis notebook includes both training and inference. We will train five folds for one epoch each. Then we will take a simple average of the fold predictions.\n\nThe top scoring public notebooks on this competition use a regression approach. I've also inlcuded a quick side note that explains how to set up the Hugging Face trainer for regression. The regression setup uses MSE loss.\n\nAt the end of this notebook there are links to helpful resources that explain concepts like fp16, gradient accumulation and weight decay.\n\nLet's get started.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n\n#from tqdm import tqdm\n# tqdm doesn't work well in colab.\n# This is the solution:\n# https://stackoverflow.com/questions/41707229/tqdm-printing-to-newline\nimport tqdm.notebook as tq\n#for i in tq.tqdm(...):\n\n\nimport string\n\nfrom sklearn import model_selection\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\n\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nprint(torch.__version__)\n#print(torchvision.__version__)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"uE5yNcuvAdBv","outputId":"c1e0bcb1-08a3-4113-b73a-5a30187aad6e","papermill":{"duration":10.003368,"end_time":"2021-01-02T12:49:30.839118","exception":false,"start_time":"2021-01-02T12:49:20.83575","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T07:28:20.472782Z","iopub.execute_input":"2022-04-09T07:28:20.473277Z","iopub.status.idle":"2022-04-09T07:28:26.979734Z","shell.execute_reply.started":"2022-04-09T07:28:20.473189Z","shell.execute_reply":"2022-04-09T07:28:26.979022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the seed values\n\nimport random\n\nseed = 1024\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:26.981879Z","iopub.execute_input":"2022-04-09T07:28:26.982119Z","iopub.status.idle":"2022-04-09T07:28:26.989255Z","shell.execute_reply.started":"2022-04-09T07:28:26.982085Z","shell.execute_reply":"2022-04-09T07:28:26.98846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:26.990737Z","iopub.execute_input":"2022-04-09T07:28:26.991148Z","iopub.status.idle":"2022-04-09T07:28:27.001126Z","shell.execute_reply.started":"2022-04-09T07:28:26.991111Z","shell.execute_reply":"2022-04-09T07:28:27.000309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '../input/us-patent-phrase-to-phrase-matching/'\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.003818Z","iopub.execute_input":"2022-04-09T07:28:27.004262Z","iopub.status.idle":"2022-04-09T07:28:27.007862Z","shell.execute_reply.started":"2022-04-09T07:28:27.004228Z","shell.execute_reply":"2022-04-09T07:28:27.007017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{"id":"KKd3Rz1yMxrK","papermill":{"duration":0.048849,"end_time":"2021-01-02T12:49:31.284417","exception":false,"start_time":"2021-01-02T12:49:31.235568","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The model is stored in a Kaggle dataset.\n# The internet connection in this notebook is off.\nMODEL_PATH = '../input/bert-for-patents/bert-for-patents/'\n\n# Set the max token length.\n# Determine this by looking at max token lengths \n# in the train set. Process is shown below.\nMAX_LEN = 64\n\nNUM_EPOCHS = 1\n\nNUM_FOLDS = 5\n\n# Specify which folds should be used in training.\n# This is helpful when you have to train the folds in \n# separate notebooks.\nSTART_FOLD = 0\nSTOP_FOLD = 5 # this number is not included\n\nNUM_CLASSES = 5 # [0, 1, 2, 3, 4]\n\nL_RATE = 2e-5\n\n# 1. Setting fp16=True (TrainingArguments) allows us to use larger batch sizes. This speeds up training.\n# 2. Also, because the gradient accmulation parameter is\n# set to 2 (TrainingArguments), the equivalent batch size is actually 40 i.e. 2*20 = 40.\n# Gradient accumulation is helpful when you have to use very small batch sizes.\nBATCH_SIZE = 20\n\nNUM_CORES = os.cpu_count()\n\n# When training with multiple GPUs, if the number\n# of workers (CPU cores) is set too high that can slow down training.\n# Not applicable on Kaggle because there's only one GPU.\nif torch.cuda.device_count() > 1:\n    NUM_CORES = 4\n\nNUM_CORES","metadata":{"id":"AWi9hw2jAdBx","outputId":"d12f0b58-90b9-4fa5-f2e8-90f3b49329dd","papermill":{"duration":0.061232,"end_time":"2021-01-02T12:49:31.397213","exception":false,"start_time":"2021-01-02T12:49:31.335981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T07:28:27.009303Z","iopub.execute_input":"2022-04-09T07:28:27.009596Z","iopub.status.idle":"2022-04-09T07:28:27.075105Z","shell.execute_reply.started":"2022-04-09T07:28:27.009563Z","shell.execute_reply":"2022-04-09T07:28:27.074327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the device","metadata":{"id":"UB1k1W7gAdBx","papermill":{"duration":0.05076,"end_time":"2021-01-02T12:49:31.497388","exception":false,"start_time":"2021-01-02T12:49:31.446628","status":"completed"},"tags":[]}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint(device)\n\nif torch.cuda.is_available():\n    print('Num GPUs:', torch.cuda.device_count())\n    print('GPU Type:', torch.cuda.get_device_name(0))","metadata":{"id":"KbDbhlA-AdBx","outputId":"ba0de962-051f-4cc9-caf1-65d864d9f713","papermill":{"duration":0.062869,"end_time":"2021-01-02T12:49:31.609881","exception":false,"start_time":"2021-01-02T12:49:31.547012","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T07:28:27.076541Z","iopub.execute_input":"2022-04-09T07:28:27.076776Z","iopub.status.idle":"2022-04-09T07:28:27.092738Z","shell.execute_reply.started":"2022-04-09T07:28:27.076745Z","shell.execute_reply":"2022-04-09T07:28:27.09199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{"id":"OjGqcOkMAdBy","papermill":{"duration":0.050978,"end_time":"2021-01-02T12:49:31.710227","exception":false,"start_time":"2021-01-02T12:49:31.659249","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train data\n\npath = base_path + 'train.csv'\ndf_data = pd.read_csv(path)\n\nprint(df_data.shape)\n\ndf_data.head()","metadata":{"id":"8-KO4R8wAdBy","papermill":{"duration":15.961495,"end_time":"2021-01-02T12:49:47.721971","exception":false,"start_time":"2021-01-02T12:49:31.760476","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T07:28:27.094108Z","iopub.execute_input":"2022-04-09T07:28:27.094416Z","iopub.status.idle":"2022-04-09T07:28:27.193249Z","shell.execute_reply.started":"2022-04-09T07:28:27.094381Z","shell.execute_reply":"2022-04-09T07:28:27.192504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of the train labels\n\ndf_data['score'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.194273Z","iopub.execute_input":"2022-04-09T07:28:27.194527Z","iopub.status.idle":"2022-04-09T07:28:27.208781Z","shell.execute_reply.started":"2022-04-09T07:28:27.194494Z","shell.execute_reply":"2022-04-09T07:28:27.208114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data\n\npath = base_path + 'test.csv'\ndf_test = pd.read_csv(path)\n\nprint(df_test.shape)\n\ndf_test.head()","metadata":{"id":"OEjOw2k7AdBy","papermill":{"duration":4.766619,"end_time":"2021-01-02T12:49:52.542317","exception":false,"start_time":"2021-01-02T12:49:47.775698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T07:28:27.210084Z","iopub.execute_input":"2022-04-09T07:28:27.210486Z","iopub.status.idle":"2022-04-09T07:28:27.22745Z","shell.execute_reply.started":"2022-04-09T07:28:27.210453Z","shell.execute_reply":"2022-04-09T07:28:27.226794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add the context meanings\n\nHere we will add the context meanings to the train and test data. We will create a new column call 'title'.","metadata":{}},{"cell_type":"code","source":"# Ref: https://en.wikipedia.org/wiki/Cooperative_Patent_Classification\n\n# The letters (keys) in this dictionary are the\n# first letters of the context. Refer to the column called 'context'.\n\ncontext_mapping_dict = {\n                        \"A\": \"Human Necessities\",\n                        \"B\": \"Operations and Transport\",\n                        \"C\": \"Chemistry and Metallurgy\",\n                        \"D\": \"Textiles\",\n                        \"E\": \"Fixed Constructions\",\n                        \"F\": \"Mechanical Engineering\",\n                        \"G\": \"Physics\",\n                        \"H\": \"Electricity\",\n                        \"Y\": \"Emerging Cross-Sectional Technologies\",\n                        }","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.230369Z","iopub.execute_input":"2022-04-09T07:28:27.230597Z","iopub.status.idle":"2022-04-09T07:28:27.235144Z","shell.execute_reply.started":"2022-04-09T07:28:27.230569Z","shell.execute_reply":"2022-04-09T07:28:27.234213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_context(x):\n    \n    # get the first letter\n    letter = x[0]\n    \n    # extract the meaning from the dictionary\n    meaning = context_mapping_dict[letter]\n    \n    return meaning\n\n\n# Create a new column\ndf_data['title'] = df_data['context'].apply(map_context)\n\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.236498Z","iopub.execute_input":"2022-04-09T07:28:27.236805Z","iopub.status.idle":"2022-04-09T07:28:27.266256Z","shell.execute_reply.started":"2022-04-09T07:28:27.236771Z","shell.execute_reply":"2022-04-09T07:28:27.265599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data.\n# Create a new column.\ndf_test['title'] = df_test['context'].apply(map_context)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.267526Z","iopub.execute_input":"2022-04-09T07:28:27.267771Z","iopub.status.idle":"2022-04-09T07:28:27.28085Z","shell.execute_reply.started":"2022-04-09T07:28:27.26774Z","shell.execute_reply":"2022-04-09T07:28:27.279946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the label column","metadata":{"id":"aW29rDAAAdBy","papermill":{"duration":0.056423,"end_time":"2021-01-02T12:49:58.221643","exception":false,"start_time":"2021-01-02T12:49:58.16522","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_label(x):\n    \n    if x == 0:\n        return 0\n\n    if x == 0.25:\n        return 1\n    \n    if x == 0.5:\n        return 2\n\n    if x == 0.75:\n        return 3\n\n    if x == 1.0:\n        return 4\n\n# Note: This column must be called 'labels'. The Hugging Face trainer\n# automatically detects the column that contains the training labels.\ndf_data['labels'] = df_data['score'].apply(create_label)\n\n# Create a dummy label column so that the dataloader works on the test set.\ndf_test['labels'] = 0\n\nprint(df_data.shape)\n\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.282553Z","iopub.execute_input":"2022-04-09T07:28:27.28314Z","iopub.status.idle":"2022-04-09T07:28:27.324654Z","shell.execute_reply.started":"2022-04-09T07:28:27.283106Z","shell.execute_reply":"2022-04-09T07:28:27.324027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['labels'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.325703Z","iopub.execute_input":"2022-04-09T07:28:27.325946Z","iopub.status.idle":"2022-04-09T07:28:27.335257Z","shell.execute_reply.started":"2022-04-09T07:28:27.325913Z","shell.execute_reply":"2022-04-09T07:28:27.334234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine the anchor and target","metadata":{}},{"cell_type":"code","source":"df_data['combined_sentence'] = df_data['anchor'] + ' vs ' + df_data['target']\n\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.336471Z","iopub.execute_input":"2022-04-09T07:28:27.337001Z","iopub.status.idle":"2022-04-09T07:28:27.360667Z","shell.execute_reply.started":"2022-04-09T07:28:27.336966Z","shell.execute_reply":"2022-04-09T07:28:27.359995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['combined_sentence'] = df_test['anchor'] + ' vs ' + df_test['target']\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.361818Z","iopub.execute_input":"2022-04-09T07:28:27.362431Z","iopub.status.idle":"2022-04-09T07:28:27.376693Z","shell.execute_reply.started":"2022-04-09T07:28:27.362391Z","shell.execute_reply":"2022-04-09T07:28:27.375969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.378098Z","iopub.execute_input":"2022-04-09T07:28:27.3788Z","iopub.status.idle":"2022-04-09T07:28:27.392703Z","shell.execute_reply.started":"2022-04-09T07:28:27.378765Z","shell.execute_reply":"2022-04-09T07:28:27.391938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the token lengths\n\nHere we want to see what the max token length is in the train set. This will help us to set the MAX_LEN parameter. Setting a shorter MAX_LEN will use less RAM and help the model train faster.","metadata":{}},{"cell_type":"code","source":"# Instantiate the tokenizer\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.394166Z","iopub.execute_input":"2022-04-09T07:28:27.394534Z","iopub.status.idle":"2022-04-09T07:28:27.83532Z","shell.execute_reply.started":"2022-04-09T07:28:27.394464Z","shell.execute_reply":"2022-04-09T07:28:27.834567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example \n\n# The parameters for tokenizer.encode can be found here:\n# https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode\n\ntext = \"Hello. How are you?\"\n\n# remove any spaces\ntext = \" \".join(text.split())\n\nencoded_text = tokenizer.encode(text)\n\nprint(text)\nprint(encoded_text)\nprint(len(encoded_text))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.836998Z","iopub.execute_input":"2022-04-09T07:28:27.837496Z","iopub.status.idle":"2022-04-09T07:28:27.851763Z","shell.execute_reply.started":"2022-04-09T07:28:27.837458Z","shell.execute_reply":"2022-04-09T07:28:27.851121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_num_tokens(x):\n    \n    # convert to type string\n    x = str(x)\n    # remove any spaces\n    x = \" \".join(x.split())\n    \n    # get a list of tokens\n    token_list = tokenizer.encode(x)\n    \n    # get the number of tokens\n    num_tokens = len(token_list)\n    \n    return num_tokens\n\n# Create new columns containing the token lengths\ndf_data['num_tokens_combined_text'] = df_data['combined_sentence'].apply(get_num_tokens)\ndf_data['num_tokens_title'] = df_data['title'].apply(get_num_tokens)\n\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:27.852742Z","iopub.execute_input":"2022-04-09T07:28:27.852972Z","iopub.status.idle":"2022-04-09T07:28:32.45176Z","shell.execute_reply.started":"2022-04-09T07:28:27.85294Z","shell.execute_reply":"2022-04-09T07:28:32.451075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the max token lengths\nprint(df_data['num_tokens_combined_text'].max())\nprint(df_data['num_tokens_title'].max())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.453152Z","iopub.execute_input":"2022-04-09T07:28:32.453639Z","iopub.status.idle":"2022-04-09T07:28:32.459578Z","shell.execute_reply.started":"2022-04-09T07:28:32.453603Z","shell.execute_reply":"2022-04-09T07:28:32.458834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on these lengths I've set MAX_LEN = 64\n# This is set in the CONFIG above.\n# When choosing the MAX_LEN we need to consider the possibility that \n# the private test set could have text with a token length greater than 64.","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.461113Z","iopub.execute_input":"2022-04-09T07:28:32.461429Z","iopub.status.idle":"2022-04-09T07:28:32.467058Z","shell.execute_reply.started":"2022-04-09T07:28:32.461372Z","shell.execute_reply":"2022-04-09T07:28:32.466251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the 5 folds","metadata":{}},{"cell_type":"code","source":"# Filter out only the columns we need.\n\ncols = ['title', 'labels', 'combined_sentence', 'anchor']\n\ndf_data = df_data[cols]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.468425Z","iopub.execute_input":"2022-04-09T07:28:32.468672Z","iopub.status.idle":"2022-04-09T07:28:32.485225Z","shell.execute_reply.started":"2022-04-09T07:28:32.46864Z","shell.execute_reply":"2022-04-09T07:28:32.4846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the anchor column for stratification.\n# Ref: https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/315220\n\nskf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n\nfor fold, (t_, val_) in enumerate(skf.split(X=df_data, y=df_data['anchor'])):\n      df_data.loc[val_ , \"fold\"] = fold\n        \ndf_data['fold'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.486507Z","iopub.execute_input":"2022-04-09T07:28:32.486946Z","iopub.status.idle":"2022-04-09T07:28:32.557533Z","shell.execute_reply.started":"2022-04-09T07:28:32.486913Z","shell.execute_reply":"2022-04-09T07:28:32.556756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.558905Z","iopub.execute_input":"2022-04-09T07:28:32.559141Z","iopub.status.idle":"2022-04-09T07:28:32.570015Z","shell.execute_reply.started":"2022-04-09T07:28:32.55911Z","shell.execute_reply":"2022-04-09T07:28:32.569337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore how the Hugging Face Dataset works\n\nWe will need to convert the Pandas dataframe to a Hugging Face dataset before the data can be fed into the Trainer.","metadata":{}},{"cell_type":"code","source":"# Example\n\n# The column containing the labels you want to predict should be named: labels\n\n# This is the dataset docs:\n# https://huggingface.co/docs/datasets/v1.2.0/exploring.html\n\nfrom datasets import Dataset\nimport pandas as pd\n\n# Create a sample dataframe\ndf = pd.DataFrame({\"col-a\": [1, 2, 3],\n                  \"col-b\": [4, 5, 6]})\n\n# Convert the dataframe to a HuggingFace dataset.\n# Imagine that it's the same as a pandas dataframe with labeled columns and rows.\ndataset = Dataset.from_pandas(df)\n\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.57131Z","iopub.execute_input":"2022-04-09T07:28:32.571775Z","iopub.status.idle":"2022-04-09T07:28:32.877825Z","shell.execute_reply.started":"2022-04-09T07:28:32.571741Z","shell.execute_reply":"2022-04-09T07:28:32.877174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the items in the first row\n\ndataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.878919Z","iopub.execute_input":"2022-04-09T07:28:32.879148Z","iopub.status.idle":"2022-04-09T07:28:32.886252Z","shell.execute_reply.started":"2022-04-09T07:28:32.879115Z","shell.execute_reply":"2022-04-09T07:28:32.885532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.shape)\nprint(dataset.num_columns)\nprint(dataset.num_rows)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.891502Z","iopub.execute_input":"2022-04-09T07:28:32.891801Z","iopub.status.idle":"2022-04-09T07:28:32.899996Z","shell.execute_reply.started":"2022-04-09T07:28:32.891774Z","shell.execute_reply":"2022-04-09T07:28:32.899343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up the tokenize function and the metric function","metadata":{}},{"cell_type":"code","source":"def tokenize_data_fn(hf_dataset):\n    \n    \"\"\"\n    This function will tokenize all text in a specified column.\n    We use it in the same way that we use 'apply' in Pandas.\n    \n    \"\"\"\n    \n    tokenized_examples = tokenizer(\n                            hf_dataset['combined_sentence'], # sentence1\n                            hf_dataset['title'], # sentence2 - context\n                            truncation=\"only_second\", # only truncate sentence2\n                            max_length=MAX_LEN,\n                            padding=\"max_length\",\n                            )\n    \n    return tokenized_examples\n\n\n\n\ndef compute_metrics(eval_pred):\n    \n    # Declare as global so we can calculate the cv score for all folds and \n    # then print it when training is complete.\n    global corr\n    \n    \"\"\"    \n    This function is used to calculate the metric during training.\n    We will save the best model based on this metric.\n    \n    \"\"\"\n    \n    from scipy.stats import pearsonr\n    \n    score_list = []\n    \n    logits, labels = eval_pred\n    \n    # logits shape: (num_rows, num_cols)\n    # labels shape: (num_rows,)\n    \n    # take the argmax\n    preds = np.argmax(logits, axis=1)\n    \n    # Calculate the correlation.\n    # preds and labels should have the same length.\n    # corr is a scalar.\n    corr, _ = pearsonr(preds, labels)\n    \n    print(f'Pearson: {corr}')\n    \n    return {\n            'pearson': corr\n            }\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.901337Z","iopub.execute_input":"2022-04-09T07:28:32.902436Z","iopub.status.idle":"2022-04-09T07:28:32.917235Z","shell.execute_reply.started":"2022-04-09T07:28:32.902391Z","shell.execute_reply":"2022-04-09T07:28:32.915706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to reduce the disk space that gets used during training","metadata":{}},{"cell_type":"markdown","source":"By default the Trainer saves logs and every checkpoint during training. On Kaggle this quickly uses up the available disk space. \n\nThe files are saved in a folder called \"runs\" and in another folder that we name. In this notebook that folder is named \"comp_folder\". We set the name as a training argument.\n\nThe info below is based on this thread:<br>\nhttps://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442\n\nThis is a full list of training arguments:<br>\n(Click the link then scroll down until you get to TrainingArguments)<br>\nhttps://huggingface.co/transformers/main_classes/trainer.html\n\n\n1. To make the trainer overwrite old log files we can set: overwrite_output_dir=True. \n\n2. Another way to reduce the amount of disk space is to set save_strategy=\"no\" and load_best_model_at_end=False. In this case nothing will be saved during training. When training finishes you will need to save the model by using trainer.save_model(\"model_name\"). This will save the last model, not the best model. \n\n3. Another option is to set save_total_limit=2. This is the option that I'm using in the training loop below. In this case only two models will be saved at any given time - the most recent model and the best model (based on the metric that's being monitoted.). Note that even if save_total_limit=1 the Trainer will still save two models, the best one and the last one. \n\n4. Also, you will note that I delete the \"runs\" folder and the \"comp_folder\" at the end of the training loop for each fold. This also reduces disk space and ensures that these folders don't appear in the output data when the notebook is committed.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# This is where the scores for each fold\n# will be stored.\nscore_list = []\n\nfor i in range(START_FOLD, STOP_FOLD):\n\n    # Choose the fold\n    df_train = df_data[df_data['fold'] != i]\n    df_val = df_data[df_data['fold'] == i]\n    \n    ####################################################\n    # FOR TESTING ONLY\n    # Comment out these two lines during training.\n    #df_train = df_train[0:1000]\n    #df_val = df_val[0:1000]\n    ####################################################\n\n    # Reset the indices\n    df_train = df_train.reset_index(drop=True)\n    df_val = df_val.reset_index(drop=True)\n    #df_test = df_test.reset_index(drop=True)\n\n    # Register the data\n    train_dataset = Dataset.from_pandas(df_train)\n    val_dataset = Dataset.from_pandas(df_val)\n    #test_dataset = Dataset.from_pandas(df_test)\n\n    # Tokenize the data\n    from transformers import AutoTokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    pad_on_right = tokenizer.padding_side == \"right\"\n\n    # Don't remove the 'labels' column.\n    # The trainer automatically detects this column and uses the labels to\n    # calculate the loss during training. If the labels column can't be detected then\n    # you will get a Keyerror: loss.\n    cols = ['title', 'combined_sentence', 'anchor', 'fold']\n    \n    # Here \"map\" is similar to \"apply\" in Pandas\n\n    tokenized_train = train_dataset.map(tokenize_data_fn, batched=True, \n                                    remove_columns=cols\n                                       )\n\n    tokenized_val = val_dataset.map(tokenize_data_fn, batched=True, \n                                        remove_columns=cols\n                                   )\n\n\n\n    # Initialize the model\n\n    from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, \n                                                               num_labels=NUM_CLASSES)\n\n\n    # Intialize the data collator\n    # I don't know the details of what this actually does.\n\n    from transformers import default_data_collator\n\n    data_collator = default_data_collator\n\n\n    # Initilaize the trainer\n    # Full list of training arguments:\n    # (Click the link then scroll down until you get to TrainingArguments)\n    # https://huggingface.co/transformers/main_classes/trainer.html\n\n    args = TrainingArguments(\n        f\"comp_folder\",\n        overwrite_output_dir=True, # This reduces the amt of disk space that gets used.\n        fp16=True,  # fp16 training to allow larger batch sizes to be used\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        learning_rate=L_RATE,\n        warmup_ratio=0.1,\n        gradient_accumulation_steps=2, #8\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=NUM_EPOCHS,\n        weight_decay=0.01,\n        dataloader_num_workers=NUM_CORES,\n        \n        # Two options to address the exceeding disk space problem:\n        # Ref: https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442/7\n        \n        # Option 1:\n        # The disk still fills up but not as fast.\n        # Two models are always saved - best model and last model.\n        # Changing the 2 to 1 doesn't make a difference because 2 models are always saved.\n        # The saved fold models as well as the two models that are saved during training\n        # also uses up the disk space.\n        # You may need to train 3 folds in one notebook and 2 folds in another.\n        save_total_limit = 2, \n        load_best_model_at_end=True, # load the best model and then save it manually.\n        \n        # Option 2\n        # Don't save anything during training. \n        # Manually save the model later.\n        # Train for a specified number of epochs.\n        # Can't use a metric to get the best model.\n        # Remember that the saved fold models also uses up the disk space.\n        # May need to train 3 folds in one notebook and 2 folds in another.\n        #save_strategy = \"no\", \n        \n        metric_for_best_model=\"pearson\" # choose the best model based on this metric\n        )\n    \n    \n\n    trainer = Trainer(\n            model,\n            args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics\n        )\n    \n    \n    #-------------------------------------------------\n    \n    # Test that the eval metric is being calculated.\n    # Comment this step out when test is complete.\n    \n    #eval_results_dict = trainer.evaluate()\n    #print('\\n Check that eval is working:')\n    #print(eval_results_dict)\n    print('\\n')\n    \n    #-------------------------------------------------\n\n    \n    # Train the model\n    print(f\"fold_{i}\")\n    print('\\nTraining...')\n    trainer.train()\n    \n    # Save the score for the metric we are monitoring\n    score_list.append(corr)\n\n    # Save the model.\n    # The best model gets loaded when training is completed.\n    trainer.save_model(f\"model_{i}\")\n    \n    \n    \n    # Delete folders to save disk space\n    \n    import shutil\n\n    if os.path.isdir('runs') == True:\n        shutil.rmtree('runs')\n\n    if os.path.isdir('comp_folder') == True:\n        shutil.rmtree('comp_folder')\n        \n\n# Print the CV score\n\nprint('\\n')\nprint('===================')\ncv = sum(score_list)/len(score_list)\nprint(f'cv: {cv}')\nprint('===================')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-09T07:29:28.858109Z","iopub.execute_input":"2022-04-09T07:29:28.858405Z","iopub.status.idle":"2022-04-09T07:33:00.281614Z","shell.execute_reply.started":"2022-04-09T07:29:28.858361Z","shell.execute_reply":"2022-04-09T07:33:00.280632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the CV score\n\nprint('===================')\ncv = sum(score_list)/len(score_list)\nprint(f'cv: {cv}')\nprint('===================')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.9552Z","iopub.status.idle":"2022-04-09T07:28:32.958131Z","shell.execute_reply.started":"2022-04-09T07:28:32.957878Z","shell.execute_reply":"2022-04-09T07:28:32.957917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# Create a list of fold model paths\n\nmodel_0 = 'model_0'\nmodel_1 = 'model_1'\nmodel_2 = 'model_2'\nmodel_3 = 'model_3'\nmodel_4 = 'model_4'\n\nMODEL_LIST = [model_0, model_1, model_2, model_3, model_4]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.961745Z","iopub.status.idle":"2022-04-09T07:28:32.964301Z","shell.execute_reply.started":"2022-04-09T07:28:32.964005Z","shell.execute_reply":"2022-04-09T07:28:32.964033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions using all fold models\n\nraw_predictions_list = []\n\n# Make a prediction using each fold model\nfor i, model_path in enumerate(MODEL_LIST):\n    \n    # Create the test dataset\n    test_dataset = Dataset.from_pandas(df_test)\n\n    test_features = test_dataset.map(\n                    tokenize_data_fn,\n                    batched=True,\n                    remove_columns=test_dataset.column_names\n                    )\n\n    # Make a prediction for one model\n    raw_predictions = trainer.predict(test_features)\n\n    # Save the predictions from each fold in a list\n    raw_predictions_list.append(raw_predictions)\n\n\nprint(len(raw_predictions_list))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.967192Z","iopub.status.idle":"2022-04-09T07:28:32.968679Z","shell.execute_reply.started":"2022-04-09T07:28:32.968121Z","shell.execute_reply":"2022-04-09T07:28:32.968147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average the predictions for all folds\n\nfor i, raw_preds in enumerate(raw_predictions_list):\n    \n    np_preds_logits = raw_preds.predictions\n\n    if i == 0:     \n        fin_logits = np_preds_logits\n        \n    else:\n        fin_logits = fin_logits + np_preds_logits\n\n        \n# Average the predictions\navg_logits = fin_logits/len(MODEL_LIST)\n\navg_logits.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.971933Z","iopub.status.idle":"2022-04-09T07:28:32.974348Z","shell.execute_reply.started":"2022-04-09T07:28:32.974083Z","shell.execute_reply":"2022-04-09T07:28:32.97411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take the argmax\n\npreds = np.argmax(avg_logits, axis=1)\n\npreds.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.975533Z","iopub.status.idle":"2022-04-09T07:28:32.976226Z","shell.execute_reply.started":"2022-04-09T07:28:32.975979Z","shell.execute_reply":"2022-04-09T07:28:32.976005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the preds to df_test\n\ndf_test['preds'] = preds","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.97758Z","iopub.status.idle":"2022-04-09T07:28:32.978476Z","shell.execute_reply.started":"2022-04-09T07:28:32.978212Z","shell.execute_reply":"2022-04-09T07:28:32.978236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the preds to the corresponding float values\n\ndef change_preds(x):\n    \n    if x == 0:\n        return 0\n\n    if x == 1:\n        return 0.25\n    \n    if x == 2:\n        return 0.5\n\n    if x == 3:\n        return 0.75\n\n    if x == 4:\n        return 1.0\n    \ndf_test['modified_preds'] = df_test['preds'].apply(change_preds)\n\n# filter out the columns we don't need\ncols = ['id', 'modified_preds']\ndf = df_test[cols]\n\nprint(df_test.shape)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.979609Z","iopub.status.idle":"2022-04-09T07:28:32.980336Z","shell.execute_reply.started":"2022-04-09T07:28:32.980067Z","shell.execute_reply":"2022-04-09T07:28:32.980093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a submission csv file\n\nHere we will ensure that the submission csv has the same order as the sample submission. We do this by performing a merge. I'm doing this to ensure that we don't get any submission errors.","metadata":{}},{"cell_type":"code","source":"# Load the sample submission\n\npath = base_path + 'sample_submission.csv'\ndf_sample = pd.read_csv(path)\n\nprint(df_sample.shape)\n\ndf_sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.98154Z","iopub.status.idle":"2022-04-09T07:28:32.982315Z","shell.execute_reply.started":"2022-04-09T07:28:32.982023Z","shell.execute_reply":"2022-04-09T07:28:32.982052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the preds to df_sample\n# The order is changed to match df_sample\n\ndf_sample = pd.merge(df_sample, df, on='id', how='left')\n\nprint(df_sample.shape)\n\ndf_sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.983795Z","iopub.status.idle":"2022-04-09T07:28:32.984651Z","shell.execute_reply.started":"2022-04-09T07:28:32.984342Z","shell.execute_reply":"2022-04-09T07:28:32.984407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overwrite the score column\ndf_sample['score'] = list(df_sample['modified_preds'])\n\n# drop the modified_preds column\ndf_sample = df_sample.drop('modified_preds', axis=1)\n\ndf_sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.985825Z","iopub.status.idle":"2022-04-09T07:28:32.986584Z","shell.execute_reply.started":"2022-04-09T07:28:32.986255Z","shell.execute_reply":"2022-04-09T07:28:32.986281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission csv file\n\npath = 'submission.csv'\ndf_sample.to_csv(path, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.987685Z","iopub.status.idle":"2022-04-09T07:28:32.988585Z","shell.execute_reply.started":"2022-04-09T07:28:32.988347Z","shell.execute_reply":"2022-04-09T07:28:32.98838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a requirements.txt file\n# This is a list of all packages and their versions that were \n# used to create this solution.\n\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.989905Z","iopub.status.idle":"2022-04-09T07:28:32.990574Z","shell.execute_reply.started":"2022-04-09T07:28:32.990335Z","shell.execute_reply":"2022-04-09T07:28:32.990367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.991661Z","iopub.status.idle":"2022-04-09T07:28:32.992353Z","shell.execute_reply.started":"2022-04-09T07:28:32.992072Z","shell.execute_reply":"2022-04-09T07:28:32.992096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to use the Hugging Face trainer for regression","metadata":{}},{"cell_type":"markdown","source":"For a regression problem we still use AutoModelForSequenceClassification but we set num_labels=1.\n\nWhen num_labels=1 the trainer automatically knows that this is a regression problem. It then uses MSE loss.","metadata":{}},{"cell_type":"code","source":"# Regression setup:\n# model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T07:28:32.993907Z","iopub.status.idle":"2022-04-09T07:28:32.994594Z","shell.execute_reply.started":"2022-04-09T07:28:32.994338Z","shell.execute_reply":"2022-04-09T07:28:32.994376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Refer to this discussion:<br>\nhttps://discuss.huggingface.co/t/how-to-set-up-trainer-for-a-regression/12994/2\n\nAlso refer to this link to the source code referenced in the above discussion:<br>\nClick the link and scroll down. The relevant section of the source code is highlighted. This confirms that MSE loss gets used when num_labels=1.\nhttps://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/models/bert/modeling_bert.py#L1564-L1575","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resources","metadata":{}},{"cell_type":"markdown","source":"\n\n- Gradient Accumulation explanation<br>\nhttps://colab.research.google.com/github/kozodoi/website/blob/master/_notebooks/2021-02-19-gradient-accumulation.ipynb#:~:text=Simply%20speaking%2C%20gradient%20accumulation%20means,might%20find%20this%20tutorial%20useful.\n\n- fp16 explanation<br>\nhttps://www.youtube.com/watch?v=ks3oZ7Va8HU\n\n- Weight Decay explanation<br>\nhttps://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_multilayer-perceptrons/weight-decay.ipynb\n\n- Docs for the Hugging Face Trainer API<br>\nhttps://huggingface.co/docs/transformers/training\n\n- Full list of training arguments<br>\n(Click the link then scroll down until you get to TrainingArguments)<br>\nhttps://huggingface.co/transformers/main_classes/trainer.html\n\n- Explanation of Pearson correlation<br>\nhttps://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/\n\n- Dataset docs<br>\nhttps://huggingface.co/docs/datasets/v1.2.0/exploring.html\n\n- The parameters for tokenizer.encode can be found here:<br>\nhttps://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.encode","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference Notebooks","metadata":{}},{"cell_type":"markdown","source":"- US Patent Phrase Matching: Adding meaning of code<br>\nhttps://www.kaggle.com/code/xhlulu/us-patent-phrase-matching-adding-meaning-of-code\n\n- [USPPPM] BERT for Patents Baseline [train]<br>\nhttps://www.kaggle.com/code/ksork6s4/uspppm-bert-for-patents-baseline-train/notebook\n\n- USPPPM-Huggingface Train & Inference Baseline<br>\nhttps://www.kaggle.com/code/phantivia/uspppm-huggingface-train-inference-baseline\n\n- USPPPM-Huggingface patent-bert<br>\nhttps://www.kaggle.com/code/danofer/uspppm-huggingface-patent-bert\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}