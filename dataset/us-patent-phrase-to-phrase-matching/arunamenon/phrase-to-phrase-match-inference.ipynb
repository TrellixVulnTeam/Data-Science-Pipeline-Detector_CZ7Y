{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## U.S. Patent Phrase to Phrase Matching\n\nTraining code link: https://www.kaggle.com/code/arunamenon/phrase-to-phrase-match-eda-modelling-deberta/notebook\n\nIf this notebook is helpful, feel free to upvote! :)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nimport re\nimport time\nimport random\nimport scipy as sp\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\ntqdm.pandas()\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nfrom sklearn import model_selection as sk_model_selection\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\nSEED=42\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:36.826197Z","iopub.execute_input":"2022-04-03T17:43:36.826468Z","iopub.status.idle":"2022-04-03T17:43:45.722391Z","shell.execute_reply.started":"2022-04-03T17:43:36.826391Z","shell.execute_reply":"2022-04-03T17:43:45.720953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load test data","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\nsubmission = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.724213Z","iopub.execute_input":"2022-04-03T17:43:45.72444Z","iopub.status.idle":"2022-04-03T17:43:45.74512Z","shell.execute_reply.started":"2022-04-03T17:43:45.724406Z","shell.execute_reply":"2022-04-03T17:43:45.744479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\ncpc_text = torch.load('../input/cpc-category-texts/cpc_texts.pth')\ntest_data['context_text'] = test_data['context'].map(cpc_text)\ntest_data['text'] = test_data['anchor'] + '[SEP]' + test_data['target'] + '[SEP]'  + test_data['context_text']","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.746774Z","iopub.execute_input":"2022-04-03T17:43:45.747286Z","iopub.status.idle":"2022-04-03T17:43:45.768651Z","shell.execute_reply.started":"2022-04-03T17:43:45.747248Z","shell.execute_reply":"2022-04-03T17:43:45.768007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.770653Z","iopub.execute_input":"2022-04-03T17:43:45.770903Z","iopub.status.idle":"2022-04-03T17:43:45.789988Z","shell.execute_reply.started":"2022-04-03T17:43:45.77087Z","shell.execute_reply":"2022-04-03T17:43:45.789352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model and process data","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.790879Z","iopub.execute_input":"2022-04-03T17:43:45.791185Z","iopub.status.idle":"2022-04-03T17:43:45.800265Z","shell.execute_reply.started":"2022-04-03T17:43:45.791145Z","shell.execute_reply":"2022-04-03T17:43:45.799514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    apex=True\n    print_freq=100\n    num_workers=4\n    model=\"microsoft/deberta-v3-large\"\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=1\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=16\n    fc_dropout=0.2\n    target_size=1\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=SEED\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    train=True","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.801458Z","iopub.execute_input":"2022-04-03T17:43:45.801786Z","iopub.status.idle":"2022-04-03T17:43:45.810098Z","shell.execute_reply.started":"2022-04-03T17:43:45.801749Z","shell.execute_reply":"2022-04-03T17:43:45.809356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:45.811229Z","iopub.execute_input":"2022-04-03T17:43:45.812682Z","iopub.status.idle":"2022-04-03T17:43:46.547025Z","shell.execute_reply.started":"2022-04-03T17:43:45.812657Z","shell.execute_reply":"2022-04-03T17:43:46.546244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.max_len = 133 # from training pipeline","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.550134Z","iopub.execute_input":"2022-04-03T17:43:46.55036Z","iopub.status.idle":"2022-04-03T17:43:46.555892Z","shell.execute_reply.started":"2022-04-03T17:43:46.550323Z","shell.execute_reply":"2022-04-03T17:43:46.555183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.labels = df['score'].values\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = torch.tensor(self.labels[item], dtype=torch.float)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.559142Z","iopub.execute_input":"2022-04-03T17:43:46.559358Z","iopub.status.idle":"2022-04-03T17:43:46.570519Z","shell.execute_reply.started":"2022-04-03T17:43:46.559334Z","shell.execute_reply":"2022-04-03T17:43:46.569867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train/notebook\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 256),\n            nn.Tanh(),\n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.573597Z","iopub.execute_input":"2022-04-03T17:43:46.574098Z","iopub.status.idle":"2022-04-03T17:43:46.589486Z","shell.execute_reply.started":"2022-04-03T17:43:46.57406Z","shell.execute_reply":"2022-04-03T17:43:46.588781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    predictions = np.concatenate(predictions)\n    return losses.avg, predictions\n\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.590974Z","iopub.execute_input":"2022-04-03T17:43:46.591524Z","iopub.status.idle":"2022-04-03T17:43:46.605842Z","shell.execute_reply.started":"2022-04-03T17:43:46.591487Z","shell.execute_reply":"2022-04-03T17:43:46.605154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['score'] = 0 # dummy value\ntest_dataset = TestDataset(CFG, test_data)\n\ntest_loader = DataLoader(test_dataset,\n                          batch_size=CFG.batch_size,\n                          shuffle=False,\n                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.607253Z","iopub.execute_input":"2022-04-03T17:43:46.607752Z","iopub.status.idle":"2022-04-03T17:43:46.618665Z","shell.execute_reply.started":"2022-04-03T17:43:46.607712Z","shell.execute_reply":"2022-04-03T17:43:46.617871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\n\npredictions = []\nmodel = CustomModel(CFG, config_path='../input/deberta-config/config.pth', pretrained=False)\n\nstate = torch.load('../input/deberta-fine-tuned-model/microsoft-deberta-v3-large_epoch_2.pth', \n                         map_location=torch.device('cpu'))\nmodel.load_state_dict(state['model'])\n\n# Get preds for test data\npredictions = inference_fn(test_loader, model, device)\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:43:46.620353Z","iopub.execute_input":"2022-04-03T17:43:46.620927Z","iopub.status.idle":"2022-04-03T17:44:14.399085Z","shell.execute_reply.started":"2022-04-03T17:43:46.620886Z","shell.execute_reply":"2022-04-03T17:44:14.397714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['score'] = predictions\ndisplay(submission.head())\nsubmission[['id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:44:14.401976Z","iopub.execute_input":"2022-04-03T17:44:14.402247Z","iopub.status.idle":"2022-04-03T17:44:14.423102Z","shell.execute_reply.started":"2022-04-03T17:44:14.402211Z","shell.execute_reply":"2022-04-03T17:44:14.422381Z"},"trusted":true},"execution_count":null,"outputs":[]}]}