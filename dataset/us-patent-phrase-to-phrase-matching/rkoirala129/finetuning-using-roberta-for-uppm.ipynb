{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport tqdm\nimport math\nimport pandas as pd\nimport torch.nn as nn\nfrom scipy import stats\nfrom transformers import AdamW, AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:56:15.973736Z","iopub.execute_input":"2022-04-18T12:56:15.97448Z","iopub.status.idle":"2022-04-18T12:56:23.841859Z","shell.execute_reply.started":"2022-04-18T12:56:15.974341Z","shell.execute_reply":"2022-04-18T12:56:23.840786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nmodel_n = \"../input/roberta-pre/patent_pretrained\"\nmax_len = 32\nbatch_size = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index=0) \n\nwarmup_ratio = 0.06\nweight_decay=0.01\ngradient_accumulation_steps = 1\nnum_train_epochs = 2\nlearning_rate = 2e-5\nadam_epsilon = 1e-08","metadata":{"execution":{"iopub.status.busy":"2022-04-18T13:00:13.055145Z","iopub.execute_input":"2022-04-18T13:00:13.055438Z","iopub.status.idle":"2022-04-18T13:00:13.062914Z","shell.execute_reply.started":"2022-04-18T13:00:13.055406Z","shell.execute_reply":"2022-04-18T13:00:13.061733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhraseDataset:\n    def __init__(self, anchor, target, context, score, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.score = score\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item] \n        context = self.context[item]\n        target = self.target[item]\n        score = self.score[item]\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            add_special_tokens = True,\n            max_length=self.max_len,\n            padding= 'max_length',\n            truncation=True,\n            return_attention_mask = True\n        )\n\n        input_ids = encoded_text.input_ids,\n        attention_mask = encoded_text.attention_mask,\n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"score\": torch.tensor(score, dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-04-18T13:00:15.163948Z","iopub.execute_input":"2022-04-18T13:00:15.164641Z","iopub.status.idle":"2022-04-18T13:00:15.174976Z","shell.execute_reply.started":"2022-04-18T13:00:15.164587Z","shell.execute_reply":"2022-04-18T13:00:15.173797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimilarPhraseModel(nn.Module):\n    def __init__(self, model_name, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model_name = model_name\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 1)\n    \n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        outputs = outputs.cpu().detach().numpy().ravel()\n        targets = targets.cpu().detach().numpy().ravel()\n        pearsonr = stats.pearsonr(outputs, targets)\n        return {\"pearsonr\": torch.tensor(pearsonr[0], device=\"cpu\")}\n    \n    \n    def forward(self, ids, mask, score):\n        transformer_out= self.transformer(ids, mask)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        loss = nn.MSELoss()(output.squeeze(), score.squeeze())\n        metrics = self.monitor_metrics(output, score)\n        return output, loss, metrics","metadata":{"execution":{"iopub.status.busy":"2022-04-18T13:00:26.420792Z","iopub.execute_input":"2022-04-18T13:00:26.421156Z","iopub.status.idle":"2022-04-18T13:00:26.434057Z","shell.execute_reply.started":"2022-04-18T13:00:26.421124Z","shell.execute_reply":"2022-04-18T13:00:26.432866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# from dataset import PhraseDataset\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n# from config import *\n# from model import SimilarPhraseModel\n# pearson_score= 0\nfor fold_ in range(10):\n    df = pd.read_csv(\"../input/newdatawithfolds/train_folds.csv\")\n\n    context_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n    }\n\n    df.context = df.context.apply(lambda x: context_mapping[x[0]])\n\n    train_df = df[df[\"kfold\"] != fold_].reset_index(drop=True)\n    valid_df = df[df[\"kfold\"] == fold_].reset_index(drop=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_n)\n    train_dataset = PhraseDataset(\n            anchor=train_df.anchor.values,\n            target=train_df.target.values,\n            context=train_df.context.values,\n            score=train_df.score.values,\n            tokenizer=tokenizer,\n            max_len=max_len,\n        )\n\n    valid_dataset = PhraseDataset(\n            anchor=valid_df.anchor.values,\n            target=valid_df.target.values,\n            context=valid_df.context.values,\n            score=valid_df.score.values,\n            tokenizer=tokenizer,\n            max_len=max_len,\n        )\n\n    model = SimilarPhraseModel(\n            model_name=model_n,\n            learning_rate=0.001,\n        )\n    model.to(device)\n\n#     optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n\n    from torch.utils.data import DataLoader\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n\n   \n    t_total = len(train_loader) // gradient_accumulation_steps * num_train_epochs\n    optimizer_grouped_parameters = []\n    custom_parameter_names = set()\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters.extend(\n        [\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n    )\n\n    warmup_steps = math.ceil(t_total * warmup_ratio)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n\n    def get_inputs_dict(batch):\n        inputs = {key: value.to(device) for key, value in batch.items()}\n        return inputs\n\n    for epoch in range(5):\n        model.train()\n        for batch in train_loader:\n            batch = get_inputs_dict(batch)\n            input_ids = batch['ids'].permute(0, 2, 1)[:, :, -1].to(device)\n            attention_mask = batch['mask'].permute(0, 2, 1)[:, :, -1].to(device)\n            labels = batch['score'].to(device)\n            abc = model(input_ids,attention_mask,labels)\n#             optimizer.zero_grad()\n            abc[1].backward()\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        print(f\"Pearson correlation for epoch {epoch}  during training is {abc[2]}\" )\n    \n\n        model.eval()\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                batch = get_inputs_dict(batch)\n                input_ids = batch['ids'].permute(0, 2, 1)[:, :, -1].to(device)\n                attention_mask = batch['mask'].permute(0, 2, 1)[:, :, -1].to(device)\n                labels = batch['score'].to(device)\n                outputs = model(input_ids,attention_mask,labels)\n                \n        print(f\"Pearson correlation for epoch {epoch}  during validation is {outputs[2]}\")\n\n#         if(outputs[2]['pearsonr'] > pearson_score):\n#             print(f\"{outputs[2]['pearsonr']},{pearson_score}\")\n#             torch.save(model.state_dict(), \"my_best_model.pt\")\n#             pearson_score = outputs[2]['pearsonr']\n    torch.save(model.state_dict(),f\"firstmodel_{fold_}.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T09:28:52.714828Z","iopub.execute_input":"2022-04-18T09:28:52.715264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom scipy import stats\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n\nmodel_n =\"../input/roberta-pre/patent_pretrained\"\nmax_len = 32\nbatch_size = 32\nepochs = 5\nlearning_rate = 0.001\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index=0) \n\n\n\ndf_test = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\n\ncontext_mapping = {\n    \"A\": \"Human Necessities\",\n    \"B\": \"Operations and Transport\",\n    \"C\": \"Chemistry and Metallurgy\",\n    \"D\": \"Textiles\",\n    \"E\": \"Fixed Constructions\",\n    \"F\": \"Mechanical Engineering\",\n    \"G\": \"Physics\",\n    \"H\": \"Electricity\",\n    \"Y\": \"Emerging Cross-Sectional Technologies\",\n}\ndf_test.context = df_test.context.apply(lambda x: context_mapping[x[0]])\n\n\nclass PhraseTestDataset:\n    def __init__(self, anchor, target, context, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item]\n        context = self.context[item]\n        target = self.target[item]\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        attention_mask = encoded_text[\"attention_mask\"]\n       \n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long)\n        }\n\n\nclass PhraseModelTest(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model_name = model_name\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        return output, 0, {}\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_n)\ntest_dataset = PhraseTestDataset(\n        anchor=df_test.anchor.values,\n        target=df_test.target.values,\n        context=df_test.context.values,\n        tokenizer=tokenizer,\n        max_len=max_len,\n    )\n\nfrom torch.utils.data import DataLoader\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\ndef get_inputs_dict(batch):\n    inputs = {key: value.to(device) for key, value in batch.items()}\n    return inputs\n\n\ndef predict(model_paths):\n    model_path = model_paths\n\n    model_test = PhraseModelTest(model_n)\n    model_test.load_state_dict(torch.load(model_path))\n    model_test.to(device)\n\n    model_test.eval()\n    test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = get_inputs_dict(batch)\n           \n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            test_output,_,_ = model_test(input_ids,attention_mask)\n            test_preds.append(test_output.detach().cpu().numpy())\n\n    new_predctions = np.array(test_preds).ravel()\n    return new_predctions\n\n\n# final_pred = predict(\"./my_best_model.pt\")\n# print(final_pred)\np1 = predict(\"./firstmodel_0.pt\")\nprint(p1)\np2 = predict(\"./firstmodel_1.pt\")\nprint(p2)\np3 = predict(\"./firstmodel_2.pt\")\nprint(p3)\np4 = predict(\"./firstmodel_3.pt\")\nprint(p4)\np5 = predict(\"./firstmodel_4.pt\")\nprint(p5)\np6 = predict(\"./firstmodel_5.pt\")\nprint(p6)\np7 = predict(\"./firstmodel_6.pt\")\nprint(p7)\np8 = predict(\"./firstmodel_7.pt\")\nprint(p8)\np9 = predict(\"./firstmodel_8.pt\")\nprint(p9)\np10 = predict(\"./firstmodel_9.pt\")\nprint(p10)\n\nfinal_pred = (p1+p2+p3+p4+p5+p6+p7+p8+p9+p10)/10\n# final_pred = p1\nsubmission = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsubmission.score = final_pred\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}