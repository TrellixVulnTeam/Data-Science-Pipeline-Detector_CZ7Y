{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"train code: https://www.kaggle.com/code/columbia2131/uspppm-roberta-base-colab-baseline-train/notebook","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport torch\n\nclass CFG_exp001:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    max_len = 64\n    batch_size = 128\n    MODEL_PATH = '../input/roberta-base'\n    WEIGHT_PATH = '../input/usp-exp001-roberta-base-epoch10/model'\n    model_prefix = 'baseline_'\n    model_weights = glob(os.path.join(WEIGHT_PATH, '*.pth'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-22T08:07:06.005464Z","iopub.execute_input":"2022-03-22T08:07:06.006057Z","iopub.status.idle":"2022-03-22T08:07:07.513922Z","shell.execute_reply.started":"2022-03-22T08:07:06.005938Z","shell.execute_reply":"2022-03-22T08:07:07.513046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ========================================\n# Library\n# ========================================\nimport os\nimport gc\nimport sys\nimport joblib\nimport random\nimport warnings\nimport itertools\nwarnings.filterwarnings('ignore')\nfrom ast import literal_eval\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport transformers\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-22T08:07:14.567765Z","iopub.execute_input":"2022-03-22T08:07:14.568475Z","iopub.status.idle":"2022-03-22T08:07:15.738384Z","shell.execute_reply.started":"2022-03-22T08:07:14.568434Z","shell.execute_reply":"2022-03-22T08:07:15.73768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =====================\n# Dataset, Model\n# =====================\ndef processing_features(df):\n    df['alp_context'] = df['context'].map(lambda x: x[0])\n    df['num_context'] = df['context'].map(lambda x: int(x[1:]))\n    df['alp_context'] = df['alp_context'].map({\n        'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':'7'\n    }).astype(int)\n    return df\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.anchor = df['anchor'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.alp_context = df['alp_context'].to_numpy()\n        self.num_context = df['num_context'].to_numpy()\n        \n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, index):\n        inputs = self.prepare_input(\n            self.cfg, \n            self.anchor[index], \n            self.target[index]\n        )        \n        alps = torch.tensor(\n            self.alp_context[index],\n            dtype=torch.long\n        )\n        nums = torch.tensor(\n            self.num_context[index],\n            dtype=torch.long\n        )\n        return inputs, alps, nums\n    \n    @staticmethod\n    def prepare_input(cfg, anchor_text, target_text):\n        inputs = cfg.tokenizer(\n            anchor_text, \n            target_text, \n            add_special_tokens=True,\n            max_length=cfg.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_offsets_mapping=False\n        )\n        inputs['input_ids'] = torch.tensor(\n            inputs['input_ids'],\n            dtype=torch.long\n        )\n        inputs['attention_mask'] = torch.tensor(\n            inputs['attention_mask'],\n            dtype=torch.long\n        )\n        inputs = {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask'],\n        }\n        return inputs\n\ndef collatte(inputs, labels=None):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    if not labels is None:\n        inputs = {\n            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n        }\n        labels =  labels[:,:mask_len]\n        return inputs, labels, mask_len\n                \n    else:\n        inputs = {\n            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n        }\n        return inputs, mask_len\n\n\nclass Exp001Model(nn.Module):\n    def __init__(self, cfg, num_alp=9, emb_alp=8, num_num=100, emb_num=8):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(\n            cfg.MODEL_PATH,\n            output_hidden_states=True\n        )\n        self.backbone = AutoModel.from_pretrained(\n            cfg.MODEL_PATH, \n            config=self.config\n        )\n        self.embedding_alp = nn.Embedding(\n            num_embeddings=num_alp,\n            embedding_dim=emb_alp,\n        )\n        self.embedding_num = nn.Embedding(\n            num_embeddings=num_num,\n            embedding_dim=emb_num,\n        )\n        \n        self.linear1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size+emb_alp+emb_num, 1024),\n            nn.SELU(),\n            nn.Linear(1024, 1024),\n            nn.SELU(),\n            nn.Linear(1024, 1)\n        )\n\n    def forward(self, inputs, alps, nums):\n        outputs = self.backbone(**inputs)[\"last_hidden_state\"]\n        outputs = outputs[:, 0, :]\n        alp_outputs = self.embedding_alp(alps)\n        num_outputs = self.embedding_num(nums)\n        \n        outputs = torch.cat([outputs, alp_outputs, num_outputs], axis=1)\n        outputs = self.linear1(outputs)\n        return outputs.flatten()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T08:07:32.141895Z","iopub.execute_input":"2022-03-22T08:07:32.142159Z","iopub.status.idle":"2022-03-22T08:07:32.162816Z","shell.execute_reply.started":"2022-03-22T08:07:32.142129Z","shell.execute_reply":"2022-03-22T08:07:32.162156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inferring(cfg, test, custom_model):\n    sub_pred = np.zeros(len(test), dtype=np.float32)\n    \n    for model_weight in cfg.model_weights:\n        # dataset, dataloader\n        test_dataset = TestDataset(cfg, test)\n        test_loader = DataLoader(\n            dataset=test_dataset, \n            batch_size=cfg.batch_size, \n            shuffle=False,\n            pin_memory=True\n        )\n        \n        # model\n        model = custom_model(cfg)\n        model.load_state_dict(torch.load(model_weight))\n        model = model.to(cfg.device)\n        \n        # evaluation\n        model.eval()\n        tmp_pred = []\n        with torch.no_grad():\n            for (inputs, alps, nums) in tqdm(test_loader, total=len(test_loader)):\n                for k, v in inputs.items():\n                    inputs[k] = v.to(cfg.device)\n                alps = alps.to(cfg.device)\n                nums = nums.to(cfg.device)\n                with autocast():\n                    output = model(inputs, alps, nums)\n                output = output.detach().cpu().numpy()      \n                tmp_pred.append(output)\n        tmp_pred = np.concatenate(tmp_pred)\n        sub_pred = sub_pred + tmp_pred / len(cfg.model_weights)\n\n    return sub_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-22T08:07:40.064025Z","iopub.execute_input":"2022-03-22T08:07:40.064303Z","iopub.status.idle":"2022-03-22T08:07:40.074138Z","shell.execute_reply.started":"2022-03-22T08:07:40.064271Z","shell.execute_reply":"2022-03-22T08:07:40.073153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =====================\n# Main\n# =====================\ntrain = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ntest = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\nsub = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\n\ntrain = processing_features(train)\ntest = processing_features(test)\n\nexp_ensemble = []\nfor (cfg_model, custmom_model) in [\n    (CFG_exp001, Exp001Model),]:\n    cfg_model.tokenizer = AutoTokenizer.from_pretrained(cfg_model.MODEL_PATH)\n    exp_pred = inferring(cfg_model, test, custmom_model)\n    exp_ensemble.append(exp_pred)\n    \nexp_ensemble = exp_ensemble[0]\nsub['score'] = exp_ensemble\nsub.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}