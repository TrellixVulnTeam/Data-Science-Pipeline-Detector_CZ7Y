{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, a pipeline is used from training in Google Colablatory to uploading to Kaggle Dataset.　　\n\n## Setting\n\nThe directory structure is assumed to be as follows.\n\n```\nMyDrive\n├USPatent \n│ └[Author Name]\n│    └Notebook  \n│       └[This Nootbook]\n└kaggle.json\n```\n\nAnd when you run everything, the directory structure will look like this, for example.\n\n```\nShareddrives  \n├USPatent  \n│ └colum2131\n│    ├Notebook  \n│    │   └Exp001-ColabTraining.ipynb\n│    ├Input\n│    │   ├train.csv\n│    │   ├test.csv\n│    │   └example_sample_submission.csv\n│    ├Output\n│    │   └Exp001-ColabTraining\n│    │      ├preds\n│    │      ├model\n│    │      └fig\n│    ├Dataset\n│    └Submission\n└kaggle.json\n```\n\nYou need to rewrite Config appropriately if you run this code.  \nThis code can also be run on the kaggle notebook.\n\n## Acknowledgments\n\nIn creating this notebook, I referred to the code of [@takoi](https://www.kaggle.com/takoihiraokazu) and [@mst8823](https://www.kaggle.com/mst8823), who teamed up with me for the Feedback Prize. I would like to take this opportunity to thank you!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"! nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nclass Config:\n    AUTHOR = \"colum2131\"\n\n    NAME = \"USP-Exp001-roberta-base-epoch10\"\n    MODEL_PATH = \"roberta-base\"\n    DATASET_PATH = []\n\n    COMPETITION = \"us-patent-phrase-to-phrase-matching\"\n    COLAB_PATH = \"/content/drive/Shareddrives/USPatent\" \n    DRIVE_PATH = os.path.join(COLAB_PATH, AUTHOR)\n\n    api_path = \"/content/drive/MyDrive/kaggle.json\"\n\n    seed = 42\n    num_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n    batch_size = 128\n    n_epochs = 10\n    max_len = 64\n    \n    fc_dropout = 0.1\n    weight_decay = 2e-5\n    beta = (0.9, 0.98)\n    lr = 2e-5\n    num_warmup_steps_rate = 0.01\n    clip_grad_norm = None\n    gradient_accumulation_steps = 1\n\n    upload_from_colab = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ========================================\n# Library\n# ========================================\nimport os\nimport gc\nimport sys\nimport json\nimport time\nimport shutil\nimport joblib\nimport random\nimport requests\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom ast import literal_eval\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport scipy \nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import (\n    StratifiedKFold, \n    KFold, \n    GroupKFold\n)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup(cfg):\n    cfg.COLAB = 'google.colab' in sys.modules\n    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if cfg.COLAB:\n        print('This environment is Google Colab')\n\n        # mount\n        from google.colab import drive\n        if not os.path.isdir('/content/drive'):\n            drive.mount('/content/drive') \n\n        # pip install\n        ! pip install -q transformers\n\n        # use kaggle api (need kaggle token)\n        f = open(cfg.api_path, 'r')\n        json_data = json.load(f) \n        os.environ['KAGGLE_USERNAME'] = json_data['username']\n        os.environ['KAGGLE_KEY'] = json_data['key']\n\n        # set dirs\n        cfg.DRIVE = cfg.DRIVE_PATH\n        cfg.EXP = (cfg.NAME if cfg.NAME is not None \n            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n        )\n        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n\n        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n\n        # make dirs\n        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n        \n        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n            # load dataset\n            ! pip install --upgrade --force-reinstall --no-deps kaggle\n            ! kaggle competitions download -c $cfg.COMPETITION -p $cfg.INPUT\n            filepath = os.path.join(cfg.INPUT,cfg.COMPETITION+'.zip')\n            ! unzip -d $cfg.INPUT $filepath\n            \n        \n        for path in cfg.DATASET_PATH:\n            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n            if not os.path.exists(datasetpath):\n                os.makedirs(datasetpath, exist_ok=True)\n                ! kaggle datasets download $path -p $datasetpath\n                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n                ! unzip -d $datasetpath $filepath\n\n    else:\n        print('This environment is Kaggle Kernel')\n\n        # set dirs\n        cfg.INPUT = f'../input/{cfg.COMPETITION}'\n        cfg.EXP = cfg.NAME\n        cfg.OUTPUT_EXP = cfg.NAME\n        cfg.SUBMISSION = './'\n        cfg.DATASET = '../input/'\n        \n        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n\n        # make dirs\n        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n    return cfg\n\n\ndef dataset_create_new(dataset_name, upload_dir):\n    dataset_metadata = {}\n    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n    dataset_metadata['title'] = dataset_name\n    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n        json.dump(dataset_metadata, f, indent=4)\n    api = KaggleApi()\n    api.authenticate()\n    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =====================\n# Utils\n# =====================\n# Seed\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n# KFold\ndef get_kfold(train, n_splits, seed):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    generator = kf.split(train)\n    fold_series = []\n    for fold, (idx_train, idx_valid) in enumerate(generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series\n\ndef get_stratifiedkfold(train, target_col, n_splits, seed):\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    generator = kf.split(train, train[target_col])\n    fold_series = []\n    for fold, (idx_train, idx_valid) in enumerate(generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series\n\ndef get_groupkfold(train, target_col, group_col, n_splits):\n    kf = GroupKFold(n_splits=n_splits)\n    generator = kf.split(train, train[target_col], train[group_col])\n    fold_series = []\n    for fold, (idx_train, idx_valid) in enumerate(generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =====================\n# Dataset, Model\n# =====================\ndef processing_features(df):\n    df['alp_context'] = df['context'].map(lambda x: x[0])\n    df['num_context'] = df['context'].map(lambda x: int(x[1:]))\n    df['alp_context'] = df['alp_context'].map({\n        'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':'7'\n    }).astype(int)\n    return df\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.anchor = df['anchor'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.score = df['score'].to_numpy()\n        self.alp_context = df['alp_context'].to_numpy()\n        self.num_context = df['num_context'].to_numpy()\n        \n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, index):\n        inputs = self.prepare_input(\n            self.cfg, \n            self.anchor[index], \n            self.target[index]\n        )        \n        labels = torch.tensor(\n            self.score[index],\n            dtype=torch.half\n        )\n        alps = torch.tensor(\n            self.alp_context[index],\n            dtype=torch.long\n        )\n        nums = torch.tensor(\n            self.num_context[index],\n            dtype=torch.long\n        )\n        return inputs, alps, nums, labels\n    \n    @staticmethod\n    def prepare_input(cfg, anchor_text, target_text):\n        inputs = cfg.tokenizer(\n            anchor_text, \n            target_text, \n            add_special_tokens=True,\n            max_length=cfg.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_offsets_mapping=False\n        )\n        inputs['input_ids'] = torch.tensor(\n            inputs['input_ids'],\n            dtype=torch.long\n        )\n        inputs['attention_mask'] = torch.tensor(\n            inputs['attention_mask'],\n            dtype=torch.long\n        )\n        inputs = {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask'],\n        }\n        return inputs\n\ndef collatte(inputs, labels=None):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    if not labels is None:\n        inputs = {\n            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n        }\n        labels =  labels[:,:mask_len]\n        return inputs, labels, mask_len\n                \n    else:\n        inputs = {\n            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n        }\n        return inputs, mask_len\n\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, num_alp=9, emb_alp=8, num_num=100, emb_num=8):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(\n            cfg.MODEL_PATH,\n            output_hidden_states=True\n        )\n        self.backbone = AutoModel.from_pretrained(\n            cfg.MODEL_PATH, \n            config=self.config\n        )\n        self.embedding_alp = nn.Embedding(\n            num_embeddings=num_alp,\n            embedding_dim=emb_alp,\n        )\n        self.embedding_num = nn.Embedding(\n            num_embeddings=num_num,\n            embedding_dim=emb_num,\n        )\n        \n        self.linear1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size+emb_alp+emb_num, 1024),\n            nn.SELU(),\n            nn.Linear(1024, 1024),\n            nn.SELU(),\n            nn.Linear(1024, 1)\n        )\n\n    def forward(self, inputs, alps, nums):\n        outputs = self.backbone(**inputs)[\"last_hidden_state\"]\n        outputs = outputs[:, 0, :]\n        alp_outputs = self.embedding_alp(alps)\n        num_outputs = self.embedding_num(nums)\n        \n        outputs = torch.cat([outputs, alp_outputs, num_outputs], axis=1)\n        outputs = self.linear1(outputs)\n        return outputs.flatten()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(cfg, train):\n    # =====================\n    # Training\n    # =====================\n    set_seed(cfg.seed)\n    oof_pred = np.zeros(len(train), dtype=np.float32)\n    for fold in cfg.trn_fold:\n        # dataset, dataloader\n        train_df = train.loc[cfg.folds!=fold]\n        valid_df = train.loc[cfg.folds==fold]\n        train_idx = list(train_df.index)\n        valid_idx = list(valid_df.index)\n\n        train_dataset = TrainDataset(cfg, train_df)\n        valid_dataset = TrainDataset(cfg, valid_df)\n        train_loader = DataLoader(\n            dataset=train_dataset, \n            batch_size=cfg.batch_size, \n            shuffle=True,\n            pin_memory=True,\n            drop_last=True\n        )\n        valid_loader = DataLoader(\n            dataset=valid_dataset,\n            batch_size=cfg.batch_size,\n            shuffle=False,\n            pin_memory=True,\n            drop_last=False\n        )\n\n        # model\n        model = CustomModel(cfg)\n        model = model.to(cfg.device)\n\n        # optimizer, scheduler\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {\n                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n                'weight_decay': cfg.weight_decay\n            },\n            {\n                'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n                'weight_decay': 0.0\n            }\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=cfg.lr,\n            betas=cfg.beta,\n            weight_decay=cfg.weight_decay,\n        )\n        num_train_optimization_steps = int(\n            len(train_loader) * cfg.n_epochs // cfg.gradient_accumulation_steps\n        )\n        num_warmup_steps = int(num_train_optimization_steps * cfg.num_warmup_steps_rate)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_train_optimization_steps\n        )\n\n        # model-training\n        criterion = nn.MSELoss()\n        best_val_score = -1\n        \n        for epoch in range(cfg.n_epochs):\n            # training\n            print(f\"# ============ start epoch:{epoch} ============== #\")\n            model.train() \n            val_losses_batch = []\n            scaler = GradScaler()\n            with tqdm(train_loader, total=len(train_loader)) as pbar:\n                for step, (inputs, alps, nums, labels) in enumerate(pbar):\n                    for k, v in inputs.items():\n                        inputs[k] = v.to(cfg.device)\n                    alps = alps.to(cfg.device)\n                    nums = nums.to(cfg.device)\n                    labels = labels.to(cfg.device)\n\n                    optimizer.zero_grad()\n                    with autocast():\n                        output = model(inputs, alps, nums)\n                    loss = criterion(output, labels)\n                    pbar.set_postfix({\n                        'loss': loss.item(),\n                        'lr': scheduler.get_lr()[0]\n                    })\n\n                    if cfg.gradient_accumulation_steps > 1:\n                        loss = loss / cfg.gradient_accumulation_steps\n\n                    scaler.scale(loss).backward()\n                    if cfg.clip_grad_norm is not None:\n                        torch.nn.utils.clip_grad_norm_(\n                            model.parameters(), \n                            cfg.clip_grad_norm\n                        )\n                    if (step+1) % cfg.gradient_accumulation_steps == 0:\n                        scaler.step(optimizer)\n                        scaler.update()\n                        scheduler.step()\n\n            # evaluating\n            val_preds = []\n            val_losses = []\n            val_nums = []\n            model.eval()\n            with torch.no_grad():\n                with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n                    for (inputs, alps, nums, labels) in pbar:\n                        for k, v in inputs.items():\n                            inputs[k] = v.to(cfg.device)\n                        alps = alps.to(cfg.device)\n                        nums = nums.to(cfg.device)\n                        labels = labels.to(cfg.device)\n                        with autocast():\n                            output = model(inputs, alps, nums)\n                        loss = criterion(output, labels.to(torch.float))\n                        \n                        output = output.detach().cpu().numpy()\n                        val_preds.append(output)\n                        val_losses.append(loss.item() * len(labels))\n                        val_nums.append(len(labels))\n                        pbar.set_postfix({\n                            'val_loss': loss.item()\n                        })\n\n            val_preds = np.concatenate(val_preds)\n            val_loss = sum(val_losses) / sum(val_nums)\n            corr_score = np.corrcoef(val_preds, valid_df['score'])[0, 1]\n\n            val_log = {\n                'val_loss': val_loss,\n                'score': corr_score,\n            }\n            display(val_log)\n\n            if best_val_score < corr_score:\n                print(\"save model weight\")\n                best_val_preds = val_preds\n                best_val_score = corr_score\n                torch.save(\n                    model.state_dict(), \n                    os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n                )\n\n        oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n        np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'), best_val_preds)\n        del model; gc.collect()\n\n    np.save(os.path.join(cfg.EXP_PREDS, 'oof_pred.npy'), oof_pred)\n\n    # =====================\n    # scoring\n    # =====================\n    corr_score = np.corrcoef(oof_pred, train['score'])[0, 1]\n    print('CV:', round(corr_score, 5))\n    return corr_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =====================\n# Main\n# =====================\n\n# setup\ncfg = setup(Config)\n\nimport transformers\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n# main\ntrain = pd.read_csv(os.path.join(cfg.INPUT, 'train.csv'))\ntest = pd.read_csv(os.path.join(cfg.INPUT, 'test.csv'))\nsub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n\ntrain = processing_features(train)\ntest = processing_features(test)\n\ncfg.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_PATH)\ncfg.folds = get_kfold(train, cfg.num_fold, cfg.seed)\ncfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\nscore = training(cfg, train)\n\nif cfg.upload_from_colab and cfg.COLAB:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}