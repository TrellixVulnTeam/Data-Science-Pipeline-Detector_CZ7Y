{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-20T09:38:57.904518Z","iopub.execute_input":"2022-05-20T09:38:57.904836Z","iopub.status.idle":"2022-05-20T09:38:57.94263Z","shell.execute_reply.started":"2022-05-20T09:38:57.904755Z","shell.execute_reply":"2022-05-20T09:38:57.941837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LDA Modeling","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom gensim.parsing.preprocessing import preprocess_string\nimport pandas as pd\nimport time\nimport pdb\n\nfrom gensim import corpora\nfrom gensim.models import LdaModel\nfrom gensim.models.coherencemodel import CoherenceModel\nimport matplotlib.pyplot as plt\nfrom gensim.models import LdaModel\nimport pyLDAvis.gensim               #pyLDAvis.gensim_models\nfrom scipy.spatial import distance\n\n#nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:38:59.850256Z","iopub.execute_input":"2022-05-20T09:38:59.850841Z","iopub.status.idle":"2022-05-20T09:39:41.624683Z","shell.execute_reply.started":"2022-05-20T09:38:59.850797Z","shell.execute_reply":"2022-05-20T09:39:41.623957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ntest = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\ntitle = pd.read_csv('/kaggle/input/cpc-codes/titles.csv')\ntrain_len = len(train)\ntest_len = len(test)\ntotal_len = train_len + test_len\nprint(total_len, train_len, test_len)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:41.626444Z","iopub.execute_input":"2022-05-20T09:39:41.628096Z","iopub.status.idle":"2022-05-20T09:39:42.395455Z","shell.execute_reply.started":"2022-05-20T09:39:41.628054Z","shell.execute_reply":"2022-05-20T09:39:42.394579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [train, test]\nall = pd.concat(frames, keys=['train', 'test'])\nall","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.396943Z","iopub.execute_input":"2022-05-20T09:39:42.397386Z","iopub.status.idle":"2022-05-20T09:39:42.436139Z","shell.execute_reply.started":"2022-05-20T09:39:42.397346Z","shell.execute_reply":"2022-05-20T09:39:42.435122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = all.loc['test']\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.43858Z","iopub.execute_input":"2022-05-20T09:39:42.438884Z","iopub.status.idle":"2022-05-20T09:39:42.461643Z","shell.execute_reply.started":"2022-05-20T09:39:42.438845Z","shell.execute_reply":"2022-05-20T09:39:42.460917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_LEFT_JOIN = pd.merge(all, title, left_on='context', right_on='code', how='left')\ndf_LEFT_JOIN","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.463076Z","iopub.execute_input":"2022-05-20T09:39:42.463351Z","iopub.status.idle":"2022-05-20T09:39:42.590603Z","shell.execute_reply.started":"2022-05-20T09:39:42.463314Z","shell.execute_reply":"2022-05-20T09:39:42.589606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_LEFT_JOIN.drop(['code','section','class','subclass','group','main_group'], axis=1)\ndata = data[['id','anchor','target','context','title','score']]\ndata","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.592244Z","iopub.execute_input":"2022-05-20T09:39:42.592547Z","iopub.status.idle":"2022-05-20T09:39:42.625331Z","shell.execute_reply.started":"2022-05-20T09:39:42.592507Z","shell.execute_reply":"2022-05-20T09:39:42.624611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(d):\n    d = str(d)\n    pattern = r'[^a-zA-Z\\'\\s]'\n    text = re.sub(pattern, '', d)\n    text = text.lower()\n    #text = [word for word in text if len(word)>2]\n    return ' '.join([w for w in text.split() if len(w) > 3])# 3이하인거 버리기\n\ndef preprocessing(d):\n    return preprocess_string(d)\n\ndef data_load(data):\n    df = pd.DataFrame({'text': data['total']})\n    tokenized_text = df['text'].apply(preprocessing)\n    tokenized_text = tokenized_text.to_list()\n    return tokenized_text \n\ndef make_topictable_per_doc(ldamodel, corpus):\n    topic_table = pd.DataFrame()\n    for i, topic_list in enumerate(ldamodel[corpus]):\n        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n        for j, (topic_num, prop_topic) in enumerate(doc):\n            if j == 0:  \n                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n            else:\n                break\n    return(topic_table)\n\ndef create_output(topictable, topicnum):\n    total_topic_list=list()\n    \n    for topic in range(len(topictable[\"각 토픽의 비중\"])):\n        topic_dict=dict()\n        tmp_list = list()\n        for k in range(len(topictable[\"각 토픽의 비중\"][topic])):\n            topic_dict[f'topic_{int(topictable[\"각 토픽의 비중\"][topic][k][0])}'] = float(topictable[\"각 토픽의 비중\"][topic][k][1])\n            tmp_list.append(int(topictable[\"각 토픽의 비중\"][topic][k][0]))\n\n        num_list = [i for i in range(topicnum)]\n        num_list = [i for i in num_list if i not in tmp_list]   ## 있는 토픽 제거\n\n        for n in num_list:\n            topic_dict[f'topic_{n}'] = 0.0      ## 확률 0인 토픽 넣어줌\n        sorted_dict = sorted(topic_dict.items())\n        total_topic_list.append(dict(sorted_dict))\n        \n    return total_topic_list\n","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.626813Z","iopub.execute_input":"2022-05-20T09:39:42.627086Z","iopub.status.idle":"2022-05-20T09:39:42.641934Z","shell.execute_reply.started":"2022-05-20T09:39:42.62705Z","shell.execute_reply":"2022-05-20T09:39:42.640943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['title'] = data['title'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.643463Z","iopub.execute_input":"2022-05-20T09:39:42.644038Z","iopub.status.idle":"2022-05-20T09:39:42.828966Z","shell.execute_reply.started":"2022-05-20T09:39:42.643999Z","shell.execute_reply":"2022-05-20T09:39:42.828323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"origin_data = data\norigin_data","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.830093Z","iopub.execute_input":"2022-05-20T09:39:42.830323Z","iopub.status.idle":"2022-05-20T09:39:42.845534Z","shell.execute_reply.started":"2022-05-20T09:39:42.830292Z","shell.execute_reply":"2022-05-20T09:39:42.844732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df = pd.DataFrame(columns=['id','total'])\ntotal_df['total'] = data['anchor'] + \" \" + data['target'] + \" \" +  data['title']\ntotal_df['id'] = data['id']\ntotal_df","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:42.849491Z","iopub.execute_input":"2022-05-20T09:39:42.849747Z","iopub.status.idle":"2022-05-20T09:39:42.889872Z","shell.execute_reply.started":"2022-05-20T09:39:42.849714Z","shell.execute_reply":"2022-05-20T09:39:42.889247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc_token = data_load(total_df)\nLDA_new_list = doc_token\n\n## Modeling\ndictionary = corpora.Dictionary(doc_token)\ncorpus = [dictionary.doc2bow(text) for text in doc_token] #문서를 bag-of-words 형태로 바꾼것\n\nNUM_TOPICS = 9\nNUM_PASSES = 30\nlda_model =LdaModel(corpus, num_topics = NUM_TOPICS, id2word = dictionary, passes = NUM_PASSES)\ntopics = lda_model.print_topics(num_words = 10)\n    \nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\npyLDAvis.save_html(vis, 'LDAvis_t9_total1.html')\n\ntopictable = make_topictable_per_doc(lda_model, corpus)\ntopictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\ntopictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\ntopictable[:10]\n\ntopic_list = create_output(topictable, NUM_TOPICS)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:39:52.992562Z","iopub.execute_input":"2022-05-20T09:39:52.993096Z","iopub.status.idle":"2022-05-20T09:44:21.577655Z","shell.execute_reply.started":"2022-05-20T09:39:52.993059Z","shell.execute_reply":"2022-05-20T09:44:21.576795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_list[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:21.57974Z","iopub.execute_input":"2022-05-20T09:44:21.579962Z","iopub.status.idle":"2022-05-20T09:44:21.5873Z","shell.execute_reply.started":"2022-05-20T09:44:21.579934Z","shell.execute_reply":"2022-05-20T09:44:21.586572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using BERT","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\ncpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntrain_df = pd.merge(train_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:21.588721Z","iopub.execute_input":"2022-05-20T09:44:21.5892Z","iopub.status.idle":"2022-05-20T09:44:22.238433Z","shell.execute_reply.started":"2022-05-20T09:44:21.589166Z","shell.execute_reply":"2022-05-20T09:44:22.237713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nimport string\n\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset, SequentialSampler, RandomSampler\n\ndef normalize_text(s):\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n    \n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\ntrain_df['title'] = train_df['title'].apply(lambda x: normalize_text(x))\ntrain_df['sen1'] = train_df['anchor'].astype('str')+' '+train_df['title'].astype('str')\ntrain_df = train_df.drop(['anchor','context','title'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:22.240488Z","iopub.execute_input":"2022-05-20T09:44:22.240743Z","iopub.status.idle":"2022-05-20T09:44:28.942936Z","shell.execute_reply.started":"2022-05-20T09:44:22.240693Z","shell.execute_reply":"2022-05-20T09:44:28.942188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_res = []\nfor res in topic_list:\n    temp = []\n    for val in res.values():\n        temp.append(val)\n    lda_res.append(str(temp)[1:-1])\n\ntrain_lda = lda_res[:train_len]\ntest_lda = lda_res[-test_len:]\nprint(len(lda_res), len(train_lda), len(test_lda))\nprint(train_lda[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:28.944345Z","iopub.execute_input":"2022-05-20T09:44:28.944598Z","iopub.status.idle":"2022-05-20T09:44:29.242213Z","shell.execute_reply.started":"2022-05-20T09:44:28.944564Z","shell.execute_reply":"2022-05-20T09:44:29.241389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['lda_dist'] = train_lda","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.243522Z","iopub.execute_input":"2022-05-20T09:44:29.243778Z","iopub.status.idle":"2022-05-20T09:44:29.253312Z","shell.execute_reply.started":"2022-05-20T09:44:29.243747Z","shell.execute_reply":"2022-05-20T09:44:29.252599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.254902Z","iopub.execute_input":"2022-05-20T09:44:29.255093Z","iopub.status.idle":"2022-05-20T09:44:29.266964Z","shell.execute_reply.started":"2022-05-20T09:44:29.25507Z","shell.execute_reply":"2022-05-20T09:44:29.266223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(train_df[['target','sen1', 'lda_dist']],train_df['score'],random_state=1234,test_size=0.1)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.268103Z","iopub.execute_input":"2022-05-20T09:44:29.268722Z","iopub.status.idle":"2022-05-20T09:44:29.289771Z","shell.execute_reply.started":"2022-05-20T09:44:29.268664Z","shell.execute_reply":"2022-05-20T09:44:29.289018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn.utils.clip_grad import clip_grad_norm\n\n\ntorch.cuda.is_available()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.290964Z","iopub.execute_input":"2022-05-20T09:44:29.291813Z","iopub.status.idle":"2022-05-20T09:44:29.354671Z","shell.execute_reply.started":"2022-05-20T09:44:29.291771Z","shell.execute_reply":"2022-05-20T09:44:29.353767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self,bert_path):\n        super(BERT,self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.AutoModel.from_pretrained(self.bert_path)\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(1033,1),\n            nn.Sigmoid()\n        )\n    def forward(self,ids,mask,token_type_ids, batch_lda):\n        out = self.bert(input_ids=ids,attention_mask=mask,token_type_ids=token_type_ids,return_dict=True)\n        pooler_output = out.get('pooler_output')\n        pooler_output = torch.cat([pooler_output, batch_lda], dim=1)\n        cls_out = self.fc_layer(pooler_output)\n        return cls_out","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.359744Z","iopub.execute_input":"2022-05-20T09:44:29.359974Z","iopub.status.idle":"2022-05-20T09:44:29.36861Z","shell.execute_reply.started":"2022-05-20T09:44:29.359937Z","shell.execute_reply":"2022-05-20T09:44:29.367745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train_dataset:\n    def __init__(self,text1,text2,label, tokenizer,max_len, lda_res):\n        self.text1=text1\n        self.text2=text2\n        self.label=label\n        self.lda_res = lda_res\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        label = self.label[idx]\n        lda = self.lda_res[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\": torch.tensor(label,dtype=torch.float),\n            'lda': torch.tensor(lda, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.370236Z","iopub.execute_input":"2022-05-20T09:44:29.370837Z","iopub.status.idle":"2022-05-20T09:44:29.387378Z","shell.execute_reply.started":"2022-05-20T09:44:29.370797Z","shell.execute_reply":"2022-05-20T09:44:29.386604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_lda[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.389058Z","iopub.execute_input":"2022-05-20T09:44:29.389674Z","iopub.status.idle":"2022-05-20T09:44:29.399876Z","shell.execute_reply.started":"2022-05-20T09:44:29.389632Z","shell.execute_reply":"2022-05-20T09:44:29.399062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len=128\ntrain_batch_size = 16\nepochs=5\nbert_path = '../input/bert-for-patents/bert-for-patents'\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(bert_path)\n\n# Training dataset prep\n\ntrain_text1 = list(x_train['target'].values)\ntrain_text2 = list(x_train['sen1'].values)\ntrain_label = list(y_train.values)\ntrain_lda = list(x_train['lda_dist'].values)\ntrain_lda = [ list(map(float, lda.split(', ')))  for lda in train_lda]\n\n\ntrain_dataset = Train_dataset(\n    text1 = train_text1,\n    text2 = train_text2,\n    label = train_label,\n    lda_res = train_lda,\n    tokenizer=tokenizer ,\n    max_len=max_len\n)\n\ntrain_data_loader = DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)\n\n# validation dataset prep\nval_text1 = list(x_test['target'].values)\nval_text2 = list(x_test['sen1'].values)\nval_label = list(y_test.values)\nval_lda = list(x_test['lda_dist'].values)\nval_lda = [ list(map(float, lda.split(', ')))  for lda in val_lda]\n\nvalid_dataset = Train_dataset(\n    text1 = val_text1,\n    text2 = val_text2,\n    label = val_label,\n    lda_res = val_lda,\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\nvalid_data_loader = DataLoader(valid_dataset,batch_size=train_batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.401156Z","iopub.execute_input":"2022-05-20T09:44:29.401812Z","iopub.status.idle":"2022-05-20T09:44:29.769832Z","shell.execute_reply.started":"2022-05-20T09:44:29.40177Z","shell.execute_reply":"2022-05-20T09:44:29.769107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, scheduler, loss_function, epochs,train_dataloader, device, clip_value=2):\n    model.train()\n    for epoch in range(epochs):\n        best_loss = []\n        for step, batch in enumerate(train_dataloader): \n            batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n            batch_lda = batch['lda'].to(device)\n            batch_token_type_ids = batch['token_type_ids'].to(device)\n            model.zero_grad()\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids, batch_lda)\n            loss = loss_function(outputs.squeeze(),batch_labels.squeeze())\n            best_loss.append(loss)\n            loss.backward()\n            clip_grad_norm(model.parameters(), clip_value)\n            optimizer.step()\n            scheduler.step()\n\n        loss2 = sum(best_loss)/len(best_loss)\n        print(f'Epoch : {epoch} ,Train loss : {loss2}')\n                \n    return model\n\ndef compute_metrics(predictions, labels):\n\n    predictions = predictions.reshape(len(predictions))\n    predictions = predictions.cpu().clone().numpy()\n    labels = labels.cpu().clone().numpy()\n    return  np.corrcoef(predictions, labels)[0][1]\n\ndef evaluate(model,loss_function,test_dataloader,device):\n    model.eval()\n    test_loss, test_pear = [], []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        batch_lda = batch['lda'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids, batch_lda)\n        loss = loss_function(outputs, batch_labels)\n        test_loss.append(loss.item())\n        pearson = compute_metrics(outputs, batch_labels)\n        test_pear.append(pearson.item())\n    return test_loss, test_pear","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.77155Z","iopub.execute_input":"2022-05-20T09:44:29.771959Z","iopub.status.idle":"2022-05-20T09:44:29.785288Z","shell.execute_reply.started":"2022-05-20T09:44:29.771921Z","shell.execute_reply":"2022-05-20T09:44:29.784479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_steps = len(train_data_loader) * epochs\n\nmodel = BERT(bert_path).to(device)\n\noptimizer = transformers.AdamW(model.parameters(),lr=3e-5,eps=1e-8)\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n)\n\nloss_function = nn.MSELoss()\n\n\nmodel = train(model, optimizer, scheduler, loss_function, epochs,train_data_loader, device)\n\n\nloss1,pear_ = evaluate(model,loss_function,valid_data_loader,device)\n\nloss = sum(loss1)/len(loss1)\npear = sum(pear_)/len(pear_)\nprint(f\"eval mean result : loss {loss}, pearson {pear}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:44:29.787907Z","iopub.execute_input":"2022-05-20T09:44:29.788286Z","iopub.status.idle":"2022-05-20T09:55:54.645182Z","shell.execute_reply.started":"2022-05-20T09:44:29.788249Z","shell.execute_reply":"2022-05-20T09:55:54.643466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),f'./my_bert')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:01.257765Z","iopub.execute_input":"2022-05-20T09:56:01.258038Z","iopub.status.idle":"2022-05-20T09:56:01.711758Z","shell.execute_reply.started":"2022-05-20T09:56:01.258008Z","shell.execute_reply":"2022-05-20T09:56:01.711021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\ncpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntest_df = pd.merge(test_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\ntest_df['title'] = test_df['title'].apply(lambda x: normalize_text(x))\ntest_df['sen1'] = test_df['anchor'].astype('str')+' '+test_df['title'].astype('str')\ntest_df = test_df.drop(['anchor','context','title'],axis=1)\ntest_df['lda_dist'] = test_lda","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:04.535106Z","iopub.execute_input":"2022-05-20T09:56:04.535739Z","iopub.status.idle":"2022-05-20T09:56:05.146988Z","shell.execute_reply.started":"2022-05-20T09:56:04.535677Z","shell.execute_reply":"2022-05-20T09:56:05.146232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:11.56053Z","iopub.execute_input":"2022-05-20T09:56:11.561057Z","iopub.status.idle":"2022-05-20T09:56:11.571601Z","shell.execute_reply.started":"2022-05-20T09:56:11.561018Z","shell.execute_reply":"2022-05-20T09:56:11.57084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test_datset:\n    def __init__(self,text1,text2,idf, tokenizer,max_len, lda_res):\n        self.text1=text1\n        self.text2=text2\n        self.idf = idf\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.lda_res = lda_res\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        idf = self.idf[idx]\n        lda = self.lda_res[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"idf\": idf,\n            'lda': torch.tensor(lda, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:16.893428Z","iopub.execute_input":"2022-05-20T09:56:16.894159Z","iopub.status.idle":"2022-05-20T09:56:16.906483Z","shell.execute_reply.started":"2022-05-20T09:56:16.89412Z","shell.execute_reply":"2022-05-20T09:56:16.905747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text1 = list(test_df['target'].values)\ntest_text2 = list(test_df['sen1'].values)\ntest_lda = list(test_df['lda_dist'].values) \ntest_lda = [ list(map(float, lda.split(', ')))  for lda in test_lda]\n\ntest_dataset = Test_datset(\n    text1 = test_text1,\n    text2 = test_text2,\n    idf=list(test_df['id'].values),\n    tokenizer=tokenizer,\n    max_len=max_len,\n    lda_res = test_lda\n)\n\ntest_data_loader = DataLoader(test_dataset,batch_size=64,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:18.598243Z","iopub.execute_input":"2022-05-20T09:56:18.598765Z","iopub.status.idle":"2022-05-20T09:56:18.605621Z","shell.execute_reply.started":"2022-05-20T09:56:18.598717Z","shell.execute_reply":"2022-05-20T09:56:18.604883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model,test_dataloader,device):\n    model.eval()\n    result = []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks = batch['ids'].to(device), batch['mask'].to(device)\n        batch_lda = batch['lda'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids, batch_lda )\n        out = [i[0] for i in outputs.cpu().detach().numpy()]\n        batch_idf = batch['idf']\n        temp = [[i,j] for i,j in zip(batch_idf,out)]\n        result.extend(temp)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:23.456008Z","iopub.execute_input":"2022-05-20T09:56:23.456834Z","iopub.status.idle":"2022-05-20T09:56:23.46422Z","shell.execute_reply.started":"2022-05-20T09:56:23.456783Z","shell.execute_reply":"2022-05-20T09:56:23.463459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERT(bert_path).to(device)\nmodel.load_state_dict(torch.load('my_bert'))\n\nfinal_res = predict(model,test_data_loader,device)\nsubmit_csv = pd.DataFrame(final_res,columns=['id','score'])\nsubmit_csv.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:56:29.673613Z","iopub.execute_input":"2022-05-20T09:56:29.673939Z","iopub.status.idle":"2022-05-20T09:56:35.176858Z","shell.execute_reply.started":"2022-05-20T09:56:29.673905Z","shell.execute_reply":"2022-05-20T09:56:35.17602Z"},"trusted":true},"execution_count":null,"outputs":[]}]}