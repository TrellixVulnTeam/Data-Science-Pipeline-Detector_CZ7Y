{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Analysis and using models from three notebooks\n\n**1.** Deberta v3 large (0.8392)\n> [Inference BERT for usPatents](https://www.kaggle.com/code/leehann/inference-bert-for-uspatents)\n\n**2.** Deberta v3 large (0.8338)\n> [PPPM / Deberta-v3-large baseline [inference]](https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-inference)\n\n**3.** Roberta-large (0.8143)\n> [PatentPhrase RoBERTa Inference](https://www.kaggle.com/code/santhoshkumarv/patentphrase-roberta-inference-lb-0-814)\n\n#### Please upvote the original notebooks!","metadata":{}},{"cell_type":"markdown","source":"#### The modelling part of this notebook is copied from [here](https://www.kaggle.com/code/renokan/2-deberta-1-roberta-analysis-and-using). Thanks a ton, Anatoly Bureknok! However, the EDA is my own analysis. The model results are also ensembled according to what worked best for me.\n\nIt would be great if you could upvote the notebook if this is worth it! Thanks in advance!","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nstyle.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os\nimport math\nimport re\nfrom collections import Counter\nfrom nltk import word_tokenize, pos_tag\nimport seaborn as sns\nimport spacy\nimport nltk\nnlp = spacy.load(\"en_core_web_lg\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:21.016677Z","iopub.execute_input":"2022-06-21T17:03:21.016946Z","iopub.status.idle":"2022-06-21T17:03:23.038464Z","shell.execute_reply.started":"2022-06-21T17:03:21.016917Z","shell.execute_reply":"2022-06-21T17:03:23.037735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading data directly in kaggle\ntrain = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\ntest = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.04022Z","iopub.execute_input":"2022-06-21T17:03:23.040534Z","iopub.status.idle":"2022-06-21T17:03:23.094813Z","shell.execute_reply.started":"2022-06-21T17:03:23.040498Z","shell.execute_reply":"2022-06-21T17:03:23.094162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train data shape: {train.shape}, test data shape: {test.shape}\")\n\ntrain.head()\ntrain.context.value_counts()\nprint(f\"Max score {train.score.max()} and min score {train.score.min()} \")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.096157Z","iopub.execute_input":"2022-06-21T17:03:23.096421Z","iopub.status.idle":"2022-06-21T17:03:23.108032Z","shell.execute_reply.started":"2022-06-21T17:03:23.096385Z","shell.execute_reply":"2022-06-21T17:03:23.107218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.score == 1]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.110294Z","iopub.execute_input":"2022-06-21T17:03:23.110547Z","iopub.status.idle":"2022-06-21T17:03:23.130299Z","shell.execute_reply.started":"2022-06-21T17:03:23.110513Z","shell.execute_reply":"2022-06-21T17:03:23.129488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.score == 0]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.131349Z","iopub.execute_input":"2022-06-21T17:03:23.131653Z","iopub.status.idle":"2022-06-21T17:03:23.148884Z","shell.execute_reply.started":"2022-06-21T17:03:23.131619Z","shell.execute_reply":"2022-06-21T17:03:23.148079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 106 contexts (or subjects) with H01 having the highest number of records.\n\n1154 records have score=1. This means in these ecords, anchor and target are an exact match. However, for 7471 records with score=0, they are a complete mismatch. Let's analyze this within a subject.","metadata":{}},{"cell_type":"code","source":"#How many records with perfect match are there per context group?\ncontext_counts = train[train.score == 1].groupby(\"context\").id.count().reset_index().sort_values(\"id\", ascending = False)\ncontext_counts","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.150068Z","iopub.execute_input":"2022-06-21T17:03:23.15052Z","iopub.status.idle":"2022-06-21T17:03:23.16503Z","shell.execute_reply.started":"2022-06-21T17:03:23.150486Z","shell.execute_reply":"2022-06-21T17:03:23.164173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_counts[context_counts[\"context\"]==\"H01\"]\n#H01 has most number of records = only 34/2186 records have perefect match","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.166333Z","iopub.execute_input":"2022-06-21T17:03:23.166577Z","iopub.status.idle":"2022-06-21T17:03:23.175261Z","shell.execute_reply.started":"2022-06-21T17:03:23.166544Z","shell.execute_reply":"2022-06-21T17:03:23.174554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[(train.context == \"H04\")&(train.score == 1)].head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.17629Z","iopub.execute_input":"2022-06-21T17:03:23.177127Z","iopub.status.idle":"2022-06-21T17:03:23.202696Z","shell.execute_reply.started":"2022-06-21T17:03:23.177089Z","shell.execute_reply":"2022-06-21T17:03:23.202052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[(train.context == \"H01\")&(train.score == 1)].head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.203935Z","iopub.execute_input":"2022-06-21T17:03:23.204594Z","iopub.status.idle":"2022-06-21T17:03:23.22223Z","shell.execute_reply.started":"2022-06-21T17:03:23.204556Z","shell.execute_reply":"2022-06-21T17:03:23.221447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Maybe instead of looking at absolute lengths, we should look at relative lengths compared to the anchor\ntrain[\"length_diff\"] = 0\nfor i in train.index:\n    train.length_diff.iloc[i] = len(train.target.iloc[i].split()) - len(train.anchor.iloc[i].split())\n    \ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:23.225327Z","iopub.execute_input":"2022-06-21T17:03:23.225503Z","iopub.status.idle":"2022-06-21T17:03:37.839343Z","shell.execute_reply.started":"2022-06-21T17:03:23.225481Z","shell.execute_reply":"2022-06-21T17:03:37.83847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['broad_context'] = train['context'].str.get(0)\ntrain.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:37.84118Z","iopub.execute_input":"2022-06-21T17:03:37.841517Z","iopub.status.idle":"2022-06-21T17:03:37.882405Z","shell.execute_reply.started":"2022-06-21T17:03:37.841478Z","shell.execute_reply":"2022-06-21T17:03:37.881184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x=\"broad_context\", data=train).set(title= \"Frequency plot of broad contexts\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:37.883295Z","iopub.execute_input":"2022-06-21T17:03:37.883488Z","iopub.status.idle":"2022-06-21T17:03:38.10933Z","shell.execute_reply.started":"2022-06-21T17:03:37.883466Z","shell.execute_reply":"2022-06-21T17:03:38.108687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*B has the most number of records. B stands for Operations and Transport.*","metadata":{}},{"cell_type":"code","source":"def prepare_column_for_pos(data, column_name = 'anchor'):   \n    tok_and_tag = lambda x: pos_tag(word_tokenize(x))\n    data['lower_'+ column_name] = data[column_name].apply(str.lower)\n    data['tagged_'+ column_name] = data['lower_'+ column_name].apply(tok_and_tag)\n    return data\n\ntrain = prepare_column_for_pos(train, column_name = 'anchor')\ntrain = prepare_column_for_pos(train, column_name = 'target')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:03:38.110504Z","iopub.execute_input":"2022-06-21T17:03:38.111305Z","iopub.status.idle":"2022-06-21T17:04:05.009553Z","shell.execute_reply.started":"2022-06-21T17:03:38.111266Z","shell.execute_reply":"2022-06-21T17:04:05.008851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_dict = {'NNS' : 'NN',\n                 'NNP' : 'NN',\n                 'JJR': 'JJ',\n                 'JJS': 'JJ',\n                 'PRP$' : 'PRP',\n                 'RBR' : 'RB',\n                 'RBS' : 'RB',\n                 'VBD' : 'VB',\n                 'VBG' : 'VB',\n                 'VBN' : 'VB',\n                 'VBP' : 'VB',\n                 'VBZ' : 'VB',\n                 'WP$' : 'WP'}\n\ndef replace_second_element(pos_tup_list):\n    \n    pos_tup_dict = dict(pos_tup_list)\n    for k,v in pos_tup_dict.items():\n        for word, replacement in pos_dict.items():\n            v = v.replace(word, replacement)\n            pos_tup_dict[k] = v\n    pos_tup_list = [(k,v) for k,v in pos_tup_dict.items()]\n    return pos_tup_list\n\ntrain['tagged_anchor'] = train['tagged_anchor'].apply(replace_second_element)\ntrain['tagged_target'] = train['tagged_target'].apply(replace_second_element)\n\ntrain['tagged_target']","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:05.01086Z","iopub.execute_input":"2022-06-21T17:04:05.011148Z","iopub.status.idle":"2022-06-21T17:04:05.584389Z","shell.execute_reply.started":"2022-06-21T17:04:05.011106Z","shell.execute_reply":"2022-06-21T17:04:05.58371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\npossible_tags_anchor = sorted(set(list(zip(*chain(*train['tagged_anchor'])))[1]))\npossible_tags_anchor","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:05.585697Z","iopub.execute_input":"2022-06-21T17:04:05.585959Z","iopub.status.idle":"2022-06-21T17:04:05.61405Z","shell.execute_reply.started":"2022-06-21T17:04:05.585926Z","shell.execute_reply":"2022-06-21T17:04:05.613382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"possible_tags_target = sorted(set(list(zip(*chain(*train['tagged_target'])))[1]))\npossible_tags_target","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:05.615259Z","iopub.execute_input":"2022-06-21T17:04:05.615577Z","iopub.status.idle":"2022-06-21T17:04:06.32915Z","shell.execute_reply.started":"2022-06-21T17:04:05.61554Z","shell.execute_reply":"2022-06-21T17:04:06.328407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ndef add_pos_with_zero_counts(counter, keys_to_add):\n    for k in keys_to_add:\n        counter[k] = counter.get(k, 0)\n    return counter\n\ndef make_categorical_pos_columns(data, column_name = \"anchor\", possible_tags=possible_tags_anchor ):\n    data['pos_counts'] = data['tagged_'+column_name].apply(lambda x: Counter(list(zip(*x))[1]))\n    data['pos_counts_with_zero'] = data['pos_counts'].apply(lambda x: add_pos_with_zero_counts(x, possible_tags))\n    data['sent_vector'] = data['pos_counts_with_zero'].apply(lambda x: [count for tag, count in sorted(x.most_common())])\n\n    # All in one.\n    data['sent_vector'] = data['tagged_'+column_name].apply(lambda x:\n        [count for tag, count in sorted(\n            add_pos_with_zero_counts(\n                Counter(list(zip(*x))[1]), \n                        possible_tags).most_common()\n             )\n        ]\n    )\n    \n\n    data_2 = pd.DataFrame(data['sent_vector'].tolist())\n    data_2.columns = [x + \"_\" + column_name for x in possible_tags]\n    data.drop(['pos_counts', 'pos_counts_with_zero', 'sent_vector'], axis=1, inplace=True)\n    data = pd.concat([data.reset_index(), data_2.reset_index(drop=True)], axis=1)\n    return data\n\ndef compute_difference_of_pos_metric(data, possible_tags_anchor):\n    col1 = \"anchor\"\n    col2 = \"target\"\n    # Detailed steps.\n    data = make_categorical_pos_columns(data, column_name = col1, possible_tags=possible_tags_anchor)\n    data = make_categorical_pos_columns(data, column_name = col2, possible_tags=possible_tags_target)\n    for col_prefix in possible_tags_anchor:\n        data[col_prefix+\"_diff\"] = data[col_prefix+\"_\"+col2] - data[col_prefix+\"_\"+col1]\n        data.drop([col_prefix+\"_\"+col1], axis=1, inplace=True)\n        data.drop([col_prefix+\"_\"+col2], axis=1, inplace=True)\n    data.drop(data.filter(regex='_anchor|_target').columns, axis=1, inplace=True)\n    return data\n\n\n\nprint(train.shape)\ntrain = compute_difference_of_pos_metric(train, possible_tags_anchor)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:06.330686Z","iopub.execute_input":"2022-06-21T17:04:06.331176Z","iopub.status.idle":"2022-06-21T17:04:08.625084Z","shell.execute_reply.started":"2022-06-21T17:04:06.331127Z","shell.execute_reply":"2022-06-21T17:04:08.624385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.NN_diff.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:08.626453Z","iopub.execute_input":"2022-06-21T17:04:08.626699Z","iopub.status.idle":"2022-06-21T17:04:08.636213Z","shell.execute_reply.started":"2022-06-21T17:04:08.626665Z","shell.execute_reply":"2022-06-21T17:04:08.635539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df=train[['score',\n 'length_diff',\n 'DT_diff',\n 'IN_diff',\n 'JJ_diff',\n 'NN_diff',\n 'PRP_diff',\n 'RB_diff',\n 'RP_diff',\n 'TO_diff',\n 'VB_diff',\n 'WP_diff']].corr()\nsns.heatmap(corr_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:08.637441Z","iopub.execute_input":"2022-06-21T17:04:08.639292Z","iopub.status.idle":"2022-06-21T17:04:08.963444Z","shell.execute_reply.started":"2022-06-21T17:04:08.639251Z","shell.execute_reply":"2022-06-21T17:04:08.96269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef jaccard_similarity(x,y):\n    \"\"\" returns the jaccard similarity between two lists \"\"\"\n    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n    union_cardinality = len(set.union(*[set(x), set(y)]))\n    return intersection_cardinality/float(union_cardinality)\n\ntrain['jacc_sim_score'] = train.apply(lambda x: jaccard_similarity(x.target, x.anchor), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:08.96452Z","iopub.execute_input":"2022-06-21T17:04:08.964768Z","iopub.status.idle":"2022-06-21T17:04:10.011078Z","shell.execute_reply.started":"2022-06-21T17:04:08.964735Z","shell.execute_reply":"2022-06-21T17:04:10.010354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 10))\nfig.suptitle('Trends of Similarity Score')\nsns.scatterplot(ax=axes[0],data=train,  x=\"score\",y=\"jacc_sim_score\").set(title='Jaccard Similarity vs Score')\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nsns.scatterplot(ax=axes[1],data=train,  x=\"score\",y=\"jacc_sim_score\", hue=\"broad_context\").set(title='Jaccard Similarity vs Score by Context')\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:10.012461Z","iopub.execute_input":"2022-06-21T17:04:10.012722Z","iopub.status.idle":"2022-06-21T17:04:12.112542Z","shell.execute_reply.started":"2022-06-21T17:04:10.012688Z","shell.execute_reply":"2022-06-21T17:04:12.11188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ttr(sent):\n    sent = re.sub(r'[^\\w]', ' ', sent)\n    sent = sent.lower()\n    tokens = nltk.word_tokenize(sent)\n    types = nltk.Counter(tokens)\n    ttr = len(types)/len(tokens)*100\n    return ttr\n\ndef ttr_ratio(sent1,sent2):\n    ttr1 = ttr(sent1)\n    ttr2 = ttr(sent2)\n    return ttr1/ttr2\n\ntrain['ttr_comp_score'] = train.apply(lambda x: ttr_ratio(x.target, x.anchor), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:12.113829Z","iopub.execute_input":"2022-06-21T17:04:12.11423Z","iopub.status.idle":"2022-06-21T17:04:20.325596Z","shell.execute_reply.started":"2022-06-21T17:04:12.114192Z","shell.execute_reply":"2022-06-21T17:04:20.324873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=train,  x=\"score\",y=\"ttr_comp_score\", hue=\"broad_context\").set(title='TTR (Text Complexity) vs Score')\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:20.327004Z","iopub.execute_input":"2022-06-21T17:04:20.327275Z","iopub.status.idle":"2022-06-21T17:04:21.841334Z","shell.execute_reply.started":"2022-06-21T17:04:20.32724Z","shell.execute_reply":"2022-06-21T17:04:21.84063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_shannon_entropy(string,base = 2.0):\n    #make set with all unrepeatable symbols from string\n    dct = dict.fromkeys(list(string))\n\n    #calculate frequencies\n    pkvec =  [float(string.count(c)) / len(string) for c in dct]\n\n    #calculate Entropy\n    H = -sum([pk  * math.log(pk) / math.log(base) for pk in pkvec ])\n    return H\n\ndef shannon_entr_ratio(str1,str2):\n    entr1 = calculate_shannon_entropy(str1)\n    entr2 = calculate_shannon_entropy(str2)\n    return np.log((entr1/entr2)+1)\n\ntrain['entr_comp_score'] = train.apply(lambda x: shannon_entr_ratio(x.target, x.anchor), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:21.842727Z","iopub.execute_input":"2022-06-21T17:04:21.842993Z","iopub.status.idle":"2022-06-21T17:04:24.049575Z","shell.execute_reply.started":"2022-06-21T17:04:21.842956Z","shell.execute_reply":"2022-06-21T17:04:24.048836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=train,  x=\"score\",y=\"entr_comp_score\", hue=\"broad_context\").set(title='Shannon Entropy Ratio vs Score')\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:24.051033Z","iopub.execute_input":"2022-06-21T17:04:24.051295Z","iopub.status.idle":"2022-06-21T17:04:25.64424Z","shell.execute_reply.started":"2022-06-21T17:04:24.051262Z","shell.execute_reply":"2022-06-21T17:04:25.643503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re, math, collections\n \ndef tokenize(_str):\n    stopwords = ['and', 'for', 'if', 'the', 'then', 'be', 'is', 'are', 'will', 'in', 'it', 'to', 'that']\n    tokens = collections.defaultdict(lambda: 0.)\n    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n        m = m.group(1).lower()\n        if len(m) < 2: continue\n        if m in stopwords: continue\n        tokens[m] += 1\n \n    return tokens\n#end of tokenize\n \n\ndef kldiv(_s, _t):\n    if (len(_s) == 0):\n        return 1e33\n\n    if (len(_t) == 0):\n        return 1e33\n\n    ssum = 0. + sum(_s.values())\n    slen = len(_s)\n\n    tsum = 0. + sum(_t.values())\n    tlen = len(_t)\n\n    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n    lenvocabdiff = len(vocabdiff)\n\n    \"\"\" epsilon \"\"\"\n    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n\n    \"\"\" gamma \"\"\"\n    gamma = 1 - lenvocabdiff * epsilon\n\n    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n    sc = sum([v/ssum for v in _s.values()])\n    st = sum([v/tsum for v in _t.values()])\n\n    ps=[] \n    pt = [] \n \n    div = 0.\n    for t, v in _s.items():\n        pts = v / ssum\n        ptt = epsilon\n        if t in _t:\n            ptt = gamma * (_t[t] / tsum)\n        ckl = (pts - ptt) * math.log(pts / ptt)\n        div +=  ckl\n \n    return div\n#end of kldiv\n\nd1= \"mixing core materials\"\nd2 = \"core materials mixed\"\nprint(kldiv(tokenize(d1), tokenize(d2)))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:25.645358Z","iopub.execute_input":"2022-06-21T17:04:25.646051Z","iopub.status.idle":"2022-06-21T17:04:25.662344Z","shell.execute_reply.started":"2022-06-21T17:04:25.646012Z","shell.execute_reply":"2022-06-21T17:04:25.661662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"tok_target\"] = train[\"target\"].apply(tokenize)\ntrain[\"tok_anchor\"] = train[\"anchor\"].apply(tokenize)\ntrain['kl_div'] = train.apply(lambda x: kldiv(x.tok_target, x.tok_anchor), axis=1)\ntrain.drop(['tok_target', 'tok_anchor'], axis=1, inplace = True, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:25.663371Z","iopub.execute_input":"2022-06-21T17:04:25.663612Z","iopub.status.idle":"2022-06-21T17:04:27.864211Z","shell.execute_reply.started":"2022-06-21T17:04:25.663576Z","shell.execute_reply":"2022-06-21T17:04:27.863497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:53.205502Z","iopub.execute_input":"2022-06-21T17:04:53.205767Z","iopub.status.idle":"2022-06-21T17:04:53.23097Z","shell.execute_reply.started":"2022-06-21T17:04:53.205738Z","shell.execute_reply":"2022-06-21T17:04:53.230174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df=train[['score',\n 'length_diff',\n 'DT_diff',\n 'IN_diff',\n 'JJ_diff',\n 'NN_diff',\n 'PRP_diff',\n 'RB_diff',\n 'RP_diff',\n 'TO_diff',\n 'VB_diff',\n 'WP_diff',\n 'jacc_sim_score',\n 'ttr_comp_score',\n 'entr_comp_score',\n 'kl_div']].corr()\nsns.heatmap(corr_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:27.908958Z","iopub.execute_input":"2022-06-21T17:04:27.90923Z","iopub.status.idle":"2022-06-21T17:04:28.256781Z","shell.execute_reply.started":"2022-06-21T17:04:27.909195Z","shell.execute_reply":"2022-06-21T17:04:28.255995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't see a strong correlation with these structural entities. The correlation scores are all in the low-mid range w.r.t score. The number of Nouns and length difference between target and anchor show good correlation. This might indicate that most cases where there is a difference in the length of the 2 columns, this difference may be caused due to absence or presence of certain nouns. Since most nouns are more relevant to the context, they might actually contain relevant information.\n\n\nWe also see a high correlation with text similarity score, which makes sense. However, there is no correlation with TTR ratio of the two text columns. A similar trend is seen for KL divergence between the two strings.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import & Def & Set & Load","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom dataclasses import dataclass\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True    \n    torch.backends.cudnn.benchmark = False\n\n    \ndef inference_fn(test_loader, model, device, is_sigmoid=True):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n        if is_sigmoid == True:\n            preds.append(output.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(output.to('cpu').numpy())\n\n    return np.concatenate(preds)    \n    \n\ndef upd_outputs(data, is_trim=False, is_minmax=False, is_reshape=False):\n    min_max_scaler = MinMaxScaler()\n    \n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.precision', 4)\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 426\nCUSTOM_BATCH = 24\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"competition_dir = \"../input/us-patent-phrase-to-phrase-matching/\"\n\nsubmission = pd.read_csv(competition_dir+'sample_submission.csv')\ntest_origin = pd.read_csv(competition_dir+'test.csv')\ntest_origin.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Extract predictions\n\n## 2.1 Deberta v3 large - 1","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           truncation=True)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg        \n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.text[item])\n        \n        return inputs\n   \n    \nclass CustomModel(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, inputs):\n        output = self.base(**inputs)\n\n        return output[0]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_path='../input/deberta-v3-large/deberta-v3-large'\n    batch_size=CUSTOM_BATCH\n    num_workers=2\n    max_len=130\n    trn_fold=[0, 1, 2, 3]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n\ncontext_mapping = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest.reset_index(inplace=True)\ntest = test.merge(titles, left_on='context', right_on='code')\ntest.sort_values(by='index', inplace=True)\ntest.drop(columns='index', inplace=True)\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_predicts_1 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size, shuffle=False,\n                             num_workers=CFG.num_workers,\n                             pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = CustomModel(CFG.model_path)    \n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_dataloader, model, DEVICE, is_sigmoid=False)\n    \n    deberta_predicts_1.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------- inference_fn([...], is_sigmoid=False)\ndeberta_predicts_1 = [upd_outputs(x, is_minmax=True, is_reshape=True) for x in deberta_predicts_1]\ndeberta_predicts_1 = pd.DataFrame(deberta_predicts_1).T\n\ndeberta_predicts_1.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Deberta v3 large - 2","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=2\n    path=\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0, 1, 2, 3]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_predicts_2 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    deberta_predicts_2.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_predicts_2 = [upd_outputs(x, is_reshape=True) for x in deberta_predicts_2]\ndeberta_predicts_2 = pd.DataFrame(deberta_predicts_2).T\n\ndeberta_predicts_2.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3. Roberta-large","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text, target):\n    inputs = cfg.tokenizer(text, target,\n                           padding=\"max_length\",\n                           max_length=cfg.max_len,\n                           truncation=True)\n\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.target = df['target'].values\n        \n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        target = self.target[item]\n        \n        inputs = prepare_input(self.cfg, text, target)\n        \n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n\n        config = AutoConfig.from_pretrained(CFG.config_path)\n\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": hidden_dropout_prob,\n                       \"layer_norm_eps\": layer_norm_eps,\n                       \"add_pooling_layer\": False})\n        \n        self.transformer = AutoModel.from_pretrained(CFG.config_path, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(config.hidden_size, CFG.num_targets)\n        \n    def forward(self, inputs):\n        transformer_out = self.transformer(**inputs)\n        last_hidden_states = transformer_out[0]\n        last_hidden_states = self.dropout(torch.mean(last_hidden_states, 1))\n        logits1 = self.output(self.dropout1(last_hidden_states))\n        logits2 = self.output(self.dropout2(last_hidden_states))\n        logits3 = self.output(self.dropout3(last_hidden_states))\n        logits4 = self.output(self.dropout4(last_hidden_states))\n        logits5 = self.output(self.dropout5(last_hidden_states))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        \n        return logits","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass CFG:\n    num_workers=2\n    config_path='../input/robertalarge'\n    model_path='../input/phrase-matching-roberta-training-pytorch-wandb'\n    model_name='roberta-large'\n    batch_size=CUSTOM_BATCH\n    max_len=128\n    num_targets=1\n    trn_fold=[0, 1, 2, 3, 4]\n    tokenizer=AutoTokenizer.from_pretrained('../input/robertalarge')\n\ncontext_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_origin.copy()\n\ntest['context_text'] = test['context'].str.slice(stop=1).map(context_mapping)\ntest['text'] = test['context_text'] + ' ' + test['anchor']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_predicts = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.model_path + f\"/{CFG.model_name.replace('-','_')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_patent_model_{fold}.pth\"\n    \n    model = CustomModel()\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state)\n\n    prediction = inference_fn(test_loader, model, DEVICE)\n    roberta_predicts.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()    \n    gc.collect()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_predicts = [upd_outputs(x, is_reshape=True) for x in roberta_predicts]\nroberta_predicts = pd.DataFrame(roberta_predicts).T\n\nroberta_predicts.head(10).style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test, test_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Comparison / Ensemble","metadata":{}},{"cell_type":"code","source":"all_predictions = pd.concat(\n    [deberta_predicts_1, deberta_predicts_2, roberta_predicts],\n    keys=['deberta 1', 'deberta 2', 'roberta'],\n    axis=1\n)\n\nall_predictions.head(10) \\\n    .assign(mean=lambda x: x.mean(axis=1)) \\\n        .style.background_gradient(cmap=cm, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mean = pd.DataFrame({\n    'deberta 1': deberta_predicts_1.mean(axis=1),\n    'deberta 2': deberta_predicts_2.mean(axis=1),\n    'roberta': roberta_predicts.mean(axis=1)\n})\n\nall_mean.head(10) \\\n    .assign(mean=lambda x: x.mean(axis=1)) \\\n        .style.highlight_max(axis=1, props=props_param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === N1 ===\nweights_ = [0.85, 0.1, 0.05]\nfinal_predictions1 = all_mean.mul(weights_).sum(axis=1)\n\n# === N2 ===\nfinal_predictions2 = all_mean.mean(axis=1)\n\nfinal_predictions = np.mean([final_predictions1, final_predictions2], axis=0)\nfinal_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of preds 1:\",len(final_predictions1))\nprint(\"Shape of preds 2:\",len(final_predictions2))\nprint(\"Shape of final preds:\",len(final_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_origin['id'],\n    'score': final_predictions,\n})\n\nsubmission.head(14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}