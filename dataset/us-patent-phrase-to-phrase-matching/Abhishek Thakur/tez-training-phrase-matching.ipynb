{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Tez can be found here: https://github.com/abhishekkrthakur/tez\n\nPlease give it some love by starring the repo!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-22T14:11:35.131733Z","iopub.execute_input":"2022-03-22T14:11:35.132354Z","iopub.status.idle":"2022-03-22T14:11:35.160848Z","shell.execute_reply.started":"2022-03-22T14:11:35.132259Z","shell.execute_reply":"2022-03-22T14:11:35.160178Z"}}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nimport pandas as pd\nimport torch.nn as nn\n\nfrom scipy import stats\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:11:35.232691Z","iopub.execute_input":"2022-03-22T14:11:35.234766Z","iopub.status.idle":"2022-03-22T14:11:41.818745Z","shell.execute_reply.started":"2022-03-22T14:11:35.234728Z","shell.execute_reply":"2022-03-22T14:11:41.817994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    model = \"anferico/bert-for-patents\"\n    max_len = 32\n    accumulation_steps = 1\n    batch_size = 32\n    epochs = 5\n    learning_rate = 2e-5","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:11:41.822118Z","iopub.execute_input":"2022-03-22T14:11:41.82239Z","iopub.status.idle":"2022-03-22T14:11:41.828679Z","shell.execute_reply.started":"2022-03-22T14:11:41.822362Z","shell.execute_reply":"2022-03-22T14:11:41.828016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhraseDataset:\n    def __init__(self, anchor, target, context, score, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.score = score\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item]\n        context = self.context[item]\n        target = self.target[item]\n        score = self.score[item]\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        attention_mask = encoded_text[\"attention_mask\"]\n        token_type_ids = encoded_text[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"score\": torch.tensor(score, dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:11:41.831424Z","iopub.execute_input":"2022-03-22T14:11:41.833037Z","iopub.status.idle":"2022-03-22T14:11:41.844246Z","shell.execute_reply.started":"2022-03-22T14:11:41.83301Z","shell.execute_reply":"2022-03-22T14:11:41.843624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass PhraseModel(nn.Module):\n    def __init__(self, model_name, learning_rate, num_train_steps, steps_per_epoch):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.steps_per_epoch = steps_per_epoch\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 1)\n\n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        outputs = outputs.cpu().detach().numpy().ravel()\n        targets = targets.cpu().detach().numpy().ravel()\n        pearsonr = stats.pearsonr(outputs, targets)\n        return {\"pearsonr\": torch.tensor(pearsonr[0], device=device)}\n\n    def optimizer_scheduler(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.01,\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        opt = torch.optim.AdamW(optimizer_parameters, lr=self.learning_rate)\n        sch = get_linear_schedule_with_warmup(\n            opt,\n            num_warmup_steps=0,\n            num_training_steps=self.num_train_steps,\n        )\n        return opt, sch\n\n    def forward(self, ids, mask, token_type_ids, score):\n        transformer_out = self.transformer(ids, mask, token_type_ids)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        loss = nn.MSELoss()(output.squeeze(), score.squeeze())\n        metrics = self.monitor_metrics(output, score)\n        return output, loss, metrics","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:11:41.846313Z","iopub.execute_input":"2022-03-22T14:11:41.846993Z","iopub.status.idle":"2022-03-22T14:11:41.861631Z","shell.execute_reply.started":"2022-03-22T14:11:41.846956Z","shell.execute_reply":"2022-03-22T14:11:41.860885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold_ in range(10):\n    df = pd.read_csv(\"../input/uspppm-folds/train_folds.csv\")\n\n    context_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n    }\n\n    df.context = df.context.apply(lambda x: context_mapping[x[0]])\n\n    train_df = df[df[\"kfold\"] != fold_].reset_index(drop=True)\n    valid_df = df[df[\"kfold\"] == fold_].reset_index(drop=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n    train_dataset = PhraseDataset(\n        anchor=train_df.anchor.values,\n        target=train_df.target.values,\n        context=train_df.context.values,\n        score=train_df.score.values,\n        tokenizer=tokenizer,\n        max_len=args.max_len,\n    )\n    valid_dataset = PhraseDataset(\n        anchor=valid_df.anchor.values,\n        target=valid_df.target.values,\n        context=valid_df.context.values,\n        score=valid_df.score.values,\n        tokenizer=tokenizer,\n        max_len=args.max_len,\n    )\n\n    num_train_steps = int(len(train_dataset) / args.batch_size / args.accumulation_steps * args.epochs)\n    steps_per_epoch = len(train_dataset) / args.batch_size\n\n    model = PhraseModel(\n        model_name=args.model,\n        learning_rate=args.learning_rate,\n        num_train_steps=num_train_steps,\n        steps_per_epoch=steps_per_epoch,\n    )\n\n    # convert model to Tez model\n    model = Tez(model)\n\n    es = EarlyStopping(\n        monitor=\"valid_pearsonr\",\n        model_path=f\"model_f{fold_}.bin\",\n        patience=2,\n        mode=\"max\",\n        save_weights_only=True,\n    )\n\n    config = TezConfig(\n        training_batch_size=args.batch_size,\n        validation_batch_size=2 * args.batch_size,\n        gradient_accumulation_steps=args.accumulation_steps,\n        epochs=args.epochs,\n        step_scheduler_after=\"batch\",\n        fp16=True,\n    )\n\n    # just like keras ;)\n    model.fit(\n        train_dataset,\n        valid_dataset=valid_dataset,\n        config=config,\n        callbacks=[es],\n    )\n    \n    # remove break to train all 10 folds\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:12:53.715153Z","iopub.execute_input":"2022-03-22T14:12:53.71541Z","iopub.status.idle":"2022-03-22T14:15:37.665692Z","shell.execute_reply.started":"2022-03-22T14:12:53.715381Z","shell.execute_reply":"2022-03-22T14:15:37.662976Z"},"trusted":true},"execution_count":null,"outputs":[]}]}