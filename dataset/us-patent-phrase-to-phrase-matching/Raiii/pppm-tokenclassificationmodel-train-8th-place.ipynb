{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Description\n\nThie notebook is a code for training a token classification model that I wrote about in <a href=\"https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/332492\" target=\"_blank\">this post</a>.\n\nAs you will see,  I referred a lot from @yasufuminakama's notebook. If you think this notebook helpful, please upvote <a href=\"https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\" target=\"_blank\">his notebook</a>.\n","metadata":{}},{"cell_type":"markdown","source":"## Initialization","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\n\nparam = {\n    'apex': True,\n    'awp_eps': 1e-2,\n    'awp_lr': 1e-4,\n    'batch_size': 1, # 2\n    'betas': (0.9, 0.999),\n    'ckpt_name': 'deberta_v3_large',\n    'debug': True, # False\n    'decoder_lr': 1e-5,\n    'encoder_lr': 1e-5,\n    'eps': 1e-6,\n    'max_grad_norm': 1000,\n    'max_len': 400, # 512\n    'min_lr': 1e-7,\n    'model_name': 'microsoft/deberta-v3-large',\n    'n_cycles': 0.5,\n    'n_epochs': 2, # 12\n    'n_eval_steps': 100,\n    'n_folds': 2, # 4\n    'n_gradient_accumulation_steps': 1,\n    'n_warmup_steps': 0,\n    'n_workers': 0,\n    'nth_awp_start_epoch': 1, # 4\n    'output_dir': './output/',\n    'print_freq': 100,\n    'scheduler_name': 'cosine',\n    'seed': 42,\n    'tar_token': '[TAR]',\n    'weight_decay': 0.01,\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:28:42.35873Z","iopub.execute_input":"2022-06-25T04:28:42.359747Z","iopub.status.idle":"2022-06-25T04:28:42.374729Z","shell.execute_reply.started":"2022-06-25T04:28:42.359627Z","shell.execute_reply":"2022-06-25T04:28:42.373518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self, d: dict) -> None:\n        for k,v in d.items():\n            setattr(self, k, v)\n\ncfg = Config(d=param)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:28:42.376572Z","iopub.execute_input":"2022-06-25T04:28:42.377153Z","iopub.status.idle":"2022-06-25T04:28:42.396437Z","shell.execute_reply.started":"2022-06-25T04:28:42.377107Z","shell.execute_reply":"2022-06-25T04:28:42.395041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nif not os.path.exists(cfg.output_dir):\n    os.makedirs(cfg.output_dir)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:28:42.400667Z","iopub.execute_input":"2022-06-25T04:28:42.401859Z","iopub.status.idle":"2022-06-25T04:28:42.410548Z","shell.execute_reply.started":"2022-06-25T04:28:42.401825Z","shell.execute_reply":"2022-06-25T04:28:42.409037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed:int) -> None:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(seed=cfg.seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:28:42.416188Z","iopub.execute_input":"2022-06-25T04:28:42.416946Z","iopub.status.idle":"2022-06-25T04:28:43.039495Z","shell.execute_reply.started":"2022-06-25T04:28:42.416912Z","shell.execute_reply":"2022-06-25T04:28:43.038038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from log import _Logger\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n\ndef get_logger(filename: str) -> _Logger:\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger(filename=cfg.output_dir+'train')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:28:43.04188Z","iopub.execute_input":"2022-06-25T04:28:43.042597Z","iopub.status.idle":"2022-06-25T04:28:43.052596Z","shell.execute_reply.started":"2022-06-25T04:28:43.042519Z","shell.execute_reply":"2022-06-25T04:28:43.050794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Split","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom sklearn.model_selection import StratifiedGroupKFold\n\ntrain_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\nif cfg.debug: \n    train_df = train_df.sample(n=1000, random_state=cfg.seed).reset_index(drop=True)\n\nkf = StratifiedGroupKFold(\n    n_splits=cfg.n_folds, \n    shuffle=True, \n    random_state=cfg.seed\n)\ntrain_df[\"score_map\"] = train_df[\"score\"].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\ntrain_df['fold'] = -1\nfor f, (tx, vx) in enumerate(kf.split(train_df, train_df[\"score_map\"], train_df[\"anchor\"])):\n    train_df.loc[vx, \"fold\"] = f\ndisplay(train_df.groupby(\"fold\").size())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:31:04.650501Z","iopub.execute_input":"2022-06-25T04:31:04.654263Z","iopub.status.idle":"2022-06-25T04:31:05.000733Z","shell.execute_reply.started":"2022-06-25T04:31:04.654211Z","shell.execute_reply":"2022-06-25T04:31:04.99874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering and Data Transformation","metadata":{}},{"cell_type":"code","source":"import re\nfrom numpy import ndarray\nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef create_word_normalizer() -> function:\n    ps = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    def normalize(word):\n        w = word.lower()\n        w = lemmatizer.lemmatize(w)\n        w = ps.stem(w)\n        return w\n    return normalize\n\ndef __normalize_words(titles: list) -> list:\n    stop_words = set(stopwords.words('english'))\n    normalizer = create_word_normalizer()\n    titles = [normalizer(t) for t in titles if t not in stop_words]\n    return titles\n\ndef normalize_words(words: ndarray, unique=True):\n    if type(words) is str:\n        words = [words]\n    sep_re = r'[\\s\\(\\){}\\[\\];,\\.]+'\n    num_re = r'\\d'\n    words = re.split(sep_re, ' '.join(words).lower())\n    words = [w for w in words if len(w) >= 3 and not re.match(num_re, w)]\n    if unique:\n        words = list(set(words))\n        words = set(__normalize_words(words))\n    else:\n        words = __normalize_words(words)\n    return words\n\ndef filter_title(title: str) -> str:\n    titles = normalize_words(title, unique=False)\n    return ','.join([t for t in titles if t in include_words])\n\ncpc_codes = pd.read_csv(\"../input/cpc-codes/titles.csv\", engine='python')\n\nnorm_titles = normalize_words(cpc_codes['title'].to_numpy(), unique=False)\nanchor_targets = train_df['target'].unique().tolist() + train_df['anchor'].unique().tolist()\nnorm_anchor_targes = normalize_words(anchor_targets)\n\ninclude_words = set(norm_titles) & norm_anchor_targes\n\ntmp_cpc_codes = cpc_codes.copy()\ntmp_cpc_codes = tmp_cpc_codes[cpc_codes['code'].str.len() >= 4]\n\ntmp_cpc_codes['section_class'] = tmp_cpc_codes['code'].apply(lambda x: x[:3])\ntitle_group_df = tmp_cpc_codes.groupby('section_class', as_index=False)[['title']].agg(list)\ntitle_group_df = title_group_df[title_group_df['section_class'].str.len() == 3]\ntitle_group_df['title'] = title_group_df['title'].apply(lambda lst: ' '.join(lst))\n\ntitle_group_df['norm_title'] = title_group_df['title'].agg(filter_title)\n\nvectorizer = CountVectorizer()\nc_vect = vectorizer.fit_transform(title_group_df['norm_title'])\nr = np.argsort(c_vect.toarray(), axis=1)[:, ::-1][::, :400]\nvect_words = vectorizer.get_feature_names_out()\nt_words = np.vectorize(lambda v: vect_words[v])(r)\n\nnorm_title = title_group_df['norm_title'].str.split(',').to_numpy().tolist()\nres = []\nfor (n, t) in zip(norm_title, t_words):\n    res.append(','.join(set(n) & set(t)))\n\ntitle_group_df['norm_title'] = res\ntitle_group_df['section'] = title_group_df.section_class.str[0:1]\ntitle_group_df['section_title'] = title_group_df['section'].map(cpc_codes.set_index('code')['title']).str.lower() + ';' + title_group_df['section_class'].map(cpc_codes.set_index('code')['title']).str.lower()\ntitle_group_df['context_text'] = title_group_df['section_title'] + '[SEP]' + title_group_df['norm_title']\ncpc_texts = dict(title_group_df[['section_class', 'context_text']].to_numpy().tolist())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:31:05.006682Z","iopub.execute_input":"2022-06-25T04:31:05.00714Z","iopub.status.idle":"2022-06-25T04:33:12.030105Z","shell.execute_reply.started":"2022-06-25T04:31:05.007093Z","shell.execute_reply":"2022-06-25T04:33:12.028585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aggregate by anchor and context\naf_dict = {}\nfor i,r in train_df[['anchor', 'fold']].iterrows():\n    af_dict[r.anchor] = r.fold\nanchor_context_grouped_target = train_df.groupby(['anchor', 'context'])['target'].apply(list)\nanchor_context_grouped_score = train_df.groupby(['anchor', 'context'])['score'].apply(list)\nanchor_context_grouped_id = train_df.groupby(['anchor', 'context'])['id'].apply(list)\ni = pd.DataFrame(anchor_context_grouped_id).reset_index()\ns = pd.DataFrame(anchor_context_grouped_score).reset_index()\nt = pd.DataFrame(anchor_context_grouped_target).reset_index()\ntrain_df = s.merge(t, on=['anchor', 'context'])\ntrain_df = train_df.merge(i, on=['anchor', 'context'])\ntrain_df['context_text'] = train_df['context'].map(cpc_texts)\ntrain_df = train_df.rename(columns={'target': 'targets', 'score': 'scores', 'id': 'ids'})\ntrain_df['fold'] = train_df['anchor'].map(af_dict)\ndisplay(train_df.head())\ndisplay(train_df.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:12.034908Z","iopub.execute_input":"2022-06-25T04:33:12.036249Z","iopub.status.idle":"2022-06-25T04:33:12.267814Z","shell.execute_reply.started":"2022-06-25T04:33:12.036196Z","shell.execute_reply":"2022-06-25T04:33:12.266587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\nspecial_tokens_dict = {'additional_special_tokens': [f'[{cfg.tar_token}]']}\ntokenizer.add_special_tokens(special_tokens_dict)\ntar_token_id = tokenizer(f'[{cfg.tar_token}]', add_special_tokens=False)['input_ids'][0]\nlogger.info(f'tar_token_id: {tar_token_id}')\nsetattr(tokenizer, 'tar_token', f'[{cfg.tar_token}]')\nsetattr(tokenizer, 'tar_token_id', tar_token_id)\ntokenizer.save_pretrained(f'{cfg.output_dir}tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:12.269433Z","iopub.execute_input":"2022-06-25T04:33:12.270269Z","iopub.status.idle":"2022-06-25T04:33:18.84341Z","shell.execute_reply.started":"2022-06-25T04:33:12.270235Z","shell.execute_reply":"2022-06-25T04:33:18.841815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"import random\nfrom pandas import DataFrame\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nclass TrainDataset(Dataset):\n    def __init__(self, df: DataFrame, is_valid: bool, tokenizer: PreTrainedTokenizer, max_len: int):\n        self.anchors = df['anchor'].to_numpy()\n        self.target_lists = df['targets'].to_numpy()\n        self.id_lists = df['ids'].to_numpy()\n        self.context_texts = df['context_text'].to_numpy()\n        self.score_lists = df['scores'].to_numpy()\n        self.is_valid = is_valid\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self) -> int:\n        return len(self.id_lists)\n\n    def __getitem__(self, item: int) -> \"tuple[dict, Tensor, Tensor]\":\n        \n        scores = np.array(self.score_lists[item])\n        target_mask = np.zeros(self.max_len)\n        targets = np.array(self.target_lists[item])\n\n        if not self.is_valid:\n            indices = list(range(len(scores)))\n            random.shuffle(indices)\n            scores = scores[indices]\n            targets = targets[indices]\n\n        text = ''\n        text += self.tokenizer.cls_token\n        text += self.anchors[item]\n        text += self.tokenizer.sep_token\n        for target in targets:\n            text += target + self.tokenizer.tar_token\n        text += self.context_texts[item] + self.tokenizer.sep_token\n        \n        encoded = self.tokenizer(\n            text,\n            max_length = self.max_len,\n            padding='max_length',\n            add_special_tokens=False,\n            truncation=True\n        )\n\n        # [cls]+[anchor]+[sep]+[target]+[tar]+[target]+[tar]...+[tar]+[cpc_text]+[sep]\n        label = torch.full([self.max_len], -1, dtype=torch.float)\n        \n        cnt_tar = 0\n        cnt_sep = 0\n        nth_target = -1\n        prev_i = -1\n\n        for i, input_id in enumerate(encoded['input_ids']):\n            if input_id == self.tokenizer.tar_token_id:\n                cnt_tar += 1\n                if cnt_tar == len(targets):\n                    break\n            if input_id == self.tokenizer.sep_token_id:\n                cnt_sep += 1\n            \n            if cnt_sep == 1 and input_id not in [self.tokenizer.pad_token_id, self.tokenizer.sep_token_id, self.tokenizer.tar_token_id]:\n                if (i-prev_i) > 1:\n                    nth_target += 1\n                label[i] = scores[nth_target]\n                target_mask[i] = 1\n                prev_i = i\n\n        for k,v in encoded.items():\n            encoded[k] = torch.tensor(v, dtype=torch.long)\n\n        return encoded, target_mask, label","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:18.845855Z","iopub.execute_input":"2022-06-25T04:33:18.846833Z","iopub.status.idle":"2022-06-25T04:33:18.870455Z","shell.execute_reply.started":"2022-06-25T04:33:18.846784Z","shell.execute_reply":"2022-06-25T04:33:18.868772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and AWP","metadata":{}},{"cell_type":"code","source":"from torch import Tensor\nfrom torch.nn import Module\nfrom transformers import AutoModel, AutoConfig\n\nclass CustomModel(Module):\n    def __init__(self, model_name: str, n_vocabs: int) -> None:\n        super().__init__()\n        self.cfg = cfg\n        self.model_config = AutoConfig.from_pretrained(\n            model_name, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(\n            model_name, config=self.model_config)\n        self.model.resize_token_embeddings(n_vocabs)\n        self.fc = nn.Linear(self.model_config.hidden_size, 1)\n        self._init_weights(self.fc)\n\n    def _init_weights(self, module: Module) -> None:\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(\n                mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(\n                mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs: dict) -> Tensor:\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs: dict) -> Tensor:\n        feature = self.feature(inputs)\n        output = self.fc(feature).squeeze(-1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:18.875056Z","iopub.execute_input":"2022-06-25T04:33:18.875489Z","iopub.status.idle":"2022-06-25T04:33:18.900888Z","shell.execute_reply.started":"2022-06-25T04:33:18.875395Z","shell.execute_reply":"2022-06-25T04:33:18.899398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import Tensor\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.nn.modules.loss import _Loss\n\nclass AWP:\n    def __init__(\n        self,\n        model: Module,\n        criterion: _Loss,\n        optimizer: Optimizer,\n        apex: bool,\n        adv_param: str=\"weight\",\n        adv_lr: float=1.0,\n        adv_eps: float=0.01\n    ) -> None:\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.adv_param = adv_param\n        self.adv_lr = adv_lr\n        self.adv_eps = adv_eps\n        self.apex = apex\n        self.backup = {}\n        self.backup_eps = {}\n\n    def attack_backward(self, inputs: dict, label: Tensor) -> Tensor:\n        with torch.cuda.amp.autocast(enabled=self.apex):\n            self._save()\n            self._attack_step() # モデルを近傍の悪い方へ改変\n            y_preds = self.model(inputs)\n            adv_loss = self.criterion(\n                y_preds.view(-1, 1), label.view(-1, 1))\n            mask = (label.view(-1, 1) != -1)\n            adv_loss = torch.masked_select(adv_loss, mask).mean()\n            self.optimizer.zero_grad()\n        return adv_loss\n\n    def _attack_step(self) -> None:\n        e = 1e-6\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                norm1 = torch.norm(param.grad)\n                norm2 = torch.norm(param.data.detach())\n                if norm1 != 0 and not torch.isnan(norm1):\n                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n                    param.data.add_(r_at)\n                    param.data = torch.min(\n                        torch.max(\n                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n                    )\n\n    def _save(self) -> None:\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                if name not in self.backup:\n                    self.backup[name] = param.data.clone()\n                    grad_eps = self.adv_eps * param.abs().detach()\n                    self.backup_eps[name] = (\n                        self.backup[name] - grad_eps,\n                        self.backup[name] + grad_eps,\n                    )\n\n    def _restore(self) -> None:\n        for name, param in self.model.named_parameters():\n            if name in self.backup:\n                param.data = self.backup[name]\n        self.backup = {}\n        self.backup_eps = {}","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:18.903407Z","iopub.execute_input":"2022-06-25T04:33:18.904078Z","iopub.status.idle":"2022-06-25T04:33:18.929445Z","shell.execute_reply.started":"2022-06-25T04:33:18.904029Z","shell.execute_reply":"2022-06-25T04:33:18.927855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainer","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:09:58.818014Z","iopub.execute_input":"2022-06-25T04:09:58.819895Z","iopub.status.idle":"2022-06-25T04:09:58.832036Z","shell.execute_reply.started":"2022-06-25T04:09:58.81984Z","shell.execute_reply":"2022-06-25T04:09:58.830674Z"}}},{"cell_type":"code","source":"import os\nimport gc\nfrom log import _Logger\nimport random\nimport warnings\nfrom functools import reduce\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nfrom numpy import ndarray\nimport scipy as sp\nimport torch\nfrom torch import inference_mode\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom IPython.display import display\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\n\ndef get_score(y_true: ndarray, y_pred: ndarray) -> float:\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\nclass Trainer:\n\n    def __init__(self, cfg:Config, logger: _Logger, tokenizer: PreTrainedTokenizer) -> None:\n        self.cfg = cfg\n        self.logger = logger\n        self.tokenizer = tokenizer\n\n    def save_ckpt(self, fold: int, model: Module, predictions: ndarray) -> None:\n        torch.save(\n            {'model': model.state_dict(), 'predictions': predictions},\n            f'{self.cfg.output_dir}{self.cfg.ckpt_name}_fold{fold}_best.pth'\n        )\n        self.logger.info('model has been saved.')\n\n    @inference_mode()\n    def valid_fn(self, dl: DataLoader, model: Module, criterion: _Loss) -> \"tuple[float, list[list[float]]]\":\n        model.eval()\n        preds = []\n        tot_loss = 0\n        for step, (inputs, target_masks, labels) in enumerate(dl):\n\n            for k, v in inputs.items():\n                inputs[k] = v.cuda()\n            labels = labels.cuda()\n            \n            y_preds = model(inputs) # (batch_size, max_len)\n\n            loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n            mask = (labels.view(-1, 1) != -1)\n            loss = torch.masked_select(loss, mask)\n            loss = loss.mean()\n            \n            if self.cfg.n_gradient_accumulation_steps > 1:\n                loss = loss / self.cfg.n_gradient_accumulation_steps\n            tot_loss += loss.item()\n\n            y_preds = y_preds.sigmoid().to('cpu').numpy()\n            labels = labels.to('cpu').numpy() # (batch_size, max_len)\n\n            anchorwise_preds = []\n            for pred, target_mask, in zip(y_preds, target_masks):\n                prev_i = -1\n                targetwise_pred_scores = []\n                for i, (p, tm) in enumerate(zip(pred, target_mask)):\n                    if tm != 0:\n                        if i-1 == prev_i:\n                            targetwise_pred_scores[-1].append(p)\n                        else:\n                            targetwise_pred_scores.append([p])\n                        prev_i = i\n                for targetwise_pred_score in targetwise_pred_scores:\n                    anchorwise_preds.append(np.mean(targetwise_pred_score))\n            preds.append(anchorwise_preds)\n\n            if step % cfg.print_freq == 0 or step == (len(dl) - 1):\n                print('EVAL: [{0}/{1}] '\n                    'Loss: {loss:.4f}({avg_loss:.4f}) '\n                    .format(step, len(dl),\n                            loss=loss.item(),\n                           avg_loss=tot_loss/(step+1))\n                )\n        \n        return tot_loss/(step+1), preds\n\n    def train_with_eval(self,\n                        fold: int,\n                        train_loader: DataLoader,\n                        valid_loader: DataLoader,\n                        valid_labels: ndarray,\n                        model: Module,\n                        criterion: _Loss,\n                        optimizer: Optimizer,\n                        epoch: int,\n                        scheduler: _LRScheduler,\n                        best_score: float) -> \"tuple[float, float]\":\n        \n        if not epoch < self.cfg.nth_awp_start_epoch:\n            self.logger.info(f'AWP training with epoch {epoch+1}')\n\n        model.train()\n        awp = AWP(\n            model, \n            criterion, \n            optimizer,\n            self.cfg.apex,\n            adv_lr=self.cfg.awp_lr, \n            adv_eps=self.cfg.awp_eps\n        )\n        scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.apex)\n        global_step = 0\n        tot_loss = 0\n        for step, (inputs, _, labels) in enumerate(train_loader):\n            for k, v in inputs.items():\n                inputs[k] = v.cuda()\n            labels = labels.cuda()\n            with torch.cuda.amp.autocast(enabled=self.cfg.apex):\n                y_preds = model(inputs)\n\n            loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n            mask = (labels.view(-1, 1) != -1)\n            loss = torch.masked_select(loss, mask).mean()\n            \n            if self.cfg.n_gradient_accumulation_steps > 1:\n                loss = loss / self.cfg.n_gradient_accumulation_steps\n            scaler.scale(loss).backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                model.parameters(), \n                self.cfg.max_grad_norm)\n\n            if self.cfg.nth_awp_start_epoch <= epoch:\n                loss = awp.attack_backward(inputs, labels)\n                scaler.scale(loss).backward()\n                awp._restore()\n            tot_loss += loss.item()\n\n            if (step + 1) % self.cfg.n_gradient_accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                global_step += 1\n                scheduler.step()\n\n            if step % self.cfg.print_freq == 0 or step == (len(train_loader) - 1):\n                self.logger.info('Epoch: [{0}][{1}/{2}] '\n                    'Loss: {loss:.4f}({avg_loss:.4f}) '\n                    'Grad: {grad_norm:.4f}  '\n                    'LR: {lr:.8f}  '\n                    .format(epoch + 1, step, len(train_loader),\n                            loss=loss.item(),\n                            avg_loss=tot_loss/(step+1),\n                            grad_norm=grad_norm,\n                            lr=scheduler.get_lr()[0]))\n\n            if (step + 1) % self.cfg.n_eval_steps == 0:\n                \n                val_loss, predictions = self.valid_fn(\n                    valid_loader, model, criterion)\n                score = get_score(\n                    valid_labels, \n                    np.array(reduce(lambda a,b: a+b, predictions)))\n                logger.info(\n                    f'Epoch {epoch+1} - avg_train_loss: {tot_loss/(step+1):.4f}  avg_val_loss: {val_loss:.4f}')\n                logger.info(f'Epoch {epoch+1} Step  Score: {score:.4f}')\n                if best_score < score:\n                    best_score = score\n                    logger.info({f\"[fold{fold}] best score\": score})\n                    self.save_ckpt(fold, model, np.array(reduce(lambda a,b: a+b, predictions)))\n                model.train()\n        return tot_loss/(step+1), best_score\n\n\n    def train_loop(self, folds: DataFrame, fold: int) -> DataFrame:\n\n        self.logger.info(f\"========== fold: {fold} training ==========\")\n\n        train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n        valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n        valid_labels = valid_folds['scores'].explode().to_numpy()\n\n        train_dataset = TrainDataset(\n            df=train_folds, is_valid=False, tokenizer=self.tokenizer, max_len=self.cfg.max_len)\n        valid_dataset = TrainDataset(\n            df=valid_folds, is_valid=True, tokenizer=self.tokenizer, max_len=self.cfg.max_len)\n\n        train_loader = DataLoader(train_dataset,\n                                batch_size=self.cfg.batch_size,\n                                shuffle=True,\n                                num_workers=self.cfg.n_workers, \n                                pin_memory=True, \n                                drop_last=True)\n        valid_loader = DataLoader(valid_dataset,\n                                batch_size=self.cfg.batch_size*2,\n                                shuffle=False,\n                                num_workers=self.cfg.n_workers, \n                                pin_memory=True, \n                                drop_last=False)\n\n        model = CustomModel(\n            self.cfg.model_name, n_vocabs=len(self.tokenizer))\n        torch.save(model.model_config, f'{self.cfg.output_dir}config.pth')\n        model.cuda()\n\n        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n            optimizer_parameters = [\n                {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n                'lr': encoder_lr, 'weight_decay': weight_decay},\n                {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n                'lr': encoder_lr, 'weight_decay': 0.0},\n                {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n                'lr': decoder_lr, 'weight_decay': 0.0}\n            ]\n            return optimizer_parameters\n\n        optimizer_parameters = get_optimizer_params(model,\n                                                    encoder_lr=self.cfg.encoder_lr,\n                                                    decoder_lr=self.cfg.decoder_lr,\n                                                    weight_decay=self.cfg.weight_decay)\n        optimizer = AdamW(\n            optimizer_parameters, \n            lr=self.cfg.encoder_lr,\n            eps=self.cfg.eps, \n            betas=self.cfg.betas)\n\n\n        def get_scheduler(scheduler_name: str, optimizer: Optimizer, num_train_steps: int, n_cycles: int) -> _LRScheduler:\n            if scheduler_name == 'linear':\n                scheduler = get_linear_schedule_with_warmup(\n                    optimizer, num_warmup_steps=cfg.n_warmup_steps, num_training_steps=num_train_steps\n                )\n            elif scheduler_name == 'cosine':\n                scheduler = get_cosine_schedule_with_warmup(\n                    optimizer, num_warmup_steps=cfg.n_warmup_steps, num_training_steps=num_train_steps, num_cycles=n_cycles\n                )\n            return scheduler\n\n        num_train_steps = int(len(train_folds) / self.cfg.batch_size * self.cfg.n_epochs)\n        scheduler = get_scheduler(\n            cfg.scheduler_name, optimizer, num_train_steps, cfg.n_cycles)\n\n        criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n        best_score = -1000.0\n\n        for epoch in range(self.cfg.n_epochs):\n\n            avg_loss, best_score = self.train_with_eval(\n                fold, \n                train_loader, \n                valid_loader, \n                valid_labels,\n                model, \n                criterion, \n                optimizer, \n                epoch, \n                scheduler, \n                best_score)\n\n            avg_val_loss, predictions = self.valid_fn(\n                valid_loader, model, criterion)\n\n            # scoring\n            score = get_score(valid_labels, np.array(reduce(lambda a,b: a+b, predictions)))\n\n            logger.info(\n                f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}')\n            logger.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n            if best_score < score:\n                best_score = score\n                logger.info(\n                    f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n                self.save_ckpt(fold, model, np.array(reduce(lambda a,b: a+b, predictions)))\n\n        predictions = torch.load(f\"{self.cfg.output_dir}{self.cfg.ckpt_name}_fold{fold}_best.pth\",\n                                map_location=torch.device('cpu'))['predictions']\n        # to no-aggregated df\n        valid_folds = valid_folds.explode(['scores', 'targets', 'ids']).rename(columns={\n            'scores': 'score',\n            'targets': 'target',\n            'ids': 'id',\n        }).reset_index(drop=True)\n        valid_folds['pred'] = predictions\n\n        torch.cuda.empty_cache(); gc.collect()\n\n        return valid_folds\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:19.02831Z","iopub.execute_input":"2022-06-25T04:33:19.029231Z","iopub.status.idle":"2022-06-25T04:33:19.095571Z","shell.execute_reply.started":"2022-06-25T04:33:19.029067Z","shell.execute_reply":"2022-06-25T04:33:19.093825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run!","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    cfg=cfg,\n    logger=logger,\n    tokenizer=tokenizer\n)\noof_df = pd.DataFrame()\nfor fold in range(cfg.n_folds):\n    _oof_df = trainer.train_loop(train_df, fold)\n    _oof_df.to_pickle(f\"{cfg.output_dir}oof_fold{fold}.pkl\")\n    oof_df = pd.concat([oof_df, _oof_df])\n    logger.info(f\"========== fold: {fold} result ==========\")\n    _score = get_score(\n        _oof_df['score'].to_numpy(), \n        _oof_df['pred'].to_numpy()\n    )\n    logger.info({f\"[fold{fold}] best score\": _score})\noof_df = oof_df.reset_index(drop=True)\n\nlogger.info(f\"========== CV ==========\")\nscore = get_score(\n    oof_df['score'].to_numpy(), \n    oof_df['pred'].to_numpy()\n)\nlogger.info({f\"overall score\": score})\noof_df.to_pickle(f'{cfg.output_dir}oof_df.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:33:19.097895Z","iopub.execute_input":"2022-06-25T04:33:19.098764Z","iopub.status.idle":"2022-06-25T04:57:19.408142Z","shell.execute_reply.started":"2022-06-25T04:33:19.098711Z","shell.execute_reply":"2022-06-25T04:57:19.406593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}