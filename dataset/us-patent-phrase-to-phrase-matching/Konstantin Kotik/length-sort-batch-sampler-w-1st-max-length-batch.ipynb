{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\n\nfrom typing import Iterator, List, Optional\n\nimport more_itertools\nimport numpy as np\n\nimport torch\nimport torch.distributed as dist\n\nfrom torch.utils.data import Sampler\nfrom torch.utils.data.sampler import T_co\n\n\nclass LengthSortBatchSamplerWithFirstMaxLength(Sampler[T_co]):\n    \"\"\"Batch Sampler that restricts data loading to a subset of the dataset. It tries to make equally-length batches\n        returning the longest batch first.\n\n        Docstring by https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler.\n        It is especially useful in conjunction with\n        :class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each\n        process can pass a :class:`~torch.utils.data.DistributedSampler` instance as a\n        :class:`~torch.utils.data.DataLoader` sampler, and load a subset of the\n        original dataset that is exclusive to it.\n\n        Args:\n            batch_size: Size of mini-batch\n            lengths: lengths list to make sampling\n            is_distributed: use or not distributed mode\n            num_replicas (int, optional): Number of processes participating in\n                distributed training. It's equvivalent to old `world_size` arg.\n                 By default, :attr:`world_size` is retrieved from the\n                current distributed group.\n            rank (int, optional): Rank of the current process within :attr:`num_replicas`.\n                By default, :attr:`rank` is retrieved from the current distributed\n                group.\n            shuffle (bool, optional): If ``True`` (default), sampler will shuffle the\n                batch indices.\n            seed (int, optional): random seed used to shuffle the sampler if\n                :attr:`shuffle=True`. This number should be identical across all\n                processes in the distributed group. Default: ``0``.\n            drop_last (bool, optional): if ``True``, then the sampler will drop the\n                tail of the data to make it evenly divisible across the number of\n                replicas. If ``True``, the sampler will drop the last batch if\n                its size would be less than ``batch_size``.\n\n        .. warning::\n            In distributed mode, calling the :meth:`set_epoch` method at\n            the beginning of each epoch **before** creating the :class:`DataLoader` iterator\n            is necessary to make shuffling work properly across multiple epochs. Otherwise,\n            the same ordering will be always used.\n        \"\"\"\n\n    def __init__(self, batch_size: int, lengths: List[int], is_distributed: bool = False,\n                 num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0,\n                 drop_last: bool = False):\n\n        super().__init__(lengths)\n\n        self.is_distributed = is_distributed\n        self.batch_size = batch_size\n        self.lengths = lengths\n        self.drop_last = drop_last\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        if self.is_distributed:\n            if num_replicas is None:\n                if not dist.is_available():\n                    raise RuntimeError(\"Requires distributed package to be available\")\n                num_replicas = dist.get_world_size()\n            if rank is None:\n                if not dist.is_available():\n                    raise RuntimeError(\"Requires distributed package to be available\")\n                rank = dist.get_rank()\n            if rank >= num_replicas or rank < 0:\n                raise ValueError(\n                    \"Invalid rank {}, rank should be in the interval\"\n                    \" [0, {}]\".format(rank, num_replicas - 1))\n\n            self.num_replicas = num_replicas\n            self.rank = rank\n            self.num_samples = math.ceil(self.__len__() / num_replicas)\n            self.total_size = self.num_samples * num_replicas\n\n    def __len__(self) -> int:\n        num_batches = len(self.lengths) // self.batch_size\n        if len(self.lengths) % self.batch_size != 0 and not self.drop_last:\n            num_batches += 1\n        return num_batches\n\n    def __iter__(self) -> Iterator[T_co]:\n\n        # construct batches\n        sorted_indices = np.argsort(self.lengths)\n        batches = list(more_itertools.chunked(sorted_indices, self.batch_size))\n\n        if self.drop_last and len(batches[-1]) < self.batch_size:\n            batches = batches[:-1]\n\n        # the batch having the longest item; w/ max length it will be first to approach OOM problem\n        longest_batch = [batches[-1]]\n\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n\n            # shuffle batch indices\n            shuffled_batches = np.array(batches[:-1])[torch.randperm(len(batches) - 1, generator=g).tolist()].tolist()\n            batches = longest_batch + shuffled_batches\n        else:\n            batches = longest_batch + batches[:-1]\n\n        if self.is_distributed:\n            assert len(batches) == self.total_size\n\n            # subsample\n            batches = batches[self.rank:self.total_size:self.num_replicas]\n            assert len(batches) == self.num_samples\n\n        return iter(batches)\n\n    def set_epoch(self, epoch: int) -> None:\n        self.epoch = epoch\n\n\nif __name__ == '__main__':\n\n    print(\"Dist mode: ON\")\n    \n    GLOO = \"gloo\"  # <SET YOUR AVAILABLE BACKEND>\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"22\"  # <SET YOUR AVAILABLE PORT>\n\n    dist.init_process_group(GLOO, rank=0, world_size=1)  # make it if it's not initialized yet\n    \n    lengths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n    for shuffle in [True, False]:\n        for batch_size in [1, 2, 3, 4, 5]:\n            dist_sampler = LengthSortBatchSamplerWithFirstMaxLength(batch_size, lengths=lengths,\n                                                                    is_distributed=True,\n                                                                    shuffle=shuffle, seed=2512)\n\n            print(list(dist_sampler))\n\n    print(\"Dist mode: OFF\")\n    lengths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\n    for shuffle in [True, False]:\n        for batch_size in [1, 2, 3, 4, 5]:\n            sampler = LengthSortBatchSamplerWithFirstMaxLength(batch_size, lengths=lengths,\n                                                               is_distributed=False,\n                                                               shuffle=shuffle,\n                                                               seed=2512)\n            print(list(sampler))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-08T19:37:16.474317Z","iopub.execute_input":"2022-06-08T19:37:16.475455Z","iopub.status.idle":"2022-06-08T19:37:18.595853Z","shell.execute_reply.started":"2022-06-08T19:37:16.475304Z","shell.execute_reply":"2022-06-08T19:37:18.594734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}