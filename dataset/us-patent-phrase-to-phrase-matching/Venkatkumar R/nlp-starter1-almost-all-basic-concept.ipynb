{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1><center><b> U.S. Patent Phrase to Phrase Matching</b></center></h1>\n\n\n\n<img src='https://www.visor.ai/wp-content/uploads/artigo-nlp-27.jpg'>\n\n\n\n### **Hello Guys! This is my first competition Based on NLP --> Two members(Me and my friend Edoziem Enyinnaya)** \n\n***I try to apply the concept in Competition data later! Now learn basics of NLP and Apply the Simple_text***\n\n### ***I am trying in Coming weeks (below the content) and finally apply the concepts in Competition data | If anyone interested in the process suggest your idea and topics guys***\n\n## **Part 1_Process**\n\n### **1. Data Cleaning and Preprocessing:**\n\n- Data-Punctuations,stopwords,number,caseconversion (Remove noise)\n- word normalization(stemming,tokenization,lemmatization,POS,NER)\n- word standartization(tables,regularexp)--> Clean text data\n\n### **2. Text Representation and word_Embedding:**\n\n- BOW\n- TF-IDF\n- Word-Embedding (word2vec,doc2vec,Glove)\n- TSNE visualization\n\n## **Part 2_Process**\n\n### **3. Transformer**\n\n- BERT, DEBERT, ROBERT, HUGGINGFACE and more process coming soon ....\n\n----------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# **Steps: D/D**\n\n### ***Import the Necessay library (Choose one NLTK,Spacy,etc....) But I'm choosing NLTK***\n\n## ***-->Part 1<--***\n\n### 1. Tokenization (Word and Sentence)\n\n### 2. Stopword\n\n### 3. Stemming\n\n### 4. Part of speech tagging\n\n### 5. Chunking\n\n### 6. Chinking\n\n### 7. Name Entity Recognition\n\n### 8. Lemmatization\n\n### 9. Corpora\n\n### 10. WordNet\n\n### 11. Word_Semantic_Similariy\n\n### 12. Apply above knowledge in one Movie review data-----> Text_Classification (Simple_NLTK_Data & NaiveBayes algo and predict output)\n\n### 13. Text Representation (One_Hot_Encoder, BOW, BOW-NGram, TF-IDF, WordEmbed(FastText, Glove, wor2vec, doc2vec))\n\n### 13. TSNE,PCA visualization\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:04.23295Z","iopub.execute_input":"2022-04-28T06:01:04.23323Z","iopub.status.idle":"2022-04-28T06:01:04.237959Z","shell.execute_reply.started":"2022-04-28T06:01:04.233199Z","shell.execute_reply":"2022-04-28T06:01:04.237095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex_text = \"Hello Mr. VK I am there, how are you doing today? The weather is great and Nltk is awesome. The Nltk is super compare to other library. Its oops based developed.\"","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:04.70422Z","iopub.execute_input":"2022-04-28T06:01:04.704959Z","iopub.status.idle":"2022-04-28T06:01:04.70871Z","shell.execute_reply.started":"2022-04-28T06:01:04.70492Z","shell.execute_reply":"2022-04-28T06:01:04.708002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Tokenizing - word tokenizers|sentence tokenizers**\n## **lexicon and corporas**\n\n- corpus -> A corpus can be defined as a collection of text documents. It can be thought as just a bunch of text files in a directory,               often alongside many other directories of text files\n- corpora -> body of text, ex: medical journals, presidential speeches, english language\n- lexicon -> words and their means\n- Token -> Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is             \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.","metadata":{}},{"cell_type":"markdown","source":"## **Sentence_Tokenize**","metadata":{}},{"cell_type":"code","source":"print(sent_tokenize(ex_text))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:05.218282Z","iopub.execute_input":"2022-04-28T06:01:05.219095Z","iopub.status.idle":"2022-04-28T06:01:05.225086Z","shell.execute_reply.started":"2022-04-28T06:01:05.219048Z","shell.execute_reply":"2022-04-28T06:01:05.224101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Word_Tokenize**","metadata":{}},{"cell_type":"code","source":"print(word_tokenize(ex_text))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:05.527987Z","iopub.execute_input":"2022-04-28T06:01:05.528914Z","iopub.status.idle":"2022-04-28T06:01:05.534892Z","shell.execute_reply.started":"2022-04-28T06:01:05.528861Z","shell.execute_reply":"2022-04-28T06:01:05.534152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in word_tokenize(ex_text):\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:05.855105Z","iopub.execute_input":"2022-04-28T06:01:05.855669Z","iopub.status.idle":"2022-04-28T06:01:05.864689Z","shell.execute_reply.started":"2022-04-28T06:01:05.855631Z","shell.execute_reply":"2022-04-28T06:01:05.863889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Stopwords**\n\nStopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”.\n","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:06.1673Z","iopub.execute_input":"2022-04-28T06:01:06.167702Z","iopub.status.idle":"2022-04-28T06:01:06.173117Z","shell.execute_reply.started":"2022-04-28T06:01:06.167666Z","shell.execute_reply":"2022-04-28T06:01:06.17114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:06.490095Z","iopub.execute_input":"2022-04-28T06:01:06.490415Z","iopub.status.idle":"2022-04-28T06:01:06.496487Z","shell.execute_reply.started":"2022-04-28T06:01:06.490364Z","shell.execute_reply":"2022-04-28T06:01:06.495546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = word_tokenize(ex_text)\n\nfiltered_sentence =[]\n\nfor w in words:\n    if w not in stop_words:\n        filtered_sentence.append(w)\n        \nprint(filtered_sentence)\n\n#filtered_sentence = [w for w in words if not w in stop_words]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:06.865962Z","iopub.execute_input":"2022-04-28T06:01:06.866562Z","iopub.status.idle":"2022-04-28T06:01:06.872542Z","shell.execute_reply.started":"2022-04-28T06:01:06.866524Z","shell.execute_reply":"2022-04-28T06:01:06.871714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Stemming**\n\nThe idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n\nThe reason why we stem is to shorten the lookup, and normalize sentences.\n\n\n\neg.. \ni was taking a ride in the car.\ni was riding in the car.\n\nPorterstemmer(1979)","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:07.246875Z","iopub.execute_input":"2022-04-28T06:01:07.247481Z","iopub.status.idle":"2022-04-28T06:01:07.253859Z","shell.execute_reply.started":"2022-04-28T06:01:07.247445Z","shell.execute_reply":"2022-04-28T06:01:07.252873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()\n\nex_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n\nfor s in ex_words:\n    print(ps.stem(s))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:07.576825Z","iopub.execute_input":"2022-04-28T06:01:07.577084Z","iopub.status.idle":"2022-04-28T06:01:07.583272Z","shell.execute_reply.started":"2022-04-28T06:01:07.577055Z","shell.execute_reply":"2022-04-28T06:01:07.582565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_t = \"It is very important to be pythoniy while you are pythoning with python. All pythoners have pythoned poorly atleast once.\"\n\nwords = word_tokenize(n_t)\n\nfor st in  words:\n    print(ps.stem(st))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:07.912522Z","iopub.execute_input":"2022-04-28T06:01:07.913002Z","iopub.status.idle":"2022-04-28T06:01:07.923451Z","shell.execute_reply.started":"2022-04-28T06:01:07.912965Z","shell.execute_reply":"2022-04-28T06:01:07.922078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Part of speech tagging**\n\nThis means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:\n\n#### **POS tag list:**\n\n- CC\tcoordinating conjunction\n- CD\tcardinal digit\n- DT\tdeterminer\n- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n- FW\tforeign word\n- IN\tpreposition/subordinating conjunction\n- JJ\tadjective\t'big'\n- JJR\tadjective, comparative\t'bigger'\n- JJS\tadjective, superlative\t'biggest'\n- LS\tlist marker\t1)\n- MD\tmodal\tcould, will\n- NN\tnoun, singular 'desk'\n- NNS\tnoun plural\t'desks'\n- NNP\tproper noun, singular\t'Harrison'\n- NNPS\tproper noun, plural\t'Americans'\n- PDT\tpredeterminer\t'all the kids'\n- POS\tpossessive ending\tparent\\'s\n- PRP\tpersonal pronoun\tI, he, she\n- RB\tadverb\tvery, silently,\n- RBR\tadverb, comparative\tbetter\n- RBS\tadverb, superlative\tbest\n- RP\tparticle\tgive up\n- TO\tto\tgo 'to' the store.\n- UH\tinterjection\terrrrrrrrm\n- VB\tverb, base form\ttake\n- VBD\tverb, past tense\ttook\n- VBG\tverb, gerund/present participle\ttaking\n- VBN\tverb, past participle\ttaken\n- VBP\tverb, sing. present, non-3d\ttake\n- VBZ\tverb, 3rd person sing. present\ttakes\n- WDT\twh-determiner\twhich\n- WP\twh-pronoun\twho, what\n- WRB\twh-abverb\twhere, when","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:08.293946Z","iopub.execute_input":"2022-04-28T06:01:08.294479Z","iopub.status.idle":"2022-04-28T06:01:08.298761Z","shell.execute_reply.started":"2022-04-28T06:01:08.294438Z","shell.execute_reply":"2022-04-28T06:01:08.298045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = \"Engineers, as practitioners of engineering, are professionals who invent, design, analyze, build and test machines, complex systems, structures, gadgets and materials to fulfill functional objectives and requirements while considering the limitations imposed by practicality, regulation, safety and cost.[1][2] The word engineer (Latin ingeniator[3]) is derived from the Latin words ingeniare (to create, generate, contrive, devise and ingenium) (cleverness).[4][5] The foundational qualifications of an engineer typically include a four-year bachelor's degree in an engineering discipline, or in some jurisdictions, a master's degree in an engineering discipline plus four to six years of peer-reviewed professional practice (culminating in a project report or thesis) and passage of engineering board examinations.The work of engineers forms the link between scientific discoveries and their subsequent applications to human and business needs and quality of life.[1]\"\nsample_text = \"Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States, in the 1970s. NLP's creators claim there is a connection between neurological processes (neuro-), language (linguistic) and behavioral patterns learned through experience (programming), and that these can be changed to achieve specific goals in life.[1][2]: 2  Bandler and Grinder also claim that NLP methodology can model the skills of exceptional people, allowing anyone to acquire those skills.[3]: 5–6 [4] They claim as well that, often in a single session, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic illnesses, near-sightedness,[5] allergy, the common cold,[Note 1] and learning disorders.[7][8] NLP has been adopted by some hypnotherapists and also by companies that run seminars marketed as leadership training to businesses and government agencies.[9][10]There is no scientific evidence supporting the claims made by NLP advocates, and it has been discredited as a pseudoscience.[11][12][13] Scientific reviews state that NLP is based on outdated metaphors of how the brain works that are inconsistent with current neurological theory and contain numerous factual errors.[10][14] Reviews also found that all of the supportive research on NLP contained significant methodological flaws and that there were three times as many studies of a much higher quality that failed to reproduce the extraordinary claims made by Bandler, Grinder, and other NLP practitioners.[12][13]\"","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:08.671089Z","iopub.execute_input":"2022-04-28T06:01:08.672676Z","iopub.status.idle":"2022-04-28T06:01:08.67892Z","shell.execute_reply.started":"2022-04-28T06:01:08.672623Z","shell.execute_reply":"2022-04-28T06:01:08.677774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_text = state_union.raw('train_text.txt')\n# sample_text = state_union.raw('sample_text.txt')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:08.971635Z","iopub.execute_input":"2022-04-28T06:01:08.971887Z","iopub.status.idle":"2022-04-28T06:01:08.97584Z","shell.execute_reply.started":"2022-04-28T06:01:08.97186Z","shell.execute_reply":"2022-04-28T06:01:08.975032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\ntokenized = custom_sent_tokenizer.tokenize(sample_text)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:09.413972Z","iopub.execute_input":"2022-04-28T06:01:09.414776Z","iopub.status.idle":"2022-04-28T06:01:09.421814Z","shell.execute_reply.started":"2022-04-28T06:01:09.414733Z","shell.execute_reply":"2022-04-28T06:01:09.4211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_content():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            print(tagged)\n\n    except Exception as e:\n        print(str(e))\n\n\nprocess_content()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:09.73644Z","iopub.execute_input":"2022-04-28T06:01:09.737015Z","iopub.status.idle":"2022-04-28T06:01:09.770143Z","shell.execute_reply.started":"2022-04-28T06:01:09.736974Z","shell.execute_reply":"2022-04-28T06:01:09.769401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Chunking**\n\nNow that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.\n\nIn order to chunk, we combine the part of speech tags with regular expressions. Mainly from regular expressions, we are going to utilize the following:","metadata":{}},{"cell_type":"markdown","source":"## **Regular expression**\n\nHere is a quick cheat sheet for various rules in regular expressions:\n\n### ***Identifiers:***\n\n- \\d = any number\n- \\D = anything but a number\n- \\s = space\n- \\S = anything but a space\n- \\w = any letter\n- \\W = anything but a letter\n- . = any character, except for a new line\n- \\b = space around whole words\n- \\. = period. must use backslash, because . normally means any character.\n\n### ***Modifiers:***\n\n- {1,3} = for digits, u expect 1-3 counts of digits, or \"places\"\n- + = match 1 or more\n- ? = match 0 or 1 repetitions.\n- * = match 0 or MORE repetitions\n- ^ = matches start of a string\n- | = matches either/or. Example x|y = will match either x or y\n- [] = range, or \"variance\"\n- {x} = expect to see this amount of the preceding code.\n- {x,y} = expect to see this x-y amounts of the precedng code\n\n### ***White Space Charts:***\n\n- \\n = new line\n- \\s = space\n- \\t = tab\n- \\e = escape\n- \\f = form feed\n- \\r = carriage return\n\n***Characters to REMEMBER TO ESCAPE IF USED!***\n\n- . + * ? [ ] $ ^ ( ) { } | \\\n\n### ***Brackets:***\n\n- [] = quant[ia]tative = will find either quantitative, or quantatative.\n- [a-z] = return any lowercase letter a-z\n- [1-5a-qA-Z] = return all numbers 1-5, lowercase letters a-q and uppercase A-Z\n- \"\"\"<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n- <VB.?>* = \"0 or more of any tense of verb,\" followed by:\n- <NNP>+ = \"One or more proper nouns,\" followed by\n- <NN>? = \"zero or one singular noun.\"\n- \"\"\"","metadata":{}},{"cell_type":"code","source":"def process_content():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            #print(tagged)\n            \n            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" \n            \n            chunkParser = nltk.RegexpParser(chunkGram)\n            chunked = chunkParser.parse(tagged)\n            print(chunked)\n            \n\n    except Exception as e:\n        print(str(e))\n\n\nprocess_content()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:10.058904Z","iopub.execute_input":"2022-04-28T06:01:10.05937Z","iopub.status.idle":"2022-04-28T06:01:10.093159Z","shell.execute_reply.started":"2022-04-28T06:01:10.059332Z","shell.execute_reply":"2022-04-28T06:01:10.092436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import tree\nimport os\nfrom IPython.display import Image, display\nfrom nltk.draw import TreeWidget\nfrom nltk.draw.util import CanvasFrame\n%matplotlib inline\n\ndef process_content():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            #print(tagged)\n            \n            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" \n            \n            chunkParser = nltk.RegexpParser(chunkGram)\n            chunked = chunkParser.parse(tagged)\n            #print(chunked)\n            chunked.draw()\n\n    except Exception as e:\n        print(str(e))\n\n\nprocess_content()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:10.413925Z","iopub.execute_input":"2022-04-28T06:01:10.416614Z","iopub.status.idle":"2022-04-28T06:01:10.433158Z","shell.execute_reply.started":"2022-04-28T06:01:10.416575Z","shell.execute_reply":"2022-04-28T06:01:10.432341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This kind of output is shows! but jupyter got didnot display\n\n<img src='https://naadispeaks.files.wordpress.com/2017/02/cap_4.gif'>","metadata":{}},{"cell_type":"markdown","source":"# **6. Chinking**\nChinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink.","metadata":{}},{"cell_type":"code","source":"def process_content():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            #print(tagged)\n            \n            chunkGram = r\"\"\"Chunk: {<.*>+}\n                                    }<VB.?|IN|DT|TO>+{\"\"\"\n            \n            chunkParser = nltk.RegexpParser(chunkGram)\n            chunked = chunkParser.parse(tagged)\n            print(chunked)\n            #chunked.draw()\n\n    except Exception as e:\n        print(str(e))\n\n\nprocess_content()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:10.829965Z","iopub.execute_input":"2022-04-28T06:01:10.830698Z","iopub.status.idle":"2022-04-28T06:01:10.866707Z","shell.execute_reply.started":"2022-04-28T06:01:10.830662Z","shell.execute_reply":"2022-04-28T06:01:10.865988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **7. Named Entity Recognition:**\n\nOne of the most major forms of chunking in natural language processing is called \"Named Entity Recognition.\" The idea is to have the machine immediately be able to pull out \"entities\" like people, places, things, locations, monetary figures, and more.\n\n\n#### **Example:**\n\nNE Type and Examples\n\n1. ORGANIZATION - Georgia-Pacific Corp., WHO\n2. PERSON - Eddy Bonte, President Obama\n3. LOCATION - Murray River, Mount Everest\n4. DATE - June, 2008-06-29\n5. TIME - two fifty a m, 1:30 p.m.\n6. MONEY - 175 million Canadian Dollars, GBP 10.40\n7. PERCENT - twenty pct, 18.75 %\n8. FACILITY - Washington Monument, Stonehenge\n9. GPE - South East Asia, Midlothian","metadata":{}},{"cell_type":"code","source":"def process_content():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            \n            print('Without Binary NER')\n            print('-----------------------------------------------------------------------------------')\n            # Try without apply binary=True\n            namedEntity = nltk.ne_chunk(tagged)\n            #namedEntity.draw()\n            print(namedEntity)\n            print('----------------------------------------------------------------------------------')\n            \n            print('With Binary NER')\n            print('-----------------------------------------------------------------------------------')\n            # try with apply binary= True\n            namedEntity_Bin = nltk.ne_chunk(tagged,binary=True)\n            print(namedEntity_Bin)\n            \n    except Exception as e:\n        print(str(e))\n\nprint(\"Go and watch the output like name entity recognition e.g., organization(NNP/NN),....\")\nprint('')\nprocess_content()\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:11.059214Z","iopub.execute_input":"2022-04-28T06:01:11.059958Z","iopub.status.idle":"2022-04-28T06:01:11.335147Z","shell.execute_reply.started":"2022-04-28T06:01:11.059916Z","shell.execute_reply":"2022-04-28T06:01:11.334201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***8. Lemmatization***\nA very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n\nSo, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n\n","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:11.376181Z","iopub.execute_input":"2022-04-28T06:01:11.376609Z","iopub.status.idle":"2022-04-28T06:01:11.387228Z","shell.execute_reply.started":"2022-04-28T06:01:11.376571Z","shell.execute_reply":"2022-04-28T06:01:11.386509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lemmatizer.lemmatize(\"cats\"))\nprint(lemmatizer.lemmatize(\"cacti\"))\nprint(lemmatizer.lemmatize(\"dogs\"))\nprint(lemmatizer.lemmatize(\"corpora\"))\nprint(lemmatizer.lemmatize(\"geese\"))\nprint(lemmatizer.lemmatize(\"rocks\"))\nprint(lemmatizer.lemmatize(\"python\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:11.893667Z","iopub.execute_input":"2022-04-28T06:01:11.894232Z","iopub.status.idle":"2022-04-28T06:01:11.903006Z","shell.execute_reply.started":"2022-04-28T06:01:11.894185Z","shell.execute_reply":"2022-04-28T06:01:11.902262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# different approach based on noun,adjective,verb\n\n#general\nprint('General_lemma:',lemmatizer.lemmatize(\"worst\"))\n\n# Adjective\nprint('Adjective_change_lemma -> worst to :',lemmatizer.lemmatize(\"worst\", pos='a'))\nprint('Adjective_change_lemma -> better to:',lemmatizer.lemmatize(\"better\",pos='a'))\n\n#verb\nprint('lemma_v:',lemmatizer.lemmatize(\"run\"))\nprint('lemma_verb_initiated:',lemmatizer.lemmatize(\"run\",pos='v'))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:12.176042Z","iopub.execute_input":"2022-04-28T06:01:12.17631Z","iopub.status.idle":"2022-04-28T06:01:12.185029Z","shell.execute_reply.started":"2022-04-28T06:01:12.17628Z","shell.execute_reply":"2022-04-28T06:01:12.18407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **9. Corpora**\n\nCorpus - Simply said that corpus means inbuilt data of text\n\nAlmost all of the files in the NLTK corpus follow the same rules for accessing them by using the NLTK module, but nothing is magical about them. These files are plain text files for the most part, some are XML and some are other formats, but they are all accessible by you manually, or via the module and Python","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import gutenberg\nfrom nltk.tokenize import sent_tokenize\n\nprint(nltk.__file__)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:12.496243Z","iopub.execute_input":"2022-04-28T06:01:12.496914Z","iopub.status.idle":"2022-04-28T06:01:12.506974Z","shell.execute_reply.started":"2022-04-28T06:01:12.496876Z","shell.execute_reply":"2022-04-28T06:01:12.506055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = gutenberg.raw(\"bible-kjv.txt\")\ntok = sent_tokenize(sample)\nprint(tok[5:15])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:12.832223Z","iopub.execute_input":"2022-04-28T06:01:12.832826Z","iopub.status.idle":"2022-04-28T06:01:14.366173Z","shell.execute_reply.started":"2022-04-28T06:01:12.832784Z","shell.execute_reply":"2022-04-28T06:01:14.365401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **10. WordNet:**\n\nWordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n\nYou can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:14.367871Z","iopub.execute_input":"2022-04-28T06:01:14.368288Z","iopub.status.idle":"2022-04-28T06:01:14.372484Z","shell.execute_reply.started":"2022-04-28T06:01:14.368248Z","shell.execute_reply":"2022-04-28T06:01:14.371764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"syns = wordnet.synsets(\"program\")\nprint('All kind of syns:',syns)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:14.373834Z","iopub.execute_input":"2022-04-28T06:01:14.374284Z","iopub.status.idle":"2022-04-28T06:01:14.386406Z","shell.execute_reply.started":"2022-04-28T06:01:14.374241Z","shell.execute_reply":"2022-04-28T06:01:14.385603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#synset\nprint('Only first index of synset:',syns[0])\nprint('------------------------------------')\n\n# lemmas\nprint('Apply lemmas in first index of synset:',syns[0].lemmas())\nprint('------------------------------------')\n# just the one word\nprint('Name of synset:',syns[0].lemmas()[0].name())\nprint('------------------------------------')\n# definition of particular word (eg.. plan)\nprint('Definition of synsets:',syns[0].definition())\nprint('------------------------------------')\n#examples\nprint('Example of synsets:',syns[0].examples())\nprint('------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:14.388102Z","iopub.execute_input":"2022-04-28T06:01:14.388318Z","iopub.status.idle":"2022-04-28T06:01:14.399246Z","shell.execute_reply.started":"2022-04-28T06:01:14.388294Z","shell.execute_reply":"2022-04-28T06:01:14.398501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Synonyms and Antonyms\n\nsynonyms = []\nantonyms = []\n\nfor syn in wordnet.synsets(\"unknown\"):\n    for l in syn.lemmas(): #lemmas means synonyms of word\n#         print(\"l:\",l) # meaning of words in nltk library is lot\n#         print('---------------------------------------------')\n        synonyms.append(l.name())\n        \n        if l.antonyms(): #opposite of meaning\n            antonyms.append(l.antonyms()[0].name())\n\nprint('Synonyms:',set(synonyms))\nprint('----------------------------------------------------')\nprint('antonyms:',set(antonyms))\nprint('----------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:14.453551Z","iopub.execute_input":"2022-04-28T06:01:14.453886Z","iopub.status.idle":"2022-04-28T06:01:14.463073Z","shell.execute_reply.started":"2022-04-28T06:01:14.453854Z","shell.execute_reply":"2022-04-28T06:01:14.462232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **11. Similarity identification**","metadata":{}},{"cell_type":"code","source":"#Similarity identification\n\n#\"Wu and Palmer method: WordNet::Similarity::wup - Perl module for computing semantic relatedness of word senses using the edge counting method of the of Wu & Palmer (1994)\"\n#Resnik (1999) revises the Wu & Palmer (1994) method of measuring semantic relatedness. Resnik uses use an edge distance method by taking into account the most specific node subsuming the two concepts. Here we have implemented the original Wu & Palmer method, which uses node-counting.\n# If you want more details refer this one: \"https://metacpan.org/release/TPEDERSE/WordNet-Similarity-1.03/view/lib/WordNet/Similarity/wup.pm'\n\n\nword1 = wordnet.synset(\"computer.n.01\") # n means Noun\nword2 = wordnet.synset(\"system.n.01\")\n\nword3 = wordnet.synset(\"cycle.n.01\") # n means Noun\nword4 = wordnet.synset(\"bike.n.01\")\n\nword5 = wordnet.synset(\"ship.n.01\") # n means Noun\nword6 = wordnet.synset(\"person.n.01\")\n\nword7 = wordnet.synset(\"ship.n.01\") # n means Noun\nword8 = wordnet.synset(\"boat.n.01\")\n\n\n\nprint('Similarity_Score of first_second words: ',word1.wup_similarity(word2))\nprint('Similarity_Score of Third_fourth words: ',word3.wup_similarity(word4))\nprint('Similarity_Score of fifth_sixth words: ',word5.wup_similarity(word6))\nprint('Similarity_Score of seventh_eigth words: ',word7.wup_similarity(word8))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:14.833487Z","iopub.execute_input":"2022-04-28T06:01:14.834552Z","iopub.status.idle":"2022-04-28T06:01:14.84866Z","shell.execute_reply.started":"2022-04-28T06:01:14.834499Z","shell.execute_reply":"2022-04-28T06:01:14.847879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Apply above knowledge in simple nltk corpus data***\n\n# **12. Text_Classification**\n\n- Text classification can be pretty broad. Maybe we're trying to classify text as about politics or the military,Culture, stock etc... \n- Maybe we're trying to classify it by the gender of the author who wrote it. In our case, we're going to try to create a sentiment analysis algorithm.\n\n**I thought NLTK corpus is awesome! Because all custom data and easy to work**\n\n1. Taken the Movie review data\n2. Identify the common negative words (convert word to features)\n3. Apply naive bayes algo (How to works?)","metadata":{}},{"cell_type":"code","source":"# Now I am trying to movie review(sentimental analysis)\nimport nltk\nimport random\nfrom nltk.corpus import movie_reviews","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:15.141241Z","iopub.execute_input":"2022-04-28T06:01:15.141517Z","iopub.status.idle":"2022-04-28T06:01:15.148361Z","shell.execute_reply.started":"2022-04-28T06:01:15.141485Z","shell.execute_reply":"2022-04-28T06:01:15.147516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# documents_review = []\n\n# for category in movie_reviews.categories():\n#     for fileid in movie_reviews.categories():\n#         documents_review.append(list(movie_reviews.words(fileid)),category)\n\n\ndocuments = [(list(movie_reviews.words(fileid)),category)\n            for category in movie_reviews.categories()\n            for fileid in movie_reviews.fileids(category)]\n\nrandom.shuffle(documents)\nprint(documents[1])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:15.466807Z","iopub.execute_input":"2022-04-28T06:01:15.467586Z","iopub.status.idle":"2022-04-28T06:01:18.915856Z","shell.execute_reply.started":"2022-04-28T06:01:15.467532Z","shell.execute_reply":"2022-04-28T06:01:18.915045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_words = []\nfor w in movie_reviews.words():\n    all_words.append(w.lower())\n\n# Print Most common words in the movie reviews (15) - How many time used\nall_words = nltk.FreqDist(all_words)\nprint(all_words.most_common(15))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:18.917468Z","iopub.execute_input":"2022-04-28T06:01:18.918256Z","iopub.status.idle":"2022-04-28T06:01:22.206922Z","shell.execute_reply.started":"2022-04-28T06:01:18.918216Z","shell.execute_reply":"2022-04-28T06:01:22.206139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('How many time word \"happy\" comes in movie reviews: ',all_words[\"happy\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:22.208069Z","iopub.execute_input":"2022-04-28T06:01:22.208414Z","iopub.status.idle":"2022-04-28T06:01:22.214461Z","shell.execute_reply.started":"2022-04-28T06:01:22.208356Z","shell.execute_reply":"2022-04-28T06:01:22.213719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **12.1. Converting Words to features**","metadata":{}},{"cell_type":"code","source":"# Already we know this below code only change is visualize the 3000 common words\ndocuments = [(list(movie_reviews.words(fileid)), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category)]\n\nrandom.shuffle(documents)\n\nall_words = []\n\nfor w in movie_reviews.words():\n    all_words.append(w.lower())\n\nall_words = nltk.FreqDist(all_words)\n\nword_features = list(all_words.keys())[:3000]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:22.216565Z","iopub.execute_input":"2022-04-28T06:01:22.217211Z","iopub.status.idle":"2022-04-28T06:01:28.805489Z","shell.execute_reply.started":"2022-04-28T06:01:22.217173Z","shell.execute_reply":"2022-04-28T06:01:28.80468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the which one commonly positive and negative\n\ndef find_features(document):\n    words = set(document)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words)\n    return features\n\nprint((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n\nfeature_sets = [(find_features(rev), category) for (rev,category) in documents ]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:28.806823Z","iopub.execute_input":"2022-04-28T06:01:28.807058Z","iopub.status.idle":"2022-04-28T06:01:29.861854Z","shell.execute_reply.started":"2022-04-28T06:01:28.807023Z","shell.execute_reply":"2022-04-28T06:01:29.860974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **12.2. Apply Algorithm and predict output**","metadata":{}},{"cell_type":"code","source":"#Feature separation(dont said category of data and predict the algo)\n\ntrain_set = feature_sets[:1900]\ntest_set = feature_sets[1900:]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:29.863314Z","iopub.execute_input":"2022-04-28T06:01:29.863607Z","iopub.status.idle":"2022-04-28T06:01:29.932178Z","shell.execute_reply.started":"2022-04-28T06:01:29.863568Z","shell.execute_reply":"2022-04-28T06:01:29.931157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Naive bayes algo best \n\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nprint(\"NaiveBayes Algorithm accuracy % :\", (nltk.classify.accuracy(classifier,test_set))*100)\nprint('----------------------------------------------------------')\nprint('')\nclassifier.show_most_informative_features(15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:29.933882Z","iopub.execute_input":"2022-04-28T06:01:29.934228Z","iopub.status.idle":"2022-04-28T06:01:38.08216Z","shell.execute_reply.started":"2022-04-28T06:01:29.934185Z","shell.execute_reply":"2022-04-28T06:01:38.081436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **12.3. Save_Load the Pickle file**","metadata":{}},{"cell_type":"code","source":"import pickle\n#save the model\n\nsave_class = open(\"naivebayes.pickle\",\"wb\")\npickle.dump(classifier,save_class)\nsave_class.close()\n\nclassifier_f = open(\"./naivebayes.pickle\", \"rb\")\nclassifier = pickle.load(classifier_f)\nclassifier_f.close()\n\nprint(\"NaiveBayes Algorithm accuracy % :\", (nltk.classify.accuracy(classifier,test_set))*100)\nprint('----------------------------------------------------------')\nprint('')\nclassifier.show_most_informative_features(30)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:38.083537Z","iopub.execute_input":"2022-04-28T06:01:38.083885Z","iopub.status.idle":"2022-04-28T06:01:39.750572Z","shell.execute_reply.started":"2022-04-28T06:01:38.083846Z","shell.execute_reply":"2022-04-28T06:01:39.74977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **13. Text Representation**\n\n Text Representation is a way to convert text in its natural form to vector form  – Machines like it and understand it in this way only! The numbers/vectors form. This is the second step in an NLP pipeline after Text Pre-processing. Let’s get started with a sample corpus, pre-process\n\n<img src= 'https://github.com/practical-nlp/practical-nlp-figures/raw/master/figures/3-1.png'>","metadata":{}},{"cell_type":"markdown","source":"## **13.1. One Hot Encoding of text (First manual function and using simple scikitlearn)**","metadata":{}},{"cell_type":"code","source":"# take input text doc and remove puncuation,dot, cvt lower all input data\n\ndocuments = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\nprocessed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\nprocessed_docs","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.751868Z","iopub.execute_input":"2022-04-28T06:01:39.752202Z","iopub.status.idle":"2022-04-28T06:01:39.797203Z","shell.execute_reply.started":"2022-04-28T06:01:39.752163Z","shell.execute_reply":"2022-04-28T06:01:39.796454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the vocabulary \nvocab = {}\ncount = 0\nfor doc in processed_docs:\n    for word in doc.split():\n        if word not in vocab:\n            count = count +1\n            vocab[word] = count\nprint(vocab)\n\n#Get one hot representation for any string based on this vocabulary. \n#If the word exists in the vocabulary, its representation is returned. \n#If not, a list of zeroes is returned for that word. \ndef get_onehot_vector(somestring):\n    onehot_encoded = []\n    for word in somestring.split():\n        temp = [0]*len(vocab)\n        if word in vocab:\n            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n        onehot_encoded.append(temp)\n    return onehot_encoded\n\n\nprint('-------------------------------------------------')\nprint(processed_docs[1])\nprint('-------------------------------------------------')\nprint(get_onehot_vector(processed_docs[1])) #one hot representation for a text from our corpus.\nprint('---------------------------------------------------')\n#try your own words\nget_onehot_vector(\"man and man are good\") ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:39.800764Z","iopub.execute_input":"2022-04-28T06:01:39.801288Z","iopub.status.idle":"2022-04-28T06:01:39.817091Z","shell.execute_reply.started":"2022-04-28T06:01:39.801247Z","shell.execute_reply":"2022-04-28T06:01:39.816276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Scikit-learn pack - OHE**","metadata":{}},{"cell_type":"code","source":"S1 = 'dog bites man'\nS2 = 'man bites dog'\nS3 = 'dog eats meat'\nS4 = 'man eats food'\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ndata = [S1.split(), S2.split(), S3.split(), S4.split()]\nvalues = data[0]+data[1]+data[2]+data[3]\nprint(\"The data: \",values)\nprint('-------------------------------------------------')\n\n#Label Encoding\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(values)\nprint(\"Label Encoded:\",integer_encoded)\nprint('----------------------------------------------------')\n\n#One-Hot Encoding\nonehot_encoder = OneHotEncoder()\nonehot_encoded = onehot_encoder.fit_transform(data).toarray()\nprint(\"Onehot Encoded Matrix:\\n\",onehot_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.818709Z","iopub.execute_input":"2022-04-28T06:01:39.819194Z","iopub.status.idle":"2022-04-28T06:01:39.832279Z","shell.execute_reply.started":"2022-04-28T06:01:39.819156Z","shell.execute_reply":"2022-04-28T06:01:39.831314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.2. Bag of words**\n\nA bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n\n\n<img src='https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Feditor.analyticsvidhya.com%2Fuploads%2F34521bagofwords.jpg&f=1&nofb=1'>","metadata":{}},{"cell_type":"code","source":"#Now, let's do the main task of finding bag of words representation. We will use CountVectorizer from sklearn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#look at the documents list\nprint(\"Our corpus: \", processed_docs)\n\ncount_vect = CountVectorizer()\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\n\n#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n\n#see the BOW rep for first 2 documents\nprint(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\nprint(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and man are friends\"])\nprint(\"Bow representation for 'dog bites are friends':\", temp.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.83354Z","iopub.execute_input":"2022-04-28T06:01:39.834339Z","iopub.status.idle":"2022-04-28T06:01:39.846405Z","shell.execute_reply.started":"2022-04-28T06:01:39.834303Z","shell.execute_reply":"2022-04-28T06:01:39.845586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In the above code, we represented the text considering the frequency of words into account. However, sometimes, we don't care about frequency much, but only want to know whether a word appeared in a text or not. That is, each document is represented as a vector of 0s and 1s. We will use the option binary=True in CountVectorizer for this purpose.\n#BoW with binary vectors\ncount_vect = CountVectorizer(binary=True)\ncount_vect.fit(processed_docs)\ntemp = count_vect.transform([\"dog man are friends\"])\nprint(\"Bow representation for 'dog bites man are friends':\", temp.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.847945Z","iopub.execute_input":"2022-04-28T06:01:39.849593Z","iopub.status.idle":"2022-04-28T06:01:39.857727Z","shell.execute_reply.started":"2022-04-28T06:01:39.84955Z","shell.execute_reply":"2022-04-28T06:01:39.856841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.3 Bag of N-Gram**\n\nOne hot encoding, BoW and TF-IDF treat words as independent units. There is no notion of phrases or word ordering. Bag of Ngrams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n countigous words/tokens. This can help us capture some context, which earlier approaches could not do\n\nExample:\n\nSo, the respective vectors for these sentences are:\n\n“This is a good job. I will not miss it for anything”=[1,1,1,1,0]\n\n”This is not good at all”=[1,0,0,1,1]\n\nCan you guess what is the problem here? Sentence 2 is a negative sentence and sentence 1 is a positive sentence. Does this reflect in any way in the vectors above? Not at all. So how can we solve this problem? Here come the N-grams to our rescue.\n\nAn N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”.\n\n<img src = 'https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fconglang.github.io%2Fimg%2Fml_feature_extraction_ngram.png&f=1&nofb=1'>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n#Ngram vectorization example with count vectorizer and uni, bi, trigrams\ncount_vect = CountVectorizer(ngram_range=(1,3))\n\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\n\n#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n\n#see the BOW rep for first 2 documents\nprint(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\nprint(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and dog are friends\"])\n\nprint(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\n\n#Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!!","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.859107Z","iopub.execute_input":"2022-04-28T06:01:39.859529Z","iopub.status.idle":"2022-04-28T06:01:39.872345Z","shell.execute_reply.started":"2022-04-28T06:01:39.859495Z","shell.execute_reply":"2022-04-28T06:01:39.871607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.4. TF-IDF**\n\n***In all the other approaches we saw so far, all the words in the text are treated equally important. There is no notion of some words in the document being more important than others. TF-IDF addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query.***\n\nTF-IDF for a word in a document is calculated by multiplying two different metrics:\n\n**The term frequency (TF)** of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are other ways to adjust the frequency. For example, by dividing the raw count of instances of a word by either length of the document, or by the raw frequency of the most frequent word in the document. The formula to calculate Term-Frequency is\n\n**TF(i,j)=n(i,j)/Σ n(i,j)**\n\nWhere,\n\nn(i,j )= number of times nth word  occurred in a document\nΣn(i,j) = total number of words in a document. \n\n**The inverse document frequency(IDF)** of the word across a set of documents. This suggests how common or rare a word is in the entire document set. The closer it is to 0, the more common is the word. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\n<img align = 'center' src=\"https://mungingdata.files.wordpress.com/2017/11/equation.png?w=430&h=336\">","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = [\"vk hit bat.\", \"vk ran street.\", \"Dog eats carrot.\", \"vk dog love very much in vk.\"]\nprocessed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\nprint(\"processed_docs:\",processed_docs)\nprint('-----------------------------------------------------')\nprint('')\n#Initialize the TFIDF\ntfidf = TfidfVectorizer()\nbow_rep_tfidf = tfidf.fit_transform(processed_docs)\n\n#IDF for all words in the vocabulary\nprint(\"IDF for all words in the vocabulary\",tfidf.idf_)\nprint(\"-\"*10)\n#All words in the vocabulary.\nprint(\"All words in the vocabulary\",tfidf.get_feature_names())\nprint(\"-\"*10)\n\n#TFIDF representation for all documents in our corpus \nprint(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \nprint(\"-\"*10)\n\ntemp = tfidf.transform([\"vk and dog are friends\"])\nprint(\"Tfidf representation for 'dog and vk are friends':\\n\", temp.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:39.873994Z","iopub.execute_input":"2022-04-28T06:01:39.874511Z","iopub.status.idle":"2022-04-28T06:01:39.893191Z","shell.execute_reply.started":"2022-04-28T06:01:39.874476Z","shell.execute_reply":"2022-04-28T06:01:39.8925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **13.5. Word Embedding**\n\nWord embeddings are an approach to representing text in NLP. In this notebook we will demonstrate how to train embeddings using Genism. \n\none of the great reference: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71","metadata":{}},{"cell_type":"code","source":"!pip install gensim==3.6.0\n!pip install requests==2.23.0\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:39.894188Z","iopub.execute_input":"2022-04-28T06:01:39.894394Z","iopub.status.idle":"2022-04-28T06:01:56.82552Z","shell.execute_reply.started":"2022-04-28T06:01:39.894356Z","shell.execute_reply":"2022-04-28T06:01:56.824611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.827721Z","iopub.execute_input":"2022-04-28T06:01:56.828639Z","iopub.status.idle":"2022-04-28T06:01:56.834414Z","shell.execute_reply.started":"2022-04-28T06:01:56.828591Z","shell.execute_reply":"2022-04-28T06:01:56.83356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define training data\n#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n#Every list contains lists of tokens of that document.\ncorpus = [['vk','birds','dog','bites','man'], [\"man\", \"bites\" ,\"dog\",\"love\"],[\"dog\",\"eats\",\"meat\",\"cat\"],[\"man\", \"eats\",\"food\"]]\n\n#Training the model\nmodel_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\nmodel_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for training \n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.836369Z","iopub.execute_input":"2022-04-28T06:01:56.837006Z","iopub.status.idle":"2022-04-28T06:01:56.890812Z","shell.execute_reply.started":"2022-04-28T06:01:56.836958Z","shell.execute_reply":"2022-04-28T06:01:56.890024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***There are two main architectures which yield the success of word2vec. The skip-gram and CBOW architectures***\n\n## **13.5.1 CBOW (Continuous BOW)**\n\nThis architecture is very similar to a feed forward neural network. This model architecture essentially tries to predict a target word from a list of context words.\n\n<img src='https://miro.medium.com/max/392/1*_8Ul4ICaCtmZWPrWqH32Ow.png'>","metadata":{}},{"cell_type":"code","source":"#Summarize the loaded model\nprint(model_cbow)\n\n#Summarize vocabulary\nwords = list(model_cbow.wv.vocab)\nprint(words)\n\n#Acess vector for one word\nprint(model_cbow['dog'])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.892569Z","iopub.execute_input":"2022-04-28T06:01:56.892764Z","iopub.status.idle":"2022-04-28T06:01:56.900063Z","shell.execute_reply.started":"2022-04-28T06:01:56.892741Z","shell.execute_reply":"2022-04-28T06:01:56.899102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compute similarity \nprint(\"Similarity between eats and bites,vk:\",model_cbow.similarity('eats',\"dog\"))\nprint(\"Similarity between eats and man, dog:\",model_cbow.similarity('eats','vk'))\nprint('------------------------------------')\nprint('')\n#Most similarity\nprint(model_cbow.most_similar('meat'))\nprint('-----------------------------------')\nprint('')\n\n# save model\nmodel_cbow.save('model_cbow.bin')\nprint('---------------------------------------')\nprint('')\n# load model\nnew_model_cbow = Word2Vec.load('model_cbow.bin')\nprint(new_model_cbow)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.901856Z","iopub.execute_input":"2022-04-28T06:01:56.902428Z","iopub.status.idle":"2022-04-28T06:01:56.921113Z","shell.execute_reply.started":"2022-04-28T06:01:56.90237Z","shell.execute_reply":"2022-04-28T06:01:56.920198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.5.2.Skipgram**\n\nIn skipgram, the task is to predict the context words from the center word.\n\n'This model essentially tries to learn and predict the context words around the specified input word'\n\n\n<img src='https://miro.medium.com/max/700/1*M6UxaLSbNMeoDFWRN_kPeQ.png'>","metadata":{}},{"cell_type":"code","source":"#Summarize the loaded model\nprint(model_skipgram)\nprint('-------------------------------------------------')\nprint('')\n#Summarize vocabulary\nwords = list(model_skipgram.wv.vocab)\nprint(words)\nprint('-------------------------------------------------')\nprint('')\n\n#Acess vector for one word\nprint(model_skipgram['dog'])\nprint('-------------------------------------------------')\nprint('')\n\n#Compute similarity \nprint(\"Similarity between eats and bites:\",model_skipgram.similarity('eats', 'bites'))\nprint(\"Similarity between eats and man:\",model_skipgram.similarity('eats', 'man'))\nprint('-------------------------------------------------')\nprint('')\n#Most similarity\nprint(model_skipgram.most_similar('meat'))\nprint('-------------------------------------------------')\nprint('')\n\n# save model\nmodel_skipgram.save('model_skipgram.bin')\n\n# load model\nnew_model_skipgram = Word2Vec.load('model_skipgram.bin')\nprint(new_model_skipgram)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.922735Z","iopub.execute_input":"2022-04-28T06:01:56.923445Z","iopub.status.idle":"2022-04-28T06:01:56.946514Z","shell.execute_reply.started":"2022-04-28T06:01:56.923405Z","shell.execute_reply":"2022-04-28T06:01:56.945793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.5.3 DOC2VEC**\n\n**Everyone got onedoubt in doc2vec and word2vec! same thing happen me! this pic easily know what happen in two process i think**\n\nDoc2Vec is another widely used technique that creates an embedding of a document irrespective to its length. While Word2Vec computes a feature vector for every word in the corpus, Doc2Vec computes a feature vector for every document in the corpus\n\n<img src='https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs40537-018-0139-2/MediaObjects/40537_2018_139_Fig8_HTML.png'>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\nfrom pprint import pprint\nimport nltk\nnltk.download('punkt')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:56.947789Z","iopub.execute_input":"2022-04-28T06:01:56.948143Z","iopub.status.idle":"2022-04-28T06:01:56.958796Z","shell.execute_reply.started":"2022-04-28T06:01:56.948102Z","shell.execute_reply":"2022-04-28T06:01:56.957751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [\"vk love nature\",\n        \"vk love dogs\",\n        \"dog eats carrot\",\n        \"vk eats food\"]\ntagged_data = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(data)]\ntagged_data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:56.960679Z","iopub.execute_input":"2022-04-28T06:01:56.961197Z","iopub.status.idle":"2022-04-28T06:01:56.970311Z","shell.execute_reply.started":"2022-04-28T06:01:56.96116Z","shell.execute_reply":"2022-04-28T06:01:56.96944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dbow\nmodel_dbow = Doc2Vec(tagged_data,vector_size=20, min_count=1, epochs=2,dm=0)\nprint(model_dbow.infer_vector(['vk','eats','meat']))#feature vector of man eats food\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.972202Z","iopub.execute_input":"2022-04-28T06:01:56.972957Z","iopub.status.idle":"2022-04-28T06:01:56.996105Z","shell.execute_reply.started":"2022-04-28T06:01:56.972783Z","shell.execute_reply":"2022-04-28T06:01:56.995353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dbow.wv.most_similar(\"vk\",topn=5)#top 5 most simlar words.","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:56.997453Z","iopub.execute_input":"2022-04-28T06:01:56.997899Z","iopub.status.idle":"2022-04-28T06:01:57.005856Z","shell.execute_reply.started":"2022-04-28T06:01:56.997859Z","shell.execute_reply":"2022-04-28T06:01:57.004903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dbow.wv.n_similarity([\"dog\"],[\"vk\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:57.007773Z","iopub.execute_input":"2022-04-28T06:01:57.008443Z","iopub.status.idle":"2022-04-28T06:01:57.015591Z","shell.execute_reply.started":"2022-04-28T06:01:57.008371Z","shell.execute_reply":"2022-04-28T06:01:57.01464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dm\nmodel_dm = Doc2Vec(tagged_data, min_count=1, vector_size=20, epochs=2,dm=1)\n\nprint(\"Inference Vector of vk eats food\\n \",model_dm.infer_vector(['vk','eats','food']))\n\nprint(\"Most similar words to vk in our corpus\\n\",model_dm.wv.most_similar(\"vk\",topn=5))\nprint(\"Similarity between vk and dog: \",model_dm.wv.n_similarity([\"dog\"],[\"vk\"]))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:01:57.017304Z","iopub.execute_input":"2022-04-28T06:01:57.018075Z","iopub.status.idle":"2022-04-28T06:01:57.042428Z","shell.execute_reply.started":"2022-04-28T06:01:57.017916Z","shell.execute_reply":"2022-04-28T06:01:57.040657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.5.4 T-SNE visualize embedding**\n\nT-SNE algorithm to visualize embeddings. This is a nice cheap technique for understanding the nature of your embeddings.\n\n\n### ***I am taking competition data and visualize***\n\nReference: \n1. https://www.kaggle.com/code/colinmorris/visualizing-embeddings-with-t-sne/notebook\n\n2. https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne/notebook","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport re\nimport nltk\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import zipfile\n# with zipfile.ZipFile('../input/quora-question-pairs/train.csv.zip', 'r') as zip_ref:\n#     zip_ref.extractall()\n\n\n# data = pd.read_csv('./train.csv').sample(50000, random_state=23)\ndata = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:57.048112Z","iopub.execute_input":"2022-04-28T06:01:57.048341Z","iopub.status.idle":"2022-04-28T06:01:57.118195Z","shell.execute_reply.started":"2022-04-28T06:01:57.048315Z","shell.execute_reply":"2022-04-28T06:01:57.117422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ndef clean_dataframe(data):\n    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n    data = data.dropna(how=\"any\")\n    \n    for col in ['context', 'target']:\n        data[col] = data[col].apply(clean_sentence)\n    \n    return data\n\ndata = clean_dataframe(data)\ndata.head(5)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:01:57.11956Z","iopub.execute_input":"2022-04-28T06:01:57.119887Z","iopub.status.idle":"2022-04-28T06:02:07.224968Z","shell.execute_reply.started":"2022-04-28T06:01:57.119847Z","shell.execute_reply":"2022-04-28T06:02:07.224268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_corpus(data):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for col in ['context','target']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(data)        \ncorpus[0:2]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:07.226495Z","iopub.execute_input":"2022-04-28T06:02:07.227008Z","iopub.status.idle":"2022-04-28T06:02:07.523696Z","shell.execute_reply.started":"2022-04-28T06:02:07.22697Z","shell.execute_reply":"2022-04-28T06:02:07.522837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)\nmodel.wv['material']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:07.52509Z","iopub.execute_input":"2022-04-28T06:02:07.525356Z","iopub.status.idle":"2022-04-28T06:02:08.162925Z","shell.execute_reply.started":"2022-04-28T06:02:07.52532Z","shell.execute_reply":"2022-04-28T06:02:08.162208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:08.164247Z","iopub.execute_input":"2022-04-28T06:02:08.164797Z","iopub.status.idle":"2022-04-28T06:02:08.17313Z","shell.execute_reply.started":"2022-04-28T06:02:08.16476Z","shell.execute_reply":"2022-04-28T06:02:08.172443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_plot(model)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:08.174938Z","iopub.execute_input":"2022-04-28T06:02:08.175571Z","iopub.status.idle":"2022-04-28T06:02:10.253891Z","shell.execute_reply.started":"2022-04-28T06:02:08.175508Z","shell.execute_reply":"2022-04-28T06:02:10.253217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A more selective model\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500, workers=4)\ntsne_plot(model)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:10.255145Z","iopub.execute_input":"2022-04-28T06:02:10.257543Z","iopub.status.idle":"2022-04-28T06:02:11.712584Z","shell.execute_reply.started":"2022-04-28T06:02:10.257502Z","shell.execute_reply":"2022-04-28T06:02:11.711885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A less selective model\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=100, workers=4)\ntsne_plot(model)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:11.713684Z","iopub.execute_input":"2022-04-28T06:02:11.714054Z","iopub.status.idle":"2022-04-28T06:02:16.811827Z","shell.execute_reply.started":"2022-04-28T06:02:11.714017Z","shell.execute_reply":"2022-04-28T06:02:16.808595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.most_similar('material')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:16.813322Z","iopub.execute_input":"2022-04-28T06:02:16.813834Z","iopub.status.idle":"2022-04-28T06:02:16.822183Z","shell.execute_reply.started":"2022-04-28T06:02:16.813796Z","shell.execute_reply":"2022-04-28T06:02:16.821434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.5.5 FastText**\n\nFastText create Facebook research team, One of best embedded method\n\nAs Word2Vec and Doc2Vec models rely on the vocabulary they had been trained on if the new text data that we want to vectorized contains words that were not previously present in the training vocabulary then these models fail to vectorized the unseen words accurately. FastText overcomes this problem.\n\nFastText does this by vectorizing each word as a combination of character n-grams. The keyword to remember when working with FastText is character n-grams. If you don’t know what n-grams are, they are a number of words taken into consideration when working with text.\n\n### ***Uses of FastText:***\n\n- Very useful for finding semantic similarities\n- Large datasets can be trained in minutes\n- Can be used for the purpose of text classification.\n\n<img src='https://kavita-ganesan.com/wp-content/uploads/fastText-vs.-Word2Vec.png'>\n\nThus the biggest advantage of using FastText over other models such as Word2Vec is that FastText can generate embeddings for sentences with words not present in the training vocabulary with the help of character n-grams whereas other models fail to do so.\n\nFull credit-Reference: https://pythonwife.com/fasttext-in-nlp/","metadata":{}},{"cell_type":"code","source":"!pip install gensim==4.0.1\nfrom gensim.models import FastText\nfrom gensim.test.utils import common_texts","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:16.823693Z","iopub.execute_input":"2022-04-28T06:02:16.824188Z","iopub.status.idle":"2022-04-28T06:02:25.967564Z","shell.execute_reply.started":"2022-04-28T06:02:16.824138Z","shell.execute_reply":"2022-04-28T06:02:25.966568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_texts","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:25.969754Z","iopub.execute_input":"2022-04-28T06:02:25.970209Z","iopub.status.idle":"2022-04-28T06:02:25.978323Z","shell.execute_reply.started":"2022-04-28T06:02:25.970142Z","shell.execute_reply":"2022-04-28T06:02:25.976788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build the model\nmodel1 = FastText(size=5, window=3, min_count=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:25.979887Z","iopub.execute_input":"2022-04-28T06:02:25.98043Z","iopub.status.idle":"2022-04-28T06:02:25.99205Z","shell.execute_reply.started":"2022-04-28T06:02:25.980373Z","shell.execute_reply":"2022-04-28T06:02:25.990845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.build_vocab(common_texts)\nmodel1.train(common_texts, total_examples=len(common_texts), epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:25.993618Z","iopub.execute_input":"2022-04-28T06:02:25.994192Z","iopub.status.idle":"2022-04-28T06:02:26.044424Z","shell.execute_reply.started":"2022-04-28T06:02:25.994086Z","shell.execute_reply":"2022-04-28T06:02:26.043603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Predict one word:',model1.wv['human'])\nprint('---------------------------')\nprint('')\nprint(model1.wv.most_similar(positive=['computer','interface'], negative=['human']))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:26.045818Z","iopub.execute_input":"2022-04-28T06:02:26.047758Z","iopub.status.idle":"2022-04-28T06:02:26.057578Z","shell.execute_reply.started":"2022-04-28T06:02:26.047718Z","shell.execute_reply":"2022-04-28T06:02:26.056653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **13.5.6 Glove**\n\n@Ruchi Bhatia (Reference: https://www.kaggle.com/discussions/getting-started/164408)\n\nGloVe: GloVe is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. The number of “contexts” is of course large, since it is essentially combinatorial in size. So then we factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.\n\n### **Why GloVe is better than Word2Vec?**\n\n### **Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.**\n\n<img src='https://miro.medium.com/max/1400/1*gcC7b_v7OKWutYN1NAHyMQ.png'>\n\nReference: https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html","metadata":{}},{"cell_type":"markdown","source":"## **Hello Guys! I'm trying Glove embed, Build lstm model and predict output**\n\n### **Features selection: I'm simply take x is target and y is score**\n","metadata":{}},{"cell_type":"code","source":"#Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.\n\nimport numpy as np\nimport pandas as pd \n\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndf = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ndf.sample(2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-28T06:02:26.059302Z","iopub.execute_input":"2022-04-28T06:02:26.059901Z","iopub.status.idle":"2022-04-28T06:02:28.734985Z","shell.execute_reply.started":"2022-04-28T06:02:26.059862Z","shell.execute_reply":"2022-04-28T06:02:28.734249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Count the score attribute**","metadata":{}},{"cell_type":"code","source":"print(df['score'].value_counts())\nsns.countplot(df['score'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:28.736266Z","iopub.execute_input":"2022-04-28T06:02:28.736513Z","iopub.status.idle":"2022-04-28T06:02:28.926024Z","shell.execute_reply.started":"2022-04-28T06:02:28.736481Z","shell.execute_reply":"2022-04-28T06:02:28.925265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Feature selection**","metadata":{}},{"cell_type":"code","source":"x = df['target']\ny = df['score']","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:28.927305Z","iopub.execute_input":"2022-04-28T06:02:28.927753Z","iopub.status.idle":"2022-04-28T06:02:28.932362Z","shell.execute_reply.started":"2022-04-28T06:02:28.927713Z","shell.execute_reply":"2022-04-28T06:02:28.931641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Tokenize,pad,vocb the input**","metadata":{}},{"cell_type":"code","source":"token = Tokenizer()\ntoken.fit_on_texts(x)\nseq = token.texts_to_sequences(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:28.934053Z","iopub.execute_input":"2022-04-28T06:02:28.934634Z","iopub.status.idle":"2022-04-28T06:02:29.530961Z","shell.execute_reply.started":"2022-04-28T06:02:28.934583Z","shell.execute_reply":"2022-04-28T06:02:29.530148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_seq = pad_sequences(seq,maxlen=300)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:29.532467Z","iopub.execute_input":"2022-04-28T06:02:29.532733Z","iopub.status.idle":"2022-04-28T06:02:29.691843Z","shell.execute_reply.started":"2022-04-28T06:02:29.532697Z","shell.execute_reply":"2022-04-28T06:02:29.691083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(token.word_index)+1","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:29.693286Z","iopub.execute_input":"2022-04-28T06:02:29.693548Z","iopub.status.idle":"2022-04-28T06:02:29.697919Z","shell.execute_reply.started":"2022-04-28T06:02:29.693513Z","shell.execute_reply":"2022-04-28T06:02:29.697182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Initialize the glove 300d input txt file and apply the input data**","metadata":{}},{"cell_type":"code","source":"embedding_vector = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_vector[word] = coef","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:02:29.699259Z","iopub.execute_input":"2022-04-28T06:02:29.699762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size,300))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Build the model**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size,300,weights = [embedding_matrix],input_length=300,trainable = False))\nmodel.add(Bidirectional(CuDNNLSTM(75)))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='rmsprop')\nhistory = model.fit(pad_seq,y,epochs = 50,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Save the model**","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\nmodel.save(\"My_Glove_LSTM_Model.h5\")\n#del model  # deletes the existing model\n    \n# returns a compiled model\n# identical to the previous one\n#model = load_model('my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Apply testdata and predict** ","metadata":{}},{"cell_type":"code","source":"testing = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\nsample = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nprint('sample_shape',sample.shape)\ntesting.sample(2)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = testing['target']\nx_test = token.texts_to_sequences(x_test)\ntesting_seq = pad_sequences(x_test,maxlen=300)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### **Predict output** ","metadata":{}},{"cell_type":"code","source":"predict = model.predict(testing_seq)\ntesting['label'] = predict\ntesting.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Generate Output_Submission CSV file**","metadata":{}},{"cell_type":"code","source":"final_predict = testing.label\nsample['score'] = final_predict\nsample.to_csv(\"submission.csv\",index=False)\nprint(\"Final achieve to send Glove_Predict output data\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***First of all this learning process in NLP is nice for me and I believe this notebook useful for all,This not just basics of NLP \"How to start nlp in begineers?\" in my experience perspective***\n\n**This Notebook is first step to learn NLP (different basic concepts and how to apply the text data)**\n\n## **\"My next step is move forward in Transformer concept\"** \n(https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/320962)","metadata":{}},{"cell_type":"markdown","source":"***Reference: Full credits -----> Sendex_NLTK video series, Nlp practical book, more resourse like blog,articles,research paper & my team member*** \n\n**1. https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/**\n\n**2. https://youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL**\n\n**3. https://github.com/practical-nlp/practical-nlp-code/blob/master/**\n\n**4. https://www.mygreatlearning.com/blog/bag-of-words/**\n\n**5. https://towardsdatascience.com/word2vec-explained-49c52b4ccb71**\n\n**6. https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove#GloVe-for-Vectorization**\n\n**7. https://medium.com/@sarin.samarth07/glove-word-embeddings-with-keras-python-code-52131b0c8b1d**\n\n\n### **\"if you see any errors and your opinion! feel free to share with me\"**\n\n\n## ⭐️⭐️⭐️NextProcess_🔥Transformer🔥_Nextprocess⭐️⭐️⭐️   ","metadata":{}},{"cell_type":"markdown","source":"# ***⭐️⭐️⭐️Thankyou for visiting Guys!⭐️⭐️⭐️***","metadata":{}}]}