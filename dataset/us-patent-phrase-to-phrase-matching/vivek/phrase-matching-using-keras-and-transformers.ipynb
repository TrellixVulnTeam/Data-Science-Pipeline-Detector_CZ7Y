{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import StratifiedKFold \nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel \nimport gc\ntf.keras.backend.clear_session()\nfrom transformers import logging\nlogging.set_verbosity_error()\nTRAIN = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T17:11:45.24582Z","iopub.execute_input":"2022-06-21T17:11:45.246242Z","iopub.status.idle":"2022-06-21T17:11:52.905351Z","shell.execute_reply.started":"2022-06-21T17:11:45.246161Z","shell.execute_reply":"2022-06-21T17:11:52.904374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\n\ndef get_cpc_texts():\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:52.907691Z","iopub.execute_input":"2022-06-21T17:11:52.908509Z","iopub.status.idle":"2022-06-21T17:11:52.917485Z","shell.execute_reply.started":"2022-06-21T17:11:52.90847Z","shell.execute_reply":"2022-06-21T17:11:52.916596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    min_delta=1e-5, \n    patience=1, \n    verbose=1,\n    mode='auto', \n    restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.9, \n    patience=1, \n    mode='auto', \n    min_delta=0.001,\n    verbose = 1\n)\n\ntokenizer = AutoTokenizer.from_pretrained('../input/deberta-l') \n\nclass CorrelationScore(tf.keras.metrics.Metric):\n    def __init__(self, name='correlation', **kwargs):\n        super(CorrelationScore, self).__init__(name=name, **kwargs)\n        self.correlation = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=1)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, (-1,1))\n        y_pred = tf.reshape(y_pred, (-1,1))\n        self.correlation.update_state(y_true, y_pred)\n        \n    def reset_state(self):\n        self.correlation.reset_state()\n    \n    def result(self):\n        return self.correlation.result()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:52.919648Z","iopub.execute_input":"2022-06-21T17:11:52.920779Z","iopub.status.idle":"2022-06-21T17:11:53.663008Z","shell.execute_reply.started":"2022-06-21T17:11:52.92074Z","shell.execute_reply":"2022-06-21T17:11:53.662068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data(df, train=True):\n    text = df[\"text\"].values.astype(str)\n    target = df[\"target\"].values.astype(str)\n    if train:\n        label = df['score'].values\n        \n    input_ids = []\n    attention_mask = []\n    token_ids = []\n    labels = []\n    \n    for i in range(df.shape[0]):\n        tokens = tokenizer(\n        text[i],\n        target[i],\n        max_length=32,\n        padding=\"max_length\",\n        truncation=True,\n            )\n        inputs, masks, ids = tokens['input_ids'], tokens[\"attention_mask\"], tokens[\"token_type_ids\"]\n        input_ids.append(inputs)\n        attention_mask.append(masks)\n        token_ids.append(ids)\n        if train:\n            labels.append(label[i])\n            \n    if train:\n        return (input_ids,attention_mask,token_ids), labels\n    else:\n        return (input_ids,attention_mask,token_ids)\n\ndef create_model():\n    input_tokens = tf.keras.layers.Input(shape=(32,), dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input(shape=(32,), dtype=tf.int32)\n    token_ids = tf.keras.layers.Input(shape=(32,), dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained('../input/deberta-l')\n    backbone = TFAutoModel.from_pretrained('../input/deberta-l', config=config)\n\n    out = backbone(input_tokens, attention_mask=attention_mask, token_type_ids=token_ids)[0]\n    out1 = tf.keras.layers.Dropout(0.9)(out)\n    out = tf.keras.layers.Dropout(0.2)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out1 = tf.keras.layers.Flatten()(out1)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Dense((config.hidden_size/2), activation='relu', dtype='float32')(out)\n    out1 = tf.keras.layers.Dense((config.hidden_size/2), activation='relu', dtype='float32')(out1)\n    out = tf.keras.layers.Average()([out, out1])\n    out = tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')(out)\n    model = tf.keras.Model(inputs=[input_tokens, attention_mask, token_ids], outputs=out)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:53.665378Z","iopub.execute_input":"2022-06-21T17:11:53.665905Z","iopub.status.idle":"2022-06-21T17:11:53.683841Z","shell.execute_reply.started":"2022-06-21T17:11:53.665863Z","shell.execute_reply":"2022-06-21T17:11:53.682826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    for fold in range(5):\n    \n        tf.keras.backend.clear_session()\n        train = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\n        train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n        Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n        for n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n            train.loc[val_index, 'fold'] = n\n        train['fold'] = train['fold'].astype(int)\n        cpc_texts = get_cpc_texts()\n        train['context_text'] = train['context'].map(cpc_texts)\n        train['text'] = train['anchor'] + '[SEP]' + train['context_text'] \n        train['target'] = train['target']  + '[SEP]' + train['context_text'] \n\n        model_save = tf.keras.callbacks.ModelCheckpoint(\n        f'deberta_{fold}.h5', \n        save_best_only = True, \n        save_weights_only = True,\n        monitor = 'val_loss', \n        mode = 'min', verbose = 1\n        )\n    \n        valid_df=train[train.fold==fold]\n        train_df=train[train.fold!=fold]\n        print('FOLD:',fold)    \n\n        train_data, train_labels = create_data(train_df,train=True)\n        val_data, val_labels = create_data(valid_df,train=True)\n        train_data_X = (np.asarray(train_data[0]),np.asarray(train_data[1]),np.asarray(train_data[2]))\n        train_data_Y = np.asarray(train_labels)\n        valid_data = ((np.asarray(val_data[0]), np.asarray(val_data[1]), np.asarray(val_data[2])), np.asarray(val_labels))\n\n        import gc\n        del train_df, valid_df, train_data, train_labels, val_data, val_labels\n        gc.collect()\n    \n        model = create_model()\n        history=model.fit(\n                train_data_X,\n                train_data_Y, \n                epochs = 5,\n                shuffle=True,\n                batch_size = 31,\n                validation_data= valid_data,\n                callbacks = [model_save, early_stop, reduce_lr],   \n            ) \n\n        del model\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:53.685812Z","iopub.execute_input":"2022-06-21T17:11:53.686731Z","iopub.status.idle":"2022-06-21T17:11:53.702904Z","shell.execute_reply.started":"2022-06-21T17:11:53.686682Z","shell.execute_reply":"2022-06-21T17:11:53.701995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\ncpc_texts = get_cpc_texts()\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['text'] = test['anchor'] + '[SEP]' + test['context_text'] \ntest['target'] = test['target']  + '[SEP]' + test['context_text'] \ntest_data = create_data(test, train=False)\nmodel = create_model()\npreds = []\nfor i in [0,1,2,3,4]:\n    model.load_weights(f'../input/cpc-deberta/deberta_{i}.h5')\n    pred = model.predict((np.asarray(test_data[0]),np.asarray(test_data[1]),np.asarray(test_data[2])))\n    preds.append(pred)\n    \nsample = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nsample['score'] = np.mean(np.array(preds), axis=0)\nsample.to_csv('submission.csv', index=False)    ","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:53.70442Z","iopub.execute_input":"2022-06-21T17:11:53.70546Z","iopub.status.idle":"2022-06-21T17:14:27.868453Z","shell.execute_reply.started":"2022-06-21T17:11:53.705421Z","shell.execute_reply":"2022-06-21T17:14:27.867351Z"},"trusted":true},"execution_count":null,"outputs":[]}]}