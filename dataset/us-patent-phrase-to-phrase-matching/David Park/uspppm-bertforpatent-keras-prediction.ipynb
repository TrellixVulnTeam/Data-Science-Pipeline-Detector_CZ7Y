{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n# import bert\nimport math\nimport os\n\nfrom tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import callbacks\nimport tensorflow.keras.backend as K\n\nimport codecs\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pickle\n\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T22:58:52.461141Z","iopub.execute_input":"2022-05-10T22:58:52.461483Z","iopub.status.idle":"2022-05-10T22:58:59.870917Z","shell.execute_reply.started":"2022-05-10T22:58:52.461378Z","shell.execute_reply":"2022-05-10T22:58:59.870157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# environmental variables","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/us-patent-phrase-to-phrase-matching/\"\npt_model_dir = \"/kaggle/input/bert-for-patents/bert-for-patents/\"\nft_model_dir = \"/kaggle/input/uspppm-bertforpatent-keras-train/usppm_bfp_v5_lstm.h5\"\nmax_seq_len = 80\nbatch_size = 32\nlearning_rate = 2e-5","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:40.493404Z","iopub.execute_input":"2022-05-10T22:59:40.493727Z","iopub.status.idle":"2022-05-10T22:59:40.503001Z","shell.execute_reply.started":"2022-05-10T22:59:40.493692Z","shell.execute_reply":"2022-05-10T22:59:40.502158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Tokenizer ","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(pt_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(pt_model_dir)\npad_idx = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nprint(tokenizer)\nprint(\"Padding token index : \", pad_idx)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:43.817104Z","iopub.execute_input":"2022-05-10T22:59:43.817377Z","iopub.status.idle":"2022-05-10T22:59:43.962929Z","shell.execute_reply.started":"2022-05-10T22:59:43.817346Z","shell.execute_reply":"2022-05-10T22:59:43.962116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# encode funtion","metadata":{}},{"cell_type":"code","source":"def dataset_split(dataset, split_val):\n    lengths = int(len(dataset) * split_val)\n    train_data = dataset[:lengths]\n    valid_data = dataset[lengths:]\n    return train_data, valid_data\n\ndef dataset_load(train_url, test_url):\n    train_data = pd.read_csv(train_url, sep=',')\n    train_data['sep_token'] = '[SEP]'\n    train_data['cls_token'] = '[CLS]'\n    train_data['context_token'] = '[' + train_data.context + ']'\n    context_tokens = list(train_data.context_token.unique())\n    train_data = train_data.sample(frac=1).reset_index(drop=True)\n    train_data, valid_data = dataset_split(dataset=train_data, split_val=0.9)\n    test_data = pd.read_csv(test_url, sep=',')\n    test_data['sep_token'] = '[SEP]'\n    test_data['cls_token'] = '[CLS]'\n    test_data['context_token'] = '[' + test_data.context + ']'\n    \n    return train_data, valid_data, test_data, context_tokens\n\ndef create_learning_rate_scheduler(max_learn_rate=5e-5,\n                                   end_learn_rate=1e-7,\n                                   warmup_epoch_count=10,\n                                   total_epoch_count=90):\n\n    def lr_scheduler(epoch):\n        \n        if epoch < warmup_epoch_count:\n            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n        else:\n            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n        return float(res)\n    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n\n    return learning_rate_scheduler\n\ndef encode_text(text, \n                tokenizer,\n                max_length):\n    \n    # With tokenizer's batch_encode_plus batch of both the sentences are\n    # encoded together and separated by [SEP] token.\n    encoded = tokenizer.batch_encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True,\n        return_tensors=\"tf\",\n    )\n\n    # Convert batch of encoded features to numpy array.\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_masks\": attention_masks,\n        \"token_type_ids\": token_type_ids\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:45.942559Z","iopub.execute_input":"2022-05-10T22:59:45.943137Z","iopub.status.idle":"2022-05-10T22:59:45.960904Z","shell.execute_reply.started":"2022-05-10T22:59:45.943098Z","shell.execute_reply":"2022-05-10T22:59:45.958741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test data load","metadata":{}},{"cell_type":"code","source":"train_data, valid_data, test_data, context_tokens = dataset_load(dataset_path + \"train.csv\", dataset_path + \"test.csv\")\nlabels = list(set(train_data[\"score\"].values))\nlabels.sort()\n\nprint(len(train_data), len(valid_data), len(test_data))\nprint(labels)\nprint(context_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:47.325987Z","iopub.execute_input":"2022-05-10T22:59:47.326242Z","iopub.status.idle":"2022-05-10T22:59:47.454328Z","shell.execute_reply.started":"2022-05-10T22:59:47.326212Z","shell.execute_reply":"2022-05-10T22:59:47.453656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:49.479932Z","iopub.execute_input":"2022-05-10T22:59:49.480502Z","iopub.status.idle":"2022-05-10T22:59:49.504035Z","shell.execute_reply.started":"2022-05-10T22:59:49.480465Z","shell.execute_reply":"2022-05-10T22:59:49.503396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load cpc title data","metadata":{}},{"cell_type":"code","source":"cpc_codes = pd.read_csv(\"/kaggle/input/cpc-codes/titles.csv\")\ncpc_codes = cpc_codes[[\"code\", \"title\"]]\n\ncondition = cpc_codes['code'].map(len) == 3\ncpc_codes = cpc_codes[condition].reset_index(drop=True)\ncpc_codes","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:51.549179Z","iopub.execute_input":"2022-05-10T22:59:51.549488Z","iopub.status.idle":"2022-05-10T22:59:52.416985Z","shell.execute_reply.started":"2022-05-10T22:59:51.549451Z","shell.execute_reply":"2022-05-10T22:59:52.416277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpc_codes.loc[cpc_codes.code == \"B29\"][\"title\"].values )","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:52.421091Z","iopub.execute_input":"2022-05-10T22:59:52.421294Z","iopub.status.idle":"2022-05-10T22:59:52.426844Z","shell.execute_reply.started":"2022-05-10T22:59:52.421271Z","shell.execute_reply":"2022-05-10T22:59:52.426012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.merge(cpc_codes, left_on='context', right_on='code', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:53.380839Z","iopub.execute_input":"2022-05-10T22:59:53.381345Z","iopub.status.idle":"2022-05-10T22:59:53.393034Z","shell.execute_reply.started":"2022-05-10T22:59:53.381309Z","shell.execute_reply":"2022-05-10T22:59:53.392164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['title'] = test_data['title'].str.lower().str.replace(\";\",\"\")\ntest_data['anchor'] = test_data['anchor'].str.lower()\ntest_data['target'] = test_data['target'].str.lower()\n\ntest_data['text'] = test_data['title'] + \" \" + test_data['anchor']\n\n# test_data['text'] = test_data['cls_token'] + \\\n#                 test_data['context_token'] + test_data['title'] + \\\n#                 test_data['sep_token'] + test_data['anchor'] + \\\n#                 test_data['sep_token'] + test_data['target'] + \\\n#                 test_data['sep_token']\n\nprint(test_data[\"title\"][0])\nprint(test_data['anchor'][0])\nprint(test_data['target'][0])\nprint(test_data['text'][0])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:54.285154Z","iopub.execute_input":"2022-05-10T22:59:54.285405Z","iopub.status.idle":"2022-05-10T22:59:54.298163Z","shell.execute_reply.started":"2022-05-10T22:59:54.285377Z","shell.execute_reply":"2022-05-10T22:59:54.297289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:55.797751Z","iopub.execute_input":"2022-05-10T22:59:55.798291Z","iopub.status.idle":"2022-05-10T22:59:55.819173Z","shell.execute_reply.started":"2022-05-10T22:59:55.798252Z","shell.execute_reply":"2022-05-10T22:59:55.81853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenize and encode the test data","metadata":{}},{"cell_type":"code","source":"encoded_test_data = encode_text(test_data[[\"text\", \"target\"]].values.tolist(), tokenizer, max_seq_len)\nprint(encoded_test_data[\"input_ids\"][0])\nprint(encoded_test_data[\"attention_masks\"][0])\nprint(encoded_test_data[\"token_type_ids\"][0])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T22:59:57.910717Z","iopub.execute_input":"2022-05-10T22:59:57.910961Z","iopub.status.idle":"2022-05-10T23:00:03.031893Z","shell.execute_reply.started":"2022-05-10T22:59:57.910935Z","shell.execute_reply":"2022-05-10T23:00:03.031121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = [encoded_test_data[\"input_ids\"], encoded_test_data[\"attention_masks\"], encoded_test_data[\"token_type_ids\"]]\nprint(\"test x shape : \", test_x[0].shape, test_x[1].shape, test_x[2].shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T23:00:03.036159Z","iopub.execute_input":"2022-05-10T23:00:03.03645Z","iopub.status.idle":"2022-05-10T23:00:03.052366Z","shell.execute_reply.started":"2022-05-10T23:00:03.036399Z","shell.execute_reply":"2022-05-10T23:00:03.051592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load USPPM Fine tuning model trained by Google BFP model","metadata":{}},{"cell_type":"code","source":"mirrored_strategy = tf.distribute.MirroredStrategy()\nwith mirrored_strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    base_model = TFBertModel.from_pretrained(pt_model_dir, from_pt=True)\n\n    base_model_output = base_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n\n    last_hidden_state = base_model_output.last_hidden_state\n    print(last_hidden_state.shape)\n    \n#     cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(last_hidden_state)\n#     output = tf.keras.layers.Dense(1, activation=\"linear\")(cls_out)\n\n#     gru = GRU(units=max_seq_len, return_sequences=False)(last_hidden_state)\n    lstm = Bidirectional(LSTM(units=max_seq_len, return_sequences=False))(last_hidden_state)\n    output = tf.keras.layers.Dense(1, activation=\"linear\", name=\"uspppm_output\")(lstm)\n    \n#     avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n#     dropout = tf.keras.layers.Dropout(0.1, name=\"uspppm_dropout\")(avg_pool)\n#     output = tf.keras.layers.Dense(1, activation=\"linear\", name=\"uspppm_output\")(dropout)\n\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n#         optimizer = tf.keras.optimizers.Adam(),\n        loss='mse'\n#         loss=tf.keras.losses.BinaryCrossentropy()\n    )\n\n# 전체 신경망 모델 요약 출력\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T23:00:31.481294Z","iopub.execute_input":"2022-05-10T23:00:31.481767Z","iopub.status.idle":"2022-05-10T23:00:56.103144Z","shell.execute_reply.started":"2022-05-10T23:00:31.481729Z","shell.execute_reply":"2022-05-10T23:00:56.102446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(ft_model_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T23:00:56.104864Z","iopub.execute_input":"2022-05-10T23:00:56.105096Z","iopub.status.idle":"2022-05-10T23:01:14.224217Z","shell.execute_reply.started":"2022-05-10T23:00:56.105061Z","shell.execute_reply":"2022-05-10T23:01:14.223475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"pred = model.predict(test_x)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T23:01:14.225839Z","iopub.execute_input":"2022-05-10T23:01:14.226086Z","iopub.status.idle":"2022-05-10T23:01:23.064494Z","shell.execute_reply.started":"2022-05-10T23:01:14.226051Z","shell.execute_reply":"2022-05-10T23:01:23.06376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsubmission['score'] = pred\nsubmission['score'] = submission.score.apply(lambda x: 0 if x < 0 else x)\nsubmission['score'] = submission.score.apply(lambda x: 1 if x > 1 else x)\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-10T23:01:23.068479Z","iopub.execute_input":"2022-05-10T23:01:23.06869Z","iopub.status.idle":"2022-05-10T23:01:23.094054Z","shell.execute_reply.started":"2022-05-10T23:01:23.068665Z","shell.execute_reply":"2022-05-10T23:01:23.093393Z"},"trusted":true},"execution_count":null,"outputs":[]}]}