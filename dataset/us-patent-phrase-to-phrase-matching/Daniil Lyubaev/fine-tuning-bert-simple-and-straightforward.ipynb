{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T15:43:59.847153Z","iopub.execute_input":"2022-05-14T15:43:59.847479Z","iopub.status.idle":"2022-05-14T15:43:59.925417Z","shell.execute_reply.started":"2022-05-14T15:43:59.847399Z","shell.execute_reply":"2022-05-14T15:43:59.924666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\nprint(train_df.info())\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:43:59.927275Z","iopub.execute_input":"2022-05-14T15:43:59.927528Z","iopub.status.idle":"2022-05-14T15:44:00.058329Z","shell.execute_reply.started":"2022-05-14T15:43:59.927493Z","shell.execute_reply":"2022-05-14T15:44:00.057607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['anchor','target','context','score']:\n    print('\\n====',col,'=====')\n    print(train_df[col].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:00.05982Z","iopub.execute_input":"2022-05-14T15:44:00.060131Z","iopub.status.idle":"2022-05-14T15:44:00.102425Z","shell.execute_reply.started":"2022-05-14T15:44:00.060091Z","shell.execute_reply":"2022-05-14T15:44:00.101623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:00.104611Z","iopub.execute_input":"2022-05-14T15:44:00.104894Z","iopub.status.idle":"2022-05-14T15:44:00.911874Z","shell.execute_reply.started":"2022-05-14T15:44:00.104856Z","shell.execute_reply":"2022-05-14T15:44:00.911165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- adding cpc title to add more context","metadata":{}},{"cell_type":"code","source":"cpc = cpc.rename(columns = {\"code\" : \"context\"})\ntrain_df = pd.merge(train_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:00.913247Z","iopub.execute_input":"2022-05-14T15:44:00.913499Z","iopub.status.idle":"2022-05-14T15:44:01.071304Z","shell.execute_reply.started":"2022-05-14T15:44:00.913472Z","shell.execute_reply":"2022-05-14T15:44:01.070459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(x):\n    t = x.lower()\n    t = t.replace(\"[\",'')\n    t = t.replace(\";\",'')\n    t = t.replace(\",\",'')\n    t = t.replace(\"]\",'')\n    t = t.replace(\":\",'')\n    return t\n\ntrain_df['title'] = train_df['title'].apply(lambda x: clean(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:01.07286Z","iopub.execute_input":"2022-05-14T15:44:01.073158Z","iopub.status.idle":"2022-05-14T15:44:01.140812Z","shell.execute_reply.started":"2022-05-14T15:44:01.07312Z","shell.execute_reply":"2022-05-14T15:44:01.139895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['sen1'] = train_df['anchor'].astype('str')+' '+train_df['title'].astype('str')\ntrain_df = train_df.drop(['anchor','context','title'],axis=1)\n# train_df['all_sen'] = train_df['sen1']+' [SEP '+train_df['target']\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:01.142394Z","iopub.execute_input":"2022-05-14T15:44:01.142761Z","iopub.status.idle":"2022-05-14T15:44:01.177501Z","shell.execute_reply.started":"2022-05-14T15:44:01.142719Z","shell.execute_reply":"2022-05-14T15:44:01.17664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq1_len = [len(i.split()) for i in train_df['sen1'].values]\npd.Series(seq1_len).hist(bins = 30)\n\ntar_len = [len(i.split()) for i in train_df['target'].values]\npd.Series(tar_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:01.17912Z","iopub.execute_input":"2022-05-14T15:44:01.179448Z","iopub.status.idle":"2022-05-14T15:44:01.645249Z","shell.execute_reply.started":"2022-05-14T15:44:01.179405Z","shell.execute_reply":"2022-05-14T15:44:01.644526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(train_df[['target','sen1']],train_df['score'],random_state=1234,test_size=0.3)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:01.648412Z","iopub.execute_input":"2022-05-14T15:44:01.648661Z","iopub.status.idle":"2022-05-14T15:44:02.465618Z","shell.execute_reply.started":"2022-05-14T15:44:01.648628Z","shell.execute_reply":"2022-05-14T15:44:02.464886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:02.467021Z","iopub.execute_input":"2022-05-14T15:44:02.467486Z","iopub.status.idle":"2022-05-14T15:44:02.476807Z","shell.execute_reply.started":"2022-05-14T15:44:02.467449Z","shell.execute_reply":"2022-05-14T15:44:02.476011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:02.478279Z","iopub.execute_input":"2022-05-14T15:44:02.478569Z","iopub.status.idle":"2022-05-14T15:44:02.490384Z","shell.execute_reply.started":"2022-05-14T15:44:02.478532Z","shell.execute_reply":"2022-05-14T15:44:02.489599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn.utils.clip_grad import clip_grad_norm\n\n\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:02.491995Z","iopub.execute_input":"2022-05-14T15:44:02.492284Z","iopub.status.idle":"2022-05-14T15:44:08.645644Z","shell.execute_reply.started":"2022-05-14T15:44:02.492247Z","shell.execute_reply":"2022-05-14T15:44:08.644886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:08.647178Z","iopub.execute_input":"2022-05-14T15:44:08.647447Z","iopub.status.idle":"2022-05-14T15:44:08.652889Z","shell.execute_reply.started":"2022-05-14T15:44:08.64741Z","shell.execute_reply":"2022-05-14T15:44:08.652176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### class defination for model , dataset and training functions","metadata":{}},{"cell_type":"code","source":"class my_model(nn.Module):\n    def __init__(self,bert_path):\n        super(my_model,self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.AutoModel.from_pretrained(self.bert_path)\n        self.fc_layer = nn.Sequential(\n            nn.Linear(1024,1),\n            nn.Tanh()\n        )\n        self.dropout = nn.Dropout(0.1)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n    def forward(self,ids,mask,token_type_ids):\n        out = self.bert(input_ids=ids,attention_mask=mask,token_type_ids=token_type_ids,return_dict=True)\n        out_hidden_state = out.last_hidden_state\n#         print(out.last_hidden_state.shape,out_hidden_state[0].shape)\n#         print(pooler_output.shape,oe1[0])\n#         pooler_output = out.get('pooler_output')\n#         print(pooler_output)\n        last_hidden_states = self.dropout(torch.mean(out_hidden_state, 1))\n        logits1 = self.fc_layer(self.dropout1(last_hidden_states))\n        logits2 = self.fc_layer(self.dropout2(last_hidden_states))\n        logits3 = self.fc_layer(self.dropout3(last_hidden_states))\n        logits4 = self.fc_layer(self.dropout4(last_hidden_states))\n        logits5 = self.fc_layer(self.dropout5(last_hidden_states))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        return logits\n     ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:08.654396Z","iopub.execute_input":"2022-05-14T15:44:08.655236Z","iopub.status.idle":"2022-05-14T15:44:08.667567Z","shell.execute_reply.started":"2022-05-14T15:44:08.655197Z","shell.execute_reply":"2022-05-14T15:44:08.666774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class my_dataset_train:\n    def __init__(self,text1,text2,label,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.label=label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        label = self.label[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\": torch.tensor(label,dtype=torch.float),\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:08.67041Z","iopub.execute_input":"2022-05-14T15:44:08.672555Z","iopub.status.idle":"2022-05-14T15:44:08.682848Z","shell.execute_reply.started":"2022-05-14T15:44:08.6725Z","shell.execute_reply":"2022-05-14T15:44:08.682107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### dataset for train and valid","metadata":{}},{"cell_type":"code","source":"max_len=32\ntrain_batch_size = 32\nepochs=5\nbert_path = '../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge'\n#'../input/bert-for-patents/bert-for-patents'#'../input/bert-base-uncased'\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(bert_path)\n\n# Training dataset prep\n\ntrain_text1 = list(x_train['target'].values)\ntrain_text2 = list(x_train['sen1'].values)\ntrain_label = list(y_train.values)\n\ntrain_dataset = my_dataset_train(\n    text1 = train_text1,\n    text2 = train_text2,\n    label = train_label,\n    tokenizer=tokenizer ,\n    max_len=max_len\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)\n\n# validation dataset prep\nval_text1 = list(x_test['target'].values)\nval_text2 = list(x_test['sen1'].values)\nval_label = list(y_test.values)\n\nvalid_dataset = my_dataset_train(\n    text1 = val_text1,\n    text2 = val_text2,\n    label = val_label,\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=train_batch_size,shuffle=True)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:08.684319Z","iopub.execute_input":"2022-05-14T15:44:08.684674Z","iopub.status.idle":"2022-05-14T15:44:09.22601Z","shell.execute_reply.started":"2022-05-14T15:44:08.684636Z","shell.execute_reply":"2022-05-14T15:44:09.225207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, scheduler, loss_function, epochs,train_dataloader, device, clip_value=2):\n    model.train()\n    for epoch in range(epochs):\n        print(epoch)\n        print(\"-----\")\n        best_loss = []\n        for step, batch in enumerate(train_dataloader): \n            batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n            batch_token_type_ids = batch['token_type_ids'].to(device)\n            model.zero_grad()\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n            loss = loss_function(outputs.squeeze(),batch_labels.squeeze())\n            best_loss.append(loss)\n            loss.backward()\n            clip_grad_norm(model.parameters(), clip_value)\n            optimizer.step()\n            scheduler.step()\n            print(f\"step > {step},loss > {loss}\")\n        loss2 = sum(best_loss)/len(best_loss)\n        print(f'Epoch : {epoch} ,Train loss : {loss2}')\n                \n    return model\n\ndef r2_score(outputs, labels):\n    labels_mean = torch.mean(labels)\n    ss_tot = torch.sum((labels - labels_mean) ** 2)\n    ss_res = torch.sum((labels - outputs) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return r2\n\ndef evaluate(model,loss_function,test_dataloader,device):\n    model.eval()\n    test_loss, test_r2 = [], []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        loss = loss_function(outputs.squeeze(), batch_labels.squeeze())\n        test_loss.append(loss.item())\n        r2 = r2_score(outputs, batch_labels)\n        test_r2.append(r2.item())\n    return test_loss, test_r2","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:09.229442Z","iopub.execute_input":"2022-05-14T15:44:09.229659Z","iopub.status.idle":"2022-05-14T15:44:09.245597Z","shell.execute_reply.started":"2022-05-14T15:44:09.229634Z","shell.execute_reply":"2022-05-14T15:44:09.244874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_steps = len(train_data_loader) * epochs\nmodel = my_model(bert_path).to(device)\n\noptimizer = transformers.AdamW(model.parameters(),lr=3e-5,eps=1e-8)\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, nesterov=True)\n# optimizer = transformers.Adafactor(model.parameters())\n# optimizer = torch.optim.Adam(model.parameters())\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n)\n\nloss_function = nn.BCELoss()\n\n\nmodel = train(model, optimizer, scheduler, loss_function, epochs,train_data_loader, device)\n\n\nloss1,r2_ = evaluate(model,loss_function,valid_data_loader,device)\nloss = sum(loss1)/len(loss1)\nr2 = sum(r2_)/len(r2_)\nprint(f\"eval mean result : loss {loss}, r2 {r2}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),f'./my_bert')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.828045Z","iopub.status.idle":"2022-05-14T15:44:42.828631Z","shell.execute_reply.started":"2022-05-14T15:44:42.828385Z","shell.execute_reply":"2022-05-14T15:44:42.828412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\n- follow evaluate function\n- create dataset class","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.82984Z","iopub.status.idle":"2022-05-14T15:44:42.830417Z","shell.execute_reply.started":"2022-05-14T15:44:42.830175Z","shell.execute_reply":"2022-05-14T15:44:42.830202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.831529Z","iopub.status.idle":"2022-05-14T15:44:42.832094Z","shell.execute_reply.started":"2022-05-14T15:44:42.831836Z","shell.execute_reply":"2022-05-14T15:44:42.831863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.8332Z","iopub.status.idle":"2022-05-14T15:44:42.833741Z","shell.execute_reply.started":"2022-05-14T15:44:42.833513Z","shell.execute_reply":"2022-05-14T15:44:42.833539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntest_df = pd.merge(test_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n\ndef clean(x):\n    t = x.lower()\n    t = t.replace(\"[\",'')\n    t = t.replace(\";\",'')\n    t = t.replace(\",\",'')\n    t = t.replace(\"]\",'')\n    t = t.replace(\":\",'')\n    return t\n\ntest_df['title'] = test_df['title'].apply(lambda x: clean(x))\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.834829Z","iopub.status.idle":"2022-05-14T15:44:42.835408Z","shell.execute_reply.started":"2022-05-14T15:44:42.835169Z","shell.execute_reply":"2022-05-14T15:44:42.835196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.83654Z","iopub.status.idle":"2022-05-14T15:44:42.837112Z","shell.execute_reply.started":"2022-05-14T15:44:42.836849Z","shell.execute_reply":"2022-05-14T15:44:42.836876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['sen1'] = test_df['anchor'].astype('str')+' '+test_df['title'].astype('str')\ntest_df = test_df.drop(['anchor','context','title'],axis=1)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.838191Z","iopub.status.idle":"2022-05-14T15:44:42.838742Z","shell.execute_reply.started":"2022-05-14T15:44:42.838506Z","shell.execute_reply":"2022-05-14T15:44:42.83853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class my_dataset_test:\n    def __init__(self,text1,text2,idf,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.idf = idf\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        idf = self.idf[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"idf\": idf\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.839849Z","iopub.status.idle":"2022-05-14T15:44:42.84042Z","shell.execute_reply.started":"2022-05-14T15:44:42.840184Z","shell.execute_reply":"2022-05-14T15:44:42.840209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_path = '../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge'\n#'../input/bert-for-patents/bert-for-patents'#'../input/bert-base-uncased'\nmax_len=64\n# tokenizer = transformers.BertTokenizer.from_pretrained(bert_path)\n\ntest_text1 = list(test_df['target'].values)\ntest_text2 = list(test_df['sen1'].values)\n\ntest_dataset = my_dataset_test(\n    text1 = test_text1,\n    text2 = test_text2,\n    idf=list(test_df['id'].values),\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.841512Z","iopub.status.idle":"2022-05-14T15:44:42.8421Z","shell.execute_reply.started":"2022-05-14T15:44:42.841825Z","shell.execute_reply":"2022-05-14T15:44:42.84185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model,test_dataloader,device):\n    model.eval()\n    result = []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks = batch['ids'].to(device), batch['mask'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        out = [i[0] for i in outputs.cpu().detach().numpy()]\n        batch_idf = batch['idf']\n        temp = [[i,j] for i,j in zip(batch_idf,out)]\n        result.extend(temp)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.843204Z","iopub.status.idle":"2022-05-14T15:44:42.843743Z","shell.execute_reply.started":"2022-05-14T15:44:42.843512Z","shell.execute_reply":"2022-05-14T15:44:42.843536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = my_model(bert_path).to(device)\nmodel.load_state_dict(torch.load('my_bert'))\n\nfinal_res = predict(model,test_data_loader,device)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.844871Z","iopub.status.idle":"2022-05-14T15:44:42.845461Z","shell.execute_reply.started":"2022-05-14T15:44:42.845233Z","shell.execute_reply":"2022-05-14T15:44:42.845257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_res","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.846545Z","iopub.status.idle":"2022-05-14T15:44:42.847115Z","shell.execute_reply.started":"2022-05-14T15:44:42.846858Z","shell.execute_reply":"2022-05-14T15:44:42.846883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.848201Z","iopub.status.idle":"2022-05-14T15:44:42.848749Z","shell.execute_reply.started":"2022-05-14T15:44:42.848518Z","shell.execute_reply":"2022-05-14T15:44:42.848543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv = pd.DataFrame(final_res,columns=['id','score'])\nsubmit_csv.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.849815Z","iopub.status.idle":"2022-05-14T15:44:42.850384Z","shell.execute_reply.started":"2022-05-14T15:44:42.85015Z","shell.execute_reply":"2022-05-14T15:44:42.850175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:44:42.851452Z","iopub.status.idle":"2022-05-14T15:44:42.852009Z","shell.execute_reply.started":"2022-05-14T15:44:42.851761Z","shell.execute_reply":"2022-05-14T15:44:42.851786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}