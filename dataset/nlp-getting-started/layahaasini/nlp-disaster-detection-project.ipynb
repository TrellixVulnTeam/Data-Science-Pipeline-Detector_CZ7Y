{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I made this kernel for Kaggle's 'NLP with Disaster Tweets' competition. In this kernel, given thousands of tweets, I tried to identify whether the tweet talks about a disaster or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Contents","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Preliminary steps\n    * Importing the necessary libraries\n    * Converting the CSV file into a pandas dataframe\n* Creating new columns\n* Visualizing the data\n* Encoding the features of the train data\n* Defining the features and prediction target\n* Creating the model\n* Fitting the model\n* Dealing with the test data\n    * Encoding the features of the test data\n* Prediction\n* Ending Note","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preliminary Steps","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing the necessary libraries - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\n\ntqdm.pandas()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the CSV file into a pandas dataframe - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/nlp-getting-started')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A look at the train data - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating new columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Creating 3 new columns - tweet_length, tweet_words and average_word_length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"tweet_length\"] = train_data[\"text\"].progress_apply(len)\ntrain_data[\"tweet_words\"] = train_data[\"text\"].progress_apply(lambda x: len(x.split()))\ntrain_data[\"average_word_length\"] = train_data[\"tweet_length\"]/train_data[\"tweet_words\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A look at the train data with the new columns - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Tweet Words Distributions - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data[\"tweet_words\"], color=\"deeppink\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tweet Length Distribution - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data[\"tweet_length\"], color=\"teal\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average Word Length Distribution - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data[\"average_word_length\"], color=\"darkorchid\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target vs. Tweet Words - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=train_data, x=\"target\", y=\"tweet_words\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target vs. Tweet Length -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=train_data, x=\"target\", y=\"tweet_length\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target vs. Average Tweet Length -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=train_data, x=\"target\", y=\"average_word_length\", palette=[\"turquoise\", \"hotpink\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding the features of the train data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Converting the tweets to vectors - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train_data[\"text\"]).todense()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining the features and prediction target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_vectors/train_vectors.max(axis=1)\ny = train_data[\"target\"].values.reshape((len(train_data), 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the training data into training data and validation data -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dropout(0.85))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Providing the input size to the model -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.build(input_shape=(None, 21637))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=train_X, y=train_y, validation_data=(val_X, val_y), epochs=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with the test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Converting the CSV file into a pandas dataframe - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding the features of the test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Encoding the features of the test data and defining a new variable to hold the features - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vectors = count_vectorizer.transform(test_data[\"text\"]).todense()\nX_test = test_vectors/test_vectors.max(axis=1)\nX_test[np.isnan(X_test)] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.round(model.predict(X_test)).reshape((len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since sample_submission.csv is of the format in which our submission is supposed to be made, I'm first importing it and converting it into a pandas dataframe -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing the 'target' column in the dataframe with the values we got -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"target\"] = np.int32(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A final look at the dataframe with our predictions -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the dataframe into a csv file without the index column -","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ending Note","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Through this project, I learnt about the conversion of text to vectors. I really enjoyed it, and look forward to learning more in the future. This being only my third ml model, I really appreciate feedback to help me improve both the accuracy and efficiency of my model :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}