{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-13T04:39:59.466875Z","iopub.execute_input":"2022-01-13T04:39:59.467241Z","iopub.status.idle":"2022-01-13T04:39:59.483276Z","shell.execute_reply.started":"2022-01-13T04:39:59.467148Z","shell.execute_reply":"2022-01-13T04:39:59.482167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Extracting data and preprocessing :**","metadata":{}},{"cell_type":"code","source":"train_ds = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.485068Z","iopub.execute_input":"2022-01-13T04:39:59.485656Z","iopub.status.idle":"2022-01-13T04:39:59.549191Z","shell.execute_reply.started":"2022-01-13T04:39:59.485614Z","shell.execute_reply":"2022-01-13T04:39:59.548471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.55061Z","iopub.execute_input":"2022-01-13T04:39:59.551124Z","iopub.status.idle":"2022-01-13T04:39:59.581534Z","shell.execute_reply.started":"2022-01-13T04:39:59.551083Z","shell.execute_reply":"2022-01-13T04:39:59.580824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.583Z","iopub.execute_input":"2022-01-13T04:39:59.5832Z","iopub.status.idle":"2022-01-13T04:39:59.597699Z","shell.execute_reply.started":"2022-01-13T04:39:59.583176Z","shell.execute_reply":"2022-01-13T04:39:59.596882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of classes is optimum. No class imbalance found. If found, handle accordingly.","metadata":{}},{"cell_type":"code","source":"train_ds.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.598949Z","iopub.execute_input":"2022-01-13T04:39:59.599269Z","iopub.status.idle":"2022-01-13T04:39:59.61077Z","shell.execute_reply.started":"2022-01-13T04:39:59.599233Z","shell.execute_reply":"2022-01-13T04:39:59.609921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we observe keyword and location variables contain null values. Since, these are not important columns we will be dropping them along with id:","metadata":{}},{"cell_type":"code","source":"train_ds = train_ds.drop(['id','keyword','location'],axis=1)\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.611686Z","iopub.execute_input":"2022-01-13T04:39:59.612911Z","iopub.status.idle":"2022-01-13T04:39:59.625966Z","shell.execute_reply.started":"2022-01-13T04:39:59.612859Z","shell.execute_reply":"2022-01-13T04:39:59.625269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = test_ds.drop(['id','keyword','location'],axis=1)\ntest_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.627019Z","iopub.execute_input":"2022-01-13T04:39:59.627429Z","iopub.status.idle":"2022-01-13T04:39:59.639344Z","shell.execute_reply.started":"2022-01-13T04:39:59.627384Z","shell.execute_reply":"2022-01-13T04:39:59.638664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally, we employ the following steps while preprocessing texts:\n<ol>\n    <li>Tokenising the string</li>\n    <li>Converting characters to lowercase</li>\n    <li>Removing stop words and punctuations</li>\n    <li>Stemming or lemmatization</li>\n</ol>","metadata":{}},{"cell_type":"code","source":"import re                                  \nimport string  \nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer  ","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:39:59.640383Z","iopub.execute_input":"2022-01-13T04:39:59.641075Z","iopub.status.idle":"2022-01-13T04:40:01.114345Z","shell.execute_reply.started":"2022-01-13T04:39:59.641016Z","shell.execute_reply":"2022-01-13T04:40:01.113547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent = []\nfor sentence in train_ds['text']:\n    sent_formatted = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence) #Removes hyperlinks\n    sent_formatted = re.sub(r'#', '', sent_formatted) #Removes hastags\n    sent_formatted = re.sub(r'[0-9]', '', sent_formatted) #Removes numbers\n    sent_formatted = re.sub(r'@[A-Za-z]*', '', sent_formatted) #Removes @ tags\n    sent.append(sent_formatted) ","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:01.115527Z","iopub.execute_input":"2022-01-13T04:40:01.116387Z","iopub.status.idle":"2022-01-13T04:40:01.177379Z","shell.execute_reply.started":"2022-01-13T04:40:01.116345Z","shell.execute_reply":"2022-01-13T04:40:01.176708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = sent[100]\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:01.181432Z","iopub.execute_input":"2022-01-13T04:40:01.181622Z","iopub.status.idle":"2022-01-13T04:40:01.188769Z","shell.execute_reply.started":"2022-01-13T04:40:01.181599Z","shell.execute_reply":"2022-01-13T04:40:01.188038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_sent = []\n\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\nfor sentence in sent:\n    tokenized_sentence = tokenizer.tokenize(sentence)\n    tokenized_sent.append(tokenized_sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:01.191727Z","iopub.execute_input":"2022-01-13T04:40:01.192064Z","iopub.status.idle":"2022-01-13T04:40:02.355213Z","shell.execute_reply.started":"2022-01-13T04:40:01.192034Z","shell.execute_reply":"2022-01-13T04:40:02.354326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = tokenized_sent[100]\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:02.356692Z","iopub.execute_input":"2022-01-13T04:40:02.356997Z","iopub.status.idle":"2022-01-13T04:40:02.362117Z","shell.execute_reply.started":"2022-01-13T04:40:02.356957Z","shell.execute_reply":"2022-01-13T04:40:02.361293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords_english = stopwords.words('english') \n\nprint('Stop words in english : \\n')\nprint(stopwords_english)\n\nprint('\\nPunctuations : \\n')\nprint(string.punctuation)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:02.363699Z","iopub.execute_input":"2022-01-13T04:40:02.36433Z","iopub.status.idle":"2022-01-13T04:40:02.374995Z","shell.execute_reply.started":"2022-01-13T04:40:02.364235Z","shell.execute_reply":"2022-01-13T04:40:02.373919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formatted_sent = []\nfor sentence in tokenized_sent:\n    formatted_words = []\n    for word in sentence:\n        if word not in stopwords_english and word not in string.punctuation and len(word)>2:  #Removes word with less than 2 characters, present in english stop words or is a punctuation\n            formatted_words.append(word)\n    formatted_sent.append(formatted_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:02.376965Z","iopub.execute_input":"2022-01-13T04:40:02.377478Z","iopub.status.idle":"2022-01-13T04:40:02.608796Z","shell.execute_reply.started":"2022-01-13T04:40:02.37744Z","shell.execute_reply":"2022-01-13T04:40:02.608014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = formatted_sent[100]\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:02.61036Z","iopub.execute_input":"2022-01-13T04:40:02.61063Z","iopub.status.idle":"2022-01-13T04:40:02.615484Z","shell.execute_reply.started":"2022-01-13T04:40:02.610594Z","shell.execute_reply":"2022-01-13T04:40:02.614726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma_sent = []\n\nlemma = WordNetLemmatizer()\n\nfor sentence in formatted_sent:\n    lemma_words = []\n    for word in sentence:\n        lemma_word = lemma.lemmatize(word)\n        lemma_words.append(lemma_word)\n    lemma_sent.append(lemma_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:02.616949Z","iopub.execute_input":"2022-01-13T04:40:02.617438Z","iopub.status.idle":"2022-01-13T04:40:04.746163Z","shell.execute_reply.started":"2022-01-13T04:40:02.617401Z","shell.execute_reply":"2022-01-13T04:40:04.745172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = lemma_sent[100]\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.747617Z","iopub.execute_input":"2022-01-13T04:40:04.747909Z","iopub.status.idle":"2022-01-13T04:40:04.752936Z","shell.execute_reply.started":"2022-01-13T04:40:04.747864Z","shell.execute_reply":"2022-01-13T04:40:04.752099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_sentence_list = []\nfor sentence in lemma_sent:\n    sent = ' '.join([str(word) for word in sentence])\n    final_sentence_list.append(sent)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.754396Z","iopub.execute_input":"2022-01-13T04:40:04.754642Z","iopub.status.idle":"2022-01-13T04:40:04.778187Z","shell.execute_reply.started":"2022-01-13T04:40:04.754609Z","shell.execute_reply":"2022-01-13T04:40:04.77742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = final_sentence_list[100]\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.779818Z","iopub.execute_input":"2022-01-13T04:40:04.78037Z","iopub.status.idle":"2022-01-13T04:40:04.78564Z","shell.execute_reply.started":"2022-01-13T04:40:04.780332Z","shell.execute_reply":"2022-01-13T04:40:04.784836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds['FormattedText'] = final_sentence_list","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.787275Z","iopub.execute_input":"2022-01-13T04:40:04.787829Z","iopub.status.idle":"2022-01-13T04:40:04.796115Z","shell.execute_reply.started":"2022-01-13T04:40:04.78779Z","shell.execute_reply":"2022-01-13T04:40:04.795352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.79768Z","iopub.execute_input":"2022-01-13T04:40:04.798233Z","iopub.status.idle":"2022-01-13T04:40:04.81236Z","shell.execute_reply.started":"2022-01-13T04:40:04.798195Z","shell.execute_reply":"2022-01-13T04:40:04.811465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_ds.drop(['text'],axis = 1)\ntrain_ds.rename(columns = {'FormattedText':'text'},inplace = True)\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.813736Z","iopub.execute_input":"2022-01-13T04:40:04.814295Z","iopub.status.idle":"2022-01-13T04:40:04.829043Z","shell.execute_reply.started":"2022-01-13T04:40:04.814257Z","shell.execute_reply":"2022-01-13T04:40:04.828136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Converting the text to a numerical vector format using tensorflow TextVectorizer:","metadata":{}},{"cell_type":"code","source":"X_train = train_ds['text']\ny_train = train_ds['target']\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.830308Z","iopub.execute_input":"2022-01-13T04:40:04.830861Z","iopub.status.idle":"2022-01-13T04:40:04.834731Z","shell.execute_reply.started":"2022-01-13T04:40:04.830824Z","shell.execute_reply":"2022-01-13T04:40:04.833993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_array = X_train.to_numpy()\ny_train_array = y_train.to_numpy()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.836065Z","iopub.execute_input":"2022-01-13T04:40:04.836507Z","iopub.status.idle":"2022-01-13T04:40:04.842667Z","shell.execute_reply.started":"2022-01-13T04:40:04.836469Z","shell.execute_reply":"2022-01-13T04:40:04.841875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_array","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.843699Z","iopub.execute_input":"2022-01-13T04:40:04.844321Z","iopub.status.idle":"2022-01-13T04:40:04.85295Z","shell.execute_reply.started":"2022-01-13T04:40:04.844284Z","shell.execute_reply":"2022-01-13T04:40:04.852169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\ntf.config.run_functions_eagerly(True)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:04.854241Z","iopub.execute_input":"2022-01-13T04:40:04.855118Z","iopub.status.idle":"2022-01-13T04:40:07.938466Z","shell.execute_reply.started":"2022-01-13T04:40:04.85508Z","shell.execute_reply":"2022-01-13T04:40:07.937449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train_array, y_train_array))\n\nfor text,label in train_dataset.take(1):\n    print('Text: ', text.numpy())\n    print('Label: ', label.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:07.940091Z","iopub.execute_input":"2022-01-13T04:40:07.940393Z","iopub.status.idle":"2022-01-13T04:40:08.10603Z","shell.execute_reply.started":"2022-01-13T04:40:07.940352Z","shell.execute_reply":"2022-01-13T04:40:08.105281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = tf.data.Dataset.from_tensor_slices((X_test_array))\n\n# for test_text in test_dataset.take(1):\n#     print('Text: ', test_text.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.111851Z","iopub.execute_input":"2022-01-13T04:40:08.113532Z","iopub.status.idle":"2022-01-13T04:40:08.116968Z","shell.execute_reply.started":"2022-01-13T04:40:08.1135Z","shell.execute_reply":"2022-01-13T04:40:08.115923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 4000\nBATCH_SIZE = 64","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.118517Z","iopub.execute_input":"2022-01-13T04:40:08.119155Z","iopub.status.idle":"2022-01-13T04:40:08.124911Z","shell.execute_reply.started":"2022-01-13T04:40:08.119115Z","shell.execute_reply":"2022-01-13T04:40:08.124215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n# test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.126354Z","iopub.execute_input":"2022-01-13T04:40:08.126983Z","iopub.status.idle":"2022-01-13T04:40:08.134832Z","shell.execute_reply.started":"2022-01-13T04:40:08.126943Z","shell.execute_reply":"2022-01-13T04:40:08.134008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Building:","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = 12000\n\n\n#This layer will only be used in LSTM and GRU architectures for obtaining numerical vector representation of words. \n#For BERT we will use bert spcific vectorization technique.\n\nencoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, target: text))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.136344Z","iopub.execute_input":"2022-01-13T04:40:08.136937Z","iopub.status.idle":"2022-01-13T04:40:08.501518Z","shell.execute_reply.started":"2022-01-13T04:40:08.136892Z","shell.execute_reply":"2022-01-13T04:40:08.500776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = np.array(encoder.get_vocabulary())\nvocabulary[10:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.503088Z","iopub.execute_input":"2022-01-13T04:40:08.503326Z","iopub.status.idle":"2022-01-13T04:40:08.548477Z","shell.execute_reply.started":"2022-01-13T04:40:08.503293Z","shell.execute_reply":"2022-01-13T04:40:08.547604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Original Text :\" +str(text))\nencoded_text = encoder(text).numpy()\nprint(\"Numeric Represenation :\" +str(encoded_text))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.550052Z","iopub.execute_input":"2022-01-13T04:40:08.55034Z","iopub.status.idle":"2022-01-13T04:40:08.5645Z","shell.execute_reply.started":"2022-01-13T04:40:08.550301Z","shell.execute_reply":"2022-01-13T04:40:08.563808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1 Text classification with LSTM:","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:08.567119Z","iopub.execute_input":"2022-01-13T04:40:08.567369Z","iopub.status.idle":"2022-01-13T04:40:10.134081Z","shell.execute_reply.started":"2022-01-13T04:40:08.567342Z","shell.execute_reply":"2022-01-13T04:40:10.133338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:10.136807Z","iopub.execute_input":"2022-01-13T04:40:10.137012Z","iopub.status.idle":"2022-01-13T04:40:10.143825Z","shell.execute_reply.started":"2022-01-13T04:40:10.136987Z","shell.execute_reply":"2022-01-13T04:40:10.142993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:10.145239Z","iopub.execute_input":"2022-01-13T04:40:10.146664Z","iopub.status.idle":"2022-01-13T04:40:10.162138Z","shell.execute_reply.started":"2022-01-13T04:40:10.146621Z","shell.execute_reply":"2022-01-13T04:40:10.161092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nhistory = model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:40:10.163648Z","iopub.execute_input":"2022-01-13T04:40:10.164182Z","iopub.status.idle":"2022-01-13T04:41:41.677851Z","shell.execute_reply.started":"2022-01-13T04:40:10.164142Z","shell.execute_reply":"2022-01-13T04:41:41.677038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 Text classification with stacked LSTMs :","metadata":{}},{"cell_type":"code","source":"stacked_model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,return_sequences=True)),\n    \n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\nstacked_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:41:41.679269Z","iopub.execute_input":"2022-01-13T04:41:41.679534Z","iopub.status.idle":"2022-01-13T04:41:44.632527Z","shell.execute_reply.started":"2022-01-13T04:41:41.679498Z","shell.execute_reply":"2022-01-13T04:41:44.631815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:41:44.634007Z","iopub.execute_input":"2022-01-13T04:41:44.634492Z","iopub.status.idle":"2022-01-13T04:41:44.640646Z","shell.execute_reply.started":"2022-01-13T04:41:44.634448Z","shell.execute_reply":"2022-01-13T04:41:44.639684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:41:44.641839Z","iopub.execute_input":"2022-01-13T04:41:44.642084Z","iopub.status.idle":"2022-01-13T04:41:44.653697Z","shell.execute_reply.started":"2022-01-13T04:41:44.642046Z","shell.execute_reply":"2022-01-13T04:41:44.652981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nstacked_history = stacked_model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:41:44.655265Z","iopub.execute_input":"2022-01-13T04:41:44.655618Z","iopub.status.idle":"2022-01-13T04:44:59.709197Z","shell.execute_reply.started":"2022-01-13T04:41:44.655583Z","shell.execute_reply":"2022-01-13T04:44:59.708279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3 Text classification with GRUs :","metadata":{}},{"cell_type":"code","source":"gru_model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16,return_sequences=True)),\n    \n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\ngru_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:44:59.710985Z","iopub.execute_input":"2022-01-13T04:44:59.711287Z","iopub.status.idle":"2022-01-13T04:45:01.183733Z","shell.execute_reply.started":"2022-01-13T04:44:59.711247Z","shell.execute_reply":"2022-01-13T04:45:01.182923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:45:01.185164Z","iopub.execute_input":"2022-01-13T04:45:01.185631Z","iopub.status.idle":"2022-01-13T04:45:01.191221Z","shell.execute_reply.started":"2022-01-13T04:45:01.185587Z","shell.execute_reply":"2022-01-13T04:45:01.189347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:45:01.192524Z","iopub.execute_input":"2022-01-13T04:45:01.192869Z","iopub.status.idle":"2022-01-13T04:45:01.205905Z","shell.execute_reply.started":"2022-01-13T04:45:01.192813Z","shell.execute_reply":"2022-01-13T04:45:01.205082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\ngru_history = gru_model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:45:01.207121Z","iopub.execute_input":"2022-01-13T04:45:01.207521Z","iopub.status.idle":"2022-01-13T04:46:38.956574Z","shell.execute_reply.started":"2022-01-13T04:45:01.207481Z","shell.execute_reply":"2022-01-13T04:46:38.955782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.4 Text Classification with BERT (Transformer Model):","metadata":{}},{"cell_type":"code","source":"#pip install tensorflow-text","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:38.958215Z","iopub.execute_input":"2022-01-13T04:46:38.958483Z","iopub.status.idle":"2022-01-13T04:46:38.962384Z","shell.execute_reply.started":"2022-01-13T04:46:38.958449Z","shell.execute_reply":"2022-01-13T04:46:38.961641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install tf-models-official","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:38.964079Z","iopub.execute_input":"2022-01-13T04:46:38.964696Z","iopub.status.idle":"2022-01-13T04:46:38.973047Z","shell.execute_reply.started":"2022-01-13T04:46:38.964656Z","shell.execute_reply":"2022-01-13T04:46:38.972215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:38.974694Z","iopub.execute_input":"2022-01-13T04:46:38.974986Z","iopub.status.idle":"2022-01-13T04:46:39.251866Z","shell.execute_reply.started":"2022-01-13T04:46:38.974949Z","shell.execute_reply":"2022-01-13T04:46:39.251029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n\n#Note: You can get these bert model and tfhub details on the tensorflow classify text with BERT page\n\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\ntfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:39.253197Z","iopub.execute_input":"2022-01-13T04:46:39.25385Z","iopub.status.idle":"2022-01-13T04:46:39.260132Z","shell.execute_reply.started":"2022-01-13T04:46:39.253804Z","shell.execute_reply":"2022-01-13T04:46:39.258824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier_model():\n    \n    #Pretrained BERT \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='vectorizing')\n    encoder_inputs = preprocessing_layer(text_input)\n    bert = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT')\n    outputs = bert(encoder_inputs)\n    \n    #Our own custom classification network\n    custom = outputs['pooled_output']\n    custom = tf.keras.layers.Dropout(0.1)(custom)\n    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(custom)\n    \n    return tf.keras.Model(text_input, classifier)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:39.261872Z","iopub.execute_input":"2022-01-13T04:46:39.262141Z","iopub.status.idle":"2022-01-13T04:46:39.271855Z","shell.execute_reply.started":"2022-01-13T04:46:39.262105Z","shell.execute_reply":"2022-01-13T04:46:39.271062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = classifier_model()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:39.273196Z","iopub.execute_input":"2022-01-13T04:46:39.273635Z","iopub.status.idle":"2022-01-13T04:46:50.916925Z","shell.execute_reply.started":"2022-01-13T04:46:39.273594Z","shell.execute_reply":"2022-01-13T04:46:50.916173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:50.918363Z","iopub.execute_input":"2022-01-13T04:46:50.918631Z","iopub.status.idle":"2022-01-13T04:46:50.932227Z","shell.execute_reply.started":"2022-01-13T04:46:50.918597Z","shell.execute_reply":"2022-01-13T04:46:50.931554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nsteps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\n#You can also try to use Adam optimizer. But when it comes to transformer based models, it is best to fine-tune them using the same parameters as their pretraining.","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:50.93447Z","iopub.execute_input":"2022-01-13T04:46:50.934702Z","iopub.status.idle":"2022-01-13T04:46:50.943042Z","shell.execute_reply.started":"2022-01-13T04:46:50.934672Z","shell.execute_reply":"2022-01-13T04:46:50.942283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer=optimizer,\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:50.944439Z","iopub.execute_input":"2022-01-13T04:46:50.944702Z","iopub.status.idle":"2022-01-13T04:46:50.95611Z","shell.execute_reply.started":"2022-01-13T04:46:50.944667Z","shell.execute_reply":"2022-01-13T04:46:50.95539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_history = bert_model.fit(train_dataset,epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T04:46:50.957564Z","iopub.execute_input":"2022-01-13T04:46:50.958099Z","iopub.status.idle":"2022-01-13T06:49:23.264379Z","shell.execute_reply.started":"2022-01-13T04:46:50.958061Z","shell.execute_reply":"2022-01-13T06:49:23.263562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preparing test data for submission:","metadata":{}},{"cell_type":"code","source":"submission_ds = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:23.26603Z","iopub.execute_input":"2022-01-13T06:49:23.266295Z","iopub.status.idle":"2022-01-13T06:49:23.292365Z","shell.execute_reply.started":"2022-01-13T06:49:23.266259Z","shell.execute_reply":"2022-01-13T06:49:23.291659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent = []\nfor sentence in test_ds['text']:\n    sent_formatted = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n    sent_formatted = re.sub(r'#', '', sent_formatted)\n    sent_formatted = re.sub(r'[0-9]', '', sent_formatted)\n    sent_formatted = re.sub(r'@[A-Za-z]*', '', sent_formatted)\n    sent.append(sent_formatted) ","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:23.293565Z","iopub.execute_input":"2022-01-13T06:49:23.293834Z","iopub.status.idle":"2022-01-13T06:49:23.324716Z","shell.execute_reply.started":"2022-01-13T06:49:23.293798Z","shell.execute_reply":"2022-01-13T06:49:23.323951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_sent = []\n\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\nfor sentence in sent:\n    tokenized_sentence = tokenizer.tokenize(sentence)\n    tokenized_sent.append(tokenized_sentence)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:23.327684Z","iopub.execute_input":"2022-01-13T06:49:23.327929Z","iopub.status.idle":"2022-01-13T06:49:23.761134Z","shell.execute_reply.started":"2022-01-13T06:49:23.327886Z","shell.execute_reply":"2022-01-13T06:49:23.760378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formatted_sent = []\nfor sentence in tokenized_sent:\n    formatted_words = []\n    for word in sentence:\n        if word not in stopwords_english and word not in string.punctuation and len(word)>2:\n            formatted_words.append(word)\n    formatted_sent.append(formatted_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:23.76252Z","iopub.execute_input":"2022-01-13T06:49:23.762781Z","iopub.status.idle":"2022-01-13T06:49:23.862345Z","shell.execute_reply.started":"2022-01-13T06:49:23.762731Z","shell.execute_reply":"2022-01-13T06:49:23.861712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma_sent = []\n\nlemma = WordNetLemmatizer()\n\nfor sentence in formatted_sent:\n    lemma_words = []\n    for word in sentence:\n        lemma_word = lemma.lemmatize(word)\n        lemma_words.append(lemma_word)\n    lemma_sent.append(lemma_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:23.863693Z","iopub.execute_input":"2022-01-13T06:49:23.863968Z","iopub.status.idle":"2022-01-13T06:49:24.006087Z","shell.execute_reply.started":"2022-01-13T06:49:23.863934Z","shell.execute_reply":"2022-01-13T06:49:24.005416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_sentence_list = []\nfor sentence in lemma_sent:\n    sent = ' '.join([str(word) for word in sentence])\n    final_sentence_list.append(sent)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.007407Z","iopub.execute_input":"2022-01-13T06:49:24.00765Z","iopub.status.idle":"2022-01-13T06:49:24.018967Z","shell.execute_reply.started":"2022-01-13T06:49:24.007615Z","shell.execute_reply":"2022-01-13T06:49:24.018262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds['text'] = final_sentence_list\ntest_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.020325Z","iopub.execute_input":"2022-01-13T06:49:24.021294Z","iopub.status.idle":"2022-01-13T06:49:24.036348Z","shell.execute_reply.started":"2022-01-13T06:49:24.021257Z","shell.execute_reply":"2022-01-13T06:49:24.035653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_ds['text']\nX_test_array = X_test.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.03783Z","iopub.execute_input":"2022-01-13T06:49:24.038087Z","iopub.status.idle":"2022-01-13T06:49:24.043536Z","shell.execute_reply.started":"2022-01-13T06:49:24.038051Z","shell.execute_reply":"2022-01-13T06:49:24.04281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_array","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.044788Z","iopub.execute_input":"2022-01-13T06:49:24.045227Z","iopub.status.idle":"2022-01-13T06:49:24.052996Z","shell.execute_reply.started":"2022-01-13T06:49:24.045187Z","shell.execute_reply":"2022-01-13T06:49:24.052265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = tf.data.Dataset.from_tensor_slices((X_test_array))\n\nfor test_text in test_dataset.take(2):\n    print('Text: ', test_text.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.054339Z","iopub.execute_input":"2022-01-13T06:49:24.054657Z","iopub.status.idle":"2022-01-13T06:49:24.069838Z","shell.execute_reply.started":"2022-01-13T06:49:24.054624Z","shell.execute_reply":"2022-01-13T06:49:24.069122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.071013Z","iopub.execute_input":"2022-01-13T06:49:24.071322Z","iopub.status.idle":"2022-01-13T06:49:24.077136Z","shell.execute_reply.started":"2022-01-13T06:49:24.071285Z","shell.execute_reply":"2022-01-13T06:49:24.076246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoded_text = []\n\n# for test_text in test_dataset:\n#     encoded_text.append(encoder(test_text).numpy())\n\n# # encoded_text\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.078572Z","iopub.execute_input":"2022-01-13T06:49:24.079145Z","iopub.status.idle":"2022-01-13T06:49:24.084119Z","shell.execute_reply.started":"2022-01-13T06:49:24.079108Z","shell.execute_reply":"2022-01-13T06:49:24.083273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = bert_model.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:49:24.085668Z","iopub.execute_input":"2022-01-13T06:49:24.085997Z","iopub.status.idle":"2022-01-13T06:51:48.245187Z","shell.execute_reply.started":"2022-01-13T06:49:24.085965Z","shell.execute_reply":"2022-01-13T06:51:48.244435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor i in y_pred:\n    if i >= 0:\n        result.append(1)\n    else:\n        result.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:51:48.246438Z","iopub.execute_input":"2022-01-13T06:51:48.248143Z","iopub.status.idle":"2022-01-13T06:51:48.26075Z","shell.execute_reply.started":"2022-01-13T06:51:48.248101Z","shell.execute_reply":"2022-01-13T06:51:48.260038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ds['target'] = result\nsubmission_ds","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:51:48.262212Z","iopub.execute_input":"2022-01-13T06:51:48.262483Z","iopub.status.idle":"2022-01-13T06:51:48.27641Z","shell.execute_reply.started":"2022-01-13T06:51:48.262449Z","shell.execute_reply":"2022-01-13T06:51:48.275615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ds['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:51:48.277683Z","iopub.execute_input":"2022-01-13T06:51:48.278009Z","iopub.status.idle":"2022-01-13T06:51:48.285828Z","shell.execute_reply.started":"2022-01-13T06:51:48.277974Z","shell.execute_reply":"2022-01-13T06:51:48.284765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ds.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T06:51:48.287442Z","iopub.execute_input":"2022-01-13T06:51:48.287699Z","iopub.status.idle":"2022-01-13T06:51:48.302294Z","shell.execute_reply.started":"2022-01-13T06:51:48.287665Z","shell.execute_reply":"2022-01-13T06:51:48.301641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Please upvote this notebook if you find it helpful.","metadata":{}},{"cell_type":"markdown","source":"References: \nhttps://www.tensorflow.org/text/tutorials/classify_text_with_bert","metadata":{}}]}