{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom tqdm.notebook import tqdm\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.random import set_seed\nfrom matplotlib import pyplot as plt\n\ntqdm.pandas()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading data\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emojies(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\ndef remove_urls(text):\n    return re.sub(r\"http\\S+\", \"\", text)\n\ndef remove_all_non_alpha(text):\n    return re.sub(r'[^a-zA-Z ]+', '', text)\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef lem_stem_rm_stopword(text):\n    word_tokens = word_tokenize(text)\n    final_sentence = []\n    for word in word_tokens:\n        if word in stop_words:\n            continue\n        preprocessed_word = lemmatizer.lemmatize(word)\n        preprocessed_word = lemmatizer.lemmatize(preprocessed_word, pos=\"v\")\n        preprocessed_word = stemmer.stem(preprocessed_word)\n        \n        final_sentence.append(preprocessed_word)\n    \n    return \" \".join(final_sentence)\n\n\ndef preprocess_sent(sent):\n    sent = remove_emojies(sent)\n    sent = remove_urls(sent)\n    sent = sent.lower()\n    sent = remove_all_non_alpha(sent)\n    sent = lem_stem_rm_stopword(sent)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_sent('Anith is a $good #man vist his website http:\\\\www.anith.com and https:\\\\www.anith.com')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing text\ntrain_df['preprocessed_text'] = train_df['text'].progress_map(preprocess_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting texts from data\ntexts = train_df['preprocessed_text']\nY = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Setting hyperparameters\nvocab_size = 15000\nmax_sent_len = 80\nrandom_seed = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting random seed for numpy and tensorflow for reproduceability\nseed(random_seed)\nset_seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data into training and testing\ntexts_train, texts_valid, y_train, y_valid = train_test_split(texts, Y, random_state = random_seed, stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing text using keras tokenizer on training data\ntknzr = Tokenizer(oov_token='<OOV>', lower=True, num_words=vocab_size)\ntknzr.fit_on_texts(texts_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating sequences of texts\ntrain_seqs = tknzr.texts_to_sequences(texts_train)\nvalid_seqs = tknzr.texts_to_sequences(texts_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# padding and truncating se\ntrain_seqs_pad = pad_sequences(train_seqs, padding='post', truncating='post', maxlen=max_sent_len)\nvalid_seqs_pad = pad_sequences(valid_seqs, padding='post', truncating='post', maxlen=max_sent_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model creation\nmodel = Sequential(\n[\n    Embedding(vocab_size, 32),\n    Bidirectional(LSTM(16, return_sequences=True)),\n    Bidirectional(LSTM(16, activity_regularizer='l2')),\n    Dropout(0.6),\n    Dense(32, activation='relu',activity_regularizer='l2'),\n    Dropout(0.6),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compiling model\nmodel.compile(loss='binary_crossentropy', metrics=['acc'])\n\n# Scheduling learning rate to decay as epochs progress\ndef scheduler(epoch, lr):\n    return lr * 0.65\n\nlr_schedule = LearningRateScheduler(scheduler)\n\nhistory = model.fit(train_seqs_pad, y_train, \n          batch_size=32, epochs=20, \n          validation_data=(valid_seqs_pad, y_valid), callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting tarining and validation accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.legend(['acc', 'val_acc'])\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.title('Accuracy trend')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing test data\ntest_df['preprocessed_text'] = test_df.text.map(preprocess_sent)\ntest_text = test_df['preprocessed_text'].values\ntest_seqs = tknzr.texts_to_sequences(test_text)\ntest_seqs_pad = pad_sequences(test_seqs, padding='post', truncating='post', maxlen=max_sent_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting and creating submission file\ntest_preds = model.predict_classes(test_seqs_pad)\ntest_preds = test_preds.reshape((len(test_preds),))\ntest_df['target'] = test_preds\ntest_df[['id', 'target']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}