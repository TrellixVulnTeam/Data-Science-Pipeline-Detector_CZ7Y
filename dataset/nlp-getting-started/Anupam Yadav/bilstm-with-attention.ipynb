{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Text classification using attention (Seq to One)\nWe use a bidirectional LSTM as encoder and an attention layer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchtext\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nimport re\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.nn.utils.rnn import pad_sequence\nimport pytorch_lightning as pl\nfrom torch.nn.functional import binary_cross_entropy_with_logits, binary_cross_entropy\nfrom torchmetrics import Accuracy, F1Score\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nimport string\nimport statistics\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:37:58.734489Z","iopub.execute_input":"2022-05-28T06:37:58.735299Z","iopub.status.idle":"2022-05-28T06:38:07.107018Z","shell.execute_reply.started":"2022-05-28T06:37:58.735209Z","shell.execute_reply":"2022-05-28T06:38:07.106033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.108887Z","iopub.execute_input":"2022-05-28T06:38:07.109388Z","iopub.status.idle":"2022-05-28T06:38:07.322793Z","shell.execute_reply.started":"2022-05-28T06:38:07.109355Z","shell.execute_reply":"2022-05-28T06:38:07.32202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration for training","metadata":{}},{"cell_type":"code","source":"class MODEL_EVAL_METRIC:\n    accuracy = \"accuracy\"\n    f1_score = \"f1_score\"\n\nclass Config:\n    VOCAB_SIZE = 0\n    BATCH_SIZE = 256\n    EMB_SIZE = 300\n    OUT_SIZE = 2\n    NUM_FOLDS = 5\n    NUM_EPOCHS = 20\n    NUM_WORKERS = 8\n    # Whether to update the pretrained embedding weights during training process\n    EMB_WT_UPDATE = True\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n    FAST_DEV_RUN = False    \n    PATIENCE = 6    \n    IS_BIDIRECTIONAL = True\n    # model hyperparameters\n    MODEL_HPARAMS = {\n        \"hidden_size\": 141, \n        \"num_layers\": 2,         \n        \"drop_out\": 0.4258,\n        \"lr\": 0.000366,\n        \"weight_decay\": 0.00001\n    }\n\n# For results reproducibility \n# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\npl.seed_everything(42, workers=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.32431Z","iopub.execute_input":"2022-05-28T06:38:07.324986Z","iopub.status.idle":"2022-05-28T06:38:07.390076Z","shell.execute_reply.started":"2022-05-28T06:38:07.324942Z","shell.execute_reply":"2022-05-28T06:38:07.38937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nprint(f\"Rows in train.csv = {len(df_train)}\")\nprint(f\"Rows in test.csv = {len(df_test)}\")\npd.set_option('display.max_colwidth', None)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.39216Z","iopub.execute_input":"2022-05-28T06:38:07.39249Z","iopub.status.idle":"2022-05-28T06:38:07.45812Z","shell.execute_reply.started":"2022-05-28T06:38:07.392456Z","shell.execute_reply":"2022-05-28T06:38:07.457399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some EDA","metadata":{}},{"cell_type":"code","source":"df_train_pos = df_train[df_train.target == 1]\ndf_train_neg = df_train[df_train.target == 0]\nprint(f\"No. of positive training examples = {len(df_train_pos)}\")\nprint(f\"No. of negative training examples = {len(df_train_neg)}\")\ntrain_keywords_unique = df_train.keyword.unique()\nprint(f\"No. of unique keywords = {len(train_keywords_unique)}\")\ndf_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\nprint(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.459226Z","iopub.execute_input":"2022-05-28T06:38:07.459647Z","iopub.status.idle":"2022-05-28T06:38:07.478012Z","shell.execute_reply.started":"2022-05-28T06:38:07.459613Z","shell.execute_reply":"2022-05-28T06:38:07.477185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Fold CV","metadata":{}},{"cell_type":"code","source":"# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n# on the data. We use stratified kfold if the target distribution is unbalanced\ndef strat_kfold_dataframe(df, target_col_name, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    # get the target data\n    y = df[\"target\"].values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_index, \"kfold\"] = fold\n    return df        \n\ndf_train = strat_kfold_dataframe(df_train, target_col_name=\"target\", num_folds=5)    ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.47923Z","iopub.execute_input":"2022-05-28T06:38:07.479494Z","iopub.status.idle":"2022-05-28T06:38:07.497974Z","shell.execute_reply.started":"2022-05-28T06:38:07.479472Z","shell.execute_reply":"2022-05-28T06:38:07.497331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tweet preprocessing","metadata":{}},{"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\ndef clean_special_chars(text, punct):\n    for p in punct:\n        text = text.replace(p, ' ')\n    return text\n\ndef process_tweet(df, text, keyword):\n    lemmatizer = WordNetLemmatizer()    \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n    processed_text = []\n    stop = stopwords.words(\"english\")\n    for tweet, keyword in zip(df[text], df[keyword]):\n        tweets_clean = []        \n        # remove stock market tickers like $GE\n        #tweet = tweet + \" \" + keyword\n        tweet = re.sub(r'\\$\\w*', '', tweet)\n        # remove old style retweet text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n        # remove hyperlinks\n        tweet = re.sub(r'http\\S+', '', tweet)\n        # remove hashtags\n        # only removing the hash #, @, ... sign from the word\n        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n        tweet = clean_special_chars(tweet, punct)\n        # remove junk characters which don't have an ascii code\n        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n        # tokenize tweets        \n        tweet_tokens = tokenizer.tokenize(tweet)\n        for word in tweet_tokens:\n            # remove stopwords and punctuation\n            #if (word.isalpha() and len(word) > 2 and word not in stop \n            #    and word not in string.punctuation):\n                #stem_word = stemmer.stem(word)  # stemming word            \n                #lem_word = lemmatizer.lemmatize(word)\n                #tweets_clean.append(lem_word) \n                tweets_clean.append(word)\n        processed_text.append(\" \".join(tweets_clean))        \n    df['processed_text'] = np.array(processed_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.499254Z","iopub.execute_input":"2022-05-28T06:38:07.499602Z","iopub.status.idle":"2022-05-28T06:38:07.509321Z","shell.execute_reply.started":"2022-05-28T06:38:07.499566Z","shell.execute_reply":"2022-05-28T06:38:07.508603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill in missing values\ndf_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\ndf_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\nprocess_tweet(df_train, 'text', \"keyword\")\nprocess_tweet(df_test, 'text', \"keyword\")\n# length of the processed tweet\ndf_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_train.iloc[50:52, :]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:07.510646Z","iopub.execute_input":"2022-05-28T06:38:07.511144Z","iopub.status.idle":"2022-05-28T06:38:09.33112Z","shell.execute_reply.started":"2022-05-28T06:38:07.511109Z","shell.execute_reply":"2022-05-28T06:38:09.330394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model building starts from here","metadata":{}},{"cell_type":"code","source":"# If you want to load the word embeddings from an already downloaded word embedding file\nemb = torchtext.vocab.Vectors(name=\"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\", cache=\"./vector_cache\")\n# Use below line to download and use fasttext embeddings from internet\n#emb = torchtext.vocab.FastText(language=\"en\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:38:09.332401Z","iopub.execute_input":"2022-05-28T06:38:09.332743Z","iopub.status.idle":"2022-05-28T06:43:10.23925Z","shell.execute_reply.started":"2022-05-28T06:38:09.33271Z","shell.execute_reply":"2022-05-28T06:43:10.238451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build tweets vocab from training data\ndef yield_tokens(df):\n    for index, row in df.iterrows():\n        yield row[\"processed_text\"].split()\n    \ntweet_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])   \nConfig.VOCAB_SIZE = len(tweet_vocab)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:10.274055Z","iopub.execute_input":"2022-05-28T06:43:10.274411Z","iopub.status.idle":"2022-05-28T06:43:10.898113Z","shell.execute_reply.started":"2022-05-28T06:43:10.274381Z","shell.execute_reply":"2022-05-28T06:43:10.897214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For the problem specific vocab, get the embedding vectors from the pre-trained embedding\n# for each word in vocab and return a matrix of shape vocab_size, embedding_dim. This matrix\n# will be the pretrained embedding weight matrix which we will use to create the embedding layer\ndef get_vocab_pt_emb_matrix(text_vocab, emb):\n    embedding_matrix = []\n    for token in text_vocab.get_itos():\n        embedding_matrix.append(emb[token])\n    return torch.stack(embedding_matrix)\n\npt_emb_weights = get_vocab_pt_emb_matrix(tweet_vocab, emb)\npt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:10.899379Z","iopub.execute_input":"2022-05-28T06:43:10.899773Z","iopub.status.idle":"2022-05-28T06:43:11.893945Z","shell.execute_reply.started":"2022-05-28T06:43:10.899734Z","shell.execute_reply":"2022-05-28T06:43:11.893119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize the processed tweet, i.e. replace each token in the tweet with its corresponding index\n# in the tweet vocab\ndf_train[\"vectorized_tweet\"] = df_train[\"processed_text\"].apply(\n    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:11.895367Z","iopub.execute_input":"2022-05-28T06:43:11.895762Z","iopub.status.idle":"2022-05-28T06:43:12.884608Z","shell.execute_reply.started":"2022-05-28T06:43:11.895722Z","shell.execute_reply":"2022-05-28T06:43:12.883674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tweet dataset","metadata":{}},{"cell_type":"code","source":"class VectorizedTweetDataSet(Dataset):\n    def __init__(self, tweet_vecs, labels):\n        self.tweet_vecs = tweet_vecs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        tweet_vec = self.tweet_vecs[idx]\n        label = self.labels[idx]\n        tweet_len = len(tweet_vec)\n        return (tweet_vec, label)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:12.886474Z","iopub.execute_input":"2022-05-28T06:43:12.887034Z","iopub.status.idle":"2022-05-28T06:43:14.227443Z","shell.execute_reply.started":"2022-05-28T06:43:12.886999Z","shell.execute_reply":"2022-05-28T06:43:14.226555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get train and validation data for a fold","metadata":{}},{"cell_type":"code","source":"def get_fold_dls(fold, df):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    X_train = train_df[\"vectorized_tweet\"].to_numpy()\n    y_train = train_df[\"target\"].to_numpy()\n    X_valid = valid_df[\"vectorized_tweet\"].to_numpy()\n    y_valid = valid_df[\"target\"].to_numpy()\n    ds_train = VectorizedTweetDataSet(X_train, y_train)\n    ds_valid = VectorizedTweetDataSet(X_valid, y_valid)\n    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n    return dl_train, dl_valid","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.229353Z","iopub.execute_input":"2022-05-28T06:43:14.230264Z","iopub.status.idle":"2022-05-28T06:43:14.249951Z","shell.execute_reply.started":"2022-05-28T06:43:14.230221Z","shell.execute_reply":"2022-05-28T06:43:14.248801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pad the input sequence","metadata":{}},{"cell_type":"code","source":"# If the goal is to train with mini-batches, one needs to pad the sequences in each batch. \n# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n# one needs to pad every sequence with a length of smaller than L with zeros and make their \n# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n# descending order.\ndef pad_collate(batch):\n    # Each element in the batch is a tuple (data, label)\n    # sort the batch (based on tweet word count) in descending order\n    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n    sequences = [x[0] for x in sorted_batch]\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    # Also need to store the length of each sequence.This is later needed in order to unpad \n    # the sequences\n    seq_len = torch.Tensor([len(x) for x in sequences])\n    labels = torch.Tensor([x[1] for x in sorted_batch])\n    return sequences_padded, seq_len, labels\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.254422Z","iopub.execute_input":"2022-05-28T06:43:14.257446Z","iopub.status.idle":"2022-05-28T06:43:14.267563Z","shell.execute_reply.started":"2022-05-28T06:43:14.257407Z","shell.execute_reply":"2022-05-28T06:43:14.266558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model architecture\n![Model architecture](https://github.com/bk-anupam/NLP/blob/master/Kaggle/DisasterTweetsPrediction/images/temp.jpg?raw=True)","metadata":{}},{"cell_type":"markdown","source":"### Build the model \nEncoder (biLSTM) => Attention layer => Fully connected layer => Sigmoid \n\n**Bidirectional RNN as encoder** <br>\noutputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other. \n\nhidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n\nThe bidirectional rnn encoder returns the hidden state from each time step as well as the final hidden state (last time step). \n\n**Attention layer** <br>\nTakes as input encoder outputs ( hidden state from each time step of last rnn layer) as well encoder final hidden state (fom the last time step).\nencoder_outputs + enc_final_hidden_state => alignment_score (use one of the methods below. We use concat and dot product implementation of alignment score)\n\n![Attention types](https://github.com/bk-anupam/NLP/blob/master/Kaggle/DisasterTweetsPrediction/images/align_score.jpg?raw=True)\n\nSoftmax(alignment_score) => attention weights <br>\nattention weights * encoder outputs => context vector <br>\nContext vector has dimensions batch_size, hidden_size and is return by the attention layer <br>\n\n**FC layer** <br>\nInput => Context vector (batch_size, hidden_size) <br>\nOutput => batch_size, out_size","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):    \n    def __init__(self, params, hparams):\n        super().__init__()                \n        self.num_layers = hparams[\"num_layers\"]\n        self.hidden_size = hparams[\"hidden_size\"]\n        self.is_bidirect = params[\"is_bidirect\"]\n        self.num_directions = 2 if self.is_bidirect else 1\n        # Embedding layer\n        self.emb_layer = nn.Embedding(params[\"vocab_size\"], params[\"emb_size\"])\n        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n        self.emb_layer.weight.data.copy_(params[\"pt_emb_weights\"])\n        # whether to update the pretrained embedding layer weights during model training\n        self.emb_layer.weight.requires_grad = params[\"emb_wt_update\"] \n        # LSTM Layer        \n        self.lstm_layer = nn.LSTM(\n                        input_size=params[\"emb_size\"], \n                        hidden_size=self.hidden_size, \n                        batch_first=True, \n                        bidirectional=self.is_bidirect, \n                        num_layers=self.num_layers, \n                        dropout=hparams[\"drop_out\"]\n                        )\n        \n    def forward(self, inputs, input_lengths, state):        \n        # inputs = [batch_size, batch_max_seq_length]        \n        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n        embeds = self.emb_layer(inputs)        \n        # final hidden state (from last time step)\n        h_final = None        \n        # embeds = [batch_size, max_seq_length, emb_dim]        \n        embeds_pack = pack_padded_sequence(embeds, input_lengths.to(\"cpu\"), batch_first=True)                \n        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n        # unpack the output\n        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)        \n        # print(f\"lstm_out.shape = {lstm_out.shape}\") # [batch_size, max_seq_length, hidden_size * num_directions]        \n        if self.is_bidirect:                        \n            h_tend_fwd = h_n[-2, :, :]\n            h_tend_bwd = h_n[-1, :, :]\n            h_final = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)            \n        else:                        \n            h_final = h_n[-1, :, :]   \n\n        # print(f\"h_final.shape = {h_final.shape}\") # [batch_size, hidden_size * num_directions]\n        return lstm_out, lstm_out_len, h_final\n\n    def init_state(self, batch_size=1):\n        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n        to zero\"\"\"\n        if not isinstance(self.lstm_layer, nn.LSTM):\n            # `nn.GRU` takes a tensor as hidden state\n            return torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n        else:\n            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n            # hidden state for each element in the batch, c0 = initial cell state\n            # for each element in the batch\n            return (torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n                    torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.273049Z","iopub.execute_input":"2022-05-28T06:43:14.275913Z","iopub.status.idle":"2022-05-28T06:43:14.299204Z","shell.execute_reply.started":"2022-05-28T06:43:14.275846Z","shell.execute_reply":"2022-05-28T06:43:14.298419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionMethod:\n    CONCAT = \"concat\"\n    DOT = \"dot\"\n    GENERAL = \"general\"","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.304252Z","iopub.execute_input":"2022-05-28T06:43:14.307129Z","iopub.status.idle":"2022-05-28T06:43:14.313211Z","shell.execute_reply.started":"2022-05-28T06:43:14.307089Z","shell.execute_reply":"2022-05-28T06:43:14.312282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionLayer(nn.Module):\n    def __init__(self, hidden_size, attn_method = AttentionMethod.CONCAT):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.attn_method = attn_method   \n        self.attn = None\n        self.v = None\n        self.concat_linear = None\n        if self.attn_method == AttentionMethod.CONCAT:     \n            self.attn = nn.Linear((hidden_size * 2) + (hidden_size * 2), hidden_size)\n            self.v = nn.Linear(hidden_size, 1, bias=False)\n        if self.attn_method == AttentionMethod.DOT:\n            self.attn = nn.Linear((hidden_size * 2) + (hidden_size * 2), hidden_size)\n\n    def attn_concat(self, encoder_outputs, enc_final_hidden_state):        \n        batch_size, seq_length, _ = encoder_outputs.shape        \n        # add seq_length dim to enc_final_hiddden_state\n        enc_final_hidden_state = enc_final_hidden_state.unsqueeze(1)\n        # enc_final_hidden_state.shape = [batch_size, 1, enc_hidden_size * 2]\n        # now repeat the final hidden state seq_length times across dim 1 so final hidden state and encoder outputs have same dimensions\n        enc_final_hidden_state = enc_final_hidden_state.repeat(1, seq_length, 1)\n        # print(f\"enc_final_hidden_state.shape = {enc_final_hidden_state.shape}\") # [batch_size, seq_length, enc_hidden_size * 2]\n        # concat the enc final hidden state and the encoder outputs (which are nothing but enc hidden states at individual time steps)\n        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, enc_final_hidden_state), dim=2)))\n        # print(f\"energy.shape = {energy.shape}\") # [batch_size, seq_length, enc_hidden_size]\n        # get attention vector corresponding to each source time step\n        attention = self.v(energy).squeeze(2)\n        return attention\n\n    def attn_dot(self, encoder_outputs, enc_final_hidden_state):        \n        attention = torch.bmm(encoder_outputs, enc_final_hidden_state.unsqueeze(2))\n        # print(f\"attention.shape = {attention.shape}\") # [batch_size, seq_length, 1]\n        return attention.squeeze(2)\n\n    def forward(self, encoder_outputs, enc_final_hidden_state):        \n        # print(f\"encoder_outputs.shape = {encoder_outputs.shape}\") #[batch_size, seq_length, enc_hidden_size * num_directions]\n        # print(f\"enc_final_hidden_state.shape = {enc_final_hidden_state.shape}\") # [batch_size, enc_hidden_size * num_directions]        \n        attention = None\n        if self.attn_method == AttentionMethod.CONCAT:\n            attention = self.attn_concat(encoder_outputs, enc_final_hidden_state)\n        elif self.attn_method == AttentionMethod.DOT:\n            attention = self.attn_dot(encoder_outputs, enc_final_hidden_state)            \n        # print(f\"(attention.shape = {attention.shape}\") # [batch_size, seq_length]\n        attn_weights = F.softmax(attention, dim=1)\n        # print(f\"(attn_weights.shape = {attn_weights.shape}\") # [batch_size, seq_length]\n        attn_weights = attn_weights.unsqueeze(1)\n        # print(f\"(attn_weights.shape = {attn_weights.shape}\") # [batch_size, 1, seq_length]\n        # apply attention weights to encoder outputs to get context vector\n        context_vector = torch.bmm(attn_weights, encoder_outputs)\n        # print(f\"(context_vector.shape = {context_vector.shape}\") # [batch_size, 1, enc_hidden_size * 2]\n        # attn_hidden = torch.tanh(self.attn(torch.cat((context_vector.squeeze(1), enc_final_hidden_state), dim=1)))\n        # print(f\"attn_hidden.shape = {attn_hidden.shape}\") # [batch_size, hidden_size]\n        return context_vector.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.317889Z","iopub.execute_input":"2022-05-28T06:43:14.320159Z","iopub.status.idle":"2022-05-28T06:43:14.343952Z","shell.execute_reply.started":"2022-05-28T06:43:14.32012Z","shell.execute_reply":"2022-05-28T06:43:14.343312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RnnAttnClassifier(nn.Module):\n    def __init__(self, params, hparams):\n        super().__init__()\n        self.encoder = Encoder(params, hparams)\n        self.attention_layer = AttentionLayer(hparams[\"hidden_size\"], params[\"attn_method\"])\n        self.fc = None\n        if params[\"attn_method\"] == AttentionMethod.CONCAT:\n            self.fc = nn.Linear(hparams[\"hidden_size\"] * 2, 2)\n        elif params[\"attn_method\"] == AttentionMethod.DOT:\n            self.fc = nn.Linear(hparams[\"hidden_size\"] * 2, 2)\n        self.act = nn.Sigmoid()\n\n    def forward(self, inputs, input_lengths, state):\n        enc_out, enc_out_len, enc_h_final = self.encoder(inputs, input_lengths, state)\n        ctx_vec = self.attention_layer(enc_out, enc_h_final)\n        out = self.fc(ctx_vec)\n        return self.act(out)\n\n    def init_state(self):\n        return self.encoder.init_state()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.349735Z","iopub.execute_input":"2022-05-28T06:43:14.350046Z","iopub.status.idle":"2022-05-28T06:43:14.782032Z","shell.execute_reply.started":"2022-05-28T06:43:14.350021Z","shell.execute_reply":"2022-05-28T06:43:14.780961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pytorch lightning wrapper for model","metadata":{}},{"cell_type":"code","source":"class DisasterTweetLitModel(pl.LightningModule):\n    def __init__(self, params, hparams, model_eval_metric=MODEL_EVAL_METRIC.accuracy):\n        super().__init__()\n        #self.save_hyperparameters()\n        self.lr = hparams[\"lr\"]\n        self.weight_decay = hparams[\"weight_decay\"]\n        self.model_eval_metric = model_eval_metric\n        self.network = RnnAttnClassifier(params, hparams)            \n\n    def forward(self, tweets, tweet_lengths, state):\n        return self.network(tweets, tweet_lengths, state)\n\n    def configure_optimizers(self):\n        model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n        return {\n            \"optimizer\": model_optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n\n    def training_step(self, batch, batch_idx):\n        tweets, tweet_lengths, targets = batch\n        # initialize the hidden and cell state of the LSTM\n        h0, c0 = self.network.init_state()\n        targets_pred = self(tweets, tweet_lengths, (h0, c0))        \n        #print(f\"targets_pred.shape = {targets_pred.shape}\")\n        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n        loss_targets = loss_targets.float()        \n        train_loss = binary_cross_entropy(targets_pred, loss_targets)\n        train_metric = None\n        train_metric_str = \"\"\n        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n            targets_pred = torch.argmax(targets_pred, dim=1)            \n            train_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n            train_metric_str = \"train_acc\"\n        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n            train_metric = F1Score(targets_pred, targets)            \n            train_metric_str = \"train_f1\"\n        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        return train_loss\n\n    def validation_step(self, batch, batch_idx):\n        tweets, tweet_lengths, targets = batch\n        # initialize the hidden and cell state of the LSTM\n        h0, c0 = self.network.init_state()\n        targets_pred = self(tweets, tweet_lengths, (h0, c0))\n        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n        loss_targets = loss_targets.float()        \n        val_loss = binary_cross_entropy(targets_pred, loss_targets)\n        val_metric = None\n        val_metric_str = \"\"\n        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:\n            targets_pred = torch.argmax(targets_pred, dim=1)\n            val_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n            val_metric_str = \"val_acc\"\n        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n            val_metric = F1Score(targets_pred, targets)            \n            val_metric_str = \"val_f1\"\n        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n        return val_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.785538Z","iopub.execute_input":"2022-05-28T06:43:14.78661Z","iopub.status.idle":"2022-05-28T06:43:14.813707Z","shell.execute_reply.started":"2022-05-28T06:43:14.786567Z","shell.execute_reply":"2022-05-28T06:43:14.812898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom lightning callback \nTo record training and validation metric values at each epoch and the best metric values across all epochs","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning import LightningModule, Trainer\n# Monitor multiple metric values that are calculated either in training or validation step and return the\n# best metric values for each epoch\nclass MetricsAggCallback(Callback):\n    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n        # dictionary with metric name as key and monitor mode (min, max) as the value\n        # ( the same names used to log metric values in training and validation step)\n        self.val_metrics_to_monitor = val_metrics_to_monitor\n        self.train_metrics_to_monitor = train_metrics_to_monitor\n        # dictionary with metric_name as key and list of metric value for each epoch\n        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the best metric value for all epochs\n        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the epoch number with the best metric value\n        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n        self.epoch_counter = 0           \n\n    @staticmethod\n    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n        metric_str = \"\"\n        for metric, mode in metrics_to_monitor.items():\n            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n            metric_str += f\"{metric} = {metric_value}, \"\n            metrics[metric].append(metric_value)\n            if mode == \"max\":\n                best_metric[metric] = max(metrics[metric])            \n            elif mode == \"min\":            \n                best_metric[metric] = min(metrics[metric])            \n            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n        print(metric_str[:-2])\n\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n        self.epoch_counter += 1        \n        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n        print(f\"For epoch {self.epoch_counter}\")\n        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.815957Z","iopub.execute_input":"2022-05-28T06:43:14.817348Z","iopub.status.idle":"2022-05-28T06:43:14.838577Z","shell.execute_reply.started":"2022-05-28T06:43:14.817308Z","shell.execute_reply":"2022-05-28T06:43:14.836603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = {\n        \"vocab_size\": Config.VOCAB_SIZE,\n        \"emb_size\": Config.EMB_SIZE,\n        \"output_size\": Config.OUT_SIZE,\n        \"pt_emb_weights\": pt_emb_weights,\n        \"emb_wt_update\": Config.EMB_WT_UPDATE,\n        \"is_bidirect\": True,\n        \"attn_method\": AttentionMethod.DOT\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.841399Z","iopub.execute_input":"2022-05-28T06:43:14.842967Z","iopub.status.idle":"2022-05-28T06:43:14.865717Z","shell.execute_reply.started":"2022-05-28T06:43:14.842923Z","shell.execute_reply":"2022-05-28T06:43:14.863181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(fold, dl_train, dl_val, find_lr=True):\n    fold_str = f\"fold{fold}\"\n    print(f\"Running training for {fold_str}\")    \n    disaster_tweet_model = DisasterTweetLitModel(\n        params=model_params,        \n        hparams=Config.MODEL_HPARAMS,\n        model_eval_metric=Config.MODEL_EVAL_METRIC                \n        )\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n    chkpt_file_name = fold_str + \"_best_model_{epoch}_{val_loss:.4f}\"\n    train_metrics_to_monitor = {\n        \"train_loss\": \"min\",\n        \"train_acc\": \"max\"\n    }\n    val_metrics_to_monitor = {\n        \"val_loss\": \"min\",\n        \"val_acc\": \"max\",\n        }\n    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n    trainer = pl.Trainer(\n        gpus = 1,\n        deterministic = True,\n        auto_select_gpus = True,\n        progress_bar_refresh_rate = 20,\n        max_epochs = Config.NUM_EPOCHS,\n        logger = tb_logger,\n        auto_lr_find = True,    \n        #precision = Config.PRECISION,   \n        fast_dev_run = Config.FAST_DEV_RUN, \n        gradient_clip_val = 1.0,        \n        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n    )        \n    if find_lr:\n        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n        print(disaster_tweet_model.lr)\n    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n    fold_train_metrics = {\n        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n        for metric in train_metrics_to_monitor.keys()\n    }\n    fold_val_metrics = {\n        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n        for metric in val_metrics_to_monitor.keys()\n    }            \n    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n    return fold_train_metrics, fold_val_metrics","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.8718Z","iopub.execute_input":"2022-05-28T06:43:14.875866Z","iopub.status.idle":"2022-05-28T06:43:14.897041Z","shell.execute_reply.started":"2022-05-28T06:43:14.875824Z","shell.execute_reply":"2022-05-28T06:43:14.895529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_lr = True\nall_fold_val_loss = []\nall_fold_val_acc = []\n\nfor fold in range(Config.NUM_FOLDS):\n    dl_train, dl_val = get_fold_dls(fold, df_train)\n    fold_train_metrics, fold_val_metrics = run_training(fold, dl_train, dl_val, find_lr=False)    \n    all_fold_val_loss.append(fold_val_metrics[\"val_loss\"][0])\n    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"][0])\n    print(f\"Best train metrics values for fold{fold}\")    \n    print(fold_train_metrics)\n    print(f\"Best val metrics values for fold{fold}\")    \n    print(fold_val_metrics)                ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:43:14.898495Z","iopub.execute_input":"2022-05-28T06:43:14.899051Z","iopub.status.idle":"2022-05-28T06:45:47.279812Z","shell.execute_reply.started":"2022-05-28T06:43:14.899016Z","shell.execute_reply":"2022-05-28T06:45:47.278919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_fold_val_loss = [x[0] for x in all_fold_val_loss]\n#all_fold_val_acc = [x[0] for x in all_fold_val_acc]\nprint(f\"val loss across folds = {all_fold_val_loss}\")\nprint(f\"val accuracy across folds = {all_fold_val_acc}\")\nmean_loss = statistics.mean(all_fold_val_loss)\nmean_acc = statistics.mean(all_fold_val_acc)\nstd_loss = statistics.stdev(all_fold_val_loss)\nstd_acc = statistics.stdev(all_fold_val_acc)\nprint(f\"mean val loss across folds = {mean_loss}, val loss stdev across fold = {std_loss}\")\nprint(f\"mean val accuracy across folds = {mean_acc}, val accuracy stdev across fold = {std_acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T06:45:47.281669Z","iopub.execute_input":"2022-05-28T06:45:47.282171Z","iopub.status.idle":"2022-05-28T06:45:47.290429Z","shell.execute_reply.started":"2022-05-28T06:45:47.282126Z","shell.execute_reply":"2022-05-28T06:45:47.289197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_model = DisasterTweetLitModel.load_from_checkpoint(\n#     checkpoint_path=\"./model/fold0_best_model_epoch=4_val_loss=0.4090.ckpt\",\n#     params = model_params,\n#     hparams = Config.MODEL_HPARAMS\n#     )\n# print(best_model)\n# tweet_vocab.set_default_index(0)\n# df_test[\"vectorized_tweet\"] = df_test[\"processed_text\"].apply(\n#     lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n#     )\n\n# # Do prediction with best performing model on the test set\n# def predict(df_test):\n#     test_output = []\n#     for index, row in df_test.iterrows():    \n#         vec_tweet = row[\"vectorized_tweet\"]\n#         if len(vec_tweet) == 0:\n#             test_output.append(0)\n#             continue\n#         vec_tweet_len = torch.IntTensor([len(vec_tweet)])\n#         vec_tweet = vec_tweet.view(1, -1)    \n#         #print(vec_tweet, vec_tweet_len)\n#         output = best_model(vec_tweet, vec_tweet_len, state=None)\n#         #print(output)\n#         test_output.append(torch.argmax(output).item())    \n#     return test_output        \n\n# test_output = predict(df_test)\n# print(len(test_output))\n\n# df_submission = pd.read_csv('./data/submission.csv')\n# df_submission['target']= test_output\n# df_submission.to_csv('my_submission.csv',index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-28T06:48:26.813118Z","iopub.execute_input":"2022-05-28T06:48:26.8136Z","iopub.status.idle":"2022-05-28T06:48:26.81982Z","shell.execute_reply.started":"2022-05-28T06:48:26.813558Z","shell.execute_reply":"2022-05-28T06:48:26.819036Z"},"trusted":true},"execution_count":null,"outputs":[]}]}