{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using %s\" % (device))\n\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()  # lowercase\n\n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    text = re.sub(r\"'\", \"\", text)\n    text = re.sub('\\s+', ' ', text).strip()  # Remove and double spaces\n    text = re.sub(r'&amp;?', r'and', text)  # replace & -> and\n    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)  # Remove URLs\n    # remove some puncts (except . ! # ?)\n    text = re.sub(r'[:\"$%&\\*+,-/:;<=>@\\\\^_`{|}~]+', '', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'EMOJI', text)\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain['text_new'] = train['text'].apply(clean_text)\ntest['text'] = test['text'].apply(clean_text)\n\ntrain_texts = list(train[\"text\"])\ntrain_labels = list(train[\"target\"])\ntrain_keywords = list(train['keyword'].fillna(''))\nres_texts = list(test[\"text\"])\nres_keywords = list(test['keyword'].fillna(''))\n\nx_train, x_test, train_label, test_label, train_keyword, test_keyword =  train_test_split(train_texts, train_labels, train_keywords, test_size=0.2)\nprint(len(res_texts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[[\"text\", \"text_new\"]]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntrain_encoding = tokenizer(x_train, truncation=True, padding=True)\ntest_encoding = tokenizer(x_test, truncation=True, padding=True)\nres_encoding = tokenizer(res_texts, truncation=True, padding=True)\ntrain_keyword_en = tokenizer(train_keyword, truncation=True, padding=True)\ntest_keyword_en = tokenizer(test_keyword, truncation=True, padding=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_keyword_en\n# list(train_encoding.items())[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TwitterDataset(Dataset):\n    def __init__(self, encodings, labels, keywords):\n        self.encodings = encodings\n        self.labels = labels\n        self.keywords = keywords\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        item['keyword'] = {key: torch.tensor(val[idx]) for key, val in self.keywords.items()}\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TwitterDataset(train_encoding, train_label, train_keyword_en)\ntest_dataset = TwitterDataset(test_encoding, test_label, test_keyword_en)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Smoke testing\nstep_nums = 30\n\nfrom transformers import DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\nmodel.to(device)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\noptim = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optim, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = step_nums)\n\nfirst_batch = next(iter(train_loader))\nfor batch_idx, batch in enumerate([first_batch] * step_nums):\n    model.train()\n    optim.zero_grad()\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    loss = outputs[0]\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optim.step()\n    scheduler.step()\n    \n    # test to make sure model can overfit one batch. \n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n    loss = outputs.loss\n    logits = outputs.logits\n    logits = logits.detach().cpu().numpy()\n    label_ids = labels.to('cpu').numpy()\n    print(f\"step: {batch_idx:d}, loss: {loss.item():.2f}, accuracy: {flat_accuracy(logits, label_ids)*100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(model.parameters()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and keep the best model\nepoth_num=5\n\nfrom transformers import DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel.to(device)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\noptim = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * epoth_num\nscheduler = get_linear_schedule_with_warmup(optim, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n\nbest_accuracy=0\n\nfor epoch in range(epoth_num):\n    print(\"------------Epoch: %d ----------------\" % epoch)\n    \n    model.train()\n    total_train_loss = 0\n    iter_num = 0\n    total_iter = len(train_loader)\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optim.step()\n        scheduler.step()\n        \n        iter_num += 1\n        if(iter_num % 100==0):\n            print(f\"epoth: {epoch}, iter_num: {iter_num}, loss: {loss.item():.4f}, {iter_num/total_iter*100:.2f}%\")\n        \n    print(f\"Epoch: {epoch}, Average training loss: {total_train_loss/len(train_loader):.4f}\")\n    \n    \n    print(\"\")\n    print(\"Running Validation...\")\n    \n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    for batch in test_dataloader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            \n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_eval_loss += loss.item()\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n    \n    if avg_val_accuracy > best_accuracy:\n        best_accuracy = avg_val_accuracy\n        print(\"Best model till now.\")\n        torch.save(model.state_dict(), './model.weights')\n    else:\n        model.load_state_dict(torch.load(\"./model.weights\"))\n    \n    print(f\"Accuracy: {avg_val_accuracy:.4f}\")\n    print(f\"Average testing loss: {total_eval_loss/len(test_dataloader):.4f}\")\n    print(\"-------------------------------\")\n    ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Restoring the best model weights.\")\nmodel.load_state_dict(torch.load(\"./model.weights\"))\nmodel.eval()\nclass TwitterValDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n# print(res_encoding.items())\n# for k,v in res_encoding.items():\n#     print(k)\nval_dataset = TwitterValDataset(res_encoding)\n# print(len(val_dataset))\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\nresult = []\nfor batch in val_loader:\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    logits = logits.detach().cpu().numpy()\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    result.extend(pred_flat)\ndf = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\ndf['target'] = result\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}