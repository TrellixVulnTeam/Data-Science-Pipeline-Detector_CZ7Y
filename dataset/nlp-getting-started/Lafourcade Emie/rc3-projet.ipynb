{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### LAFOURCADE Emie p1802635\n## **Projet RC3 - Kaggle Challenge**\n### Lifprojet Printemps 2021\n### Encadrant : CAZABET Remy","metadata":{}},{"cell_type":"markdown","source":"### **Description :**\nLe projet consiste à découvrir Kaggle et participer à une \"compétition\" sur cette plateforme.\n\nSur ce notebook se trouve le code que j'ai réalisé pour la compétition [\"Natural Language Processing with Disaster Tweets\"](http://www.kaggle.com/c/nlp-getting-started). Il s'agit de déterminer si les tweets à tester sont des tweets parlant d'une catastrophe (disaster) ou non à partir d'une base de données de tweets dont nous connaissons l'issue (disaster ou non disaster).\n\n### **Exécution :**\nPour éxécuter ce code il faut se mettre en \"edit mode\" et cliquer sur la double flèche \"Run all\". Cependant, il met une 15ène de minutes à s'éxécuter. Toutes les sorties que vous obtiendrez sont affichées en viewer mode alors il n'est pas nécéssaire de le lancer.\nCe code n'est éxécutable que sur Kaggle (emplacement des données et librairies propre).\n\nD'autres noteboooks sont consultables sur mon profil.\n### **Résultats :**\nJ'ai testé plusieurs solutions et la meilleure reste celle avec le random forest classificateur qui donne un résultat d'environ 79% de réussite.\n\n### **Sommaire :**\n*   I. Importation des librairies et données\n*   II. Data Visualization\n*   III. Traitement des données\n*   IV. Exploitations des données\n*   V. Resultats","metadata":{}},{"cell_type":"markdown","source":"# **I-Importation des librairies et données**","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, metrics,manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import PorterStemmer \nimport shelve\nfrom wordcloud import WordCloud \nimport spacy \nimport xgboost as xgb\nfrom xgboost import XGBClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **II-Data Visualization**","metadata":{}},{"cell_type":"markdown","source":"## **II.1-Forme des données**","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.axes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### train_df est composé de 5 colonnes : id, keyword, location, text et target et 7613 lignes","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.axes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### test_df est composé de 4 colonnes : id, keyword, location et text et 3263 lignes","metadata":{}},{"cell_type":"markdown","source":"## **II.2-Eléments nuls**","metadata":{}},{"cell_type":"code","source":"nulls = pd.DataFrame({\"Num_Null\": train_df.isnull().sum()})\nnulls[\"Pct_Null\"] = nulls[\"Num_Null\"] / train_df.count() * 100\nnulls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Valeurs nulles dans train_df","metadata":{}},{"cell_type":"code","source":"nulls = pd.DataFrame({\"Num_Null\": test_df.isnull().sum()})\nnulls[\"Pct_Null\"] = nulls[\"Num_Null\"] / test_df.count() * 100\nnulls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Valeurs nulles dans test_df","metadata":{}},{"cell_type":"markdown","source":"#### Environ la moitier des tweets totaux n'indique pas de localisation. La colonne \"location\" ne sera donc pas utilisée. \n\n#### On pourra cependant utiliser la colonne des mots clés \"keywords\".","metadata":{}},{"cell_type":"markdown","source":"## **II.3-Taux des tweets disaster - non disaster**","metadata":{}},{"cell_type":"code","source":"target_vc = train_df[\"target\"].value_counts(normalize=True)\nprint(\"Non Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogramme Disaster vs Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **II.4-Longueurs des tweets**","metadata":{}},{"cell_type":"code","source":"train_df[\"tweet_length\"] = train_df[\"text\"].apply(len)\ng = sns.FacetGrid(train_df, col=\"target\", height=5)\ng = g.map(sns.histplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### La longueur des tweet n'est pas déterminante dans notre recherche de \"disaster tweets\".","metadata":{}},{"cell_type":"markdown","source":"## **II.5-Keywords les plus utilisés**","metadata":{}},{"cell_type":"code","source":"keywords_vc = pd.DataFrame({\"Count\": (train_df[\"keyword\"].append(test_df[\"keyword\"])).value_counts()})\nsns.set()\nplt.figure(figsize = (10,40))\nsns.barplot(y=keywords_vc[0:300].index, x=keywords_vc[0:300][\"Count\"])\nplt.title(\"Keywords les plus utilisés\", fontsize = 20)\nplt.xlabel(\"nb occurences\", fontsize = 15)\nplt.ylabel(\"Keywords\",fontsize = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords = train_df.loc[train_df[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train_df.loc[train_df[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Keywords les plus utilisés dans les Disaster Tweets\",fontsize = 25)\nax[0].set_xlabel(\"nb occurences\",fontsize = 20)\nax[1].set_title(\"Keywords les plus utilisés dans les Non-Disaster Tweets\",fontsize = 25)\nax[1].set_xlabel(\"nb occurences\",fontsize = 20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dictionnaire avec les keywords de train_df et le nombre de fois qu'ils apparaissent dans train_df\nkeywords_proba_df = pd.DataFrame({\"keyword\":train_df[\"keyword\"].value_counts().index,\"Count\": (train_df[\"keyword\"].value_counts().values)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def proba_keywords(n, df=keywords_proba_df[\"keyword\"]):    \n    \"\"\"\n    Arguments :\n    :n: integer\n    :df: liste de keyword\n    :return: [int,float]\n    \n    Cette fonction renvoie un couple d'int \n    dont le premier indique si le keyword df[n] a été déterminé comme tweet disaster(1) ou non (0), \n    et le second indique la probabilité qu'il soit un tweet disaster ou non.\n    \"\"\"\n    tweets_keywords=train_df.loc[train_df[\"keyword\"]==df[n]]\n    tot=0\n    for i in tweets_keywords.index:\n        if tweets_keywords['target'][i]==1:\n            tot+=1\n    if len(tweets_keywords) >0:proba=tot/len(tweets_keywords)\n    else: proba=-1\n    return([0, proba] if proba<0.4297 else [-1,proba] if proba==-1 else [1, proba])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(keywords_proba_df)):\n    keywords_proba_df.loc[i, \"proba_target\"],keywords_proba_df.loc[i, \"proba\"] =proba_keywords(i)\n    \nkeywords_proba_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords = train_df.loc[train_df[\"target\"] == 1][\"keyword\"].value_counts()\n\ndk_proba = pd.DataFrame({\"keyword\":disaster_keywords.keys(), \"Count\" :disaster_keywords.values})\n\nfor i in range(len(dk_proba)):\n    dk_proba.loc[i, \"proba_target\"],dk_proba.loc[i, \"proba\"]=proba_keywords(i,disaster_keywords.keys() )\n    \ndk_proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = dk_proba[\"Count\"]\ny = dk_proba[\"proba\"]\nplt.plot(x, y)\nplt.xlabel(\"Nombre d'apparitions du keyword dans train\", fontsize=15)\nplt.ylabel(\"Probabilité de disaster\", fontsize=15)\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Les keywords les plus utilisés dans les disaster tweets sont un bon indicateur pour classer les tweets. Un tweet ayant comme keyword un mot très utilisé dans les disaster tweets a de grandes chances d'en être un aussi.","metadata":{}},{"cell_type":"code","source":"non_disaster_keywords = train_df.loc[train_df[\"target\"] == 0][\"keyword\"].value_counts()\n\nndk_proba = pd.DataFrame({\"keyword\":non_disaster_keywords.keys(), \"Count\" :non_disaster_keywords.values})\n\nfor i in range(len(ndk_proba)):\n    ndk_proba.loc[i, \"proba_target\"],ndk_proba.loc[i, \"proba\"]=proba_keywords(i,non_disaster_keywords.keys() )\n    \nndk_proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = ndk_proba[\"Count\"]\ny = ndk_proba[\"proba\"]\nfig, ax = plt.subplots()\n\nplt.plot(x, y)\nplt.xlabel(\"Nombre d'apparitions du keyword dans train\", fontsize=15)\nplt.ylabel(\"Probabilité de non disaster\", fontsize=15)\nax.invert_yaxis()\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Même chose pour les non-disaster tweets.","metadata":{}},{"cell_type":"markdown","source":"# **III-Traitement des données**","metadata":{}},{"cell_type":"code","source":"def cleaning(sentence):\n    \"\"\"\n    Arguments :\n    :sentence: str\n    :return: str \n    \n    Cette fonction renvoie la phrase passée en paramètre \"nétoyée\".\n    Elle est passée en minuscule, les abréviations sont remplacées par les mots complets\n    et la ponctuation est enlevée.\n    \"\"\"\n    sentence = sentence.lower()\n    sentence = re.sub(r\"won't\", \"will not\", sentence)\n    sentence = re.sub(r\"gon\", \"going\", sentence)\n    sentence = re.sub(r\"gonna\", \"going\", sentence)\n    sentence = re.sub(r\"whats\", \"what is\", sentence)\n    sentence = re.sub(r\"im\", \"i am\", sentence)\n    sentence = re.sub(r\"ill\", \"i will\", sentence)\n    sentence = re.sub(r\"na\", \"\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n    sentence = re.sub(r\"cant\", \"can not\", sentence)\n    sentence = re.sub(r\"cannot\", \"can not\", sentence)\n    sentence = re.sub(r\"didnt\", \"did not\", sentence)\n    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence) \n    return sentence\n\nstop_words = set(stopwords.words('english'))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english')+[\"co\",\"http\"])\nsnow = nltk.stem.SnowballStemmer('english')\n\ndef mots_cles(df):\n    \"\"\"\n    Arguments :\n    :df: dataframe\n    \n    Cette fonction ajoute deux colonnes au dataframe passé en paramètre.\n    Une colonne \"mots_clés\" qui correspond au tweet de base dépourvus des mots inutiles (stowords)\n    et une colonne \"mots_clés_snowball\" qui correspond à ces mêmes mots appliqués à la fonction snowballStemmer.\n    \"\"\"\n    temp = []\n    temp_s=[]\n\n    for each_sentence in df.text:\n        each_sentence = cleaning(each_sentence)\n        each_word=[]\n        each_word_s=[]\n        for word in each_sentence.split():\n            if word not in stop_words:\n                each_word.append(word)\n                each_word_s.append(snow.stem(word))\n        \n        temp.append(each_word)\n        temp_s.append(each_word_s) \n        \n        final_word = []\n        final_word_s=[]\n\n    for row in temp:\n        seq = ''\n        for word in row:\n            seq = seq + word + ' '\n        final_word.append(seq)\n        \n    for row in temp_s:\n        seq = ''\n        for word in row:\n            seq = seq + word + ' '\n        final_word_s.append(seq)\n\n    for i in range(len(df)):\n        df.loc[i, \"mots_cles\"]=final_word[i]\n        df.loc[i, \"mots_cles_snowball\"]=final_word_s[i]\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=mots_cles(train_df)\ntest_df=mots_cles(test_df)\n\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### On obtient maintenant les mots utiles de nos tweets dans \"mots_cles\". On s'est débarrassé des stopwords.\n\n#### La fonction snowball consiste à retirer les pré-fixes et post-fixes des mots pour n'obtenir que les corps de ces derniers. Par exemple \"evacuation\" et \"evacuate\" deviennent tous deux \"evacuat\". Dans le code qui suit, je n'ai pas utilisé ces mots clés car ils donnaient de bien moins bons résultats.","metadata":{}},{"cell_type":"markdown","source":"## Visualisation des données traitées","metadata":{}},{"cell_type":"code","source":"def to_corpus(target):\n    corpus = []\n\n    for w in train_df.loc[train_df[\"target\"] == target][\"mots_cles\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef corpus_to_dict(target):\n    corpus = to_corpus(target)\n            \n    dict = defaultdict(int)\n    for w in corpus:\n        dict[w] += 1\n    return sorted(dict.items(), key=lambda x:x[1], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_dict = corpus_to_dict(0)\nnon_disaster_dict = corpus_to_dict(1)\n\ndisaster_x, disaster_y = zip(*disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"mots non stop words les plus utilisés Disaster Tweets\",fontsize = 25)\nax[0].set_xlabel(\"nb occurences\",fontsize = 20)\nax[1].set_title(\"mots non stop words les plus utilisés Non-Disaster Tweets\",fontsize = 25)\nax[1].set_xlabel(\"nb occurences\",fontsize = 20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bigrams(target):\n    corpus = train_df[train_df[\"target\"] == target][\"mots_cles\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    mots = count_vec.transform(corpus)\n    somme_mots = mots.sum(axis=0) \n    freq_mots = [(w, somme_mots[0, idx]) for w, idx in count_vec.vocabulary_.items()]\n    freq_mots =sorted(freq_mots, key = lambda x: x[1], reverse=True)\n    return freq_mots","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\", fontsize=20)\nax[0].set_xlabel(\"Frequence\", fontsize=20)\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\", fontsize=20)\nax[1].set_xlabel(\"Frequence\", fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordclouds","metadata":{}},{"cell_type":"code","source":"def wordcloud(mots, titre):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=130).generate(mots) \n    plt.figure(figsize=(10, 7)) \n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off') \n    print(titre)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud(' '.join([text for text in (train_df[\"mots_cles\"].append(test_df[\"mots_cles\"]))]), \"Mots principaux tweets totaux\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud(' '.join([text for text in train_df['mots_cles'][train_df['target'] == 1]]) , \"Mots principaux train tweets disaster\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud(' '.join([text for text in train_df['mots_cles'][train_df['target'] == 0]]) , \"Mots principaux train tweets non disaster\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IV-Exploitation des données** ","metadata":{}},{"cell_type":"markdown","source":"## IV.1-Fonctions utiles ","metadata":{}},{"cell_type":"code","source":"def conf_matrix(pred, titre):\n    \"\"\"\n    Arguments :\n    :pred: int[]\n    :titre: str\n    \n    Cette fonction crée une matrice de confusion entre des prédictions passées en parametre et les prédictions officielles de train_df\"\"\"\n    cmatrix = confusion_matrix(train_df[\"target\"],pred)\n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=titre)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exemple de matrice de confusion avec les valeurs réelles des tweets\nconf_matrix(train_df[\"target\"],\"Exemple\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Lire une matrice de confusion :**\n#### La case en haut à gauche indique le nombre de tweets que le modèle a estimé à 0 et qui valent réellement 0\n#### La case en haut à droite indique le nombre de tweets que le modèle a estimé à 1 et qui valent réellement 0\n#### La case en bas à gauche indique le nombre de tweets que le modèle a estimé à 0 et qui valent réellement 1\n#### La case en bas à droite indique le nombre de tweets que le modèle a estimé à 1 et qui valent réellement 1","metadata":{}},{"cell_type":"code","source":"compare_model=[]\n#tableau de dictionnaires qui va nous permettre de comparer les différente façons de calculer les tweet disaster ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score(pred):\n    \"\"\"\n    Arguments :\n    :pred: int[]\n    \n    Calcule le taux de réussite des prédictions\"\"\"\n    tot=0\n    for i in range(len(train_df)):\n        if pred[i]==train_df[\"target\"][i]:\n            tot+=1\n    s=tot/len(train_df)\n    print (\"Score Train -->\", round(s *100,2), \" %\")\n    return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **IV.2 Solutions** \n## **Solution 1 : Keywords**\n\n#### On réutilise la fonction proba_keywords définie au dessus pour déterminer si les tweets à tester sont des disaster ou non selon la moyenne des résultats des train tweets partageant le même keyword.","metadata":{}},{"cell_type":"code","source":"keywords_proba_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_keywords(i,df=test_df, affichage=False):\n    \"\"\"\n    Arguments :\n    :i: int\n    :df: dataframe\n    :affichage: booleen\n    \n    Determine si le tweet est disaster ou non grâce au tableau keywords_proba_df\"\"\"\n    if affichage :\n        print (\"tweet testé :\",df.text[i])\n        print(\"keyword :\",df.keyword[i])\n        if df.text[i]==train_df.text[i]:\n            print(\"target :\",df.target[i],\"\\n\")\n            \n    if df.keyword[i] in list(keywords_proba_df[\"keyword\"]):\n        pred=round(keywords_proba_df.proba_target[keywords_proba_df.keyword==df.keyword[i]].values[0])\n#   else :pred=(random.randint(0, 1)) #score : 73.74\n#   else :pred=9 #score : 73.74\n    else :pred=1 #score : 74.04\n    if affichage : print (\"test :\",pred,\"--> tweet non disaster\" if pred==0 else \" --> tweet disaster\")\n    else: return pred\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plusieurs tweets ne possèdent pas de keywords. Dans ce cas on met ces tweets à 1 (disaster) car cela donne un meilleur score quand on teste cette technique sur les train tweets.","metadata":{}},{"cell_type":"code","source":"prediction_keywords(508,train_df, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_keywords(509,train_df, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_keywords(66,affichage =True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_train_keywords=[]\nfor i in range(len(train_df)):\n    predictions_train_keywords.append(prediction_keywords(i,train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix(predictions_train_keywords, \"pred keywords\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_keywords=score(predictions_train_keywords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_keywords=[]\nfor i in range(len(test_df)):\n    predictions_keywords.append(prediction_keywords(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred={\"model\":\"keywords()\", \"predictions\":np.array(predictions_keywords), \"train_predictions\":np.array(predictions_train_keywords), \"score\":score_keywords, \"tfidf\":False}\ncompare_model.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Solution 2 : Vecteurs, Word Embedding**\n#### On crée les vecteurs des tweets puis on regarde quels sont les vecteurs des train tweets les plus proches pour chaque tweet à tester. Selon la moyenne des \"target\" de ces train tweets on détermine si le tweet est un disaster ou non.\n### Création des vecteurs ","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_lg\")\n\ndef valeurTweet(dct):\n    \"\"\"\n    Arguments :\n    :dct: dictionnary\n    :return:liste de taille 300 de float\n    \n    Crée les vecteurs de chaque mots du dictionnaire avec Spacy (word embedding)\"\"\"\n    vec = []\n    for i in range(len(dct)):\n        valTweet={}\n        for token in dct[i][\"Mots\"]:\n            valTweet[str(token)] = nlp(token).vector\n        vec.append(valTweet)\n    return vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calcul des poids tfidf de chaque mots de la data\nmodel_tfidf=TfidfVectorizer()\ntfidf_general = model_tfidf.fit_transform(list(train_df[\"mots_cles\"])+list(test_df[\"mots_cles\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dictionnaire(df, tfidf, decalage=0):\n    \"\"\"\n    Arguments :\n    :df: dataframe\n    :tfidf: matrice tfidf\n    :decalage: int\n    :return: dictionnary\n    \n    Crée le dictionnaire des données nécéssaires par la suite \"\"\"\n    dct=[]\n    for i in df.index:\n        dct.append(\n            {\"id\": df.id[i]}) \n    \n    for i in df.index:\n        dct[i][\"Mots\"]=[df[\"mots_cles\"][i].split()[j] for j in range(len(df[\"mots_cles\"][i].split())) if df[\"mots_cles\"][i].split()[j] in model_tfidf.vocabulary_]\n        \n    for i in range(len(dct)):\n        dct[i][\"index\"]=[model_tfidf.vocabulary_[str(k)] for k in dct[i][\"Mots\"] if str(k) in model_tfidf.vocabulary_ ]\n        \n    for i in range(len(dct)):\n        dct[i][\"poids_tfidf\"]=[(tfidf[i+decalage, k]) for k in dct[i][\"index\"]]\n        \n    vec=valeurTweet(dct)\n    for i in range(len(dct)):\n        dct[i][\"vectors\"]=[]\n        dct[i][\"vectors_tfidf\"]=[]\n        for k in range(len(dct[i][\"Mots\"])):\n            dct[i][\"vectors\"].append([0 for l in range(300)])\n            dct[i][\"vectors_tfidf\"].append([0 for l in range(300)])\n        for j in range(len(dct[i][\"Mots\"])):\n            #print(dct[i][\"Mots\"][j], i)\n            dct[i][\"vectors\"][j]=vec[i][dct[i][\"Mots\"][j]]\n            dct[i][\"vectors_tfidf\"][j]=dct[i][\"vectors\"][j]*dct[i][\"poids_tfidf\"][j]\n            \n    for i in range(len(dct)):\n        if dct[i][\"Mots\"]==[]:\n            dct[i][\"moy_vector\"]=[0 for l in range(300)]\n            dct[i][\"moy_vector_tfidf\"]=[0 for l in range(300)]\n        else : \n            dct[i][\"moy_vector\"]=np.mean([(dct[i][\"vectors\"][j]) for j in range(len(dct[i][\"vectors\"]))], axis=0)\n            dct[i][\"moy_vector_tfidf\"]=np.mean([(dct[i][\"vectors_tfidf\"][j]) for j in range(len(dct[i][\"vectors_tfidf\"]))], axis=0)\n            \n    return dct","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n\n# dct_train=dictionnaire(train_df, tfidf_general)\n# dct_test=dictionnaire(test_df, tfidf_general, len(train_df))\n\n# def list_to_file(list_dct, nomFichier):\n#     shelf = shelve.open(nomFichier)\n#     shelf[\"my_dict\"] = list_dct\n#     shelf.close()\n#     return shelf \n\n# shelf_train=list_to_file(dct_train, \"list_dct_train.shlf\")\n# shelf_test=list_to_file(dct_test, \"list_dct_test.shlf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### On ne lance pas cette cellule car elle met beaucoup de temps à s'éxécuter. Elle permet la création de nos dictionnaires et l'écriture de ceux-ci dans des fichiers shelves. On récupère les données de ces fichiers dans la cellule suivante.","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/dictionnaries ./\n\nshelf_train = shelve.open(\"./dictionnaries/list_dct_train.shlf\")\ndct_train = shelf_train[\"my_dict\"]\nshelf_train.close()\nshelf_test = shelve.open(\"./dictionnaries/list_dct_test.shlf\")\ndct_test = shelf_test[\"my_dict\"]\nshelf_test.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"id :\",dct_train[0]['id'])\nprint(\"Mots :\",dct_train[0]['Mots'])\nprint(\"index :\",dct_train[0]['index'])\nprint(\"poids_tfidf :\",dct_train[0]['poids_tfidf'])\nprint(\"vectors :\",dct_train[0]['vectors'][0][:5], \"etc...\")\nprint(\"vectors tfidf:\",dct_train[0]['vectors_tfidf'][0][:5], \"etc...\")\nprint(\"moy vector:\",dct_train[0]['moy_vector'][:5], \"etc...\")\nprint(\"moy vector tfidf:\",dct_train[0]['moy_vector_tfidf'][:5], \"etc...\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pour chaque tweet, on a son id dans son dataframe, les mots importants qui le composent, son numero d'index dans le dictionnaire tfidf, son poids tfidf, un vecteur de taille 300 pour chaque mot du tweet, ce même vecteur multiplié par le poids tfidf du mot, un vecteur de taille 300 faisant la moyenne des vecteurs de chaque mots et un vecteur de taille 300 faisant la moyenne pondérée avec tfidf des vecteurs de chaque mot.","metadata":{}},{"cell_type":"code","source":"X=np.array([dct_train[i][\"moy_vector\"] for i in range(len(dct_train))]) #Tableau des vecteurs par tweets train\nX_test=np.array([dct_test[i][\"moy_vector\"] for i in range(len(dct_test))]) #Tableau des vecteurs par tweets test\n\nX_tfidf=np.array([dct_train[i][\"moy_vector_tfidf\"] for i in range(len(dct_train))]) #Tableau des vecteurs tfidf par tweets train\nX_test_tfidf=np.array([dct_test[i][\"moy_vector_tfidf\"] for i in range(len(dct_test))]) #Tableau des vecteurs tfidf par tweets test\n\ny = train_df[\"target\"] #Tableau des target de train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modele_tsne = manifold.TSNE(2)\ndef tsne (x=X, y_tsne=y):\n    \"\"\"\n    Arguments :\n    :x:int[]\n    :y_tsne:int[]\n    :return: figure tsne\n    \n    Crée et affiche le modèle tsne de x y_tsne\"\"\"\n    modele_tsne_fit=modele_tsne.fit_transform(x)\n\n    df_plot=pd.DataFrame(modele_tsne_fit)\n    df_plot['target']=y_tsne\n    df_plot.columns=[\"x\",\"y\",'target']\n    return sns.scatterplot(data=df_plot, x='x', y='y', hue='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Le modèle tsne permet de rammener les vecteurs de taille 300 à une taille 2 ce qui permet de visualiser si les vecteurs des tweets disaster et non disaster sont distinguables dans un environnement 2D.","metadata":{}},{"cell_type":"code","source":"tsne()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Les vecteurs sans tfidf forment un nuage de points mais on observe une légère séparation des tweets disaster et non disaster. ","metadata":{}},{"cell_type":"code","source":"tsne(x=X_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Même chose pour les vecteurs pondérés avec tfidf.","metadata":{}},{"cell_type":"markdown","source":"### Calculs des distances et vecteurs proches.","metadata":{}},{"cell_type":"code","source":"distance_vec=metrics.pairwise_distances(X_test,X) #distance entre les vecteurs de test_df et train_df\ndistance_vec_train=metrics.pairwise_distances(X,X)#distance entre les vecteurs de train_df\ndistance_vec_tfidf=metrics.pairwise_distances(X_test_tfidf,X_tfidf)#distance entre les vecteurs tfidf de test_df et train_df\ndistance_vec_train_tfidf=metrics.pairwise_distances(X_tfidf,X_tfidf)#distance entre les vecteurs tfidf de train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vecteurs_proches(id_tweet_testé, nb_tweets=100, dist=distance_vec, df=test_df, affichage=False):\n    \"\"\"\n    Arguments :\n    :id_tweet_testé:int\n    :nb_tweets:int\n    :dist: float[]\n    :df: dataframe\n    :affichage: booleen\n    :return: int\n    \n    Calcule si le tweet df[id_tweet_testé] est un disaster(1) ou non(0) \n    grace à la moyenne des nb_tweets target des vecteurs train les plus proches de lui\"\"\"\n    tab=dist[id_tweet_testé]\n    if affichage:\n        print (\"Tweet testé :\",df.text[id_tweet_testé])\n        print (\"Tweet plus ressemblant :\",train_df.text[tab.argsort()[0]])\n        print(\"Target :\",train_df.target[tab.argsort()[0]])\n    val=[]\n    for i in tab.argsort()[0:nb_tweets]:\n        val.append(train_df.target[i])\n    if affichage: print (\"Test :\",\"0 --> tweet non disaster\" if np.mean(val)<0.4297else \"1 --> tweet disaster\")\n    return(0 if np.mean(val)<0.4297 else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=vecteurs_proches(0, affichage=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=vecteurs_proches(0, dist=distance_vec_tfidf, affichage=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=vecteurs_proches(1,affichage=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=vecteurs_proches(1, dist=distance_vec_tfidf,affichage=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=vecteurs_proches(0,dist=distance_vec_train, df=train_df,affichage=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_train_vectors=[]\nfor i in range(len(train_df)):\n    predictions_train_vectors.append(vecteurs_proches(i,dist=distance_vec_train, df=train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix(predictions_train_vectors, \"100 tweets proches\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_vectors=score(predictions_train_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_vectors=[]\nfor i in range(len(test_df)):\n    predictions_vectors.append(vecteurs_proches(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred={\"model\":\"vecteurs_proches()\", \"predictions\":np.array(predictions_vectors), \"train_predictions\":np.array(predictions_train_vectors), \"score\":score_vectors, \"tfidf\":False}\ncompare_model.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_train_vectors_tfidf=[]\nfor i in range(len(train_df)):\n    predictions_train_vectors_tfidf.append(vecteurs_proches(i,dist=distance_vec_train_tfidf, df=train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix(predictions_train_vectors_tfidf, \"100 tweets proches tfidf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_vectors_tfidf=score(predictions_train_vectors_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_vectors_tfidf=[]\nfor i in range(len(test_df)):\n    predictions_vectors_tfidf.append(vecteurs_proches(i,dist=distance_vec_tfidf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tfidf={\"model\":\"vecteurs_proches()\", \"predictions\":np.array(predictions_vectors_tfidf), \"train_predictions\":np.array(predictions_train_vectors_tfidf), \"score\":score_vectors_tfidf, \"tfidf\":True}\ncompare_model.append(pred_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Solution 3 : Classificateurs**\n#### On passe  les vecteurs créés au dessus à différents modèles qui classifient.\n#### On teste le Random forest, logistic regression, Gaussian NB, XGboost avec les paramètres minimum puis ces mêmes modèles avec des paramètres déterminés avec grid search et enfin un modèle \"voting classifier\" avec ces 4 classificateurs.","metadata":{}},{"cell_type":"code","source":"def predire(model, titre='', tfidf=False):\n    \"\"\"\n    Arguments :\n    :model:classifier\n    :titree:str\n    :tfidf:booleen\n    :return: dictionnary\n    \n    Crée un dictionnaire avec les prédictions sur test_df, les prédictions sur train_df et le score fait avec le modèle model\"\"\"\n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n    predictions = model.predict(test_X)\n    train_predictions=model.predict(train_X)\n    conf_matrix(train_predictions , titre)\n    score=model.score(train_X, y)\n    print (\"Score Train -->\", round(score *100,2), \" %\")\n    pred={\"model\":str(model), \"predictions\":predictions, \"train_predictions\":train_predictions, \"score\":score, \"tfidf\":tfidf}\n    return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grid_search(param, base,tfidf=False):\n    \"\"\"\n    Arguments :\n    :param:parametre du classificateur\n    :base:model classifier\n    :tfidf:booleen\n    :return: model classifier\n    \n     Recherche du meilleur classificateur grace à une grid search\"\"\"\n    \n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n    sh=HalvingGridSearchCV(base, param, cv=5,factor=2, max_resources=50).fit(train_X, y)\n    model=sh.best_estimator_\n    print(str(model)[:200])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lc=predire(LogisticRegression().fit(X,y), \"logistic regression\")\npred_lc_tfidf=predire(LogisticRegression().fit(X_tfidf,y),\"logistic regression tfidf\", tfidf=True)\n\ncompare_model.append(pred_lc)\ncompare_model.append(pred_lc_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_lc = { 'max_iter':[50,100,150,200], 'random_state':[None, 0,1]}\nbase_lc = LogisticRegression()\n\npred_lc_gs=predire(grid_search(param_lc, base_lc), \"logistic regression grid search\")\npred_lc_tfidf_gs=predire(grid_search(param_lc, base_lc, tfidf=True), \"logistic regression grid search tfidf\",tfidf=True)\n\ncompare_model.append(pred_lc_gs)\ncompare_model.append(pred_lc_tfidf_gs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_rf=predire(RandomForestClassifier().fit(X,y), \"random forest\")\npred_rf_tfidf=predire(RandomForestClassifier().fit(X_tfidf,y),\"random forest tfidf\", tfidf=True)\n\ncompare_model.append(pred_rf)\ncompare_model.append(pred_rf_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_rf = {'n_estimators':[1,10, 100, 150], 'max_depth':[None, 3,10,50,100], 'random_state':[None, 0,1]}\nbase_rf = RandomForestClassifier()\n\npred_rf_gs=predire(grid_search(param_rf, base_rf), \"random forest grid search\")\npred_rf_tfidf_gs=predire(grid_search(param_rf, base_rf, tfidf=True), \"random forest grid search tfidf\", tfidf=True)\n\ncompare_model.append(pred_rf_gs)\ncompare_model.append(pred_rf_tfidf_gs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_gnb=predire(GaussianNB().fit(X,y), \"Gaussian NB\")\npred_gnb_tfidf=predire(GaussianNB().fit(X_tfidf,y),\"Gaussian NB tfidf\", tfidf=True)\n\ncompare_model.append(pred_gnb)\ncompare_model.append(pred_gnb_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_xgb=predire(XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"], random_state=0).fit(X,y), \"xgboost\")\npred_xgb_tfidf=predire(XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"], random_state=0).fit(X_tfidf,y),\"xgboost tfidf\", tfidf=True)\n\ncompare_model.append(pred_xgb)\ncompare_model.append(pred_xgb_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_xgb = {'n_estimators':[1,10, 100, 150], 'random_state':[None, 0,1], 'max_depth':[None, 3,10,50,100]}\nbase_xgb = XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"])\n\npred_xgb_gs=predire(grid_search(param_xgb, base_xgb), \"xgboost grid search\")\npred_xgb_tfidf_gs=predire(grid_search(param_xgb, base_xgb, tfidf=True), \"xgboost grid search tfidf\", tfidf=True)\n\ncompare_model.append(pred_xgb_gs)\ncompare_model.append(pred_xgb_tfidf_gs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\nclf4=XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"])\n\ndef voting_classifier(param='hard', lr=1, rf=1, gnb=1, xgb=1, titre='', tfidf=False):\n    \"\"\"\n    Arguments :\n    :param:str, parametre du voting classifier (\"hard\" ou \"soft\")\n    :lr,rf,xgb: int\n    :titre: str\n    :tfidf:booleen\n    :return: fct predire()\n    \n     Création du modèle voting classifier avec ses poids lr, rf, gnb, xgb et param\"\"\"\n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n        \n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),('gnb', clf3), ('xgb', clf4)], voting=param, weights=[lr,rf,gnb, xgb]) \n    eclf1 = eclf1.fit(train_X, y)\n    return predire(eclf1, titre, tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_voting_hard=voting_classifier(\"hard\",titre=\"Voting hard\")\npred_voting_hard_tfidf=voting_classifier(\"hard\",titre=\"voting hard tfidf\",tfidf=True)\n\ncompare_model.append(pred_voting_hard)\ncompare_model.append(pred_voting_hard_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_voting_soft=voting_classifier(\"soft\",titre=\"voting soft\")\npred_voting_soft_tfidf=voting_classifier(\"soft\", titre=\"voting soft tfidf\",tfidf=True)\n\ncompare_model.append(pred_voting_soft)\ncompare_model.append(pred_voting_soft_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_soft_1_5rfxgb=voting_classifier(\"soft\",titre=\"voting soft, poids random forest=1.5 xgboost=1.5\",rf=1.5, xgb=1.5)\npred_soft_1_5rfxgb_tfidf=voting_classifier(\"soft\", titre=\"voting soft, poids random forest=1.5 xgboost=1.5 tfidf\",rf=1.5, xgb=1.5, tfidf=True)\n\ncompare_model.append(pred_soft_1_5rfxgb)\ncompare_model.append(pred_soft_1_5rfxgb_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_hard_1_5rfxgb=voting_classifier(\"hard\",titre=\"voting hard, poids random forest=1.5 xgboost=1.5\",rf=1.5, xgb=1.5)\npred_hard_1_5rfxgb_tfidf=voting_classifier(\"hard\", titre=\"voting hard, poids random forest=1.5 xgboost=1.5 tfidf\",rf=1.5, xgb=1.5, tfidf=True)\n\ncompare_model.append(pred_hard_1_5rfxgb)\ncompare_model.append(pred_hard_1_5rfxgb_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **V-Resultats**\n#### On réccupère les modèles de calcul ayant le meilleur score.","metadata":{}},{"cell_type":"code","source":"best_score=np.max(np.array([compare_model[i][\"score\"] for i in range(len(compare_model))]))\nmeilleurs_models=[]\nfor i in range(len(compare_model)):\n    if compare_model[i][\"score\"]==best_score :\n        meilleurs_models.append(compare_model[i])\nfor i in range(len(meilleurs_models)):\n    print(meilleurs_models[i][\"model\"][:200],\"\\n tfidf:\",meilleurs_models[i][\"tfidf\"],\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(meilleurs_models[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### On utilise les résultats du premier modèle ayant le meilleur score.\n","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'id': test_df.id, 'target': meilleurs_models[0][\"predictions\"]})\noutput.to_csv('my_submission.csv', index=False)\nsubmition_df=pd.read_csv('my_submission.csv')\nsubmition_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}