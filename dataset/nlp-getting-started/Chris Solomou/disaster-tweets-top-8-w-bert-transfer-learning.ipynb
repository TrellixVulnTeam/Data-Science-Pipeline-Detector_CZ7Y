{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:21:38.992226Z","iopub.execute_input":"2021-09-10T10:21:38.992615Z","iopub.status.idle":"2021-09-10T10:21:38.997411Z","shell.execute_reply.started":"2021-09-10T10:21:38.992577Z","shell.execute_reply":"2021-09-10T10:21:38.996428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install dependencies \n# A dependency of the preprocessing for BERT inputs\n!pip install -q -U tensorflow-text\n# A dependency for using the AdamW optimizer\n!pip install -q tf-models-official \n\n# Load necessary modules \nfrom sklearn.model_selection import train_test_split \nimport shutil \nimport os\nimport tensorflow as tf \nimport numpy as np \nimport pandas as pd \nimport tensorflow_hub as hub \nimport tensorflow_text as text \nfrom official.nlp import optimization \nimport matplotlib.pyplot as plt \n\ntf.get_logger().setLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:21:39.003546Z","iopub.execute_input":"2021-09-10T10:21:39.004084Z","iopub.status.idle":"2021-09-10T10:21:55.66774Z","shell.execute_reply.started":"2021-09-10T10:21:39.004031Z","shell.execute_reply":"2021-09-10T10:21:55.666275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n# Select only the columns of interest\ntrain_df = train_df[['text','target']]\n# Split the training data so we get validation data as well  \nX_train, X_test, y_train, y_test = train_test_split(train_df['text'],train_df['target'],test_size = 0.12, random_state = 42, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:21:55.670119Z","iopub.execute_input":"2021-09-10T10:21:55.670457Z","iopub.status.idle":"2021-09-10T10:21:55.714695Z","shell.execute_reply.started":"2021-09-10T10:21:55.670423Z","shell.execute_reply":"2021-09-10T10:21:55.713874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Basic information about the model and preprocessing:**\n* The weights of this model are those released by the original BERT authors. \n* This model has been pre-trained for English on the Wikipedia and BooksCorpus. \n\n* Text inputs has been lower-cased before tokenization into word pieces, and any accent markers have been stripped.\n* For training, random input masking has been applied independently to word pieces (as in the original BERT paper).","metadata":{}},{"cell_type":"code","source":"# Build the model using Keras functional API \ndef build_model():\n  # Get the shape of the input text\n  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  # Load preprocessing layer tensorflow hub \n  preprocessing_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\", name='preprocessing_layer')\n  # Apply preprocessing \n  processed_text = preprocessing_layer(input_text)\n  # Load encoder from tensorflow hub \n  encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=True, name='encoder')\n  # Apply encoding \n  outputs = encoder(processed_text)\n  # The BERT model returns a map with 3 keys: pooled_output, sequence_output, encoder_outputs\n  # For the fine-tuning we are going to use the pooled_output array which creates an embedding \n  # entire dataset\n  x = outputs['pooled_output']\n  # Apply Dropout to avoid overfitting \n  x = tf.keras.layers.Dropout(0.1)(x)\n  # Apply the classifier layer and use sigmoid activation function for fine-tuning \n  x = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(x)\n  return tf.keras.Model(input_text, x)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:21:55.717266Z","iopub.execute_input":"2021-09-10T10:21:55.717987Z","iopub.status.idle":"2021-09-10T10:21:55.727266Z","shell.execute_reply.started":"2021-09-10T10:21:55.717939Z","shell.execute_reply":"2021-09-10T10:21:55.726041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model \nmodel = build_model()\n# Use Binary Cross Entropy as loss function \nloss = tf.keras.losses.BinaryCrossentropy()\n# Use Binary Accuracy to assess fitness accuracy \nmetrics = tf.metrics.BinaryAccuracy()\n# Plot the model \ntf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:21:55.729284Z","iopub.execute_input":"2021-09-10T10:21:55.729688Z","iopub.status.idle":"2021-09-10T10:22:14.516692Z","shell.execute_reply.started":"2021-09-10T10:21:55.729651Z","shell.execute_reply":"2021-09-10T10:22:14.515287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2\n\nsteps_per_epoch = tf.data.experimental.cardinality(tf.data.Dataset.range(len(train_df))).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\n\n#For fine-tuning, we use the same optimizer that BERT was originally trained. \n#This optimizer minimizes the prediction loss and does regularization by weight decay (aka AdamW).\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:22:14.518689Z","iopub.execute_input":"2021-09-10T10:22:14.519019Z","iopub.status.idle":"2021-09-10T10:22:14.535147Z","shell.execute_reply.started":"2021-09-10T10:22:14.518986Z","shell.execute_reply":"2021-09-10T10:22:14.53433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x = X_train,\n                    y = y_train, \n                    validation_data = (X_test,y_test),\n                    batch_size = 16, \n                    epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:22:14.536295Z","iopub.execute_input":"2021-09-10T10:22:14.536741Z","iopub.status.idle":"2021-09-10T13:26:59.742303Z","shell.execute_reply.started":"2021-09-10T10:22:14.536713Z","shell.execute_reply":"2021-09-10T13:26:59.741232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model \nmodel.save('/kaggle/working/classifier_model', include_optimizer = True)\n# Load the test data \ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n# Predict the target values \npredictions = model.predict(test_df['text'])\n# Squeeze them to a list \npredictions = tf.squeeze(predictions, axis = 1)\n# Apply rounding so we get values between 0 and 1\npredictions = np.rint(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T13:26:59.744593Z","iopub.execute_input":"2021-09-10T13:26:59.745066Z","iopub.status.idle":"2021-09-10T13:40:27.300577Z","shell.execute_reply.started":"2021-09-10T13:26:59.745032Z","shell.execute_reply":"2021-09-10T13:40:27.299319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_curves(history):\n    '''\n    Returns loss curves for training and validation metrics (if available)\n    '''\n    if \"val_loss\" in history.history:\n        loss = history.history[\"loss\"]\n        val_loss = history.history[\"val_loss\"]\n        accuracy = history.history[\"binary_accuracy\"]\n        val_accuracy = history.history[\"val_binary_accuracy\"]\n\n        epochs = range(len(history.history[\"loss\"])) #number of epochs \n\n        # Plot losses \n        plt.figsize=(10,7)\n        plt.plot(epochs, loss, label = 'training_loss')\n        plt.plot(epochs, val_loss, label = 'val_loss')\n        plt.title('loss')\n        plt.xlabel('epochs')\n        plt.legend()\n\n        # Plot accuracy \n        plt.figure()\n        plt.plot(epochs, accuracy, label = 'training_accuracy')\n        plt.plot(epochs, val_accuracy, label = 'val_accuracy')\n        plt.title('accuracy')\n        plt.xlabel('epochs')\n        plt.legend()\n    \n    else:\n        # Plot training loss and accuracy together \n        loss = history.history[\"loss\"]\n        accuracy = history.history[\"accuracy\"]\n\n        epochs = range(len(history.history[\"loss\"])) #number of epochs \n\n        fig, ax1 = plt.subplots(figsize=(11, 9))\n        ax1.plot(epochs, accuracy, label = 'training_accuracy')\n        plt.xlabel('epochs')\n        ax1.set_ylabel('Training Accuracy')\n        \n        ax2 = ax1.twinx()\n        ax2.plot(epochs, loss, label = 'training_loss', color = 'tab:red')\n        ax2.set_ylabel('Training Loss')\n        \nplot_loss_curves(history)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T13:40:27.303185Z","iopub.execute_input":"2021-09-10T13:40:27.303665Z","iopub.status.idle":"2021-09-10T13:40:27.778229Z","shell.execute_reply.started":"2021-09-10T13:40:27.303603Z","shell.execute_reply":"2021-09-10T13:40:27.777265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission data \ntest_df['target'] = predictions \ntest_df['target'] = test_df['target'].astype(int)\nsubmission = test_df[['id','target']]\nsubmission.to_csv('submission.csv', index = False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-09-10T13:40:27.780506Z","iopub.execute_input":"2021-09-10T13:40:27.780959Z","iopub.status.idle":"2021-09-10T13:40:27.818788Z","shell.execute_reply.started":"2021-09-10T13:40:27.780926Z","shell.execute_reply":"2021-09-10T13:40:27.817303Z"},"trusted":true},"execution_count":null,"outputs":[]}]}