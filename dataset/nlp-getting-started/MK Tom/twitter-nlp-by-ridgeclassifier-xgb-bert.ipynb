{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n**This notebook linear model part is based on the tutorial notebook**\n\nhttps://www.kaggle.com/philculliton/nlp-getting-started-tutorial\n\n**The sections for RidgeClassifier and XGBClassifier do not contribute to the final score but they are alternative models for this problem**\n\n**The BERT section contributes to the final score**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nimport re\nfrom collections import Counter\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, decomposition\nimport xgboost as xgb \n!pip install contractions\nimport IPython\nimport contractions\nfrom datetime import datetime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-21T03:05:51.283929Z","iopub.execute_input":"2021-12-21T03:05:51.284361Z","iopub.status.idle":"2021-12-21T03:06:10.26614Z","shell.execute_reply.started":"2021-12-21T03:05:51.284318Z","shell.execute_reply":"2021-12-21T03:06:10.265408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint(len(train_df), len(test_df))","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-21T03:06:10.268162Z","iopub.execute_input":"2021-12-21T03:06:10.268432Z","iopub.status.idle":"2021-12-21T03:06:10.333975Z","shell.execute_reply.started":"2021-12-21T03:06:10.268398Z","shell.execute_reply":"2021-12-21T03:06:10.333285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing for linear models","metadata":{}},{"cell_type":"code","source":"twt = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\nstop = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\nstemmer = nltk.stem.PorterStemmer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n# print(stop)\n\ndef clean_text(df, col='text', normalize='lemmatize', stopwords=True, add_keyword=False, fill_empty='NULL', drop_dupe=False, shuffle=False):\n    cleaned_text, pos, neg = [], [], []\n    text_df = df[col]\n    if drop_dupe:\n        df = df.drop_duplicates(col)\n    \n    try: \n        targets = df.target\n    except:\n        targets = -np.ones(len(df))\n        \n    if add_keyword:\n        df.keyword = df.keyword.str.replace(\"%20\", \" \").fillna(\"\")\n        text_df = df.text + \" \" + df.keyword\n    \n    for (target, text) in zip(targets, text_df):\n#         print(text)\n        text = text.lower().split(\" \")\n        text = [word for word in text if \"http\" not in word]\n        text = [contractions.fix(word) for word in text]\n        text = \" \".join(text).lower()\n        text = re.sub(r'\\d+|#', '', text)\n        text = twt.tokenize(text)\n        if stopwords:\n            text = [word for word in text if word not in stop]\n        text = [word for word in text if word not in [\"rt\", \"û_\", \"amp\", \"ûª\", \"ûªs\", \"ûò\", \"ûï\", \"ûó\", \"åè\", \"ìñ1\", \"\\x89\", \"...\", \"..\", \"via\"]]\n        if normalize == 'lemmatize':\n            text = [lemmatizer.lemmatize(word) for word in text]\n        if normalize == 'stem':\n            text = [stemmer.stem(word) for word in text]\n            \n        if target == 1: \n            pos.append(text)\n        if target == 0: \n            neg.append(text)\n        text = \" \".join(text)\n        cleaned_text.append(text)\n#         print(text)\n        \n    df[\"clean_text\"] = cleaned_text\n    df.clean_text = df.clean_text.replace(\"%20\", \" \")\n    if fill_empty != False:\n        df.loc[df.clean_text.str.len() == 0, 'clean_text'] = fill_empty\n    if shuffle:\n        df = df.sample(frac=1)\n    \n    return pos, neg, df\n        \npos_text, neg_text, train_df = clean_text(train_df, add_keyword=False, drop_dupe=True, shuffle=True)\n_, _, test_df = clean_text(test_df, add_keyword=False)\npos_text = [item for sublist in pos_text for item in sublist]\nneg_text = [item for sublist in neg_text for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:10.33516Z","iopub.execute_input":"2021-12-21T03:06:10.335689Z","iopub.status.idle":"2021-12-21T03:06:15.185198Z","shell.execute_reply.started":"2021-12-21T03:06:10.335649Z","shell.execute_reply":"2021-12-21T03:06:15.18449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_common = pd.DataFrame(Counter(pos_text).most_common(20))\nneg_common = pd.DataFrame(Counter(neg_text).most_common(20))\npd.concat([pos_common, neg_common], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.186327Z","iopub.execute_input":"2021-12-21T03:06:15.186645Z","iopub.status.idle":"2021-12-21T03:06:15.218148Z","shell.execute_reply.started":"2021-12-21T03:06:15.186608Z","shell.execute_reply":"2021-12-21T03:06:15.217441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_df.loc[(train_df.clean_text == \"NULL\"), :])\ndisplay(test_df.loc[(test_df.clean_text == \"NULL\"), :])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.220329Z","iopub.execute_input":"2021-12-21T03:06:15.220868Z","iopub.status.idle":"2021-12-21T03:06:15.244339Z","shell.execute_reply.started":"2021-12-21T03:06:15.220833Z","shell.execute_reply":"2021-12-21T03:06:15.243725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_df.sample(frac=1).head(10))\ndisplay(test_df.sample(frac=1).head(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.245475Z","iopub.execute_input":"2021-12-21T03:06:15.24578Z","iopub.status.idle":"2021-12-21T03:06:15.269226Z","shell.execute_reply.started":"2021-12-21T03:06:15.245736Z","shell.execute_reply":"2021-12-21T03:06:15.268644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Count and Vectorize approach (1-gram)","metadata":{}},{"cell_type":"code","source":"feature_col = \"clean_text\"\n\ncount_vectorizer = feature_extraction.text.CountVectorizer()\ncount_vectorizer_sw = feature_extraction.text.CountVectorizer()\ntfidf = feature_extraction.text.TfidfVectorizer()\nLSA = decomposition.TruncatedSVD(n_components=100)\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:50])\nexample_train_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col][0:50])\nexample_tfidf = tfidf.fit_transform(train_df[feature_col][0:50])\nexample_tfidf_lsa = LSA.fit_transform(example_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.270385Z","iopub.execute_input":"2021-12-21T03:06:15.270625Z","iopub.status.idle":"2021-12-21T03:06:15.316519Z","shell.execute_reply.started":"2021-12-21T03:06:15.270593Z","shell.execute_reply":"2021-12-21T03:06:15.315835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint('No cleaning')\nprint(example_train_vectors[0].todense().shape)\n# print(example_train_vectors[0].todense())\nprint('Cleaned')\nprint(example_train_vectors_sw[0].todense().shape)\n# print(example_train_vectors_sw[0].todense())\nprint('TF-IDF cleaned')\nprint(example_tfidf[0].todense().shape)\n# print(example_tfidf[0].todense())\nprint('TF-IDF + LSA cleaned')\nprint(example_tfidf_lsa[0].shape)\n# print(example_tfidf_lsa[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.320907Z","iopub.execute_input":"2021-12-21T03:06:15.323024Z","iopub.status.idle":"2021-12-21T03:06:15.334662Z","shell.execute_reply.started":"2021-12-21T03:06:15.322982Z","shell.execute_reply":"2021-12-21T03:06:15.333686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_df[feature_col])\ntrain_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col])\ntrain_tfidf = tfidf.fit_transform(train_df[feature_col])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[feature_col])\ntest_vectors_sw = count_vectorizer_sw.transform(test_df[feature_col])\ntest_tfidf = tfidf.transform(test_df[feature_col])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.33595Z","iopub.execute_input":"2021-12-21T03:06:15.336419Z","iopub.status.idle":"2021-12-21T03:06:15.860601Z","shell.execute_reply.started":"2021-12-21T03:06:15.336318Z","shell.execute_reply":"2021-12-21T03:06:15.859846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidf_lsa = LSA.fit_transform(train_tfidf)\ntest_tfidf_lsa = LSA.transform(test_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:15.862008Z","iopub.execute_input":"2021-12-21T03:06:15.862269Z","iopub.status.idle":"2021-12-21T03:06:16.530644Z","shell.execute_reply.started":"2021-12-21T03:06:15.862235Z","shell.execute_reply":"2021-12-21T03:06:16.529798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Linear Model: Ridge Classifier**","metadata":{}},{"cell_type":"code","source":"# clf = linear_model.RidgeClassifier(class_weight='balanced')\n# ridge_params = {\n#     \"alpha\": np.linspace(0, 2, 100),\n#     \"tol\": np.linspace(1e-5, 1e-1, 2000)\n# }\n# ridge_rscv = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)\n# ridge_rscv_lsa = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.532187Z","iopub.execute_input":"2021-12-21T03:06:16.5327Z","iopub.status.idle":"2021-12-21T03:06:16.536948Z","shell.execute_reply.started":"2021-12-21T03:06:16.532662Z","shell.execute_reply":"2021-12-21T03:06:16.536051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# search = ridge_rscv.fit(train_tfidf, train_df[\"target\"])\n# search_lsa = ridge_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n# IPython.display.clear_output()\n# print(\"Best RidgeClassifier TF-IDF\")\n# print(search.best_score_)\n# print(search.best_params_)\n# print(\"Best RidgeClassifier TF-IDF LSA\")\n# print(search_lsa.best_score_)\n# print(search_lsa.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.538567Z","iopub.execute_input":"2021-12-21T03:06:16.539205Z","iopub.status.idle":"2021-12-21T03:06:16.549244Z","shell.execute_reply.started":"2021-12-21T03:06:16.539167Z","shell.execute_reply":"2021-12-21T03:06:16.548444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores_tfidf = model_selection.cross_validate(clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# scores_tfidf_lsa = model_selection.cross_validate(clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.550469Z","iopub.execute_input":"2021-12-21T03:06:16.551276Z","iopub.status.idle":"2021-12-21T03:06:16.558067Z","shell.execute_reply.started":"2021-12-21T03:06:16.551235Z","shell.execute_reply":"2021-12-21T03:06:16.557236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores_tfidf = model_selection.cross_validate(ridge_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# scores_tfidf_lsa = model_selection.cross_validate(ridge_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"Best RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('Best RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"Best RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('Best RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('Best RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('Best RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.563369Z","iopub.execute_input":"2021-12-21T03:06:16.566357Z","iopub.status.idle":"2021-12-21T03:06:16.570748Z","shell.execute_reply.started":"2021-12-21T03:06:16.566314Z","shell.execute_reply":"2021-12-21T03:06:16.569996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GBDT: XGB Classifier**\nTurns out not as good as ridge","metadata":{}},{"cell_type":"code","source":"# xgb_clf = xgb.XGBClassifier(random_state=765, tree_method='gpu_hist', predictor='gpu_predictor')\n# xgb_params = {\n#     \"max_depth\": [i for i in range(4, 14)],\n#     \"min_child_weight\": np.linspace(0.25, 0.45, 100),\n#     \"gamma\": np.linspace(0, 0.015, 1000),\n#     \"learning_rate\": np.linspace(0.2, 0.5, 100),\n# }\n# xgb_rscv = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)\n# xgb_rscv_lsa = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.573476Z","iopub.execute_input":"2021-12-21T03:06:16.574003Z","iopub.status.idle":"2021-12-21T03:06:16.581815Z","shell.execute_reply.started":"2021-12-21T03:06:16.573964Z","shell.execute_reply":"2021-12-21T03:06:16.581102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# search = xgb_rscv.fit(train_tfidf, train_df[\"target\"])\n# search_lsa = xgb_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n# IPython.display.clear_output()\n# print(\"Best XGBClassifier TF-IDF\")\n# print(search.best_score_)\n# print(search.best_params_)\n# print(\"Best XGBClassifier TF-IDF LSA\")\n# print(search_lsa.best_score_)\n# print(search_lsa.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.583428Z","iopub.execute_input":"2021-12-21T03:06:16.584Z","iopub.status.idle":"2021-12-21T03:06:16.590777Z","shell.execute_reply.started":"2021-12-21T03:06:16.583958Z","shell.execute_reply":"2021-12-21T03:06:16.590114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores_tfidf = model_selection.cross_validate(xgb_clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n# scores_tfidf_lsa = model_selection.cross_validate(xgb_clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.592198Z","iopub.execute_input":"2021-12-21T03:06:16.592708Z","iopub.status.idle":"2021-12-21T03:06:16.59958Z","shell.execute_reply.started":"2021-12-21T03:06:16.592671Z","shell.execute_reply":"2021-12-21T03:06:16.598886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores_tfidf = model_selection.cross_validate(xgb_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n# scores_tfidf_lsa = model_selection.cross_validate(xgb_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"Best XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('Best XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"Best XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('Best XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('Best XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('Best XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.601165Z","iopub.execute_input":"2021-12-21T03:06:16.601668Z","iopub.status.idle":"2021-12-21T03:06:16.610237Z","shell.execute_reply.started":"2021-12-21T03:06:16.601633Z","shell.execute_reply":"2021-12-21T03:06:16.609519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN CONTENT: BERT**\n\n**Note that this is not the best version, fine tunings and validation may help obtain a better score**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:16.61152Z","iopub.execute_input":"2021-12-21T03:06:16.612068Z","iopub.status.idle":"2021-12-21T03:06:18.023738Z","shell.execute_reply.started":"2021-12-21T03:06:16.612032Z","shell.execute_reply":"2021-12-21T03:06:18.023126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding additional tokens for masking URLs and usernames in tweets","metadata":{}},{"cell_type":"code","source":"bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\nbert_tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:18.025057Z","iopub.execute_input":"2021-12-21T03:06:18.025328Z","iopub.status.idle":"2021-12-21T03:06:18.032956Z","shell.execute_reply.started":"2021-12-21T03:06:18.02529Z","shell.execute_reply":"2021-12-21T03:06:18.032197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As BERT is able to read complete passages and learn from the context, too much text preprocessing may not be beneficial.\n\nSome minor preprocessing with URLs, @usernames, and #hashtag, as they may be tokenized weirdly and the token make no sense\n\n*Note: The BERT model still did pretty good without the above processing*\n\nThen tokenize the data","metadata":{}},{"cell_type":"code","source":"def bert_tokenize(df, tokenizer=bert_tokenizer, max_seq_len = 100):\n    input_sequences = []\n    # The attention mask is an optional argument used when batching sequences together.\n    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n    attention_masks = []\n    bert_text = []\n    \n    # some very minor text processing, try to keep the text as close as original\n    for i, text in enumerate(df['text']):\n#         print(i, text)\n        text = text.replace(\"\\n\", \" \").split(\" \")\n        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n        text = \" \".join(text)\n        text = re.sub(r'#', '', text)\n        bert_text.append(text)\n        \n#         print(i, text)\n        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n        input_ids = sequence_dict['input_ids']\n        att_mask = sequence_dict['attention_mask']\n#         print(i, tokenizer.tokenize(text))\n        input_sequences.append(input_ids)\n        attention_masks.append(att_mask)\n    \n    df['bert_text'] = bert_text\n    return input_sequences, attention_masks, df\n\ntrain_X, train_att, train_df = bert_tokenize(train_df)\ntrain_y = train_df['target'].values\ntest_X, test_att, test_df = bert_tokenize(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:11:51.343989Z","iopub.execute_input":"2021-12-21T03:11:51.344273Z","iopub.status.idle":"2021-12-21T03:11:58.264497Z","shell.execute_reply.started":"2021-12-21T03:11:51.344238Z","shell.execute_reply":"2021-12-21T03:11:58.263767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the tokenized format\nprint(train_X[0])\nprint(train_att[0])\nprint(test_X[0])\nprint(test_att[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:25.170787Z","iopub.execute_input":"2021-12-21T03:06:25.171033Z","iopub.status.idle":"2021-12-21T03:06:25.176826Z","shell.execute_reply.started":"2021-12-21T03:06:25.170999Z","shell.execute_reply":"2021-12-21T03:06:25.176081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Forming dataset","metadata":{}},{"cell_type":"code","source":"train_X = torch.tensor(train_X)\ntrain_y = torch.tensor(train_y)\ntrain_att = torch.tensor(train_att)\ntest_X = torch.tensor(test_X)\ntest_att = torch.tensor(test_att)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:25.178026Z","iopub.execute_input":"2021-12-21T03:06:25.178471Z","iopub.status.idle":"2021-12-21T03:06:25.443672Z","shell.execute_reply.started":"2021-12-21T03:06:25.178435Z","shell.execute_reply":"2021-12-21T03:06:25.442954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\ntrain_sampler = torch.utils.data.RandomSampler(train_data)\ntrain_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\ntest_data = torch.utils.data.TensorDataset(test_X, test_att)\ntest_sampler = torch.utils.data.SequentialSampler(test_data)\ntest_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:25.444843Z","iopub.execute_input":"2021-12-21T03:06:25.445086Z","iopub.status.idle":"2021-12-21T03:06:25.452889Z","shell.execute_reply.started":"2021-12-21T03:06:25.445055Z","shell.execute_reply":"2021-12-21T03:06:25.450742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pretrained model from bert-base-uncased**\n\nresize_token_embeddings is required as we have added new special tokens","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel.resize_token_embeddings(len(bert_tokenizer))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:25.454314Z","iopub.execute_input":"2021-12-21T03:06:25.454614Z","iopub.status.idle":"2021-12-21T03:06:42.064893Z","shell.execute_reply.started":"2021-12-21T03:06:25.454575Z","shell.execute_reply":"2021-12-21T03:06:42.064148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel.to(device)\nIPython.display.clear_output()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:42.066139Z","iopub.execute_input":"2021-12-21T03:06:42.066541Z","iopub.status.idle":"2021-12-21T03:06:46.486553Z","shell.execute_reply.started":"2021-12-21T03:06:42.066502Z","shell.execute_reply":"2021-12-21T03:06:46.485855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nloss_fct = torch.nn.NLLLoss()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:46.487639Z","iopub.execute_input":"2021-12-21T03:06:46.489372Z","iopub.status.idle":"2021-12-21T03:06:46.49972Z","shell.execute_reply.started":"2021-12-21T03:06:46.489331Z","shell.execute_reply":"2021-12-21T03:06:46.499089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define train and test functions","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n    t0 = datetime.now()\n    model.train()\n    for i, batch in enumerate(train_dataloader, start=1):\n        batch = tuple(t.to(device) for t in batch)\n        inputs, att_masks, labels = batch\n        model.zero_grad()  \n        \n        logits = model(inputs, attention_mask=att_masks)\n        outputs = F.log_softmax(logits[0], dim=1)\n        \n        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        \n        if i % 20 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}'.format(\n                epoch, i * len(inputs), len(train_dataloader.dataset),\n                 i / len(train_dataloader), datetime.now() - t0, loss.item()\n            ))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:46.50077Z","iopub.execute_input":"2021-12-21T03:06:46.502396Z","iopub.status.idle":"2021-12-21T03:06:46.513941Z","shell.execute_reply.started":"2021-12-21T03:06:46.502355Z","shell.execute_reply":"2021-12-21T03:06:46.513259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test():\n    t0 = datetime.now()\n    model.eval()\n    test_loss, test_acc = 0, 0\n    for batch in test_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs, att_masks, labels = batch\n        with torch.no_grad():\n            logits = model(inputs, attention_mask=att_masks)\n            outputs = F.log_softmax(logits[0], dim=1)\n            \n            loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n\n        test_loss += loss.item()\n        outputs = outputs.detach().cpu().numpy()\n\n        pred = np.argmax(outputs, axis=1)\n        labels = labels.cpu().numpy()\n        \n        test_acc += accuracy_score(pred, labels)\n\n    test_loss /= len(test_dataloader)\n    test_acc /= len(test_dataloader)\n    print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n        test_loss, test_acc, datetime.now() - t0\n    ))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:46.515238Z","iopub.execute_input":"2021-12-21T03:06:46.515621Z","iopub.status.idle":"2021-12-21T03:06:46.524356Z","shell.execute_reply.started":"2021-12-21T03:06:46.515585Z","shell.execute_reply":"2021-12-21T03:06:46.52348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_epoch = 2\nfor epoch in range(1, nb_epoch+1):\n    train(epoch)\n#     test()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:06:46.525798Z","iopub.execute_input":"2021-12-21T03:06:46.526351Z","iopub.status.idle":"2021-12-21T03:09:15.001092Z","shell.execute_reply.started":"2021-12-21T03:06:46.526314Z","shell.execute_reply":"2021-12-21T03:09:14.999923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating predictions for test data**","metadata":{}},{"cell_type":"code","source":"def predict(text):\n    # pre-process text\n    input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n    logits = model.eval()(input_ids=input_)[0]\n    pred = F.softmax(logits, dim=1)[0]\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:15.005084Z","iopub.execute_input":"2021-12-21T03:09:15.00711Z","iopub.status.idle":"2021-12-21T03:09:15.014295Z","shell.execute_reply.started":"2021-12-21T03:09:15.007063Z","shell.execute_reply":"2021-12-21T03:09:15.013264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor text in test_df.text:\n    prob = predict(text)\n    pred = np.argmax(prob.cpu().detach().numpy())\n    predictions.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:15.015599Z","iopub.execute_input":"2021-12-21T03:09:15.015917Z","iopub.status.idle":"2021-12-21T03:09:55.617804Z","shell.execute_reply.started":"2021-12-21T03:09:15.015877Z","shell.execute_reply":"2021-12-21T03:09:55.617088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:55.619096Z","iopub.execute_input":"2021-12-21T03:09:55.619519Z","iopub.status.idle":"2021-12-21T03:09:55.632407Z","shell.execute_reply.started":"2021-12-21T03:09:55.61948Z","shell.execute_reply":"2021-12-21T03:09:55.631519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_prediction = ridge_rscv.best_estimator_.predict(train_tfidf)\n# train_df['pred_target'] = train_prediction\n\n# ridge with rscv\n# sample_submission[\"target\"] = ridge_rscv.best_estimator_.predict(test_tfidf)\n\n# bert\nsample_submission[\"target\"] = predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:55.633696Z","iopub.execute_input":"2021-12-21T03:09:55.633958Z","iopub.status.idle":"2021-12-21T03:09:55.639764Z","shell.execute_reply.started":"2021-12-21T03:09:55.633925Z","shell.execute_reply":"2021-12-21T03:09:55.639111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean_text_wc = train_df.clean_text.str.count(' ').add(1)\n# short_text_incorrect = train_df.loc[(clean_text_wc < 5) & (train_df.target != train_df.pred_target), :]\n# (short_text_incorrect.target == 1).sum(), (short_text_incorrect.target == 0).sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:55.641444Z","iopub.execute_input":"2021-12-21T03:09:55.642015Z","iopub.status.idle":"2021-12-21T03:09:55.649721Z","shell.execute_reply.started":"2021-12-21T03:09:55.641973Z","shell.execute_reply":"2021-12-21T03:09:55.649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display(sample_submission.head(30))\n# display(test_df['text'].head(30))\npd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:12:16.324994Z","iopub.execute_input":"2021-12-21T03:12:16.32559Z","iopub.status.idle":"2021-12-21T03:12:16.348313Z","shell.execute_reply.started":"2021-12-21T03:12:16.325549Z","shell.execute_reply":"2021-12-21T03:12:16.347627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T03:09:55.677944Z","iopub.execute_input":"2021-12-21T03:09:55.678172Z","iopub.status.idle":"2021-12-21T03:09:55.691385Z","shell.execute_reply.started":"2021-12-21T03:09:55.678141Z","shell.execute_reply":"2021-12-21T03:09:55.690705Z"},"trusted":true},"execution_count":null,"outputs":[]}]}