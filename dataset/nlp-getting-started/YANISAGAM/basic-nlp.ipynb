{"cells":[{"metadata":{},"cell_type":"markdown","source":"# รายงานนี้เป็นส่วนหนึ่งของวิชา DSI 206 Multimedia Representation Management"},{"metadata":{},"cell_type":"markdown","source":"**รายชื่อสมาชิก**\n\n1. นางสาว ญาณิศา จินตนไชยวัฒน์ 6109656287\n2. นางสาวณัฐชยา ฉันเฟื่องฟู      6109656493\n3. นางสาวอัญวีณ์ พันธ์บูรณานนท์   6109656048"},{"metadata":{},"cell_type":"markdown","source":"# **1. Data Exploration**\n\n> ขั้นตอนแรก ต้องทำการสำรวจและวิเคราะห์ข้อมูลว่า data ของเรามีอะไรบ้าง สามารถนำข้อมูลส่วนใดไปใช้งานได้ \n\n> และมีส่วนใดที่ต้องทำการ clean เพื่อทำให้ model ที่ได้นั้นมีประสิทธิภาพมากที่สุด"},{"metadata":{},"cell_type":"markdown","source":"**- Import Library**\n\n> นำเข้า library ที่ต้องใช้งาน"},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear algebra\nimport numpy as np \n# data processing\nimport pandas as pd \n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# text processing libraries\nimport string\n# regular expression\nimport re\n# natural language processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\n# scikit-learn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Reading Dataset**\n> นำเข้าข้อมูลและเรียกดูข้อมูลหัวคอลัมน์ทั้ง Train และ Test data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load data\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Columns ทั้งหมดประกอบด้วย 5 columns คือ**\n1. **id** [ unique identifier ] เลขลำดับ id ของผู้ใช้\n2. **keyword** [ from that tweet ] คีย์เวิร์ดที่อยู่ใน tweet \n3. **location** [ that tweet sent from ] พิกัดของผู้ใช้งาน\n4. **text** [ of a tweet ] ข้อความในทวีต\n5. **target** - the label we want to predict [ 0 = non-disaster , 1 = disaster ] \n\n> ซึ่งใน test data จะไม่มี column target เพื่อใช้ทดสอบ model "},{"metadata":{},"cell_type":"markdown","source":"**- Check data shape of each data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data Shape',train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Data มี **5 columns**** (id , keyword , location , text , target) , และมี **7613 rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Testing Data Shape',test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing Data มี **4 columns** (id , keyword , location , text ) , และมี **3263 rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sub Sample Data Shape',sub_sample.shape)\nsub_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sub Sample มี **2 columns** (id , target) , และมี **3263 rows**\n"},{"metadata":{},"cell_type":"markdown","source":"**- Check Class Distribution of two classes (0 and 1) using Train Data**"},{"metadata":{},"cell_type":"markdown","source":"* ตรวจสอบจำนวน Target\n\n> 0 เป็นทวีตที่ไม่เกี่ยวข้องกับภัยพิบัติ และ\n> 1 เป็นทวีตที่เกี่ยวข้องกับภัยพิบัติ"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* สร้างตารางพื่อเปรียบเทียบจำนวนทวีตที่เกี่ยวข้องและไม่เกี่ยวข้องกับภัยพิบัติ"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train.groupby('target').count()['text'].reset_index()\ntemp['label'] = temp['target'].apply(lambda x : 'Disaster Tweet' if x==1 else 'Non Disaster Tweet')\ntemp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> จะเห็นว่าทวีตที่เกี่ยวข้องกับภัยพิบัติ (1) มีจำนวนน้อยกว่า ทวีตไม่เกี่ยวข้องกับภัยพิบัติ (0)"},{"metadata":{},"cell_type":"markdown","source":"* Graph เปรียบเทียบจำนวนข้อความที่เป็น disaster กับ non-disaster"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts()\n            ,palette='Spectral')\nplt.title('Comparing disaster tweets and non disaster tweets',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Counting Number of Missing Values**"},{"metadata":{},"cell_type":"markdown","source":"* check แต่ละคอลัมน์ของ Train data ว่ามี missing values จำนวนเท่าใด\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* check แต่ละคอลัมน์ของ Test data ว่ามี missing values จำนวนเท่าใด\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Target Distribution in ' Keyword '**"},{"metadata":{},"cell_type":"markdown","source":"* ตรวจสอบว่า Train และ Test มีจำนวน keyword ที่ไม่ซ้ำกันทั้งหมดเท่าใด ใน train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.nunique(),test.keyword.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 10 keyword ที่มีจำนวนมากที่สุดใน Train data\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(9,6))\n# Bar chart showing amount of keywords values\nsns.barplot(y=train['keyword'].value_counts()[:10].index,\n            x=train['keyword'].value_counts()[:10])\n# Add title\nplt.title(' Top 10 Keyword ') \n# Add label for x axis\nplt.xlabel('COUNT')\n# Add label for y axis\nplt.ylabel('KEYWORD')\n# Rotate the label text for hotizontal axis\nplt.xticks(rotation=90) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Top 10 Keyword ที่เกี่ยวภัยพิบัติ และที่ไม่เกี่ยวกับภัยพิบัติ"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create variables a,b (disaster , non-disaster)\na = train[train.target==1].keyword.value_counts().head(10)\nb = train[train.target==0].keyword.value_counts().head(10)\n# Set the width and height of the figure\nplt.figure(figsize=(13,5))\n# Bar chart showing amount of disaster keywords values\nplt.subplot(121)\nsns.barplot(a, a.index, color='orange')\n# Add title\nplt.title('Top keywords for disaster tweets')\n# Bar chart showing amount of non-disaster keywords values\nplt.subplot(122)\nsns.barplot(b, b.index, color='pink')\n# Add title\nplt.title('Top keywords for non-disaster tweets')\n# display a graph \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target Distribution in ' location '**"},{"metadata":{},"cell_type":"markdown","source":"* ตรวจสอบว่า Train และ Test มีจำนวน keyword ที่ไม่ซ้ำกันทั้งหมดเท่าใด ใน test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.location.nunique(),test.location.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 10 location ที่มีจำนวนสูงที่สุดใน Train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(9,6))\n# Bar chart showing amount of location values and groups the top 10 location\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:10].index)\n# Add title\nplt.title('Top 10 locations')\n# display a graph \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำการแทนที่ชื่อของรัฐต่างๆให้อยู่ในรูปแบบชื่อประเทศ \n\n> ให้ข้อมูลของ Location แสดงผลเป็นชื่อประเทศ"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                           \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* เปรียบเทียบจำนวนตัวอักษรทวีต ระหว่างทวีตที่เกี่ยวข้องกับภัยพิบัติและไม่เกี่ยวข้องกับภัยพิบัติ "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\n\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets',fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> จากกราฟแสดงให้เห็นว่า จำนวนทวีตที่เกี่ยวข้องกับภัยพิบัติมีจำนวนตัวอักษรในทวีตอยู่ในช่วง 120-140 \nเช่นเดียวกันกับทวีตที่ไม่เกี่ยวข้องกับภัยพิบัติ"},{"metadata":{},"cell_type":"markdown","source":"* เปรียบเทียบจำนวนคำในทวีต ระหว่างทวีตที่เกี่ยวข้องกับภัยพิบัติและไม่เกี่ยวข้องกับภัยพิบัติ "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweets',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> จากกราฟแสดงให้เห็นว่า ทวีตที่เกี่ยวข้องกับภัยพิบัติมีจำนวนคำอยู่ในช่วง 17-18 คำ มากที่สุด และทวีตที่ไม่เกี่ยวข้องกับภัยพิบัติมีจำนวนคำอยู่ในช่วง 18-19 คำ มากที่สุด"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Cleaning\n\n> จากการที่ได้สำรวจข้อมูลแล้ว ขั้นตอนต่อไป คือ การทำความสะอาดข้อมูล \n\n> โดยการลบ data ส่วนที่ไม่เกี่ยวข้องหรือส่วนที่ไม่จำเป็นออกไป ให้ข้อมูลมีคุณภาพมากที่สุด เพื่อเตรียมพร้อมในการทำ model "},{"metadata":{},"cell_type":"markdown","source":"* แทนที่ช่องว่าง null ด้วยคำว่า \n\n> no_location ในคอลัมน์ location\n\n> no_keyword ในคอลัมน์ Keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['keyword', 'location']:\n    train[col] = train[col].fillna(f'no_{col}')\nfor col in ['keyword', 'location']:\n    test[col] = test[col].fillna(f'no_{col}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำการ Clean data โดยใช้ Regular Expression \n\n> ทำตัวอักษรให้เป็นตัวพิมพ์เล็กทั้งหมด\n\n> ลบเครื่องหมายต่างๆที่ไม่ต้องการ เช่น !€@%#*&~ รวมไปถึง URL , HTML , ขึ้นบรรทัดใหม่ , เครื่องหมายวรรคตอน , คำที่มีตัวเลขคั่น "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub(r'<.*?>',' ' ,text)\n    \n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำการแบ่งคำ โดยใช้ RegexpTokenizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# Tokenizing the training and the test set\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำการลบ stop words ในภาษาอังกฤษ \n\n> เช่น the, a, at, for, above, on, is, all เป็นต้น โดยใช้ nltk library\nเนื่องจากไม่มีความหมายในการใช้วิเคราะห์ "},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\n    train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\n    test['text'] = test['text'].apply(lambda x : remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำการ remove emoji \n\n> โดยการแทน emoji เป็นรหัสในทุกๆหมวดหมู่ และลบโดยใช้ library re"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* เปลี่ยนคำในทวีตให้อยู่ในรูปเดียวกันโดยการทำ Stemmimg\n\n> คือการตัดคำลงท้ายออก (เช่น s, es, ed, ing) \n\n> stem สามารถเขียนได้หลายรูป stemmimg, stemmed,stems เราจะทำการแปลงให้มันอยู่ในรูปเดียวคือ stem"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemming(words):\n     ps=PorterStemmer()\n     return [ps.stem(word) for word in words]\ntrain['text']=train['text'].apply(lambda x: stemming(x))\ntest['text']=test['text'].apply(lambda x: stemming(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lemmatizing คือการทำให้คำที่อยู่ในรูปแบบต่างๆ แปลงกลับมาอยู่ในรูปปกติ root word\n\n> เช่น Feet เป็น Foot //\nwolves เป็น wolf //\nis,am,are เป็น be"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizing(words):\n            lemmatizer =WordNetLemmatizer()\n            return [lemmatizer.lemmatize(word) for word in words]\ntrain['text']=train['text'].apply(lambda x: lemmatizing(x))\ntest['text']=test['text'].apply(lambda x: lemmatizing(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* เอาคำในประโยคมาเรียงต่อกัน ด้วยการ join word "},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_text(words):\n     return ' '.join(words)\ntrain['text']=train['text'].apply(lambda x:final_text(x))\ntest['text']=test['text'].apply(lambda x:final_text(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Modeling\n"},{"metadata":{},"cell_type":"markdown","source":"# *> Bag-of-word*"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำ bag-of-word โดยการนับว่ามีคำๆนั้นปรากฎทั้งหมดกี่ครั้งในประโยค แล้วนำคำศัพท์ไปใส่ด้วย function CountVectorizer ที่จะแปลงข้อความเป็น vector เพื่อลดมิติของข้อมูล และนำไปคำนวนได้ \nโดยการนำกลุ่มของ token มีสร้างเป็น matrix โดยใช้กลุ่มของคำที่มีเป็นตัวอ้างอิง คำที่มีในประโยคจะถูกตั้งค่าเป็น 1 คำที่ไม่มีจะเป็น 0 \n\n> เช่น มีกลุ่มของคำ [“This”, “is”, “am”, “are”, “a”, “be”, “test”, “word”, “sentence”] \n\n> ประโยค “This is a test sentence” จะแปลงเป็น matrix ได้ดังนี้ [1, 1, 0, 0, 1, 0, 1, 0 ,1]"},{"metadata":{},"cell_type":"markdown","source":"* **ปัญหาที่พบ** คือ ทวีตที่ยาวจะมีน้ำหนักของคำเยอะกว่าทวีตที่สั้นกว่า ทำให้ผลลัพธ์ที่ได้ไม่แม่นยำ จึงเลือกใช้เป็น TF-IDF แทน เพื่อแก้ไขปัญหานี้"},{"metadata":{},"cell_type":"markdown","source":"# *> TF-IDF*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_vectors = tfidf.fit_transform(train['text'])\ntest_vectors = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ทำ bag-of-word โดยการใช้ TfidfVectorizer ที่เป็น function ใช้แปลงข้อความเป็น vector เช่นเดียวกันกับ CountVectorizer\n\n> เพื่อเพิ่มความถูกต้องในการนับ โดยการใช้ Term Frequency ของจำนวนครั้งที่คำนั้นปรากฏในทวีต คำนวณโดยใช้สูตร TF = จำนวนครั้งคำนั้นๆ/จำนวนคำทั้งหมดในแต่ละทวีต\n\n> แล้วจึงนำมาคูณกับ Inverse Document Frequency แล้วทำการ take log \n> วิธีนี้จึงเป็นวิธีการหาความสำคัญของคำ ที่ใช้แนวคิดว่ายิ่งคำนั้นปรากฏน้อยจะยิ่งมีความสำคัญมาก โดยนำค่า TF กับ IDF มาคูณกัน"},{"metadata":{},"cell_type":"markdown","source":"# *> ทำ Text Classification โดยการใช้ Logistic Regression และ Cross Validation*"},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation**\n* แบ่งข้อมูลเรียนรู้ออกเป็น k ชุดเท่า ๆ กัน และทำการคำนวน error จำนวน k รอบ\n* ใช้ข้อมูลส่วนที่เหลือ (k-1 ชุด) เพื่อทำการสร้าง model\n* เก็บข้อมูลที่แบ่งไว้ 1 ชุด เพื่อทำการ evaluate\n* วนทำซ้ำจนข้อมูลทุกส่วนถูกนำมาทดสอบ"},{"metadata":{},"cell_type":"markdown","source":"เราได้เลือกใช้ Logistic Regression ในการ predict probability โดยกำหนดค่า parameter C เท่ากับ 0.9 และเก็บในตัวแปร clf จากนั้นจึงคำนวนหาค่า error ของแต่ละรอบการคำนวน"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(C=0.9,max_iter=1000,penalty='l2')\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=7, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"นำผลที่ได้มา fit เข้ากับ train_vector และ column target ของ train"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"นำ model ที่ได้มา predict กับ test_vectors และนำมา check กับเฉลย คือ target ในไฟล์ sample_submission จากนั้นจึงสร้างไฟล์ csv เพื่อทำการ submit ผลที่ได้ใน kaggle "},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\n"},{"metadata":{},"cell_type":"markdown","source":"> sahib singh ,\"NLP Starter for Beginners\", https://www.kaggle.com/sahib12/nlp-starter-for-beginners\n\n> Bavalpreet ,\"NLP with Disaster Tweets\" , https://www.kaggle.com/bavalpreet26/nlp-with-disaster-tweets"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}