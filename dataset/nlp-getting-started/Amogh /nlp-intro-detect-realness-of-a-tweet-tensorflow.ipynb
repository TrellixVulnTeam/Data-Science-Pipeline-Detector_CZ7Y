{"cells":[{"metadata":{},"cell_type":"markdown","source":"A great course to get you started:\n\n[Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow/)\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"target = train['target']\nsns.countplot(target)\ntrain.drop(['target'], inplace =True,axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def concat_df(train, test):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train, test], sort=True).reset_index(drop=True)\ndf_all = concat_df(train, test)\nprint(train.shape)\nprint(test.shape)\nprint(df_all.shape)\ndf_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['keyword','location']\nfor feat in features : \n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(df_all[feat].isnull().sum())+ \" for the combined dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(train[feat].isnull().sum())+ \" for the train dataset\")\n    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(test[feat].isnull().sum())+ \" for the test dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# To check if there are any keywords which are missing in the train set but present in the test set\nkeyw_train = train['keyword'].unique()\nkeyw_test = test['keyword'].unique()\nprint(set(keyw_train)==set(keyw_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsentences = train['text']\n# 80% of total data\ntrain_size = int(7613*0.8)\ntrain_sentences = sentences[:train_size]\ntrain_labels = target[:train_size]\n\ntest_sentences = sentences[train_size:]\ntest_labels = target[train_size:]\n\n# Setting our parameters for the tokenizer (currently using default, we will tune them once we have optimised the rest of the model)\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(train_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(test_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(14, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_epochs = 10\nhistory = model.fit(padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us analyse our model performance in an accuracy vs epoch graph\nimport matplotlib.pyplot as plt\n\ndef plot(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\nplot(history, \"accuracy\")\nplot(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we jump on to applying our testing data let us retrain our model with the entire train set"},{"metadata":{"trusted":false},"cell_type":"code","source":"tokenizer_1 = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer_1.fit_on_texts(train['text'])\nword_index = tokenizer_1.word_index\nsequences = tokenizer_1.texts_to_sequences(train['text'])\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntrue_test_sentences = test['text']\ntesting_sequences = tokenizer_1.texts_to_sequences(true_test_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_2.summary()\nnum_epochs = 10\nhistory = model_2.fit(padded, target, epochs=num_epochs, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now let us deal with testing data\noutput = model_2.predict(testing_padded)\npred_plot =  pd.DataFrame(output, columns=['target'])\npred_plot.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_output = []\nfor val in pred_plot.target:\n    if val > 0.5:\n        final_output.append(1)\n    else:\n        final_output.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission['target'] = final_output\n# submission['id'] = test['id']\nsubmission.to_csv(\"final.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is just the baseline prediction, stay tuned for a updated version with data cleaning, feature generation, and more!  **"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}