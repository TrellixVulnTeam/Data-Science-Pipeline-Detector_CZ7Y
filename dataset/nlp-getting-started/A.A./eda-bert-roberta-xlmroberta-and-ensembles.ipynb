{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T17:03:59.150158Z","iopub.execute_input":"2022-06-21T17:03:59.150654Z","iopub.status.idle":"2022-06-21T17:03:59.201016Z","shell.execute_reply.started":"2022-06-21T17:03:59.15057Z","shell.execute_reply":"2022-06-21T17:03:59.199883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP Prediction of Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n* 1. [Data Loading](#dataloading)\n    * 1.1 [Libraries](#libs)\n    * 1.2 [NLTK and Spacy Language Load](#lang)\n    * 1.3 [Load CSV](#loadcsv)\n* 2. [Data Preprocessing and EDA](#preprocessing)\n    * 2.1 [Duplicates Removal](#dupl)\n    * 2.2 [Missing Values Removal](#misvals)\n    * 2.3 [Lowercasing](#lowercase)\n    * 2.4 [Most Common Punctuation](#punct)\n    * 2.5 [Most Common Locations](#toploc)\n    * 2.6 [Basic Cleaning](#basiccl)\n        * 2.6.1 [Tokenization, and Punctuations, Digits, URLs, Non-ASCII, Emails Removal](#basiccl1)\n        * 2.6.2 [Other Special Characters Removal](#basiccl2) \n    * 2.7 [Advanced Cleaning (Optional)](#advcl)\n        * 2.7.1 [Tokenization, Lemmatization, and Punctuations, Stop Words, Digits, URLs, Non-ASCII, Emails removal](#advcl1)\n        * 2.7.2 [Other Special Characters Removal](#advcl2)\n        * 2.7.3 [Non-English Words Removal](#advcl3)\n    * 2.8 [Duplicates Removal](#dupl2)\n* 3. [More of EDA](#eda)\n    * 3.1 [Word Clouds](#wordclouds)\n    * 3.2 [Word Frequencies](#wordfreqs)\n    * 3.3 [N-Gram Analysis](#ngrams)\n        * 3.3.1 [2-Grams](#twograms)\n        * 3.3.2 [3-Grams](#threegrams)\n    * 3.4 [Distribution of Characters](#charactersdist)\n    * 3.5 [Distribution of Words](#wordsdist)\n* 4. [Modeling](#modeling)\n    * 4.1 [Datasets Load](#datasetclass)\n    * 4.2 [Single Models](#singlemodel)\n    * 4.3 [Ensemble](#ensemble)\n    * 4.4 [Train/Validation/Test Split](#datasplit)\n    * 4.5 [Train and Test](#traintest)\n    * 4.6 [Predict](#predict)\n    * 4.7 [Run](#run)","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"dataloading\"></a>\n## 1. Data Loading","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Libraries <a class=\"anchor\" id=\"libs\"></a>","metadata":{}},{"cell_type":"code","source":"import warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport spacy\nimport joblib\nimport sys\nimport nltk\nimport gc\nfrom pathlib import Path\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom transformers import AdamW, BertTokenizer, RobertaTokenizer, XLMRobertaTokenizer, BertModel, RobertaModel, XLMRobertaModel\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:03.827544Z","iopub.execute_input":"2022-06-21T17:04:03.827963Z","iopub.status.idle":"2022-06-21T17:04:19.152282Z","shell.execute_reply.started":"2022-06-21T17:04:03.827923Z","shell.execute_reply":"2022-06-21T17:04:19.151218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:21.947975Z","iopub.execute_input":"2022-06-21T17:04:21.949239Z","iopub.status.idle":"2022-06-21T17:04:21.961033Z","shell.execute_reply.started":"2022-06-21T17:04:21.949129Z","shell.execute_reply":"2022-06-21T17:04:21.957001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 NLTK and Spacy Language Load <a class=\"anchor\" id=\"lang\"></a>","metadata":{}},{"cell_type":"code","source":"nltk.download('words')\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:24.164046Z","iopub.execute_input":"2022-06-21T17:04:24.164459Z","iopub.status.idle":"2022-06-21T17:04:25.259826Z","shell.execute_reply.started":"2022-06-21T17:04:24.164428Z","shell.execute_reply":"2022-06-21T17:04:25.258822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Import All Data <a class=\"anchor\" id=\"loadcsv\"></a>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:29.159206Z","iopub.execute_input":"2022-06-21T17:04:29.159621Z","iopub.status.idle":"2022-06-21T17:04:29.225624Z","shell.execute_reply.started":"2022-06-21T17:04:29.159589Z","shell.execute_reply":"2022-06-21T17:04:29.224568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are', len(train_df.index), 'training samples.')\nprint('There are', len(test_df.index), 'testing samples.')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:34.261651Z","iopub.execute_input":"2022-06-21T17:04:34.262246Z","iopub.status.idle":"2022-06-21T17:04:34.271986Z","shell.execute_reply.started":"2022-06-21T17:04:34.262201Z","shell.execute_reply":"2022-06-21T17:04:34.270224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing and EDA <a class=\"anchor\" id=\"preprocessing\"></a>\n#### I prefer to do certain data preprocessing before EDA or concurrently.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Remove duplicate tweets (if any) in the training dataset <a class=\"anchor\" id=\"dupl\"></a>","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop_duplicates(subset='text', keep=\"first\")\nprint('Duplicate tweets has been removed!')\nprint('There are', len(train_df.index), 'training samples now.')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:45.113322Z","iopub.execute_input":"2022-06-21T17:04:45.113808Z","iopub.status.idle":"2022-06-21T17:04:45.142226Z","shell.execute_reply.started":"2022-06-21T17:04:45.113754Z","shell.execute_reply":"2022-06-21T17:04:45.141312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of samples among given classes","metadata":{}},{"cell_type":"code","source":"train_text_samples = train_df.target.value_counts()\nsns.set(rc={'figure.figsize':(6,6)})\n\ncolors = ['salmon' if (x < max(train_text_samples)) else 'yellowgreen' for x in train_text_samples]\nsns.barplot(x = train_text_samples.index, y = train_text_samples, palette = colors)      \n\nplt.gca().set_xlabel('Classes')\nplt.gca().set_ylabel('# of Samples')\nplt.suptitle('Distribution of Training Tweets')\nprint('There are', len(train_df[train_df['target'] == 0]['text']), 'samples are labeled as non-disaster.')\nprint('There are', len(train_df[train_df['target'] == 1]['text']), 'samples are labeled as disaster.')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:47.386555Z","iopub.execute_input":"2022-06-21T17:04:47.387021Z","iopub.status.idle":"2022-06-21T17:04:47.640219Z","shell.execute_reply.started":"2022-06-21T17:04:47.386984Z","shell.execute_reply":"2022-06-21T17:04:47.639201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We observe slight class imbalance, but not critical. Therefore, we won't need to apply various data imbalance techniques to make the training data more balanced.","metadata":{}},{"cell_type":"markdown","source":"### 2.2 Find and replace all NaNs <a class=\"anchor\" id=\"misvals\"></a>","metadata":{}},{"cell_type":"code","source":"percent_missing = train_df.isnull().sum() * 100 / len(train_df)\nmissing_vals = pd.DataFrame({'col_name': train_df.columns,\n                                 'percent': percent_missing})","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:53.98067Z","iopub.execute_input":"2022-06-21T17:04:53.98135Z","iopub.status.idle":"2022-06-21T17:04:54.001615Z","shell.execute_reply.started":"2022-06-21T17:04:53.981314Z","shell.execute_reply":"2022-06-21T17:04:54.000628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(6,6)})\ncols = list(missing_vals['col_name'])\npercent = list(missing_vals['percent'])\nplt.gca().set_ylabel('% of Missing Values')\nplt.gca().set_xlabel('Column Name')\nplt.suptitle('Percentage of Missing Train Values Among Columns')\nsns.barplot(x = cols, y = percent)\n#plt.savefig(\"./plots/missing-values.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:56.1081Z","iopub.execute_input":"2022-06-21T17:04:56.109377Z","iopub.status.idle":"2022-06-21T17:04:56.450611Z","shell.execute_reply.started":"2022-06-21T17:04:56.109314Z","shell.execute_reply":"2022-06-21T17:04:56.449473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['location'] = train_df['location'].fillna('None')\ntrain_df['keyword'] = train_df['keyword'].fillna('None')\ntest_df['location'] = test_df['location'].fillna('None')\ntest_df['keyword'] = test_df['keyword'].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:04:59.193733Z","iopub.execute_input":"2022-06-21T17:04:59.194412Z","iopub.status.idle":"2022-06-21T17:04:59.211313Z","shell.execute_reply.started":"2022-06-21T17:04:59.194359Z","shell.execute_reply":"2022-06-21T17:04:59.209795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Lowercasing <a class=\"anchor\" id=\"lowercase\"></a>","metadata":{}},{"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: x.lower())\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: x.lower())\ntrain_df[\"location\"] = train_df[\"location\"].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:05:01.339292Z","iopub.execute_input":"2022-06-21T17:05:01.339703Z","iopub.status.idle":"2022-06-21T17:05:01.361399Z","shell.execute_reply.started":"2022-06-21T17:05:01.339673Z","shell.execute_reply":"2022-06-21T17:05:01.360295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Visualization of the most common punctuation characters in the train dataset <a class=\"anchor\" id=\"punct\"></a>","metadata":{}},{"cell_type":"code","source":"# I use SpaCy library to find all punctuations\ndef spacy_punct(text):\n    punct = []\n    doc = nlp(text) #necessary to use SpaCy\n    punct = [token.lemma_ for token in doc if token.is_punct]\n    return punct\n\ntrain_df['punct'] = train_df['text'].apply(spacy_punct)\ntrain_df['punct'] = [' '.join(map(str, l)) for l in train_df['punct']]\n\npunct_col = train_df['punct'].tolist()\npunct_list = []\nfor sublist in punct_col:\n    for item in sublist:\n        punct_list.append(item)\npunct_freq = dict(Counter(punct_list))\npunct_freq = {i: j for i, j in sorted(punct_freq.items(), key = lambda item: item[1], reverse = True)}\ndel punct_freq[' ']\npunct_keys = list(punct_freq.keys())\npunct_vals = list(punct_freq.values())","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:05:12.104573Z","iopub.execute_input":"2022-06-21T17:05:12.105026Z","iopub.status.idle":"2022-06-21T17:06:23.472666Z","shell.execute_reply.started":"2022-06-21T17:05:12.104994Z","shell.execute_reply":"2022-06-21T17:06:23.47074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Top Locations <a class=\"anchor\" id=\"toploc\"></a>","metadata":{}},{"cell_type":"code","source":"# location column needs some preprocessing for more precise results\ndef spacy_location(text):\n    preprocessed = []\n    doc = nlp(text)\n    preprocessed = [token.lemma_ for token in doc if not token.is_punct and not token.is_digit and not token.like_url and not token.like_email and token.is_ascii]\n    return preprocessed\ntrain_df['location_names'] = train_df['location'].apply(spacy_location)\ntrain_df['location_names'] = [' '.join(map(str, l)) for l in train_df['location_names']]\ntrain_df['location_names'] = train_df['location_names'].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace(' [a-z][a-z]', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace('@', \"\")\n\nlocation_col = train_df['location_names'].tolist()\n#location_list = []\n#for sublist in location_col:\n    #for item in sublist:\n        #location_list.append(item)\nlocation_freq = dict(Counter(location_col))\nlocation_freq = {i: j for i, j in sorted(location_freq.items(), key=lambda item: item[1], reverse=True)}\ndel location_freq[' ']\ndel location_freq['']\ndel location_freq['none']\nlocation_keys = list(location_freq.keys())\nlocation_vals = list(location_freq.values())","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:06:26.772069Z","iopub.execute_input":"2022-06-21T17:06:26.772495Z","iopub.status.idle":"2022-06-21T17:07:15.248364Z","shell.execute_reply.started":"2022-06-21T17:06:26.772465Z","shell.execute_reply":"2022-06-21T17:07:15.247172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,8)})\nfig, ax = plt.subplots(1, 2)\nsns.barplot(ax = ax[0], x = punct_keys, y = punct_vals)\nax[0].set_title('Common Punctuations')\nax[0].set_ylabel('Frequency')\nax[0].set_xlabel('Punctuations')\n        \nsns.barplot(ax = ax[1], x = location_keys[:20], y = location_vals[:20])\nax[1].set_title('Top Locations')\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel('Location Name')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation = 45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:07:17.909194Z","iopub.execute_input":"2022-06-21T17:07:17.909975Z","iopub.status.idle":"2022-06-21T17:07:19.127592Z","shell.execute_reply.started":"2022-06-21T17:07:17.909821Z","shell.execute_reply":"2022-06-21T17:07:19.126557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop punct column as we don't need it anymore\ntrain_df = train_df.drop('punct', axis=1)\ntrain_df = train_df.drop('location_names', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:07:26.911788Z","iopub.execute_input":"2022-06-21T17:07:26.912626Z","iopub.status.idle":"2022-06-21T17:07:26.925442Z","shell.execute_reply.started":"2022-06-21T17:07:26.912594Z","shell.execute_reply":"2022-06-21T17:07:26.92424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6 Basic Data Cleaning <a class=\"anchor\" id=\"basiccl\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### 2.6.1 Tokenization, punctuation, digits, URLs, non-ascii and emails tokens removal <a class=\"anchor\" id=\"basiccl1\"></a>","metadata":{}},{"cell_type":"code","source":"def spacy_clean(text):\n    preprocessed = []\n    doc = nlp(text)\n    preprocessed = [token.lemma_ for token in doc if not token.is_punct and not token.is_digit and not token.like_url and not token.like_email and token.is_ascii]\n    return preprocessed\n\ntrain_df['new_text'] = train_df['text'].apply(spacy_clean)\ntest_df['new_text'] = test_df['text'].apply(spacy_clean)\ntrain_df['new_text'] = [' '.join(map(str, l)) for l in train_df['new_text']]\ntest_df['new_text'] = [' '.join(map(str, l)) for l in test_df['new_text']]\n\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:07:36.229064Z","iopub.execute_input":"2022-06-21T17:07:36.229498Z","iopub.status.idle":"2022-06-21T17:09:12.087203Z","shell.execute_reply.started":"2022-06-21T17:07:36.229468Z","shell.execute_reply":"2022-06-21T17:09:12.085926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.6.2 Remove other special characters <a class=\"anchor\" id=\"basiccl2\"></a>","metadata":{}},{"cell_type":"code","source":"train_df['new_text'] = train_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace('&amp', \"\").str.replace('@', \"\")\ntest_df['new_text'] = test_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace('&amp ', \"\").str.replace('@', \"\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:16.080182Z","iopub.execute_input":"2022-06-21T17:09:16.080586Z","iopub.status.idle":"2022-06-21T17:09:16.203103Z","shell.execute_reply.started":"2022-06-21T17:09:16.080554Z","shell.execute_reply":"2022-06-21T17:09:16.202068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.7 Advanced Data Cleaning (Optional) <a class=\"anchor\" id=\"advcl\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### 2.7.1 Tokenization, stop words, punctuation, digits, URLs, non-ascii and emails removal, lemmatization using Spacy <a class=\"anchor\" id=\"advcl1\"></a>","metadata":{}},{"cell_type":"code","source":"# UNCOMMENT TO USE\n\"\"\"def spacy_clean(text):\n    preprocessed = []\n    doc = nlp(text)\n    preprocessed = [token.lemma_ for token in doc if not token.is_stop and not nlp.vocab[token.lemma_].is_stop and not token.is_punct and not token.is_digit and not token.like_url and not token.like_email and token.is_ascii]\n    return preprocessed\n\ntrain_df['new_text'] = train_df['text'].apply(spacy_clean)\ntest_df['new_text'] = test_df['text'].apply(spacy_clean)\ntrain_df['new_text'] = [' '.join(map(str, l)) for l in train_df['new_text']]\ntest_df['new_text'] = [' '.join(map(str, l)) for l in test_df['new_text']]\ntrain_df['new_text'] = train_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace(' amp ', \"\").str.replace('@', \"\")\ntest_df['new_text'] = test_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace(' amp ', \"\").str.replace('@', \"\")\ntrain_df\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-05T19:42:36.247571Z","iopub.execute_input":"2022-06-05T19:42:36.247947Z","iopub.status.idle":"2022-06-05T19:42:36.255318Z","shell.execute_reply.started":"2022-06-05T19:42:36.24791Z","shell.execute_reply":"2022-06-05T19:42:36.25429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.7.2 Remove other special characters <a class=\"anchor\" id=\"advcl2\"></a>","metadata":{}},{"cell_type":"code","source":"# UNCOMMENT TO USE\n\"\"\"\ntrain_df['new_text'] = train_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace('&amp', \"\").str.replace('@', \"\")\ntest_df['new_text'] = test_df[\"new_text\"].str.replace('[^\\w\\s]', \"\").str.replace('[0-9]', \"\").str.replace(' [a-z] ', \"\").str.replace('-', \"\").str.replace('_', \"\").str.replace('&amp ', \"\").str.replace('@', \"\")\n\"\"\"\n\n# Optional\n#train_df = train_df.drop('text', axis=1)\n#test_df = test_df.drop('text', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:20.470562Z","iopub.execute_input":"2022-06-21T17:09:20.471044Z","iopub.status.idle":"2022-06-21T17:09:20.489016Z","shell.execute_reply.started":"2022-06-21T17:09:20.471015Z","shell.execute_reply":"2022-06-21T17:09:20.487603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:22.514161Z","iopub.execute_input":"2022-06-21T17:09:22.515785Z","iopub.status.idle":"2022-06-21T17:09:22.532005Z","shell.execute_reply.started":"2022-06-21T17:09:22.515692Z","shell.execute_reply":"2022-06-21T17:09:22.529968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.7.3 non-English words removal using NLTK <a class=\"anchor\" id=\"advcl3\"></a>","metadata":{}},{"cell_type":"code","source":"# UNCOMMENT TO USE\n\"\"\"nltk_words = set(nltk.corpus.words.words())\npreprocessed = []\nfor i in train_df['new_text']:\n    doc = nltk.wordpunct_tokenize(i)\n    preprocessed.append(\" \".join(w for w in doc if w.lower() in nltk_words or not w.isalpha()))\ntrain_df['new_text'] = preprocessed\ntrain_df\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.8 Duplicate tweets removal <a class=\"anchor\" id=\"dupl2\"></a>","metadata":{}},{"cell_type":"code","source":"# target = 0\ntrain_tweets_freq0 = train_df[train_df['target'] == 0]['new_text'].value_counts()\ntrain_tweets_freq0","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:28.418227Z","iopub.execute_input":"2022-06-21T17:09:28.418764Z","iopub.status.idle":"2022-06-21T17:09:28.439468Z","shell.execute_reply.started":"2022-06-21T17:09:28.418735Z","shell.execute_reply":"2022-06-21T17:09:28.43816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target = 1\ntrain_tweets_freq1 = train_df[train_df['target'] == 1]['new_text'].value_counts()\ntrain_tweets_freq1","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:31.229326Z","iopub.execute_input":"2022-06-21T17:09:31.22988Z","iopub.status.idle":"2022-06-21T17:09:31.246127Z","shell.execute_reply.started":"2022-06-21T17:09:31.229836Z","shell.execute_reply":"2022-06-21T17:09:31.244823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_new = train_df.drop_duplicates(subset='new_text', keep=\"first\")\ntrain_df_new","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:33.857928Z","iopub.execute_input":"2022-06-21T17:09:33.858381Z","iopub.status.idle":"2022-06-21T17:09:33.88549Z","shell.execute_reply.started":"2022-06-21T17:09:33.858349Z","shell.execute_reply":"2022-06-21T17:09:33.884472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. More of EDA <a class=\"anchor\" id=\"dupl2\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Word Clouds <a class=\"anchor\" id=\"wordclouds\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Word Cloud of the text samples that are labeled as disaster i.e. target = 1","metadata":{}},{"cell_type":"code","source":"from skimage import io\nimport requests\n\nmask = io.imread('/kaggle/input/tweeter-mask/twitter_mask.png')\nword_cloud_before1 = '  '.join(list(train_df_new[train_df_new['target'] == 1]['text']))\nword_cloud_before1 = WordCloud(background_color='white', width = 400, height = 300, colormap='Set1', mask = mask).generate(word_cloud_before1)\nword_cloud_after1 = '  '.join(list(train_df_new[train_df_new['target'] == 1]['new_text']))\nword_cloud_after1 = WordCloud(background_color='white', width = 400, height = 300, colormap='Set1', mask = mask).generate(word_cloud_after1)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 14))\nax[0].imshow(word_cloud_before1, interpolation=\"bilinear\")\nax[1].imshow(word_cloud_after1, interpolation=\"bilinear\")\n\nax[0].title.set_text('Before Text Preprocessing\\n')\nax[1].title.set_text('After Text Preprocessing\\n')\n#ax[0].figure.savefig('./word_cloud_1.png')\nax[0].axis('off')\nax[1].axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:45.851635Z","iopub.execute_input":"2022-06-21T17:09:45.852044Z","iopub.status.idle":"2022-06-21T17:09:49.933952Z","shell.execute_reply.started":"2022-06-21T17:09:45.852015Z","shell.execute_reply":"2022-06-21T17:09:49.932965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word Clouds of the text samples that are not labeled as disaster i.e. target = 0","metadata":{}},{"cell_type":"code","source":"word_cloud_before0 = '  '.join(list(train_df_new[train_df_new['target'] == 0]['text']))\nword_cloud_before0 = WordCloud(background_color='white', width = 400, height = 300, mask = mask).generate(word_cloud_before0)\nword_cloud_after0 = '  '.join(list(train_df_new[train_df_new['target'] == 0]['new_text']))\nword_cloud_after0 = WordCloud(background_color='white', width = 400, height = 300, mask = mask).generate(word_cloud_after0)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 14))\nax[0].imshow(word_cloud_before0, interpolation=\"bilinear\")\nax[1].imshow(word_cloud_after0, interpolation=\"bilinear\")\nax[0].title.set_text('Before Text Preprocessing\\n')\nax[1].title.set_text('After Text Preprocessing\\n')\n#ax[0].figure.savefig('./word_cloud_0.png')\nax[0].axis('off')\nax[1].axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:09:53.075797Z","iopub.execute_input":"2022-06-21T17:09:53.076396Z","iopub.status.idle":"2022-06-21T17:09:57.021673Z","shell.execute_reply.started":"2022-06-21T17:09:53.076359Z","shell.execute_reply":"2022-06-21T17:09:57.019638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Observed frequencies of words in both classes <a class=\"anchor\" id=\"wordfreqs\"></a>","metadata":{}},{"cell_type":"code","source":"train_words_freq1 = train_df_new[train_df_new['target'] == 1]['new_text'].str.split(expand = True).stack().value_counts()\ntrain_words_freq0 = train_df_new[train_df_new['target'] == 0]['new_text'].str.split(expand = True).stack().value_counts()\n\nsns.set(rc = {'figure.figsize':(12,9)})\nfig, ax = plt.subplots(1, 2)\n\nsns.barplot(ax = ax[0], x = train_words_freq1[:30], y = train_words_freq1.index[:30])\nax[0].set_title('Target = 1')\nax[0].set_xlabel('Frequency')\nax[0].set_ylabel('Words')\n\nsns.barplot(ax = ax[1], x = train_words_freq0[:30], y = train_words_freq0.index[:30])\nax[1].set_title('Target = 0')\nax[1].set_xlabel('Frequency')\nax[1].set_ylabel('Words')\n#ax[0].figure.savefig(\"./plots/words-freqs.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:04.189028Z","iopub.execute_input":"2022-06-21T17:10:04.189757Z","iopub.status.idle":"2022-06-21T17:10:05.561618Z","shell.execute_reply.started":"2022-06-21T17:10:04.18971Z","shell.execute_reply":"2022-06-21T17:10:05.560159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 N-gram Analysis <a class=\"anchor\" id=\"ngrams\"></a>\n#### 3.3.1 Bi-grams <a class=\"anchor\" id=\"twograms\"></a>","metadata":{}},{"cell_type":"code","source":"def bigram(corpus, n = None):\n    vectorizer = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vectorizer.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n\nsns.set(rc = {'figure.figsize':(7, 7)})\nplt.suptitle('Bi-grams')\nplt.gca().set_xlabel('Frequency')\ntop_bigrams = bigram(train_df_new['new_text'])[:20]\nx, y = map(list,zip(*top_bigrams))\nsns.barplot(x = y, y = x)\n#plt.savefig(\"./plots/bigrams.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:14.262515Z","iopub.execute_input":"2022-06-21T17:10:14.263074Z","iopub.status.idle":"2022-06-21T17:10:15.293096Z","shell.execute_reply.started":"2022-06-21T17:10:14.263026Z","shell.execute_reply":"2022-06-21T17:10:15.291881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2 3-grams <a class=\"anchor\" id=\"threegrams\"></a>","metadata":{}},{"cell_type":"code","source":"def threegram(corpus, n = None):\n    vectorizer = CountVectorizer(ngram_range = (3, 3)).fit(corpus)\n    bag_of_words = vectorizer.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n\nsns.set(rc = {'figure.figsize':(7, 7)})\nplt.suptitle('3-grams')\nplt.gca().set_xlabel('Frequency')\ntop_bigrams = threegram(train_df_new['new_text'])[:20]\nx, y = map(list,zip(*top_bigrams))\nsns.barplot(x = y, y = x)\n#plt.savefig(\"./plots/threegrams.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:22.631027Z","iopub.execute_input":"2022-06-21T17:10:22.631451Z","iopub.status.idle":"2022-06-21T17:10:23.631066Z","shell.execute_reply.started":"2022-06-21T17:10:22.631422Z","shell.execute_reply":"2022-06-21T17:10:23.630105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Distribution of Characters <a class=\"anchor\" id=\"charactersdist\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Distribution of characters in disaster tweets (target = 1)","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,5)})\nfig, ax = plt.subplots(1, 2)\n\ntext_len1 = train_df[train_df['target'] == 1]['text'].str.len()\nax[0].hist(text_len1, color = \"salmon\")\nax[0].set_title('Before preprocessing')\nax[0].set_xlabel('Number of Characters')\nax[0].set_ylabel('Frequency')\n\ntext_len2 = train_df_new[train_df_new['target'] == 1]['new_text'].str.len()\nax[1].hist(text_len2, color = \"salmon\")\nax[1].set_title('After preprocessing')\nax[1].set_xlabel('Number of Characters')\nax[1].set_ylabel('Frequency')\nfig.suptitle('Distribution of Characters in Disaster Tweets (target = 1)')\n#ax[0].figure.savefig(\"./plots/distribution-characters1.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:31.062911Z","iopub.execute_input":"2022-06-21T17:10:31.06343Z","iopub.status.idle":"2022-06-21T17:10:31.613795Z","shell.execute_reply.started":"2022-06-21T17:10:31.063384Z","shell.execute_reply":"2022-06-21T17:10:31.612684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distribution of characters in non-disaster tweets (target = 0)","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,5)})\nfig, ax = plt.subplots(1, 2)\n\ntext_len1 = train_df[train_df['target'] == 0]['text'].str.len()\nax[0].hist(text_len1, color = \"yellowgreen\")\nax[0].set_title('Before preprocessing')\nax[0].set_xlabel('Number of Characters')\nax[0].set_ylabel('Frequency')\n\ntext_len2 = train_df_new[train_df_new['target'] == 0]['new_text'].str.len()\nax[1].hist(text_len2, color = \"yellowgreen\")\nax[1].set_title('After preprocessing')\nax[1].set_xlabel('Number of Characters')\nax[1].set_ylabel('Frequency')\nfig.suptitle('Distribution of Characters in Non-Disaster Tweets (target = 0)')\n#ax[0].figure.savefig(\"./plots/distribution-characters0.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:34.24973Z","iopub.execute_input":"2022-06-21T17:10:34.250653Z","iopub.status.idle":"2022-06-21T17:10:34.79036Z","shell.execute_reply.started":"2022-06-21T17:10:34.250619Z","shell.execute_reply":"2022-06-21T17:10:34.789461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5 Distribution of Words <a class=\"anchor\" id=\"wordsdist\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Distribution of words in disaster tweets (target = 1)","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,5)})\nfig, ax = plt.subplots(1, 2)\n\ntext_len1 = train_df[train_df['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax[0].hist(text_len1, color = \"salmon\")\nax[0].set_title('Before preprocessing')\nax[0].set_xlabel('Number of Words')\nax[0].set_ylabel('Frequency')\n\ntext_len2 = train_df_new[train_df_new['target'] == 1]['new_text'].str.split().map(lambda x: len(x))\nax[1].hist(text_len2, color = \"salmon\")\nax[1].set_title('After preprocessing')\nax[1].set_xlabel('Number of Words')\nax[1].set_ylabel('Frequency')\nfig.suptitle('Distribution Words in Disaster Tweets (target = 1)')\n#ax[0].figure.savefig(\"./plots/distribution-words1.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:44.841854Z","iopub.execute_input":"2022-06-21T17:10:44.842323Z","iopub.status.idle":"2022-06-21T17:10:45.379997Z","shell.execute_reply.started":"2022-06-21T17:10:44.842292Z","shell.execute_reply":"2022-06-21T17:10:45.378817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distribution of words in non-disaster tweets (target = 0)","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,5)})\nfig, ax = plt.subplots(1, 2)\n\ntext_len1 = train_df[train_df['target'] == 0]['text'].str.split().map(lambda x: len(x))\nax[0].hist(text_len1, color = \"yellowgreen\")\nax[0].set_title('Before preprocessing')\nax[0].set_xlabel('Number of Words')\nax[0].set_ylabel('Frequency')\n\ntext_len2 = train_df_new[train_df_new['target'] == 0]['new_text'].str.split().map(lambda x: len(x))\nax[1].hist(text_len2, color = \"yellowgreen\")\nax[1].set_title('After preprocessing')\nax[1].set_xlabel('Number of Words')\nax[1].set_ylabel('Frequency')\nfig.suptitle('Distribution Words in Non-Disaster Tweets (target = 0)')\n#ax[0].figure.savefig(\"./plots/distribution-words0.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:10:49.075325Z","iopub.execute_input":"2022-06-21T17:10:49.075865Z","iopub.status.idle":"2022-06-21T17:10:49.752229Z","shell.execute_reply.started":"2022-06-21T17:10:49.075808Z","shell.execute_reply":"2022-06-21T17:10:49.750861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Modeling - BERT, RoBERTa, XLMRoBERTa and ensembles <a class=\"anchor\" id=\"modeling\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Dataset Load <a class=\"anchor\" id=\"datasetclass\"></a>","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, pretrained = 'bert-base-uncased'):\n        self.labels = df['target'].to_list()\n        self.tokenizer = BertTokenizer.from_pretrained(pretrained) \n        #self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        #self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.texts = [self.tokenizer(text, padding = 'max_length', max_length = 512, truncation = True,\n                                return_tensors = 'pt') for text in df['new_text']]\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:02.348864Z","iopub.execute_input":"2022-06-21T17:11:02.349615Z","iopub.status.idle":"2022-06-21T17:11:02.360944Z","shell.execute_reply.started":"2022-06-21T17:11:02.34958Z","shell.execute_reply":"2022-06-21T17:11:02.359476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Single Models <a class=\"anchor\" id=\"singlemodel\"></a>","metadata":{}},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, dropout = 0.5, pretrained = 'bert-base-uncased'):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(pretrained)\n        #self.roberta = RobertaModel.from_pretrained('roberta-base')\n        #self.xlmroberta = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 2)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask):\n        _, pooled_output = self.bert(input_ids = input_id, attention_mask = mask, return_dict = False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.relu(linear_output)\n        return final_layer","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:09.611276Z","iopub.execute_input":"2022-06-21T17:11:09.61171Z","iopub.status.idle":"2022-06-21T17:11:09.621446Z","shell.execute_reply.started":"2022-06-21T17:11:09.611648Z","shell.execute_reply":"2022-06-21T17:11:09.619662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model1(nn.Module):\n    def __init__(self, dropout = 0.5, pretrained = 'bert-base-uncased'):\n        super(Model1, self).__init__()\n        self.model1 = BertModel.from_pretrained(pretrained)\n        #self.model1 = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 768)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask):\n            _, pooled_output = self.model1(input_ids = input_id, attention_mask = mask, return_dict = False)\n            dropout_output = self.dropout(pooled_output)\n            linear_output = self.linear(dropout_output)\n            final_layer = self.relu(linear_output)\n            return final_layer","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:12.216511Z","iopub.execute_input":"2022-06-21T17:11:12.216873Z","iopub.status.idle":"2022-06-21T17:11:12.226771Z","shell.execute_reply.started":"2022-06-21T17:11:12.216837Z","shell.execute_reply":"2022-06-21T17:11:12.224975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model2(nn.Module):\n    def __init__(self, dropout = 0.5, pretrained = 'bert-base-uncased'):\n        super(Model2, self).__init__()\n        self.model2 = BertModel.from_pretrained(pretrained)\n        #self.model2 = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 768)\n        self.relu = nn.ReLU()\n        \n\n    def forward(self, input_id, mask):\n            _, pooled_output = self.model2(input_ids = input_id, attention_mask = mask, return_dict = False)\n            dropout_output = self.dropout(pooled_output)\n            linear_output = self.linear(dropout_output)\n            final_layer = self.relu(linear_output)\n            return final_layer","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:14.92069Z","iopub.execute_input":"2022-06-21T17:11:14.921233Z","iopub.status.idle":"2022-06-21T17:11:14.934847Z","shell.execute_reply.started":"2022-06-21T17:11:14.921188Z","shell.execute_reply":"2022-06-21T17:11:14.933692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Ensemble <a class=\"anchor\" id=\"ensemble\"></a>","metadata":{}},{"cell_type":"code","source":"class MyEnsemble(nn.Module):\n    def __init__(self, modelA, modelB):\n        super(MyEnsemble, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        self.modelA.fc = nn.Identity()\n        self.modelB.fc = nn.Identity()\n        self.classifier = nn.Linear(1536, 2)\n        \n    def forward(self, x, mask):\n        x1 = self.modelA(x.clone(), mask.clone())\n        x1 = x1.view(x1.size(0), -1)\n        x2 = self.modelB(x.clone(), mask.clone())\n        x2 = x2.view(x2.size(0), -1)\n        x = torch.cat((x1, x2), dim=1)\n        x = self.classifier(F.relu(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:24.03957Z","iopub.execute_input":"2022-06-21T17:11:24.040141Z","iopub.status.idle":"2022-06-21T17:11:24.054987Z","shell.execute_reply.started":"2022-06-21T17:11:24.040082Z","shell.execute_reply":"2022-06-21T17:11:24.053764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean GPU cache if necessary\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:26.440587Z","iopub.execute_input":"2022-06-21T17:11:26.441184Z","iopub.status.idle":"2022-06-21T17:11:26.85769Z","shell.execute_reply.started":"2022-06-21T17:11:26.441134Z","shell.execute_reply":"2022-06-21T17:11:26.855768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nprint('CUDA:', use_cuda)\ndevice = torch.device('cuda' if use_cuda else 'cpu')\nprint('You are using:', torch.cuda.get_device_name(device))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:28.878841Z","iopub.execute_input":"2022-06-21T17:11:28.879253Z","iopub.status.idle":"2022-06-21T17:11:28.891985Z","shell.execute_reply.started":"2022-06-21T17:11:28.879224Z","shell.execute_reply":"2022-06-21T17:11:28.890682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Train/Validation/Test Split <a class=\"anchor\" id=\"datasplit\"></a>","metadata":{}},{"cell_type":"code","source":"train_data, val_data, test_data = np.split(train_df_new.sample(frac = 1, random_state = 42), [int(.8*len(train_df_new)), int(.9*len(train_df_new))])\nprint('============= Train/Validation/Test Split =============')\nprint('Train/Validation/Test dataset size: ', len(train_data), '/', len(val_data), '/', len(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:36.589138Z","iopub.execute_input":"2022-06-21T17:11:36.589649Z","iopub.status.idle":"2022-06-21T17:11:36.605516Z","shell.execute_reply.started":"2022-06-21T17:11:36.589617Z","shell.execute_reply":"2022-06-21T17:11:36.604114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5 Train and Test <a class=\"anchor\" id=\"traintest\"></a>","metadata":{}},{"cell_type":"code","source":"class Train():\n    def __init__(self, model, train_data, val_data, criterion, optimizer, epochs, batch_size):\n        self.model = model\n        self.train_data = train_data\n        self.val_data = val_data\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n    def train_plots(self, epochs_list, train_losses, val_losses, train_accs, val_accs):\n        %matplotlib inline\n        sns.set(rc={'figure.figsize':(19, 9)})\n        fig, ax = plt.subplots(1,2)\n        ax[0].plot(epochs_list, train_losses, label = 'Training Loss', marker='o')\n        ax[0].plot(epochs_list, val_losses, label = 'Validation Loss', marker='o')\n        ax[0].set_title('Loss Values')\n        ax[0].set_xlabel('Epoch')\n        ax[0].set_ylabel('Value')\n        ax[1].plot(epochs_list, train_accs, label = 'Training Accuracy', marker='o')\n        ax[1].plot(epochs_list, val_accs, label = 'Validation Accuracy', marker='o')\n        ax[1].set_title('Accuracy Values')\n        ax[1].set_xlabel('Epoch')\n        ax[1].set_ylabel('Percent (%)')\n        ax[0].legend()\n        ax[1].legend()\n        plt.show()\n        #fig.savefig('./plots/train-val-loss-accs.png')\n\n    def start_train(self):\n        train, val = Dataset(self.train_data), Dataset(self.val_data)\n        train_dataloader = torch.utils.data.DataLoader(train, self.batch_size, shuffle = True)\n        val_dataloader = torch.utils.data.DataLoader(val, self.batch_size)\n\n        use_cuda = torch.cuda.is_available()\n        print('CUDA:', use_cuda)\n        device = torch.device('cuda' if use_cuda else 'cpu')\n        print('You are using:', torch.cuda.get_device_name(device))\n        total_steps = len(self.train_data)*self.epochs\n        scheduler = get_linear_schedule_with_warmup(self.optimizer,  num_warmup_steps = 0, num_training_steps = total_steps)\n\n        if use_cuda:\n            self.model = self.model.cuda()\n            self.criterion = self.criterion.cuda()\n\n        train_losses = []\n        val_losses = []\n        train_accs = []\n        val_accs = []\n        epochs_list = []\n    \n        for epoch_num in range(self.epochs):\n            print('\\n====================== Epoch {:} / {:} =====================\\n'.format(epoch_num + 1, self.epochs))\n            total_loss_train = 0\n            total_acc_train = 0\n            self.model.train()\n            for train_input, train_label in tqdm(train_dataloader):\n                train_label = train_label.type(torch.LongTensor)\n                train_label = train_label.to(device)\n                mask = train_input['attention_mask'].to(device)\n                input_id = train_input['input_ids'].squeeze(1).to(device)\n\n                output = self.model(input_id, mask)\n\n                batch_loss = self.criterion(output, train_label)\n                total_loss_train += batch_loss.item()\n\n                acc_tr = (output.argmax(dim = 1) == train_label).sum().item()\n                total_acc_train += acc_tr\n                \n                self.model.zero_grad()\n                batch_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                scheduler.step()\n\n            ############ Validation ###############\n            total_acc_val = 0\n            total_loss_val = 0\n            self.model.eval()\n            with torch.no_grad():\n                for val_input, val_label in val_dataloader:\n                    val_label = val_label.type(torch.LongTensor)\n                    val_label = val_label.to(device)\n                        \n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = self.model(input_id, mask)\n                    #label_ids = val_label.to('cpu').numpy()\n                    \n                    batch_loss = self.criterion(output, val_label)\n                    total_loss_val += batch_loss.item()\n                    \n                    acc_val = (output.argmax(dim=1) == val_label).sum().item()\n                    total_acc_val += acc_val\n            # Losses\n            final_train_loss = total_loss_train/len(self.train_data)\n            final_val_loss = total_loss_val/len(self.val_data)\n            train_losses.append(final_train_loss)\n            train_losses.sort(reverse=True)\n            val_losses.append(final_val_loss)\n\n            # Accuracies\n            final_train_acc = (total_acc_train/len(self.train_data))*100\n            final_val_acc = (total_acc_val/len(self.val_data))*100\n            train_accs.append(final_train_acc)\n            val_accs.append(final_val_acc)\n            epochs_list.append(epoch_num + 1)\n            \n            # Plots\n            self.train_plots(epochs_list, train_losses, val_losses, train_accs, val_accs)\n\n            print(f'Train Loss: {final_train_loss: .3f} | Train Accuracy: {final_train_acc: .3f}%')\n            print(f'Validation Loss: {final_val_loss: .3f} | Validation Accuracy: {final_val_acc: .3f}%\\n')\n            \n            # Save the model\n            #torch.save(model.state_dict(), '/kaggle/input/nlpdisastertweetsbertmodel/nlp_disaster_tweets_bert2.pth')\n            #print('Model Has Been Saved!')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:39.266355Z","iopub.execute_input":"2022-06-21T17:11:39.26679Z","iopub.status.idle":"2022-06-21T17:11:39.314073Z","shell.execute_reply.started":"2022-06-21T17:11:39.266745Z","shell.execute_reply":"2022-06-21T17:11:39.312686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test():\n    def __init__(self, model, test_data, batch_size):\n        self.model = model\n        self.test_data = test_data\n        self.batch_size = batch_size\n        \n    def plot_metrics(self, labels, outputs):\n        labels = torch.cat(labels, dim = 0)\n        labels = labels.cpu().numpy()\n        outputs = torch.cat(outputs, dim = 0)\n        probs = F.softmax(outputs, dim = 1).cpu().numpy()\n        preds = probs[:, 1]\n        \n        # ROC\n        fpr, tpr, threshold = roc_curve(labels, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        # Classification Report\n        y_pred = np.where(preds > 0.5, 1, 0)\n        \n        print('\\nClassification Report:\\n', classification_report(labels, y_pred))\n        \n        # Confusion Matrix\n        cm = confusion_matrix(labels, y_pred)\n        \n        %matplotlib inline\n        sns.set(rc={'figure.figsize':(9, 6)})\n        plt.title('Receiver Operating Characteristic')\n        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n        plt.legend(loc = 'lower right')\n        plt.plot([0, 1], [0, 1],'r--')\n        plt.xlim([0, 1])\n        plt.ylim([0, 1])\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        #plt.savefig('./plots/roc-curve.png')\n        plt.show()\n        \n        #sns.set(rc={'figure.figsize':(7,5)})\n        cm_disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n        cm_disp.plot()\n        \n\n    def start_test(self):\n        test = Dataset(self.test_data)\n        test_dataloader = torch.utils.data.DataLoader(test, self.batch_size)\n\n        use_cuda = torch.cuda.is_available()\n        device = torch.device('cuda' if use_cuda else 'cpu')\n\n        if use_cuda:\n            self.model = self.model.cuda()\n\n        self.model.eval()\n        total_acc_test = 0\n        test_outputs = []\n        test_labels = []\n        with torch.no_grad():\n            for test_input, test_label in test_dataloader:\n                test_label = test_label.type(torch.LongTensor)\n                test_label = test_label.to(device)\n                mask = test_input['attention_mask'].to(device)\n                input_id = test_input['input_ids'].squeeze(1).to(device)\n\n                output = self.model(input_id, mask)\n                acc = (output.argmax(dim = 1) == test_label).sum().item()\n                total_acc_test += acc\n                \n                test_labels.append(test_label)\n                test_outputs.append(output)\n        self.plot_metrics(test_labels, test_outputs)\n        print(f'Test Accuracy: {(total_acc_test / len(self.test_data))*100: .3f} %')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:48.471135Z","iopub.execute_input":"2022-06-21T17:11:48.471494Z","iopub.status.idle":"2022-06-21T17:11:48.500324Z","shell.execute_reply.started":"2022-06-21T17:11:48.471467Z","shell.execute_reply":"2022-06-21T17:11:48.499419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6 Predict <a class=\"anchor\" id=\"predict\"></a>","metadata":{}},{"cell_type":"code","source":"class UnseenDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, pretrained = 'bert-base-uncased'):\n        self.tokenizer = BertTokenizer.from_pretrained(pretrained)\n        self.texts = [self.tokenizer(text, \n                               padding = 'max_length', max_length = 512, truncation = True,\n                                return_tensors = 'pt') for text in df['new_text']]\n    def __len__(self):\n        return len(self.texts)\n\n    def get_batch_texts(self, idx):\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n\n        return batch_texts","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:55.707789Z","iopub.execute_input":"2022-06-21T17:11:55.708837Z","iopub.status.idle":"2022-06-21T17:11:55.718452Z","shell.execute_reply.started":"2022-06-21T17:11:55.708805Z","shell.execute_reply":"2022-06-21T17:11:55.717253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Predict():\n    def __init__(self, model, model_path, unseen_data, batch_size):\n        self.model = model\n        self.model_path = model_path \n        self.unseen_data = unseen_data\n        self.batch_size = batch_size\n\n    def start_predict(self):\n        test = UnseenDataset(self.unseen_data)\n        test_dataloader = torch.utils.data.DataLoader(test, self.batch_size, shuffle = False)\n\n        use_cuda = torch.cuda.is_available()\n        device = torch.device('cuda' if use_cuda else 'cpu')\n\n        self.model.load_state_dict(torch.load(self.model_path, map_location = 'cpu'))\n\n        if use_cuda:\n            self.model = self.model.cuda()\n            \n        predictions = []\n        model.eval()\n        with torch.no_grad():\n            for test_input in test_dataloader:\n                mask = test_input['attention_mask'].to(device)\n                input_id = test_input['input_ids'].squeeze(1).to(device)\n                output = model(input_id, mask)\n                predictions.append(output.cpu().numpy())\n        predictions = np.concatenate(predictions, axis = 0)\n        self.unseen_data['target'] = predictions.argmax(axis = 1)\n        self.unseen_data['target'] = self.unseen_data['target'].astype(int)\n        predicted_data = self.unseen_data[['id', 'target']]\n        predicted_data.to_csv('/kaggle/input/nlpdisastertweetssubmission/submission-bert.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:11:57.978936Z","iopub.execute_input":"2022-06-21T17:11:57.979486Z","iopub.status.idle":"2022-06-21T17:11:57.9931Z","shell.execute_reply.started":"2022-06-21T17:11:57.979455Z","shell.execute_reply":"2022-06-21T17:11:57.991808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.7 Run <a class=\"anchor\" id=\"run\"></a>","metadata":{}},{"cell_type":"code","source":"#To launch MyEnsemble class\nmodel1 = Model1()\nmodel2 = Model2()\nparameters = list(model1.parameters()) + list(model2.parameters())\nmodel = MyEnsemble(model1, model2)\noptimizer = AdamW(parameters, lr = 3e-6, eps = 1e-8)\n\n# To launch a single model \n\"\"\"\nmodel = BertClassifier()\noptimizer = AdamW(model.parameters(), lr = 3e-6, eps = 1e-8)\n\n\"\"\"\nloss_func = nn.CrossEntropyLoss()   \nepochs = 4\nbatch_size = 2\nmodel_path = '/kaggle/input/nlp-getting-started/nlp_disaster_tweets_bert.pth'\nunseen_data = test_df","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:12:05.682067Z","iopub.execute_input":"2022-06-21T17:12:05.68245Z","iopub.status.idle":"2022-06-21T17:12:26.780529Z","shell.execute_reply.started":"2022-06-21T17:12:05.682421Z","shell.execute_reply":"2022-06-21T17:12:26.779459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    path = Path(model_path)\n    print('============= Mode Selection =============')\n    user_input = input('Press t to start training and testing\\nPress p to make predictions using the existing BERT model\\nPress q to exit\\n')\n    if (user_input == 't'):\n        print('============= Train/Validation/Test Split =============')\n        print('Train/Validation/Test dataset size: ', len(train_data), '/', len(val_data), '/', len(test_data))\n        print('============= Training Started =============')\n        train = Train(model, train_data, val_data, loss_func, optimizer, epochs, batch_size) \n        train.start_train()\n        print('Training Completed!')\n        print('============= Testing Started =============')\n        test = Test(model, test_data, batch_size)\n        test.start_test()\n        print('Testing Completed!')\n    else:\n        if (user_input == 'p'):\n            if path.is_file():\n                print('============= Making Prediction =============')\n                predict = Predict(model, model_path, unseen_data, batch_size)\n                predict.start_predict()\n                print('Predictions Made and Saved!')\n            else:\n                print('OOPS! THERE IS NO EXISTING BERT MODEL FOUND. PLEASE TRAIN AND TEST ONE IN ORDER TO HAVE ONE :)')\n        if (user_input == 'q'):\n            sys.exit()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T17:12:26.783098Z","iopub.execute_input":"2022-06-21T17:12:26.783629Z","iopub.status.idle":"2022-06-21T18:08:42.041066Z","shell.execute_reply.started":"2022-06-21T17:12:26.783582Z","shell.execute_reply":"2022-06-21T18:08:42.038988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlpdisastertweetssubmission/submission-bert.csv')\nsubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"### The results shown below were obtained using a randomly sampled training dataset of 5530 samples, a validation dataset of 691 samples, and a testing dataset of 692 samples. The accuracies may vary depending on the randomly sampled training, validation, and test datasets.\n### I have done 3 runs for each model, so 3 different randomly sampled datasets. Therefore, it is hard to say if BERT completely outperforms RoBERTa, and their ensembles on that data, but both BERT and RoBERTa outperformed XLMRoBERTa in all 3 runs. However, the test accuracies for BERT and RoBERTa were always between approximately 81% and 83.09% in all 3 runs.\n### In case of XLMRoBERTa, other models outperformed them in all 3 runs.\n### In case of all ensembles, BERT seems to be the weakest as it had the lowest accuracy in all 3 runs. RoBERTa and XLMRoBERTa ensembles performed nearly the same.\n### Vayring loss functions, optimizers, learning rate, etc. didn't produce much of improvement/decrease in accuracy.\n### I used BERT with the test accuracy of 83.09% as my final submission that scored 0.82684.\n\n### Code available on my Github https://github.com/alite13/NLP-Disaster-Tweets-Classification","metadata":{}},{"cell_type":"markdown","source":"| Model | Loss Function | Optimizer | Epochs | Accuracy |\n| --- | --- | --- | --- | --- |\n| BERT | nn.CrossEntropy() | AdamW | 4 | 83.09% |\n| RoBERTa | nn.CrossEntropy() | AdamW | 4 | 81.79% |\n| XLMRoBERTa | nn.CrossEntropy() | AdamW | 4 | 79.89% |\n| BERT + BERT (Ensemble) | nn.CrossEntropy() | AdamW | 4 | 82.94% |\n| RoBERTa + RoBERTa (Ensemble) | nn.CrossEntropy() | AdamW | 4 | 82.37% |\n| XLMRoBERTa + XLMRoBERTa (Ensemble) | nn.CrossEntropy() | AdamW | 4 | 81.2% |\n","metadata":{}}]}