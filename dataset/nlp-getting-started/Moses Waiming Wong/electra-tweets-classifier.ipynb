{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\n\n# Input data files are available in the read-only \"../input/\" directorypip\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparameters\nmy_learning_rate = 3E-6 # default is 5E-5\nmy_adam_epsilon = 1E-8 # default is 1E-8\nmy_number_of_epochs = 15\nmy_warmup = 3\nmy_mini_batch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('I will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nupdated_test=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nupdated_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocess(text):\n\n    text=text.lower()\n    # remove hyperlinks\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n    #Replace &amp, &lt, &gt with &,<,> respectively\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>')\n    #remove hashtag sign\n    #text=re.sub(r\"#\",\"\",text)   \n    #remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    #text=re.sub(r\"@\",\"\",text)\n    #remove non ascii chars\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    #remove some puncts (except . ! ?)\n    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n    \n    text=\" \".join(text.split())\n    return text\n\nupdated_train['text'] = updated_train['text'].apply(preprocess)\nupdated_test['text'] = updated_test['text'].apply(preprocess)\nupdated_train = updated_train[updated_train[\"text\"]!='']\n\nupdated_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_train = updated_train[[\"text\",\"target\"]]\nupdated_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(updated_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_train[\"target\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the lists of texts and labels.\ntexts = updated_train.text.values\nlabels = updated_train.target.values\n\n#take a look at the first 50 training examples\nfor n in range(50):\n    print(texts[n], \"(LABEL:\", labels[n], \")\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW #Huggingface transformer algorithms and pretrain weights.\n                #Electra is selected here, its pretraining method is more advanced than BERT's MLM. AdamW is Adam with weight decay correction.\nimport torch\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator') \nmodel = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2) #let's try out electra's base discriminator\nmodel.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram showing the length of text, text lengths in 40 bins\nimport matplotlib.pyplot as plt\ndef plot_sentence_text_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=40);\n    ax.set_xlabel(\"Length of Text Embeddings\");\n    ax.set_ylabel(\"Number of Text\");\n    return\nplot_sentence_text_length(texts, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices=tokenizer.batch_encode_plus(texts,\n                                    max_length=64,\n                                    add_special_tokens=True, \n                                    return_attention_mask=True,\n                                    pad_to_max_length=True,\n                                    truncation=True)\ninput_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nvalidation_labels = torch.tensor(validation_labels, dtype=torch.long)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\nvalidation_masks = torch.tensor(validation_masks, dtype=torch.long)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = my_mini_batch_size\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = my_learning_rate, #args.learning_rate\n                  eps = my_adam_epsilon  #args.adam_epsilon\n                )\n\nfrom transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs\nepochs = my_number_of_epochs\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = my_warmup, \n                                            num_training_steps = total_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n#about evalution mert - here we use accuracy, which is good enough because the data is\n#is binary classified and distribution is pretty even between positive and negative\n#however better evaluation should be use later F1 or AUC ROC because emergency are\n#events rare tweets\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Base on GLUE from huggingface, this is classification problem best suit our problem, here, look at the\n# example Python code from hungingface gtihub, here is the here training loop\n\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 100 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # `batch` contains three pytorch tensors: [0]: input ids ,[1]: attention masks,[2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Clear any previously calculated gradients.\n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Evaluate the model on this training batch.\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches \n        total_loss += loss.item()\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0. to prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n      \nprint(\"\")\nprint(\"Training complete!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nprint(\"\")\nprint(\"Running Validation...\")\n\nt0 = time.time()\nmodel.eval()\n\npreds=[]\ntrue=[]\n\n# Tracking variables \neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# Evaluate data for one epoch\nfor batch in validation_dataloader:\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    \n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():        \n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    \n    # Get the \"logits\" output by the model. The \"logits\" are the output values prior to applying an activation function like the softmax.\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    preds.append(logits)\n    true.append(label_ids)\n    # Calculate the accuracy for this batch.\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    # Accumulate the total accuracy.\n    eval_accuracy += tmp_eval_accuracy\n\n    # Track the number of batches\n    nb_eval_steps += 1\n\n# Report the final accuracy for this validation run.\nprint(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\nprint(\"  Validation took: {:}\".format(format_time(time.time() - t0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in preds for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true for item in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(flat_predictions,flat_true_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = updated_test.text.values\n\nindices1=tokenizer.batch_encode_plus(text1,\n                                     max_length=128,\n                                     add_special_tokens=True, \n                                     return_attention_mask=True,\n                                     pad_to_max_length=True,\n                                     truncation=True)\ninput_ids1=indices1[\"input_ids\"]\nattention_masks1=indices1[\"attention_mask\"]\n\nprediction_inputs1= torch.tensor(input_ids1)\nprediction_masks1 = torch.tensor(attention_masks1)\n\n\n# Set the batch size.  \nbatch_size = my_mini_batch_size\n\n# Create the DataLoader.\nprediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\nprediction_sampler1 = SequentialSampler(prediction_data1)\nprediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions = []\n\n# Predict \nfor batch in prediction_dataloader1:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids1, b_input_mask1 = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs1 = model(b_input_ids1, token_type_ids=None, \n                      attention_mask=b_input_mask1)\n\n  logits1 = outputs1[0]\n\n  # Move logits and labels to CPU\n  logits1 = logits1.detach().cpu().numpy()\n  \n  \n  # Store predictions and true labels\n  predictions.append(logits1)\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})\n#submit.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_leak = pd.read_csv('/kaggle/input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv', encoding ='ISO-8859-1')[['choose_one', 'text']]\n\n# Creating target and id\ndf_leak['target'] = (df_leak['choose_one'] == 'Relevant').astype(np.int8)\ndf_leak['id'] = df_leak.index.astype(np.int16)\ndf_leak.drop(columns=['choose_one', 'text'], inplace=True)\n\n# Merging target to test set\nupdated_test = updated_test.merge(df_leak, on=['id'], how='left')\n\nprint('Leaked Data Set Shape = {}'.format(df_leak.shape))\nprint('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() / 1024**2))\n\nperfect_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nperfect_submission['target'] = updated_test['target'].values\nperfect_submission.to_csv('submission.csv',index=False)\nperfect_submission.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}