{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Recently I have been learning about RNNs (Recurrent Neural Networks) and NLP (Natural Language Processing) through Andrew Ngs excellent \"Sequence Models\" course on Coursera ([link](https://www.coursera.org/learn/nlp-sequence-models)). I wanted to have a go implementing a language model using this knowledge and Tensorflow v2.\n\nI picked the \"Real or Not? NLP with Disaster Tweets\" ([link](https://www.kaggle.com/c/nlp-getting-started/overview)) getting started competition for its straight forward task (label tweets as either reporting a disaster or not reporting disaster) and the size of the dataset (large enough to contain enough information for the model but not so much that there will be a lot of processing).\n\nFirst things first then, let's load the libraries.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom keras. utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Flatten\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import *\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nNow I'll load the training dataset. ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\n    '/kaggle/input/nlp-getting-started/train.csv', \n    usecols=['text', 'target'], \n    dtype={'text': str, 'target': np.int64}\n)\n\nlen(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\n    '/kaggle/input/nlp-getting-started/test.csv', \n    usecols=['text', 'id'], \n    dtype={'text': str, 'id': str}\n)\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mislabelled examples\nThere are a number of examples in the training dataset that are mislabelled. The keyword can be used to find these.\n\nThanks to Dmitri Kalyaevs whose notebook is where I found to do this: https://www.kaggle.com/dmitri9149/transformer-svm-semantically-identical-tweets","metadata":{}},{"cell_type":"code","source":"indices = [4415, 4400, 4399,4403,4397,4396, 4394,4414, 4393,4392,4404,4407,4420,4412,4408,4391,4405]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [6840,6834,6837,6841,6816,6828,6831]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [601,576,584,608,606,603,592,604,591, 587]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [3913,3914,3936,3921,3941,3937,3938,3136,3133,3930,3933,3924,3917]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [246,270,266,259,253,251,250,271]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [6119,6122,6123,6131,6160,6166,6167,6172,6212,6221,6230,6091,6108]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [7435,7460,7464,7466,7469,7475,7489,7495,7500,7525,7552,7572,7591,7599]\ndf_train.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[indices, 'target'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df_train['target'])\nplt.title('Counts of Target')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['len_text'] = df_train['text'].str.split().apply(lambda x: len(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nax1.hist(df_train[df_train['target']==1]['len_text'],color = 'red')\nax1.set_title('Disaster Tweet')\nax2.hist(df_train[df_train['target']==0]['len_text'])\nax2.set_title('Not Disaster Tweet')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()\ndef preprocess_data(data):\n    review = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) # remove URL\n    review = re.sub(r'<.*>',' ',review) # remove HTML tags\n    review = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',review)\n    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(words) for words in review if words not in stopwords.words('english') and words.isalpha()]\n    review = ' '.join(review)\n    return review","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['new_text']= df_train['text'].apply(preprocess_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['new_text'] = df_test['text'].apply(preprocess_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we have cleaned our data ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets create WordCloud with common words for Disaster and Not Disaster texts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud(background_color = 'white')\nwc.generate(' '.join(df_train[df_train['target']==1]['new_text']))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.title('Real Disaster')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc1= WordCloud(background_color = 'white')\nwc1.generate(' '.join(df_train[df_train['target']==0]['new_text']))\nplt.imshow(wc1,interpolation = 'bilinear')\nplt.title('Not Disaster')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets analyse top 50 words of Disaster and Not Disaster Text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_tweet = df_train[df_train['target']==1]['new_text']\nnotdisaster_tweet = df_train[df_train['target']==0]['new_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_disaster = pd.Series(' '.join([i for i in disaster_tweet]).split())\nseries_disaster_top = series_disaster.value_counts().head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_disaster_top.plot(kind = 'bar',figsize = (20,20))\nplt.title('Disaster Tweet')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_not_disaster_top = pd.Series(' '.join([i for i in notdisaster_tweet ]).split())\nseries_not_disaster_top = series_not_disaster_top.value_counts().head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_not_disaster_top.plot(kind = 'bar',figsize = (20,20))\nplt.title('Not Disaster')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_words = set(series_disaster_top.index).intersection(set(series_not_disaster_top.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(data):\n    review = ' '.join([i for i in data.split() if i not in common_words])\n    return review","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['new_text'] = df_train['new_text'].apply(text_cleaning)\ndf_test['new_text'] = df_test['new_text'].apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['target'] = df_train['target'].astype('category')\ndf_train['target'] = df_train['target'].cat.codes\ndf_train_target = to_categorical([df_train['target']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['new_text'])\ndf_train_text = tokenizer.texts_to_sequences(df_train['new_text'])\ndf_train_text = pad_sequences(df_train_text, maxlen = 120)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer1 = Tokenizer()\ntokenizer.fit_on_texts(df_train['new_text'])\ndf_test_text = tokenizer.texts_to_sequences(df_test['new_text'])\ndf_test_text = pad_sequences(df_test_text,maxlen = 120)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_text_train = df_train_text[:6500]\ndf_train_text_test = df_train_text[6500:]\ndf_train_target_train = df_train_target[:6500]\ndf_train_target_test = df_train_target[6500:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_target_train = df_train_target[0][:6500]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_target_test = df_train_target[0][6500:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Start to establish Deep learning model\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim = len(tokenizer.word_index) + 1,input_length = 120,output_dim =120))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.add(Dropout(0.35))\nmodel.add(LSTM(120))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(32,activation='relu'))\n\nmodel.add(Dense(2,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, \n                           mode='min', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\n\nhistory = model.fit(x = df_train_text_train,y = df_train_target_train,validation_data=(df_train_text_test,df_train_target_test),callbacks=[early_stop,reduce_lr],epochs=30,batch_size= 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(df_test_text)\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = [np.argmax(i) for i in pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['target'] = pred\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(history.history)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'] )\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We see that epoch 4 is the best for prevent overfitting","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The accuracy of our model is {}'.format(model.evaluate(df_train_text_test,df_train_target_test)[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = model.predict(df_train_text_train)\ntrain_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred =  [np.argmax(i)for i in train_pred]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(df_train.target[:6500],train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = df_test[['id','target']]\nsubmission.to_csv(\"Submission.csv\",index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}