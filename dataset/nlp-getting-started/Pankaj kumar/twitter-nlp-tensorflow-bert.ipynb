{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='background:#FF008C; font-size:36px; color:white; padding-top:25px;'><center> <b> Tweets Classification with BERT </b> </center> </h1>","metadata":{}},{"cell_type":"markdown","source":"![](http://miro.medium.com/max/688/0*B8VDCnh8qBnwUuM4)","metadata":{}},{"cell_type":"markdown","source":"> <h3 style='color:#0084FF;'> Importing Required Libraries && tensorflow_text is must for Loading Preprocessor from Tensorflow hub. </h3>","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_text\nimport tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:26:32.405584Z","iopub.execute_input":"2022-06-25T16:26:32.406504Z","iopub.status.idle":"2022-06-25T16:28:12.263288Z","shell.execute_reply.started":"2022-06-25T16:26:32.406073Z","shell.execute_reply":"2022-06-25T16:28:12.262508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\n\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\n\nimport plotly.express as px\n\nimport tensorflow as tf\nimport tensorflow_hub as hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T16:28:12.26502Z","iopub.execute_input":"2022-06-25T16:28:12.265818Z","iopub.status.idle":"2022-06-25T16:28:14.718379Z","shell.execute_reply.started":"2022-06-25T16:28:12.265779Z","shell.execute_reply":"2022-06-25T16:28:14.717633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nraw_data = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:19.732521Z","iopub.execute_input":"2022-06-25T16:28:19.733157Z","iopub.status.idle":"2022-06-25T16:28:19.77532Z","shell.execute_reply.started":"2022-06-25T16:28:19.733114Z","shell.execute_reply":"2022-06-25T16:28:19.774617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <h3 style='color:pink; font:16px;'> The Below codes show some data of the CSV Datset </h3>","metadata":{}},{"cell_type":"code","source":"raw_data.head().style.set_table_styles(\n    [{'selector': 'tr:hover',\n      'props': [('background-color', 'green')]}]\n).set_properties(**{\n    'font-size': '16pt',})","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:22.565917Z","iopub.execute_input":"2022-06-25T16:28:22.566734Z","iopub.status.idle":"2022-06-25T16:28:22.639214Z","shell.execute_reply.started":"2022-06-25T16:28:22.566701Z","shell.execute_reply":"2022-06-25T16:28:22.638516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:22.797922Z","iopub.execute_input":"2022-06-25T16:28:22.798305Z","iopub.status.idle":"2022-06-25T16:28:22.808543Z","shell.execute_reply.started":"2022-06-25T16:28:22.798279Z","shell.execute_reply":"2022-06-25T16:28:22.807826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Below code show some king of basic Preprocessing for Natural Language Preprocessing.**\n\nThe Preprocessing steps are as follows:\n> 1. Removing Stopwords.\n> 2. Removing Punctuations & Symbols.\n> 3. Removing Http Code using Beautiful Soup.","metadata":{}},{"cell_type":"code","source":"stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n             \"yourselves\"]\n\ntable = str.maketrans('', '', string.punctuation)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:23.667397Z","iopub.execute_input":"2022-06-25T16:28:23.668119Z","iopub.status.idle":"2022-06-25T16:28:23.678853Z","shell.execute_reply.started":"2022-06-25T16:28:23.668086Z","shell.execute_reply":"2022-06-25T16:28:23.677813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_replication(sentence):\n    sentence = re.sub(r'http\\S+', '', sentence)\n    sentence = sentence.replace(\",\", \" , \")\n    sentence = sentence.replace(\".\", \" . \")\n    sentence = sentence.replace(\"-\", \" - \")\n    sentence = sentence.replace(\"/\", \" / \")\n    sentence = sentence.replace(\"\\\\\", \" \\\\ \")\n    sentence = sentence.replace(\"0\", \" \")\n    sentence = sentence.replace(\"1\", \" \")\n    sentence = sentence.replace(\"2\", \" \")\n    sentence = sentence.replace(\"3\", \" \")\n    sentence = sentence.replace(\"4\", \" \")\n    sentence = sentence.replace(\"5\", \" \")\n    sentence = sentence.replace(\"6\", \" \")\n    sentence = sentence.replace(\"7\", \" \")\n    sentence = sentence.replace(\"8\", \" \")\n    sentence = sentence.replace(\"9\", \" \")\n    soup = BeautifulSoup(sentence)\n    sentence = soup.getText()\n    return sentence\n\ndef PreProcessing(df, training=True):\n    \n    df['keyword'] = df['keyword'].fillna('')\n    df['location'] = df['location'].fillna('')\n    \n    SENTENCE = []\n    LABELS = []\n    \n    for idx in df.index:\n        sentence = raw_data.iloc[idx, 1].lower()\n        sentence += ' '\n        sentence += raw_data.iloc[idx, 2].lower()\n        sentence += ' '\n        sentence += raw_data.iloc[idx, 3].lower()\n        sentence = sentence_replication(sentence)\n        words = sentence.split()\n        filtered_sentence = ''\n        for word in words:\n            word = word.translate(table)\n            if word not in stopwords:\n                filtered_sentence = filtered_sentence + word + ' '\n        SENTENCE.append(filtered_sentence)\n        \n        if training:\n            label = raw_data.iloc[idx, 4]\n            LABELS.append(label)\n            \n    if training:\n        return np.array(SENTENCE), np.array(LABELS)\n    \n    return np.array(SENTENCE)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:24.106882Z","iopub.execute_input":"2022-06-25T16:28:24.107566Z","iopub.status.idle":"2022-06-25T16:28:24.121552Z","shell.execute_reply.started":"2022-06-25T16:28:24.107535Z","shell.execute_reply":"2022-06-25T16:28:24.120637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = PreProcessing(raw_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:24.96961Z","iopub.execute_input":"2022-06-25T16:28:24.970319Z","iopub.status.idle":"2022-06-25T16:28:29.176082Z","shell.execute_reply.started":"2022-06-25T16:28:24.970285Z","shell.execute_reply":"2022-06-25T16:28:29.175292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://miro.medium.com/max/1200/1*fnfos3Z_xWdK9GdxX_pGxQ.jpeg)","metadata":{}},{"cell_type":"markdown","source":" <h1 style='color:#ff8300; font-size:40px;'> <center> Bidirectional Encoder Representations from Transformers </center> </h1>","metadata":{}},{"cell_type":"markdown","source":"<div style='font-size:24px;'>\n    \n> BERT provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. \n\n> Along with the Encoder, There is need to load the Bert Preprocessor which take cares of all of the text processing automatically that is being directly feed to the network.    \n    \n> ***BERT is the Model which is being used by Google in the Google Search Engine.***\n    \n> It is being trained on millions of wikipedia and other popular websites texts.\n    \n> Tensorflow provide API to load these Pretrained Models and Layers.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"bert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\nbert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:29.177654Z","iopub.execute_input":"2022-06-25T16:28:29.178081Z","iopub.status.idle":"2022-06-25T16:28:53.260139Z","shell.execute_reply.started":"2022-06-25T16:28:29.178044Z","shell.execute_reply":"2022-06-25T16:28:53.25923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_input')\n    preprocessed_text = bert_preprocessor(text_input)\n    output = bert_encoder(preprocessed_text)\n    o = tf.keras.layers.Dropout(0.1, name='Dropout')(output['pooled_output'])\n    o = tf.keras.layers.Dense(units=1, activation='sigmoid')(o)\n    \n    model = tf.keras.Model(inputs=[text_input], outputs=[o])\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:53.261775Z","iopub.execute_input":"2022-06-25T16:28:53.262109Z","iopub.status.idle":"2022-06-25T16:28:53.269294Z","shell.execute_reply.started":"2022-06-25T16:28:53.262076Z","shell.execute_reply":"2022-06-25T16:28:53.268376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <h2 style='color:#ff6f00'> Model summary shows that we need to only train the Parameter of our DNN i.e. 769. </h2>","metadata":{}},{"cell_type":"code","source":"tweet_model = build_model()\ntweet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:53.270619Z","iopub.execute_input":"2022-06-25T16:28:53.271363Z","iopub.status.idle":"2022-06-25T16:28:54.294022Z","shell.execute_reply.started":"2022-06-25T16:28:53.271323Z","shell.execute_reply":"2022-06-25T16:28:54.293372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:54.296141Z","iopub.execute_input":"2022-06-25T16:28:54.296516Z","iopub.status.idle":"2022-06-25T16:28:54.307586Z","shell.execute_reply.started":"2022-06-25T16:28:54.296479Z","shell.execute_reply":"2022-06-25T16:28:54.306794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Examples   : ', x_train.shape)\nprint('Training Labels     : ', y_train.shape)\nprint('Validation Examples : ', x_valid.shape)\nprint('Validation Labels   : ', y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:54.309201Z","iopub.execute_input":"2022-06-25T16:28:54.309839Z","iopub.status.idle":"2022-06-25T16:28:54.315363Z","shell.execute_reply.started":"2022-06-25T16:28:54.309802Z","shell.execute_reply":"2022-06-25T16:28:54.314604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = tweet_model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=40)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:44:43.510506Z","iopub.execute_input":"2022-06-25T16:44:43.51132Z","iopub.status.idle":"2022-06-25T16:46:05.468922Z","shell.execute_reply.started":"2022-06-25T16:44:43.511279Z","shell.execute_reply":"2022-06-25T16:46:05.46808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style='color:#1A00FF;'> <center> Visualization of Model Training Accuracy & Loss </center> </h1>","metadata":{}},{"cell_type":"code","source":"hist_df = hist.history\nhist_df = pd.DataFrame(hist_df)\nfig = px.line(hist_df)\nfig.update_layout(template='plotly_dark', width=1200, title='Metrics')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:48:00.329702Z","iopub.execute_input":"2022-06-25T16:48:00.33024Z","iopub.status.idle":"2022-06-25T16:48:00.426874Z","shell.execute_reply.started":"2022-06-25T16:48:00.33021Z","shell.execute_reply":"2022-06-25T16:48:00.426029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nid_col = test_df.iloc[:, 0]\nx_test = PreProcessing(test_df, training=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:48:08.177513Z","iopub.execute_input":"2022-06-25T16:48:08.177867Z","iopub.status.idle":"2022-06-25T16:48:09.493066Z","shell.execute_reply.started":"2022-06-25T16:48:08.177841Z","shell.execute_reply":"2022-06-25T16:48:09.492296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = tweet_model.predict(x_test)\npred = np.where(pred>=0.5, 1, 0)\npred = pred.reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:48:10.31676Z","iopub.execute_input":"2022-06-25T16:48:10.317096Z","iopub.status.idle":"2022-06-25T16:48:32.789235Z","shell.execute_reply.started":"2022-06-25T16:48:10.317069Z","shell.execute_reply":"2022-06-25T16:48:32.788472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = {'Id':id_col, 'target':pred}\ndf = pd.DataFrame(df)\ndf.to_csv('Submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:48:32.792176Z","iopub.execute_input":"2022-06-25T16:48:32.79248Z","iopub.status.idle":"2022-06-25T16:48:32.806024Z","shell.execute_reply.started":"2022-06-25T16:48:32.792432Z","shell.execute_reply":"2022-06-25T16:48:32.80517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style='background:black;'>\n    <h1 style='padding:25px 0 0 0; color:#00E9FF'> <center> Thanks for looking ! </center></h1>\n    <h1 style='color:#00E9FF;'> <center> Good to hear from You! </center></h1>\n</div>","metadata":{}}]}