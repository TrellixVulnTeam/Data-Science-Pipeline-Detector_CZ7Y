{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow.compat.v1 as tf\n#tf.compat.v1.enable_eager_execution()\ntf.disable_eager_execution()\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nfrom keras import regularizers\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate,Conv1D,MaxPooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nnp.random.seed(10)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom absl import logging\nfrom nltk.stem import PorterStemmer \nps = PorterStemmer()\nimport spacy\nnlp=spacy.load(\"en_core_web_lg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ntrain.keyword = train['keyword'].str.replace(\"[^a-zA-Z#]\", \" \")\nkeyword = train.keyword[train.keyword.notnull()].tolist()\nkeyword = Counter(keyword)\nkeywords = pd.DataFrame(keyword.most_common(10), columns=['Keyword', 'Count'])\nsns.set(rc={'figure.figsize':(14,6)})\nsns.barplot(data = keywords, x = 'Keyword', y = 'Count')\nplt.title(\"Most Common Keywords\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['target'])\nplt.title(\"Distribution Of Target\")\nsns.set(rc={'figure.figsize':(10,8)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\nstopwords = list(STOP_WORDS)\nimport string\npunct=string.punctuation\n\n\ndef text_data_cleaning(sentence):\n    doc = nlp(sentence)\n    \n    tokens = []\n    for token in doc:\n        if token.lemma_ != \"-PRON-\":\n            temp = token.lemma_.lower().strip()\n        else:\n            temp = token.lower_\n        tokens.append(temp)\n    \n    cleaned_tokens = []\n    for token in tokens:\n        if token not in stopwords and token not in punct:\n            cleaned_tokens.append(token)\n    return \" \".join(cleaned_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"'''import re\nimport string\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[‘’“”…]', '', text)\n    text = re.sub('\\n', '', text)\n    return text'''\n\ntrain['text'] = train.text.apply(lambda x: text_data_cleaning(x))\n#train['text'] = train['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n#train['text'] = train['text'].apply(lambda x: ' '.join([ps.stem(w) for w in x.split() if len(w)>3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword = train.keyword.fillna(\"\")\ntrain['new_text'] = train.text\ntest.keyword = test.keyword.fillna(\"\")\ntest['text'] = test.text\ntest['text'] = test.text.apply(lambda x: text_data_cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.new_text.values, train.target.values, random_state = 42, test_size=0.2)\ntest_data = test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nembed = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#elmo = hub.Module('https://tfhub.dev/google/elmo/3', trainable=True, name=\"{}_module\".format(\"mymod\"))\n#elmo_layer = hub.KerasLayer(elmo,trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embed):\n    model = Sequential([\n        Input(shape=[], dtype=tf.string),\n        embed,\n        Dense(1024, activation='elu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(512, activation='elu'),\n        BatchNormalization(),\n        Dropout(0.35),\n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.1),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(embed)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\nmcp_save = ModelCheckpoint('model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=2, epsilon=1e-4, mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test,y_test),\n        epochs=35,\n        callbacks=[earlyStopping,reduce_lr_loss,mcp_save],\n        batch_size=128\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n#plt.show()\nplt.subplot(2,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"with tf.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('model.hdf5')\n    y_pred = model.predict(X_test)\n    \nfrom sklearn.metrics import confusion_matrix, classification_report\n#print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred.round().astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('model.hdf5')\n    sub = model.predict(test_data)\n    \n\nsubm = pd.DataFrame()\nsubm['id'] = test['id']\nsubm['target'] = sub.round().astype(int)\nsubm.to_csv(\"pred.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}