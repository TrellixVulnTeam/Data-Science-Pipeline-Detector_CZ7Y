{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)"},{"metadata":{},"cell_type":"markdown","source":"This notebook shows diferent basic steps for classifying disaster tweets."},{"metadata":{},"cell_type":"markdown","source":"## NLP:\n---\n\n* [Tf-idf (term frequency–inverse document frequency)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n* [Fasttext](https://fasttext.cc/)\n* [Shallow network](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n---\n\n1. Import libraries\n2. Import datasets\n3. Data cleaning and processing\n4. Basic EDA\n \n 4.1 Clustering\n \n5. Fasttext Supervised\n6. Tf-idf & TruncatedSVD\n7. Shallow neural network (pytorch)"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries\n---"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport fasttext\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nimport gc\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtext.data.utils import ngrams_iterator\nfrom torch.utils.data import DataLoader\nimport time\nfrom torch.utils.data.dataset import random_split\nimport torch\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport torchtext\nfrom torchtext import data\nimport spacy\nimport pandas_profiling as pp\nfrom collections import Counter\nfrom itertools import chain\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import datasets\n----"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3 = train_df\ntest_temp = test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning and pre-processing\n-------\n\n* Removing punctuation, special characters, emojis, numbers, links\n* Lower letters only and split them\n* Removing stopwords \n> \"Words such as “the”, “will”, and “you” — called stopwords — appear the most in a corpus of text, but are of very little significance. Instead, the words which are rare are the ones that actually help in distinguishing between the data, and carry more weight.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_words( raw_review ):\n    \n    review_text = BeautifulSoup(raw_review).get_text() \n    \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    words = letters_only.lower().split()                             \n \n    stops = set(stopwords.words(\"english\"))                  \n\n    meaningful_words = [w for w in words if not w in stops]   \n\n    return( \" \".join( meaningful_words )) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n------"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3['is_train'] = True\ntest_temp['is_train'] = False\n\ndf = pd.concat([temp_3, test_temp], sort=False, ignore_index=True).set_index('id').sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Class distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3.target.value_counts().plot.bar(title=\"Balanced target variable\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster = temp_3.loc[temp_3.target == 1]\nnondisaster = temp_3.loc[temp_3.target == 0]\n\ndisaster['text'] = disaster[\"text\"].apply(review_to_words) \nnondisaster['text'] = nondisaster[\"text\"].apply(review_to_words) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common words related to disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, values = zip(*Counter(\" \".join(disaster[\"text\"]).split()).most_common(20)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width)\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('20 Most common disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common words related to nondisaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, values = zip(*Counter(\" \".join(nondisaster[\"text\"]).split()).most_common(20)[3:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('20 Most common non disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common locations of disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster[\"location\"].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common locations of disaster tweets\", fontsize=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common locations of nondisaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"nondisaster[\"location\"].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common locations of nondisaster tweets\", fontsize=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common keywords of disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster['keyword'].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common keywords of disaster tweets\", fontsize=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 20 Most common keywords of nondisaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"nondisaster['keyword'].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common keywords of nondisaster tweets\", fontsize=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* N-grams visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_ngrams(input_list, n):\n    return list(zip(*[input_list[i:] for i in range(n)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster['bigrams'] = disaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n\nbigrams = disaster['bigrams'].tolist()\nbigrams = list(chain(*bigrams))\nbigrams = [(x.lower(), y.lower()) for x,y in bigrams]\n\nlabels, values = zip(*Counter(bigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common bigrams disaster label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nondisaster['bigrams'] = nondisaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n\nbigrams = nondisaster['bigrams'].tolist()\nbigrams = list(chain(*bigrams))\nbigrams = [(x.lower(), y.lower()) for x,y in bigrams]\n\nlabels, values = zip(*Counter(bigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common bigrams nondisaster label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster['trigrams'] = disaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 3))\n\ntrigrams = disaster['trigrams'].tolist()\ntrigrams = list(chain(*trigrams))\ntrigrams = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams]\n\nlabels, values = zip(*Counter(trigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common trigrams disaster label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nondisaster['trigrams'] = nondisaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 3))\n\ntrigrams = nondisaster['trigrams'].tolist()\ntrigrams = list(chain(*trigrams))\ntrigrams = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams]\n\nlabels, values = zip(*Counter(trigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common trigrams nondisaster label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del temp_3, test_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering\n----"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(review_to_words) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* #### Vectorize with HashingVectorizer\n* #### Dimensionality Reduction with t-SNE\n* #### Clustering with K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bigrams'] = train['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropcols = ['id', 'keyword', 'location', 'target', 'bigrams']\n\ntext = train.drop(dropcols, axis=1)\ntext_arr = text.stack().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\nfor ii in range(0,len(text)):\n    words.append(str(text.iloc[ii]['text']).split(\" \"))\n\nwords[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_gram_all = []\n\nfor word in words:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)\n\nn_gram_all[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, ngram_range=(1,2), n_features=2**12)\n#vectorizer = TfidfVectorizer(lowercase=False, analyzer=lambda l:l)\n\n# features matrix X\nX = hvec.fit_transform(n_gram_all)\n#X = vectorizer.fit_transform(n_gram_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n#X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tsne = TSNE(verbose=1, perplexity=10)\ntsne = TSNE(verbose=1, perplexity=5)\nX_embedded = tsne.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", 1)\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"Disaster and non disaster Tweets - Clustered\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 2\nkmeans = KMeans(n_clusters=k, n_jobs=6, verbose=10)\ny_pred = kmeans.fit_predict(X_train)\ny_train = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = kmeans.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"Disaster and non disaster Tweets - Clustered(K-means)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* #### Vectorize with Tf-idf\n* #### MiniBatchKMeans with Tf-idf\n* #### Dimensionality Reduction with t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=2**12)\nX = vectorizer.fit_transform(train['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 2\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\ny = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(verbose=1)\nX_embedded = tsne.fit_transform(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", len(set(y)))\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\nplt.title(\"Disaster and non disaster Tweets - Clustered(K-Means)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3 = train_df\ntest_temp = test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fasttext Supervised\n-----\n### SCORE (0.78936)"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3['location'] = temp_3['location'].fillna('no')\ntemp_3['keyword'] = temp_3['keyword'].fillna('no')\ntest_temp['location'] = test_temp['location'].fillna('no')\ntest_temp['keyword'] = test_temp['keyword'].fillna('no')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3['text'] = temp_3[\"text\"].apply(review_to_words) \ntest_temp['text'] = test_temp[\"text\"].apply(review_to_words) \ntemp_3['location'] = temp_3[\"location\"].apply(review_to_words) \ntest_temp['location'] = test_temp[\"location\"].apply(review_to_words) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_target(x):\n    if x == 1:\n        x = \"disaster\"\n    else:\n        x = \"nodisaster\"\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3['target'] = temp_3.target.apply(lambda x : convert_target(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xyz(x):\n    d = str(x['text']) + \" \" + str(x['keyword'])+ \" \" + str(x['location'])    \n    c = str(x['target'])\n    p = \"__label__\" + c\n    final = p + \" \" + d\n    return final\n\ntemp_3 = temp_3.apply(xyz, axis=1)\ntemp_3_copy = temp_3.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3.to_csv(r'm.txt', header=None, index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system(\"head -n 7613 m.txt > m.train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = fasttext.train_supervised(input=\"m.train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target(x):\n    d = str(x['text']) + \" \" + str(x['keyword'])+ \" \" + str(x['location'])  \n    target = model.predict(d)\n    return target[0][0][9]\n\ntest_temp['target'] = test_temp.apply(target, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_target(x):\n    if x == \"d\":\n        x = 1\n    else:\n        x = 0\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_temp['target'] = test_temp.target.apply(lambda x : convert_target(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_temp[[\"id\", \"target\"]].to_csv('submissionfasttext.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del temp_3, test_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tf-idf & TruncatedSVD\n-----------\n### Score (0.80163)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3 = train_df\ntest_temp = test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> \"Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases.\"\n\n> \"Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. It is given by the equation below.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_3['text'] = temp_3[\"text\"].apply(review_to_words) \nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(temp_3['text'])\ntest_vectors = vectorizer.transform(test_df[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> \"This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition.(...)\"\n\n> \"In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text.(...)\""},{"metadata":{"trusted":true},"cell_type":"code","source":"tsvd = TruncatedSVD(2000)\ntrain_vectors_svd = tsvd.fit_transform(train_vectors)\ntest_vectors_svd = tsvd.transform(test_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = linear_model.RidgeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model_selection.cross_val_score(clf, train_vectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_vectors_svd, train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"target\"] = clf.predict(test_vectors_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shallow neural network\n\n-------------\n* Score (0.58793)\n* Score (0.68711) init weights\n* Score (0.69631) without freezing\n* Score (0.72290) without freezing & learning rate 0.5 \n* Score (0.66257) without freezing & learning rate 0.3 & 80% training dataset \n\n\n![](https://miro.medium.com/max/391/1*CfdaqnNb6RHLzPJTt1UXjQ.png)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%config InlineBackend.figure_format = 'retina' \nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnlp = spacy.load(\"en\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_token_allowed(token):\n    '''\n    Only allow valid tokens which are not stop words\n    and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return token.lemma_.strip().lower()\n\ndef tokenizer(text):\n    return [ preprocess_token(token) for token in nlp.tokenizer(text) if is_token_allowed(token)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = data.Field(tokenize=tokenizer, pad_first=True)\n\ntrain_dataset = data.TabularDataset(\n            path=\"/kaggle/input/nlp-getting-started/train.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text),\n                    ('target', data.Field())], \n            skip_header=True)\n\ntest_dataset = data.TabularDataset(path=\"/kaggle/input/nlp-getting-started/test.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text)], \n            skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_FREQ = 2\n\ntext.build_vocab(train_dataset, test_dataset, min_freq=MIN_FREQ)\n\nVOCAB_SIZE = len(text.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGRAMS = 2\nBATCH_SIZE = 8\nEMBED_DIM = 768\n\nclass ShallowNeuralNetwork(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n    \n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n    \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return F.softmax(self.fc(embedded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_batch(batch):\n    label = torch.tensor([int(entry.target[0]) for entry in batch])\n    _text = []\n    for entry in batch:\n        _entry = []\n        for t in entry.text:\n            _entry.append(text.vocab.stoi[t])\n        _text.append(torch.tensor(_entry,dtype=torch.long))\n    offsets = [0] + [len(entry) for entry in _text]\n    \n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    _text = torch.cat(_text)\n    return _text, offsets, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_func(sub_train_):\n    # Train the model\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n    \n    for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n    # Adjust the learning rate\n    scheduler.step()\n    \n    return train_loss / len(sub_train_), train_acc / len(sub_train_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(data_):\n    loss = 0\n    acc = 0\n    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss += loss.item()\n            acc += (output.argmax(1) == cls).sum().item()\n\n    return loss / len(data_), acc / len(data_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Learning rate 0.5\n* Training dataset 0.95"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ShallowNeuralNetwork(VOCAB_SIZE, EMBED_DIM, 2).to(device)\n\nN_EPOCHS = 5\nmin_valid_loss = float('inf')\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n\ntrain_len = int(len(train_dataset) * 0.95)\nsub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    train_loss, train_acc = train_func(sub_train_)\n    valid_loss, valid_acc = test(sub_valid_)\n\n    secs = int(time.time() - start_time)\n    mins = secs / 60\n    secs = secs % 60\n\n    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(_text, model, vocab, ngrams):\n    if len(_text) == 0:\n        return 0\n    \n    with torch.no_grad():\n        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n        output = model(torch.tensor(_text), torch.tensor([0]))\n        return output.argmax(1).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(\"cpu\")\npredictions = [predict(entry.text, model, text.vocab, NGRAMS) for entry in test_dataset]\ntweet_id = [entry.id[0] for entry in test_dataset]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': tweet_id, 'target': predictions})\noutput.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}