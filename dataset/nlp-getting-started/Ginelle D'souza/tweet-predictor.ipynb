{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Tweet Predictor**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Loading**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_tweet_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_tweet_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of 7613 records with 5 distinct features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of 3263 records with 4 distinct features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_tweet_train.columns)\nprint(df_tweet_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features has 2 numeric values and 3 categoric values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features has 1 numeric values and 3 categoric values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Data Cleaning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(((((df_tweet_train.isnull().sum())/(df_tweet_train.shape[0])).sort_values(ascending=False))*100),3))\nprint('\\n')\nprint(round(((((df_tweet_test.isnull().sum())/(df_tweet_test.shape[0])).sort_values(ascending=False))*100),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Location and keywords have minor missing values in the training as well as the testing datasets, thus we need to handle these values. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_train['keyword'].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_train['location'] = df_tweet_train['location'].fillna(\n                             df_tweet_train['location'].mode()[0])\n\ndf_tweet_test['location'] = df_tweet_test['location'].fillna(\n                            df_tweet_test['location'].mode()[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet_train['keyword'] = df_tweet_train['keyword'].fillna(\n                            df_tweet_train['keyword'].mode()[0])\n\ndf_tweet_test['keyword'] = df_tweet_test['keyword'].fillna(\n                           df_tweet_test['keyword'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(((((df_tweet_train.isnull().sum())/(df_tweet_train.shape[0])).sort_values(ascending=False))*100),3))\nprint()\nprint(round(((((df_tweet_test.isnull().sum())/(df_tweet_test.shape[0])).sort_values(ascending=False))*100),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Visualization**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_tweet_train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of Fake tweets are more than Real tweets which are denoted as 0 and 1 respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntlen=df_tweet_train[df_tweet_train['target']==1]['text'].str.len()\nax1.hist(tlen)\nax1.set_title('Real tweets')\n\ntlen=df_tweet_train[df_tweet_train['target']==0]['text'].str.len()\nax2.hist(tlen)\nax2.set_title('Fake tweets')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum length of tweets of Disastrous tweets and Non-Disastrous tweets lie between 120-150.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ndf_tweet_train['keyword'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Keywords\")\nplt.ylabel(\"Number of Tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fatalities is the most used keyword to indicate an accident within the tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ndf_tweet_train['location'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Location\")\nplt.ylabel(\"Number of Tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum tweets have been tweeted from USA. \nWe notice there are inconsistencies with respect to the location as some tweets are labelled with only the country, some along with the state and the rest with only the state. We will furthur look into this inconsistency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_tweet_train.groupby(['keyword','target'])\n\nplt.figure(figsize=(15,10))\ntarget.count()['id'][:30].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Keywords (Fake and Real Tweets) \")\nplt.ylabel(\"Number of Tweets\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ignoring certain inconsistencies we can identify that accident, apocalypse and armagedon are the three trending words posted in a tweet to indicate a fatality. Whereas aftershock, army, annihilation are less trending words used. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word(text):\n    \n    comment_words = ' '\n    stopwords = list(STOP_WORDS) \n    \n    for val in text: \n\n         \n        val = str(val)   \n        tokens = val.split() \n\n        \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 500, height = 400, \n                    background_color ='black', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n                            \n    plt.figure(figsize = (12, 12), facecolor = None ) \n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df_tweet_train.text.values\nword(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df_tweet_train.location.values\nword(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We notice that our \"Text\" is very dirty. Thus it is necessary that we filter out all the unwanted signs, symbols, hyperlinks, hastags, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk, re\ndata = [df_tweet_train, df_tweet_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def html_tag(value):\n    \n    result = re.sub(r\"<[^>]+#>\", \"\", value)\n    return result\n\ndef hyperlink(value):\n    \n    result = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", value)\n    return result\n\ndef hashtag(value):\n    \n    result = re.sub(r\"#\", \"\", value)\n    return result\n\ndef emoticon(value):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  \n        u\"\\U0001F300-\\U0001F5FF\"  \n        u\"\\U0001F680-\\U0001F6FF\"  \n        u\"\\U0001F1E0-\\U0001F1FF\"  \n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data:\n    \n    dataset['text'] = dataset['text'].apply(html_tag)\n    dataset['text'] = dataset['text'].apply(hyperlink)\n    dataset['text'] = dataset['text'].apply(hashtag)\n    dataset['text'] = dataset['text'].apply(emoticon)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During the analysis we notice many inconsitencies in the loction feature. We now try to extract the correct loctation of each tweet with the help of pycountry library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pycountry\n\ndef findcon1(text):\n        \n    for country in pycountry.countries:\n        if country.name in text:   \n            a = country.name\n            return a\n        else:\n            try:\n                a = pycountry.countries.search_fuzzy(text.split()[-1])[0].name\n                return a\n            \n            except:\n                return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data:\n    \n    dataset['nlocation'] = dataset['location'].apply(lambda x : findcon1(x))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ndf_tweet_train['nlocation'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Location\")\nplt.ylabel(\"Number of Tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As compared to the previous inconistent locaton output, we get a better view so as to which countries have the highest number of tweets. The highest number of tweets belong to The United States.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Data Modeling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_tweet_train[['target','text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Machine Learning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Naive Bayes Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import confusion_matrix\n\npipe1 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', MultinomialNB())])\n\npipe1.fit(X_train, y_train)\n\nprediction1 = pipe1.predict(X_test)\n\naccuracy1 = round((pipe1.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy1,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Linear Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\npipe2 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', SGDClassifier())])\n\npipe2.fit(X_train, y_train)\n\nprediction2 = pipe2.predict(X_test)\n\naccuracy2 = round((pipe2.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy2,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test, prediction2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\npipe3 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', SVC())])\n\npipe3.fit(X_train, y_train)\n\nprediction3 = pipe3.predict(X_test)\n\naccuracy3 = round((pipe3.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy3,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test, prediction3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SpaCy**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = list(STOP_WORDS)\npunctuations = string.punctuation\nparser = English()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(sentence):\n    tokens = parser(sentence)\n    tokens = [ word.lemma_.lower() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n    tokens = [ word for word in tokens if word not in stopwords and word not in punctuations ]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import TransformerMixin \nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n    def fit(self, X, y=None, **fit_params):\n        return self\n    def get_params(self, deep=True):\n        return {}\n\n \n    def clean_text(text):     \n        return text.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Count Vectorizer along with Support Vector Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1)) \nclassifier = SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe4 = Pipeline([('cleaner', predictors()),('vectorizer', vectorizer),('classifier', classifier)])\n\npipe4.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction4 = pipe4.predict(X_test)\n\naccuracy4 = round((pipe4.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy4,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* TF-IDF Vectorizer along with Support Vector Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfvectorizer = TfidfVectorizer(tokenizer = tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe5 = Pipeline([('cleaner', predictors()),('vectorizer', tfvectorizer),('classifier', classifier)])\n\npipe5.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction5 = pipe4.predict(X_test)\n\naccuracy5 = round((pipe5.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy5,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Neural Network**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = df['text'].values\ny = df['target'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Basic Neural Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(layers.Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel1.add(layers.Dense(64,activation='relu'))\nmodel1.add(layers.Dense(1, activation='sigmoid'))\n\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model1.evaluate(X_test, y_test, verbose=False)\n\naccuracy6 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy6,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Bag Of Words (BOW)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = {}  \nword_encoding = 1\ndef bow(sentence):\n    \n    text = parser(sentence)\n    text = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text ]\n    text = [ word for word in text if word not in stopwords and word not in punctuations ]\n         \n    global word_encoding\n    words = text \n    bag = {}  \n\n    for word in words:\n        \n        if word in vocab:\n            encoding = vocab[word]  \n        else:\n            vocab[word] = word_encoding\n            encoding = word_encoding\n            word_encoding += 1\n    \n        if encoding in bag:\n            bag[encoding] += 1\n        else:\n            bag[encoding] = 1\n  \n    return bag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(tokenizer = bow)\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(layers.Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel2.add(layers.Dense(64,activation='relu'))\nmodel2.add(layers.Dense(128,activation='relu'))\nmodel2.add(layers.Dense(256,activation='relu'))\nmodel2.add(layers.Dense(1, activation='sigmoid'))\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model2.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\n\naccuracy7 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy7,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Word Embedding  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vsize = len(tokenizer.word_index) + 1\nedim = 50\nmaxlen = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(layers.Embedding(input_dim=vsize,output_dim=edim,input_length=maxlen))\nmodel3.add(layers.Flatten())\nmodel3.add(layers.Dense(256, activation='relu'))\nmodel3.add(layers.Dense(128, activation='relu'))\nmodel3.add(layers.Dense(64, activation='relu'))\nmodel3.add(layers.Dense(32, activation='relu'))\nmodel3.add(layers.Dense(1, activation='sigmoid'))\n\nmodel3.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model3.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model3.evaluate(X_test, y_test, verbose=False)\n\naccuracy8 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy8,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Long-Short Term Memory (LSTM)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = Sequential()\nmodel4.add(layers.Embedding(input_dim=vsize,output_dim=edim,input_length=maxlen))\nmodel4.add(layers.LSTM(50))\nmodel4.add(layers.Dense(10, activation='relu'))\nmodel4.add(layers.Dense(1, activation='sigmoid'))\n\nmodel4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel4.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model4.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model4.evaluate(X_test, y_test, verbose=False)\n\naccuracy9 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy9,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Neural Network with Pooling layers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = Sequential()\nmodel5.add(layers.Embedding(input_dim=vsize, output_dim=edim,input_length=maxlen))\nmodel5.add(layers.GlobalMaxPooling1D())\nmodel5.add(layers.Dense(10, activation='relu'))\nmodel5.add(layers.Dense(1, activation='sigmoid'))\n\nmodel5.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\nmodel5.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model5.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model5.evaluate(X_test, y_test, verbose=False)\n\naccuracy10 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy10,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [['Naive Bayes Classifier',accuracy1],\n       ['Linear Support Vector Machine',accuracy2],\n       ['Support Vector Machine',accuracy3],\n       ['Count Vectorizer along with Support Vector Classifier',accuracy4],\n       ['TF-IDF Vectorizer along with Support Vector Classifier',accuracy5],\n       ['Basic Neural Model',accuracy6],\n       ['Bag Of Words (BOW)',accuracy7],\n       ['Word Embedding',accuracy8],\n       ['Long-Short Term Memory (LSTM)',accuracy9],\n       ['Neural Network with Pooling layers',accuracy10]]\n\nfinal = pd.DataFrame(data,columns=['Algorithm','Precision'],index=[1,2,3,4,5,6,7,8,9,10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The results of Data Modeling are as follows:\\n \")\nprint(final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **References**\n1. SapCy  \nhttps://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Text%20Classification%20With%20Machine%20Learning,SpaCy,Sklearn(Sentiment%20Analysis)/Text%20Classification%20&%20Sentiment%20Analysis%20with%20SpaCy,Sklearn.ipynb\n2. Neural Network  \nhttps://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\n3. Neural Network (BOW) https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true&scrollTo=Fo3WY-e86zX2","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}