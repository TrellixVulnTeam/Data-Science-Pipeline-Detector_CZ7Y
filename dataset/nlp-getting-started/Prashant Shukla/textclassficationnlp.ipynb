{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-21T00:55:33.630364Z","iopub.execute_input":"2021-11-21T00:55:33.630701Z","iopub.status.idle":"2021-11-21T00:55:33.647497Z","shell.execute_reply.started":"2021-11-21T00:55:33.630615Z","shell.execute_reply":"2021-11-21T00:55:33.646683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NLP Text Classification Model\n\nIt is a supervised machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n\nSteps involved in development of text classification model are below :-\n\n1. Importing Libraries\n2. Loading the data set & perform Exploratory Data Analysis\n3. Text pre-processing\n4. Split the train dataset\n4. Extracting vectors from text (Vectorization) using Bag-of-Words(with Tf-Idf) and Word2Vec\n5. ML Model algorithms\n6. Testing ML models on test dataset\n\n","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"#for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#for text pre-processing\nimport re, string\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\n\n#for model-building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#for word embedding\nimport gensim\nfrom gensim.models import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:55:36.76851Z","iopub.execute_input":"2021-11-21T00:55:36.769327Z","iopub.status.idle":"2021-11-21T00:55:37.582667Z","shell.execute_reply.started":"2021-11-21T00:55:36.769274Z","shell.execute_reply":"2021-11-21T00:55:37.581951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load data set into dataframe\ndf_train=pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint(df_train.shape)\ndf_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:08.145463Z","iopub.execute_input":"2021-11-21T00:56:08.146083Z","iopub.status.idle":"2021-11-21T00:56:08.204145Z","shell.execute_reply.started":"2021-11-21T00:56:08.14603Z","shell.execute_reply":"2021-11-21T00:56:08.203248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:09.221682Z","iopub.execute_input":"2021-11-21T00:56:09.222351Z","iopub.status.idle":"2021-11-21T00:56:09.233953Z","shell.execute_reply.started":"2021-11-21T00:56:09.222296Z","shell.execute_reply":"2021-11-21T00:56:09.232954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA","metadata":{}},{"cell_type":"code","source":"## find missing values in df\ndf_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:12.633434Z","iopub.execute_input":"2021-11-21T00:56:12.634229Z","iopub.status.idle":"2021-11-21T00:56:12.646123Z","shell.execute_reply.started":"2021-11-21T00:56:12.634173Z","shell.execute_reply":"2021-11-21T00:56:12.645193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CLASS distribution\nx=df_train['target'].value_counts()\nprint(x)\nsns.barplot(x.index,x)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:13.95794Z","iopub.execute_input":"2021-11-21T00:56:13.958658Z","iopub.status.idle":"2021-11-21T00:56:14.097938Z","shell.execute_reply.started":"2021-11-21T00:56:13.958611Z","shell.execute_reply":"2021-11-21T00:56:14.097017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Count number of words in a tweet\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\nprint(\"Number of words in Disaster tweets:\",df_train[df_train['target']==1]['word_count'].mean()) \nprint(\"Number of words in Non-Disaster tweets:\",df_train[df_train['target']==0]['word_count'].mean()) \n\n\n#Plot word-count per tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\ndf_train_words=df_train[df_train['target']==1]['word_count']\nax1.hist(df_train_words,color='red')\nax1.set_title('Disaster tweets')\ndf_train_words=df_train[df_train['target']==0]['word_count']\nax2.hist(df_train_words,color='green')\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Words per tweet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:14.677427Z","iopub.execute_input":"2021-11-21T00:56:14.677704Z","iopub.status.idle":"2021-11-21T00:56:15.054313Z","shell.execute_reply.started":"2021-11-21T00:56:14.677672Z","shell.execute_reply":"2021-11-21T00:56:15.053524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Count number of character\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\nprint(\"Number of characters in Disaster tweets:\",df_train[df_train['target']==1]['char_count'].mean()) \nprint(\"Number of characters in Non-Disaster tweets:\",df_train[df_train['target']==0]['char_count'].mean()) \n\n#Plot word-count per tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\ndf_train_words=df_train[df_train['target']==1]['char_count']\nax1.hist(df_train_words,color='red')\nax1.set_title('Disaster tweets')\ndf_train_words=df_train[df_train['target']==0]['char_count']\nax2.hist(df_train_words,color='green')\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Character count per tweet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:15.884907Z","iopub.execute_input":"2021-11-21T00:56:15.885209Z","iopub.status.idle":"2021-11-21T00:56:16.243802Z","shell.execute_reply.started":"2021-11-21T00:56:15.885177Z","shell.execute_reply":"2021-11-21T00:56:16.24298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Count number of unique word count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\nprint(\"Number of unique words in Disaster tweets:\",df_train[df_train['target']==1]['unique_word_count'].mean()) \nprint(\"Number of unique words in Non-Disaster tweets:\",df_train[df_train['target']==0]['unique_word_count'].mean()) ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:16.797678Z","iopub.execute_input":"2021-11-21T00:56:16.797988Z","iopub.status.idle":"2021-11-21T00:56:16.837269Z","shell.execute_reply.started":"2021-11-21T00:56:16.797954Z","shell.execute_reply":"2021-11-21T00:56:16.836362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Pre-Processing","metadata":{}},{"cell_type":"code","source":"## Simple text cleaning processe :- Remove punctuations, special characters, URLs, hashtag, leading, trailing & extra white spaces/tabs,typos, slangs are corrected.\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n\n#convert to lowercase and remove punctuations and characters and then strip\ndef preprocess(text):\n    text = text.lower() #lowercase text\n    text=text.strip()  #remove leading/trailing whitespace \n    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. \n    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n    \n    return text\n\ntext=preprocess(text)\nprint(text)  #text is a string","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:18.859636Z","iopub.execute_input":"2021-11-21T00:56:18.859976Z","iopub.status.idle":"2021-11-21T00:56:18.870353Z","shell.execute_reply.started":"2021-11-21T00:56:18.859943Z","shell.execute_reply":"2021-11-21T00:56:18.869238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## LEXICON-BASED Text Processing\n\n#1. STOP-WORD Removal\ndef stopword(string):\n    a= [i for i in string.split() if i not in stopwords.words('english')]\n    return ' '.join(a)\n\ntext=stopword(text)\nprint(text)\n\n#2. STEMMING\n \n# Initialize the stemmer\nsnow = SnowballStemmer('english')\ndef stemming(string):\n    a=[snow.stem(i) for i in word_tokenize(string) ]\n    return \" \".join(a)\ntext=stemming(text)\nprint(text)\n\n#3. LEMMATIZATION\n# Initialize the lemmatizer\nwl = WordNetLemmatizer()\n \n# Helper function to map NTLK position tags\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n# Tokenize the sentence\ndef lemmatizer(string):\n    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n    return \" \".join(a)\n\ntext = lemmatizer(text)\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:20.127015Z","iopub.execute_input":"2021-11-21T00:56:20.127431Z","iopub.status.idle":"2021-11-21T00:56:22.453468Z","shell.execute_reply.started":"2021-11-21T00:56:20.127402Z","shell.execute_reply":"2021-11-21T00:56:22.452527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def finalpreprocess(string):\n    return lemmatizer(stopword(preprocess(string)))\ndf_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:56:22.455255Z","iopub.execute_input":"2021-11-21T00:56:22.455657Z","iopub.status.idle":"2021-11-21T00:56:51.648901Z","shell.execute_reply.started":"2021-11-21T00:56:22.455611Z","shell.execute_reply":"2021-11-21T00:56:51.648281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word2Vec model\nBag-of-Words (BoW) and Word Embedding (with Word2Vec) are two well-known methods for converting text data to numerical data.","metadata":{}},{"cell_type":"code","source":"# create Word2vec model\n\n#convert preprocessed sentence to tokenized sentence\ndf_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] \n\n#min_count=1 means word should be present at least across all documents,\nmodel = Word2Vec(df_train['clean_text_tok'],min_count=1)  \n\n#combination of word and its vector\nw2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n\n#for converting sentence to vectors/numbers from word vectors result by Word2Vec\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty should return a vector of zeros\n        self.dim = len(next(iter(word2vec.values())))\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:11.290578Z","iopub.execute_input":"2021-11-21T00:57:11.291151Z","iopub.status.idle":"2021-11-21T00:57:14.167256Z","shell.execute_reply.started":"2021-11-21T00:57:11.291115Z","shell.execute_reply":"2021-11-21T00:57:14.166389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the data","metadata":{}},{"cell_type":"code","source":"##Split the training dataset into test and train data\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"target\"],test_size=0.2,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:17.297828Z","iopub.execute_input":"2021-11-21T00:57:17.298399Z","iopub.status.idle":"2021-11-21T00:57:17.305302Z","shell.execute_reply.started":"2021-11-21T00:57:17.29836Z","shell.execute_reply":"2021-11-21T00:57:17.304112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word2Vec runs on tokenized sentences\nX_train_tok= [nltk.word_tokenize(i) for i in X_train]  \nX_test_tok= [nltk.word_tokenize(i) for i in X_test]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:18.444362Z","iopub.execute_input":"2021-11-21T00:57:18.444632Z","iopub.status.idle":"2021-11-21T00:57:19.915063Z","shell.execute_reply.started":"2021-11-21T00:57:18.444604Z","shell.execute_reply":"2021-11-21T00:57:19.914146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vectorization using Bag-of-Words (with Tf-Idf ) and Word2Vec","metadata":{}},{"cell_type":"code","source":"#TF-IDF\n# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\ntfidf_vectorizer = TfidfVectorizer(use_idf=True)\n#tfidf runs on non-tokenized sentences unlike word2vec\nX_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n# Only transform x_test (not fit and transform)\nX_val_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n\n#Word2vec\n# Fit and transform\nmodelw = MeanEmbeddingVectorizer(w2v)\nX_train_vectors_w2v = modelw.transform(X_train_tok)\nX_val_vectors_w2v = modelw.transform(X_test_tok)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:20.456406Z","iopub.execute_input":"2021-11-21T00:57:20.457032Z","iopub.status.idle":"2021-11-21T00:57:20.894304Z","shell.execute_reply.started":"2021-11-21T00:57:20.456993Z","shell.execute_reply":"2021-11-21T00:57:20.893324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ML Models Development","metadata":{}},{"cell_type":"code","source":"## Classification Model using Logistic Regression(tf-idf)\n\nlr_tfidf=LogisticRegression(solver = 'liblinear', C=1, penalty = 'l2')\nlr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_tfidf.predict(X_val_vectors_tfidf)\ny_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n\nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('Area under the curve (AUC) :', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:22.52259Z","iopub.execute_input":"2021-11-21T00:57:22.522883Z","iopub.status.idle":"2021-11-21T00:57:22.579833Z","shell.execute_reply.started":"2021-11-21T00:57:22.522853Z","shell.execute_reply":"2021-11-21T00:57:22.578787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Classification Model using Naive Bayes(tf-idf)\n\nnb_tfidf = MultinomialNB()\nnb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n\n#Predict y value for test dataset\ny_predict = nb_tfidf.predict(X_val_vectors_tfidf)\ny_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n\nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('AUC:', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:25.947523Z","iopub.execute_input":"2021-11-21T00:57:25.947782Z","iopub.status.idle":"2021-11-21T00:57:25.970375Z","shell.execute_reply.started":"2021-11-21T00:57:25.947755Z","shell.execute_reply":"2021-11-21T00:57:25.969772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Classification Model using Logistic Regression (W2v)\nlr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\nlr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_w2v.predict(X_val_vectors_w2v)\ny_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n \nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('AUC:', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:57:26.827698Z","iopub.execute_input":"2021-11-21T00:57:26.828187Z","iopub.status.idle":"2021-11-21T00:57:26.961354Z","shell.execute_reply.started":"2021-11-21T00:57:26.828155Z","shell.execute_reply":"2021-11-21T00:57:26.960246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Model on Test Dataset","metadata":{}},{"cell_type":"code","source":"## Testing with the best model\n\n#Preprocess the data\ndf_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) \nX_test=df_test['clean_text'] \nX_vector=tfidf_vectorizer.transform(X_test) \n\n## Use the trained model on X_vector\ny_predict = nb_tfidf.predict(X_vector)      \ny_prob = nb_tfidf.predict_proba(X_vector)[:,1]\ndf_test['predict_prob']= y_prob\ndf_test['target']= y_predict\nprint(df_test.head())\nfinal=df_test[['id','target']].reset_index(drop=True)\nfinal.to_csv('NLP_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T01:01:12.045731Z","iopub.execute_input":"2021-11-21T01:01:12.046063Z","iopub.status.idle":"2021-11-21T01:01:24.799593Z","shell.execute_reply.started":"2021-11-21T01:01:12.04603Z","shell.execute_reply":"2021-11-21T01:01:24.798753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}