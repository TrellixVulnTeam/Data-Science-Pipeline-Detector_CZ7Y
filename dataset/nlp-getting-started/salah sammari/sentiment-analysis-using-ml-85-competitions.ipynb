{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is Sentiment Analysis?\n\nSentiment Analysis is a process of extracting opinions that have different polarities. By polarities, we mean positive, negative or neutral. It is also known as opinion mining and polarity detection. With the help of sentiment analysis, you can find out the nature of opinion that is reflected in documents, websites, social media feed, etc. Sentiment Analysis is a type of classification where the data is classified into different classes. These classes can be binary in nature (positive or negative) or, they can have multiple classes (happy, sad, angry, etc.).\n\n# Please If you find this kernel helpful, upvote it to help others see it ðŸ˜Š\n![](https://d1sjtleuqoc1be.cloudfront.net/wp-content/uploads/2019/04/25112909/shutterstock_1073953772.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## What files do I need?\n\nYou'll need train.csv, test.csv and sample_submission.csv.\n\n## What should I expect the data format to be?\n\nEach sample in the train and test set has the following information:\n\nThe text of a tweet\nA keyword from that tweet (although this may be blank!)\nThe location the tweet was sent from (may also be blank)\n\n## What am I predicting?\n\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n### Files\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n\n### Columns\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 1) import library & packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn import svm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) load data & analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### finding missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values=train.isnull().sum()\npercent_missing = train.isnull().sum()/train.shape[0]*100\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing\n}\nframe=pd.DataFrame(value)\nframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 92 redundants sapmles in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\ntrain.groupby('target').id.count().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"labels are not balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numbers of word for each sapmle in train & test data\ntrain['text_length'] = train.text.apply(lambda x: len(x.split()))\ntest['text_length'] = test.text.apply(lambda x: len(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Max number of words in all data is 31 and min is 1!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_word_count(df, data_name):\n  sns.distplot(df['text_length'].values)\n  plt.title(f'Sequence char count: {data_name}')\n  plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ig = plt.figure(figsize=(16,6))\n#plt.hist(train[\"text_length\"], bins = 30)\n#plt.show()\nplt.subplot(1, 2, 1)\nplot_word_count(train, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_word_count(test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collecting all words in single list\nlist_= []\nfor i in train.text:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 31480 different words in our train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df,target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#most frequent 20 words when label == 0 \nimport collections\nallWords=create_corpus(train,target=0)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#most frequent 20 words when label == 1 \nimport collections\nallWords=create_corpus(train,target=1)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Removing Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of punctuations and we will remove them from our corpus\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for  example\ntext='hi !! whats up bro :) i hope you enjoy with me '\n\"\".join([char for char in text if char not in string.punctuation])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for example \ntext='hey 4 look 333 at me0 58999632'\nre.sub('[0-9]', '', text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of stopwords\nstopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for example\ntext='hey this is me and I am here to help you  '\ntokens = word_tokenize(text)\ntokens=[word for word in tokens if word not in stopwords.words('english')]\n' '.join(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's Build a function that clean our data\n\nI just added lower function in order to lowercase all words and stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"pstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    #tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"clean\"]=train[\"text\"].apply(clean_text)\ntest[\"clean\"]=test[\"text\"].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the effect of cleaning\ntrain[[\"text\",\"clean\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collecting all words in single list\nlist_= []\nfor i in train.clean:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)\nlen(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we reduced our data from 31480 unique words to 19920"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(sublinear_tf=True,max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\nfeatures = tfidf.fit_transform(train.clean).toarray()\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_test = tfidf.transform(test.clean).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4) machine leaning algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into 4 parts with same distribution of classes.\nskf = StratifiedKFold(n_splits=4, random_state=48, shuffle=True)\naccuracy=[] # list contains the accuracy for each fold\nn=1\ny=train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for trn_idx, test_idx in skf.split(features, y):\n  start_time = time()\n  X_tr,X_val=features[trn_idx],features[test_idx]\n  y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n  model= LogisticRegression(max_iter=1000,C=3)\n  #model=MultinomialNB(alpha=0.5)\n  #model=svm.SVC(max_iter=1000)\n  model.fit(X_tr,y_tr)\n  s = model.predict(X_val)\n  sub[str(n)]= model.predict(features_test) \n  \n  accuracy.append(accuracy_score(y_val, s))\n  print((time() - start_time)/60,accuracy[n-1])\n  n+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(accuracy)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5) Evaluating Model on Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\npred_valid_y = model.predict(X_val)\nprint(classification_report(y_val, pred_valid_y ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_val, pred_valid_y ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=sub[['1','2','3','4']].mode(axis=1)# select the most frequent predicted class by our model\nsub['target']=df[0]    \nsub=sub[['id','target']]\nsub['target']=sub['target'].apply(lambda x : int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}