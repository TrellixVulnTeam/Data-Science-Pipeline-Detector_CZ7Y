{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Importing the required libraries.**","metadata":{}},{"cell_type":"code","source":"!pip install texthero","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:20.731402Z","iopub.execute_input":"2021-07-11T18:34:20.73177Z","iopub.status.idle":"2021-07-11T18:34:27.7315Z","shell.execute_reply.started":"2021-07-11T18:34:20.731739Z","shell.execute_reply":"2021-07-11T18:34:27.730634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport texthero as hero\nfrom texthero import preprocessing as ppe\nfrom texthero import visualization as viz\nimport spacy\nfrom spacy import displacy\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:27.733279Z","iopub.execute_input":"2021-07-11T18:34:27.733561Z","iopub.status.idle":"2021-07-11T18:34:27.745529Z","shell.execute_reply.started":"2021-07-11T18:34:27.733534Z","shell.execute_reply":"2021-07-11T18:34:27.744509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the training and test dataset**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:54.784958Z","iopub.execute_input":"2021-07-11T18:34:54.785348Z","iopub.status.idle":"2021-07-11T18:34:54.823993Z","shell.execute_reply.started":"2021-07-11T18:34:54.785313Z","shell.execute_reply":"2021-07-11T18:34:54.823138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:56.904035Z","iopub.execute_input":"2021-07-11T18:34:56.904444Z","iopub.status.idle":"2021-07-11T18:34:56.929907Z","shell.execute_reply.started":"2021-07-11T18:34:56.904397Z","shell.execute_reply":"2021-07-11T18:34:56.929029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:58.459447Z","iopub.execute_input":"2021-07-11T18:34:58.459837Z","iopub.status.idle":"2021-07-11T18:34:58.478231Z","shell.execute_reply.started":"2021-07-11T18:34:58.459806Z","shell.execute_reply":"2021-07-11T18:34:58.477278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:34:59.873354Z","iopub.execute_input":"2021-07-11T18:34:59.87371Z","iopub.status.idle":"2021-07-11T18:34:59.889246Z","shell.execute_reply.started":"2021-07-11T18:34:59.873678Z","shell.execute_reply":"2021-07-11T18:34:59.887824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = 'target', data = train_df, facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                   edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Class Names')\nplt.ylabel('Count')\nplt.title('Distribution of classes in the training dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:02.64991Z","iopub.execute_input":"2021-07-11T18:35:02.650374Z","iopub.status.idle":"2021-07-11T18:35:02.763395Z","shell.execute_reply.started":"2021-07-11T18:35:02.650336Z","shell.execute_reply":"2021-07-11T18:35:02.762423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Location column value distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot(y=train_df['location'].value_counts()[:10].index,x=train_df['location'].value_counts()[:10],orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 3,\n           edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Location Count')\nplt.title('Top 10 locations with the maximum occurence in the training dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:05.332554Z","iopub.execute_input":"2021-07-11T18:35:05.332963Z","iopub.status.idle":"2021-07-11T18:35:05.529566Z","shell.execute_reply.started":"2021-07-11T18:35:05.33291Z","shell.execute_reply":"2021-07-11T18:35:05.52855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keyword column value distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot(y=train_df['keyword'].value_counts()[:10].index,x=train_df['keyword'].value_counts()[:10],orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 2,\n           edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Keyword Count')\nplt.title('Top 10 keywords with the maximum occurence in the training dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:07.909758Z","iopub.execute_input":"2021-07-11T18:35:07.910146Z","iopub.status.idle":"2021-07-11T18:35:08.114754Z","shell.execute_reply.started":"2021-07-11T18:35:07.910112Z","shell.execute_reply":"2021-07-11T18:35:08.113977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.keyword.value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:10.04204Z","iopub.execute_input":"2021-07-11T18:35:10.042665Z","iopub.status.idle":"2021-07-11T18:35:10.055472Z","shell.execute_reply.started":"2021-07-11T18:35:10.042611Z","shell.execute_reply":"2021-07-11T18:35:10.054276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.location.value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:12.212163Z","iopub.execute_input":"2021-07-11T18:35:12.212566Z","iopub.status.idle":"2021-07-11T18:35:12.224403Z","shell.execute_reply.started":"2021-07-11T18:35:12.212527Z","shell.execute_reply":"2021-07-11T18:35:12.223161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dropping the Keyword and location columns from both training and test set.**","metadata":{}},{"cell_type":"code","source":"train_df.drop(['keyword', 'location'], axis = 1, inplace = True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:14.638067Z","iopub.execute_input":"2021-07-11T18:35:14.638407Z","iopub.status.idle":"2021-07-11T18:35:14.650332Z","shell.execute_reply.started":"2021-07-11T18:35:14.63838Z","shell.execute_reply":"2021-07-11T18:35:14.649329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop(['keyword', 'location'], axis = 1, inplace = True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:16.145704Z","iopub.execute_input":"2021-07-11T18:35:16.146102Z","iopub.status.idle":"2021-07-11T18:35:16.158371Z","shell.execute_reply.started":"2021-07-11T18:35:16.146048Z","shell.execute_reply":"2021-07-11T18:35:16.157318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the word count of the text column","metadata":{}},{"cell_type":"code","source":"train_df['word count'] = train_df.text.apply(len)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:18.741227Z","iopub.execute_input":"2021-07-11T18:35:18.741817Z","iopub.status.idle":"2021-07-11T18:35:18.760872Z","shell.execute_reply.started":"2021-07-11T18:35:18.741775Z","shell.execute_reply":"2021-07-11T18:35:18.759764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **describe()** method is used for calculating statistical data like percentile, mean and std of the numerical values of the Series or DataFrame. We're using this method below on the word count column.","metadata":{}},{"cell_type":"code","source":"train_df['word count'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:20.684898Z","iopub.execute_input":"2021-07-11T18:35:20.6855Z","iopub.status.idle":"2021-07-11T18:35:20.698659Z","shell.execute_reply.started":"2021-07-11T18:35:20.685447Z","shell.execute_reply":"2021-07-11T18:35:20.697381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most common and uncommon words in the text column","metadata":{}},{"cell_type":"code","source":"#most common words\nfreq = pd.Series(''.join(train_df['text']).split()).value_counts()[:10]\nfreq","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:22.852955Z","iopub.execute_input":"2021-07-11T18:35:22.853571Z","iopub.status.idle":"2021-07-11T18:35:22.935906Z","shell.execute_reply.started":"2021-07-11T18:35:22.853524Z","shell.execute_reply":"2021-07-11T18:35:22.93529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#uncommon words\nnot_freq = pd.Series(''.join(train_df['text']).split()).value_counts()[-10:]\nnot_freq","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:25.737889Z","iopub.execute_input":"2021-07-11T18:35:25.738517Z","iopub.status.idle":"2021-07-11T18:35:25.818714Z","shell.execute_reply.started":"2021-07-11T18:35:25.738462Z","shell.execute_reply":"2021-07-11T18:35:25.818018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\nFor data pre-processing, we're mainly gonna use the awesome **TextHero** library. Under the hoods, Texthero utilizes various NLP and AI tool compartments like **Gensim, NLTK, SpaCy** and **scikit-learn**. We're gonna use mainly the pre-processing toolkit of this library. \n\nSo, with this library, we can create custom pipeline where we can mention various textual data cleaning techniques like **removing whitespaces, stop words, punctuations**, etc. and then apply this on the text column.","metadata":{}},{"cell_type":"code","source":"custom_pipeline = [ppe.fillna, ppe.lowercase, ppe.remove_punctuation, ppe.remove_whitespace, \n                  ppe.remove_stopwords, ppe.remove_urls, ppe.remove_digits]\n\ntrain_df['cleaned_text'] = hero.clean(train_df['text'], custom_pipeline)\ntest_df['cleaned_text'] = hero.clean(test_df['text'], custom_pipeline)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:29.279185Z","iopub.execute_input":"2021-07-11T18:35:29.279729Z","iopub.status.idle":"2021-07-11T18:35:30.173851Z","shell.execute_reply.started":"2021-07-11T18:35:29.27968Z","shell.execute_reply":"2021-07-11T18:35:30.17306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:31.669493Z","iopub.execute_input":"2021-07-11T18:35:31.66985Z","iopub.status.idle":"2021-07-11T18:35:31.681309Z","shell.execute_reply.started":"2021-07-11T18:35:31.669819Z","shell.execute_reply":"2021-07-11T18:35:31.680625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:33.374202Z","iopub.execute_input":"2021-07-11T18:35:33.374777Z","iopub.status.idle":"2021-07-11T18:35:33.384636Z","shell.execute_reply.started":"2021-07-11T18:35:33.374725Z","shell.execute_reply":"2021-07-11T18:35:33.383955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization\nWe use lemmatizer to convert the words into their root words. So, suppose if the data contains 2 words where one of them is in past tense (**believed**) and another one in future tense (**believing**), that particular word will be converted to its root word (**believe**). \n\nThis particularly helps us while training our model, as the model doesn't need to learn 2 different words which basically have the same meaning.","metadata":{}},{"cell_type":"code","source":"def lemmatizer(r):\n    wnl = WordNetLemmatizer()\n    words = nltk.word_tokenize(r)\n    lemmatized_words = [wnl.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n    return \" \".join(lemmatized_words)\n\ntrain_df['lemma_cleaned_text'] = train_df['cleaned_text'].apply(lemmatizer)\ntest_df['lemma_cleaned_text'] = test_df['cleaned_text'].apply(lemmatizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:35.766885Z","iopub.execute_input":"2021-07-11T18:35:35.767499Z","iopub.status.idle":"2021-07-11T18:35:55.307931Z","shell.execute_reply.started":"2021-07-11T18:35:35.767451Z","shell.execute_reply":"2021-07-11T18:35:55.307144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Special Character removal\nWe're gonna use the below code to remove special characters like @, #, $ , etc. \nThis step is really important as these characters can really hamper our model's performance while training.\n","metadata":{}},{"cell_type":"code","source":"def remove_special_characters(text):\n    pattern = r'[^a-zA-Z]'\n    text = re.sub(pattern, ' ', text)\n    return text\n\ntrain_df['special_char_cleaned_text'] = train_df['lemma_cleaned_text'].apply(remove_special_characters)\ntest_df['special_char_cleaned_text'] = test_df['lemma_cleaned_text'].apply(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:35:55.309495Z","iopub.execute_input":"2021-07-11T18:35:55.310063Z","iopub.status.idle":"2021-07-11T18:35:55.399564Z","shell.execute_reply.started":"2021-07-11T18:35:55.31002Z","shell.execute_reply":"2021-07-11T18:35:55.398794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:36:05.94065Z","iopub.execute_input":"2021-07-11T18:36:05.941171Z","iopub.status.idle":"2021-07-11T18:36:05.955855Z","shell.execute_reply.started":"2021-07-11T18:36:05.941138Z","shell.execute_reply":"2021-07-11T18:36:05.954799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:36:09.65939Z","iopub.execute_input":"2021-07-11T18:36:09.659747Z","iopub.status.idle":"2021-07-11T18:36:09.672712Z","shell.execute_reply.started":"2021-07-11T18:36:09.659718Z","shell.execute_reply":"2021-07-11T18:36:09.671341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WordCloud \nThis Wordcloud displays the most frequent words in the training dataset.","metadata":{}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(' '.join(train_df['special_char_cleaned_text']))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:36:12.521358Z","iopub.execute_input":"2021-07-11T18:36:12.521721Z","iopub.status.idle":"2021-07-11T18:36:13.662067Z","shell.execute_reply.started":"2021-07-11T18:36:12.521687Z","shell.execute_reply":"2021-07-11T18:36:13.661303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:36:15.718537Z","iopub.execute_input":"2021-07-11T18:36:15.719121Z","iopub.status.idle":"2021-07-11T18:36:15.911202Z","shell.execute_reply.started":"2021-07-11T18:36:15.719056Z","shell.execute_reply":"2021-07-11T18:36:15.910188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Named Entity Recognition (NER)\nNamed-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. We're using the **SpaCy** library for that.","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\nl = []\nfor i in train_df['special_char_cleaned_text']:\n    doc = nlp(i)\n    if doc.ents:\n        for ent in doc.ents:\n            ner = {\n                    'Text' : [ent.text],\n                    'NER Label' : [ent.label_],\n                    'Label explaination' : [str(spacy.explain(ent.label_))]\n                }\n            l.append(ner)   \n            df1 = pd.DataFrame(data = l)\n            df1['Text'] = df1['Text'].str.get(0)\n            df1['NER Label'] = df1['NER Label'].str.get(0)\n            df1['Label explaination'] = df1['Label explaination'].str.get(0)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:36:21.904645Z","iopub.execute_input":"2021-07-11T18:36:21.905015Z","iopub.status.idle":"2021-07-11T18:40:09.365155Z","shell.execute_reply.started":"2021-07-11T18:36:21.904985Z","shell.execute_reply":"2021-07-11T18:40:09.364262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualing the NER labels and the frequency of their occurence**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot(y=df1['NER Label'].value_counts().index,x=df1['NER Label'].value_counts(),orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 2,\n           edgecolor=sns.color_palette(\"dark\", 11))\nplt.xlabel('NER Label Count')\nplt.title('NER Labels and frequency of their occurence')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:40:17.771386Z","iopub.execute_input":"2021-07-11T18:40:17.771841Z","iopub.status.idle":"2021-07-11T18:40:18.043319Z","shell.execute_reply.started":"2021-07-11T18:40:17.771806Z","shell.execute_reply":"2021-07-11T18:40:18.042185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization\nIn basic terms, Vectorization is the **process of converting text into numerical representation** which are also called **embeddings**. \nSince, the computers are not as intelligent as us (till now atleast ;), they can't understand textual data, so to make our data understandable to a computer, we first convert it to a numerical format.\n\nThere are various techniques for text vectorization like:-\n* **Bag of Words**\n* **Count Vectorizer**\n* **TF-IDF Vectorizer**\n\nHere, we're gonna use the **TF-IDF Vectorizer** approach. \nSo, TF-IDF is an acronym for **Term Frequency - Inverse Document Frequency**. \n* TF makes sure to give high score to the word that appears frequently.\n* IDF makes sure to give low score to the word if it appears pretty frequently in documents (not a unique identifier).\n\nSo, the amalgamation of **TF * IDF** is how the score is calculated for this vectorizer.","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df['special_char_cleaned_text'])\ntest_tfidf = tfidf_vectorizer.transform(test_df['special_char_cleaned_text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:40:23.542802Z","iopub.execute_input":"2021-07-11T18:40:23.543188Z","iopub.status.idle":"2021-07-11T18:40:23.808612Z","shell.execute_reply.started":"2021-07-11T18:40:23.543156Z","shell.execute_reply":"2021-07-11T18:40:23.807467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training our model\nSo, here we're gonna use the** XGBClasifier** which basically harnesses the power of **boosting trees**. \nSo, boosting trees are a little different than your normal decision trees. In decision trees, we're ensembling a model on top of another but at the end of the day we're using a single model for our prediction. But boosting trees take a smarter approach when it comes to training a model efficiently. So, rather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. \nModels are added sequentially until no further improvements can be made.\n\nThe main advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models. In a standard ensemble method, like random forest,  where models are trained in isolation, all of the models might simply end up making the same mistakes.\n\nSince we have imbalanced data in our classes, we're gonna use **scale_pos_weight**. So, here we have defined a definite set of weights which we're gonna use in this hyper parameter.\n\nThen, we'll be using **RepeatedStratifiedKFold** method which is going to repeat Stratified K-Fold 3 times with different randomization in each repetition. We're putting number of splits as 5, so it'll be a 5 fold cross validation.\n\nAfter that, we're using **GridSearchCV** to feed and iterate through our mentioned weights and other hyper parameters. We're using **ROC AUC** curve for scoring our model's performance. So, the ROC AUC curve is the measure of the ability of a classifier to distinguish between classes. The higher the AUC, the better the performance of the model at distinguishing one class from another.\n\nLastly, we're summarizing the best configuration and printing them out.","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier()\n# define grid\nweights = [1, 10, 15, 20]\nparam_grid = dict(scale_pos_weight=weights)\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# define grid search\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n# execute the grid search\ngrid_result = grid.fit(train_tfidf, train_df.target)\n# report the best configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# report all configurations\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:40:29.181307Z","iopub.execute_input":"2021-07-11T18:40:29.181658Z","iopub.status.idle":"2021-07-11T18:59:56.464869Z","shell.execute_reply.started":"2021-07-11T18:40:29.181623Z","shell.execute_reply":"2021-07-11T18:59:56.463671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicting on the test set**","metadata":{}},{"cell_type":"code","source":"pred = grid_result.predict(test_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T19:00:34.139596Z","iopub.execute_input":"2021-07-11T19:00:34.13997Z","iopub.status.idle":"2021-07-11T19:00:34.15851Z","shell.execute_reply.started":"2021-07-11T19:00:34.139932Z","shell.execute_reply":"2021-07-11T19:00:34.157665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Output :)**","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndata={\"id\":[],\"target\":[]}\nfor id,pred_1 in zip(sample['id'].unique(),pred): \n    data[\"id\"].append(id) \n    data[\"target\"].append(pred_1)\n\n    \noutput=pd.DataFrame(data,columns=[\"id\",\"target\"])\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T19:00:49.596642Z","iopub.execute_input":"2021-07-11T19:00:49.596994Z","iopub.status.idle":"2021-07-11T19:00:49.628542Z","shell.execute_reply.started":"2021-07-11T19:00:49.596965Z","shell.execute_reply":"2021-07-11T19:00:49.627502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2021-07-11T19:00:53.646871Z","iopub.execute_input":"2021-07-11T19:00:53.64728Z","iopub.status.idle":"2021-07-11T19:00:53.661538Z","shell.execute_reply.started":"2021-07-11T19:00:53.647248Z","shell.execute_reply":"2021-07-11T19:00:53.660605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References:-\nSome of the literature and learnings are borrowed from below sources. Feel free to check them out :)\n* https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n* https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7\n* https://texthero.org/docs/getting-started#preprocessing\n* https://towardsdatascience.com/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6","metadata":{}},{"cell_type":"markdown","source":"# If you like my work,  don't forget to upvote ;)","metadata":{}}]}