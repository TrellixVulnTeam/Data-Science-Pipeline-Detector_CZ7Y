{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1.Importing libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport pandas as pd \nimport random \nimport time\nimport datetime\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.metrics import f1_score\nimport numpy as np \nfrom torch.utils.data import TensorDataset,Subset\nfrom transformers import BertTokenizer\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.nn import functional as F","execution_count":null,"outputs":[]},{"metadata":{"id":"by5zkDazybkf"},"cell_type":"markdown","source":"# 2.Enabling Gpu \n","execution_count":null},{"metadata":{"id":"oYsV4H8fCpZ-","outputId":"9128471b-e9d2-4e22-a5ef-19b60bb45dee","trusted":true},"cell_type":"code","source":"# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"id":"-8kEDRvShcU5"},"cell_type":"markdown","source":"# 3.Organising Train Data","execution_count":null},{"metadata":{"id":"Z474sSC6oe7A","outputId":"5e9fc033-2a6a-48e3-aac9-fed66b5d9e60","trusted":true},"cell_type":"code","source":"# Load the BERT tokenizer.\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"yNU2Z5yk8iUn","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\nsentences = train.text.values\nlabels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"id":"2bBdb3pt8LuQ","outputId":"9e8d8f8e-6a20-4782-f7b2-e7a8be453b99","trusted":true},"cell_type":"code","source":"# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in sentences:\n  \n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 64,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"BfX6p9Y0ejAf","trusted":true},"cell_type":"code","source":"batch_size = 16 \nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n#helper function to get train and val data loaders for each fold \ndef get_data_loaders(dataset,train_indexes,val_indexes):\n    train_tensor = Subset(dataset,train_indexes)\n    val_tensor = Subset(dataset,val_indexes)\n    train_dataloader = DataLoader(\n            train_tensor, \n            sampler = RandomSampler(train_tensor), \n            batch_size = batch_size\n        )\n\n    val_dataloader = DataLoader(\n            val_tensor, \n            sampler = SequentialSampler(val_tensor), \n            batch_size = batch_size \n        )\n    return train_dataloader,val_dataloader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"6gNcyNpmv88D"},"cell_type":"markdown","source":"# 4. Organising test data for predictions","execution_count":null},{"metadata":{"id":"mAN0LZBOOPVh","outputId":"43cc9400-8ff5-4816-82da-51f152348116","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsentences = df.text.values\ninput_ids = []\nattention_masks = []\nfor sent in sentences:\n\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 64,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\n# Set the batch size.  \nbatch_size = 16  \n# Create the DataLoader.\nprediction_data = TensorDataset(input_ids, attention_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"id":"RqfmWwUR_Sox"},"cell_type":"markdown","source":"# 5. Training Loop","execution_count":null},{"metadata":{"id":"QsOYopORzKXp","trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\ndef get_bert_model():\n    model = BertForSequenceClassification.from_pretrained(\n      \"bert-base-uncased\", \n      num_labels = 2,           \n      output_attentions = False, \n      output_hidden_states = False, \n    )\n    # Tell pytorch to run this model on the GPU.\n    model.cuda()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"9cQNvaZ9bnyy","trusted":true},"cell_type":"code","source":"import numpy as np\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"id":"gpt6tR83keZD","trusted":true},"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"LbB0miJTbLda","trusted":false},"cell_type":"code","source":"# Set the seed value all over the place to make this reproducible.\nseed_val = 1000\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"khUH0dGRg4CM","trusted":true},"cell_type":"code","source":"total_folds = 12\ncurrent_fold = 0\nall_folds_preds = []\nepochs = 1\nfold=StratifiedKFold(n_splits=total_folds, shuffle=True, random_state=1000)\n\ntraining_stats = []","execution_count":null,"outputs":[]},{"metadata":{"id":"6J-FYdx6nFE_","outputId":"9f9bc3eb-4217-4379-82d3-0816826dad5b","trusted":true},"cell_type":"code","source":"# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n#for each fold..\nfor train_index, test_index in fold.split(train,train['target']):\n    model = get_bert_model()\n    optimizer = AdamW(model.parameters(),lr = 5e-5,eps = 1e-8)\n    current_fold = current_fold+1\n    train_dataloader,validation_dataloader = get_data_loaders(dataset,train_index,test_index)\n    print(\"\")\n    print('================= Fold {:} / {:} ================='.format(current_fold,total_folds))\n    # For each epoch...\n    for epoch_i in range(0, epochs):\n        # ========================================\n        #               Training\n        # ========================================\n\n        # Perform one full pass over the training set.\n\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n\n        # Measure how long the training epoch takes.\n        t0 = time.time()\n\n        # Reset the total loss for this epoch.\n        total_train_loss = 0\n        model.train()\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()        \n\n            loss, logits = model(b_input_ids, \n                              token_type_ids=None, \n                              attention_mask=b_input_mask, \n                              labels=b_labels)\n\n\n            total_train_loss += loss.item()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            #update weights\n            optimizer.step()\n\n\n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_train_loss / len(train_dataloader)            \n\n        # Measure how long this epoch took.\n        training_time = format_time(time.time() - t0)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epcoh took: {:}\".format(training_time))\n\n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n\n        # Put the model in evaluation mode--the dropout layers behave differently\n        # during evaluation.\n        model.eval()\n\n        # Tracking variables \n        total_f1_score = 0\n        total_eval_accuracy = 0\n        total_eval_loss = 0\n        nb_eval_steps = 0\n\n        # Evaluate data for one epoch\n        for batch in validation_dataloader:\n\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            with torch.no_grad():        \n                (loss, logits) = model(b_input_ids, \n                                        token_type_ids=None, \n                                        attention_mask=b_input_mask,\n                                        labels=b_labels)\n\n            # Accumulate the validation loss.\n            total_eval_loss += loss.item()\n\n            # Move logits and labels to CPU\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            # Calculate the accuracy for this batch of test sentences, and\n            # accumulate it over all batches.\n            total_eval_accuracy += flat_accuracy(logits, label_ids)\n            total_f1_score += f1_score(np.argmax(logits,axis=1),label_ids)\n\n        # Report the final accuracy and f1_score for this validation run.\n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n        \n        avg_f1_score = total_f1_score / len(validation_dataloader)\n        print(\"  F1_score: {0:.2f}\".format(avg_f1_score))\n\n        # Calculate the average loss over all of the batches.\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n\n        # Measure how long the validation run took.\n        validation_time = format_time(time.time() - t0)\n\n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        # Record all statistics from this epoch.\n        training_stats.append(\n          {\n              'epoch': epoch_i + 1,\n              'Training Loss': avg_train_loss,\n              'Valid. Loss': avg_val_loss,\n              'Valid. Accur.': avg_val_accuracy,\n              'f1_score' : avg_f1_score,\n              'Training Time': training_time,\n              'Validation Time': validation_time,\n              'fold' : current_fold\n              \n          }\n        )\n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n\n    # ========================================\n    # Predicting and saving predictions for all folds\n    # ========================================\n\n    print(\"\")\n    print(\"now predicting for this fold\")\n\n    # Put model in evaluation mode\n    model.eval()\n    # Tracking variables \n    predictions  = []\n    # Predict \n    for batch in prediction_dataloader:\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask = batch\n        # speeding up prediction\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            outputs = model(b_input_ids, token_type_ids=None, \n                            attention_mask=b_input_mask)\n\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n\n\n        predictions.append(logits)\n\n    stack = np.vstack(predictions)\n    final_preds = F.softmax(torch.from_numpy(stack))[:,1].numpy()\n    all_folds_preds.append(final_preds)\nprint(\"Completed\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6O_NbXFGMukX","trusted":true},"cell_type":"code","source":"pd.set_option('precision', 2)\ndf_stats = pd.DataFrame(data=training_stats)\ndf_stats = df_stats.set_index('fold')\ndf_stats","execution_count":null,"outputs":[]},{"metadata":{"id":"mkyubuJSOzg3"},"cell_type":"markdown","source":"# 6. Setting File Submission\n","execution_count":null},{"metadata":{"id":"3bWYxgST3X8k","trusted":true},"cell_type":"code","source":"to_submit =np.mean(all_folds_preds,0)","execution_count":null,"outputs":[]},{"metadata":{"id":"844CyjlFjwfw","outputId":"16b63c8f-2cda-4b80-dcc4-4e221a0e6c7b","trusted":true},"cell_type":"code","source":"sub=pd.DataFrame()\nsub['id'] = df['id']\nsub['target'] = to_submit\nsub['target'] = sub['target'].apply(lambda x: 1 if x>0.5 else 0)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"b7SeQk7Sl4YZ","trusted":true},"cell_type":"code","source":"sub.to_csv('bert_base_12_2e-5_64.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}