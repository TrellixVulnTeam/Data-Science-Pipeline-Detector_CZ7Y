{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook can use `sentence-transformers/bert-base-nli-mean-tokens` or `cardiffnlp/twitter-roberta-base` or `roberta-base`\n\n## References\n1. [roBERTA-base + PyTorch for Sent. Classification](https://www.kaggle.com/bumjunkoo/roberta-for-sentiment-classification)\n2. [NLP with Disaster Tweet](https://www.kaggle.com/theblackmamba31/nlp-with-disaster-tweet)\n3. [Balanced Sampling between classes with torchvision DataLoader](https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3)","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-28T11:50:19.912064Z","iopub.execute_input":"2021-06-28T11:50:19.912419Z","iopub.status.idle":"2021-06-28T11:50:19.922579Z","shell.execute_reply.started":"2021-06-28T11:50:19.912364Z","shell.execute_reply":"2021-06-28T11:50:19.921719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport multiprocessing\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport emoji\nimport re\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:19.927528Z","iopub.execute_input":"2021-06-28T11:50:19.928074Z","iopub.status.idle":"2021-06-28T11:50:22.84549Z","shell.execute_reply.started":"2021-06-28T11:50:19.928036Z","shell.execute_reply":"2021-06-28T11:50:22.844559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading\n\nWe will now load our data into different DataFrames.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\", sep=\",\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\", sep=\",\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.847092Z","iopub.execute_input":"2021-06-28T11:50:22.847415Z","iopub.status.idle":"2021-06-28T11:50:22.886236Z","shell.execute_reply.started":"2021-06-28T11:50:22.847379Z","shell.execute_reply":"2021-06-28T11:50:22.885438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.887621Z","iopub.execute_input":"2021-06-28T11:50:22.887937Z","iopub.status.idle":"2021-06-28T11:50:22.905348Z","shell.execute_reply.started":"2021-06-28T11:50:22.887903Z","shell.execute_reply":"2021-06-28T11:50:22.904022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.906691Z","iopub.execute_input":"2021-06-28T11:50:22.907167Z","iopub.status.idle":"2021-06-28T11:50:22.917595Z","shell.execute_reply.started":"2021-06-28T11:50:22.907128Z","shell.execute_reply":"2021-06-28T11:50:22.916322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Length of training data: {len(train_df)}\")\nprint(f\"Length of testing data: {len(test_df)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.91934Z","iopub.execute_input":"2021-06-28T11:50:22.919788Z","iopub.status.idle":"2021-06-28T11:50:22.927279Z","shell.execute_reply.started":"2021-06-28T11:50:22.919749Z","shell.execute_reply":"2021-06-28T11:50:22.926052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Missing data in training:\\n{train_df.isnull().sum()}\")\nprint(\"-\" * 20)\nprint(f\"Missing data in testing:\\n{test_df.isnull().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.929409Z","iopub.execute_input":"2021-06-28T11:50:22.929799Z","iopub.status.idle":"2021-06-28T11:50:22.943538Z","shell.execute_reply.started":"2021-06-28T11:50:22.929761Z","shell.execute_reply":"2021-06-28T11:50:22.942325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the data.\n\nWe will clean the tweets for each entry in text.","metadata":{}},{"cell_type":"code","source":"def clean_tweet(txt):\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    txt = re.sub(r'\\n','',txt)\n    # to remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+','',txt)\n    txt = re.sub(r\"https?://\\S+|www\\.\\S+\",\"\",txt)\n    txt = re.sub(r\"<.*?>\",\"\",txt)\n    return str.lower(txt)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.946755Z","iopub.execute_input":"2021-06-28T11:50:22.94715Z","iopub.status.idle":"2021-06-28T11:50:22.954494Z","shell.execute_reply.started":"2021-06-28T11:50:22.947122Z","shell.execute_reply":"2021-06-28T11:50:22.95372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\n\nprint(train_df.iloc[0, -2])\ntrain_df.text = train_df.text.progress_apply(clean_tweet)\nprint(train_df.iloc[0, -2])","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:22.957083Z","iopub.execute_input":"2021-06-28T11:50:22.957557Z","iopub.status.idle":"2021-06-28T11:50:26.607722Z","shell.execute_reply.started":"2021-06-28T11:50:22.957515Z","shell.execute_reply":"2021-06-28T11:50:26.604421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.iloc[0, -1])\ntest_df.text = test_df.text.progress_apply(clean_tweet)\nprint(test_df.iloc[0, -1])                            ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:26.609126Z","iopub.execute_input":"2021-06-28T11:50:26.609901Z","iopub.status.idle":"2021-06-28T11:50:28.214705Z","shell.execute_reply.started":"2021-06-28T11:50:26.609859Z","shell.execute_reply":"2021-06-28T11:50:28.213697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset","metadata":{}},{"cell_type":"code","source":"# model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\nmodel_name = \"cardiffnlp/twitter-roberta-base\"\n# model_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:28.216103Z","iopub.execute_input":"2021-06-28T11:50:28.216483Z","iopub.status.idle":"2021-06-28T11:50:31.545185Z","shell.execute_reply.started":"2021-06-28T11:50:28.216443Z","shell.execute_reply":"2021-06-28T11:50:31.544302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentences(sentences):\n    encoded = tokenizer(sentences, padding=True, return_attention_mask=True, return_tensors='pt')\n    return encoded","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.546636Z","iopub.execute_input":"2021-06-28T11:50:31.547179Z","iopub.status.idle":"2021-06-28T11:50:31.552044Z","shell.execute_reply.started":"2021-06-28T11:50:31.547138Z","shell.execute_reply":"2021-06-28T11:50:31.551263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_batch(batch):\n    sentences, targets = list(zip(*batch))\n    encoded = encode_sentences(list(sentences))\n    targets = torch.tensor(targets)\n    return encoded, targets\n\nclass DisasterDataset(torch.utils.data.Dataset):    \n    def __init__(self, df):\n        self.df = df.text.to_list()\n        self.targets = df.target.to_list()\n     \n    def __getitem__(self, idx):\n        sentence = self.df[idx]\n        target = self.targets[idx]\n        return sentence, target\n     \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.553404Z","iopub.execute_input":"2021-06-28T11:50:31.554007Z","iopub.status.idle":"2021-06-28T11:50:31.56214Z","shell.execute_reply.started":"2021-06-28T11:50:31.553968Z","shell.execute_reply":"2021-06-28T11:50:31.5611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = DisasterDataset(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.563421Z","iopub.execute_input":"2021-06-28T11:50:31.563871Z","iopub.status.idle":"2021-06-28T11:50:31.572088Z","shell.execute_reply.started":"2021-06-28T11:50:31.563832Z","shell.execute_reply":"2021-06-28T11:50:31.571244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display text and label.\nprint('\\nFirst iteration of data set: ', next(iter(dataset)), '\\n')\n# Print how many items are in the data set\nprint('Length of data set: ', len(dataset), '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.573208Z","iopub.execute_input":"2021-06-28T11:50:31.573705Z","iopub.status.idle":"2021-06-28T11:50:31.583735Z","shell.execute_reply.started":"2021-06-28T11:50:31.573664Z","shell.execute_reply":"2021-06-28T11:50:31.582794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super(Model, self).__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        self.classify = nn.Sequential(\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, num_classes)\n        )\n        \n    def mean_pooling(self, outputs, attention_mask):\n        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n    def forward(self, inputs):\n        outputs = self.encoder(**inputs)\n        return self.classify(outputs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.585306Z","iopub.execute_input":"2021-06-28T11:50:31.585753Z","iopub.status.idle":"2021-06-28T11:50:31.594652Z","shell.execute_reply.started":"2021-06-28T11:50:31.585713Z","shell.execute_reply":"2021-06-28T11:50:31.593577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(model_name, 2)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:31.59614Z","iopub.execute_input":"2021-06-28T11:50:31.596537Z","iopub.status.idle":"2021-06-28T11:50:37.539829Z","shell.execute_reply.started":"2021-06-28T11:50:31.596499Z","shell.execute_reply":"2021-06-28T11:50:37.538633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    example_sentence = \"On the plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE\"\n    example_enc = encode_sentences([example_sentence]).to(device)\n    example_output = model(example_enc)\n    print(example_output)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:37.541832Z","iopub.execute_input":"2021-06-28T11:50:37.542678Z","iopub.status.idle":"2021-06-28T11:50:37.87821Z","shell.execute_reply.started":"2021-06-28T11:50:37.542605Z","shell.execute_reply":"2021-06-28T11:50:37.876598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train_df.target, data=train_df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:37.879507Z","iopub.execute_input":"2021-06-28T11:50:37.879836Z","iopub.status.idle":"2021-06-28T11:50:37.996214Z","shell.execute_reply.started":"2021-06-28T11:50:37.879796Z","shell.execute_reply":"2021-06-28T11:50:37.995445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will try to balance the dataset","metadata":{}},{"cell_type":"code","source":"def make_weights_for_balanced_classes(targets, n_classes):                        \n    count = [0] * n_classes                                                      \n    for t in targets:                                                         \n        count[t] += 1                                                     \n    weight_per_class = [0.] * n_classes                                      \n    N = float(sum(count))                                                   \n    for i in range(n_classes):                                                   \n        weight_per_class[i] = N / float(count[i])                                 \n    weight = [0] * len(targets)    \n    for idx, val in enumerate(targets):                                          \n        weight[idx] = weight_per_class[val]                                  \n    return weight ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:37.997623Z","iopub.execute_input":"2021-06-28T11:50:37.997996Z","iopub.status.idle":"2021-06-28T11:50:38.004356Z","shell.execute_reply.started":"2021-06-28T11:50:37.997956Z","shell.execute_reply":"2021-06-28T11:50:38.003147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataloaders(train_df):\n    X_train, X_val = train_test_split(train_df, test_size=0.10, random_state=0, stratify=train_df.target)\n    \n#     weights = make_weights_for_balanced_classes(X_train.target, 2)\n#     sampler = torch.utils.data.sampler.WeightedRandomSampler(torch.DoubleTensor(weights), len(weights))\n    \n    train_dataset = DisasterDataset(X_train)\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, collate_fn=collate_batch, num_workers=(multiprocessing.cpu_count() - 1), pin_memory=True)\n\n    val_dataset = DisasterDataset(X_val)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, collate_fn=collate_batch, num_workers=(multiprocessing.cpu_count() - 1), pin_memory=True)\n\n    return train_dataloader, val_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.006065Z","iopub.execute_input":"2021-06-28T11:50:38.006548Z","iopub.status.idle":"2021-06-28T11:50:38.017505Z","shell.execute_reply.started":"2021-06-28T11:50:38.006505Z","shell.execute_reply":"2021-06-28T11:50:38.016701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, val_dataloader = prepare_dataloaders(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.018545Z","iopub.execute_input":"2021-06-28T11:50:38.018801Z","iopub.status.idle":"2021-06-28T11:50:38.035336Z","shell.execute_reply.started":"2021-06-28T11:50:38.018777Z","shell.execute_reply":"2021-06-28T11:50:38.034514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"EPOCHS = 5\nloss_fn = nn.CrossEntropyLoss().to(device)\noptimizer = optim.AdamW(model.parameters(), betas = (0.99, 0.98), lr=2e-5)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.037014Z","iopub.execute_input":"2021-06-28T11:50:38.037265Z","iopub.status.idle":"2021-06-28T11:50:38.045601Z","shell.execute_reply.started":"2021-06-28T11:50:38.037241Z","shell.execute_reply":"2021-06-28T11:50:38.044557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader, loss_fn, optimizer, scheduler):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for sentences, targets in tqdm(dataloader):\n        input_ids = sentences[\"input_ids\"].to(device)\n        attention_mask = sentences[\"attention_mask\"].to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()        \n        outputs = model(dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ))\n        \n        train_loss = loss_fn(outputs, targets)\n        train_loss.backward()\n        losses.append(train_loss.item())\n        \n        _, preds = torch.max(outputs, dim=1)\n        correct_predictions += torch.sum(preds == targets)\n        \n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.047184Z","iopub.execute_input":"2021-06-28T11:50:38.04793Z","iopub.status.idle":"2021-06-28T11:50:38.057042Z","shell.execute_reply.started":"2021-06-28T11:50:38.04789Z","shell.execute_reply":"2021-06-28T11:50:38.056285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataloader, loss_fn):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for sentences, targets in tqdm(dataloader):\n            input_ids = sentences[\"input_ids\"].to(device)\n            attention_mask = sentences[\"attention_mask\"].to(device)\n            targets = targets.to(device)\n            \n            outputs = model(dict(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            ))\n            \n            val_loss = loss_fn(outputs, targets)\n            losses.append(val_loss.item())\n            \n            _, preds = torch.max(outputs, dim=1)\n            correct_predictions += torch.sum(preds == targets)\n            \n    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.059922Z","iopub.execute_input":"2021-06-28T11:50:38.060201Z","iopub.status.idle":"2021-06-28T11:50:38.070227Z","shell.execute_reply.started":"2021-06-28T11:50:38.060175Z","shell.execute_reply":"2021-06-28T11:50:38.069375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch: {epoch + 1} / {EPOCHS}\")\n    \n    train_accuracy, train_loss = train(model, train_dataloader, loss_fn, optimizer, scheduler)\n    val_accuracy, val_loss = validate(model, val_dataloader, loss_fn)\n    \n    print(f\"Training Loss: {train_loss} | Training Accuracy: {train_accuracy}\")\n    print(f\"Validation Loss: {val_loss} | Validation Accuracy: {val_accuracy}\")    ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:50:38.075725Z","iopub.execute_input":"2021-06-28T11:50:38.07616Z","iopub.status.idle":"2021-06-28T11:53:35.489415Z","shell.execute_reply.started":"2021-06-28T11:50:38.076129Z","shell.execute_reply":"2021-06-28T11:53:35.488457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_encoded = encode_sentences(test_df.text.to_list())\n    input_ids = test_encoded[\"input_ids\"].to(device)\n    attention_mask = test_encoded[\"attention_mask\"].to(device)\n    predictions = model(dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:53:35.492155Z","iopub.execute_input":"2021-06-28T11:53:35.492734Z","iopub.status.idle":"2021-06-28T11:53:36.445333Z","shell.execute_reply.started":"2021-06-28T11:53:35.492689Z","shell.execute_reply":"2021-06-28T11:53:36.444489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.argmax(predictions.cpu(), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:53:36.446768Z","iopub.execute_input":"2021-06-28T11:53:36.447096Z","iopub.status.idle":"2021-06-28T11:53:41.772802Z","shell.execute_reply.started":"2021-06-28T11:53:36.447059Z","shell.execute_reply":"2021-06-28T11:53:41.771916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([test_df.id, pd.Series(predictions)], axis=1)\nsubmission.rename(columns = {0:'target'}, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T11:53:41.774052Z","iopub.execute_input":"2021-06-28T11:53:41.774395Z","iopub.status.idle":"2021-06-28T11:53:41.901983Z","shell.execute_reply.started":"2021-06-28T11:53:41.774358Z","shell.execute_reply":"2021-06-28T11:53:41.901253Z"},"trusted":true},"execution_count":null,"outputs":[]}]}