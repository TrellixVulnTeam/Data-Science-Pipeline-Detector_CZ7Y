{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Essential Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing packages\nimport pandas as pd # for dataframe\nimport numpy as np  # for algebric \nimport nltk         #for nlp \nimport matplotlib.pyplot as plt # for visualization\nimport seaborn as sns           # for visualization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing csv files\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# viewing each unique values in each columns\nfor i in train.columns:\n    print('Column Name  :',i)\n    print(train[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# id is not essential. so, i'm droping it\ndf_train = train.drop(['id'],axis=1)\ndf_test = test.drop(['id'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Hence when it comes twitter, Hashtag is one of important thing. so, i'm created new column contains only hashtags\n# for this i used regrx package.\nimport re\ndf_train['Hashtag'] = df_train['text'].map(lambda x: re.findall(r'#(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_test['Hashtag'] = df_test['text'].map(lambda x: re.findall(r'#(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_train['@'] = df_train['text'].map(lambda x: re.findall(r'@(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_test['@'] = df_test['text'].map(lambda x: re.findall(r'@(\\w+)',x)).apply(lambda x: \", \".join(x))\n\n# data Cleaning\n# i'm defining function to clean data. it will make work easier\ndef remove_punctuation(txt):\n    import string\n    result = txt.translate(str.maketrans('','',string.punctuation))\n    return result\ndef lower_text(txt):\n    return txt.lower()\ndef remove_no(txt):\n    import re\n    return re.sub(r\"\\d+\",\"\",txt)\ndef remove_html_tags(text):\n    \"\"\"Remove html tags from a string\"\"\"\n    import re\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\ndef removeurl(txt):\n    import re\n    return re.sub(r'http?\\S+|www\\.\\S+', '',txt)\n\n# defining all function to one for this problem\ndef norm(txt):\n    x = remove_punctuation(txt)\n    x = lower_text(x)\n    x = remove_html_tags(x)\n    x = remove_no(x)\n    x = removeurl(x)\n    return x\n\n# applying data cleaning function for text\ndf_train['text'] = df_train['text'].map(lambda x:  norm(x))\ndf_test['text'] = df_test['text'].map(lambda x:  norm(x))\n\n#There are some magic words \"%20\" which indicates space in this data.so,Removing magic words\ndf_train['keyword'] = df_train['keyword'].map(lambda s: s.replace('%20',' ') if isinstance(s, str) else s)\ndf_test['keyword'] = df_test['keyword'].map(lambda s: s.replace('%20',' ') if isinstance(s, str) else s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaned Data\ndf_test\ndf_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Twitts has Hash Tag."},{"metadata":{},"cell_type":"markdown","source":"# Splitting data to view word distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperating data to explore it\ntrain_yes = df_train[df_train[\"target\"]==1]\ntrain_no = df_train[df_train[\"target\"]==0]\ntrain_yes[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# created a para from all text for each target\n# Sepereate Data to two \ntrain_txt_yes = \" \".join(str(i) for i in train_yes['text'])\ntrain_txt_no = \" \".join(str(i) for i in train_no['text'])\ntest_txt = \" \".join(str(i) for i in df_test['text'])\n#train_txt_yes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing empty space with null values\ntrain_yes[\"Hashtag\"]= train_yes[\"Hashtag\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_no[\"Hashtag\"]= train_yes[\"Hashtag\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_yes[\"@\"]= train_yes[\"@\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_no[\"@\"]= train_yes[\"@\"].replace(r'^\\s*$', np.nan, regex=True)\n#train_yes[\"Hashtag\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total No. of disaster tweets                        :\",train_yes[\"text\"].count())\nprint(\"Total No. of hashtag present in disaster tweets     :\",train_yes[\"Hashtag\"].notnull().sum())\nprint(\"Total No. of non-disaster tweets                    :\",train_no[\"text\"].count())\nprint(\"Total No. of hashtag present in non-disaster tweets :\",train_no[\"Hashtag\"].notnull().sum(),\"\\n\")\n\nprint(\"Total No. of disaster tweets                  :\",train_yes[\"text\"].count())\nprint(\"Total No. of @ present in disaster tweets     :\",train_yes[\"@\"].notnull().sum())\nprint(\"Total No. of non-disaster tweets              :\",train_no[\"text\"].count())\nprint(\"Total No. of @ present in non-disaster tweets :\",train_no[\"@\"].notnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From this, we can't find disaster tweets. because hashtag & @ count is not present in non-disaster tweets."},{"metadata":{},"cell_type":"markdown","source":"### Tokenization & Lemminization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# deffining function for tokenization & lemmatize.\n# Tokenization function\ndef Token_and_removestopword(txt):\n    import nltk\n    from nltk.corpus import stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    words = nltk.word_tokenize(txt)\n    without_stop_words = []\n    for word in words:\n        if word not in stop_words:\n            without_stop_words.append(word)\n    return without_stop_words\n#lemmatizing tokenized words\ndef lemmatize_word(tokens,pos=\"v\"): \n    import nltk\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # provide context i.e. part-of-speech \n    lemmas = [lemmatizer.lemmatize(word, pos =pos) for word in tokens] \n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok_yes = Token_and_removestopword(train_txt_yes)\ntok_no = Token_and_removestopword(train_txt_no)\ntest_tok = Token_and_removestopword(test_txt)\ntok_yes[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lem_wd = [lemmatizer.lemmatize(x, pos ='v') for x in tok]\nlem_tok_yes = lemmatize_word(tok_yes)\nlem_tok_no = lemmatize_word(tok_no)\nlem_test = lemmatize_word(test_tok)\nprint(len(tok_yes))\nprint(len(lem_tok_yes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_word_dis(lem_tok,TopN=10):\n    fq = nltk.FreqDist(lem_tok)\n    rslt = pd.DataFrame(fq.most_common(TopN),\n                        columns=['Word', 'Frequency']).set_index('Word')\n    plt.style.use('ggplot')\n    rslt.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_word_dis(lem_tok_yes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_word_dis(lem_tok_no)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lem_test[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF-IDF Vectorizer\ndef tfidf(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    #to convert token input into text\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = TfidfVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    # to get both transform for train & split\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n\"\"\"\nAbove step is not necessary. you can also use below step\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(train_data)\nvectorizer.transform(train_data)  ---> output as array\nvectorizer.transform(test_data) ---> essential if you has test data & to change its shape for further process\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def countvectorizer(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import CountVectorizer\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = CountVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n\"\"\"\nsame as for tdidf\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimentional Reduction\ndef PCA(X_train,Y_train=None,X_test=None,n=1000):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=n)\n    X = pca.fit(X_train,y_train)\n    train = pca.transform(X_train)\n    if X_test.any() == None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ML Algorithms\ndef logreg(X_train, y_train,X_test,n=20):\n    from sklearn.linear_model import LogisticRegression\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\ndef naivebiase(X_train, y_train,X_test,n=20):\n    from sklearn.naive_bayes import GaussianNB\n    model = GaussianNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\ndef accuracy(y_true, y_pred):\n    from sklearn.metrics import accuracy_score\n    return accuracy_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Models - Test Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaned Data\ndf_test\ndf_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#apply tokenization & lemmatization to each text\ndf_train[\"tokenized_text\"] = df_train[\"text\"].apply(lambda x:lemmatize_word(Token_and_removestopword(x)))\ndf_test[\"tokenized_text\"] = df_test[\"text\"].apply(lambda x:lemmatize_word(Token_and_removestopword(x)))\ndf_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization\nconverting into numeric formate\n* Tf-IDF\n* Countvectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfor vectorization in countervector & tfidf the input be in text formate.\nthat's why i join all text in list in above mentioned function.\n\"\"\"\ntfidf_train,tfidf_test = tfidf(df_train[\"tokenized_text\"],df_test[\"tokenized_text\"])\ncnt_train,cnt_test = countvectorizer(df_train[\"tokenized_text\"],df_test[\"tokenized_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check whether shape aligns or not\nprint(tfidf_train.shape)\nprint(cnt_train.shape)\nprint(df_train[\"target\"].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test & train Split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX=tfidf_train\ny=df_train[\"target\"]\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.1,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# dimentionality reduction ---> it is not necessary for this problem. because it is very small data\nx_train,x_test = PCA(x_train,y_train,x_test,n=500)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# prediction using logistic regression model. it is defined as function above\npreds = logreg(x_train, y_train,x_test)\naccuracy(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction using naivebiase Gaussian model. it is defined as function above\npreds = naivebiase(x_train, y_train,x_test)\naccuracy(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CounterVectorizer"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX=cnt_train\ny=df_train[\"target\"]\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.1,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"x_train,x_test = PCA(x_train,y_train,x_test,n=500)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"preds = logreg(x_train, y_train,x_test)\naccuracy(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = naivebiase(x_train, y_train,x_test)\naccuracy(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights:\nBoth Vectorizer provides same accuracy.\nhence logistic regression model provides high accuracy than naive bayes"},{"metadata":{},"cell_type":"markdown","source":"# Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = cnt_train\ny_train = df_train[\"target\"] \nx_test = cnt_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check wheather shape alligns or not\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if shape is not aligned check wheather do you transform test data in vectorization part or not.\n#if test data is not transformed as per train data it may led to error due to mismatch in features.\nx_train,x_test = PCA(x_train,y_train,x_test,n=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = logreg(x_train,y_train,x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submmision"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test[[\"id\"]]\nsubmission[\"target\"] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thanks for viewing this kernal.  Its my first kernal on NLP in Kaggle :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}