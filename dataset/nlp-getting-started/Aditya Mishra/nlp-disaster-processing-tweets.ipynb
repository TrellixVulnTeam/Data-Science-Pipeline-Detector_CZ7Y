{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-31T11:10:40.547157Z","iopub.execute_input":"2021-12-31T11:10:40.547582Z","iopub.status.idle":"2021-12-31T11:10:40.578035Z","shell.execute_reply.started":"2021-12-31T11:10:40.547475Z","shell.execute_reply":"2021-12-31T11:10:40.576514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain = train[['text','target']]\ntrain.drop_duplicates(inplace=True)\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest = test[['id','text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:40.579688Z","iopub.execute_input":"2021-12-31T11:10:40.580035Z","iopub.status.idle":"2021-12-31T11:10:40.682049Z","shell.execute_reply.started":"2021-12-31T11:10:40.580005Z","shell.execute_reply":"2021-12-31T11:10:40.681251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning the tweets","metadata":{}},{"cell_type":"code","source":"import string\nimport re\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef clean(text):\n    # only aplhabest\n    text =  re.sub(r'[^A-Za-z ]+', '', text) \n    # converting all the teweets to lower case\n    text = text.lower()\n    # remove urls and html tags\n    url = re.compile(r\"https?://\\s+|www\\.\\S+\")\n    text = url.sub(r\"\", text)\n    html = re.compile(r\"<.*?>\")\n    text = html.sub(r\"\",text)\n    # removing emojis\n    emoji = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji.sub(r'',text)\n    #removing punctutions\n    punctuations = str.maketrans(\"\",\"\", string.punctuation)\n    text = text.translate(punctuations)\n    #lemmatization\n    text = text.split(' ')\n    text = [lemmatizer.lemmatize(i) for i in text]\n    text = ' '.join(text) \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:40.683782Z","iopub.execute_input":"2021-12-31T11:10:40.684143Z","iopub.status.idle":"2021-12-31T11:10:42.489452Z","shell.execute_reply.started":"2021-12-31T11:10:40.684116Z","shell.execute_reply":"2021-12-31T11:10:42.4881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train.text.apply(lambda x: clean(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:42.491616Z","iopub.execute_input":"2021-12-31T11:10:42.491876Z","iopub.status.idle":"2021-12-31T11:10:45.134611Z","shell.execute_reply.started":"2021-12-31T11:10:42.491847Z","shell.execute_reply":"2021-12-31T11:10:45.13347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet_cleaner(tweet):\n    # Acronyms and miswritten words\n    tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"year-old\", \"years old\", tweet)\n    tweet = re.sub(r\"yearold\", \"years old\", tweet)\n    tweet = re.sub(r\"yr old\", \"years old\", tweet)\n    tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n    tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n    tweet = re.sub(r\"rly\", \"railway\", tweet)\n    tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n    tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n    tweet = re.sub(r\"pm\", \"\", tweet)\n    tweet = re.sub(r\"PM\", \"\", tweet)\n    tweet = re.sub(r\"nan\", \" \", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n    tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n    tweet = re.sub(r\"RT\", \"retweet\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n\n    # Special characters\n    tweet = re.sub(r\"%20\", \" \", tweet)\n    tweet = re.sub(r\"%\", \" \", tweet)\n    tweet = re.sub(r\"@\", \" \", tweet)\n    tweet = re.sub(r\"#\", \" \", tweet)\n    tweet = re.sub(r\"'\", \" \", tweet)\n    tweet = re.sub(r\"\\x89û_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89ûò\", \" \", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"re\\x89û_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89û\", \" \", tweet)\n    tweet = re.sub(r\"\\x89Û\", \" \", tweet)\n    tweet = re.sub(r\"re\\x89Û\", \"re \", tweet)\n    tweet = re.sub(r\"re\\x89û\", \"re \", tweet)\n    tweet = re.sub(r\"\\x89ûª\", \"'\", tweet)\n    tweet = re.sub(r\"\\x89û\", \" \", tweet)\n    tweet = re.sub(r\"\\x89ûò\", \" \", tweet)\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n\n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"Im\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"can not\", tweet)\n    tweet = re.sub(r\"cant\", \"can not\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"dont\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:40:32.435928Z","iopub.execute_input":"2021-12-31T11:40:32.43626Z","iopub.status.idle":"2021-12-31T11:40:32.480635Z","shell.execute_reply.started":"2021-12-31T11:40:32.436228Z","shell.execute_reply":"2021-12-31T11:40:32.479507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train.text.apply(lambda x: tweet_cleaner(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:40:48.422153Z","iopub.execute_input":"2021-12-31T11:40:48.422479Z","iopub.status.idle":"2021-12-31T11:40:49.607238Z","shell.execute_reply.started":"2021-12-31T11:40:48.422447Z","shell.execute_reply":"2021-12-31T11:40:49.606353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing stop words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = set(stopwords.words(\"english\"))\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    return \" \".join(text)\ntrain[\"text\"] = train[\"text\"].map(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:45.136052Z","iopub.execute_input":"2021-12-31T11:10:45.136588Z","iopub.status.idle":"2021-12-31T11:10:45.184228Z","shell.execute_reply.started":"2021-12-31T11:10:45.136341Z","shell.execute_reply":"2021-12-31T11:10:45.183238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.text","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:45.185875Z","iopub.execute_input":"2021-12-31T11:10:45.186125Z","iopub.status.idle":"2021-12-31T11:10:45.197912Z","shell.execute_reply.started":"2021-12-31T11:10:45.186095Z","shell.execute_reply":"2021-12-31T11:10:45.196918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embeddings using GloVe","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nimport nltk\n\ndef create_corpus_tk(df):\n    corpus=[]\n    for text in train[\"text\"]:\n        words = [word.lower() for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:45.199752Z","iopub.execute_input":"2021-12-31T11:10:45.200106Z","iopub.status.idle":"2021-12-31T11:10:45.21445Z","shell.execute_reply.started":"2021-12-31T11:10:45.20005Z","shell.execute_reply":"2021-12-31T11:10:45.213257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = create_corpus_tk(train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:45.215571Z","iopub.execute_input":"2021-12-31T11:10:45.21658Z","iopub.status.idle":"2021-12-31T11:10:46.121874Z","shell.execute_reply.started":"2021-12-31T11:10:45.216538Z","shell.execute_reply":"2021-12-31T11:10:46.120494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(corpus)\nprint(num_words)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:46.123756Z","iopub.execute_input":"2021-12-31T11:10:46.124117Z","iopub.status.idle":"2021-12-31T11:10:46.131291Z","shell.execute_reply.started":"2021-12-31T11:10:46.124071Z","shell.execute_reply":"2021-12-31T11:10:46.130239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train/Test Split","metadata":{}},{"cell_type":"code","source":"train_size = int(train.shape[0]*0.8)\n\ntrain_sentences = train.text[:train_size]\ntrain_labels = train.target[:train_size]\n\ntest_sentences = train.text[train_size:]\ntest_labels = train.target[train_size:]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:46.13481Z","iopub.execute_input":"2021-12-31T11:10:46.135296Z","iopub.status.idle":"2021-12-31T11:10:46.150135Z","shell.execute_reply.started":"2021-12-31T11:10:46.135258Z","shell.execute_reply":"2021-12-31T11:10:46.148756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert texts to Sequence of numbers","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmax_len = 50","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:46.151673Z","iopub.execute_input":"2021-12-31T11:10:46.151946Z","iopub.status.idle":"2021-12-31T11:10:52.503035Z","shell.execute_reply.started":"2021-12-31T11:10:46.151915Z","shell.execute_reply":"2021-12-31T11:10:52.502084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = num_words)\ntokenizer.fit_on_texts(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.504821Z","iopub.execute_input":"2021-12-31T11:10:52.505054Z","iopub.status.idle":"2021-12-31T11:10:52.636216Z","shell.execute_reply.started":"2021-12-31T11:10:52.505027Z","shell.execute_reply":"2021-12-31T11:10:52.635224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences = tokenizer.texts_to_sequences(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.637713Z","iopub.execute_input":"2021-12-31T11:10:52.637977Z","iopub.status.idle":"2021-12-31T11:10:52.723691Z","shell.execute_reply.started":"2021-12-31T11:10:52.63795Z","shell.execute_reply":"2021-12-31T11:10:52.722552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentences.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.727321Z","iopub.execute_input":"2021-12-31T11:10:52.727818Z","iopub.status.idle":"2021-12-31T11:10:52.735694Z","shell.execute_reply.started":"2021-12-31T11:10:52.727786Z","shell.execute_reply":"2021-12-31T11:10:52.734686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences[1:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.73723Z","iopub.execute_input":"2021-12-31T11:10:52.737603Z","iopub.status.idle":"2021-12-31T11:10:52.749951Z","shell.execute_reply.started":"2021-12-31T11:10:52.737571Z","shell.execute_reply":"2021-12-31T11:10:52.74875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# padding our sequences to pass it in keras model (inputs should be of the same size)\ntrain_padded = pad_sequences(\n    train_sequences, maxlen=max_len, truncating=\"post\", padding=\"post\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.752418Z","iopub.execute_input":"2021-12-31T11:10:52.752795Z","iopub.status.idle":"2021-12-31T11:10:52.790447Z","shell.execute_reply.started":"2021-12-31T11:10:52.752751Z","shell.execute_reply":"2021-12-31T11:10:52.789555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.79173Z","iopub.execute_input":"2021-12-31T11:10:52.792141Z","iopub.status.idle":"2021-12-31T11:10:52.800206Z","shell.execute_reply.started":"2021-12-31T11:10:52.792098Z","shell.execute_reply":"2021-12-31T11:10:52.799136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# padding the test data\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(\n    test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.801584Z","iopub.execute_input":"2021-12-31T11:10:52.802628Z","iopub.status.idle":"2021-12-31T11:10:52.842388Z","shell.execute_reply.started":"2021-12-31T11:10:52.802586Z","shell.execute_reply":"2021-12-31T11:10:52.841271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_padded","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.843801Z","iopub.execute_input":"2021-12-31T11:10:52.84429Z","iopub.status.idle":"2021-12-31T11:10:52.851465Z","shell.execute_reply.started":"2021-12-31T11:10:52.844256Z","shell.execute_reply":"2021-12-31T11:10:52.8506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.text[0])\nprint(train_sequences[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.852868Z","iopub.execute_input":"2021-12-31T11:10:52.853475Z","iopub.status.idle":"2021-12-31T11:10:52.866587Z","shell.execute_reply.started":"2021-12-31T11:10:52.853437Z","shell.execute_reply":"2021-12-31T11:10:52.8658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a dictionary out of the words and their respective sequences\nword_index = tokenizer.word_index\nprint(len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.867728Z","iopub.execute_input":"2021-12-31T11:10:52.86807Z","iopub.status.idle":"2021-12-31T11:10:52.878489Z","shell.execute_reply.started":"2021-12-31T11:10:52.868042Z","shell.execute_reply":"2021-12-31T11:10:52.877313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index[\"like\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.880526Z","iopub.execute_input":"2021-12-31T11:10:52.881228Z","iopub.status.idle":"2021-12-31T11:10:52.8976Z","shell.execute_reply.started":"2021-12-31T11:10:52.881181Z","shell.execute_reply":"2021-12-31T11:10:52.896666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Embedding dict","metadata":{}},{"cell_type":"code","source":"embedding_dict = {}\nwith open(\"../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.array(values[1:],\"float32\")\n        embedding_dict[word] = vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:10:52.899783Z","iopub.execute_input":"2021-12-31T11:10:52.900501Z","iopub.status.idle":"2021-12-31T11:11:29.961654Z","shell.execute_reply.started":"2021-12-31T11:10:52.900457Z","shell.execute_reply":"2021-12-31T11:11:29.960503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words,100))\n\nfor word, i in word_index.items():\n    if i<num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:11:29.962927Z","iopub.execute_input":"2021-12-31T11:11:29.963156Z","iopub.status.idle":"2021-12-31T11:11:30.012699Z","shell.execute_reply.started":"2021-12-31T11:11:29.963129Z","shell.execute_reply":"2021-12-31T11:11:30.011636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:11:30.014543Z","iopub.execute_input":"2021-12-31T11:11:30.014852Z","iopub.status.idle":"2021-12-31T11:11:30.022376Z","shell.execute_reply.started":"2021-12-31T11:11:30.014819Z","shell.execute_reply":"2021-12-31T11:11:30.021127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_padded.shape)\nprint(train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:11:30.024304Z","iopub.execute_input":"2021-12-31T11:11:30.024733Z","iopub.status.idle":"2021-12-31T11:11:30.03487Z","shell.execute_reply.started":"2021-12-31T11:11:30.024621Z","shell.execute_reply":"2021-12-31T11:11:30.033478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_padded.shape)\nprint(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:11:30.036802Z","iopub.execute_input":"2021-12-31T11:11:30.037194Z","iopub.status.idle":"2021-12-31T11:11:30.048963Z","shell.execute_reply.started":"2021-12-31T11:11:30.037129Z","shell.execute_reply":"2021-12-31T11:11:30.047538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline Model with GloVe","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import adam_v2\n\nmodel = Sequential()\n\nmodel.add(\n    Embedding(\n        num_words,\n        100,\n        embeddings_initializer = Constant(embedding_matrix),\n        input_length = max_len,\n        trainable = False\n    )\n)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n# model.add(LSTM(100,dropout=0.1))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\noptimizer = adam_v2.Adam(learning_rate=5e-3)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nmodel.summary() ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:12:43.614653Z","iopub.execute_input":"2021-12-31T11:12:43.614942Z","iopub.status.idle":"2021-12-31T11:12:43.789015Z","shell.execute_reply.started":"2021-12-31T11:12:43.614913Z","shell.execute_reply":"2021-12-31T11:12:43.788085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history  = model.fit(\n    train_padded,\n    train_labels,\n    epochs=15,\n    validation_data=(test_padded, test_labels),\n    verbose=1,\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:12:48.276945Z","iopub.execute_input":"2021-12-31T11:12:48.277296Z","iopub.status.idle":"2021-12-31T11:20:40.122483Z","shell.execute_reply.started":"2021-12-31T11:12:48.277261Z","shell.execute_reply":"2021-12-31T11:20:40.121484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data for submission\ntest['text'] = test.text.apply(lambda x: clean(x))\ntest[\"text\"] = test[\"text\"].map(remove_stopwords)\ntest['text'] = test.text.apply(lambda x: tweet_cleaner(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:22:03.360634Z","iopub.execute_input":"2021-12-31T11:22:03.360959Z","iopub.status.idle":"2021-12-31T11:22:03.647657Z","shell.execute_reply.started":"2021-12-31T11:22:03.360928Z","shell.execute_reply":"2021-12-31T11:22:03.646264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(test.text)\npadded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:22:11.655937Z","iopub.execute_input":"2021-12-31T11:22:11.656263Z","iopub.status.idle":"2021-12-31T11:22:11.721573Z","shell.execute_reply.started":"2021-12-31T11:22:11.65623Z","shell.execute_reply":"2021-12-31T11:22:11.720281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(padded)\npred_int = pred.round().astype(\"int\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:15.754985Z","iopub.execute_input":"2021-12-31T11:23:15.755338Z","iopub.status.idle":"2021-12-31T11:23:20.709691Z","shell.execute_reply.started":"2021-12-31T11:23:15.755306Z","shell.execute_reply":"2021-12-31T11:23:20.708814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:20.711677Z","iopub.execute_input":"2021-12-31T11:23:20.71205Z","iopub.status.idle":"2021-12-31T11:23:20.719035Z","shell.execute_reply.started":"2021-12-31T11:23:20.712019Z","shell.execute_reply":"2021-12-31T11:23:20.717937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_int.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:20.720593Z","iopub.execute_input":"2021-12-31T11:23:20.721127Z","iopub.status.idle":"2021-12-31T11:23:20.735881Z","shell.execute_reply.started":"2021-12-31T11:23:20.721088Z","shell.execute_reply":"2021-12-31T11:23:20.734601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:24.393764Z","iopub.execute_input":"2021-12-31T11:23:24.394614Z","iopub.status.idle":"2021-12-31T11:23:24.401141Z","shell.execute_reply.started":"2021-12-31T11:23:24.394571Z","shell.execute_reply":"2021-12-31T11:23:24.400231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"target\"] = pred_int\nsubmission = test[['id','target']]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:30.472533Z","iopub.execute_input":"2021-12-31T11:23:30.472843Z","iopub.status.idle":"2021-12-31T11:23:30.482744Z","shell.execute_reply.started":"2021-12-31T11:23:30.472809Z","shell.execute_reply":"2021-12-31T11:23:30.481368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:31.249095Z","iopub.execute_input":"2021-12-31T11:23:31.249403Z","iopub.status.idle":"2021-12-31T11:23:31.262636Z","shell.execute_reply.started":"2021-12-31T11:23:31.249374Z","shell.execute_reply":"2021-12-31T11:23:31.261622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(100)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T11:23:32.018831Z","iopub.execute_input":"2021-12-31T11:23:32.019184Z","iopub.status.idle":"2021-12-31T11:23:32.042614Z","shell.execute_reply.started":"2021-12-31T11:23:32.019133Z","shell.execute_reply":"2021-12-31T11:23:32.041649Z"},"trusted":true},"execution_count":null,"outputs":[]}]}