{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip -q install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\nimport string\nfrom spellchecker import SpellChecker\nfrom tqdm.notebook import tqdm\nimport warnings\n\nfrom sklearn.metrics import roc_auc_score, f1_score, matthews_corrcoef\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dropout, Dense, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.models import Model\n\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import TFBertModel, AdamWeightDecay, BertTokenizerFast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constants","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nMAX_LEN = 192\nEPOCHS = 6\nNUM_SPLITS = 5\nLR = 3e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"target\"] = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_emoji(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    try:\n        return url_pattern.sub(r'', text)\n    except:\n        print(text)\n    \ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_html(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/slangtext/slang.txt\", \"r\") as file:\n    chat_words_str = file.read()\n\nchat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\" and \"=\" in line:\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[df[\"target\"]!=-1]\ntest = df[df[\"target\"]==-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Save the loaded tokenizer locally\nsave_path = 'distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_tokenized = fast_encode(test[\"text\"].astype(str), fast_tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss & Metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc(y_true, y_pred):\n    def fallback_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, loss='binary_crossentropy', max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    # last hidden state : (batch_size, sequence_length, hidden_size)\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Dropout(0.35)(cls_token)\n    out = Dense(2, activation='softmax')(x)\n    \n    optimizer = tfa.optimizers.RectifiedAdam(lr=LR)\n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(optimizer=optimizer, loss=loss, metrics=[tf.keras.metrics.AUC()])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callbacks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=0.000001, lr_max=0.000004, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eas = EarlyStopping(monitor='val_auc', min_delta=0.001, patience=3,\n                    verbose=1, mode='max', baseline=None, restore_best_weights=True)\nlrfn = build_lrfn()\nlrs = LearningRateScheduler(lrfn, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test_tokenized)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_steps = len(train) // BATCH_SIZE\nfinal_preds = np.zeros((len(test)))\ntotal_preds = np.zeros((len(train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_idx, valid_idx) in enumerate(kfold.split(X=train['text'], y=train['target'])):\n    print(\"*\"*60)\n    print(\"*\"+\" \"*26+f\"FOLD {fold+1}\"+\" \"*26+\"*\")\n    print(\"*\"*60, end=\"\\n\\n\")\n\n    X_train = train.iloc[train_idx].reset_index(drop=True)\n    X_valid = train.iloc[valid_idx].reset_index(drop=True)\n    \n    y_train = X_train[\"target\"]\n    y_valid = X_valid[\"target\"]\n    \n    X_train_tokenized = fast_encode(X_train.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n    X_valid_tokenized = fast_encode(X_valid.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tokenized, to_categorical(y_train)))\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.shuffle(2048)\n    train_dataset = train_dataset.batch(BATCH_SIZE)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    valid_dataset = tf.data.Dataset\n    valid_dataset = valid_dataset.from_tensor_slices((X_valid_tokenized, to_categorical(y_valid)))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n    \n    # release memory when building models in a loop\n    K.clear_session()\n    with strategy.scope():\n        transformer_layer = TFBertModel.from_pretrained('bert-base-uncased')\n        model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=MAX_LEN)\n    \n    history = model.fit(train_dataset,\n                    steps_per_epoch=num_steps,\n                    validation_data=valid_dataset,\n                    callbacks=[eas, lrs], \n                    epochs=EPOCHS)\n    \n    valid_preds = model.predict(valid_dataset)[:, 1]\n    total_preds[valid_idx] = valid_preds\n    \n    test_preds = model.predict(test_dataset)[:, 1]\n    final_preds += test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actuals = train[\"target\"].values\ntotal_preds[total_preds >= 0.5] = 1\ntotal_preds[total_preds < 0.5] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"AUC: {auc(actuals, total_preds)}\")\nprint(f\"F1 Score: {f1_score(actuals, total_preds)}\")\nprint(f\"MCC: {matthews_corrcoef(actuals, total_preds)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"target\"] = final_preds/NUM_SPLITS\nsubmission[\"target\"] = submission[\"target\"].apply(lambda x: 1 if x>=0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}