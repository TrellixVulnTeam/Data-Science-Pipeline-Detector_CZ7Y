{"cells":[{"metadata":{},"cell_type":"markdown","source":"![title](https://pngriver.com/wp-content/uploads/2018/04/Download-Twitter-PNG-HD-1-768x289.png \"Header\")"},{"metadata":{},"cell_type":"markdown","source":"<h1>Twitter Tweet's Model For Identifying Real Disaster Tweet's</h1>"},{"metadata":{},"cell_type":"markdown","source":"**Data Source :-** https://www.kaggle.com/c/nlp-getting-started/data <br>\n\n**Twitter Tweet's Data Overview :-**<br>\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\nBut, it’s not always clear whether a person’s words are actually announcing a disaster or what.\n\nSo, to indentify this I have build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n\n***Number of Tweet's in Train Data-Set :-*** 7,613<br> \n***Number of Tweet's in Test Data-Set :-*** 3263<br>\n***Total Number of Tweet's :-*** 10,876<br>\n\n**Attribute's Information :-**<br>\n\n***1.*** id - a unique identifier for each tweet<br>\n***2.*** keyword - a particular keyword from the tweet (may be blank)<br>\n***3.*** location - the location the tweet was sent from (may be blank)<br>\n***4.*** text - the text of the tweet<br>\n***5.*** target - this denotes whether a tweet is about a real disaster (1) or not (0)<br>\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom bs4 import BeautifulSoup\nimport re\nfrom tqdm import tqdm\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud\nfrom matplotlib import rc_params\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import precision_recall_curve, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom prettytable import PrettyTable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nrcParams[\"figure.figsize\"] = 8,8\nsns.set_style(\"darkgrid\")\n# plt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Loading The Data-Set's :-"},{"metadata":{},"cell_type":"markdown","source":"***There are three files to load :-***<br><br>\n**2.1.** train.csv (consisting of {id, keyword, location, text, target} as columns)<br>\n**2.2.** test.csv  (consisting of {id, keyword, location, text} as columns)<br>\n**2.3.** y_test.csv  (consisting of target column of test.csv data)<br>"},{"metadata":{},"cell_type":"markdown","source":"# 2.1. Loading Train Data :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nprint(train.info())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By loading **train.csv** we can see through **train.info()** & through **train.head()** that there are columns having **missing values(NaN).**"},{"metadata":{},"cell_type":"markdown","source":"# 2.2. & 2.3. Loading Test Data with y_test Data :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ny_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\n# Droping id column from y_test data-set as that same id is present in test data-set in same order.\ny_test = y_test.drop(\"id\", axis=1)\n\n# Joining test with y_test data-set to make a complete new test data-set.\ntest = pd.DataFrame.join(test, y_test)\nprint(test.info())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After creating a **New Test Data** we can see through **test.info()** & through **test.head()** that there are columns having **missing values(NaN)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are **7613 rows** and **5 columns** in **Train**<br>\nThere are **3263 rows** and **5 columns** in **Test**"},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analysis (EDA) :-<br>\n\n***Following are the parts of Exploratory Data Analysis (EDA) :-***<br><br>\n**3.1.** Heatmap For Finding Both Data-Frame's Missing Values<br>\n**3.2.** Top 10 Locations From Both Data Tweet's<br>\n**3.3.** Data Cleaning<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.3.1** Distribution Of Classes<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.3.2** Top 10 Keywords From Both Data Tweets<br>\n**3.4.** Data Pre-Processing<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.4.1.** Creating Final Data-Frame (Inclusive Of Train & Test Both)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.4.2.** Top 15 Frequent Words In Tweet's<br><br>\nIn the following section I have tried to understand what data is about and what answers can I generate through some visual representations before & after Data Cleaning / Data Pre-Processing."},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Heatmap For Finding Both Data-Frame's Missing Values :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\nsns.heatmap(train.isnull(), ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.heatmap(test.isnull(), ax=axes[1]).set_title(\"Test Data-Frame\")\n\nplt.suptitle(\"Heatmap For Finding Both Data-Frame's Missing Values\", fontsize=25)\n# plt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Through Heatmap** we can see that there are many **missing values** in **location column** of both **Train & Test Data** and a few are in **keyword column** as well.<br><br> We will handle them in **Data Cleaning part.**"},{"metadata":{},"cell_type":"markdown","source":"# 3.2. Top 10 Locations From Both Data Tweet's :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\ncommon_locations_train = train.location.value_counts()[:10]\ncommon_locations_test = test.location.value_counts()[:10]\nsns.barplot(x=common_locations_train, y=common_locations_train.index, ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=common_locations_test, y=common_locations_test.index, ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Top 10 Locations From Both Data Tweet's\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above diagram's we can see the most frequent locations from where the tweet's are actually posted on **Twitter** related to **Natural Disaster's**.<br><br>\nAnd we can easily conclude that in both of the data-set's **New York**, **USA**, **United States** & **Canada** are in the Top Five List."},{"metadata":{},"cell_type":"markdown","source":"# 3.3. Data Cleaning :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(\"location\", axis=1)\n\n# Droping the Missing Values \ntest = test.dropna(axis=0)\n\n# As Data's indexs are not in order so :\ntest = test.reset_index()\n\n# Now, droping the old indexs as it became a column\ntest = test.drop(\"index\", axis=1)\nprint(test.info())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we saw above that the **location** column in **Test Data** has too much missing value's.<br><br>\nSo, I am **droping** that column because that column doesn't also help us in any way to possibly predict whether a tweet is talking about a **real disaster or not**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(\"location\", axis=1)\n\n# Droping the Missing Values\ntrain = train.dropna(axis=0)\n\n# As Data's indexs are not in order so :\ntrain = train.reset_index()\n\n# Now, droping the old indexs as it became a column\ntrain = train.drop(\"index\", axis=1)\nprint(train.info())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we saw above that the **location** column in **Train Data** has too much missing value's.<br><br>\nSo, I am **droping** that column because that column doesn't also help us in any way to possibly predict whether a tweet is talking about a **real disaster or not**."},{"metadata":{},"cell_type":"markdown","source":"# 3.3.1. Distribution Of Classes :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\nsns.barplot(x=train.target.value_counts().index, y=train.target.value_counts(), ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=test.target.value_counts().index, y=test.target.value_counts(), ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Distribution Of Classes\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that our **classes are not much imbalanced** in **Train Data** & we can conclude that in our **Train Data only there are two classes like {0,1} whereas in our Test Data only 0 value target tweet's are stored.**<br><br>\nWe will handle that as well a little later after **Data Pre-Processing part.**"},{"metadata":{},"cell_type":"markdown","source":"# 3.3.2. Top 10 Keywords From Both Data Tweet's :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\ncommon_keywords_train = train.keyword.value_counts()[:10]\ncommon_keywords_test = test.keyword.value_counts()[:10]\nsns.barplot(x=common_keywords_train, y=common_keywords_train.index, ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=common_keywords_test, y=common_keywords_test.index, ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Top 10 Keywords From Both Data Tweets\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above diagram's we can easily notice that there are **barely 2 to 3 keywords** which are **common** in both data-set's **Top 10 Keywords List**.<br><br>\nAnd these **Keywords** are even not playing any important role in predicting that whether a tweet is talking about a **real disaster or not**."},{"metadata":{},"cell_type":"markdown","source":"# 3.4. Data Pre-Processing :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.text[20])\nprint(\"=\"*100)\nprint(train.text[120])\nprint(\"=\"*100)\nprint(train.text[220])\nprint(\"=\"*100)\nprint(train.text[320])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As** we can see here all the tweet's of **Train Data** are in a need of some **Polishing(Pre-Processing)** because there are various **stopwords, http:// tags & various punchuations** which aren't required while **predicting** that whether a **tweet** is talking about a **real disaster or not**."},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"like\", \"via\", \"u\", \"video\", \"would\", \"one\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the **list of words** which are **most frequently** used in any **english paragraph** or in a **group of sentences**, and which **doesn't** play any important role in **creating a model to predict anything**.<br> **So**, we are simply naming them as **stopwords** and in the **following line of codes** I will be **removing these stopwords** from our **Data of Tweet's**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre_processing all the train data :-\nlemmatizer = WordNetLemmatizer()\ntrain.text = train.text.apply(lambda a: a.lower())\npreprocessed_train = []\nfor sentance in tqdm(train.text.values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join(lemmatizer.lemmatize(e) for e in sentance.split() if e not in stopwords)\n    preprocessed_train.append(sentance.strip())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above line of code ***Pre-Processed*** all the ***Train Data*** and ***stored*** it all in a list named ***preprocessed_train***."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(preprocessed_train[20])\nprint(\"=\"*100)\nprint(preprocessed_train[120])\nprint(\"=\"*100)\nprint(preprocessed_train[220])\nprint(\"=\"*100)\nprint(preprocessed_train[320])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see all our **Train Data** is **cleaned** and **Pre-Processed**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.text[20])\nprint(\"=\"*100)\nprint(test.text[120])\nprint(\"=\"*100)\nprint(test.text[220])\nprint(\"=\"*100)\nprint(test.text[320])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As** we can see here all the tweet's of **Test Data** are in a need of some **Polishing(Pre-Processing)** because there are various **stopwords, http:// tags & various punchuations** which aren't required while **predicting** that whether a **tweet** is talking about a **real disaster or not**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre_processing all the test data :-\ntest.text = test.text.apply(lambda a: a.lower())\npreprocessed_test = []\nfor sentance in tqdm(test.text.values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join(lemmatizer.lemmatize(e) for e in sentance.split() if e not in stopwords)\n    preprocessed_test.append(sentance.strip())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above line of code ***Pre-Processed*** all the ***Test Data*** and ***stored*** it all in a list named ***preprocessed_test***."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(preprocessed_test[20])\nprint(\"=\"*100)\nprint(preprocessed_test[120])\nprint(\"=\"*100)\nprint(preprocessed_test[220])\nprint(\"=\"*100)\nprint(preprocessed_test[320])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see all our **Test Data** is **cleaned** and **Pre-Processed**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting preprocessed_train List into a Series to join it back in Train Data : \nfinal_text_train = pd.Series(preprocessed_train)\nfinal_text_train.name = \"final_text\"\n\n# Joining Train Data with preprocessed_train Series & droping the old text(Tweet) column :\ntrain = pd.DataFrame.join(train, final_text_train)\ntrain = train.drop(\"text\", axis=1)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting preprocessed_test List into a Series to join it back in Test Data :\nfinal_text_test = pd.Series(preprocessed_test)\nfinal_text_test.name = \"final_text\"\n\n# Joining Test Data with preprocessed_test Series & droping the old text(Tweet) column :\ntest = pd.DataFrame.join(test, final_text_test)\ntest = test.drop(\"text\", axis=1)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.4.1. Creating Final Data-Frame (Inclusive Of Train & Test Both):-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.append(test, ignore_index=True)\n\n# Id & Keyword columns are not of any use for Creating Model or for any Prediction's :\ndf = df.drop([\"id\", \"keyword\"], axis=1)\n\nprint(df.info())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Now there are {} rows & {} columns in train.'.format(train.shape[0],train.shape[1]))\nprint('Now there are {} rows & {} columns in test.'.format(test.shape[0],test.shape[1]))\nprint('Now there are {} rows & {} columns in df(The Final Data-Frame).'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now** there are **7552 rows** & **4 columns** in **train**.<br>\n**Now** there are **3237 rows** & **4 columns** in **test**.<br><br>\n**And** there are **10789 rows** & **2 columns** in **df(The Final Data-Frame)**."},{"metadata":{},"cell_type":"markdown","source":"# 3.4.2. Top 15 Frequent Words In Tweet's :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\nfor sentences in tqdm(df.final_text.values):\n    sentences = \"\".join(sentences.lower())\n    words.append(sentences)\nwords = ''.join(word for word in words)\nwords = nltk.word_tokenize(words)\nwords = pd.Series(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code as we want to **plot a barplot** of some of the **most frequent words** in our whole **Data of Tweet's**.<br> **First** we will have to **join all the tweet's** with **lowering all the words and alphabet's** so that our system will easily find out the **words** which are **most frequently repeated**.<br> **Second**, we will have to **convert all the tweets** to **single-single words** to know the **frequency** of them, this can be done by using **nltk.word_tokenize()** and in the **last** we will convert all **single-single words** from a **list** to a **column** to apply a function called **value_counts()**."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.barplot(x=words.value_counts()[:15], y=words.value_counts()[:15].index)\nplt.title(\"Top 15 Frequent Words In Tweet's\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we can see here the **words** which are widely used in our **Data of Tweet's**."},{"metadata":{},"cell_type":"markdown","source":"# 4. Spliting Final Data-Frame Into Train, Cross-Validation,  Test :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Input & Output :\nX = df[\"final_text\"]\ny = df[\"target\"]\n\n# Spliting Final Data-Frame Into Train, Cross-Validation, Test :\nX_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I have used **train_test_split()** function to **break** my **Data of Tweet's** into **3 categories equally proportional to each class {0,1}** namely **:-**<br> {**Train** (For Training the Model), **Cross-Validation** (For cross checking the predictions) **&** **Test** (For final testing of our model)}."},{"metadata":{},"cell_type":"markdown","source":"# 4.1. Equal Distribution Of Classes :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(10,10))\nsns.barplot(x=df.target.value_counts().index, y=df.target.value_counts(), ax=axes[0,0]).set_title(\"Final Data-Frame\")\nsns.barplot(x=y_train.value_counts().index, y=y_train.value_counts(), ax=axes[0,1]).set_title(\"Y_Train\")\nsns.barplot(x=y_cv.value_counts().index, y=y_cv.value_counts(), ax=axes[1,1]).set_title(\"Y_CV\")\nsns.barplot(x=y_test.value_counts().index, y=y_test.value_counts(), ax=axes[1,0]).set_title(\"Y_Test\")\nplt.suptitle(\"Distribution Of Classes\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above all the **3 categories** are now having **equal proportion of {0,1} Classes.**<br><br>\nThis tells us that now our **Data** is all set to go for **Featurization** & then for **Creating Model by various Algorithms**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Length Of X_train :-\", X_train.shape[0])\nprint(\"Length Of y_train :-\", y_train.shape[0])\nprint(\"Length Of X_test :-\", X_test.shape[0])\nprint(\"Length Of y_test :-\", y_test.shape[0])\nprint(\"Length Of X_cv :-\", X_cv.shape[0])\nprint(\"Length Of y_cv :-\", y_cv.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Featurization & Applying Algorithms :- \n"},{"metadata":{},"cell_type":"markdown","source":"***Following are the parts of my Featurization & Applying Algorithms section :-***<br>\n# 5.1.\nIn this part I am going to convert all my **Tweet's text Data into Vectors** using **Bag-Of-Words** represented by **bow** and then will apply following **Algorithms :-**<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.1.** - **First** will apply **Logistic Regression** which is **represented** here by **log**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.2.** - **Second** will apply **Multi-Nomial Naive Bayes** which is **represented** here by **mnb**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.3.** - **Third** will apply **Decision-Tree Classifier** which is **represented** here by **dtc**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.4.** - **Fourth** there will be **Comaprison of Bag-Of-Words Test's** {To **find out** which **Algorithm** worked **best with Bag-Of-Words**}.\n\n# 5.2.\nIn this part I am going to convert all my **Tweet's text Data into Vectors** using **TF-IDF (Term Frequency - Inverse Document Frequency)** represented by **tfidf** and then will apply following **Algorithms :-**<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.1.** - **First** will apply **Logistic Regression** which is **represented** here by **log**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.2.** - **Second** will apply **Multi-Nomial Naive Bayes** which is **represented** here by **mnb**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.3.** - **Third** will apply **Decision-Tree Classifier** which is **represented** here by **dtc**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.4.** - **Fourth** there will be **Comaprison of TF-IDF Test's** {To **find out** which **Algorithm** worked **best with TF-IDF**}.\n\n# 5.3\nHere in **Final Comparison** I will compare all the **selected Algorithms** which i will get from **both Featurization techniques {Bag-Of-Words & TF-IDF}** by **ROC & AUC Curve's**.\n\n# 5.4\nNow comes the **Conclusion** part where I will create a **Conclusion Table** with the help of **prettytable library** and will show the **results** of all of the **Algorithms** to **easily compare**."},{"metadata":{},"cell_type":"markdown","source":"# 5.1. - Bag Of Words :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"bow = CountVectorizer(ngram_range=(1,2), min_df=2)\nX_train_bow = bow.fit_transform(X_train).toarray()\nX_cv_bow = bow.transform(X_cv).toarray()\nX_test_bow = bow.transform(X_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I have used **Bag-Of-Words** for **converting** all the **tweet's** from **text** to **vectors**. "},{"metadata":{},"cell_type":"markdown","source":"# 5.1.1. - Logisctic Regression :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"log = LogisticRegression()\nlog.fit(X_train_bow, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_bow_log = log.predict(X_cv_bow)\npre_test_bow_log = log.predict(X_test_bow)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **Bow & Logistic Regression** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BOW CV Classification Report by Logistic Regression\")\nprint(classification_report(y_cv, pre_cv_bow_log))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Logistic Regression\")\nprint(classification_report(y_test, pre_test_bow_log))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_bow_log = log.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_log = log.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_log = log.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_log_roc, tpr_cv_bow_log_roc, threshold_cv_bow_log_roc = roc_curve(y_cv, preproba_cv_bow_log)\nfpr_test_bow_log_roc, tpr_test_bow_log_roc, threshold_test_bow_log_roc = roc_curve(y_test, preproba_test_bow_log)\nfpr_train_bow_log_roc, tpr_train_bow_log_roc, threshold_train_bow_log_roc = roc_curve(y_train, preproba_train_bow_log)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\nax.plot(fpr_cv_bow_log_roc, tpr_cv_bow_log_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_log)*100))\nax.plot(fpr_train_bow_log_roc,tpr_train_bow_log_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_bow_log)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_bow_log_pr, recall_test_bow_log_pr, threshold_test_bow_log_pr = precision_recall_curve(y_test, preproba_test_bow_log)\nprecision_cv_bow_log_pr, recall_cv_bow_log_pr, threshold_cv_bow_log_pr = precision_recall_curve(y_cv, preproba_cv_bow_log)\nprecision_train_bow_log_pr, recall_train_bow_log_pr, threshold_train_bow_log_pr = precision_recall_curve(y_train, preproba_train_bow_log)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_log_pr, precision_test_bow_log_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_log_pr, precision_cv_bow_log_pr, label=\"CV\")\nax_1.plot(recall_train_bow_log_pr, precision_train_bow_log_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_bow_log = confusion_matrix(y_test, pre_test_bow_log)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_log, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1.2. - Naive Bayes {Multi-Nomial Naive Bayes}:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nmnb.fit(X_train_bow, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_bow_mnb = mnb.predict(X_cv_bow)\npre_test_bow_mnb = mnb.predict(X_test_bow)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **Bow & Multi-Nomial Naive Bayes** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BOW CV Classification Report by Multi-Nomial Naiye Bayes\")\nprint(classification_report(y_cv, pre_cv_bow_mnb))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Multi-Nomial Naiye Bayes\")\nprint(classification_report(y_test, pre_test_bow_mnb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_bow_mnb = mnb.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_mnb = mnb.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_mnb = mnb.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_mnb_roc, tpr_cv_bow_mnb_roc, threshold_cv_bow_mnb_roc = roc_curve(y_cv, preproba_cv_bow_mnb)\nfpr_test_bow_mnb_roc, tpr_test_bow_mnb_roc, threshold_test_bow_mnb_roc = roc_curve(y_test, preproba_test_bow_mnb)\nfpr_train_bow_mnb_roc, tpr_train_bow_mnb_roc, threshold_train_bow_mnb_roc = roc_curve(y_train, preproba_train_bow_mnb)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\nax.plot(fpr_cv_bow_mnb_roc, tpr_cv_bow_mnb_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_mnb)*100))\nax.plot(fpr_train_bow_mnb_roc, tpr_train_bow_mnb_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_bow_mnb)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_bow_mnb_pr, recall_test_bow_mnb_pr, threshold_test_bow_mnb_pr = precision_recall_curve(y_test, preproba_test_bow_mnb)\nprecision_cv_bow_mnb_pr, recall_cv_bow_mnb_pr, threshold_cv_bow_mnb_pr = precision_recall_curve(y_cv, preproba_cv_bow_mnb)\nprecision_train_bow_mnb_pr, recall_train_bow_mnb_pr, threshold_train_bow_mnb_pr = precision_recall_curve(y_train, preproba_train_bow_mnb)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_mnb_pr, precision_test_bow_mnb_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_mnb_pr, precision_cv_bow_mnb_pr, label=\"CV\")\nax_1.plot(recall_train_bow_mnb_pr, precision_train_bow_mnb_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_bow_mnb = confusion_matrix(y_test, pre_test_bow_mnb)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_mnb, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1.3. - Decision-Tree Classifier :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_bow, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_bow_dtc = dtc.predict(X_cv_bow)\npre_test_bow_dtc = dtc.predict(X_test_bow)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **Bow & Decision-Tree Classifier** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BOW CV Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_cv, pre_cv_bow_dtc))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_test, pre_test_bow_dtc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_bow_dtc = dtc.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_dtc = dtc.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_dtc = dtc.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_dtc_roc, tpr_cv_bow_dtc_roc, threshold_cv_bow_dtc_roc = roc_curve(y_cv, preproba_cv_bow_dtc)\nfpr_test_bow_dtc_roc, tpr_test_bow_dtc_roc, threshold_test_bow_dtc_roc = roc_curve(y_test, preproba_test_bow_dtc)\nfpr_train_bow_dtc_roc, tpr_train_bow_dtc_roc, threshold_train_bow_dtc_roc = roc_curve(y_train, preproba_train_bow_dtc)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_dtc_roc,tpr_test_bow_dtc_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_dtc)*100))\nax.plot(fpr_cv_bow_dtc_roc, tpr_cv_bow_dtc_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_dtc)*100))\nax.plot(fpr_train_bow_dtc_roc, tpr_train_bow_dtc_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_bow_dtc)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_bow_dtc_pr, recall_test_bow_dtc_pr, threshold_test_bow_dtc_pr = precision_recall_curve(y_test, preproba_test_bow_dtc)\nprecision_cv_bow_dtc_pr, recall_cv_bow_dtc_pr, threshold_cv_bow_dtc_pr = precision_recall_curve(y_cv, preproba_cv_bow_dtc)\nprecision_train_bow_dtc_pr, recall_train_bow_dtc_pr, threshold_train_bow_dtc_pr = precision_recall_curve(y_train, preproba_train_bow_dtc)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_dtc_pr, precision_test_bow_dtc_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_dtc_pr, precision_cv_bow_dtc_pr, label=\"CV\")\nax_1.plot(recall_train_bow_dtc_pr, precision_train_bow_dtc_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_bow_dtc = confusion_matrix(y_test, pre_test_bow_dtc)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_dtc, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1.4. - Comaprison of Bag Of Words Test's :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Logistic Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\nax.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\nax.plot(fpr_test_bow_dtc_roc,tpr_test_bow_dtc_roc, label='Decision-Tree Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_dtc)*100))\nplt.title('ROC Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in BOW', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_log_pr, precision_test_bow_log_pr, label=\"Logistic Test PR Curve\")\nax_1.plot(recall_test_bow_mnb_pr, precision_test_bow_mnb_pr, label=\"Multi-Nomial Test PR Curve\")\nax_1.plot(recall_test_bow_dtc_pr, precision_test_bow_dtc_pr, label=\"Decision-Tree Test PR Curve\")\nplt.title('PR Curve Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in BOW', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2. - TF-IDF :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\nX_train_tfidf = tfidf.fit_transform(X_train).toarray()\nX_cv_tfidf = tfidf.transform(X_cv).toarray()\nX_test_tfidf = tfidf.transform(X_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I have used **TF-IDF (Term Frequency - Inverse Term Frequency)** for **converting** all the **tweet's** from **text** to **vectors**. "},{"metadata":{},"cell_type":"markdown","source":"# 5.2.1. - Logisctic Regression :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"log = LogisticRegression()\nlog.fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_tfidf_log = log.predict(X_cv_tfidf)\npre_test_tfidf_log = log.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **TF-IDF & Logistic Regression** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TF-IDF CV Classification Report by Logistic Regresion\")\nprint(classification_report(y_cv, pre_cv_tfidf_log))\nprint(\"=\"*100)\nprint(\"TF-IDF Test Classification Report by Logistic Regresion\")\nprint(classification_report(y_test, pre_test_tfidf_log))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_tfidf_log = log.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_log = log.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_log = log.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_log_roc, tpr_cv_tfidf_log_roc, threshold_cv_tfidf_log_roc = roc_curve(y_cv, preproba_cv_tfidf_log)\nfpr_test_tfidf_log_roc, tpr_test_tfidf_log_roc, threshold_test_tfidf_log_roc = roc_curve(y_test, preproba_test_tfidf_log)\nfpr_train_tfidf_log_roc, tpr_train_tfidf_log_roc, threshold_train_tfidf_log_roc = roc_curve(y_train, preproba_train_tfidf_log)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax.plot(fpr_cv_tfidf_log_roc, tpr_cv_tfidf_log_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_log)*100))\nax.plot(fpr_train_tfidf_log_roc,tpr_train_tfidf_log_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_tfidf_log)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_tfidf_log_pr, recall_test_tfidf_log_pr, threshold_test_tfidf_log_pr = precision_recall_curve(y_test, preproba_test_tfidf_log)\nprecision_cv_tfidf_log_pr, recall_cv_tfidf_log_pr, threshold_cv_tfidf_log_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_log)\nprecision_train_tfidf_log_pr, recall_train_tfidf_log_pr, threshold_train_tfidf_log_pr = precision_recall_curve(y_train, preproba_train_tfidf_log)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_log_pr, precision_test_tfidf_log_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_log_pr, precision_cv_tfidf_log_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_log_pr, precision_train_tfidf_log_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_tfidf_log = confusion_matrix(y_test, pre_test_tfidf_log)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_log, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2.2. - Naive Bayes {Multi-Nomial Naive Bayes} :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nmnb.fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_tfidf_mnb = mnb.predict(X_cv_tfidf)\npre_test_tfidf_mnb = mnb.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **TF-IDF & Multi-Nomial Naive Bayes** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TF-IDF CV Classification Report by Multi-Nomial Naive Bayes\")\nprint(classification_report(y_cv, pre_cv_tfidf_mnb))\nprint(\"=\"*100)\nprint(\"TF-IDF Test Classification Report by Multi-Nomial Naive Bayes\")\nprint(classification_report(y_test, pre_test_tfidf_mnb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_tfidf_mnb = mnb.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_mnb = mnb.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_mnb = mnb.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_mnb_roc, tpr_cv_tfidf_mnb_roc, threshold_cv_tfidf_mnb_roc = roc_curve(y_cv, preproba_cv_tfidf_mnb)\nfpr_test_tfidf_mnb_roc, tpr_test_tfidf_mnb_roc, threshold_test_tfidf_mnb_roc = roc_curve(y_test, preproba_test_tfidf_mnb)\nfpr_train_tfidf_mnb_roc, tpr_train_tfidf_mnb_roc, threshold_train_tfidf_mnb_roc = roc_curve(y_train, preproba_train_tfidf_mnb)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax.plot(fpr_cv_tfidf_mnb_roc, tpr_cv_tfidf_mnb_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_mnb)*100))\nax.plot(fpr_train_tfidf_mnb_roc,tpr_train_tfidf_mnb_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_tfidf_mnb)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_tfidf_mnb_pr, recall_test_tfidf_mnb_pr, threshold_test_tfidf_mnb_pr = precision_recall_curve(y_test, preproba_test_tfidf_mnb)\nprecision_cv_tfidf_mnb_pr, recall_cv_tfidf_mnb_pr, threshold_cv_tfidf_mnb_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_mnb)\nprecision_train_tfidf_mnb_pr, recall_train_tfidf_mnb_pr, threshold_train_tfidf_mnb_pr = precision_recall_curve(y_train, preproba_train_tfidf_mnb)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_mnb_pr, precision_test_tfidf_mnb_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_mnb_pr, precision_cv_tfidf_mnb_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_mnb_pr, precision_train_tfidf_mnb_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_tfidf_mnb = confusion_matrix(y_test, pre_test_tfidf_mnb)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_mnb, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2.3. - Decision-Tree Classifier :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cv_tfidf_dtc = dtc.predict(X_cv_tfidf)\npre_test_tfidf_dtc = dtc.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After **training the model** with the help of **TF-IDF & Decision-Tree Classifier** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TF-IDF CV Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_cv, pre_cv_tfidf_dtc))\nprint(\"=\"*100)\nprint(\"TF-_IDF Test Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_test, pre_test_tfidf_dtc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"preproba_cv_tfidf_dtc = dtc.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_dtc = dtc.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_dtc = dtc.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_dtc_roc, tpr_cv_tfidf_dtc_roc, threshold_cv_tfidf_dtc_roc = roc_curve(y_cv, preproba_cv_tfidf_dtc)\nfpr_test_tfidf_dtc_roc, tpr_test_tfidf_dtc_roc, threshold_test_tfidf_dtc_roc = roc_curve(y_test, preproba_test_tfidf_dtc)\nfpr_train_tfidf_dtc_roc, tpr_train_tfidf_dtc_roc, threshold_train_tfidf_dtc_roc = roc_curve(y_train, preproba_train_tfidf_dtc)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_dtc_roc,tpr_test_tfidf_dtc_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100))\nax.plot(fpr_cv_tfidf_dtc_roc, tpr_cv_tfidf_dtc_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_dtc)*100))\nax.plot(fpr_train_tfidf_dtc_roc, tpr_train_tfidf_dtc_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_tfidf_dtc)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_test_tfidf_dtc_pr, recall_test_tfidf_dtc_pr, threshold_test_tfidf_dtc_pr = precision_recall_curve(y_test, preproba_test_tfidf_dtc)\nprecision_cv_tfidf_dtc_pr, recall_cv_tfidf_dtc_pr, threshold_cv_tfidf_dtc_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_dtc)\nprecision_train_tfidf_dtc_pr, recall_train_tfidf_dtc_pr, threshold_train_tfidf_dtc_pr = precision_recall_curve(y_train, preproba_train_tfidf_dtc)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_dtc_pr, precision_test_tfidf_dtc_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_dtc_pr, precision_cv_tfidf_dtc_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_dtc_pr, precision_train_tfidf_dtc_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_test_tfidf_dtc = confusion_matrix(y_test, pre_test_tfidf_dtc)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_dtc, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2.4. - Comparison of TF-IDF Test's :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Multi-Nomial Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax.plot(fpr_test_tfidf_dtc_roc,tpr_test_tfidf_dtc_roc, label='Decision-Tree Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100))\nplt.title('ROC Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in TF-IDF', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_log_pr, precision_test_tfidf_log_pr, label=\"Logistic Test PR Curve\")\nax_1.plot(recall_test_tfidf_mnb_pr, precision_test_tfidf_mnb_pr, label=\"Multi-Nomial Test PR Curve\")\nax_1.plot(recall_test_tfidf_dtc_pr, precision_test_tfidf_dtc_pr, label=\"Decision-Tree Test PR Curve\")\nplt.title('PR Curve Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in TF-IDF', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax_1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.3. - Final Comparison :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_2 = plt.figure(figsize=(15,15))\n\n# Ploting fig_2\nax_5 = plt.subplot(221)\nax_5.set_title(\"Comparing Logistic's\", fontsize=20)\nax_5.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax_5.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Logistic BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\n\nax_6 = plt.subplot(222)\nax_6.set_title(\"Comparing Multi-Nomial Naive Baye's\", fontsize=20)\nax_6.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Multi-Nomial TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax_6.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\n\nax_7 = plt.subplot(212)\nax_7.set_title(\"Finally Comparing Top ROC AUC Curve's\", fontsize=20)\nax_7.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax_7.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\n\n\nax_5.legend()\nax_6.legend()\nax_7.legend()\nplt.suptitle('Final ROC AUC Comparison Of Logistic & Multi-Nomial In BOW Vs TF-IDF', fontsize=25)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.4. - Conclusion :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = PrettyTable()\nx.field_names = [\"Vectorizer\", \"Model\", \"AUC (in %)\", \"Precision Score (in %)\", \"Recall Score (in %)\"]\nx.add_row([\"BOW\", \"Logistic Regression\", round(roc_auc_score(y_test,preproba_test_bow_log)*100), round(precision_score(y_test,pre_test_bow_log)*100), round(recall_score(y_test,pre_test_bow_log)*100)])\nx.add_row([\"BOW\", \"Multi-Nomial Naive Bayes\", round(roc_auc_score(y_test,preproba_test_bow_mnb)*100), round(precision_score(y_test,pre_test_bow_mnb)*100), round(recall_score(y_test,pre_test_bow_mnb)*100)])\nx.add_row([\"BOW\", \"Decision-Tree Classifier\", round(roc_auc_score(y_test,preproba_test_bow_dtc)*100), round(precision_score(y_test,pre_test_bow_dtc)*100), round(recall_score(y_test,pre_test_bow_dtc)*100)])\nx.add_row([\"TF-IDF\", \"Logistic Regression\", round(roc_auc_score(y_test,preproba_test_tfidf_log)*100), round(precision_score(y_test,pre_test_tfidf_log)*100), round(recall_score(y_test,pre_test_tfidf_log)*100)])\nx.add_row([\"TF-IDF\", \"Multi-Nomial Naive Bayes\", round(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100), round(precision_score(y_test,pre_test_tfidf_mnb)*100), round(recall_score(y_test,pre_test_tfidf_mnb)*100)])\nx.add_row([\"TF-IDF\", \"Decision-Tree Classifier\", round(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100), round(precision_score(y_test,pre_test_tfidf_dtc)*100), round(recall_score(y_test,pre_test_tfidf_dtc)*100)])\nprint(x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***So, by all these Comparison's and by this Conclusion Table we can select the Best Algorithm with Best Featurization Technique which suit's our priorities / requirements {Like :- Some gives more priority to ROC-AUC or some give priority to Precision-Recall}.***"},{"metadata":{},"cell_type":"markdown","source":"![title](http://pluspng.com/img-png/thanks-png-hd-images-simple-graphic-tnku0195-640.png \"Header\")"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}