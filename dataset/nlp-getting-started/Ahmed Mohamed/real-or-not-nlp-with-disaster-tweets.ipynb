{"cells":[{"metadata":{},"cell_type":"markdown","source":"### begore any thing you must read dataset carefully and then import libraires (from packages first import package) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords                         # to load stopwords and remove the stopwords from text\nfrom nltk import wordpunct_tokenize                       # Tokenize text \nfrom nltk.tokenize import RegexpTokenizer                 # Tokenize text by regular expression\nfrom nltk.stem.porter import PorterStemmer                # convert words to root words like cats to cat\nfrom nltk.stem.wordnet import WordNetLemmatizer           # the same portertstemmer\n# FOR CLASSIFIAR\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom torch.autograd import Variable             # for transfer inputs to autograde vaiable to requires grad\nfrom string import punctuation                  # load punctuation like (\">?/][]!`\") and remove then from text\nfrom gensim.models import Word2Vec             # to load word vectors to represnet any text to numerical numbers \n\nimport pandas as pd                            # to read file as dataframe\nimport numpy as np                             # Linear algebra library\nimport torch                                   # pytorch Framework \nimport torch.nn as nn                          # import neural netowrk from torch framework\nimport collections\nimport nltk\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text=text.lower()\n    stp=set(stopwords.words(\"english\"))\n    placesp = re.compile('[/(){}\\[\\]\\|@,;]')\n    removech= re.compile('[^0-9a-z #+_]')\n    st=WordNetLemmatizer()\n    text=re.sub(placesp,' ',text)\n    text=re.sub(removech,' ',text)\n    text=text.split()\n    text=[w for w in text if not w in stp]\n    text=[st.lemmatize(w) for w in text]\n    text=\" \".join(text)\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Glove File py passing url of file"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n Read Glove File take url of file return the two dictionaries ( word to index and word to vector in embedding )\n and one list of index to word  \n (glove file url) --> words_to_index, index_to_words, word_to_vec_map\n \n \"\"\"\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r',encoding='UTF-8') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWord Embeddings of words take dictionary of word to embedding and word to index\nand return Embeddings Matrix [index,Embedding] \n\n\"\"\"\n\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    vocab_len = len(word_to_index) + 1\n    emb_matrix = np.zeros((vocab_len,300))\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n    return emb_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTransfer sentence to indeces word in Embedding\ntake text and word to index dictionary \nreturn list of indeces word in Embedding\n\n\"\"\"\ndef transfer_sent(text,word_to_index):\n    text=text.split(' ')\n    ret=[]\n    for w in text:\n        if w in word_to_index and w !=\"\":\n            ret.append(word_to_index[w])\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncalculate the Max Length in every column in Data Frame \ntake Data Frame \nreturn Max lenght of columns\n\n\"\"\"\n\ndef retmax(dftrain):\n\n    lomax,temax,kemax=0,0,0\n    for i in range(dftrain.shape[0]):\n\n        temax=max(temax,len(np.array(dftrain.loc[i,'text'])))\n\n        kemax=max(kemax,len(np.array(dftrain.loc[i,'keyword'])))\n\n        lomax=max(lomax,len(np.array(dftrain.loc[i,'location'])))\n\n        return kemax,lomax,temax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nConvert Data Frame to Matrix 2D by Adding padding zeros to every columns that not have lenght not equal max\nlenght.\ntake Data Frame list of Max Lenghts of Columns\nreturn Matrix after convert\n\n\"\"\"\n\ndef convert2D(Xs,max_lens):\n    \n    X_indices = np.zeros((Xs[0].shape[0], sum(max_lens)))\n    pls=0\n    for i in range(Xs[0].shape[0]):\n        pls=0\n     \n        for j in range(0,len(Xs[0][i])):\n            X_indices[i][j+pls]=Xs[0][i][j]\n        pls=max_lens[0]\n\n        for j in range(0,len(Xs[1][i])):\n            X_indices[i][j+pls]=Xs[1][i][j]\n        pls=max_lens[1]+max_lens[0]\n\n        for j in range(0,len(Xs[2][i])):\n            X_indices[i][j+pls]=Xs[2][i][j]\n    return X_indices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Show files in inputs using ls"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls \"../input/glove6b300dtxt\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ndftest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ndftrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Eexploration data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of data\",dftrain.shape)\ndftrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of NAN value in keyword\",dftrain.keyword.isnull().sum())\nprint(\"Number of NAN value in location\",dftrain.location.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain.head()\ndftrain.keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing id column ... to delete row use axis=0 column use axis=1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain=dftrain.drop(['id'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word representation using glove text 300dim"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nCall read_glove_vecs function and then call pretrained_embedding_layer to calc word Embedding of Words\n\n\"\"\"\n\nword_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"../input/glove6b300dtxt/glove.6B.300d.txt\")\nword_embedding=pretrained_embedding_layer(word_to_vec_map, word_to_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perprocessing Data \n\n1. Remove NAN value from data\n2. Remove punctuation like .,/][]';! from Data\n3. tokenize data by space the most tokenization using in English text"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain=dftrain.dropna(axis=0)\nlabels=dftrain.target\ndftrain=dftrain.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kews=dftest.keyword.values\nkews=list(set(kews))\nlocs=dftest.location.values\nlocs=list(set(locs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TO CLEAN data using Function clean \nfor i in range(dftrain.shape[0]):\n    dftrain.at[i,'keyword']=transfer_sent(clean(dftrain.loc[i,'keyword']),word_to_index)\n    dftrain.at[i,'location']=transfer_sent(clean(dftrain.loc[i,'location']),word_to_index)\n    dftrain.at[i,'text']=transfer_sent(clean(dftrain.loc[i,'text']),word_to_index)\n    \nfor i in range(dftest.shape[0]):\n    if type(dftest.loc[0,'keyword'])==float:\n        dftest.at[0,'keyword']=kews[np.random.randint(1,221)]\n    if type(dftest.loc[0,'location'])==float:\n        dftest.at[0,'location']=locs[np.random.randint(1,1602)]\n        \n    \n    dftest.at[i,'keyword']=transfer_sent(clean(str(dftest.loc[i,'keyword'])),word_to_index)\n    dftest.at[i,'location']=transfer_sent(clean(str(dftest.loc[i,'location'])),word_to_index)\n    dftest.at[i,'text']=transfer_sent(clean(dftest.loc[i,'text']),word_to_index)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kemax,lomax,temax=retmax(dftrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Max len of text\",temax)\nprint(\"Max len of location\",lomax)\nprint(\"Max len of keyword\",kemax)\n\ndftrain=np.array(convert2D([dftrain.keyword,dftrain.location,dftrain.text],[0,0,25]),dtype=np.int64)\ndftest=np.array(convert2D([dftest.keyword,dftest.location,dftest.text],[0,0,25]),dtype=np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OneVsRestClassifier(LogisticRegression(penalty='l2', C=1.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(dftrain,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(dftest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target']=y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}