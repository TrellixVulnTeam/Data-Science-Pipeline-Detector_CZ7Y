{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Hello everyone , \n In this kernel we will go together into  ****Visualization**** Disaster Tweets data to learn how words related together using t-SNE natural language processing NLP techniques.\n \n** Before any thing , What is T-SNE ?\n** \n\n T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n \n [To read more](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)"},{"metadata":{},"cell_type":"markdown","source":"#### This kernel will be devided into the following parts\n\n1. Data Exploration\n2. Data Preprocessing\n3. Data Vizualization "},{"metadata":{},"cell_type":"markdown","source":"Load libraries : "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport re\nimport nltk\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\nSTOP_WORDS = nltk.corpus.stopwords.words()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! ls \"../input/nlp-getting-started\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Data Exploration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsubmission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Data Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"* clean nan values\n* delete columns not help in training or viusalize like id,location and keyword \n* convert letters in text to lowercase \n* remove numbers and symobls \n*  take toknize of text \n* Create Corpus of Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', text).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.dropna(axis=0)\ntrain=train.reset_index()\nfor i in range (train.shape[0]):\n    train.at[i,'text']=clean(train.loc[i,'text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Corpus "},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\n\nfor i in range(train.shape[0]):\n    corpus.append(train['text'][i].split(\" \"))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corpus[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word 2 Vec\n\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space\n\nin this example have 100 dims"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, size=100, window=10, min_count=35, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv['news']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drawing(model):\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drawing(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can Using Word2Vec to get Simialr words "},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('news')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## congratulation \n> ## Happy End"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}