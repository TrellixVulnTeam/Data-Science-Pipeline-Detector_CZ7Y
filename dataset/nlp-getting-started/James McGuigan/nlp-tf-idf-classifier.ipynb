{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP TF-IDF Classifier\n\n[Disaster Tweets Dataset](https://www.kaggle.com/c/nlp-getting-started)\n\n> Term Frequency – Inverse Document Drequency (TF-IDF) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n\nTF-IDF scores are computed for each word to discover which words are most correlated with each label, and used to create a solver that guesses tweet sentiment based on relative TF-IDF scores.\n\nThe first stage uses TF-IDF to make a prediction, which we can then use to generate a new data field \"accuracy\". This allows us to recompute TF-IDF using \"accuracy\" (to split between true and false positive) rather than on the \"target\" label.\n\nTF-IDF is normally done with many documents within a large corpus. Here I am treating the labels as two giant documents so the \"document\" is almost as large as the corpus. With small documents, stopwords will cancel out, however due to the large document size, stopwords have high values in both categories.\n\n\nBy taking the ratio, we are looking for words with a high TF-IDF value in one category, but a low value in the other category. Stopwords now cancel out at close to 1. Using a ratio seemed natural given that we only have 2 documents.\n\nWords that appear mostly in one category but not the other will have a much higher category. Note how \"Hiroshima\" suddenly jumps to the top of the list, compared with \":\". This is a highly relevant keyword that rarely gets seen in the other category.\n\n> By adding the true_positive_ratio_scaled to disaster_ratio, what you’re saying is “increase the importance of these words, by this much since they are a  good indicator of disaster tweets” and by adding false_positive_ratio_scaled to non_disaster_ratio, what you are saying is “these words are usually present in tweets that are actually NOT disasters but are falsely classified as disasters. So increase their non_disaster_ratios so that they are more likely to be treated as indicators for non-disaster tweets” - Pawan Bhandarkar"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy  as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport pydash\nimport math\nimport os\nfrom pydash import flatten\nfrom collections import Counter, OrderedDict\nfrom humanize import intcomma\nfrom operator import itemgetter\nfrom typing import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CSV Data"},{"metadata":{},"cell_type":"markdown","source":"First we read our CSV into a pandas dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv', index_col=0)\ndf_test  = pd.read_csv('../input/nlp-getting-started/test.csv', index_col=0)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization\n\nTokeniztion is the process of splitting text strings into a word array. There are a range of different algorithms, each with slightly different rules for how to define word boundries. For now we will just use [nltk.tokenize.casual.TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_df( dfs: List[pd.DataFrame] ) -> List[str]:\n    # TweetTokenizer defaults produce the best result\n    # Lowercasing, stripping handles and stemming reduce submission accuracy to 0.72387 \n    tokenizer = nltk.TweetTokenizer(preserve_case=True,  reduce_len=False, strip_handles=False)  # defaults \n    # tokenizer = nltk.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True) \n    tokens = flatten([\n        tokenizer.tokenize(tweet_text)    \n        for df in flatten([ dfs ])\n        for tweet_text in df['text']\n    ])\n    \n    # stemmer   = nltk.PorterStemmer()\n    # tokens = [ stemmer.stem(token) for token in tokens ]\n    return tokens\n\ntokens_all          = tokenize_df([ df_train, df_test ])\ntokens_train        = tokenize_df( df_train )\ntokens_test         = tokenize_df( df_test )\ntokens_disaster     = tokenize_df( df_train[ df_train['target'] == 1 ] )\ntokens_not_disaster = tokenize_df( df_train[ df_train['target'] == 0 ] )\ntokens_shared       = set(tokens_train) & set(tokens_test) & set(tokens_disaster) & set(tokens_not_disaster)\n\nprint('Unique Tokens:');\nprint('  tokens_all          ', intcomma(len(set(tokens_all))))\nprint('  tokens_train        ', intcomma(len(set(tokens_train))))\nprint('  tokens_test         ', intcomma(len(set(tokens_test))))\nprint('  tokens_disaster     ', intcomma(len(set(tokens_disaster))))\nprint('  tokens_not_disaster ', intcomma(len(set(tokens_not_disaster))))\nprint('  tokens_shared       ', intcomma(len(set(tokens_shared))))\nprint()\nprint('New Tokens:');\nprint(f'  tokens_test         - tokens_train        {intcomma(len(set(tokens_test) - set(tokens_train)))           :>6s} ({len(set(tokens_test) - set(tokens_train))/len(set(tokens_test))*100:.1f}%)')\nprint(f'  tokens_train        - tokens_test         {intcomma(len(set(tokens_train) - set(tokens_test)))           :>6s} ({len(set(tokens_train) - set(tokens_test))/len(set(tokens_train))*100:.1f}%)')\nprint(f'  tokens_disaster     - tokens_not_disaster {intcomma(len(set(tokens_disaster) - set(tokens_not_disaster))):>6s} ({len(set(tokens_disaster) - set(tokens_not_disaster))/len(set(tokens_disaster))*100:.1f}%)')\nprint(f'  tokens_not_disaster - tokens_disaster     {intcomma(len(set(tokens_not_disaster) - set(tokens_disaster))):>6s} ({len(set(tokens_not_disaster) - set(tokens_disaster))/len(set(tokens_not_disaster))*100:.1f}%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total vocabulary of 35k word tokens. \n\nHalf the word tokens that appear in the test dataset are not found in the training dataset, and three quarters of words in the training dataset are not see again the test dataset. \n\nSimilar 66-73% figures are found when comparing the split between disaster and safe tweets, suggesting that only a third of vocabulary is shared between dataset splits."},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF \n\nTerm Frequency Inverse Document Frequency shows the relative statistical importance of a word in a document relative to how many times it appears in all text.\n\nWe are treating all tweets with a given label as a single document, and comparing the freqency of words between labels. As we are comparing two large \"documents\" stopwords and punctuation get high scores. These will be eliminated below by comparing the relative TF-IDF between labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def term_frequency( tokens: List[str] ) -> Counter:\n    tf = {\n        token: count / len(tokens)\n        for token, count in Counter(tokens).items()\n    }\n    tf = Counter(dict(Counter(tf).most_common()))  # sort and cast\n    return tf\n    \n    \ndef inverse_document_frequency( tokens: List[str] ) -> Counter:\n    idf = {\n        token: math.log( len(tokens) / count ) \n        for token, count in Counter(tokens).items()\n    }\n    idf = Counter(dict(Counter(idf).most_common()))  # sort and cast\n    return idf\n\n\ndef tf_idf( document_tokens: List[str], all_tokens: List[str] ) -> Counter:\n    tf  = term_frequency(document_tokens)\n    idf = inverse_document_frequency(all_tokens)\n    tf_idf = {\n        token: tf[token] * idf[token]\n        for token in set(document_tokens)\n    }    \n    tf_idf = Counter(dict(Counter(tf_idf).most_common()))  # sort and cast\n    return tf_idf\n\n\ntf_disaster         = term_frequency(tokens_disaster)\ntf_not_disaster     = term_frequency(tokens_not_disaster)\nidf                 = inverse_document_frequency(tokens_all)\ntf_idf_disaster     = tf_idf(tokens_disaster, tokens_all)\ntf_idf_not_disaster = tf_idf(tokens_not_disaster, tokens_all)\n\ndisplay('tf_idf_disaster')\ndisplay(tf_idf_disaster.most_common(5))\n\ndisplay('tf_idf_not_disaster')\ndisplay(tf_idf_not_disaster.most_common(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display('tf_idf_disaster')\ndisplay(tf_idf_disaster.most_common(5))\n\ndisplay('tf_idf_not_disaster')\ndisplay(tf_idf_not_disaster.most_common(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However by comparing the ratio of TF-IDF scores between datasets, we can filter out the stopwords (which have high scores in both datasets) and return a list of \nkeywords that are most indicative of specific label"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tf_idf_ratio( tf_idf_true: Counter, tf_idf_false: Counter ) -> Counter:\n    tf_idf_false_tokens = set(tf_idf_false.keys()) \n    tf_idf_ratio = {\n        token: tf_idf_true[token] / tf_idf_false[token]\n        for token in tf_idf_true.keys()\n        if  token in tf_idf_false_tokens        \n    }\n    tf_idf_ratio = Counter(dict(Counter(tf_idf_ratio).most_common()))  # sort and cast\n    return tf_idf_ratio\n\n\ntf_idf_ratio_disaster     = tf_idf_ratio(tf_disaster, tf_not_disaster)\ntf_idf_ratio_not_disaster = tf_idf_ratio(tf_not_disaster, tf_disaster)\n\n\ndisplay('tf_idf_ratio_disaster')\ndisplay(tf_idf_ratio_disaster.most_common(30))\ndisplay('tf_idf_ratio_not_disaster')\ndisplay(tf_idf_ratio_not_disaster.most_common(30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solver\n\nThe solver here is fairly simple.\n\nWe tokenize the tweet and compare each word against the TF-IDF scores for each label. The label with the highest score wins.\n\nDespite this being a very simple method, we get a respectable 81% accuracy on the training dataset and 77.4% accuracy on the submission test dataset.\n\nApplying math.log() to the TF-IDF scores helps reduce overfitting, resulting in a +0.49% improvement on submission test_df scores but with a -0.37% reduction in training accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tf_idf_classifer_score(tweet_text: str, tf_idf_ratio_disaster, tf_idf_ratio_not_disaster) -> float:\n    score  = 0.0\n    tokens = nltk.TweetTokenizer().tokenize(tweet_text)\n    for token in tokens:\n        if token in tokens_shared:\n            # math.log() improves test_df score from 0.76892 -> 0.77382 (+0.49%)\n            #            reduces train_df score from 0.80861 -> 0.81229 (-0.37%)\n            # score += tf_idf_ratio_disaster.get(token, 0)     \n            # score -= tf_idf_ratio_not_disaster.get(token, 0) \n            score += math.log( tf_idf_ratio_disaster.get(token, 1)     )  # log(1) == 0\n            score -= math.log( tf_idf_ratio_not_disaster.get(token, 1) )\n    return score\n\n\ndef tf_idf_classifer(tweet_text: str, tf_idf_ratio_disaster, tf_idf_ratio_not_disaster) -> int:\n    score = tf_idf_classifer_score(tweet_text, tf_idf_ratio_disaster, tf_idf_ratio_not_disaster)\n    label = 1 if score > 0 else 0  # NOTE: label == 0 if score == 0 \n    return label\n\n\ndef tf_idf_classifer_df(df: pd.DataFrame) -> np.ndarray:\n    return np.array([\n        tf_idf_classifer( row['text'], tf_idf_ratio_disaster, tf_idf_ratio_not_disaster )\n        for index, row in df.iterrows()    \n    ])\n\ndef tf_idf_classifer_accuracy(df: pd.DataFrame, tf_idf_ratio_disaster, tf_idf_ratio_not_disaster) -> float:\n    correct = 0\n    total   = 0\n    for index, row in df.iterrows():\n        label = tf_idf_classifer( row['text'], tf_idf_ratio_disaster, tf_idf_ratio_not_disaster )\n        if label == row['target']:\n            correct += 1\n        total += 1\n    accuracy = correct / total\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = tf_idf_classifer_accuracy(df_train, tf_idf_ratio_disaster, tf_idf_ratio_not_disaster)\nprint('accuracy =', accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# False Positive TF-IDF\n\nWe can repeat this process and use TF-IDF to find the words likely to indicate a false positive match.\n\nThis improves train accuracy by +1.15%"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions           = tf_idf_classifer_df(df_train)\ntokens_true_positive  = tokenize_df( df_train[ df_train['target'] == predictions ] )\ntokens_false_positive = tokenize_df( df_train[ df_train['target'] != predictions ] )\n\ntf_idf_true_positive  = tf_idf( tokens_true_positive,  tokens_all )\ntf_idf_false_positive = tf_idf( tokens_false_positive, tokens_all )\n\ntf_idf_true_positive_ratio  = tf_idf_ratio(tf_idf_true_positive,  tf_idf_false_positive)\ntf_idf_false_positive_ratio = tf_idf_ratio(tf_idf_false_positive, tf_idf_true_positive)\n\ndisplay('tf_idf_true_positive_ratio')\ndisplay(tf_idf_true_positive_ratio.most_common(30))\ndisplay('tf_idf_false_positive_ratio')\ndisplay(tf_idf_false_positive_ratio.most_common(30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we perform a hyper-parameter search to find the best ratios for combining this new metadata with our solver."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ratio_hyperparameter_search():\n    results = {}\n    for scale1 in     [ 0.5, 1, 1.5, 2, 2.5, 3, 4, 8, 16 ]:\n        for scale2 in [ 0.5, 1, 1.5, 2, 2.5, 3, 4, 8, 16 ]:   \n            tf_idf_true_positive_ratio_scaled  = Counter({ token: count/scale1 for token, count in tf_idf_true_positive_ratio.items()  }) \n            tf_idf_false_positive_ratio_scaled = Counter({ token: count/scale2 for token, count in tf_idf_false_positive_ratio.items() }) \n\n            accuracy = tf_idf_classifer_accuracy(\n                df_train, \n                tf_idf_ratio_disaster     + tf_idf_true_positive_ratio_scaled, \n                tf_idf_ratio_not_disaster + tf_idf_false_positive_ratio_scaled,\n            )\n            results[(scale1, scale2)] = accuracy \n    display(Counter(results).most_common(10))\n  \n\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Batch':\n    ratio_hyperparameter_search()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hardcode optimal values\ntf_idf_true_positive_ratio_scaled  = Counter({ token: count/3 for token, count in tf_idf_true_positive_ratio.items()  }) \ntf_idf_false_positive_ratio_scaled = Counter({ token: count/2 for token, count in tf_idf_false_positive_ratio.items() }) \naccuracy = tf_idf_classifer_accuracy(\n    df_train, \n    tf_idf_ratio_disaster     + tf_idf_true_positive_ratio_scaled, \n    tf_idf_ratio_not_disaster + tf_idf_false_positive_ratio_scaled,\n)\nprint('accuracy =', accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv', index_col=0)\nfor index, row in df_test.iterrows():\n    label = tf_idf_classifer( \n        row['text'], \n        tf_idf_ratio_disaster     + tf_idf_true_positive_ratio_scaled, \n        tf_idf_ratio_not_disaster + tf_idf_false_positive_ratio_scaled,\n    )\n    df_submission.loc[index] = label \ndf_submission.to_csv('submission.csv')\ndf_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.74164 - [NLP Logistic Regression](https://www.kaggle.com/jamesmcguigan/disaster-tweets-logistic-regression/)\n- 0.77536 - [NLP TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier)\n- 0.79742 - [NLP Naive Bayes](https://www.kaggle.com/jamesmcguigan/nlp-naive-bayes)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}