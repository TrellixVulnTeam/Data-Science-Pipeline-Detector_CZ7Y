{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Naive Bayes\n\n[Disaster Tweets Dataset](https://www.kaggle.com/c/nlp-getting-started)\n\nNaive Bayes computes the probability of each word appearing under each label, based on token frequency, the sums the log Likelihood of all words in a tweet added to the log prior (to account for an unbalanced dataset)."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy  as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport pydash\nimport math\nimport os\nimport time\nfrom pydash import flatten\nfrom collections import Counter, OrderedDict\nfrom humanize import intcomma\nfrom operator import itemgetter\nfrom typing import *\nfrom sklearn.model_selection import train_test_split\n\n# import spacy\n# nlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CSV Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv', index_col=0)\ndf_test  = pd.read_csv('../input/nlp-getting-started/test.csv', index_col=0)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization and Word Frequencies\n\nHere we tokenize the text using nltk.TweetTokenizer, apply lowercasing, tweet preprocessing, and stemming.\n\nThen compute a dictionary lookup of word counts for each label"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_df(\n    dfs: List[pd.DataFrame], \n    keys          = ('text', 'keyword', 'location'), \n    stemmer       = False, \n    ngrams        = 1,\n    preserve_case = True, \n    reduce_len    = False, \n    strip_handles = True,\n    use_stopwords = True,\n    **kwargs,\n) -> List[List[str]]:\n    # tokenizer = nltk.TweetTokenizer(preserve_case=True,  reduce_len=False, strip_handles=False)  # defaults \n    tokenizer = nltk.TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len, strip_handles=strip_handles) \n    porter    = nltk.PorterStemmer()\n    stopwords = set(nltk.corpus.stopwords.words('english') + [ 'nan' ])\n\n    output    = []\n    for df in flatten([ dfs ]):\n        for index, row in df.iterrows():\n            tokens = flatten([\n                tokenizer.tokenize(str(row[key] or \"\"))\n                for key in keys    \n            ])\n            if use_stopwords:\n                tokens = [ \n                    token \n                    for token in tokens \n                    if token.lower() not in stopwords\n                    and len(token) >= 2\n                ]        \n            if stemmer:\n                tokens = [ \n                    porter.stem(token) \n                    for token in tokens \n                ]\n            if ngrams:\n                tokens = [\n                    \" \".join(tokens[i:i+n])\n                    for n in range(1,ngrams+1)\n                    for i in range(0,len(tokens)-n+1)\n                ]            \n            output.append(tokens)\n\n    return output\n\n\ndef get_labeled_tokens(df, **kwargs) -> Dict[int, List[str]]:\n    tokens = {\n        0: flatten(tokenize_df( df[df['target'] == 0], **kwargs )),\n        1: flatten(tokenize_df( df[df['target'] == 1], **kwargs )),\n    }\n    return tokens\n\n\ndef get_word_frequencies(df, **kwargs) -> Dict[int, Counter]:\n    tokens = get_labeled_tokens(df, **kwargs)\n    freqs = { \n        0: Counter(dict(Counter(tokens[0]).most_common())), \n        1: Counter(dict(Counter(tokens[1]).most_common())), \n    }  # sort and cast\n    return freqs\n\n\ndef get_log_likelihood(df, vocab_df, **kwargs):\n    vocab  = set(flatten(tokenize_df(vocab_df, **kwargs)))\n    tokens = tokenize_df( df, **kwargs )\n    freqs  = get_word_frequencies(df, **kwargs)\n    log_likelihood = {}\n    for token in vocab:\n        # Implement Laplacian Smoothing\n        p_false = (freqs[0].get(token, 0) + 1) / ( len(tokens[0]) + len(vocab) )  # [0] == False \n        p_true  = (freqs[1].get(token, 0) + 1) / ( len(tokens[1]) + len(vocab) )  # [1] == True\n        log_likelihood[token] = np.log( p_true / p_false )\n    return log_likelihood\n    \n    \ndef get_logprior(df, **kwargs):\n    \"\"\" Log probability of a word being positive given imbalanced data \"\"\"\n    tokens = tokenize_df( df, **kwargs )\n    return np.log( len(tokens[0]) / len(tokens[1]) ) if len(tokens[1]) else 0   \n    return np.log( len(tokens[0]) / len(tokens[1]) ) if len(tokens[1]) else 0   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logprior\n\n`exp()` undoes `log()` and `** -1` inverts the ratio. \n\nThis shows we have a nearly balanced dataset with about 15% more tokens in the disaster category"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_logprior():\n    tokens   = get_labeled_tokens(df_train)\n    logprior = get_logprior(df_train)\n\n    print('len(tokens[0])                    =', len(tokens[0]))\n    print('len(tokens[1])                    =', len(tokens[1]))\n    print('logprior(df_test)                 =', logprior)\n    print('math.exp(logprior(df_test))       =', math.exp(logprior))\n    print('math.exp(logprior(df_test)) ** -1 =', math.exp(logprior)**-1)\n    \nprint_logprior()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Solver\n\nSimply sum up the negative log likelihood and logprior for each word in each tweet and check the number is positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_bayes_classifier( df_train, df_test, **kwargs ) -> np.array:\n    vocab_df       = [ df_train, df_test ]\n    log_likelihood = get_log_likelihood( df_train, vocab_df, **kwargs )    \n    logprior       = get_logprior(df_train, **kwargs)\n    \n    predictions = []\n    for tweet_tokens in tokenize_df(df_test, **kwargs):\n        log_prob = np.sum([ \n            log_likelihood.get(token, 0)\n            for token in tweet_tokens\n        ]) + logprior\n        prediction = int(log_prob > 0)\n        predictions.append(prediction)\n    \n    return np.array(predictions)            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_accuracy(splits=3, **kwargs):\n    time_start  = time.perf_counter()\n\n    accuracy = 0\n    for _ in range(splits):\n        train, test = train_test_split(df_train, test_size=1/splits)      \n        predictions = naive_bayes_classifier(train, test, **kwargs)\n        accuracy   += np.sum( test['target'] == predictions ) / len(predictions) / splits\n        \n    time_taken  = time.perf_counter() - time_start\n    time_taken /= splits\n    print(f'ngrams = {ngrams} | accuracy = {accuracy*100:.2f}% | time = {time_taken:.1f}s')\n    \nfor ngrams in [1,2,3,4,5]:\n    test_accuracy( splits=3, ngrams=ngrams )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"kwargs = { \"ngrams\": 3 }\ndf_submission = pd.DataFrame({\n    \"id\":     df_test.index,\n    \"target\": naive_bayes_classifier(df_train, df_test, **kwargs)\n})\ndf_submission.to_csv('submission.csv', index=False)\n! head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.77536 - [NLP TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier)\n- 0.74164 - [NLP Logistic Regression](https://www.kaggle.com/jamesmcguigan/disaster-tweets-logistic-regression/)\n- 0.79773 - [NLP Naive Bayes](https://www.kaggle.com/jamesmcguigan/nlp-naive-bayes)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}