{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP - LASER Embeddings + Keras\n\nThis approach encodes the tweets using [LASER](https://github.com/yannvgn/laserembeddings) multilingual sentence embeddings,\nfollowed by a [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) dense neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q laserembeddings laserembeddings[zh] laserembeddings[ja]\n!pip install -q ftfy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import fasttext\nimport ftfy\nimport html\nimport laserembeddings\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport sys\n\nfrom fastcache import clru_cache\nfrom laserembeddings import Laser\nfrom typing import List, Union\nfrom urllib.parse import unquote\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv', index_col=0).fillna('')\ndf_test  = pd.read_csv('../input/nlp-getting-started/test.csv',  index_col=0).fillna('')\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess Text\n\nKeyword, location and text fields into a single string. \n\nSimple preprocessing is performed to remove HTML and encoded elements, @usernames, hashtag prefixes and urls."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(df):\n    texts = df[['keyword', 'location', 'text']].agg(' '.join, axis=1)\n    texts = texts.apply(ftfy.fix_text)   # fix \\x89\n    texts = texts.apply(html.unescape)  \n    texts = texts.apply(unquote)         # remove %20\n    texts = texts.apply(lambda s: re.sub('@\\w+', ' ', s))            # remove @usernames\n    texts = texts.apply(lambda s: re.sub('#',    ' ', s))            # remove hashtag prefixes    \n    texts = texts.apply(lambda s: re.sub('\\n',   ' ', s))            # remove newlines\n    texts = texts.apply(lambda s: re.sub('\\w+://\\S+',  '<URL>', s))  # remove urls    \n    texts = texts.apply(lambda s: re.sub('\\s+',  ' ', s))            # remove multiple spaces    \n    return list(texts)\n    \npreprocess_text(df_train)[:10]\npreprocess_text(df_test)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LASER Embeddings\n\nThis encodes each of the strings as a LASER embedding (1024 dimentional vector)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\n# DOCS: https://github.com/facebookresearch/LASER/blob/master/install_models.sh\n\nmkdir -p models/laser/\n# for FILE in bilstm.eparl21.2018-11-19.pt eparl21.fcodes eparl21.fvocab bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\nfor FILE in bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n    wget -cq https://dl.fbaipublicfiles.com/laser/models/$FILE -O models/laser/$FILE\ndone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from config import config\n# from src.utils.fasttest_model import language_detect\n# from src.utils.punkt_tokenizer import punkt_tokenize_sentences\n\nconfig = {\n    \"laser\": {\n        \"base_dir\":  \"./models/laser\",\n        \"bpe_codes\": \"./models/laser/93langs.fcodes\",\n        \"bpe_vocab\": \"./models/laser/93langs.fvocab\",\n        \"encoder\":   \"./models/laser/bilstm.93langs.2018-12-26.pt\",\n    }\n}\n\n# Instantiate encoder\n# BUG: CUDA GPU memory is exceeded if both laser and labse are loaded together \n# @clru_cache(None)\ndef get_laser_model():\n    laser_model = Laser(\n        bpe_codes = config['laser']['bpe_codes'],\n        bpe_vocab = config['laser']['bpe_vocab'],\n        encoder   = config['laser']['encoder'],\n        tokenizer_options = None,\n        embedding_options = None\n    )\n    return laser_model\n\n\ndef laser_encode(text: Union[str, List[str]], lang='en', normalize=True) -> np.ndarray:\n    \"\"\"\n    Encodes a corpus of text using LASER\n    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences\n    :param lang: 2 digit language code (optional autodetect)\n    :return:     embedding matrix\n    \"\"\"\n    laser_model = get_laser_model()\n    \n    # lang = lang or language_detect(text, threshold=0.0)\n    if isinstance(text, str):\n        # sentences = punkt_tokenize_sentences(text, lang=lang)\n        sentences = [ text ]\n    else:\n        sentences = list(text)\n\n    embedding = laser_model.embed_sentences(sentences, lang=lang)\n    \n    if normalize:\n        embedding = embedding / np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)\n        \n    return embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nX_train = laser_encode(preprocess_text(df_train))\nY_train = df_train['target']\n\nprint('X_train.shape', X_train.shape)\nprint('Y_train.shape', Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network - TF Keras\n\nDefine and train a dense neural network. \n\nThis inputs a 1024 LASER embedding and outputs a 1 bit classification prediction.\n\nA triangular shaped architecture is used, including Dropout and BatchNorm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DOCS: https://keras.io/examples/keras_recipes/antirectifier/\n\n# Build the model\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(1024,)),\n    tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),\n])\n\n\ndef model_compile_fit(model, X, Y):\n    model.summary()\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n        \n    # Compile the model\n    model.compile(\n        loss      = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n        # optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001),\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n        metrics   = [ tf.keras.metrics.BinaryAccuracy() ],\n    )\n    \n    # Train the model\n    model.fit(\n        X_train, Y_train, \n        batch_size = 32, \n        epochs     = 1000, \n        # validation_split = 0.2,\n        callbacks = [\n            # tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10),\n            # tf.keras.callbacks.ModelCheckpoint('model.h5',  monitor='binary_accuracy', mode='max', verbose=0, save_best_only=True)\n        ],\n        verbose=2\n    )\n    model.save('model.h5')\n    \n    print()\n    print('Train Accuracy')\n    model.evaluate(X_train, Y_train)\n\n    print('Test Accuracy')\n    model.evaluate(X_test, Y_test)\n\n    \nmodel_compile_fit(model, X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nX_test = laser_encode(preprocess_text(df_test))\nY_test = tf.math.round( model.predict(X_test) ).numpy().astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv', index_col=0).fillna('')\ndf_submission['target'] = Y_test\ndf_submission.to_csv('submission.csv')\n!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.74164 - [NLP Logistic Regression](https://www.kaggle.com/jamesmcguigan/disaster-tweets-logistic-regression)\n- 0.76677 - [NLP LASER Embeddings + Keras](https://www.kaggle.com/jamesmcguigan/nlp-laser-embeddings-keras)\n- 0.77536 - [NLP TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier)\n- 0.79742 - [NLP Naive Bayes](https://www.kaggle.com/jamesmcguigan/nlp-naive-bayes)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}