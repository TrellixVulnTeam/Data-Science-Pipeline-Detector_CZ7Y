{"cells":[{"metadata":{},"cell_type":"markdown","source":"This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don’t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.\n\nCompetition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing labraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd ## Data Processing CSV file I/O\n\nimport numpy as np ## Linear Algebra\n\nimport matplotlib.pyplot as plt ## Visualization\n\nimport seaborn as sns ## Visualization\n\nimport tensorflow as tf\n\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data with pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Taking a quick peak at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets visualize some data xD"},{"metadata":{"trusted":true},"cell_type":"code","source":"## size of the plot\nplt.figure(figsize = (8,5))\n\n## grab data from the DataFrame\nsns.countplot(x = 'target', data = df_train, palette = 'dark', linewidth = 5)\n\n## Display the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,6))\n\nsns.countplot(y = df_train.keyword,order = df_train['keyword'].value_counts()\n              .sort_values(ascending=False).iloc[0:20].index)\nplt.title(\"Keywords Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Setting paremeters for disaster tweets\ndis_tweet = df_train.groupby('keyword')['target'].mean().sort_values(ascending = False).head(15)\n\n## Setting paremeters for non disaster tweets\nnon_dis_tweet = df_train.groupby('keyword')['target'].mean().sort_values().head(15)\n\n## Setting up the bar plot\nplt.figure(figsize = (9,6))\n## for disaster keyword\nsns.barplot(dis_tweet, dis_tweet.index, color = 'red')\nplt.title('Words with highest % of disaster')\n## for non disaster keywords\nsns.barplot(non_dis_tweet, non_dis_tweet.index, color = 'blue')\nplt.title('Words with lowest % disaster')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (14,7))\n\ntweet_loc = df_train.location.value_counts()\ntop_loc_dis = list(tweet_loc[tweet_loc >= 10].index)\ntop_dis = df_train[df_train.location.isin(top_loc_dis)]\n\ntop_loc = top_dis.groupby('location')['target'].mean().sort_values(ascending = False)\nsns.barplot( x = top_loc.index, y = top_loc)\nplt.xticks(rotation = 90)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.keyword.fillna('None', inplace = True)\n\ndf_train.location.fillna('None', inplace = True)\n\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def deconstruction(phrase):\n    \n    \n    phrase = re.sub(r\"won/'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can/'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('\\[.*?\\]', ' ', phrase) \n    phrase = re.sub('https?://\\S+|www\\.\\S+', ' ', phrase)\n    phrase = re.sub('<.*?>+', ' ', phrase)\n    phrase = re.sub('\\n', ' ', phrase)\n    phrase = re.sub('\\w*\\d\\w*', ' ', phrase)\n    \n    phrase = phrase.lower()\n    \n    return phrase\n\ndf_train.text = [deconstruction(tweet) for tweet in df_train.text]\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nlemmatizer = WordNetLemmatizer()\n\nps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}