{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\nfrom datetime import datetime\nfrom pytz import timezone\ndatetime.now(timezone('Asia/Tokyo')).strftime('%Y/%m/%d %H:%M:%S')\n\ndef refer_args(x):\n    if type(x) == 'method':\n        print(*x.__code__.co_varnames.split(), sep='\\n')\n    else:\n        print(*[x for x in dir(x) if not x.startswith('__')], sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams \nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\ndef matplotlib_config():\n    rcParams['font.family'] = 'sans-serif'\n    rcParams['font.sans-serif'] = [\n        'Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao',\n        'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP'\n    ]\n    rcParams['figure.figsize'] = 12, 8\n    rcParams[\"font.size\"] = 12\n\nmatplotlib_config()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_val = train_test_split(df_train, train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = torch.hub.load(\n    'huggingface/pytorch-transformers',\n    'tokenizer',\n    'bert-base-uncased'\n)\nconfig = torch.hub.load(\n    'huggingface/pytorch-transformers',\n    'config',\n    'bert-base-uncased'\n)\nbert = torch.hub.load(\n    'huggingface/pytorch-transformers',\n    'model',\n    'bert-base-uncased',\n    config=config\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NLP_with_Disaster_Tweets_Dataset(Dataset):\n    def __init__(self, df, transforms=None, mode='test'):\n        self.raw_texts = self.texts = df['text'].tolist()\n        if transforms is not None:\n            self.texts = transforms(self.texts)\n        self.mode = mode\n        if mode != 'test':\n            self.labels = df['target'].values\n    \n    def __iter__(self):\n        if self.mode != 'test':\n            return zip(self.texts, self.labels)\n        else:\n            return iter(self.texts)\n    \n    def __list__(self):\n        return list(self.__iter__)\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        if self.mode != 'test':\n            return self.texts[idx], self.labels[idx]\n        else:\n            return self.texts[idx]\n\ndef transforms(texts):\n    embeds = []    \n    for text in texts:\n        embed = tokenizer.encode(\n            text,                      \n            add_special_tokens = True, \n            max_length = 160,           \n            pad_to_max_length = True,\n            return_tensors = 'pt'  \n        )\n        embeds.append(embed)    \n    embeds = torch.cat(embeds, dim=0)\n    return embeds\n\ntrain_dataset = NLP_with_Disaster_Tweets_Dataset(df_train, transforms=transforms, mode='train')\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    drop_last=False\n)\n\nval_dataset = NLP_with_Disaster_Tweets_Dataset(df_val, transforms=transforms, mode='val')\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,\n    drop_last=False\n)\n\ntest_dataset = NLP_with_Disaster_Tweets_Dataset(df_test, transforms=transforms, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,\n    drop_last=False\n)\n\ntrain_size = len(train_dataset)\nval_size = len(val_dataset)\ntest_size = len(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self):\n        super(BertClassifier, self).__init__()\n        self.bert = bert\n        self.linear = nn.Linear(768, 1)\n        # initialing weights and bias\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, inputs, **kwargs):\n        vec, cls = self.bert(inputs, **kwargs)\n        out = self.linear(cls)\n        out = out.view(-1)\n        return torch.sigmoid(out)\n\nclassifier = BertClassifier()\ncriterion = nn.BCELoss()\noptimizer = Adam(\n    classifier.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    eps=1e-08,\n    weight_decay=0,\n    amsgrad=False\n)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nclassifier = classifier.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses = []\ntrain_accs = []\ntrain_preds = []\n\nval_losses = []\nval_accs = []\nval_preds = []\n\nnum_epoch = 2\nfor epoch in range(num_epoch):\n    train_loss = train_acc = 0\n    train_pred = torch.tensor([]).to(device)\n    _ = classifier.train()\n    _ = torch.set_grad_enabled(True)\n    loop = tqdm(train_dataloader)\n    for x, y in loop:\n        x, y = x.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        z = classifier(x, attention_mask=(x > 0))\n        loss = criterion(z, y.float())\n        loss.backward()\n        optimizer.step()\n        \n        train_pred = torch.cat([train_pred, torch.round(z)])\n        train_loss += loss.item()\n        acc = (torch.round(z) == y).sum()\n        train_acc += acc.item()\n        \n        loop.set_description(f'Epoch {epoch+1}/{num_epoch}')\n        loop.set_postfix(loss=f'{train_loss/train_size:.5f}', acc=f'{train_acc/train_size:.5f}')\n    \n    train_losses.append(train_loss/train_size)\n    train_accs.append(train_acc/train_size)\n    train_preds.append(train_pred.to('cpu').detach().numpy())\n    \n    val_loss = val_acc = 0\n    val_pred = torch.tensor([]).to(device)\n    _ = classifier.eval()\n    _ = torch.set_grad_enabled(False)\n    loop = val_dataloader\n    for x, y in loop:\n        x, y = x.to(device), y.to(device)\n        \n        with torch.no_grad():\n            z = classifier(x, attention_mask=(x > 0))\n            loss = criterion(z, y.float())\n        val_loss += loss.item()\n        acc = (torch.round(z) == y).sum()\n        val_acc += acc.item()\n        val_pred = torch.cat([val_pred, torch.round(z)])\n        \n        \n    print(f'loss={val_loss/val_size:.5f}, acc={val_acc/val_size:.5f}')\n    \n    val_losses.append(val_loss/val_size)\n    val_accs.append(val_acc/val_size)\n    val_preds.append(val_pred.to('cpu').detach().numpy())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = len(test_dataset)\ntest_loss = test_acc = 0\ntest_pred = torch.tensor([]).to(device)\n\n_ = classifier.eval()\n_ = torch.set_grad_enabled(False)\nloop = tqdm(test_dataloader)\nfor x in loop:\n    x = x.to(device)\n    with torch.no_grad():\n        z = classifier(x, attention_mask=(x > 0))\n    test_pred = torch.cat([test_pred, torch.round(z)])\n\ntest_pred = test_pred.to('cpu').detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df_test['id']\nsubmission['target'] = list(map(int, test_pred))\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}