{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"reference:\n\nhttps://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert","metadata":{}},{"cell_type":"markdown","source":"## 0. Import libraries <a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import decomposition\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:38:01.247925Z","iopub.execute_input":"2021-09-14T05:38:01.248229Z","iopub.status.idle":"2021-09-14T05:38:01.405713Z","shell.execute_reply.started":"2021-09-14T05:38:01.248174Z","shell.execute_reply":"2021-09-14T05:38:01.405037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Explore Data <a class=\"anchor\" id=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain_raw = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_raw = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:36.063015Z","iopub.execute_input":"2021-09-14T05:32:36.063299Z","iopub.status.idle":"2021-09-14T05:32:36.61612Z","shell.execute_reply.started":"2021-09-14T05:32:36.063227Z","shell.execute_reply":"2021-09-14T05:32:36.615427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw.shape, test_raw.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:37.672918Z","iopub.execute_input":"2021-09-14T05:32:37.673212Z","iopub.status.idle":"2021-09-14T05:32:37.681042Z","shell.execute_reply.started":"2021-09-14T05:32:37.673159Z","shell.execute_reply":"2021-09-14T05:32:37.680018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw[:3]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:39.956416Z","iopub.execute_input":"2021-09-14T05:32:39.95672Z","iopub.status.idle":"2021-09-14T05:32:39.979907Z","shell.execute_reply.started":"2021-09-14T05:32:39.956665Z","shell.execute_reply":"2021-09-14T05:32:39.978886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:42.430896Z","iopub.execute_input":"2021-09-14T05:32:42.431504Z","iopub.status.idle":"2021-09-14T05:32:42.453082Z","shell.execute_reply.started":"2021-09-14T05:32:42.431278Z","shell.execute_reply":"2021-09-14T05:32:42.451864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_raw.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:45.388817Z","iopub.execute_input":"2021-09-14T05:32:45.389137Z","iopub.status.idle":"2021-09-14T05:32:45.39907Z","shell.execute_reply.started":"2021-09-14T05:32:45.389083Z","shell.execute_reply":"2021-09-14T05:32:45.397737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What is the difference bwetween target value 1 and 0?\nLet's take a glimpse.","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    example = train_raw[ train_raw['target'] == 0 ]['text'][:10].tolist()\n    print(example[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:48.976844Z","iopub.execute_input":"2021-09-14T05:32:48.977144Z","iopub.status.idle":"2021-09-14T05:32:49.004216Z","shell.execute_reply.started":"2021-09-14T05:32:48.97709Z","shell.execute_reply":"2021-09-14T05:32:49.002609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    example = train_raw[ train_raw['target'] == 1 ]['text'][:10].tolist()\n    print(example[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:51.939632Z","iopub.execute_input":"2021-09-14T05:32:51.940188Z","iopub.status.idle":"2021-09-14T05:32:51.967416Z","shell.execute_reply.started":"2021-09-14T05:32:51.9399Z","shell.execute_reply":"2021-09-14T05:32:51.966522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the texts are classified by some particular keywords. For example, ''Target 1 text' has the words which has something to do with disaster such as evacuation, earthquake or fire.","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Clean Data","metadata":{}},{"cell_type":"markdown","source":"Before go any deeper, I'll clean the text first so we can only handle the words relevant.","metadata":{"execution":{"iopub.status.busy":"2021-08-26T07:06:43.248175Z","iopub.execute_input":"2021-08-26T07:06:43.248488Z","iopub.status.idle":"2021-08-26T07:06:43.254964Z","shell.execute_reply.started":"2021-08-26T07:06:43.248435Z","shell.execute_reply":"2021-08-26T07:06:43.253664Z"}}},{"cell_type":"code","source":"df=pd.concat([train_raw,test_raw], sort=True, axis=0, ignore_index=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:32:59.001433Z","iopub.execute_input":"2021-09-14T05:32:59.001741Z","iopub.status.idle":"2021-09-14T05:32:59.017481Z","shell.execute_reply.started":"2021-09-14T05:32:59.001689Z","shell.execute_reply":"2021-09-14T05:32:59.016416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_URL(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:18.392004Z","iopub.execute_input":"2021-09-14T05:33:18.392332Z","iopub.status.idle":"2021-09-14T05:33:18.449718Z","shell.execute_reply.started":"2021-09-14T05:33:18.392273Z","shell.execute_reply":"2021-09-14T05:33:18.448946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x : remove_html(str(x)))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:21.074089Z","iopub.execute_input":"2021-09-14T05:33:21.074417Z","iopub.status.idle":"2021-09-14T05:33:21.102297Z","shell.execute_reply.started":"2021-09-14T05:33:21.074353Z","shell.execute_reply":"2021-09-14T05:33:21.101505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x : remove_emoji(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:23.676031Z","iopub.execute_input":"2021-09-14T05:33:23.676354Z","iopub.status.idle":"2021-09-14T05:33:23.76101Z","shell.execute_reply.started":"2021-09-14T05:33:23.676295Z","shell.execute_reply":"2021-09-14T05:33:23.760399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:45.812814Z","iopub.execute_input":"2021-09-14T05:33:45.813132Z","iopub.status.idle":"2021-09-14T05:33:45.877714Z","shell.execute_reply.started":"2021-09-14T05:33:45.813077Z","shell.execute_reply":"2021-09-14T05:33:45.877083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:7]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:49.732417Z","iopub.execute_input":"2021-09-14T05:33:49.732698Z","iopub.status.idle":"2021-09-14T05:33:49.748437Z","shell.execute_reply.started":"2021-09-14T05:33:49.732647Z","shell.execute_reply":"2021-09-14T05:33:49.747788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df.loc[:len(train_raw)-1, ['text', 'target']]\ntest = df.loc[len(train_raw):, ['text', 'target']]\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:53.765987Z","iopub.execute_input":"2021-09-14T05:33:53.766312Z","iopub.status.idle":"2021-09-14T05:33:53.77943Z","shell.execute_reply.started":"2021-09-14T05:33:53.76626Z","shell.execute_reply":"2021-09-14T05:33:53.778254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Would there be any difference in text length?","metadata":{}},{"cell_type":"code","source":"train['length'] = train['text'].apply(lambda x: len(x.split()))\ntrain[:2]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:33:56.863966Z","iopub.execute_input":"2021-09-14T05:33:56.864425Z","iopub.status.idle":"2021-09-14T05:33:56.888145Z","shell.execute_reply.started":"2021-09-14T05:33:56.86421Z","shell.execute_reply":"2021-09-14T05:33:56.887024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (20, 6)\nbins = 200\nplt.hist(train[train['target'] == 0]['length'], alpha = 0.6, bins=bins, label='0')\nplt.hist(train[train['target'] == 1]['length'], alpha = 0.8, bins=bins, label='1')\nplt.legend(loc='upper right')\nplt.xlim(0,train['length'].max())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:34:28.852921Z","iopub.execute_input":"2021-09-14T05:34:28.853268Z","iopub.status.idle":"2021-09-14T05:34:29.883083Z","shell.execute_reply.started":"2021-09-14T05:34:28.853189Z","shell.execute_reply":"2021-09-14T05:34:29.882225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, the text of target 1 is slightly shorter than those of target 0.","metadata":{}},{"cell_type":"markdown","source":"What are the top 10 words that shows frequently in target 0 and target 1?","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop_words = set(stopwords.words('english'))\n\ncorpus_0 = []\ncorpus_1 = []\nfor i in range(len(train)):\n    words = word_tokenize(train.iloc[i]['text'])\n    for word in words:\n        word = word.lower()\n        if (word not in stop_words) and (word not in string.punctuation):\n            if train.iloc[i]['target'] == 0:\n                corpus_0.append(word)\n            else:\n                corpus_1.append(word)\n\nprint(len(corpus_0))\nprint(len(corpus_1))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:35:19.991512Z","iopub.execute_input":"2021-09-14T05:35:19.991821Z","iopub.status.idle":"2021-09-14T05:35:38.065635Z","shell.execute_reply.started":"2021-09-14T05:35:19.991768Z","shell.execute_reply":"2021-09-14T05:35:38.064569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ncounter_0=Counter(corpus_0)\nmost_0=counter_0.most_common()\nmost_0[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:36:09.626723Z","iopub.execute_input":"2021-09-14T05:36:09.627011Z","iopub.status.idle":"2021-09-14T05:36:09.644797Z","shell.execute_reply.started":"2021-09-14T05:36:09.626958Z","shell.execute_reply":"2021-09-14T05:36:09.644018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter_1=Counter(corpus_1)\nmost_1=counter_1.most_common()\nmost_1[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:36:12.838796Z","iopub.execute_input":"2021-09-14T05:36:12.839095Z","iopub.status.idle":"2021-09-14T05:36:12.851555Z","shell.execute_reply.started":"2021-09-14T05:36:12.83904Z","shell.execute_reply":"2021-09-14T05:36:12.850787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to https://www.geeksforgeeks.org/generating-word-cloud-python/","metadata":{}},{"cell_type":"code","source":"comment_words_list0 = []\n\nfor i in range(len(corpus_0)):\n    word = corpus_0[i]\n    comment_words_list0.append(word)\ncomment_words_0 = ' '.join(comment_words_list0)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:36:15.7592Z","iopub.execute_input":"2021-09-14T05:36:15.759567Z","iopub.status.idle":"2021-09-14T05:36:15.785447Z","shell.execute_reply.started":"2021-09-14T05:36:15.759519Z","shell.execute_reply":"2021-09-14T05:36:15.784716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud_0 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(comment_words_0)\n\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud_0)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.title('Target 0', fontname = 'monospace')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:36:18.731826Z","iopub.execute_input":"2021-09-14T05:36:18.732148Z","iopub.status.idle":"2021-09-14T05:36:20.543684Z","shell.execute_reply.started":"2021-09-14T05:36:18.732092Z","shell.execute_reply":"2021-09-14T05:36:20.542869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_words_list1 = []\nfor i in range(len(corpus_1)):\n    word = corpus_1[i]\n    comment_words_list1.append(word)\ncomment_words_1 = ' '.join(comment_words_list1)\n\nwordcloud_1 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(comment_words_1)\n\n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud_1)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.title('Target 1', fontname = 'monospace')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:36:35.797443Z","iopub.execute_input":"2021-09-14T05:36:35.797853Z","iopub.status.idle":"2021-09-14T05:36:37.372164Z","shell.execute_reply.started":"2021-09-14T05:36:35.797775Z","shell.execute_reply":"2021-09-14T05:36:37.371406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Target 0' contains more about self and personal life while 'Target 1' is more about outter world issue or accident.","metadata":{}},{"cell_type":"markdown","source":"## Vectorize features","metadata":{}},{"cell_type":"markdown","source":"With clean version of text,\nconvert our features(each words in the text) to vectors.\n\nWhat mapping method would be good?","metadata":{}},{"cell_type":"markdown","source":"1) CountVectorizer\n\nCountvectorizer counts(using one hot encoding) how many times the given word apperas in the text.","metadata":{}},{"cell_type":"code","source":" from sklearn.model_selection import train_test_split\n\ntrain_X = train[\"text\"].values\ntrain_Y = train[\"target\"].values\ntest_X = test[\"text\"].values\n\nrandom_state_split = 42\ntrain_x, val_x, train_y, val_y = train_test_split(train_X, train_Y, \n                                                  test_size=0.2, shuffle=True,\n                                                  random_state=random_state_split)\nprint(train_x.shape, train_y.shape, val_x.shape, val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:37:44.154726Z","iopub.execute_input":"2021-09-14T05:37:44.155036Z","iopub.status.idle":"2021-09-14T05:37:44.16512Z","shell.execute_reply.started":"2021-09-14T05:37:44.154982Z","shell.execute_reply":"2021-09-14T05:37:44.164093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x[:3]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:38:10.376786Z","iopub.execute_input":"2021-09-14T05:38:10.377104Z","iopub.status.idle":"2021-09-14T05:38:10.3823Z","shell.execute_reply.started":"2021-09-14T05:38:10.377052Z","shell.execute_reply":"2021-09-14T05:38:10.381566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the feature matrix.\ntake some sample.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer_5 =  CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts_5 = count_vectorizer_5.fit_transform(train_x[:5])\nsample = pd.DataFrame(train_x_counts_5.A, columns=count_vectorizer_5.get_feature_names())\nsample","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:10.120335Z","iopub.execute_input":"2021-09-14T05:39:10.120645Z","iopub.status.idle":"2021-09-14T05:39:10.154365Z","shell.execute_reply.started":"2021-09-14T05:39:10.120594Z","shell.execute_reply":"2021-09-14T05:39:10.153702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(count_vectorizer_5.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:14.22988Z","iopub.execute_input":"2021-09-14T05:39:14.230177Z","iopub.status.idle":"2021-09-14T05:39:14.234904Z","shell.execute_reply.started":"2021-09-14T05:39:14.230123Z","shell.execute_reply":"2021-09-14T05:39:14.234156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After finishing mapping of all train_x, the number of features ends up with 15470.","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts = count_vectorizer.fit_transform(train_x)\nval_x_counts = count_vectorizer.transform(val_x)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:20.507656Z","iopub.execute_input":"2021-09-14T05:39:20.507963Z","iopub.status.idle":"2021-09-14T05:39:20.694637Z","shell.execute_reply.started":"2021-09-14T05:39:20.507913Z","shell.execute_reply":"2021-09-14T05:39:20.693967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Input of Train feature vector matrix shape: {}'.format(train_x_counts.shape))\nprint('Input of Validation feature vector matrix shape: {}'.format(val_x_counts.shape))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:23.547868Z","iopub.execute_input":"2021-09-14T05:39:23.548158Z","iopub.status.idle":"2021-09-14T05:39:23.555727Z","shell.execute_reply.started":"2021-09-14T05:39:23.548106Z","shell.execute_reply":"2021-09-14T05:39:23.553396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of features(15470) is greater than those of examples(6090).\n\nIf the matrix is too fat, learning is not easy.\n\nWe need to do dimensionality reduction like PCA or manifold.\n\nHere, I'll directly select the important features using 'most_0' and 'most_1'.\n\nIf key word is appeared in corpus less than three times, remove it. \n\nBecause if the frequency is more than 2, it can be taken as not coincidence.","metadata":{}},{"cell_type":"code","source":"most_1_dict = dict(most_1)\nprint(len(most_1_dict))\nlist(most_1_dict.keys())[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:27.952962Z","iopub.execute_input":"2021-09-14T05:39:27.953259Z","iopub.status.idle":"2021-09-14T05:39:27.962757Z","shell.execute_reply.started":"2021-09-14T05:39:27.953198Z","shell.execute_reply":"2021-09-14T05:39:27.961831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The less frequent words seem like not very informative for model to learn.","metadata":{}},{"cell_type":"code","source":"for i in list(most_1_dict.keys()):\n    if most_1_dict[i] < 3:\n        del most_1_dict[i]\nprint(len(most_1_dict))\nlist(most_1_dict.keys())[-20:]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:30.152295Z","iopub.execute_input":"2021-09-14T05:39:30.152598Z","iopub.status.idle":"2021-09-14T05:39:30.163986Z","shell.execute_reply.started":"2021-09-14T05:39:30.152547Z","shell.execute_reply":"2021-09-14T05:39:30.162969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After deleting those words, the less frequent words gets closer to 'disaster'.\n\nOur model might see these words useful.","metadata":{}},{"cell_type":"code","source":"most_0_dict = dict(most_0)\nfor i in list(most_0_dict.keys()):\n    if most_0_dict[i] < 3:\n        del most_0_dict[i]\nprint(len(most_0_dict))\nlist(most_0_dict.keys())[-20:]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:32.897724Z","iopub.execute_input":"2021-09-14T05:39:32.898019Z","iopub.status.idle":"2021-09-14T05:39:32.911988Z","shell.execute_reply.started":"2021-09-14T05:39:32.897968Z","shell.execute_reply":"2021-09-14T05:39:32.911127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To vectorizer, feed the revised version of data.","metadata":{}},{"cell_type":"code","source":"# combine two dictionaries\nmost_dict = most_0_dict.copy()\nmost_dict.update(most_1_dict)\nlen(most_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:36.112543Z","iopub.execute_input":"2021-09-14T05:39:36.112945Z","iopub.status.idle":"2021-09-14T05:39:36.119902Z","shell.execute_reply.started":"2021-09-14T05:39:36.112888Z","shell.execute_reply":"2021-09-14T05:39:36.118617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revised_train = train.copy()\nfor index in range(len(revised_train)):\n    text = revised_train.loc[index, 'text']\n    words = str(text).split()\n    revised_text = []\n    for word in words:\n        if word in list(most_dict.keys()):\n            revised_text.append(word)\n    revised_train.at[index, 'text'] = ' '.join(revised_text)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:39:38.220053Z","iopub.execute_input":"2021-09-14T05:39:38.220378Z","iopub.status.idle":"2021-09-14T05:39:45.621826Z","shell.execute_reply.started":"2021-09-14T05:39:38.220313Z","shell.execute_reply":"2021-09-14T05:39:45.620987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('origianl       : ', train.loc[6879, 'text'])\nprint('revised version: ', revised_train.loc[6879, 'text'])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:31.143624Z","iopub.execute_input":"2021-09-14T05:40:31.14392Z","iopub.status.idle":"2021-09-14T05:40:31.150284Z","shell.execute_reply.started":"2021-09-14T05:40:31.143864Z","shell.execute_reply":"2021-09-14T05:40:31.149471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('origianl       : ', train.loc[5781, 'text'])\nprint('revised version: ', revised_train.loc[5781, 'text'])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:33.553561Z","iopub.execute_input":"2021-09-14T05:40:33.553852Z","iopub.status.idle":"2021-09-14T05:40:33.56275Z","shell.execute_reply.started":"2021-09-14T05:40:33.553799Z","shell.execute_reply":"2021-09-14T05:40:33.561951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('origianl       : ', train.loc[0, 'text'])\nprint('revised version: ', revised_train.loc[0, 'text'])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:35.785862Z","iopub.execute_input":"2021-09-14T05:40:35.78619Z","iopub.status.idle":"2021-09-14T05:40:35.793098Z","shell.execute_reply.started":"2021-09-14T05:40:35.786134Z","shell.execute_reply":"2021-09-14T05:40:35.791149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After revising, relatively essential key words are left.","metadata":{}},{"cell_type":"code","source":"train_X = revised_train[\"text\"].values\ntrain_Y = revised_train[\"target\"].values\n\nrandom_state_split = 42\ntrain_x_revised, val_x_revised, train_y_revised, val_y_revised = train_test_split(\n    train_X, train_Y, \n    test_size=0.2, shuffle=False,\n    random_state=random_state_split)\n\ncount_vectorizer_new = CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts_revised = count_vectorizer_new.fit_transform(train_x_revised)\nval_x_counts_revised = count_vectorizer_new.transform(val_x_revised)\n\ntrain_x_counts_revised.shape, val_x_counts_revised.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:38.089491Z","iopub.execute_input":"2021-09-14T05:40:38.089797Z","iopub.status.idle":"2021-09-14T05:40:38.176009Z","shell.execute_reply.started":"2021-09-14T05:40:38.089741Z","shell.execute_reply":"2021-09-14T05:40:38.175343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(count_vectorizer_new.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:40.738537Z","iopub.execute_input":"2021-09-14T05:40:40.738896Z","iopub.status.idle":"2021-09-14T05:40:40.743927Z","shell.execute_reply.started":"2021-09-14T05:40:40.738826Z","shell.execute_reply":"2021-09-14T05:40:40.743188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_df = pd.DataFrame(train_x_counts_revised.A, \n                          columns=count_vectorizer_new.get_feature_names())\ntrain_x_df","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:42.98845Z","iopub.execute_input":"2021-09-14T05:40:42.988738Z","iopub.status.idle":"2021-09-14T05:40:43.055966Z","shell.execute_reply.started":"2021-09-14T05:40:42.988686Z","shell.execute_reply":"2021-09-14T05:40:43.054994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before 15470, now the feature size gets down to 2902.","metadata":{}},{"cell_type":"code","source":"val_x_df = pd.DataFrame(val_x_counts_revised.A, \n                          columns=count_vectorizer_new.get_feature_names())\nval_x_df","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:40:46.221044Z","iopub.execute_input":"2021-09-14T05:40:46.221368Z","iopub.status.idle":"2021-09-14T05:40:46.256097Z","shell.execute_reply.started":"2021-09-14T05:40:46.221313Z","shell.execute_reply":"2021-09-14T05:40:46.255099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a saprse matrix. ","metadata":{"execution":{"iopub.status.busy":"2021-08-30T03:57:40.66604Z","iopub.execute_input":"2021-08-30T03:57:40.66634Z","iopub.status.idle":"2021-08-30T03:57:40.671771Z","shell.execute_reply.started":"2021-08-30T03:57:40.666288Z","shell.execute_reply":"2021-08-30T03:57:40.670709Z"}}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(train_x_df.values)\ntrain_scaled = scaler.transform(train_x_df.values)\nval_scaled = scaler.transform(val_x_df.values)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:41:05.353572Z","iopub.execute_input":"2021-09-14T05:41:05.353886Z","iopub.status.idle":"2021-09-14T05:41:05.591786Z","shell.execute_reply.started":"2021-09-14T05:41:05.353829Z","shell.execute_reply":"2021-09-14T05:41:05.59102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import decomposition\nfrom sklearn.neighbors import KNeighborsClassifier\n\nsvd = decomposition.TruncatedSVD(algorithm='randomized', \n                                 n_iter=10, random_state=0, tol=0.0)\ntrain_pca = svd.fit_transform(train_scaled)\nval_pca = svd.transform(val_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:41:23.432906Z","iopub.execute_input":"2021-09-14T05:41:23.43357Z","iopub.status.idle":"2021-09-14T05:41:25.739534Z","shell.execute_reply.started":"2021-09-14T05:41:23.433218Z","shell.execute_reply":"2021-09-14T05:41:25.738589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neigh = KNeighborsClassifier(n_neighbors=290, algorithm='kd_tree')\nneigh.fit(train_pca, train_y) \n\nneigh.score(val_pca, val_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:41:28.101429Z","iopub.execute_input":"2021-09-14T05:41:28.101722Z","iopub.status.idle":"2021-09-14T05:41:28.248965Z","shell.execute_reply.started":"2021-09-14T05:41:28.101669Z","shell.execute_reply":"2021-09-14T05:41:28.248272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Accuracy is not good. \n\nRather than doing a further parameter tunning or finding another model with this matrix,\n\nin order to find a good classifier,\n\nwe need a better feature representation.\n\nPre-trained one, like Bert, converts 'the given words' to vectors quite well.","metadata":{}},{"cell_type":"code","source":"# Bert","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To be continued...\n\nThank you for reading :)","metadata":{}},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}