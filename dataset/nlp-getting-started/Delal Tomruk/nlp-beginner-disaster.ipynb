{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom collections import Counter\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.location.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [word_tokenize(texts) for texts in train.text]\nlen_tokens=[]\n\nfor i in range(len(tokens)):\n    len_tokens.append(len(tokens[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text_tokens\"] = len_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text = train.text.str.replace('[^\\w\\s]','')\ntrain.text = train.text.str.lower()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(\"target\", axis = 1)\ny = train[\"target\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = [\"keyword\", \"location\"]\nimputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\ntransformer = ColumnTransformer([(\"imputer\", imputer, cat)], remainder=\"passthrough\")\n\n# fill simple imputer with X values since you want to fill only the features, not the target\nfilled_X = transformer.fit_transform(X_train)\n\nfilled = pd.DataFrame(filled_X, columns=[\"id\", \"keyword\", \"location\", \"text\", \"text_tokens\"])\nfilled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing URLs\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    # replace the pattern url in text with None\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove html tags\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_URL(x))\ntest['text'] = test['text'].apply(lambda x : remove_URL(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_html(x))\ntest['text'] = test['text'].apply(lambda x : remove_html(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x : remove_emoji(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstopwords = ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\ntext = \" \".join(text for text in train.text)\ncloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\nplt.imshow(cloud);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values\na = sns.heatmap(train.isnull())\nplt.suptitle(\"Null Values\");\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zero = train[train.target==0].text_tokens\none = train[train.target==1].text_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,5))\nax0.hist(zero,color='purple')\nfig.suptitle('Number of Tokens in Text')\nax0.set_title(\"Not Disaster\")\nax1.set_title(\"Disaster\")\nax1.hist(one,color='blue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zero = train[train.target==0].text.str.len()\none = train[train.target==1].text.str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,5))\nax0.hist(zero,color='purple')\nax0.set_title(\"Not Disaster\")\nax1.set_title(\"Disaster\")\nfig.suptitle(\"Number of Characters in Text\")\nax1.hist(one,color='blue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"target\",data=train,  kind=\"count\")\nplt.suptitle(\"Target Comparison\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\nsns.set(rc={'figure.figsize':(5,5)})\nsns.heatmap(corr)\nplt.suptitle(\"Correlation\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zero = train[train.target==0].text\none = train[train.target==1].text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nword = one.str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax = ax1,color='blue')\nax1.set_title('Disaster')\n\nword = zero.str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not Disaster')\nfig.suptitle('Average Word Length');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(max_features = 10, stop_words=stopwords)\ntfIdf = vect.fit(train.text)\nX = vect.transform(train.text)\nX_df = pd.DataFrame(X.toarray(), columns = vect.get_feature_names())\nX_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(ngram_range=(1,3), max_features = 100, max_df=500, stop_words= stopwords)\nvectorizer.fit(train.text)\nX = vectorizer.transform(train.text)\nBoW = pd.DataFrame(X.toarray(), columns= vectorizer.get_feature_names())\nBoW","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit a Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = BoW\ny = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train , X_test , y_train , y_test = train_test_split(X, y,test_size=0.20, random_state=55, shuffle =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n\n\ndecisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\n\ndecisionTreeModel.fit(X_train, y_train);\n\n\n# ### Gradient Boosting\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n\n\ngradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train);\n\n\n# ### K-Nearest Neighbors\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n\n\nKNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train);\n\n\n# ### Logistic Regression Model\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\n\nLogisticRegression = LogisticRegression(penalty='l2', \n                                        solver='saga', \n                                        random_state = 55)  \n\nLogisticRegression.fit(X_train,y_train);\n\n\n# ### Bernoulli Naive Bayes Model\n\n\n\nfrom sklearn.naive_bayes import BernoulliNB\n\n\n\n\nbernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n\n\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, bernoulliNBModel]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test ,y_pred))\n    print('********************************************************************')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FEATURES = [\"id\", \"keyword\", \"location\", \"text\", \"target\",\"text_tokens\"]\nTEST_FEATURES = [\"id\", \"keyword\", \"location\", \"text\"]\n\ntrain[TRAIN_FEATURES].to_pickle('train.pkl')\ntest[TEST_FEATURES].to_pickle('test.pkl')\n\nprint('Training Set Shape = {}'.format(train[TRAIN_FEATURES].shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train[TRAIN_FEATURES].memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(test[TEST_FEATURES].shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test[TEST_FEATURES].memory_usage().sum() / 1024**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}