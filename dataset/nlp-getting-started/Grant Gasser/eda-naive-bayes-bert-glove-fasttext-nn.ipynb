{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Natural Language Processing Project (NU MSAI)\n### Authors: Grant Gasser, Sundar Thevar, Blaine Rothrock, Zhili Wang"},{"metadata":{},"cell_type":"markdown","source":"#### Notebook Credit\n* https://www.kaggle.com/ratan123/in-depth-guide-to-google-s-bert/output\n* https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n* https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n\n**Note:** Make sure GPU is on!"},{"metadata":{"_uuid":"251ae659-dbda-4057-a933-ee22bbc2722e","_cell_guid":"8d8bd775-0979-4e61-b12b-6aa4fea406df","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport tensorflow as tf\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/nlp-getting-started/'\nSEED = 42\nDROPOUT = 0.5\nEPOCHS = 3\nLEARN_RATE = 1e-4\nSPLIT = 0.2\nBATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read and Inspect Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nds = ['train', 'test', 'sample submission']\nprint(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\nprint(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n\nprint()\nprint(train.columns)\nprint(test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly not disasters.."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.target == 0].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Disasters"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keywords\nNot including NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Keyword Distribution:\\n\\n')\nprint(train.keyword.value_counts())\nprint('\\n', '-' * 50, '\\n')\nprint('Test Keyword Distribution:\\n\\n')\nprint(test.keyword.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Balance \n* (4342 0's and 3271 1's)"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train['target'].value_counts(dropna = False).reset_index()\ntemp.columns = ['target', 'counts']\n\ncountplt = sns.countplot(x = 'target', data = train, hue = train['target'])\ncountplt.set_xticklabels(['0: Not Disaster (4342)', '1: Disaster (3271)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Distribution in Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain.drop(columns=['target_mean'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values (NaNs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count NaN:')\nprint(train.isnull().sum(), '\\n')\nprint('Percentage NaN:')\nprint(train.isnull().sum()/ len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count NaN:')\nprint(test.isnull().sum(), '\\n')\nprint('Percentage NaN:')\nprint(test.isnull().sum()/ len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of Characters in `text` - Train Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len = train[train['target'] == 0]['text'].str.len()\nax1.hist(train_len,color='blue')\nax1.set_title('Not A Disaster')\ntrain_len = train[train['target'] == 1]['text'].str.len()\nax2.hist(train_len,color='red')\nax2.set_title('Disaster')\nfig.suptitle('Characters in Train Set\\'s Text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of Words in `text` - Train Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len = train[train['target'] == 0]['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='blue')\nax1.set_title('Not A Disaster')\ntrain_len = train[train['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='red')\nax2.set_title('Disaster')\nfig.suptitle('Words in Train Set\\'s Text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Fix mislabelled samples\n* Copied from [here](https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data)\n* My hesitation with this is that if samples are mislabbeled in the training set, the are likely samples in the test set mislabelled..."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain.loc[train['id'].isin(ids_with_target_error),'target'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing and Removing punctutation\nThoughts:\n   * use some regex code from [here](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook)\n   * use the Regex Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"#### Prepend keyword to text - Run only once!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['keyword'].notnull(), 'text'] = train['keyword'] + ' ' + train['text']\ntest.loc[test['keyword'].notnull(), 'text'] = test['keyword'] + ' ' + test['text']\n\n# view\ntrain[train['keyword'].notnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['id', 'keyword', 'location'], axis=1)\ntest = test.drop(['keyword', 'location'], axis=1) # keep id\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n# NLTK Tweet Tokenizer for now\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(strip_handles=True)\n\ncorpus = []\n\n# clean up text\ndef clean_text(text):\n    \"\"\"\n    Copied from other notebooks\n    \"\"\"\n    # expand acronyms\n    \n    # special characters\n    text = re.sub(r\"\\x89Û_\", \"\", text)\n    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n    text = re.sub(r\"\\x89Û÷\", \"\", text)\n    text = re.sub(r\"\\x89Ûª\", \"\", text)\n    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n    text = re.sub(r\"å_\", \"\", text)\n    text = re.sub(r\"\\x89Û¢\", \"\", text)\n    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n    text = re.sub(r\"åÊ\", \"\", text)\n    text = re.sub(r\"åÈ\", \"\", text)\n    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n    text = re.sub(r\"Ì©\", \"e\", text)\n    text = re.sub(r\"å¨\", \"\", text)\n    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n    text = re.sub(r\"åÇ\", \"\", text)\n    text = re.sub(r\"å£3million\", \"3 million\", text)\n    text = re.sub(r\"åÀ\", \"\", text)\n    \n    # emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    \n    \"\"\"\n    Our Stuff\n    \"\"\"\n    # remove numbers\n    text = re.sub(r'[0-9]', '', text)\n    \n    # remove punctuation and special chars (keep '!')\n    for p in string.punctuation.replace('!', ''):\n        text = text.replace(p, '')\n        \n    # remove urls\n    text = re.sub(r'http\\S+', '', text)\n    \n    # tokenize\n    text = tknzr.tokenize(text)\n    \n    # remove stopwords\n    text = [w.lower() for w in text if not w in stop_words]\n    corpus.append(text)\n    \n    # join back\n    text = ' '.join(text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain['text'] = train['text'].apply(lambda s: clean_text(s))\ntest['text'] = test['text'].apply(lambda s: clean_text(s))\n\n# see some cleaned data\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove infrequent words"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['text'].to_numpy()\nword_freq = {}\n\nfor text in texts:\n    for word in text.split():\n        word_freq[word] = word_freq.get(word, 0) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remove words occuring < 3 times\n# freq_threshold = 3\n# for i, text in enumerate(texts):\n#     for word in text.split():\n#         if word_freq[word] < freq_threshold:\n#             print(word)\n#             texts[i].replace(word, '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 40\n\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nnum_words = len(word_index) + 1\nprint('Found %s unique tokens.' % (num_words - 1))\n\n# pad \ndata = pad_sequences(\n    sequences, \n    maxlen=MAX_SEQUENCE_LENGTH,\n    padding='post', \n    truncating='post'\n)\n\nlabels = train['target'].to_numpy()\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data\ny_train = labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes (Non-Neural Baseline Method)\nThe goal of this step is to learn about naive bayes and establish a baseline non-neural network model\n\n* Great Naive Bayes [Explanation and Tutorial](https://towardsdatascience.com/algorithms-for-text-classification-part-1-naive-bayes-3ff1d116fdd8)\n* Another [Explanation and Tutorial](https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/)"},{"metadata":{},"cell_type":"markdown","source":"### Result of Naive Bayes\nSee [Version 8](https://www.kaggle.com/grantgasser/eda-naive-bayes-twitter-embeddings-nn?scriptVersionId=30306784), where NB model scored **.804**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n\nvectorizer = CountVectorizer()\nx_train_vectorized = vectorizer.fit_transform(train['text'])\n\n# print vocabulary\nprint(vectorizer.get_feature_names()[2500:2600])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit and predict (Naive Bayes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_NB = np.array(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha is smoothing param\nmodel_NB = BernoulliNB(alpha=1.0)\nmodel_NB.fit(x_train_vectorized, y_train_NB)\n\n# prepare test\nx_test_NB = vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-Trained Embeddings\n\n#### Fasttext embeddings\nThese embeddings were created using a [created](https://fredericgodin.com/research/twitter-word-embeddings/) by Fréderic Godin at Ghent University in Belgium. [This blog post](https://fredericgodin.com/research/twitter-word-embeddings/) explains the work. The fasttext embedding model is too large to load in to kaggle (~15GB) so we created a specialized embedding data set that only contains words of our vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# read fasttext twitter embeddings\nembeddings_df = pd.read_pickle('/kaggle/input/fasttext-twitter-derived-embeddings/twitter_derived_embeddings')\nembeddings_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 400\n\nfasttext_embedding_idx = {}\nfor idx, row in embeddings_df.iterrows():\n    word = row[0]\n    embeddings = np.asarray(row[1], 'float32')\n    fasttext_embedding_idx[word] = embeddings\n\n# print only 20\nfasttext_embedding_idx['earthquake'][:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GloVe Twitter embeddings\nFound [here](https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: comment out if using fasttext\n# glove_embedding_idx = {}\n# EMBEDDING_DIM = 100\n# with open('/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt','r') as f:\n#     for line in f:\n#         values=line.split()\n#         word=values[0]\n#         vectors=np.asarray(values[1:],'float32')\n#         glove_embedding_idx[word] = vectors\n# f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create embeddings layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"iv = 0\noov = 0\n\nembedding_idx = fasttext_embedding_idx # swap between embeddings\n\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embedding_idx.get(word)\n    if embedding_vector is not None:\n        iv += 1\n        # words not found in the embedding space are all zeros\n        embedding_matrix[i] = embedding_vector\n    else: oov += 1\n        \nprint('%i tokens in vocab, %i tokens out of vocab' % (iv, oov)) # TODO: must reduce out of vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the embedding layer, this will not be trainable!\nfrom tensorflow.keras.layers import Embedding\n\nmodel_NN = tf.keras.Sequential()\n\nembedding_layer = Embedding(\n    num_words, \n    EMBEDDING_DIM,\n    weights = [embedding_matrix],\n    input_length=MAX_SEQUENCE_LENGTH,\n    trainable=False\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Model: Simple Bi-directional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_NN = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'),\n    embedding_layer,\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(DROPOUT),\n    tf.keras.layers.Dense(1, activation='sigmoid') # add sigmoid to get [0,1]\n])\n\nloss = tf.keras.losses.BinaryCrossentropy(\n    from_logits=False, \n    name='binary_crossentropy'\n)\n\noptimizer = tf.keras.optimizers.Adam(LEARN_RATE)\n\nmodel_NN.compile(\n    loss=loss,\n    optimizer=optimizer,\n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_NN.fit(x_train, y_train, epochs=EPOCHS, validation_split=SPLIT, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokenizer = Tokenizer()\ntest_texts = test['text'].to_numpy()\ntest_tokenizer.fit_on_texts(texts)\ntest_sequences = test_tokenizer.texts_to_sequences(test_texts)\n\ntest_word_index = test_tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\ntest_embedding_matrix = np.zeros((len(test_word_index) + 1, EMBEDDING_DIM))\nfor word, i in test_word_index.items():\n    embedding_vector = embedding_idx.get(word)\n    if embedding_vector is not None:\n        # words not found in the embedding space are all zeros\n        test_embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_lstm_preds = model_NN.predict(test_data)\nlstm_preds = raw_lstm_preds.round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT and TFHub"},{"metadata":{},"cell_type":"markdown","source":"Re-read data to nullify pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n    \nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BERT Usage\nLayer from TensorFlow Hub. [See Here](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1) for explanation and tutorial\n"},{"metadata":{},"cell_type":"markdown","source":"**References:**\n* Official tokenization script created by the Google team: https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n* Bert_Encode function from Kaggle's Bert-Starter-Inference: https://www.kaggle.com/user123454321/bert-starter-inference\n* All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nmax_seq_length = 180\nbert_url = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\"\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\nbert_inputs = [input_word_ids, input_mask, segment_ids]\n\nbert_layer = hub.KerasLayer(bert_url, trainable=True)\npooled_output, sequence_output = bert_layer(bert_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab_file is a url, do_lower_case is a tf.Variable bool, and tokenizer is an object\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_bert = bert_encode(train['text'].values, tokenizer, max_len=max_seq_length)\nx_test_bert = bert_encode(test['text'].values, tokenizer, max_len=max_seq_length)\ny_train_bert = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropout = tf.keras.layers.Dropout(DROPOUT)(sequence_output[:, 0, :])\ndense = tf.keras.layers.Dense(64, activation='relu')(sequence_output[:, 0, :])\npred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n\n# callbacks, adaptive learning rate\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n\nmodel_NN_bert = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n\noptimizer = tf.keras.optimizers.Adam(LEARN_RATE)\n\nmodel_NN_bert.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel_NN_bert.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_NN_bert.fit(\n    x_train_bert,\n    y_train_bert,\n    validation_split=SPLIT,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_preds = model_NN_bert.predict(x_test_bert)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Enter submission"},{"metadata":{},"cell_type":"markdown","source":"#### Naive Bayes Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# probabilities [0,1]\n# print(model_NB.classes_)\nnb_preds_prob = model_NB.predict_proba(x_test_NB)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_preds = model_NB.predict(x_test_NB)\nsample_submission['target'] = nb_preds\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.allclose(nb_preds, nb_preds_prob.round().astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission_NB.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neural Net Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = lstm_preds\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission_NN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### BERT Neural Net Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = bert_preds.round().astype(int)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission_NN_bert.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ensemble Submission (Naive Bayes + LSTM + BERT NN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_preds_prob[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_lstm_preds = raw_lstm_preds[:, 0]\nraw_lstm_preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_preds = bert_preds[:, 0]\nbert_preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_preds = .5*nb_preds_prob + .5*bert_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = ensemble_preds.round().astype(int)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission_ensemble.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}