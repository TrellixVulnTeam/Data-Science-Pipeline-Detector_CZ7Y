{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel demonstrates binary classification of tweets using Sequence model.\n\n\nLet's start with importing all the necessary packages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter, namedtuple\n\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.optimizers import Nadam,adam\n\nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's see how data looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for class imbalance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns = ['id','keyword','location'], inplace=True)\nneg, pos = np.bincount(data.target)\nprint(f'Total: {len(data)} \\nPositive: {pos} \\nNegative: {neg}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no class imbalance problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Check for null values in data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's work with tweets\n\nClean the text by removing urls, html tags, emojis and stopwords.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    #remove urls\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n    \n    #remove html\n    html_pattern = re.compile(r'<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    #remove emojis\n    emoji_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'',text)\n    \n    #remove punctuations\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    text = text.translate(table)\n    \n    #remove stopwords\n    stop = set(stopwords.words('english'))\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return ' '.join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cannot directly use textual data as input to our sequence model. We need to map each word in the tweet to an integer. We can then use Embedding layer of keras to vector encode the words.\n\nLet's find the vocabulary size first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_counter(text):  \n    \n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count    \n\ntext = data['text']\ncounter = word_counter(text)\n\nvocab_size = len(counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to have a fixed sized input for the model, here I am using maximum length as 20. Try with different values to find the best one. Usually a smaller value is recommended since it makes the input less sparse when padded with zeros.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To map the words to unique integer values, we will be using keras Tokenizer.\n\nKeras Tokenizer can be used to get the sequence for each tweet. It maps each word to an integer, representing an index of that word in word_index list.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Tokenizer(num_words = vocab_size)\nt.fit_on_texts(data['text'])\n\nword_index = t.word_index\n\ndict(list(word_index.items())[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use this tokenizer later on train and test tweets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take initial 7500 examples for training and validation, remaining for testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[:7500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's build a sequential model using keras.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length = max_len))\nmodel.add(LSTM(80))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nnadam = Nadam(learning_rate=0.0001)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer=nadam, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\nX = df['text']\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\n# train model on 5 folds\nfor train_index, test_index in skf.split(X, y):\n    \n    train_x, test_x = X[train_index], X[test_index]\n    train_y, test_y = y[train_index], y[test_index]\n    print(\"Tweet before tokenization: \", train_x.iloc[0])\n    \n    #Tokenize the tweets using tokenizer.\n    train_tweets = t.texts_to_sequences(train_x)\n    test_tweets = t.texts_to_sequences(test_x)\n    print(\"Tweet after tokenization: \", train_tweets[0])\n    \n    #pad the tokenized tweet data\n    train_tweets_padded = pad_sequences(train_tweets, maxlen=max_len, padding='post', truncating='post')\n    test_tweets_padded = pad_sequences(test_tweets, maxlen=max_len, padding='post', truncating='post')\n    print('Tweet after padding: ', train_tweets_padded[0])\n    \n    #train model on processed tweets\n    history = model.fit(train_tweets_padded, train_y, epochs=5, validation_data = (test_tweets_padded,test_y))\n    \n    #make predictions\n    pred_y = model.predict_classes(test_tweets_padded)\n    print(\"Validation accuracy : \",accuracy_score(pred_y, test_y))\n    \n    #store validation accuracy\n    accuracy.append(accuracy_score(pred_y, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation accuracy of the model :\", np.mean(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is trained with validation accuracy of 91%, let's see how it performs on unseen tweets from test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = data[7501:]\n\ntokenized_tweets = t.texts_to_sequences(test_df['text'])\npadded_tweets = pad_sequences(tokenized_tweets, maxlen=max_len, padding='post', truncating='post')\ntest_y = test_df['target']\npred_y = model.predict_classes(padded_tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(pred_y, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We acheived 92% test accuracy!!ðŸŽ‰","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"References:\n\nhttps://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n\nhttps://www.youtube.com/watch?v=j7EB7yeySDw","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}