{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport string, re\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud\nfrom keras.preprocessing import text, sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\ntf.random.set_seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv', index_col='id')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first check the distributions of fake and real disaster tweets. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x=\"target\", y=\"target\", data=train_df, estimator=lambda x: len(x) / len(train_df.index) * 100)\nax.set(ylabel=\"Percent\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the distributions are pretty even, so the minimum performance should be approx 53% accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.keyword.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that we see that some of the keywords have '%20' in place of spaces, so we replace these later by spaces.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we import a set of stopwords such as 'the' and 'a' which can be removed from tweets, as well as punctuation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\n# add punctuation to the list of stopwords\npunctuation = list(string.punctuation)\nstop.update(punctuation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We wish to remove html links, any words between square brackets, urls, %20's, and stopwords from the text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_url(text):\n    return re.sub(r'http\\S+', '', text)\n\ndef add_space(text):\n    return re.sub('%20', ' ', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = add_space(text)\n    text = remove_url(text)\n    text = remove_stopwords(text)\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)As part of preprocessing, we add the location column and combine the keywords with the text into one big text column. Then we apply the denoise function to the text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df):\n    df = df.fillna(\"\")\n    df['text'] = df['location'] + \" \" + df['keyword'] + \" \" + df['text']\n    del df['keyword']\n    del df['location']\n    df['text'] = df['text'].apply(denoise_text)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = preprocess_df(train_df)\ntest_df = preprocess_df(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to choose a model architecture, we split the training data into train and dev sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_dev, y_train, y_dev = train_test_split(train_df.text.values, train_df.target.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set max words and max length hyperparameters\nmax_features = 10000\nmax_len = 300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we tokenise the text into arrays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the tokenizer on the training data\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize and pad each set of texts\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=max_len)\n\ntokenized_dev = tokenizer.texts_to_sequences(X_dev)\nX_dev = sequence.pad_sequences(tokenized_dev, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we create the glove embedding matrix to add to the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove-twitter/glove.twitter.27B.100d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary of words and their feature vectors from the embedding file\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(list(embeddings_index.values()))\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\n# Find dims of embedding matrix\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\n# Randomly initialize the embedding matrix\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# Add each vector to the embedding matrix, corresponding to each token that we set earlier\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, set the key hyperparameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024\nepochs = 15\nembed_size = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We add learning rate reduction to the model to achieve good model fitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model architecture consists of two LSTM layers, followed by a Dense layer and a sigmoid activation function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=max_len, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = False , recurrent_dropout = 0.3 , dropout = 0.3))\nmodel.add(Dense(units=64 , activation = 'relu', kernel_regularizer='l2'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we fit the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size = batch_size , \n                    validation_data = (X_dev,y_dev) , \n                    epochs = epochs , callbacks = [learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see how the model does on the training and development data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Dev Data is - \" , model.evaluate(X_dev,y_dev)[1]*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also track the progress of the loss and accuracy on the train and dev sets over each epoch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\nepochs = np.arange(epochs)\nplt.subplot(2, 2, 1)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.plot(epochs, history.history['loss'])\n\nplt.subplot(2, 2, 2)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.plot(epochs, history.history['accuracy'])\n\nplt.subplot(2, 2, 3)\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.plot(epochs, history.history['val_loss'])\n\nplt.subplot(2, 2, 4)\nplt.xlabel('epochs')\nplt.ylabel('val_accuracy')\nplt.plot(epochs, history.history['val_accuracy'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_dev = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_dev, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = model.predict_classes(X_test)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(\n    {'id': list(test_df.index.values),\n     'target': list(classes),\n    }).set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}