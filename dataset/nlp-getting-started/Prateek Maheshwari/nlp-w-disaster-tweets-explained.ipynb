{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a id=\"top_section\"></a>\n\n<div align='center'><font size=\"5\" color=\"#000000\"><b>NLP with disaster tweets!-Starter modelling , data cleaning and explanation <br>(~80% accuracy)</b></font></div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">About the problem</font></div>\n<hr>\n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.<br>\nI have two notebooks on this competition , the first one is using basic naive-bayes model whereas the second is by using BERT pre-trained model. If you're a beginner I highly recommend you to to start with this notebook! After that if you want to enhance your accuracy and read about how we can implement this model using BERT then do check out the second notebook here : <br><br>\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https://www.kaggle.com/friskycodeur/nlp-with-disaster-tweets-bert-explained\" role=\"tab\">NLP with disaster tweets!(BERT explained)</a>\n\n<br>\n<a href=\"https://ibb.co/nm4kTk1\"><img src=\"https://i.ibb.co/54Ccdcj/Aquamarine-and-Orange-Pixel-Games-Collection-You-Tube-Icon.png\" alt=\"Aquamarine-and-Orange-Pixel-Games-Collection-You-Tube-Icon\" border=\"0\" height=300 width=300></a>\n\n\n### Here are the things I will try to cover in this Notebook:\n\n- Basic EDA of the text data.\n- Data cleaning\n- Transforming text into vectors\n- Building our model \n\n### If you like this kernel feel free to upvote and leave feedback, thanks!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc_section\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> Table of Content</h3>\n\n* [Introduction](#top_section)\n* [Importing the Required Libraries and Data](#sec1)\n* [Exploring the Data](#sec2)\n    - [Visualizing given dataset](#sec3)\n* [Text Pre-processing](#sec4)\n    - [Data cleaning](#sec5)\n    - [Using NLP processing](#sec7)\n    - [Stemming](#sec8)\n    - [Frequent words using WordCloud](#sec9)\n* [Transform token in vectors](#sec10)\n    - [Bag of words](#sec11)\n* [Modelling](#sec13)\n* [Submission & Some Last Words](#sectionlst)\n* [References](#sec14)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n## Importing the required libraries and data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us start with importing all the required libraries ! We will use the basic libraries to play with data(numpy,pandas,etc),some text related libraries (re,string,nltk,etc) and various model libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem.snowball import SnowballStemmer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's import our datasets , both train and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ndataset=pd.concat([train,test])\nprint(f'train:{train.shape}\\ntest:{test.shape}\\ndataset:{dataset.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n## Exploring the data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Mind take a sneak peak at our data set ! ;)\n\n<img src='https://media1.tenor.com/images/41597f32f2989333d14515fb1b7a9b4f/tenor.gif?itemid=13480143'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how much of our data is missing !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(train.isnull().sum()[train.isnull().sum()>0]/len(train))*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Test Data Missing':(test.isnull().mean()*100).sort_values(ascending=False)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will deal with the missing data a bit later. But first let's look at some examples of disaster and non-disaster tweets !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"non_dis = train[train.target==0]['text']\nnon_dis.values[7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis=train[train.target==1]['text']\ndis.values[7]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how many disaster and non-disaster tweets are actually there in our data !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n## Visualizing the data !\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have seen how our data is , how much it is missing and some counts, let's visualize our data so that we can to more explore and make better of it!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First let's see the count of disaster and non-disaster tweets !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.barplot(train.target.value_counts().index,train.target.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how much of the keywords were actualy unique ! We will use the nunique function of pandas for this !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the top 15 most used keywords ! Maybe we can get some insights from this !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.keyword.value_counts().index[:15],x=train.keyword.value_counts()[:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some highly used keywords are fatalities , sinking , harm , damage , etc which can actually be very helpful in finding either the given tweet is disaster related or not !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's see the unique locations that the tweets in our dataset were tweeted from !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.location.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the top 15 locations where the most tweets come from !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.location.value_counts().index[:15],x=train.location.value_counts()[:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the places where the least tweets were tweeted from ? Let's find out !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.barplot(y=train.location.value_counts().index[-10:],x=train.location.value_counts()[-10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have seen how some locations have very high tweeting activity whereas some have very low , and how alot of keywords were highly used and how many of them were alot hinting towards the nature of the tweet(i.e disastarious or non-disastarious).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n## Text Pre-processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now comes one of the most important parts of any Natural Language Processing Problem ! Let's clean our data !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://media.tenor.com/images/0bf00f08e5e5cce9bb1ec5899cbc046b/tenor.gif'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec5\"></a>\n### Data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will start with cleaning basic text noises such as URLS , Email IDS , punctautions etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"All the functions are below and quiet basic !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lowercase_text(text):\n    return text.lower()\n\ntrain.text=train.text.apply(lambda x: lowercase_text(x))\ntest.text=test.text.apply(lambda x: lowercase_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_noise(text):\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text=train.text.apply(lambda x: remove_noise(x))\ntest.text=test.text.apply(lambda x: remove_noise(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec7\"></a>\n### Using NLP processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will use NLP preprocessing to process our data ! This actually gave me better results so , let's use it !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nlppreprocess\nfrom nlppreprocess import NLP\n\nnlp = NLP()\n\ntrain['text'] = train['text'].apply(nlp.process)\ntest['text'] = test['text'].apply(nlp.process)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec8\"></a>\n### Stemming","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we have to stem our text , will be using SnowballStemmer as it is quite good for the job ! So let's just get to the code !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n\ntrain['text'] = train['text'].apply(stemming)\ntest['text'] = test['text'].apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec9\"></a>\n### Frequent words using wordcloud","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is just a fun part , I loved this thing i found in one of the notebooks so i added it in mine ! <br>\nThis is a wordcloud of the frequent words in our text and it's actually quite cool to look at !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfig , ax1 = plt.subplots(1,figsize=(12,12))\nwordcloud=WordCloud(background_color='white',width=600,height=600).generate(\" \".join(train.text))\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=24)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src='https://i.gifer.com/EP97.gif'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec10\"></a>\n##  Transform token in vectors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Up until now , we have done all the processing to the texts , but you and I both know that our system cannot really read any language(English in this case) so how do we train it on this data ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Simple we will convert the text data into numerical vectors ! ;) <br>\nFor this we can use two approaches , the first one being Bag-of-Words and the second one being TFIDF.<br>\nFor this model I will be using bag of words !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec11\"></a>\n### Using Bag of words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So let's create our bag of words then ! If you do not know about bag of words , you can read about it here >>\n[BAG OF WORDS](https://machinelearningmastery.com/gentle-introduction-bag-words-model/#:~:text=A%20bag%2Dof%2Dwords%20is,the%20presence%20of%20known%20words.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer=CountVectorizer(analyzer='word',binary=True)\ncount_vectorizer.fit(train.text)\n\ntrain_vec = count_vectorizer.fit_transform(train.text)\ntest_vec = count_vectorizer.transform(test.text)\n\nprint(train_vec[7].todense())\nprint(test_vec[7].todense())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec13\"></a>\n## Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have pre-processed our data , converted it so that our machine can actually process and use it ! <br>\nNow comes the final step , let's get our model ready !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we will store the target data into a variable !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a multinomial Naive Bayes model for this notebook ! You can go ahead and choose your own model as per you like , can also play with this model's parameters so as to increase it's accuracy! But for me this gave a accuracy of around 79.6% ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nmodel =MultinomialNB(alpha=1)\nscores= model_selection.cross_val_score(model,train_vec,y,cv=6,scoring='f1')\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's train our model !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_vec,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sectionlst\"></a>\n#  Submission\n\n<a href=\"#toc_section\" class=\"btn btn-primary\" style=\"color:white;\" >Back to Table of Content</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will use the sample_submission csv file as reference and fill the target column with our predictions !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fill the target column !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.target= model.predict(test_vec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mind taking a sneak-peak? :P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally ,let's convert our predictions into .csv file and submit it !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do you want to increase your accuracy ? Do you want to know how to get to 84-85 % accuracy ? Do you want to know how BERT can help attain that accuract? Do you want to know if it is possible to get to 100% accuracy ?If yes , then Check out my other notebook on the same problem here :\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https://www.kaggle.com/friskycodeur/nlp-with-disaster-tweets-bert-explained\" role=\"tab\">NLP with disaster tweets!(BERT explained)</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec14\"></a>\n#  References","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- [Basic EDA,Cleaning and GloVe](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove)\n- [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n- [Disaster NLP: Keras BERT using TFHub](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://i.pinimg.com/originals/2f/08/84/2f088410e696203853ecf91a3fbcd0f4.gif'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Some last words:\n\nThank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"3\" color=\"#000000\"><b>And again please star if you liked this notebook so it can reach more people, Thanks!</b></font></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media1.giphy.com/media/j2ersR5s9rDnUpMDBI/giphy.gif\" alt=\"Thank you!\" width=\"500\" height=\"600\">","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}