{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div style=\"height:200px;\">\n    <img src=\"https://storage.googleapis.com/kaggle-media/competitions/nlp1-cover.jpg\"/>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Intro</center></h3>"},{"metadata":{},"cell_type":"markdown","source":"# Goal"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to learn what is the Naive Bayes algorithm by implementing from the scratch the sklearn MultinomialNB class."},{"metadata":{},"cell_type":"markdown","source":"# Definition"},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes is a classification model based on Bayes Theorem.\nBayes Theorem universal example is to classify email messages between spam and ham.\n\nThe equations of Naive Bayes for this 'Real or Not?' is:\n\nP(real | text) = (P(text | real) * P(real)) / P(text)<br>\nP(fake | text) = (P(text | fake) * P(fake)) / P(text)"},{"metadata":{},"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Data</center></h3>"},{"metadata":{},"cell_type":"markdown","source":"# Load the data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.utils.validation import check_X_y, check_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/nlp-getting-started/\"\ntrain_df = pd.read_csv(path + \"train.csv\")\nsubmit_df = pd.read_csv(path + \"test.csv\")\n\ny_train = train_df['target']\ny_train.unique()\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create the vocabulary, the word count and prepare the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = []\n_ = [vocabulary.extend(x.split()) for i,x in enumerate(train_df['text'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = np.array(vocabulary)\nvocab = np.unique(vocabulary)\n\nprint(\"Vocab:\", len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(vocabulary=vocab)\nword_counts = vectorizer.fit_transform(train_df.text.to_numpy()).toarray()\n\nX_train = pd.DataFrame(word_counts, columns=vocab).to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Implementation</center></h3>"},{"metadata":{},"cell_type":"markdown","source":"|  Variable      | Math     |  Description                                                      |\n|----------------|----------|-------------------------------------------------------------------|\n| prior          | P(y)     |  Probability of any random selected message belonging to a class  |\n| ik_word        | P(Xi&#124;y)  |  Likelihood of each word, conditional on message class            |\n| lk_message     | P(x&#124;y)   |  Likelihood of an entire message belonging to a particular class  |\n| normalize_term | P(x)     |  Likelihood of an entire message across all possible classes      |"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NaiveBayes():\n    def __init__(self, alpha=1.0):\n        self.prior = None\n        self.word_counts = None\n        self.lk_word = None\n        self.alpha = alpha\n        \n    def fit(self, x, y):\n        '''\n        Fit the features and the labels\n        Calculate prior, word_counts and lk_word\n        '''\n        x, y = check_X_y(x, y)\n        n = x.shape[0]\n        \n        # calculate the prior - number of text belonging to a particular class (real or fake)\n        x_per_class = np.array([x[y == c] for c in np.unique(y)])\n        self.prior = np.array([len(x_class) / n for x_class in x_per_class])\n        \n        # calculate the likelihood for each word 'lk_word'\n        self.word_counts = np.array([sub_arr.sum(axis=0) for sub_arr in x_per_class]) + self.alpha\n        self.lk_word = self.word_counts / self.word_counts.sum(axis=1).reshape(-1, 1)\n        \n        return self\n    \n    def _get_class_numerators(self, x):\n        '''\n        Calculate for each class, the likelihood that an entire message conditional\n        on the message belonging to a particular class (real or fake)\n        '''\n        n, m = x.shape[0], self.prior.shape[0]\n        \n        class_numerators = np.zeros(shape=(n, m))\n        for i, word in enumerate(x):\n            word_exists = word.astype(bool)\n            lk_words_present = self.lk_word[:, word_exists] ** word[word_exists]\n            lk_message = (lk_words_present).prod(axis=1)\n            class_numerators[i] = lk_message * self.prior\n        \n        return class_numerators\n    \n    def _normalized_conditional_probs(self, class_numerators):\n        '''\n        Conditional probabilities = class_numerators / normalize_term\n        '''\n        # normalize term is the likelihood of an entire message (addition of all words in a row)\n        normalize_term = class_numerators.sum(axis=1).reshape(-1,1)\n        conditional_probs = class_numerators / normalize_term\n        assert(conditional_probs.sum(axis=1) - 1 < 0.001).all(), 'rows should sum to 1'\n        \n        return conditional_probs\n    \n    def predict_proba(self, x):\n        '''\n        Return the probabilities for each class (fake or real)\n        '''\n        class_numerators = self._get_class_numerators(x)\n        conditional_probs = self._normalized_conditional_probs(class_numerators)\n        \n        return conditional_probs\n    \n\n    def predict(self, x):\n        '''\n        Return the answer with the highest probability (argmax)\n        '''\n        return self.predict_proba(x).argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NaiveBayes().fit(X_train, y_train).predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Submission</center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word counts for submit using vectorizer from sklearn\nword_counts_submit = vectorizer.fit_transform(submit_df.text.to_numpy()).toarray()\n\nsubmit_x = pd.DataFrame(word_counts_submit, columns=vocab).to_numpy()\n\n# use naive bayes to predict the submission\nresult = NaiveBayes().fit(X_train, y_train).predict(submit_x)\n\nfinal_result = list(map(list, zip(submit_df.id.to_numpy(), result)))\n\nfinal_df = pd.DataFrame(final_result, columns=['id', 'target'])\nfinal_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you and don't forget to up-vote in order to support the community ;)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}