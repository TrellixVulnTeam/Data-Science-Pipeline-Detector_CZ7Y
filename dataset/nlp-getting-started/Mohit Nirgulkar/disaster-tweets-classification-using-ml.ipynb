{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Machine Learning approach to predict real or fake tweets about disaster**\n---","metadata":{"id":"ptkHaiID_EWG","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-09T08:08:42.845821Z","iopub.execute_input":"2021-06-09T08:08:42.846187Z","iopub.status.idle":"2021-06-09T08:10:23.759465Z","shell.execute_reply.started":"2021-06-09T08:08:42.846105Z","shell.execute_reply":"2021-06-09T08:10:23.758484Z"}}},{"cell_type":"markdown","source":"# Project Description\n\n[Twitter](https://twitter.com/?lang=en) has become an important communication channel in times of emergency.   \nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time.    \nBecause of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\n# Objective     \n\n[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is a common use case of [NLP](https://machinelearningmastery.com/natural-language-processing/) where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet.     \nThis problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\nIn this project [Machine Learning](https://www.geeksforgeeks.org/machine-learning/) models are implemented for predicting that a tweet regarding a disaster is real or fake    \nWhole code below is in [Python](https://www.python.org/) using various libraries. Open source library [Scikit-Learn](https://scikit-learn.org/) is used for creating the models.\n\n<p align=\"center\">\n    <br clear=\"right\"/>\n    <img src=\"https://geeklevel1000.com/wp-content/uploads/2019/01/Machine-learning-768x512.jpg\" alt=\"Tweets\" width=\"800\" height=\"1000\" />\n</p>","metadata":{}},{"cell_type":"markdown","source":"#  Table of Contents\n\n\n 1. Dependancies and Dataset\n\n 2. Data Exploration\n \n 3. Data Cleaning\n\n 4. Extra Data Exploration and Analysis on Cleaned Text\n\n 5. Spliting data\n\n 6. Machine Learning Models\n\n 7. Comparing accuracies of all models\n \n 8. Conclusion\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Dependancies and Dataset","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Importing dependancies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\n\n#libraries for NLP\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nfrom IPython.display import HTML\n!pip install chart_studio\nimport plotly\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nplotly.offline.init_notebook_mode(connected=True)\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\nimport plotly.express as px\nfrom collections import defaultdict\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T14:48:41.220652Z","iopub.execute_input":"2021-06-13T14:48:41.221176Z","iopub.status.idle":"2021-06-13T14:48:58.915725Z","shell.execute_reply.started":"2021-06-13T14:48:41.22106Z","shell.execute_reply":"2021-06-13T14:48:58.914635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Reading and preparation of data","metadata":{}},{"cell_type":"markdown","source":"Reading [data](https://www.kaggle.com/c/nlp-getting-started/data) and choosing important columns using [pandas](https://pandas.pydata.org/)","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"id":"maaZD2m7GrYZ","execution":{"iopub.status.busy":"2021-06-13T14:48:58.917517Z","iopub.execute_input":"2021-06-13T14:48:58.917894Z","iopub.status.idle":"2021-06-13T14:48:58.964163Z","shell.execute_reply.started":"2021-06-13T14:48:58.917846Z","shell.execute_reply":"2021-06-13T14:48:58.963241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying first 10 rows of our data using [DataFrame.head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"id":"NluCZIHYHDQU","outputId":"5e7f9692-a804-44e3-f2bf-f91d8de296a4","execution":{"iopub.status.busy":"2021-06-13T14:48:58.966309Z","iopub.execute_input":"2021-06-13T14:48:58.966741Z","iopub.status.idle":"2021-06-13T14:48:58.993346Z","shell.execute_reply.started":"2021-06-13T14:48:58.966698Z","shell.execute_reply":"2021-06-13T14:48:58.992479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concise summarization of total information provided by the data using [DataFrame.info()](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.info.html)","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"id":"j7WlTIr9HE9a","outputId":"efaeacad-dcbf-479b-fe93-feb5ba103831","execution":{"iopub.status.busy":"2021-06-13T14:48:58.99469Z","iopub.execute_input":"2021-06-13T14:48:58.994916Z","iopub.status.idle":"2021-06-13T14:48:59.016537Z","shell.execute_reply.started":"2021-06-13T14:48:58.994894Z","shell.execute_reply":"2021-06-13T14:48:59.015562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only use text and target column of dataset for rest of our work as there lot's of null values inside other columns","metadata":{}},{"cell_type":"code","source":"data = data[['text','target']]\ndata.head()","metadata":{"id":"pYXi7Ltic6iG","outputId":"a0ba0fd6-5162-4946-87b6-e07d4932dce1","execution":{"iopub.status.busy":"2021-06-13T14:48:59.017768Z","iopub.execute_input":"2021-06-13T14:48:59.018055Z","iopub.status.idle":"2021-06-13T14:48:59.032957Z","shell.execute_reply.started":"2021-06-13T14:48:59.018026Z","shell.execute_reply":"2021-06-13T14:48:59.032253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Visualising counts of real and fake tweets","metadata":{}},{"cell_type":"markdown","source":"Let's plot the counts of values under the target column","metadata":{}},{"cell_type":"code","source":"fig = px.bar(x=[\"0\",\"1\"], y=data[\"target\"].value_counts(),color=[\"red\", \"goldenrod\"])\n\n#Change this value for bar widths\nfor dt in fig.data:\n    dt[\"width\"] = 0.4 \n\nfig.update_layout(\n    title_text = \"Counts for Disaster and Non-Disaster Tweets\",\n    title_x=0.5,\n    width=800,\n    height=550,\n    xaxis_title=\"Targets\",\n    yaxis_title=\"Count\",\n    showlegend=False\n).show()\n\n# py.plot(fig,filename='Counts for Disaster and Non-Disaster Tweets',auto_open=False,show_link=False)","metadata":{"id":"K-Ly_g_lHShG","execution":{"iopub.status.busy":"2021-06-13T14:48:59.034017Z","iopub.execute_input":"2021-06-13T14:48:59.034251Z","iopub.status.idle":"2021-06-13T14:49:00.007762Z","shell.execute_reply.started":"2021-06-13T14:48:59.034228Z","shell.execute_reply":"2021-06-13T14:49:00.006761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot shows that our data is quite balanced, you can also click on the plot to explore more about [interactive plots](https://plotly.com/) ","metadata":{}},{"cell_type":"markdown","source":"\n<div>\n    <a href=\"https://plotly.com/~raklugrin01/1/?share_key=hgjA8Zkl35RjZtywNHe0jm\" target=\"_blank\" title=\"Counts for Disaster and Non-Disaster Tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/1.png?share_key=hgjA8Zkl35RjZtywNHe0jm\" alt=\"Counts for Disaster and Non-Disaster Tweets\" style=\"max-width: 100%;width: 1000px;\"  width=\"1000\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:1\" sharekey-plotly=\"hgjA8Zkl35RjZtywNHe0jm\" src=\"https://plotly.com/embed.js\" async></script>\n</div>","metadata":{"id":"q5FZ58sah2jH","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.2 Visualising lengths of tweets","metadata":{}},{"cell_type":"markdown","source":"Analyzing lengths of words in a tweets according to it being real or fake target value by ploting [histograms](https://plotly.com/python/histograms/)","metadata":{}},{"cell_type":"code","source":"from plotly.subplots import make_subplots\n\nword_len_dis = data[data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nword_len_non_dis = data[data['target']==0]['text'].str.split().map(lambda x : len(x))\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n            go.Histogram(x=word_len_dis,marker_line=dict(color='black'),marker_line_width=1.2),\n            row=1, col=1\n).add_trace(\n            go.Histogram(x=word_len_non_dis,marker_line=dict(color='black'),marker_line_width=1.2),\n            row=1, col=2\n).update_layout(title_text=\"Length of words in Tweets\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Length of words in Tweets',auto_open=False,show_link=False)","metadata":{"id":"rph6u8Ewjxra","execution":{"iopub.status.busy":"2021-06-13T14:49:00.008939Z","iopub.execute_input":"2021-06-13T14:49:00.009204Z","iopub.status.idle":"2021-06-13T14:49:00.132369Z","shell.execute_reply.started":"2021-06-13T14:49:00.009177Z","shell.execute_reply":"2021-06-13T14:49:00.131458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot we can say that the number of words in the tweets ranges from 2 to 30 in both cases","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/3/?share_key=c65IIAyuBQBfgU1Rfovdfb\" target=\"_blank\" title=\"Length of words in Tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/3.png?share_key=c65IIAyuBQBfgU1Rfovdfb\" alt=\"Length of words in Tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:3\" sharekey-plotly=\"c65IIAyuBQBfgU1Rfovdfb\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"xM2GJvWTmOSh","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.3 Visualising average word lengths of tweets","metadata":{}},{"cell_type":"markdown","source":"Checking average word length for both type of tweets","metadata":{}},{"cell_type":"code","source":"def avgwordlen(strlist):\n    sum=[]\n    for i in strlist:\n        sum.append(len(i))\n    return sum\n\navgword_len_dis = data[data['target']==1]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\navgword_len_non_dis = data[data['target']==0]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\ngroup_labels = ['Disaster', 'Non-Disaster']\ncolors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n\nfig = ff.create_distplot([avgword_len_dis, avgword_len_non_dis], group_labels, bin_size=.2, colors=colors,)\n\nfig.update_layout(title_text=\"Average word length in tweets\",title_x=0.5,xaxis_title=\"Text\",yaxis_title=\"Density\").show()\n\n# py.plot(fig,filename='Average word length in tweets',auto_open=False,show_link=False)","metadata":{"id":"FV1c2BhymfUN","execution":{"iopub.status.busy":"2021-06-13T14:49:00.134998Z","iopub.execute_input":"2021-06-13T14:49:00.135612Z","iopub.status.idle":"2021-06-13T14:49:00.75537Z","shell.execute_reply.started":"2021-06-13T14:49:00.135566Z","shell.execute_reply":"2021-06-13T14:49:00.754469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the [distplot](https://plotly.com/python/distplot/), average word countss for real disaster tweets are found to be in the range(5-7.5)                 \nwhile for fake disaster tweets are in the range of (4-6).","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/5/?share_key=tfNQPMyUblqOh7JL1sEiqW\" target=\"_blank\" title=\"Average word length in tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/5.png?share_key=tfNQPMyUblqOh7JL1sEiqW\" alt=\"Average word length in tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:5\" sharekey-plotly=\"tfNQPMyUblqOh7JL1sEiqW\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"WsevjsJpqbAU","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.4 Visualising most common stop words in the text data","metadata":{}},{"cell_type":"markdown","source":"\n### What is a corpus?\n\nIn linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts.   \nSuch collections may be formed of a single language of texts, or can span multiple languages\n   \nFunction for creating sample [corpus](https://21centurytext.wordpress.com/home-2/special-section-window-to-corpus/what-is-corpus/) for further analysis.    ","metadata":{}},{"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for i in data[data['target']==target]['text'].str.split():\n        for x in i:\n            corpus.append(x)\n    return corpus","metadata":{"id":"rtAFdb6xpDK9","execution":{"iopub.status.busy":"2021-06-13T14:49:00.75742Z","iopub.execute_input":"2021-06-13T14:49:00.758029Z","iopub.status.idle":"2021-06-13T14:49:00.762899Z","shell.execute_reply.started":"2021-06-13T14:49:00.757988Z","shell.execute_reply":"2021-06-13T14:49:00.761882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What are stopwords?\n\nIn computing, stop words are words that are filtered out before or after the natural language data (text) are processed.       \nWhile “stop words” typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.  \n\nAnalysing most occuring [stop words](https://en.wikipedia.org/wiki/Stop_word) in the text using corpus creating function(create_corpus)","metadata":{}},{"cell_type":"code","source":"values_list = []\n\ndef analyze_stopwords(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        dic[word] += 1\n    \n    top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n    x_items,y_values = zip(*top)\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing stopwords for 0 and 1 target labels\nanalyze_stopwords(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n      go.Bar(x=values_list[1],y=values_list[0],orientation='h',marker=dict(color= 'rgba(152, 255, 74,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[3],y=values_list[2],orientation='h',marker=dict(color= 'rgba(255, 143, 92,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top stop words in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top stop words in the text',auto_open=False,show_link=False)","metadata":{"id":"mAAtFo91xRBw","execution":{"iopub.status.busy":"2021-06-13T14:49:00.764397Z","iopub.execute_input":"2021-06-13T14:49:00.764816Z","iopub.status.idle":"2021-06-13T14:49:00.873351Z","shell.execute_reply.started":"2021-06-13T14:49:00.764779Z","shell.execute_reply":"2021-06-13T14:49:00.872643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The [Bar Charts](https://plotly.com/python/bar-charts/) displays the top 10 stop words in tweets where **'the'** is most frequent in both groups","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/13/?share_key=icoxxtajqMGbKIizrTLUX0\" target=\"_blank\" title=\"Top stop words in the text\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/13.png?share_key=icoxxtajqMGbKIizrTLUX0\" alt=\"Top stop words in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:13\" sharekey-plotly=\"icoxxtajqMGbKIizrTLUX0\" src=\"https://plotly.com/embed.js\" async></script>\n</div>","metadata":{"id":"zPUJ70ZnqipE","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.5 Visualising most common punctuations in the text data","metadata":{}},{"cell_type":"markdown","source":"Now let's have a look at the punctuations inside our data","metadata":{}},{"cell_type":"code","source":"#The above Bar Charts displays the top 10 stop words in tweets where the occurs the most in both groups\n# Anaysing Punctuations\nfrom string import punctuation\nvalues_list = []\ndef analyze_punctuations(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in punctuation:\n            dic[word] += 1 \n    x_items, y_values = zip(*dic.items())\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing punctuations for 0 and 1 target labels\nanalyze_punctuations(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n  \nfig.add_trace(\n      go.Bar(x=values_list[0],y=values_list[1],\n             marker=dict(color= 'rgba(196, 94, 255,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[2],y=values_list[3],\n             marker=dict(color= 'rgba(255, 163, 102,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top Punctuations in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top Punctuations in the text',auto_open=False,show_link=False)","metadata":{"id":"tabTI99y9FVa","execution":{"iopub.status.busy":"2021-06-13T14:49:00.874364Z","iopub.execute_input":"2021-06-13T14:49:00.874758Z","iopub.status.idle":"2021-06-13T14:49:00.946555Z","shell.execute_reply.started":"2021-06-13T14:49:00.874717Z","shell.execute_reply":"2021-06-13T14:49:00.945775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/15/?share_key=9JgPThmm677jJmNjJTc0BZ\" target=\"_blank\" title=\"Top Punctuations in the text\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/15.png?share_key=9JgPThmm677jJmNjJTc0BZ\" alt=\"Top Punctuations in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:15\" sharekey-plotly=\"9JgPThmm677jJmNjJTc0BZ\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"cpQiMex1qqbg","_kg_hide-input":true}},{"cell_type":"markdown","source":"# 3. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Removing unwanted text using regular expressions","metadata":{}},{"cell_type":"markdown","source":"### What is Stemming?  \nStemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.   \nStemming is important in natural language understanding (NLU) and natural language processing (NLP). Here we use SnowballStemmer.\n\nFunction for cleaning the data, we use [RegEx](https://docs.python.org/3/library/re.html) i.e. re python library and [SnowballStemmer()](https://www.nltk.org/_modules/nltk/stem/snowball.html) to stem the words.","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\n\ndef preprocess_data(data):\n    \n    #removal of url\n    text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) \n    \n    #decontraction\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    \n    #removal of html tags\n    text = re.sub(r'<.*?>',' ',text) \n    \n    # Match all digits in the string and replace them by empty string\n    text = re.sub(r'[0-9]', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    \n    # filtering out miscellaneous text.\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n    \n    # remove mentions\n    text = re.sub('@\\S+', '', text)  \n    \n    # remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n    \n\n    # Lowering all the words in text\n    text = text.lower()\n    text = text.split()\n    \n    text = [stemmer.stem(words) for words in text if words not in stopwords.words('english')]\n    \n    # Removal of words with length<2\n    text = [i for i in text if len(i)>2] \n    text = ' '.join(text)\n    return text\n\ndata[\"Cleaned_text\"] = data[\"text\"].apply(preprocess_data)","metadata":{"id":"sckzCfiLegfL","execution":{"iopub.status.busy":"2021-06-13T14:49:00.947626Z","iopub.execute_input":"2021-06-13T14:49:00.947999Z","iopub.status.idle":"2021-06-13T14:49:13.614289Z","shell.execute_reply.started":"2021-06-13T14:49:00.947959Z","shell.execute_reply":"2021-06-13T14:49:13.6133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying Cleaned Data ","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"id":"iwX5VY5bK13S","outputId":"7ecbefb1-5c8c-4e22-aebb-8264be851a15","execution":{"iopub.status.busy":"2021-06-13T14:49:13.615413Z","iopub.execute_input":"2021-06-13T14:49:13.615675Z","iopub.status.idle":"2021-06-13T14:49:13.625238Z","shell.execute_reply.started":"2021-06-13T14:49:13.61565Z","shell.execute_reply":"2021-06-13T14:49:13.624151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Extra Data Exploration and Analysis on Cleaned Text","metadata":{}},{"cell_type":"markdown","source":"## 4.1  Creating function and data for visualising words","metadata":{}},{"cell_type":"markdown","source":"Using the popular [WordCloud](https://www.python-graph-gallery.com/wordcloud/) python library for visulaising the cleaned data","metadata":{}},{"cell_type":"code","source":"def wordcloud(data,title):\n    words = ' '.join(data['Cleaned_text'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords,width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n    \ndata_disaster = data[data['target'] == 1]\ndata_non_disaster = data[data['target'] == 0]","metadata":{"id":"rA860VD7K5zQ","execution":{"iopub.status.busy":"2021-06-13T14:49:13.626375Z","iopub.execute_input":"2021-06-13T14:49:13.626695Z","iopub.status.idle":"2021-06-13T14:49:13.637299Z","shell.execute_reply.started":"2021-06-13T14:49:13.626664Z","shell.execute_reply":"2021-06-13T14:49:13.636353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Visualising words inside Real Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"we can see that most common words in disaster tweets are fire,storm,flood , police etc. ","metadata":{}},{"cell_type":"code","source":"wordcloud(data_disaster,\"Disaster Tweets\")","metadata":{"id":"zH60osphMhQ0","outputId":"9f045705-2762-4571-ebf5-3e946c14e2a4","execution":{"iopub.status.busy":"2021-06-13T14:49:13.638404Z","iopub.execute_input":"2021-06-13T14:49:13.638776Z","iopub.status.idle":"2021-06-13T14:49:14.748612Z","shell.execute_reply.started":"2021-06-13T14:49:13.638736Z","shell.execute_reply":"2021-06-13T14:49:14.747659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Visualising words inside Fake Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"love,new,time etc are the most common words as we can see in wordcloud of Non-disaster tweets","metadata":{}},{"cell_type":"code","source":"wordcloud(data_non_disaster,\"Non-Disaster Tweets\")","metadata":{"id":"6USUwvWKNFXX","outputId":"9c6d292d-3f95-4252-ad04-d85b51e3ba75","execution":{"iopub.status.busy":"2021-06-13T14:49:14.749829Z","iopub.execute_input":"2021-06-13T14:49:14.750135Z","iopub.status.idle":"2021-06-13T14:49:15.878132Z","shell.execute_reply.started":"2021-06-13T14:49:14.750107Z","shell.execute_reply":"2021-06-13T14:49:15.877514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Removing unwanted words with high frequency","metadata":{}},{"cell_type":"markdown","source":"Our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model,    \nresulting in false prediction. Now, we will further remove some words with high frequency from text based on above charts.","metadata":{}},{"cell_type":"code","source":"common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ndata[\"Cleaned_text\"] = data[\"Cleaned_text\"].apply(text_cleaning)","metadata":{"id":"-_-USeyzNoPr","execution":{"iopub.status.busy":"2021-06-13T14:49:15.879263Z","iopub.execute_input":"2021-06-13T14:49:15.879512Z","iopub.status.idle":"2021-06-13T14:49:15.924538Z","shell.execute_reply.started":"2021-06-13T14:49:15.879474Z","shell.execute_reply":"2021-06-13T14:49:15.923737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5 Analysing top 10 N-grams where N is 1,2,3","metadata":{}},{"cell_type":"markdown","source":"### What do you mean by N-grams?  \nN-grams of texts are extensively used in text mining and natural language processing tasks.     They are basically a set of co-occurring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).  \n\nFor example, for the sentence “The cow jumps over the moon”. If N=2 (known as bigrams), then the ngrams would be:  \n* the cow \n* cow jumps \n* jumps over \n* over the \n* the moon\n\nBelow we perform [N-grams](https://en.wikipedia.org/wiki/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.) analysis on cleaned data","metadata":{}},{"cell_type":"code","source":"def top_ngrams(data,n,grams):\n    count_vec = CountVectorizer(ngram_range=(grams,grams)).fit(data)\n    bow = count_vec.transform(data)\n    add_words = bow.sum(axis=0)\n    word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True) \n    return word_freq[:n]","metadata":{"id":"7FrdJlJNQGCa","execution":{"iopub.status.busy":"2021-06-13T14:49:15.925655Z","iopub.execute_input":"2021-06-13T14:49:15.92597Z","iopub.status.idle":"2021-06-13T14:49:15.931254Z","shell.execute_reply.started":"2021-06-13T14:49:15.925884Z","shell.execute_reply":"2021-06-13T14:49:15.930344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating data of top 10 n-grams for n = 1, 2, 3","metadata":{}},{"cell_type":"code","source":"common_uni = top_ngrams(data[\"Cleaned_text\"],10,1)\ncommon_bi = top_ngrams(data[\"Cleaned_text\"],10,2)\ncommon_tri = top_ngrams(data[\"Cleaned_text\"],10,3)\ncommon_uni_df = pd.DataFrame(common_uni,columns=['word','freq'])\ncommon_bi_df = pd.DataFrame(common_bi,columns=['word','freq'])\ncommon_tri_df = pd.DataFrame(common_tri,columns=['word','freq'])","metadata":{"id":"BmAP6cJhXQGU","execution":{"iopub.status.busy":"2021-06-13T14:49:15.932235Z","iopub.execute_input":"2021-06-13T14:49:15.932505Z","iopub.status.idle":"2021-06-13T14:49:16.718654Z","shell.execute_reply.started":"2021-06-13T14:49:15.932465Z","shell.execute_reply":"2021-06-13T14:49:16.717629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6 Visualising top 10 N-grams for N = 1, 2, 3","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=3, cols=1,subplot_titles=(\"Top 20 Unigrams in Text\", \"Top 20 Bigrams in Text\",\"Top 20 Trigrams in Text\"))\n  \nfig.add_trace(\n      go.Bar(x=common_uni_df[\"word\"],y=common_uni_df[\"freq\"],\n             marker=dict(color= 'rgba(255, 170, 59,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=common_bi_df[\"word\"],y=common_bi_df[\"freq\"],\n             marker=dict(color= 'rgba(89, 255, 147,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=2, col=1\n).add_trace(\n      go.Bar(x=common_tri_df[\"word\"],y=common_tri_df[\"freq\"],\n             marker=dict(color= 'rgba(89, 153, 255,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=3, col=1\n).update_layout(title_text=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",\n                title_x=0.5,showlegend=False,width=800,height=1600,).update_xaxes(tickangle=-90).show()\n\n# py.plot(fig,filename='Visualization of Top 20 Unigrams, Bigrams and Trigrams',auto_open=False,show_link=False)","metadata":{"id":"OzX7m9cHWg4c","execution":{"iopub.status.busy":"2021-06-13T15:25:46.079766Z","iopub.execute_input":"2021-06-13T15:25:46.080698Z","iopub.status.idle":"2021-06-13T15:25:46.13746Z","shell.execute_reply.started":"2021-06-13T15:25:46.080635Z","shell.execute_reply":"2021-06-13T15:25:46.136403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/17/?share_key=rHBUmASeWITErHR7rEdZqJ\" target=\"_blank\" title=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/17.png?share_key=rHBUmASeWITErHR7rEdZqJ\" alt=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:17\" sharekey-plotly=\"rHBUmASeWITErHR7rEdZqJ\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"GvvSq0MZq98J","_kg_hide-input":true}},{"cell_type":"markdown","source":"# 5. Data Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Spliting original data after cleaning ","metadata":{}},{"cell_type":"code","source":"X_inp_clean = data['Cleaned_text']\nX_inp_original = data['text']\ny_inp = data['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:16.777074Z","iopub.execute_input":"2021-06-13T14:49:16.777375Z","iopub.status.idle":"2021-06-13T14:49:16.782244Z","shell.execute_reply.started":"2021-06-13T14:49:16.777343Z","shell.execute_reply":"2021-06-13T14:49:16.781191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using [scikit-learn's train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the data into training and validation dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_inp_clean, y_inp, test_size=0.2, random_state=42, stratify=y_inp)\ny_train = np.array(y_train)\ny_valid = np.array(y_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:16.783603Z","iopub.execute_input":"2021-06-13T14:49:16.784071Z","iopub.status.idle":"2021-06-13T14:49:16.801924Z","shell.execute_reply.started":"2021-06-13T14:49:16.784027Z","shell.execute_reply":"2021-06-13T14:49:16.801158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"checking size of data after train test split","metadata":{}},{"cell_type":"code","source":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:16.805628Z","iopub.execute_input":"2021-06-13T14:49:16.806048Z","iopub.status.idle":"2021-06-13T14:49:16.811544Z","shell.execute_reply.started":"2021-06-13T14:49:16.806005Z","shell.execute_reply":"2021-06-13T14:49:16.810686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Creating function to encode data using BoW or TF-IDF","metadata":{}},{"cell_type":"markdown","source":"### What is BoW?   \nBoW stands for \"*bag of words*\" which is a representation of text that describes the occurrence of words within a document.   \nWe just keep track of word counts and disregard the grammatical details and the word order.   \nIt is called a “bag” of words because any information about the order or structure of words in the document is discarded. \nThe model is only concerned with whether known words occur in the document, not where in the document.\n    \n    \n### What is TF-IDF?\nTF-IDF which means Term Frequency and Inverse Document Frequency, is a scoring measure widely used in information retrieval (IR) or summarization.     \nTF-IDF is intended to reflect how relevant a term is in a given document. It is a technique in Natural Language Processing for converting words in Vectors and with some semantic information and it gives weighted to uncommon words , used in various NLP applications.    \n\nFor [BoW](https://www.mygreatlearning.com/blog/bag-of-words/) approach we use scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and for [TF-IDF](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76) we use [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)","metadata":{}},{"cell_type":"code","source":"def encoding(train_data,valid_data,bow=False,n=1,tf_idf=False):\n    if bow==True:\n        cv = CountVectorizer(ngram_range=(n,n))\n        cv_df_train = cv.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(cv_df_train,columns=cv.get_feature_names())\n        cv_df_valid = cv.transform(valid_data).toarray()\n        valid_df = pd.DataFrame(cv_df_valid,columns=cv.get_feature_names())\n        \n    elif tf_idf==True:\n        \n        tfidf = TfidfVectorizer(ngram_range=(n, n), use_idf=1,smooth_idf=1,sublinear_tf=1)    \n        tf_df_train = tfidf.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names())\n        tf_df_valid = tfidf.transform(valid_data).toarray()\n        valid_df = pd.DataFrame(tf_df_valid,columns=tfidf.get_feature_names())\n        \n    return train_df,valid_df ","metadata":{"id":"-Cw5J87AnyN4","execution":{"iopub.status.busy":"2021-06-13T14:49:16.814169Z","iopub.execute_input":"2021-06-13T14:49:16.814729Z","iopub.status.idle":"2021-06-13T14:49:16.823717Z","shell.execute_reply.started":"2021-06-13T14:49:16.814681Z","shell.execute_reply":"2021-06-13T14:49:16.822547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Encoding training and validation data","metadata":{}},{"cell_type":"markdown","source":"We encode our data in all possible combinations provided by our function","metadata":{}},{"cell_type":"code","source":"X_train_bow1 , X_valid_bow1 = encoding(X_train,X_valid,bow=True) \nX_train_bow2 , X_valid_bow2 = encoding(X_train,X_valid,bow=True,n=2) \nX_train_bow3 , X_valid_bow3 = encoding(X_train,X_valid,bow=True,n=3) \nX_train_tfidf1 , X_valid_tfidf1 = encoding(X_train,X_valid,tf_idf=True) \nX_train_tfidf2 , X_valid_tfidf2 = encoding(X_train,X_valid,tf_idf=True,n=2) \nX_train_tfidf3 , X_valid_tfidf3 = encoding(X_train,X_valid,tf_idf=True,n=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:16.824786Z","iopub.execute_input":"2021-06-13T14:49:16.825053Z","iopub.status.idle":"2021-06-13T14:49:18.900756Z","shell.execute_reply.started":"2021-06-13T14:49:16.825028Z","shell.execute_reply":"2021-06-13T14:49:18.899721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Training and tuning Machine Learining Models","metadata":{}},{"cell_type":"markdown","source":"### What is a classification report?\nA Classification report is used to measure the quality of predictions from a classification algorithm.   \nThe report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. \n### What is a confusion matrix?\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.  \n\n#### In a confusion matrix there are 4 basic terminologies :\n* **true positives (TP)** : We predicted yes (they are real tweets), and they are actually real.\n* **true negatives (TN)** : We predicted no, and they are fake.\n* **false positives (FP)**: We predicted yes, but they are't actually real. (Also known as a \"Type I error.\")\n* **false negatives (FN)**: We predicted no, but they are real. (Also known as a \"Type II error.\")\n\nNow let's create functions to display model's [classification report](https://datascience.stackexchange.com/questions/64441/how-to-interpret-classification-report-of-scikit-learn) and [confusion matrix](https://machinelearningmastery.com/confusion-matrix-machine-learning/)","metadata":{}},{"cell_type":"code","source":"def c_report(y_true,y_pred):\n    print(\"Classifictaion Report\")\n    print(classification_report(y_true, y_pred))\n    acc_scr = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_scr))\n    return acc_scr\n\ndef plot_cm(y_true,y_pred,cmap = \"Blues\"):\n    mtx = confusion_matrix(y_true, y_pred)\n    sns.heatmap(mtx, annot = True, fmt='d', linewidth=0.5,\n               cmap=cmap, cbar = False)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:18.901959Z","iopub.execute_input":"2021-06-13T14:49:18.902264Z","iopub.status.idle":"2021-06-13T14:49:18.907979Z","shell.execute_reply.started":"2021-06-13T14:49:18.902232Z","shell.execute_reply":"2021-06-13T14:49:18.906844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.1 Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"### About Logistic Regression   \n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\nNow let's create a [Logistic Regression](https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640) model and train it.","metadata":{}},{"cell_type":"code","source":"model_bow1_logreg = LogisticRegression()\nmodel_bow1_logreg.fit(X_train_bow1,y_train)\npred_bow1_logreg = model_bow1_logreg.predict(X_valid_bow1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:18.909205Z","iopub.execute_input":"2021-06-13T14:49:18.909509Z","iopub.status.idle":"2021-06-13T14:49:24.489206Z","shell.execute_reply.started":"2021-06-13T14:49:18.909462Z","shell.execute_reply":"2021-06-13T14:49:24.487743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=1) model","metadata":{}},{"cell_type":"code","source":"acc_bow1_logreg = c_report(y_valid,pred_bow1_logreg)\nplot_cm(y_valid,pred_bow1_logreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:24.490742Z","iopub.execute_input":"2021-06-13T14:49:24.49113Z","iopub.status.idle":"2021-06-13T14:49:24.639395Z","shell.execute_reply.started":"2021-06-13T14:49:24.491086Z","shell.execute_reply":"2021-06-13T14:49:24.63841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now training another Logistic Regression model with n-grams=2 and BoW ","metadata":{}},{"cell_type":"code","source":"model_bow2_logreg = LogisticRegression()\nmodel_bow2_logreg.fit(X_train_bow2,y_train)\npred_bow2_logreg = model_bow2_logreg.predict(X_valid_bow2)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:24.640815Z","iopub.execute_input":"2021-06-13T14:49:24.641418Z","iopub.status.idle":"2021-06-13T14:49:35.236675Z","shell.execute_reply.started":"2021-06-13T14:49:24.641376Z","shell.execute_reply":"2021-06-13T14:49:35.235325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=2) model","metadata":{}},{"cell_type":"code","source":"acc_bow2_logreg = c_report(y_valid,pred_bow2_logreg)\nplot_cm(y_valid,pred_bow2_logreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:35.238335Z","iopub.execute_input":"2021-06-13T14:49:35.239019Z","iopub.status.idle":"2021-06-13T14:49:35.384853Z","shell.execute_reply.started":"2021-06-13T14:49:35.238968Z","shell.execute_reply":"2021-06-13T14:49:35.383826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that as n is increasing model accuracy is decreasing   \nlet's try to increase n one last time just to be sure","metadata":{}},{"cell_type":"code","source":"model_bow3_logreg = LogisticRegression()\nmodel_bow3_logreg.fit(X_train_bow3,y_train)\npred_bow3_logreg = model_bow3_logreg.predict(X_valid_bow3)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:35.386424Z","iopub.execute_input":"2021-06-13T14:49:35.387103Z","iopub.status.idle":"2021-06-13T14:49:44.810305Z","shell.execute_reply.started":"2021-06-13T14:49:35.387048Z","shell.execute_reply":"2021-06-13T14:49:44.808916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=3) model","metadata":{}},{"cell_type":"code","source":"acc_bow3_logreg = c_report(y_valid,pred_bow3_logreg)\nplot_cm(y_valid,pred_bow3_logreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:44.816268Z","iopub.execute_input":"2021-06-13T14:49:44.820552Z","iopub.status.idle":"2021-06-13T14:49:45.014041Z","shell.execute_reply.started":"2021-06-13T14:49:44.820469Z","shell.execute_reply":"2021-06-13T14:49:45.012815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above results it's clear that using n = 1 will always give us more accuray,  \nnow let's use tfidf approach with n = 1 to train our Logistic Regression model","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:11:24.298095Z","iopub.execute_input":"2021-06-09T08:11:24.298447Z","iopub.status.idle":"2021-06-09T08:11:24.306226Z","shell.execute_reply.started":"2021-06-09T08:11:24.29841Z","shell.execute_reply":"2021-06-09T08:11:24.305086Z"}}},{"cell_type":"code","source":"model_tfidf1_logreg = LogisticRegression(C=1.0)\nmodel_tfidf1_logreg.fit(X_train_tfidf1,y_train)\npred_tfidf1_logreg = model_tfidf1_logreg.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:45.01931Z","iopub.execute_input":"2021-06-13T14:49:45.022745Z","iopub.status.idle":"2021-06-13T14:49:47.771248Z","shell.execute_reply.started":"2021-06-13T14:49:45.022632Z","shell.execute_reply":"2021-06-13T14:49:47.770143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(TF-IDF,n-grams=1) model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_logreg = c_report(y_valid,pred_tfidf1_logreg)\nplot_cm(y_valid,pred_tfidf1_logreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:47.772677Z","iopub.execute_input":"2021-06-13T14:49:47.773237Z","iopub.status.idle":"2021-06-13T14:49:47.904326Z","shell.execute_reply.started":"2021-06-13T14:49:47.773193Z","shell.execute_reply":"2021-06-13T14:49:47.903645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From Logistic Regression we saw n-grams = 1 gives the best results ","metadata":{}},{"cell_type":"markdown","source":"## 6.2 Multinomial Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"### About Multinomial Naive Bayes  \n\nMultinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\n\nNaive Bayes classifier is a collection of many algorithms where all the algorithms share one common principle, and that is each feature being classified is not related to any other feature. The presence or absence of a feature does not affect the presence or absence of the other feature.\n    \nNow let's create [MultinomialNB](https://www.upgrad.com/blog/multinomial-naive-bayes-explained/) model and train it.","metadata":{}},{"cell_type":"code","source":"model_bow1_NB = MultinomialNB(alpha=0.7)\nmodel_bow1_NB.fit(X_train_bow1,y_train)\npred_bow1_NB = model_bow1_NB.predict(X_valid_bow1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:47.905381Z","iopub.execute_input":"2021-06-13T14:49:47.905855Z","iopub.status.idle":"2021-06-13T14:49:49.616297Z","shell.execute_reply.started":"2021-06-13T14:49:47.905824Z","shell.execute_reply":"2021-06-13T14:49:49.615019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix  for the predictions of MultinomialNB(BoW,n-grams=1) model","metadata":{}},{"cell_type":"code","source":"acc_bow1_NB = c_report(y_valid,pred_bow1_NB)\nplot_cm(y_valid,pred_bow1_NB)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:49.617875Z","iopub.execute_input":"2021-06-13T14:49:49.618284Z","iopub.status.idle":"2021-06-13T14:49:49.765997Z","shell.execute_reply.started":"2021-06-13T14:49:49.618243Z","shell.execute_reply":"2021-06-13T14:49:49.764928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a MultinomialNB model and training it with TF-IDF approach","metadata":{}},{"cell_type":"code","source":"model_tfidf1_NB = MultinomialNB(alpha=0.7)\nmodel_tfidf1_NB.fit(X_train_tfidf1,y_train)\npred_tfidf1_NB = model_tfidf1_NB.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:49.767533Z","iopub.execute_input":"2021-06-13T14:49:49.768142Z","iopub.status.idle":"2021-06-13T14:49:50.088038Z","shell.execute_reply.started":"2021-06-13T14:49:49.768093Z","shell.execute_reply":"2021-06-13T14:49:50.086781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions of MultinomialNB(TF-IDF,n-grams=1) model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_NB = c_report(y_valid,pred_tfidf1_NB)\nplot_cm(y_valid,pred_tfidf1_NB)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:50.09Z","iopub.execute_input":"2021-06-13T14:49:50.090519Z","iopub.status.idle":"2021-06-13T14:49:50.23466Z","shell.execute_reply.started":"2021-06-13T14:49:50.090451Z","shell.execute_reply":"2021-06-13T14:49:50.233606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"### About Random Forest Classifier   \n\nRandom forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n\nRandom forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.   \n\nNow let's create a [RandomForestClassifier](https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/) model and train it.","metadata":{}},{"cell_type":"code","source":"model_tfidf1_RFC = RandomForestClassifier()\nmodel_tfidf1_RFC.fit(X_train_tfidf1,y_train)\npred_tfidf1_RFC = model_tfidf1_RFC.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:49:50.236091Z","iopub.execute_input":"2021-06-13T14:49:50.236684Z","iopub.status.idle":"2021-06-13T14:51:16.631585Z","shell.execute_reply.started":"2021-06-13T14:49:50.236639Z","shell.execute_reply":"2021-06-13T14:51:16.630611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for predictions of RandomForestClassifier model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_RFC = c_report(y_valid,pred_tfidf1_RFC)\nplot_cm(y_valid,pred_tfidf1_RFC)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:51:16.632843Z","iopub.execute_input":"2021-06-13T14:51:16.633096Z","iopub.status.idle":"2021-06-13T14:51:16.756947Z","shell.execute_reply.started":"2021-06-13T14:51:16.633065Z","shell.execute_reply":"2021-06-13T14:51:16.755897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 eXtreme Gradient Boosting Classifier","metadata":{}},{"cell_type":"markdown","source":"### About XGBClassifier\nThe XGBoost stands for eXtreme Gradient Boosting, which is a boosting algorithm based on gradient boosted decision trees algorithm.     \nXGBoost applies a better regularization technique to reduce overfitting, and it is one of the differences from the gradient boosting.    \n\nNow let's create a [XGBClassifier](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/) model and train it.","metadata":{}},{"cell_type":"code","source":"model_tfidf1_XGB = XGBClassifier(eval_metric='mlogloss')\nmodel_tfidf1_XGB.fit(X_train_tfidf1,y_train)\npred_tfidf1_XGB = model_tfidf1_XGB.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:51:16.758871Z","iopub.execute_input":"2021-06-13T14:51:16.759284Z","iopub.status.idle":"2021-06-13T14:52:50.887966Z","shell.execute_reply.started":"2021-06-13T14:51:16.759244Z","shell.execute_reply":"2021-06-13T14:52:50.887229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by the XGBClassifier model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_XGB = c_report(y_valid,pred_tfidf1_XGB)\nplot_cm(y_valid,pred_tfidf1_XGB)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:52:50.889161Z","iopub.execute_input":"2021-06-13T14:52:50.8897Z","iopub.status.idle":"2021-06-13T14:52:51.016682Z","shell.execute_reply.started":"2021-06-13T14:52:50.889659Z","shell.execute_reply":"2021-06-13T14:52:51.015604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 CatBoostClassifier","metadata":{}},{"cell_type":"markdown","source":"### About CatBoostClassifier\n\nCatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.    \n\nIt yields state-of-the-art results without extensive data training typically required by other machine learning methods. \n   \n“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library.      \nGradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n\n\nNow let's create a [CatBoostClassifier](https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html) model with 100 iterations and training it","metadata":{}},{"cell_type":"code","source":"model_tfidf1_CBC = CatBoostClassifier(iterations=100)\nmodel_tfidf1_CBC.fit(X_train_tfidf1,y_train)\npred_tfidf1_CBC = model_tfidf1_CBC.predict(X_valid_tfidf1)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T14:52:51.018767Z","iopub.execute_input":"2021-06-13T14:52:51.019199Z","iopub.status.idle":"2021-06-13T14:53:00.378096Z","shell.execute_reply.started":"2021-06-13T14:52:51.019156Z","shell.execute_reply":"2021-06-13T14:53:00.377158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the predictions made by the above model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_CBC = c_report(y_valid,pred_tfidf1_CBC)\nplot_cm(y_valid,pred_tfidf1_CBC)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:04:50.626969Z","iopub.execute_input":"2021-06-13T15:04:50.627352Z","iopub.status.idle":"2021-06-13T15:04:50.759144Z","shell.execute_reply.started":"2021-06-13T15:04:50.627319Z","shell.execute_reply":"2021-06-13T15:04:50.756817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.6 Support Vector CLassifier ","metadata":{}},{"cell_type":"markdown","source":"### About SVC\nSVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition.\n\nSVM is an exciting algorithm and the concepts are relatively simple. The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier. SVM finds an optimal hyperplane which helps in classifying new data points.   \n\nNow let's create a [SVC](https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/) model and training it","metadata":{}},{"cell_type":"code","source":"model_tfidf1_SVC = SVC(kernel='linear', degree=3, gamma='auto')\nmodel_tfidf1_SVC.fit(X_train_tfidf1,y_train)\npred_tfidf1_SVC = model_tfidf1_SVC.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:53:00.507426Z","iopub.execute_input":"2021-06-13T14:53:00.508039Z","iopub.status.idle":"2021-06-13T14:56:18.02494Z","shell.execute_reply.started":"2021-06-13T14:53:00.507994Z","shell.execute_reply":"2021-06-13T14:56:18.023702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for the SVC model","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_SVC = c_report(y_valid,pred_tfidf1_SVC)\nplot_cm(y_valid,pred_tfidf1_SVC)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:56:18.026962Z","iopub.execute_input":"2021-06-13T14:56:18.027393Z","iopub.status.idle":"2021-06-13T14:56:18.168864Z","shell.execute_reply.started":"2021-06-13T14:56:18.027351Z","shell.execute_reply":"2021-06-13T14:56:18.167873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.7 Voting Classifier","metadata":{}},{"cell_type":"markdown","source":"### About Voting Classifier\n\nA Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on    \ntheir highest probability of chosen class as the output. It simply aggregates the findings of each classifier passed into Voting Classifier    \nand predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n\n#### Voting Classifier supports two types of votings :  \n\n\n* **Hard Voting** : In hard voting, the predicted output class is a class with the highest majority of votes i.e the class    which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n\n\n* **Soft Voting** : In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier\n \n \nNow let's create a [VotingClassifier](https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/) with soft voting and train it ","metadata":{}},{"cell_type":"code","source":"estimators = []\nestimators.append(('LR', \n                  LogisticRegression()))\nestimators.append(('NB', MultinomialNB(alpha=0.7)))\nestimators.append(('XBG', XGBClassifier(eval_metric='mlogloss')))\n\nmodel_tfidf1_VC = VotingClassifier(estimators=estimators,voting='soft')\nmodel_tfidf1_VC.fit(X_train_tfidf1,y_train)\npred_tfidf1_VC = model_tfidf1_VC.predict(X_valid_tfidf1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:01:30.047998Z","iopub.execute_input":"2021-06-13T15:01:30.04832Z","iopub.status.idle":"2021-06-13T15:03:09.129223Z","shell.execute_reply.started":"2021-06-13T15:01:30.04829Z","shell.execute_reply":"2021-06-13T15:03:09.128277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing classification report and ploting confusion matrix for VotingClasssifier","metadata":{}},{"cell_type":"code","source":"acc_tfidf1_VC = c_report(y_valid,pred_tfidf1_VC)\nplot_cm(y_valid,pred_tfidf1_VC,cmap = \"Greens\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:03:13.705864Z","iopub.execute_input":"2021-06-13T15:03:13.706214Z","iopub.status.idle":"2021-06-13T15:03:13.846461Z","shell.execute_reply.started":"2021-06-13T15:03:13.706183Z","shell.execute_reply":"2021-06-13T15:03:13.844137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Comparing the Accuracy of all models","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame([[\"Logistic Regression BoW1\",acc_bow1_logreg],[\"Logistic Regression BoW2\",acc_bow2_logreg],\n                       [\"Logistic Regression BoW3\",acc_bow3_logreg],[\"Logistic Regression Tf-Idf1\",acc_tfidf1_logreg],\n                       [\"Naive Bayes Tf-Idf1\",acc_tfidf1_NB],[\"Random Forest Tf-Idf1\",acc_tfidf1_RFC],\n                       [\"XGBClassifier Tf-Idf1\",acc_tfidf1_XGB],[\"CatBoost Tf-Idf1\",acc_tfidf1_CBC],\n                        [\"SVC Tf-Idf1\",acc_tfidf1_SVC],[\"Voting Tf-Idf1\",acc_tfidf1_VC]],\n                       columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\n\nresults.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:05:09.493007Z","iopub.execute_input":"2021-06-13T15:05:09.493333Z","iopub.status.idle":"2021-06-13T15:05:09.539615Z","shell.execute_reply.started":"2021-06-13T15:05:09.493304Z","shell.execute_reply":"2021-06-13T15:05:09.538482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusion","metadata":{}},{"cell_type":"markdown","source":"Among all Simple classification models used above Voting Classifier performed best with tf-idf and ngrams = 1","metadata":{}},{"cell_type":"code","source":"#lets import test data\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:05:28.257633Z","iopub.execute_input":"2021-06-13T15:05:28.257967Z","iopub.status.idle":"2021-06-13T15:05:33.848607Z","shell.execute_reply.started":"2021-06-13T15:05:28.257937Z","shell.execute_reply":"2021-06-13T15:05:33.847711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)\ntfidf = TfidfVectorizer(ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1)    \ntf_df_data = tfidf.fit_transform(X_inp_clean).toarray()\ndata_df = pd.DataFrame(tf_df_data,columns=tfidf.get_feature_names())\ntf_df_test = tfidf.transform(test['Cleaned_text']).toarray()\ntest_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:05:33.850155Z","iopub.execute_input":"2021-06-13T15:05:33.850572Z","iopub.status.idle":"2021-06-13T15:05:34.361404Z","shell.execute_reply.started":"2021-06-13T15:05:33.850527Z","shell.execute_reply":"2021-06-13T15:05:34.360414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_VC = VotingClassifier(estimators=estimators,voting='soft')\nmodel_VC.fit(data_df,y_inp)\nsubmission = model_VC.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:05:34.363308Z","iopub.execute_input":"2021-06-13T15:05:34.363717Z","iopub.status.idle":"2021-06-13T15:07:54.847961Z","shell.execute_reply.started":"2021-06-13T15:05:34.363674Z","shell.execute_reply":"2021-06-13T15:07:54.847023Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_test = pd.DataFrame(submission)\ntest_id = pd.DataFrame(test[\"id\"])\nsubmission = pd.concat([test_id,predictions_test],axis=1)\nsubmission.columns = [\"id\",\"target\"]\nsubmission.to_csv(\"Submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:07:54.849243Z","iopub.execute_input":"2021-06-13T15:07:54.849558Z","iopub.status.idle":"2021-06-13T15:07:54.863409Z","shell.execute_reply.started":"2021-06-13T15:07:54.849523Z","shell.execute_reply":"2021-06-13T15:07:54.862604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If you like my work then make sure to upvote it , any kind of suggestions are welcome ","metadata":{}}]}