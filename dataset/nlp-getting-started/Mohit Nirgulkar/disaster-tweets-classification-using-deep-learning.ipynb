{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Learning approach to predict real or fake tweets about disaster**\n---","metadata":{"id":"ptkHaiID_EWG","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-10T18:17:05.827889Z","iopub.execute_input":"2021-06-10T18:17:05.828417Z","iopub.status.idle":"2021-06-10T18:17:40.26371Z","shell.execute_reply.started":"2021-06-10T18:17:05.82828Z","shell.execute_reply":"2021-06-10T18:17:40.262629Z"}}},{"cell_type":"markdown","source":"# Project Description\n\n[Twitter](https://twitter.com/?lang=en) has become an important communication channel in times of emergency.   \nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time.    \nBecause of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\n# Objective     \n\n\n[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is a common use case of [NLP](https://machinelearningmastery.com/natural-language-processing/) where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet.     \nThis problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\nIn this project [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) models are implemented for predicting that a tweet regarding a disaster is real or fake    \nWhole code below is in [Python](https://www.python.org/) using various libraries. Open source library [Tensorflow](https://www.tensorflow.org/) and [Transformers](https://huggingface.co/transformers/model_doc/auto.html) are used for creating the models and tuning them with [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner).\n<p align=\"center\">\n    <br clear=\"right\"/>\n    <img src=\"https://cdn-images-1.medium.com/max/1018/1*I5O6NX_DIKYI1VBuLfX77Q.jpeg\" alt=\"Tweets\" width=\"800\" height=\"1000\" />\n</p>","metadata":{}},{"cell_type":"markdown","source":"#  Table of Contents\n\n1. Dependancies and Dataset\n\n2. Data Exploration\n\n3. Data Cleaning\n\n4. Extra Data Exploration and Analysis with Cleaned Text\n\n5. Data Preprocessing and Creating Word Embedding Matrix\n\n6. Training and Tuning Deep Learning Models\n\n7. Conclusion","metadata":{}},{"cell_type":"markdown","source":"# 1. Dependancies and Dataset","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Importing dependancies","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import *\nfrom keras.layers import *\nfrom keras.models import Sequential,Model\nimport kerastuner as kt\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom transformers import AutoConfig, AutoModel\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer,LancasterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n!pip install chart_studio\nfrom IPython.display import HTML\nimport plotly\nimport cufflinks\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\n\nplotly.offline.init_notebook_mode(connected=True)\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nfrom string import punctuation\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"ptkHaiID_EWG","_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Reading and preparation of data","metadata":{}},{"cell_type":"markdown","source":"Reading [data](https://www.kaggle.com/c/nlp-getting-started/data) and choosing important columns using [pandas](https://pandas.pydata.org/)","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"id":"maaZD2m7GrYZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying first 10 rows of our data using [DataFrame.head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"id":"NluCZIHYHDQU","outputId":"5e7f9692-a804-44e3-f2bf-f91d8de296a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concise summarization of total information provided by the data using [DataFrame.info()](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.info.html)","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"id":"j7WlTIr9HE9a","outputId":"efaeacad-dcbf-479b-fe93-feb5ba103831","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only use text and target column of dataset for rest of our work as there lot's of null values inside other columns","metadata":{}},{"cell_type":"code","source":"data = data[['text','target']]\ndata.head()","metadata":{"id":"pYXi7Ltic6iG","outputId":"a0ba0fd6-5162-4946-87b6-e07d4932dce1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Visualising counts of real and fake tweets","metadata":{}},{"cell_type":"markdown","source":"Let's plot the counts of values under the target column","metadata":{}},{"cell_type":"code","source":"fig = px.bar(x=[\"0\",\"1\"], y=data[\"target\"].value_counts(),\n             color=[\"red\", \"goldenrod\"])\n\n#Change this value for bar widths\nfor dt in fig.data:\n    dt[\"width\"] = 0.4 \n\nfig.update_layout(\n    title_text = \"Counts for Disaster and Non-Disaster Tweets\",\n    title_x=0.5,\n    width=800,\n    height=550,\n    xaxis_title=\"Targets\",\n    yaxis_title=\"Count\",\n    showlegend=False\n).show()\n\n# py.plot(fig,filename='Counts for Disaster and Non-Disaster Tweets',auto_open=False,show_link=False)","metadata":{"id":"K-Ly_g_lHShG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot shows that our data is quite balanced, you can also click on the plot to explore more about [interactive plots](https://plotly.com/)","metadata":{}},{"cell_type":"markdown","source":"\n<div>\n    <a href=\"https://plotly.com/~raklugrin01/1/?share_key=hgjA8Zkl35RjZtywNHe0jm\" target=\"_blank\" title=\"Counts for Disaster and Non-Disaster Tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/1.png?share_key=hgjA8Zkl35RjZtywNHe0jm\" alt=\"Counts for Disaster and Non-Disaster Tweets\" style=\"max-width: 100%;width: 1000px;\"  width=\"1000\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:1\" sharekey-plotly=\"hgjA8Zkl35RjZtywNHe0jm\" src=\"https://plotly.com/embed.js\" async></script>\n</div>","metadata":{"id":"q5FZ58sah2jH","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.2 Visualising lengths of tweets","metadata":{}},{"cell_type":"markdown","source":"Analyzing lengths of words in a tweets according to it being real or fake target value by ploting [histograms](https://www.investopedia.com/terms/h/histogram.asp#:~:text=A%20histogram%20is%20a%20bar,used%20to%20visualize%20data%20distributions.)","metadata":{}},{"cell_type":"code","source":"word_len_dis = data[data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nword_len_non_dis = data[data['target']==0]['text'].str.split().map(lambda x : len(x))\n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n    \n    go.Histogram(x=word_len_dis,marker_line=dict(color='black'),\n                 marker_line_width=1.2),row=1, col=1\n    \n).add_trace(\n    \n    go.Histogram(x=word_len_non_dis,marker_line=dict(color='black'),\n                 marker_line_width=1.2),row=1, col=2\n    \n).update_layout(title_text=\"Length of words in Tweets\",\n                title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Length of words in Tweets',auto_open=False,show_link=False)","metadata":{"id":"rph6u8Ewjxra","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot we can say that the number of words in the tweets ranges from 2 to 30 in both cases","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/3/?share_key=c65IIAyuBQBfgU1Rfovdfb\" target=\"_blank\" title=\"Length of words in Tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/3.png?share_key=c65IIAyuBQBfgU1Rfovdfb\" alt=\"Length of words in Tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:3\" sharekey-plotly=\"c65IIAyuBQBfgU1Rfovdfb\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"xM2GJvWTmOSh","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.3 Visualising average word lengths of tweets","metadata":{}},{"cell_type":"markdown","source":"Checking average word length for both type of tweets","metadata":{}},{"cell_type":"code","source":"def avgwordlen(strlist):\n    sum=[]\n    for i in strlist:\n        sum.append(len(i))\n    return sum\n\nnon_dis_data = data[data['target']==0]['text'].str.split()\ndis_data = data[data['target']==1]['text'].str.split()\n\navgword_len_dis = dis_data.apply(avgwordlen).map(lambda x: np.mean(x))\navgword_len_non_dis = non_dis_data.apply(avgwordlen).map(lambda x: np.mean(x))\n\ngroup_labels = ['Disaster', 'Non-Disaster']\ncolors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n\nfig = ff.create_distplot([avgword_len_dis, avgword_len_non_dis], \n                         group_labels, bin_size=.2, colors=colors,)\n\nfig.update_layout(title_text=\"Average word length in tweets\",title_x=0.5,\n                  xaxis_title=\"Text\",yaxis_title=\"Density\").show()\n\n# py.plot(fig,filename='Average word length in tweets',auto_open=False,show_link=False)","metadata":{"id":"FV1c2BhymfUN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average word countss for real disaster tweets are found to be in the range(5-7.5)                 \nwhile for fake disaster tweets are in the range of (4-6).","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/5/?share_key=tfNQPMyUblqOh7JL1sEiqW\" target=\"_blank\" title=\"Average word length in tweets\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/5.png?share_key=tfNQPMyUblqOh7JL1sEiqW\" alt=\"Average word length in tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:5\" sharekey-plotly=\"tfNQPMyUblqOh7JL1sEiqW\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"WsevjsJpqbAU","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.4 Visualising most common stop words in the text data","metadata":{}},{"cell_type":"markdown","source":"### What is a corpus?\n\nIn linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts.   \nSuch collections may be formed of a single language of texts, or can span multiple languages\n   \nFunction for creating sample [corpus](https://21centurytext.wordpress.com/home-2/special-section-window-to-corpus/what-is-corpus/) for further analysis.    ","metadata":{}},{"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for i in data[data['target']==target]['text'].str.split():\n        for x in i:\n            corpus.append(x)\n    return corpus","metadata":{"id":"rtAFdb6xpDK9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What are stopwords?\n\nIn computing, stop words are words that are filtered out before or after the natural language data (text) are processed.       \nWhile “stop words” typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.  \n\nAnalysing most occuring [stop words](https://en.wikipedia.org/wiki/Stop_word) in the text using corpus creating function(create_corpus)","metadata":{}},{"cell_type":"code","source":"values_list = []\n\ndef analyze_stopwords(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        dic[word] += 1\n    \n    top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n    x_items,y_values = zip(*top)\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing stopwords for 0 and 1 target labels\nanalyze_stopwords(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n      go.Bar(x=values_list[1],y=values_list[0],orientation='h',marker=dict(color= 'rgba(152, 255, 74,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[3],y=values_list[2],orientation='h',marker=dict(color= 'rgba(255, 143, 92,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top stop words in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top stop words in the text',auto_open=False,show_link=False)","metadata":{"id":"mAAtFo91xRBw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[The Bar Charts](https://plotly.com/python/bar-charts/) displays the top 10 stop words in tweets where **'the'** is most frequent in both groups","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/13/?share_key=icoxxtajqMGbKIizrTLUX0\" target=\"_blank\" title=\"Top stop words in the text\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/13.png?share_key=icoxxtajqMGbKIizrTLUX0\" alt=\"Top stop words in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:13\" sharekey-plotly=\"icoxxtajqMGbKIizrTLUX0\" src=\"https://plotly.com/embed.js\" async></script>\n</div>","metadata":{"id":"zPUJ70ZnqipE","_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 2.5 Visualising most common punctuations in the text data","metadata":{}},{"cell_type":"markdown","source":"Now let's have a look at the punctuations inside our data","metadata":{}},{"cell_type":"code","source":"values_list = []\n\ndef analyze_punctuations(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in punctuation:\n            dic[word] += 1 \n    \n    x_items, y_values = zip(*dic.items())\n    \n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing punctuations for 0 and 1 target labels\nanalyze_punctuations(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n  \nfig.add_trace(\n    \n    go.Bar(x=values_list[0],y=values_list[1],\n           marker=dict(color= 'rgba(196, 94, 255,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=1, col=1\n    \n).add_trace(\n    \n    go.Bar(x=values_list[2],y=values_list[3],\n          marker=dict(color= 'rgba(255, 163, 102,0.8)'),\n          marker_line=dict(color='black'),marker_line_width=1.2),\n          row=1, col=2\n    \n).update_layout(title_text=\"Top Punctuations in the text\",\n                title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top Punctuations in the text',auto_open=False,show_link=False)","metadata":{"id":"tabTI99y9FVa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As observed from the plots the most occuring punctuations in both disaster/non-disaster tweets is '-' (350+) and '|'    \nwhile the least occuring for non-disaster are '%' , '/:' , '_' and for disaster tweets is '=>' , ')'.","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/15/?share_key=9JgPThmm677jJmNjJTc0BZ\" target=\"_blank\" title=\"Top Punctuations in the text\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/15.png?share_key=9JgPThmm677jJmNjJTc0BZ\" alt=\"Top Punctuations in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:15\" sharekey-plotly=\"9JgPThmm677jJmNjJTc0BZ\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"cpQiMex1qqbg","_kg_hide-input":true}},{"cell_type":"markdown","source":"# 3. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Removing unwanted text using regular expressions","metadata":{}},{"cell_type":"markdown","source":"### What is lemmatizing?  \nLemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.   \nLemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n  \nText preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same.    \nActually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n\nFunction for cleaning the data, we use [RegEx](https://docs.python.org/3/library/re.html) i.e. re python library and [WordNetLemmatizer()](https://www.nltk.org/_modules/nltk/stem/wordnet.html).","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n# stemmer = LancasterStemmer()\n\ndef preprocess_data(data):\n    \n    #removal of url\n    text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) \n    \n    #decontraction\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    \n    #removal of html tags\n    text = re.sub(r'<.*?>',' ',text) \n    \n    # Match all digits in the string and replace them by empty string\n    text = re.sub(r'[0-9]', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    \n    # filtering out miscellaneous text.\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n    \n    # remove mentions\n    text = re.sub('@\\S+', '', text)  \n    \n    # remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n    \n\n    # Lowering all the words in text\n    text = text.lower()\n    text = text.split()\n    \n    text = [lemmatizer.lemmatize(words) for words in text if words not in stopwords.words('english')]\n    \n    # Removal of words with length<2\n    text = [i for i in text if len(i)>=2] \n    text = ' '.join(text)\n    return text\n\ndata[\"Cleaned_text\"] = data[\"text\"].apply(preprocess_data)","metadata":{"id":"sckzCfiLegfL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying Cleaned Data ","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"id":"iwX5VY5bK13S","outputId":"7ecbefb1-5c8c-4e22-aebb-8264be851a15","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Extra Data Exploration and Analysis with Cleaned Text","metadata":{}},{"cell_type":"markdown","source":"## 4.1  Creating function and data for visualising words","metadata":{}},{"cell_type":"markdown","source":"Using the popular [WordCloud](https://www.python-graph-gallery.com/wordcloud/) python library for visulaising the cleaned data","metadata":{}},{"cell_type":"code","source":"def wordcloud(data,title):\n    words = ' '.join(data['Cleaned_text'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords,width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n\ndata_disaster = data[data['target'] == 1]\ndata_non_disaster = data[data['target'] == 0]","metadata":{"id":"rA860VD7K5zQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Visualising words inside Real Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"we can see that most common words in disaster tweets are fire,storm,flood , police etc. ","metadata":{}},{"cell_type":"code","source":"wordcloud(data_disaster,\"Disaster Tweets\")","metadata":{"id":"zH60osphMhQ0","outputId":"9f045705-2762-4571-ebf5-3e946c14e2a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" \nnow let's have a look on Non-Disaster tweets","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Visualising words inside Fake Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"love,new,time etc are the most common words in wordcloud of Non-disaster tweets","metadata":{}},{"cell_type":"code","source":"wordcloud(data_non_disaster,\"Non-Disaster Tweets\")","metadata":{"id":"6USUwvWKNFXX","outputId":"9c6d292d-3f95-4252-ad04-d85b51e3ba75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Removing unwanted words with high frequency","metadata":{}},{"cell_type":"markdown","source":"Our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model,    \nresulting in false prediction. Now, we will further remove some words with high frequency from text based on above charts.","metadata":{}},{"cell_type":"code","source":"common_words = ['via','like','build','get','would','one','two','feel',\n                'lol','fuck','take','way','may','first','latest','want',\n                'make','back','see','know','let','look','come','got',\n                'still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ndata[\"Cleaned_text\"] = data[\"Cleaned_text\"].apply(text_cleaning)","metadata":{"id":"-_-USeyzNoPr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5 Analysing top 10 N-grams where N is 1,2,3","metadata":{}},{"cell_type":"markdown","source":"### What do you mean by N-grams?  \nN-grams of texts are extensively used in text mining and natural language processing tasks.     They are basically a set of co-occurring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).  \n\nFor example, for the sentence “The cow jumps over the moon”.    \nIf N=2 (known as bigrams), then the ngrams would be:  \n* the cow \n* cow jumps \n* jumps over \n* over the \n* the moon\n\nBelow we perform [N-grams](https://en.wikipedia.org/wiki/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.) analysis on cleaned data","metadata":{}},{"cell_type":"code","source":"def top_ngrams(data,n,grams):\n    count_vec = CountVectorizer(ngram_range=(grams,grams)).fit(data)\n    bow = count_vec.transform(data)\n    add_words = bow.sum(axis=0)\n    word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True) \n    return word_freq[:n]","metadata":{"id":"7FrdJlJNQGCa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating data of top 10 n-grams for n = 1, 2, 3","metadata":{}},{"cell_type":"code","source":"common_uni = top_ngrams(data[\"Cleaned_text\"],10,1)\ncommon_bi = top_ngrams(data[\"Cleaned_text\"],10,2)\ncommon_tri = top_ngrams(data[\"Cleaned_text\"],10,3)\ncommon_uni_df = pd.DataFrame(common_uni,columns=['word','freq'])\ncommon_bi_df = pd.DataFrame(common_bi,columns=['word','freq'])\ncommon_tri_df = pd.DataFrame(common_tri,columns=['word','freq'])","metadata":{"id":"BmAP6cJhXQGU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6 Visualising top 10 N-grams for N = 1, 2, 3","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=3, cols=1,subplot_titles=(\"Top 20 Unigrams in Text\",\n                    \"Top 20 Bigrams in Text\",\"Top 20 Trigrams in Text\"))\n  \nfig.add_trace(\n    \n    go.Bar(x=common_uni_df[\"word\"],y=common_uni_df[\"freq\"],\n           marker=dict(color= 'rgba(255, 170, 59,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=1, col=1\n\n).add_trace(\n    \n    go.Bar(x=common_bi_df[\"word\"],y=common_bi_df[\"freq\"],\n           marker=dict(color= 'rgba(89, 255, 147,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=2, col=1\n\n).add_trace(\n    \n    go.Bar(x=common_tri_df[\"word\"],y=common_tri_df[\"freq\"],\n           marker=dict(color= 'rgba(89, 153, 255,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=3, col=1\n    \n).update_layout(title_text=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",\n                title_x=0.5,showlegend=False,\n                width=800,height=1600,).update_xaxes(tickangle=-90).show()\n\n# py.plot(fig,filename='Visualization of Top 20 Unigrams, Bigrams and Trigrams',auto_open=False,show_link=False)","metadata":{"id":"OzX7m9cHWg4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <a href=\"https://plotly.com/~raklugrin01/17/?share_key=rHBUmASeWITErHR7rEdZqJ\" target=\"_blank\" title=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"display: block; text-align: center;\"><img src=\"https://plotly.com/~raklugrin01/17.png?share_key=rHBUmASeWITErHR7rEdZqJ\" alt=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https://plotly.com/404.png';\" /></a>\n    <script data-plotly=\"raklugrin01:17\" sharekey-plotly=\"rHBUmASeWITErHR7rEdZqJ\" src=\"https://plotly.com/embed.js\" async></script>\n</div>\n","metadata":{"id":"GvvSq0MZq98J","_kg_hide-input":true}},{"cell_type":"markdown","source":"# 5. Data Preprocessing and Creating Word Embedding Matrix","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Spliting original data after cleaning ","metadata":{}},{"cell_type":"code","source":"#original data after cleaning \nX_inp_clean = data['Cleaned_text']\nX_inp_original = data['text']\ny_inp = data['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Data preprocessing and creating padded sentences","metadata":{}},{"cell_type":"markdown","source":"### What is Tokenization  \n\nTokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units,  \nsuch as individual words or terms. Each of these smaller units are called tokens.  \n\nThe tokens could be words, numbers or punctuation marks. In tokenization, smaller units are created by  \nlocating word boundaries, which are the ending point of a word and the beginning of the next word. \n\nNow we use [keras](https://www.tensorflow.org/api_docs/python/tf/keras) text preprocessing [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to fit on text on Cleaned data","metadata":{}},{"cell_type":"code","source":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(X_inp_clean.values)\nvocab_length = len(word_tokenizer.word_index) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating padded sentences using [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) function","metadata":{}},{"cell_type":"code","source":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(X_inp_clean.values, key=lambda sentence: len(word_tokenize(sentence)))\n\nlength_long_sentence = len(word_tokenize(longest_train))\n\npadded_sentences = pad_sequences(embed(X_inp_clean.values), \n                                 length_long_sentence, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Using Glove word embeddings for creating Embedding Matrix","metadata":{}},{"cell_type":"markdown","source":"### Why Glove?\n\nThe advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words),   \nbut incorporates global statistics (word co-occurrence) to obtain word vectors\n\nCreating Embedding dictionary using [Glove Twitter data](https://www.kaggle.com/danielwillgeorge/glove6b100dtxt)","metadata":{}},{"cell_type":"code","source":"embeddings_dictionary = dict()\n\nembedding_dim = 100\n\nglove_file = open('../input/glove6b100dtxt/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n\nglove_file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating Word Embedding matrix which is a list of all words and their corresponding embeddings.","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    \n    embedding_vector = embeddings_dictionary.get(word)\n    \n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Spliting data into training and validation dataset","metadata":{}},{"cell_type":"markdown","source":"Using [scikit-learn's train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the data into training and validation dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(padded_sentences,\n                                                  y_inp.values,test_size=0.2,random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Training and Tuning Deep Learning Models","metadata":{}},{"cell_type":"markdown","source":"Function to summarise history of train our model","metadata":{}},{"cell_type":"code","source":"def model_history(model_history):\n    fig,(ax1,ax2) =  plt.subplots(1,2,figsize=(12,5))\n    \n    # summarize history for accuracy\n    ax1.plot(model_history.history['accuracy'])\n    ax1.plot(model_history.history['val_accuracy'])\n    ax1.set_title('model accuracy')\n    ax1.set_ylabel('accuracy')\n    ax1.set_xlabel('epoch')\n    ax1.legend(['train', 'test'], loc='upper right')\n\n    # summarize history for loss\n    ax2.plot(model_history.history['loss'])\n    ax2.plot(model_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper right')\n    \n    fig.suptitle(\"Model History\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.1 Training and Tuning Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"### What is a CNN?  \nCNNs are basically just several layers of convolutions with nonlinear activation functions like ReLU or tanh applied to the results.     \nIn a traditional feedforward neural network we connect each input neuron to each output neuron in the next layer.    \nThat’s also called a fully connected layer, or affine layer. In CNNs we don’t do that. Instead, we use convolutions over the input layer to compute the output.   \nThis results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters,   \ntypically hundreds or thousands like the ones showed above, and combines their results.\n\nFunction for creating [Convolutional Neural Network](https://www.tensorflow.org/tutorials/images/cnn)","metadata":{}},{"cell_type":"code","source":"def CNN(hp):\n    \n    model = keras.Sequential()\n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5])\n    \n    model.add(Embedding(vocab_length, 100, weights=[embedding_matrix],\n                                     input_length=length_long_sentence,trainable=False))\n    \n    model.add(Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=200,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5]),\n                                activation='relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(MaxPooling1D(pool_size=2))\n  \n    model.add(Flatten())\n\n    model.add(Dense(units = hp.Int('dense_1',min_value=21,max_value=150,step=14),\n                                   activation='relu'))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1,activation='sigmoid'))\n  \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a tuner using Keras-Tuner(kt) for above function","metadata":{}},{"cell_type":"code","source":"tuner_CNN = kt.Hyperband(CNN,objective='val_accuracy',\n                         max_epochs=15,factor=5,\n                         directory='my_dir',\n                         project_name='DisasterTweets_kt',\n                         overwrite=True)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Searching for a model with highest accuracy and saving it's [hyperparameters](https://deepai.org/machine-learning-glossary-and-terms/hyperparameter)","metadata":{}},{"cell_type":"code","source":"stop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=10)\n\ntuner_CNN.search(X_train, y_train, epochs=15,\n                 validation_data=(X_val,y_val),callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_CNN=tuner_CNN.get_best_hyperparameters(num_trials=1)[0]","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a model using the best hyperparameters and training it using [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) to save  the model at appropriate [epoch](https://radiopaedia.org/articles/epoch-machine-learning)","metadata":{}},{"cell_type":"code","source":"model_CNN = tuner_CNN.hypermodel.build(best_hps_CNN)\n\ncheckpoint = ModelCheckpoint(\n    'model_CNN.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_CNN = model_CNN.fit(X_train, y_train,epochs=50,\n                            validation_data=(X_val,y_val),\n                            callbacks=[checkpoint,stop_early])","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"printing model's [summary](https://www.ibm.com/docs/SSLVMB_24.0.0/spss/tutorials/curveest_modelsummary_virus.html#:~:text=The%20model%20summary%20table%20reports,values%20of%20the%20dependent%20variable.)","metadata":{}},{"cell_type":"code","source":"model_CNN.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ploting model's training and validation history","metadata":{}},{"cell_type":"code","source":"model_history(history_CNN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Training and Tuning Multi-Channel Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"### Why Mulichannel CNN?   \n\nA multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels.   \nThis allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n\nFunction for creating [Multichannel CNN](https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/)","metadata":{}},{"cell_type":"code","source":"def MultichannelCNN(hp): \n    \n    inputs1 = Input(shape=(length_long_sentence,))\n    \n    embedding1 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_sentence, trainable=False)(inputs1) \n    \n    conv1 = Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding1) \n    \n    drop1 = Dropout(0.3)(conv1) \n    \n    pool1 = MaxPooling1D()(drop1) \n    \n    flat1 = Flatten()(pool1)\n    \n    inputs2 = Input(shape=(length_long_sentence,)) \n    \n    embedding2 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_sentence,trainable=False)(inputs2) \n    \n    conv2 = Conv1D(filters=hp.Int('conv_2_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_2_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding2) \n    \n    drop2 = Dropout(0.3)(conv2) \n    \n    pool2 = MaxPooling1D()(drop2) \n    \n    flat2 = Flatten()(pool2) \n    \n    # merge \n    merged = concatenate([flat1, flat2]) \n    \n    dense1 = Dense(units = hp.Int('dense_1',min_value=21,max_value=120,step=14),\n                               activation='relu')(merged)\n    \n    drop4 = Dropout(0.5)(dense1)\n    \n    outputs = Dense(1, activation='sigmoid')(drop4) \n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs) \n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5]) \n    \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a tuner using Keras-Tuner(kt) for above function, searching for best hyperparameters","metadata":{}},{"cell_type":"code","source":"tuner_MCNN = kt.Hyperband(MultichannelCNN,objective='val_accuracy',\n                          max_epochs=15,factor=5,\n                          directory='my_dir',\n                          project_name='DisasterTweetsMCNN_kt',\n                          overwrite=True)\n\nstop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=10)\n\ntuner_MCNN.search([X_train,X_train], y_train, epochs=15,\n                  validation_data=([X_val,X_val], y_val),\n                  callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_MCNN=tuner_MCNN.get_best_hyperparameters(num_trials=1)[0]","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a model for best found hyperparameters and training it","metadata":{}},{"cell_type":"code","source":"model_MCNN = tuner_MCNN.hypermodel.build(best_hps_MCNN)\n\ncheckpoint = ModelCheckpoint(\n    'model_MCNN.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_MCNN = model_MCNN.fit([X_train,X_train], y_train,epochs=50,\n                              validation_data=([X_val,X_val], y_val),\n                              callbacks=[checkpoint,stop_early])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ploting model's summary","metadata":{}},{"cell_type":"code","source":"model_MCNN.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ploting model training and validating history","metadata":{}},{"cell_type":"code","source":"model_history(history_MCNN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Training and Tuning Bidirectinal Long Short-Term Memory (LSTM) networks","metadata":{}},{"cell_type":"markdown","source":"### What are Bidirectional-LSTM networks?  \n\nBidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together.   \nThis structure allows the networks to have both backward and forward information about the sequence at every time step\n   \nUsing bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs \nthis approach   \nfrom unidirectional is that in the LSTM that runs backward you preserve information from the future and using the two hidden states combined  \nyou are able in any point in time to preserve information from both past and future.\n\nFunction for creating [Bidirectional Long Short-Term Memory](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/#:~:text=Bidirectional%20LSTMs%20are%20an%20extension,LSTMs%20on%20the%20input%20sequence.) (LSTM) networks","metadata":{}},{"cell_type":"code","source":"def BiLSTM(hp):\n    model = Sequential()\n    \n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=length_long_sentence,trainable = False))\n    \n    model.add(Bidirectional(CuDNNLSTM(units = hp.Int('dense_1',\n                                      min_value=21,max_value=120,step=14)\n                                      ,return_sequences = True)))\n    \n    model.add(GlobalMaxPool1D())\n   \n    model.add(BatchNormalization())\n    \n    model.add(Dropout(0.2))\n    \n    model.add(Dense(units = hp.Int('dense_1',min_value=21,\n                                   max_value=120,step=14),\n                    activation = \"relu\"))\n    \n    model.add(Dropout(0.3))\n    \n    model.add(Dense(units = hp.Int('dense_1',min_value=21,\n                                   max_value=100,step=14),\n                    activation = \"relu\"))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    hp_learning_rate = hp.Choice('learning_rate', \n                                 values=[3e-2, 3e-3, 3e-4, 3e-5]) \n    \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a tuner using Keras-Tuner(kt) for above function, searching for best hyperparameters","metadata":{}},{"cell_type":"code","source":"tuner_BiLSTM = kt.Hyperband(BiLSTM,objective='val_accuracy',\n                     max_epochs=15,factor=5,\n                     directory='my_dir',\n                     project_name='DisasterTweetsBiLSTM_kt',\n                     overwrite=True)\n\nstop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=12)\n\ntuner_BiLSTM.search(X_train, y_train, epochs=15,\n                    validation_data=(X_val, y_val),\n                    callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_BiLSTM=tuner_BiLSTM.get_best_hyperparameters(num_trials=1)[0]","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a model for best found hyperparameters and training it","metadata":{}},{"cell_type":"code","source":"model_BiLSTM = tuner_BiLSTM.hypermodel.build(best_hps_BiLSTM)\n\ncheckpoint = ModelCheckpoint(\n    'model_BiLSTM.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_BiLSTM = model_BiLSTM.fit(X_train, y_train, epochs=50,\n                                  validation_data=(X_val, y_val),\n                                  callbacks=[checkpoint,stop_early])","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"printing model's summary","metadata":{}},{"cell_type":"code","source":"model_BiLSTM.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ploting training and validation history of the model","metadata":{}},{"cell_type":"code","source":"model_history(history_BiLSTM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Training DistilBert transformer","metadata":{}},{"cell_type":"markdown","source":"Using scikit-learn's [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to [one hot encode](https://www.sciencedirect.com/topics/computer-science/one-hot-encoding) the target column   \nand spliting the data into training and validation set.","metadata":{}},{"cell_type":"code","source":"onehot_encoder = OneHotEncoder(sparse=False)\n\ny = (np.asarray(y_inp)).reshape(-1,1)\n\nY = onehot_encoder.fit_transform(y)\n\nX_train, X_val, y_train, y_val = train_test_split(X_inp_clean,Y,\n                                                  test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is DistilBert?\nDistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased ,    \nruns 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nCreating model checkpoint for [DistilBert](https://huggingface.co/distilbert-base-uncased) model and importing it's pretrained AutoTokenizer","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An example of how tokenizer works","metadata":{}},{"cell_type":"code","source":"tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Tokenizing](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) training and validation data","metadata":{}},{"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    \n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        add_special_tokens = True,\n        truncation=True\n    )\n    \n    return np.array(enc_di['input_ids'])\n\nX_train_t = regular_encode(list(X_train), tokenizer, maxlen=512)\n\nX_val_t = regular_encode(list(X_val), tokenizer, maxlen=512)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating train and validation dataset with [batch size](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) 16","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\nbatch_size = 16\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_t, y_val))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function that will return our model","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32,\n                           name=\"input_word_ids\")\n    \n    sequence_output = transformer(input_word_ids)[0]\n    \n    cls_token = sequence_output[:, 0, :]\n    \n    out = Dense(2, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=1e-5),\n                  loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building the Model","metadata":{}},{"cell_type":"code","source":"transformer_layer = TFAutoModel.from_pretrained(model_checkpoint)\n\nmodel_DistilBert = build_model(transformer_layer)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"printing model's summary","metadata":{}},{"cell_type":"code","source":"model_DistilBert.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model","metadata":{}},{"cell_type":"code","source":"n_steps = X_train.shape[0] // batch_size\n\nhistory_DistilBert = model_DistilBert.fit(train_dataset,\n                                          steps_per_epoch=n_steps,\n                                          validation_data=valid_dataset,\n                                          epochs=3)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ploting model's training and validation history","metadata":{}},{"cell_type":"code","source":"model_history(history_DistilBert)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing and preprocessing test data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)\n\ntest[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)\n\ntest_sentences = pad_sequences(embed(test.Cleaned_text.values),\n                               length_long_sentence, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions of CNN model","metadata":{}},{"cell_type":"code","source":"predsCNN = model_CNN.predict_classes(test_sentences)\n\npredictions_test = pd.DataFrame(predsCNN)\n\ntest_id = pd.DataFrame(test[\"id\"])\n\nsubmissionCNN = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionCNN.columns = [\"id\",\"target\"]\n\nsubmissionCNN.to_csv(\"submissionCNN.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions of Multichannel CNN","metadata":{}},{"cell_type":"code","source":"predsMCNN = model_MCNN.predict([test_sentences,test_sentences])\n\npredsMCNN = (predsMCNN[:,0] > 0.5).astype(np.int)\n\npredictions_test = pd.DataFrame(predsMCNN)\n\nsubmissionMCNN = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionMCNN.columns = [\"id\",\"target\"]\n\nsubmissionMCNN.to_csv(\"submissionMCNN.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions of Bidirectional LSTM model","metadata":{}},{"cell_type":"code","source":"predsBiLSTM = model_BiLSTM.predict(test_sentences)\n\npredsBiLSTM = (predsBiLSTM[:,0] > 0.5).astype(np.int)\n\npredictions_test = pd.DataFrame(predsBiLSTM)\n\nsubmissionBiLSTM = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionBiLSTM.columns = [\"id\",\"target\"]\n\nsubmissionBiLSTM.to_csv(\"submissionBiLSTM.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"predictions of DistilBert model","metadata":{}},{"cell_type":"code","source":"X_test = regular_encode(list(test.Cleaned_text), tokenizer, maxlen=512)\n\ntest1 = (tf.data.Dataset.from_tensor_slices(X_test).batch(batch_size))\n\npred = model_DistilBert.predict(test1,verbose = 0)\n\npred = np.argmax(pred,axis=-1)\n\npred = pred.astype('int32')\n\nres=pd.read_csv('../input/nlp-getting-started/sample_submission.csv',index_col=None)  \n\nres['target'] = pred\n\nres.to_csv('submissionDistilBert.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}