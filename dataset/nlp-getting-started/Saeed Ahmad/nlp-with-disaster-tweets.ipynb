{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"id":"othhS_iKsCCY","execution":{"iopub.status.busy":"2022-04-14T09:29:46.971876Z","iopub.execute_input":"2022-04-14T09:29:46.972118Z","iopub.status.idle":"2022-04-14T09:29:46.976096Z","shell.execute_reply.started":"2022-04-14T09:29:46.972091Z","shell.execute_reply":"2022-04-14T09:29:46.97528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_sumbission_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"id":"oC78F4B6l8UC","execution":{"iopub.status.busy":"2022-04-14T09:29:49.453803Z","iopub.execute_input":"2022-04-14T09:29:49.454566Z","iopub.status.idle":"2022-04-14T09:29:49.520229Z","shell.execute_reply.started":"2022-04-14T09:29:49.454512Z","shell.execute_reply":"2022-04-14T09:29:49.519543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df['text'])","metadata":{"id":"AuR7FnEYyC2b","outputId":"8ac727bd-7714-4a1d-ec70-d720a6937217","execution":{"iopub.status.busy":"2022-04-14T09:29:52.315312Z","iopub.execute_input":"2022-04-14T09:29:52.315656Z","iopub.status.idle":"2022-04-14T09:29:52.340659Z","shell.execute_reply.started":"2022-04-14T09:29:52.3156Z","shell.execute_reply":"2022-04-14T09:29:52.339976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"s4Lf6lkbmU3L","outputId":"c0b7d6f0-a5fc-4943-9259-fd18e983038e","execution":{"iopub.status.busy":"2022-04-14T09:29:56.374084Z","iopub.execute_input":"2022-04-14T09:29:56.37435Z","iopub.status.idle":"2022-04-14T09:30:04.981549Z","shell.execute_reply.started":"2022-04-14T09:29:56.37432Z","shell.execute_reply":"2022-04-14T09:30:04.980697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text","metadata":{"id":"rM_P5T8usdQv","execution":{"iopub.status.busy":"2022-04-14T09:31:51.500402Z","iopub.execute_input":"2022-04-14T09:31:51.501073Z","iopub.status.idle":"2022-04-14T09:31:51.505979Z","shell.execute_reply.started":"2022-04-14T09:31:51.501033Z","shell.execute_reply":"2022-04-14T09:31:51.505224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Original: ', train_df.iloc[0]['text'])\nprint('Processed: ', text_preprocessing(train_df.iloc[0]['text']))","metadata":{"id":"419qSW4vsg3D","outputId":"6352761c-1091-473a-b80b-125b509a9bfe","execution":{"iopub.status.busy":"2022-04-14T09:31:53.978122Z","iopub.execute_input":"2022-04-14T09:31:53.978372Z","iopub.status.idle":"2022-04-14T09:31:53.986212Z","shell.execute_reply.started":"2022-04-14T09:31:53.978344Z","shell.execute_reply":"2022-04-14T09:31:53.984451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nimport torch\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\nMAX_LEN = 84\n\n# Create a function to tokenize a set of texts\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True      # Return attention mask\n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"id":"LwHJzzdls7wE","execution":{"iopub.status.busy":"2022-04-14T09:31:55.85924Z","iopub.execute_input":"2022-04-14T09:31:55.859907Z","iopub.status.idle":"2022-04-14T09:31:56.632856Z","shell.execute_reply.started":"2022-04-14T09:31:55.859865Z","shell.execute_reply":"2022-04-14T09:31:56.632132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate train data and test data\nall_tweets = np.concatenate([train_df['text'], test_df['text']])\n\n# Encode our concatenated data\nencoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n\n# Find the maximum length\nmax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max length: ', max_len)","metadata":{"id":"l3mprtyrtmyi","outputId":"8cc3e355-2470-4889-e946-43d7474e132c","execution":{"iopub.status.busy":"2022-04-14T09:32:00.301968Z","iopub.execute_input":"2022-04-14T09:32:00.302224Z","iopub.status.idle":"2022-04-14T09:32:08.50396Z","shell.execute_reply.started":"2022-04-14T09:32:00.302196Z","shell.execute_reply":"2022-04-14T09:32:08.503157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify `MAX_LEN`\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([train_df.iloc[0]['text']])[0].squeeze().numpy())\nprint('Original: ',train_df.iloc[0]['text'])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\n#print('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(train_df['text'])\n","metadata":{"id":"ETsBlYvyuMYE","outputId":"4a45af6d-7562-40c4-af39-69c8804c2076","execution":{"iopub.status.busy":"2022-04-14T09:32:10.967605Z","iopub.execute_input":"2022-04-14T09:32:10.968395Z","iopub.status.idle":"2022-04-14T09:32:17.093225Z","shell.execute_reply.started":"2022-04-14T09:32:10.96835Z","shell.execute_reply":"2022-04-14T09:32:17.092448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(train_df['target'])\n\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs[0:7000], train_masks[0:7000], train_labels[0:7000])\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(train_inputs[7000:7613], train_masks[7000:7613], train_labels[7000:7613])\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"id":"_cPgvfKru4iR","execution":{"iopub.status.busy":"2022-04-14T09:32:21.144105Z","iopub.execute_input":"2022-04-14T09:32:21.144393Z","iopub.status.idle":"2022-04-14T09:32:21.187217Z","shell.execute_reply.started":"2022-04-14T09:32:21.144362Z","shell.execute_reply":"2022-04-14T09:32:21.186527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_dataloader)","metadata":{"id":"Ks3Ch3a19W15","outputId":"77d01850-b2d5-4aec-c24a-e6b95b6c2a29","execution":{"iopub.status.busy":"2022-04-14T09:39:08.019458Z","iopub.execute_input":"2022-04-14T09:39:08.019729Z","iopub.status.idle":"2022-04-14T09:39:08.025525Z","shell.execute_reply.started":"2022-04-14T09:39:08.019699Z","shell.execute_reply":"2022-04-14T09:39:08.024782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n          \n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"id":"udcjBF8Bv5S8","outputId":"2e05470b-f662-43cd-fdbe-24d8d5f9d8e5","execution":{"iopub.status.busy":"2022-04-14T09:39:39.521338Z","iopub.execute_input":"2022-04-14T09:39:39.521808Z","iopub.status.idle":"2022-04-14T09:39:39.53255Z","shell.execute_reply.started":"2022-04-14T09:39:39.52177Z","shell.execute_reply":"2022-04-14T09:39:39.531557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"Z6DYZqhnwPmn","outputId":"7e2bcc7f-a9be-4c40-a452-b1bc04a88c29","execution":{"iopub.status.busy":"2022-04-14T09:38:38.159802Z","iopub.execute_input":"2022-04-14T09:38:38.160303Z","iopub.status.idle":"2022-04-14T09:38:38.166363Z","shell.execute_reply.started":"2022-04-14T09:38:38.160265Z","shell.execute_reply":"2022-04-14T09:38:38.165495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-7    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"id":"E5cZzPdDv_Kk","execution":{"iopub.status.busy":"2022-04-14T09:38:40.464446Z","iopub.execute_input":"2022-04-14T09:38:40.464913Z","iopub.status.idle":"2022-04-14T09:38:40.470319Z","shell.execute_reply.started":"2022-04-14T09:38:40.464876Z","shell.execute_reply":"2022-04-14T09:38:40.469681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader, epochs=4, evaluation=True):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"id":"8CZkdtKxwU9W","execution":{"iopub.status.busy":"2022-04-14T09:38:43.397581Z","iopub.execute_input":"2022-04-14T09:38:43.398109Z","iopub.status.idle":"2022-04-14T09:38:43.418058Z","shell.execute_reply.started":"2022-04-14T09:38:43.398073Z","shell.execute_reply":"2022-04-14T09:38:43.417163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(30)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=5)\ntrain(bert_classifier, train_dataloader,val_dataloader, epochs=5, evaluation=True)","metadata":{"id":"ImmTDOc1w_9a","outputId":"2648e840-0e13-4e4e-d6e1-4df3f7acd06f","execution":{"iopub.status.busy":"2022-04-14T09:39:45.978011Z","iopub.execute_input":"2022-04-14T09:39:45.978666Z","iopub.status.idle":"2022-04-14T09:45:06.355598Z","shell.execute_reply.started":"2022-04-14T09:39:45.97862Z","shell.execute_reply":"2022-04-14T09:45:06.354825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"id":"0crCsAwDxy-U","execution":{"iopub.status.busy":"2022-04-14T09:45:16.753252Z","iopub.execute_input":"2022-04-14T09:45:16.75352Z","iopub.status.idle":"2022-04-14T09:45:16.762394Z","shell.execute_reply.started":"2022-04-14T09:45:16.753474Z","shell.execute_reply":"2022-04-14T09:45:16.759864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run `preprocessing_for_bert` on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(test_df['text'])\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","metadata":{"id":"Sk16lvRExkxJ","outputId":"03fec72a-2456-47d6-dcd1-f3be54dd1809","execution":{"iopub.status.busy":"2022-04-14T09:45:20.183802Z","iopub.execute_input":"2022-04-14T09:45:20.184054Z","iopub.status.idle":"2022-04-14T09:45:22.689191Z","shell.execute_reply.started":"2022-04-14T09:45:20.184025Z","shell.execute_reply":"2022-04-14T09:45:22.688474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.9\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted non-negative\nprint(\"Number of tweets predicted non-negative: \", preds.sum())","metadata":{"id":"iDEJMvt9xlre","outputId":"2762f10c-20c2-4003-f656-8067446b3acb","execution":{"iopub.status.busy":"2022-04-14T09:45:27.634129Z","iopub.execute_input":"2022-04-14T09:45:27.634383Z","iopub.status.idle":"2022-04-14T09:45:36.922955Z","shell.execute_reply.started":"2022-04-14T09:45:27.634354Z","shell.execute_reply":"2022-04-14T09:45:36.922165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"id":"D20bxle0yKPZ","outputId":"36458736-3d3a-49fb-b2af-190e17207989"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_test_df = sample_sumbission_df.iloc[0:0]\npredict_test_df['id'] = test_df['id']\npredict_test_df['target'] = preds\npredict_test_df","metadata":{"id":"lbQ2yj-SyOdo","outputId":"1c400ed7-4f01-4889-d0c4-f51b47688b32","execution":{"iopub.status.busy":"2022-04-14T09:45:51.202977Z","iopub.execute_input":"2022-04-14T09:45:51.203234Z","iopub.status.idle":"2022-04-14T09:45:51.221653Z","shell.execute_reply.started":"2022-04-14T09:45:51.203205Z","shell.execute_reply":"2022-04-14T09:45:51.220948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_test_df.head()","metadata":{"id":"84jt8vNZzDR2","outputId":"fc128f6e-734b-48c1-c55d-21fe475598f2","execution":{"iopub.status.busy":"2022-04-14T09:45:55.249154Z","iopub.execute_input":"2022-04-14T09:45:55.249764Z","iopub.status.idle":"2022-04-14T09:45:55.257934Z","shell.execute_reply.started":"2022-04-14T09:45:55.249723Z","shell.execute_reply":"2022-04-14T09:45:55.257039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_test_df.to_csv('predict.csv', index=False)","metadata":{"id":"9PPe5tswzVsd","execution":{"iopub.status.busy":"2022-04-14T09:46:14.005132Z","iopub.execute_input":"2022-04-14T09:46:14.005391Z","iopub.status.idle":"2022-04-14T09:46:14.017847Z","shell.execute_reply.started":"2022-04-14T09:46:14.005362Z","shell.execute_reply":"2022-04-14T09:46:14.016948Z"},"trusted":true},"execution_count":null,"outputs":[]}]}