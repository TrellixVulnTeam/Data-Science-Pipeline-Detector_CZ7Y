{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n    import pycaret\nexcept:\n    !pip install pycaret-nightly\n\ntry:\n    import missingno\nexcept:\n    !pip install missingno\n    \ntry:\n    import interpret\nexcept:\n    !pip install interpret","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https://miro.medium.com/max/1400/1*fTPhu7PqgIbnngbWG5zFWA.gif)\n\nPicture Credit: https://miro.medium.com\n\n**What is Natural language processing?**\n> Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n> \n> Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n\nRef: https://en.wikipedia.org/wiki/Natural_language_processing","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom fastai.text.all import *\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use(\"dark_background\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------\n# EDA","metadata":{}},{"cell_type":"code","source":"train_df.head().style.set_properties(**{'background-color': 'black',\n                           'color': 'white',\n                           'border-color': 'white'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------\n## Checking Missing Values","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df=train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(['keyword','location','id'],axis=1,inplace=True)\ntest_df.drop(['keyword','location','id'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------\n## Checking Target Imbalance","metadata":{}},{"cell_type":"code","source":"colors = ['gold', 'mediumturquoise']\nlabels = ['Non-Disaster','Disaster']\nvalues = train_df['target'].value_counts()/train_df['target'].shape[0]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.update_layout(\n    title_text=\"Target Balance\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"yellow\",\n    paper_bgcolor=\"black\",\n    plot_bgcolor='black',\n    font_color=\"white\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Seeing Disater Tweets","metadata":{}},{"cell_type":"code","source":"train_df[train_df['target']==1].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Seeing Non-disater Tweets","metadata":{}},{"cell_type":"code","source":"train_df[train_df['target']==0].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:Blue\"> Observation:    \n    \nNon-disater tweets seem shorter.","metadata":{}},{"cell_type":"markdown","source":"--------------------------\n# Visualizing using Topic Modeling\n\n![](https://miro.medium.com/max/1400/1*cDwKSHmfp5awjqjobV707g.png)\n\nPicture Credit: https://miro.medium.com\n\n**What is Topic Modeling?**\n> In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n\nRef: https://en.wikipedia.org/wiki/Topic_model","metadata":{}},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n!python -m textblob.download_corpora","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom pycaret.nlp import *\nnlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time \ndisaster_nlp = setup(data = train_df, \n                     target = 'text',\n                     html = False,\n                     custom_stopwords = ['CO','.co','co','https','http'],\n                     session_id = 123)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:Blue\"> Observation:\n    \n* 9233 vocabularies were created from a total of 7613 documents.","metadata":{}},{"cell_type":"markdown","source":"----------------------\n# LDA(Latent Dirichlet allocation)\n\n![](https://ars.els-cdn.com/content/image/1-s2.0-S0164121218302103-gr6.jpg)\n\nPicture Credit: https://ars.els-cdn.com\n\n> In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model.\n\nRef: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation","metadata":{}},{"cell_type":"code","source":"lda = create_model('lda',multi_core=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_df = assign_model(lda, verbose=True)\nlda_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lda, plot = 'topic_distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lda, plot = 'topic_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------\n## Ploting Wordcloud","metadata":{}},{"cell_type":"code","source":"plot_model(lda, plot = 'wordcloud')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------------\n## Checking N-grams\n\n> In the fields of computational linguistics and probability, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n\nRef: https://en.wikipedia.org/wiki/N-gram","metadata":{}},{"cell_type":"code","source":"plot_model(lda, plot = 'bigram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lda, plot = 'trigram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------\n## Checking Frequency","metadata":{}},{"cell_type":"code","source":"plot_model(lda, plot = 'frequency')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------\n## Checking Part of Speech Frequency (POS)\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/tree.png)\n\nPicture Credit: https://cdn.analyticsvidhya.com\n\n> In traditional grammar, a part of speech or part-of-speech (abbreviated as POS or PoS) is a category of words (or, more generally, of lexical items) that have similar grammatical properties. Words that are assigned to the same part of speech generally display similar syntaxic behavior (they play similar roles within the grammatical structure of sentences), sometimes similar morphology in that they undergo inflection for similar properties and even similar semantic behavior.\n> \nCommonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, or determiner. \n\nRef: https://en.wikipedia.org/wiki/Part_of_speech","metadata":{}},{"cell_type":"code","source":"plot_model(lda, plot = 'pos')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------\n## Visualizing after Dimensionality Reduction\n\nLet's show the 7613 documents by dimensionality reduction in 2D and 3D.","metadata":{}},{"cell_type":"code","source":"plot_model(lda, plot = 'umap')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lda, plot = 'tsne')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Preprocessing, Modeling and Training","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"ticks\", context=\"talk\")\nplt.style.use(\"dark_background\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------\n# Making Pipeline and Dataloaders\n\nThe following is the process of tokenizing text and batching it through dataloader.","metadata":{}},{"cell_type":"code","source":"tweet_datablock = DataBlock(\n    blocks=(TextBlock.from_df('text', seq_len=36), CategoryBlock),\n    get_x=ColReader('text'), get_y=ColReader('target'))\n\ndls = tweet_datablock.dataloaders(train_df, bs=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Showing Batch","metadata":{}},{"cell_type":"code","source":"dls.show_batch(max_n=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few special tokens are described as following.\n* xxbos: marks the beginning of the text.\n* xxmaj: Indicates that the next word starts with a capital letter.\n* xxunk: Indicates that the current word is not in the list.","metadata":{}},{"cell_type":"markdown","source":"## Showing Basic processing Rules","metadata":{}},{"cell_type":"code","source":"defaults.text_proc_rules","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------\n# Modeling","metadata":{}},{"cell_type":"code","source":"learn = text_classifier_learner(dls,\n                                AWD_LSTM,\n                                drop_mult=0.5,\n                                metrics=accuracy,                        \n                                cbs = [EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3),ActivationStats(with_hist=True)])\n\nlearn.model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------\n# Training","metadata":{}},{"cell_type":"code","source":"sr = learn.lr_find()\nsr.valley","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(100,sr.valley)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.recorder.plot_loss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------\n# Interpreting\n\nWe can confirm that the training was successful with the activation distribution.","metadata":{}},{"cell_type":"code","source":"def plot_layer_stats(self, idx):\n    plt,axs = subplots(1, 3, figsize=(15,3))\n    plt.subplots_adjust(wspace=0.5)\n    for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n        ax.plot(o)\n        ax.set_title(f\"{-1*layer}th layer {title}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in range(1,4):\n    plot_layer_stats(learn.activation_stats,-1*layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def color_dim(self, idx):\n    with plt.rc_context({\"figure.figsize\": (10,40), \"figure.dpi\": (600)}):\n        res = self.hist(idx)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.imshow(res, origin='lower')\n        ax.set_title(f\"{idx}th activation histogram\")\n        ax.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matplotlib.rcParams['image.cmap'] = 'rainbow_r'\nfor layer in range(1,4):\n    color_dim(learn.activation_stats,-1*layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(3,3),dpi=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------\n# Predicting","metadata":{}},{"cell_type":"code","source":"test_dl = learn.dls.test_dl(test_df)\ntest_dl.show_batch(n_max=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = learn.get_preds(dl=test_dl)\nresults = preds[0].argmax(axis=1)\nresults = results.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data['target'] = results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">","metadata":{}}]}