{"cells":[{"metadata":{"_uuid":"95da9ecd-c797-4b26-9122-18418e678c66","_cell_guid":"c6f22fd7-e425-4078-bd07-6bc6686bfef0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk \nimport string\nimport re\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n#import geopandas as gpd\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['location'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Finding tweets based on keyword 'accident'*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['keyword'] == 'accident'].head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Finding tweets based on location 'Canada'*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['location'] == 'Canada'].head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Distribution of train and test tweets*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetLengthTrain = train['text'].str.len()\ntweetLengthTest = test['text'].str.len()\n\nplt.hist(tweetLengthTrain,bins=20,label='Train_Tweet')\nplt.hist(tweetLengthTest,bins=20,label='Test_Tweet')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Target Distribution of keywords*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"target_mean\"] = train.groupby(\"keyword\")[\"target\"].transform(\"mean\")\n\nplt.figure(figsize=(8,72))\n\nsns.countplot(y=train.sort_values(\"target\",ascending=False)[\"keyword\"],hue=\\\n              train.sort_values(\"target\",ascending=False)[\"target\"])\nplt.tick_params(axis=\"x\",labelsize=15)\nplt.tick_params(axis=\"y\",labelsize=15)\nplt.title(\"Target Distribution in Keywords\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train\ntest_df=test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Combining train and test datasets for furthur model evaluation*"},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = train.append(test,ignore_index=True)\nprint('Shape of new Dataset:',combine.shape)\ncombine.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_tweet = combine[\"text\"]\ntext_tweet.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  pd.DataFrame(combine[['text']])\ndf.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing punctuation from tweets*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation]) #all punctuations \n    text = re.sub('[0-9]+', '', text) #all numbers\n    return text\n\ndf['Tweet_punct'] = df['text'].apply(lambda x: remove_punct(x))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Tweet_punct\"].drop_duplicates(inplace = True)\ndf[\"Tweet_punct\"].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine[\"Tweet_punct\"] = df['Tweet_punct']\ncombine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#important libraries for preprocessing using NLTK\nimport nltk\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.porter import * ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = combine[combine['Tweet_punct'].notna()]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Tokenizing Tweets*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\ndf['Tweet_tokenized'] = df['Tweet_tokenized'].apply(lambda x : [stemmer.stem(i) for i in x]  )\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Joining the tokenized tweets*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(df['Tweet_tokenized'])):\n    df['Tweet_tokenized'][i] = ' '.join(df['Tweet_tokenized'][i])    \ndf['cleanedText'] = df['Tweet_tokenized']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating word Cloud for all Words in all tweets\nfrom wordcloud import WordCloud\nallWords = ' '.join([text for text in df['cleanedText']])\nwordcloud = WordCloud(background_color='white', width=800, height=500, random_state=25, max_font_size=110).generate(allWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Evaluating sentiment polarity of tweets using TextBlob*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\ndef sentiment_polarity(text):\n    tweet_text = TextBlob(str(text))\n    sentiment_value = tweet_text.sentiment.polarity\n    return sentiment_value\n\ndf['Tweet_Polarity'] = df['cleanedText'].apply(lambda x: sentiment_polarity(x.lower()))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Categorizing sentiments based on polarity value*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_analysis(value):\n    sentiment=\"\"\n    if(value<0.0):\n        sentiment = \"negative\"\n    elif(value>0.0):\n        sentiment = \"positive\"\n    else:\n        sentiment = \"neutral\"\n    return sentiment\n\ndf['Tweet_Sentiments'] = df['Tweet_Polarity'].apply(lambda x: sentiment_analysis(float(x)))\ndf[[\"Tweet_punct\",\"Tweet_Polarity\",\"Tweet_Sentiments\"]].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.countplot(x=\"Tweet_Sentiments\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['cleanedText'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Topic Modelling: LDA**"},{"metadata":{},"cell_type":"markdown","source":"*Applying Dictionary of gensim to turn our tokenized documents into a id <-> term dictionary*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora import Dictionary\n\n#create dictionary\ntext_dict = Dictionary(df.Tweet_tokenized)\n\n#view integer mappings\ntext_dict.token2id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*convert tokenized documents into a document-term matrixa*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_bow = [text_dict.doc2bow(tweet) for tweet in df['Tweet_tokenized']]\ntweets_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Generate LDA model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.ldamodel import LdaModel\n\nk = 10\ntweets_lda = LdaModel(tweets_bow,\n                      num_topics = k,\n                      id2word = text_dict,\n                      random_state = 1,\n                      passes=10)\n\ntweets_lda.show_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word_count\ntrain[\"word_count\"] = train[\"text\"].map(lambda x: len(str(x).split()))\ntest[\"word_count\"] = test[\"text\"].map(lambda x: len(str(x).split()))\n\n# unique_word_count \ntrain[\"unique_word_count\"] = train[\"text\"].map(lambda x:len(set(str(x).split())))\ntest[\"unique_word_count\"] = test[\"text\"].map(lambda x:len(set(str(x).split())))\n\n# stop_word_count\ntrain[\"stop_word_count\"] = train[\"text\"].map(lambda x: len([elt for elt in str(x).lower().split() \\\n                                                                  if elt in stopwords.words(\"english\")]))\ntest[\"stop_word_count\"] = test[\"text\"].map(lambda x:len([elt for elt in str(x).lower().split()\\\n                                                              if elt in stopwords.words(\"english\")]))\n# url_count \ntrain[\"url_count\"] = train[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                            if 'http' or 'https' in w]))\ntest[\"url_count\"] = test[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                          if \"http\" or \"https\" in w ]))\n# mean_word_length\ntrain[\"mean_word_length\"] = train[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\ntest[\"mean_word_length\"] = test[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\n\n#char_count \ntrain[\"char_count\"] = train[\"text\"].map(lambda x:len(str(x)))\ntest[\"char_count\"] = test[\"text\"].map(lambda x:len(str(x)))\n\n#punctuation_count\ntrain[\"punctuation_count\"] = train[\"text\"].map(lambda x: len([elt for elt in str(x) if elt in string.punctuation]))\ntest[\"punctuation_count\"] = test[\"text\"].map(lambda x:len([elt for elt in str(x) if elt in string.punctuation]))\n#hashtag_count\ntrain[\"hashtag_count\"] = train[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\ntest[\"hashtag_count\"] = test[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\n\n#mention_count\ntrain[\"mention_count\"] = train[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))\ntest[\"mention_count\"] = test[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heureunder, we will explore the distribution of each mega-feature per target and per dataset\n# (train & test)\nMETA_FEATURES = [\"word_count\",\"unique_word_count\",\"stop_word_count\",\"url_count\",\\\n                 \"mean_word_length\",\"char_count\",\"punctuation_count\",\"hashtag_count\",\"mention_count\"]\nfig,ax = plt.subplots(nrows=len(META_FEATURES),ncols=2,figsize=(20,50),dpi=100)\nmask = train[\"target\"]==1\nfor i,feature in enumerate(META_FEATURES):\n\n    \n   sns.distplot(train[mask][feature],ax=ax[i,0],label=\"Disaster\",kde=False)\n   sns.distplot(train[~mask][feature],ax=ax[i,0],label=\"Not Disaster\",kde=False)\n   ax[i,0].set_title(\"{} target distribution in trainning dataset\".format(feature),fontsize=13)\n \n   sns.distplot(train[feature],ax=ax[i,1],label=\"Train Dataset\",kde=False)\n   sns.distplot(test[feature],ax=ax[i,1],label=\"Test Dataset\",kde=False)\n   ax[i,1].set_title(\"{} training and test dataset distributions \".format(feature),fontsize=13)\n   for j in range(2):\n        ax[i,j].set_xlabel(\" \")\n        ax[i,j].tick_params(axis=\"x\",labelsize=13)\n        ax[i,j].tick_params(axis=\"y\",labelsize=13)\n        ax[i,j].legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(20,6))\n\ntrain.groupby(\"target\").count()[\"id\"].plot(kind=\"pie\",labels=[\"Not Disaster\",\"Disaster\"],\\\n                                              autopct=\"%1.1f pourcents\",ax=ax[0])\nsns.countplot(x=train[\"target\"],hue=train[\"target\"],ax=ax[1])\nax[1].set_xticklabels([\"Non Disaster\",\"Disaster\"])\nax[0].tick_params(axis=\"x\",labelsize=15)\nax[0].tick_params(axis=\"y\",labelsize=15)\nax[0].set_ylabel(\"\")\nax[1].tick_params(axis=\"x\",labelsize=15)\nax[1].tick_params(axis=\"y\",labelsize=15)\nax[0].set_title(\"Target distribution in training set\",fontsize=13)\nax[1].set_title(\"Target count in training set\",fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the function which will be able to generate n_grams for each row of dataset.\ndef gen_n_grams(text,n_grams=1):\n    \"\"\" This function allow to extract the n_gram in the introduced text.\n    \n      @param text(str): the text that the function, will use to extract features (n_grams).\n      @param n_grams(int): the length of n_gram, that we will use.\n      @return ngrams(list): list of the ngrams in the intriduced text.\n    \"\"\"\n    tokens = [token for token in str(text).lower().split() if token not in stopwords.words(\"english\")]\n    ngrams = zip(*[tokens[i:] for i in range(n_grams)])\n    \n    return [\" \".join(gram) for gram in ngrams]\n\n# create the function which will be able to generate dataframe of n_gram features for disaster\n# and non disaster tweets.\ndef gen_df_ngrams(n_grams=1):\n    \"\"\" This function, allow to generate dataframes for n_grams in disaster tweets and non \n        disaster tweet\n    \"\"\"\n    mask = train[\"target\"]==1\n    disaster_unigrams = defaultdict(int)\n    non_disaster_unigrams = defaultdict(int)\n    \n    for tweet in train.loc[mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            disaster_unigrams[gram] +=1\n    for tweet in train.loc[~mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            non_disaster_unigrams[gram] +=1\n    df_disaster_n_grams = pd.DataFrame(sorted(disaster_unigrams.items(),reverse=True,key=\\\n                                              lambda item:item[1]))\n    df_non_disaster_n_grams = pd.DataFrame(sorted(non_disaster_unigrams.items(),reverse=True,key=\\\n                                                  lambda item:item[1]))\n    return df_disaster_n_grams,df_non_disaster_n_grams\n\n# Define function, which allow to plot the N most occured n_gram in disaster tweet \n# and non disaster tweet.\n\ndef plot_ngrams(df_disaster_n_grams,df_non_disaster_unigrams,N=100,n_grams=1):\n    \"\"\"This function,allow to plot the top most n_grams in disaster tweet and non disaser tweet.\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(18,50))\n    sns.barplot(y=df_disaster_n_grams[0].values[:N],x=df_disaster_n_grams[1].values[:N],ax=ax[0],\\\n               color=\"red\")\n    for i in range(2):\n        ax[i].tick_params(axis=\"x\",labelsize=15)\n        ax[i].tick_params(axis=\"y\",labelsize=15)\n        ax[i].set_xlabel(\"Occurences\")\n        ax[i].spines[\"right\"].set_visible(False)\n    sns.barplot(y=df_non_disaster_unigrams[0].values[:N],x=df_non_disaster_unigrams[1].values[:N],\\\n               ax=ax[1],color=\"green\")\n    ax[0].set_title(\"Top most {} {}_grams for disaster tweets\".format(N,n_grams),size=15)\n    ax[1].set_title(\"Top most {} {}_grams for non disaster tweets\".format(N,n_grams),size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams()\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=2)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the most 100 bigrams per target (Disaster and not Disaster) when n_grams=3\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=3)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(text):\n    \"\"\"\n    Removes punctuations(if any), stopwords and returns a list words\n    \"\"\"\n    rm_pun = [char for char in text if char not in string.punctuation]\n    rm_pun = ''.join(rm_pun)\n    \n    return [word for word in rm_pun.split() if word.lower() not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(analyzer=process_text).fit(train['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train\ntest_data=test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cv.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text10 = train['text'][9]\ntext10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv10 = cv.transform([text10])\nprint(cv10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv.get_feature_names()[5375])\nprint(cv.get_feature_names()[11579])\nprint(cv.get_feature_names()[11856])\nprint(cv.get_feature_names()[13190])\nprint(cv.get_feature_names()[25334])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv10.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_cv = cv.transform(train['text'])\nprint(train_data_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer().fit(train_data_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf10 = tfidf.transform(cv10)\nprint(tfidf10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tfidf = tfidf.transform(train_data_cv)\nprint(train_data_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Naive Bayes Classifier Training the model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\ntarget_model = MultinomialNB().fit(train_data_tfidf, train_data['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('predicted:', target_model.predict(tfidf10)[0])\nprint('expected:', train_data.target[9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Predictions for test data test data vector and tf-idf*"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_cv = cv.transform(test_data['text'])\n\ntest_data_tfidf = tfidf.transform(test_data_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_model.predict(test_data_tfidf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}