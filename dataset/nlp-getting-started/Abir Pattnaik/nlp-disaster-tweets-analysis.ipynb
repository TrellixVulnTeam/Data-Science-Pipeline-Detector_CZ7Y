{"cells":[{"metadata":{"_uuid":"4df1c426-3ec6-4bb6-bac7-89ed4a742e6c","_cell_guid":"baa56d3a-e000-46ce-90a1-89537e970667","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport regex\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a9884ad4-31fd-4dd0-be1a-9282d95efb86","_cell_guid":"f02fbbf6-9ea3-418c-80d2-26cfd28ed857","trusted":true},"cell_type":"markdown","source":"1. /kaggle/input/nlp-getting-started/train.csv\n# 2. /kaggle/input/nlp-getting-started/test.csv\n# 3. /kaggle/input/nlp-getting-started/sample_submission.csv"},{"metadata":{"_uuid":"27e8d714-fa82-406a-94e7-b769131775e9","_cell_guid":"c2d25f19-7007-4297-b42f-740fa9ff8e4b","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_data.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"686b02a0-9373-485a-a2d2-b5c38d2970be","_cell_guid":"397fce1e-7ba4-4adc-a484-e2d64fa842ae","trusted":true},"cell_type":"code","source":"print('The train set contains {0} rows and {1} columns '.format(train_data.shape[0],train_data.shape[1]))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8d12974b-d5c4-4799-bef7-daa5050b8f66","_cell_guid":"13671d22-dc29-4f33-9f9c-c0ac7c56bee5","trusted":true},"cell_type":"markdown","source":"Since train set contains 5 columns with target variables, I can do preliminary analysis by using the following method :\n#     \n#     1. Understand on the no. of tweets which are disaster related and which are not.Bar plots would do in this case.\n#     2. How keyword and location helps in the tweets and would they help in prediction or not.\n#     3. Filtering out the stop words(words that can be filtered out as they may not be needed for the analysis)\n#     4. Checking the words present in the tweet and then analysing based on the target variable."},{"metadata":{"_uuid":"ca53e30b-a9b1-41b4-a907-61a1d7b52c06","_cell_guid":"b1d4ca99-26bb-422d-abda-93f94a6b4f36","trusted":true},"cell_type":"code","source":"ax=sns.countplot(data=train_data,x=train_data['target'])\nplt.xlabel('Target Variable- Disaster or not disaster tweet')\nplt.ylabel('Count of tweets')\nplt.title('Count of disaster and non-disaster tweets')\ntotal = len(train_data)\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5))\n\n#https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\n# Study the document for matplotlib","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2b19eede-51d7-40f4-af13-d198fff6ef24","_cell_guid":"2dfcc217-e291-49c0-8463-129fb69e7852","trusted":true},"cell_type":"markdown","source":"Seeing from the tweets we can do an analysis as it can be construed an imbalanced dataset.In the initial analysis where I have went through the dataset I found out that the data needs to be cleaned **a lot** .For this, I am going column wise i.e. \n \n **keyword --> location --> tweet **\n \nIn the keywords column I found out that there are %20 added between words .I am thinking this has been fetched from the search strings hence I was thinking I remove it and replace it with *_* to make it look presentable."},{"metadata":{"_uuid":"efee8f4a-f6bd-420c-b162-3decd5a188a0","_cell_guid":"20d339ed-cf86-40f4-815e-92143be2251c","trusted":true},"cell_type":"code","source":"train_data['keyword']=train_data.keyword.str.replace('%20','_')\ntrain_data['keyword'] = train_data['keyword'].replace(np.nan, '', regex=True)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"b0554ed4-ae68-418e-bdbc-e9c19c3434e3","_cell_guid":"5b06b568-7182-4083-93fa-eac024d1873c","trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(100,50))\n\n#sns.catplot(data=train_data,y='keyword',col='target',height=200,aspect=0.5,kind='count')","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f00e9e9b-2ed5-4301-b927-9ef9191a8c6e","_cell_guid":"b38bb445-fc61-41c3-83df-ee35d47c9d15","trusted":true},"cell_type":"markdown","source":"1. 1. Above graph is not that clear, I will try to analyse in a tabular manner"},{"metadata":{"_uuid":"a95e3ebe-2b32-4177-a9cc-c508ee9050d2","_cell_guid":"418a9f6e-e5de-45b8-affa-f71c5769d25b","trusted":true},"cell_type":"code","source":"#pd.set_option('display.max_rows' ,None)\nkeyword_count=pd.DataFrame(train_data.groupby(['keyword','target']).agg(['count']).sort_values(by=('id', 'count'),ascending=False)[('id', 'count')])","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3eba1a5f-ec5f-4ca5-96ea-27736c11f8f9","_cell_guid":"bdd4c3cb-598a-4053-b62c-96903c0364e7","trusted":true},"cell_type":"code","source":"keyword_count.columns","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"9c13a6ac-2e6d-48b0-a0e5-d9e1427e5f40","_cell_guid":"b172d718-8874-4463-8bc9-fd46ca4183c7","trusted":true},"cell_type":"code","source":"\n#keyword_count['keyword_value']=keyword_count.keyword[0][0]\n'''\nkeyword_count['keyword']=keyword_count.index\nkeyword_count.columns=keyword_count.columns.droplevel()\nkeyword_count.columns=['count','keyword']\n\nfor value in range(0,len(keyword_count)):\n    #print(keyword_count.keyword[value][0])\n    if 'keyword_value' not in keyword_count.columns:\n        keyword_count['keyword_value']=keyword_count.keyword[0][0]\n    else:\n        keyword_count['keyword_value'][value]=keyword_count.keyword[value][0]\n    #print(keyword_count.keyword[value][1])\n    if 'target_value' not in keyword_count.columns:\n        keyword_count['target_value']=keyword_count.keyword[0][1]\n    else:\n        keyword_count['target_value'][value]=keyword_count.keyword[value][1]\n\nif 'keyword' in keyword_count.columns:\n    keyword_count=keyword_count.drop(['keyword'],axis=1)\n#Index(['count', 'keyword', 'keyword_value', 'target_value'], dtype='object')\n'''","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"6cb7e460-0304-4c1d-af5a-b12b9ce5a922","_cell_guid":"d0d2e0c0-689f-4433-b674-7da54ae82bac","trusted":true},"cell_type":"code","source":"keyword_count","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d8d04cac-adaa-44b5-be7d-05a49d728041","_cell_guid":"fda95c1b-1f3b-45a4-b32c-f2a4a2e78b4d","trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=80, \n                          random_state=42,\n    collocations=False,\n    colormap=\"Oranges_r\"\n                         ).generate(' '.join(train_data[train_data['target']==1]['keyword']))\n#.join(text2['Crime Type']))\n\nplt.figure(figsize=(10,10))\nplt.title('Major keywords for disaster tweets', fontsize=30)\nplt.imshow(wordcloud)\n\nplt.axis('off')\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8dcd6db7-23d6-40b8-9bf3-1c20bbfdb497","_cell_guid":"986ee746-aee3-4792-9fab-2f977dbfc62c","trusted":true},"cell_type":"markdown","source":"Above code specifies that these are the major words with the highest no. of frequencies when it has been mentioned for disaster tweets.\n# \n# I will list out some of them :\n#     **forest fire,death,flood,derailment,outbreak,typhoon**"},{"metadata":{"_uuid":"01c6c1dc-69aa-4ed8-8ff3-16660634b37c","_cell_guid":"611115f0-0776-4d6f-bebf-839f62852cf8","trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=40, \n                            collocations=False,\n    colormap=\"PuOr\"\n                         ).generate(' '.join(train_data[train_data['target']==0]['keyword']))\n#.join(text2['Crime Type']))\n\nprint(wordcloud)\nplt.figure(figsize=(10,25))\nplt.imshow(wordcloud)\nplt.title('Major keywords for non-disaster tweets', fontsize=30)\nplt.axis('off')\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0351ed02-d4e0-4699-ae94-a0545840ff1b","_cell_guid":"630a54f0-ac1b-4cb6-89fc-039461d9dff4","trusted":true},"cell_type":"markdown","source":"Above code represents the code for the non-disaster tweets.As one can see, the list is quite different from the disaster tweets.\n# Also, the level of words is weaker than the tweets keywords given in the disaster tweets.These keywords are more of generalised words taht have been used like **body bag,death,siren,wrecked etc**."},{"metadata":{"_uuid":"beaa05a2-a21b-4ae6-9169-cb3ee88aff16","_cell_guid":"f1489bde-e5d5-4ad8-be9a-592934dfd6b4","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"7b80512e-e554-4661-b90f-8de3efb66ecd","_cell_guid":"2a3bae4c-43e7-4da9-bc3f-c48449a13d82","trusted":true},"cell_type":"markdown","source":"## Tweet analysis based on Location\n \nWe need to first clean the data for the location variable leaving out all the un-necessary words."},{"metadata":{"_uuid":"40fa459a-77fa-4a45-93ec-e06071cb6db7","_cell_guid":"a0bd758f-a01e-49b7-8fd0-656bd689d9fa","trusted":true},"cell_type":"code","source":"train_data['location'].value_counts()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"aa8c4dc1-7fee-4401-88f8-b98cc0692dcd","_cell_guid":"a28f7dba-8cfb-4ef7-9aa5-81a78e21df70","trusted":true},"cell_type":"code","source":"#pd.set_option('display.max_rows' ,None)\ntrain_data['location']","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"5738075d-94f8-4433-bf2c-98a97c3af3e1","_cell_guid":"92d3b304-1446-43af-a611-6632bbc94bdc","trusted":true},"cell_type":"markdown","source":"Location"},{"metadata":{"_uuid":"4a8410da-6638-4561-ab60-e81654ca68be","_cell_guid":"bb889d41-aca0-4e5a-990c-aa70831aec74","trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"287c8051-a285-4f58-9570-32e84795780b","_cell_guid":"55e6c0ff-f517-4be2-aa8e-0548a9b7d7e3","trusted":true},"cell_type":"code","source":"train_data['location_cleaned']=train_data['location'].map(lambda s:preprocess(s))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8d8160ee-01f2-4f0c-8e61-8b58ee3a13ed","_cell_guid":"17fda89a-2e19-41e0-94d2-62a5042c7d6d","trusted":true},"cell_type":"code","source":"#train_data['location_cleaned']","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"bb6dae05-240c-46ce-8664-18bdc3f8f375","_cell_guid":"d9d1411c-14aa-430d-aa85-f0e5b9090a57","trusted":true},"cell_type":"code","source":"train_data[\"location_cleaned\"].replace({\"united states\": \"usa\", \n                                        \"world\": \"worldwide\",\n                                        \"nyc\":\"new york\",\n                                       \"california usa\":\"california\",\n                                        \"new york city\":\"new york\",\n                                        \"california united states\":\"california\",\n                                        \"mumbai\":\"india\"\n                                       }, inplace=True)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e71d7bc2-0c22-45c9-bc91-223e08bde6a3","_cell_guid":"613c85a1-ebc8-4d58-8bf2-19e47aac24ef","trusted":true},"cell_type":"code","source":"train_data['location_cleaned'].value_counts().nlargest(20)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"1fdcabb4-4d13-4d23-881c-e4178aa2f9e1","_cell_guid":"9b22a452-b83d-4664-ad08-c589dbed7f69","trusted":true},"cell_type":"markdown","source":"Most of the location tweets weren't part of the tweet.That amounts to be around **2700 tweets** that have no location. Apart from that USA based location tweets for USA,New York,Washington,California,Chicago that amounts to be around **450 tweets**. London has approx. **40 tweets**.India has over **50 tweets**."},{"metadata":{"_uuid":"48e3d957-379f-489c-8163-27b4430dd8b7","_cell_guid":"b1ccc0b1-37fe-4f1a-ae58-9c87252e0dcb","trusted":true},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"41aa6ded-f640-472e-875c-95f029becf41","_cell_guid":"9092790a-1771-449c-942f-035d23aee775","trusted":true},"cell_type":"code","source":"#Preprocessing text\ntrain_data['text_cleaned']=train_data['text'].map(lambda s:preprocess(s)) \ntest_data['text_cleaned']=test_data['text'].map(lambda s:preprocess(s))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2eef9c0a-75e9-4e11-b512-a2d139f081b9","_cell_guid":"267cd266-14d2-4b24-9e6f-338a72aef18a","trusted":true},"cell_type":"code","source":"#train_data\ntrain_text = train_data['text_cleaned']\ntest_text = test_data['text_cleaned']\ntrain_target = train_data['target']\nall_text = train_text.append(test_text)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"9aeca9f4-47eb-4b82-8134-97d54a30689b","_cell_guid":"672ec03f-c5ed-4bf4-b80a-0c5c858dab87","trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(all_text)\n\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit(all_text)\n\ntrain_text_features_cv = count_vectorizer.transform(train_text)\ntest_text_features_cv = count_vectorizer.transform(test_text)\n\ntrain_text_features_tf = tfidf_vectorizer.transform(train_text)\ntest_text_features_tf = tfidf_vectorizer.transform(test_text)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"68919af3-a179-4bd9-83a6-9bc8cb68694f","_cell_guid":"3d5926ae-75d6-41ea-8f67-d39af7eab420","trusted":true},"cell_type":"code","source":"train_text.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"1c70fbf4-c330-4d2b-9b54-65ae55e06c7d","_cell_guid":"04153b18-6612-4478-9ea8-9ffacc4c3ef8","trusted":true},"cell_type":"code","source":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 2018)\ntest_preds = 0\noof_preds = np.zeros([train_data.shape[0],])\n\nfor i, (train_idx,valid_idx) in enumerate(kfold.split(train_data)):\n    x_train, x_valid = train_text_features_tf[train_idx,:], train_text_features_tf[valid_idx,:]\n    y_train, y_valid = train_target[train_idx], train_target[valid_idx]\n    classifier = LogisticRegression()\n    print('fitting.......')\n    classifier.fit(x_train,y_train)\n    print('predicting......')\n    print('\\n')\n    oof_preds[valid_idx] = classifier.predict_proba(x_valid)[:,1]\n    test_preds += 0.2*classifier.predict_proba(test_text_features_tf)[:,1]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"1683be8a-909f-4f76-a8a2-232a170442b1","_cell_guid":"a082a9ed-7ba2-4b8b-8d6e-3737f4f92fe3","trusted":true},"cell_type":"code","source":"pred_train = (oof_preds > .25).astype(np.int)\nf1_score(train_target, pred_train)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"5d24a8c6-18b8-40b1-a904-99a2fe3cb9ca","_cell_guid":"d371f39f-3a5c-461a-bb67-4781e9f8095a","trusted":true},"cell_type":"code","source":"#submission1 = pd.DataFrame.from_dict({'id': test['id']})\n#submission1['prediction'] = (test_preds>0.25).astype(np.int)\n#submission1.to_csv('submission.csv', index=False)\n#submission1['prediction'] = (test_preds>0.25)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f686a684-92be-41b0-a779-3fefbe3bfdb5","_cell_guid":"d90132ba-10b5-4b17-a534-615930634f96","trusted":true},"cell_type":"code","source":"submission1=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission1['target'] = (test_preds>0.25).astype(np.int)\nsubmission1.to_csv('submission.csv', index=False)\nsubmission1['target'] = (test_preds>0.25)\nsubmission1['target']=submission1['target'].map(lambda x:int(x==True))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"265595d4-204a-4d36-a19c-bac708c566be","_cell_guid":"c0c667a7-63a4-4c00-8ffd-27c03942ba5e","trusted":true},"cell_type":"code","source":"submission1.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a5db148f-c34a-48aa-9e00-32bda4673c3c","_cell_guid":"51fdbbe6-6403-4c20-9212-1b0c5fa12b55","trusted":true},"cell_type":"markdown","source":"Important Docs:\n\n Thanking the below people to help me understand how to approach the problem.\n \n     1. (Beginner's guide)(https://www.kaggle.com/frtgnn/beginner-s-stop-to-text-data-a-simple-intro)\n     2. https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n     3. https://www.kaggle.com/ratan123/start-from-here-disaster-tweets-eda-basic-model\n     4.https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python\n     5.https://www.geeksforgeeks.org/generating-word-cloud-in-python-set-2/\n     6. https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb/54398984"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}