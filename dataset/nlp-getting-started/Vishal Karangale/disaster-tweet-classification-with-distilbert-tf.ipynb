{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers[sentencepiece]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-20T15:07:29.745099Z","iopub.execute_input":"2021-12-20T15:07:29.745768Z","iopub.status.idle":"2021-12-20T15:07:38.918601Z","shell.execute_reply.started":"2021-12-20T15:07:29.745648Z","shell.execute_reply":"2021-12-20T15:07:38.917648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some resources that i used\n* **[Hugging Face Course](https://huggingface.co/course/chapter0/1?fw=pt)**\n* **[Fine-Tunning Pretrained Models](https://towardsdatascience.com/russian-troll-tweets-classification-using-bert-abec09e43558)**\n* **[classify tweet with bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)**","metadata":{}},{"cell_type":"markdown","source":"# Importing Tensorflow and Transformers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\n\nimport transformers\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, RobertaTokenizer, DistilBertConfig, BertConfig, TFDistilBertModel","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:38.920795Z","iopub.execute_input":"2021-12-20T15:07:38.921054Z","iopub.status.idle":"2021-12-20T15:07:45.86854Z","shell.execute_reply.started":"2021-12-20T15:07:38.921017Z","shell.execute_reply":"2021-12-20T15:07:45.867837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:45.869754Z","iopub.execute_input":"2021-12-20T15:07:45.869997Z","iopub.status.idle":"2021-12-20T15:07:45.977817Z","shell.execute_reply.started":"2021-12-20T15:07:45.869965Z","shell.execute_reply":"2021-12-20T15:07:45.976886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:45.980169Z","iopub.execute_input":"2021-12-20T15:07:45.980676Z","iopub.status.idle":"2021-12-20T15:07:45.998721Z","shell.execute_reply.started":"2021-12-20T15:07:45.980634Z","shell.execute_reply":"2021-12-20T15:07:45.997928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:46.000022Z","iopub.execute_input":"2021-12-20T15:07:46.000344Z","iopub.status.idle":"2021-12-20T15:07:46.00615Z","shell.execute_reply.started":"2021-12-20T15:07:46.000305Z","shell.execute_reply":"2021-12-20T15:07:46.0054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_df['target'].unique()\nlen_labels = len(labels)\n\nlabels, len_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:46.007632Z","iopub.execute_input":"2021-12-20T15:07:46.008112Z","iopub.status.idle":"2021-12-20T15:07:46.019382Z","shell.execute_reply.started":"2021-12-20T15:07:46.008077Z","shell.execute_reply":"2021-12-20T15:07:46.018585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spliting train data into traun and validation datasets","metadata":{}},{"cell_type":"code","source":"# split train dataset into train, validation and test sets\ntrain_text, val_text, train_labels, val_labels = train_test_split(train_df['text'], train_df['target'], \n                                                                    random_state=101, \n                                                                    test_size=0.2, \n                                                                    stratify=train_df['target'])\n\ntest_text = test_df['text']\n\nlen(train_text), len(val_text), len(test_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:46.020671Z","iopub.execute_input":"2021-12-20T15:07:46.021112Z","iopub.status.idle":"2021-12-20T15:07:46.040065Z","shell.execute_reply.started":"2021-12-20T15:07:46.021055Z","shell.execute_reply":"2021-12-20T15:07:46.039399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing text Data \n\n* Removing HTML content from tweets\n* removing punctuation\n* Removing stop words\n* Lowercasing all tweets\n\n**You can download my small nlp healper functions script from https://github.com/vishalrk1/pytorch/blob/main/nlp_helper.py**","metadata":{}},{"cell_type":"code","source":"# Downloading My nlp healper function script\n!wget https://raw.githubusercontent.com/vishalrk1/pytorch/main/nlp_helper.py","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:46.041351Z","iopub.execute_input":"2021-12-20T15:07:46.041632Z","iopub.status.idle":"2021-12-20T15:07:46.948554Z","shell.execute_reply.started":"2021-12-20T15:07:46.041586Z","shell.execute_reply":"2021-12-20T15:07:46.94776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nlp_helper import remove_html, remove_punctuation, lowercase_text, word_lemmatizer\nimport nltk\nnltk.download('wordnet')\n\n# Remove punctuation\ndef remove_punctuation(text):\n    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    no_punct = ''.join([c for c in text if c not in punc])\n    no_punct = no_punct.lower()\n    return no_punct\n\ndef preprocess_text(text):\n    text = remove_html(text)\n    text = remove_punctuation(text)\n      # text = remove_stopwords(text)\n      # text = lowercase_text(text)\n    text = word_lemmatizer(text)\n    text = ''.join(text)\n    return text\n\n\ntrain_text = train_text.apply(lambda x: preprocess_text(x))\nval_text = val_text.apply(lambda x: preprocess_text(x))\ntest_text = test_text.apply(lambda x: preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:46.950203Z","iopub.execute_input":"2021-12-20T15:07:46.950771Z","iopub.status.idle":"2021-12-20T15:07:56.646662Z","shell.execute_reply.started":"2021-12-20T15:07:46.950727Z","shell.execute_reply":"2021-12-20T15:07:56.645922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Sequence length","metadata":{}},{"cell_type":"code","source":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:07:56.649335Z","iopub.execute_input":"2021-12-20T15:07:56.649682Z","iopub.status.idle":"2021-12-20T15:07:56.941191Z","shell.execute_reply.started":"2021-12-20T15:07:56.649642Z","shell.execute_reply":"2021-12-20T15:07:56.940532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert Model & tokenizer","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer, RobertaTokenizer\ndistil_bert = 'distilbert-base-uncased'\n\n# Defining DistilBERT tokonizer\ntokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True, max_length=30, pad_to_max_length=True)\n\n# downloading model config for model \nconfig = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\nconfig.output_hidden_states = False\n\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config=config)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:44.488505Z","iopub.execute_input":"2021-12-20T15:46:44.488775Z","iopub.status.idle":"2021-12-20T15:46:45.892395Z","shell.execute_reply.started":"2021-12-20T15:46:44.488744Z","shell.execute_reply":"2021-12-20T15:46:45.891573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizing all sentences and creating iinputs for model\nfrom tqdm.notebook import tqdm \n\ndef tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=30, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=True)\n\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:45.89417Z","iopub.execute_input":"2021-12-20T15:46:45.894934Z","iopub.status.idle":"2021-12-20T15:46:45.901896Z","shell.execute_reply.started":"2021-12-20T15:46:45.894893Z","shell.execute_reply":"2021-12-20T15:46:45.900846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Tokenized data","metadata":{}},{"cell_type":"code","source":"print('Input Tokens')\ninput_train_ids, input_train_masks, input_train_segments = tokenize(train_text.to_list(), tokenizer)\n\nprint('Validation Tokens')\ninput_val_ids, input_val_masks, input_val_segments = tokenize(val_text.to_list(), tokenizer)\n\nprint('test Tokens')\ninput_test_ids, input_test_masks, input_test_segments = tokenize(test_text.to_list(), tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:45.903029Z","iopub.execute_input":"2021-12-20T15:46:45.903775Z","iopub.status.idle":"2021-12-20T15:46:54.717265Z","shell.execute_reply.started":"2021-12-20T15:46:45.903739Z","shell.execute_reply":"2021-12-20T15:46:54.716439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating TF Datasets for model Training","metadata":{}},{"cell_type":"code","source":"train_input = tf.data.Dataset.from_tensor_slices((input_train_ids, input_train_masks))\ntrain_output = tf.data.Dataset.from_tensor_slices(train_labels.to_numpy())\ntrain_dataset = tf.data.Dataset.zip((train_input, train_output))\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset, len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:54.71931Z","iopub.execute_input":"2021-12-20T15:46:54.719645Z","iopub.status.idle":"2021-12-20T15:46:54.733057Z","shell.execute_reply.started":"2021-12-20T15:46:54.719606Z","shell.execute_reply":"2021-12-20T15:46:54.732251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_input = tf.data.Dataset.from_tensor_slices((input_val_ids, input_val_masks))\nval_output = tf.data.Dataset.from_tensor_slices(val_labels.to_numpy())\nval_dataset = tf.data.Dataset.zip((val_input, val_output))\nval_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\nval_dataset, len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:54.734569Z","iopub.execute_input":"2021-12-20T15:46:54.735078Z","iopub.status.idle":"2021-12-20T15:46:54.747127Z","shell.execute_reply.started":"2021-12-20T15:46:54.73504Z","shell.execute_reply":"2021-12-20T15:46:54.746446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input = tf.data.Dataset.from_tensor_slices((input_test_ids, input_test_masks))\ntest_dataset = tf.data.Dataset.zip((test_input))\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntest_dataset, len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:54.748387Z","iopub.execute_input":"2021-12-20T15:46:54.748625Z","iopub.status.idle":"2021-12-20T15:46:54.760774Z","shell.execute_reply.started":"2021-12-20T15:46:54.748592Z","shell.execute_reply":"2021-12-20T15:46:54.760159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model","metadata":{}},{"cell_type":"code","source":"input_ids_in = tf.keras.layers.Input(shape=(30,), name='input_token', dtype='int32')\ninput_masks_in = tf.keras.layers.Input(shape=(30,), name='masked_token', dtype='int32') \n\nembedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)\nX = tf.keras.layers.Dropout(0.4)(X)\nX = tf.keras.layers.LSTM(128, return_sequences=False)(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\nfor layer in model.layers[:3]:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:54.762482Z","iopub.execute_input":"2021-12-20T15:46:54.762926Z","iopub.status.idle":"2021-12-20T15:46:56.256501Z","shell.execute_reply.started":"2021-12-20T15:46:54.762888Z","shell.execute_reply":"2021-12-20T15:46:56.255776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:46:56.257809Z","iopub.execute_input":"2021-12-20T15:46:56.258319Z","iopub.status.idle":"2021-12-20T15:46:56.284252Z","shell.execute_reply.started":"2021-12-20T15:46:56.25828Z","shell.execute_reply":"2021-12-20T15:46:56.283599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# optimizer, Loss function and training Model","metadata":{}},{"cell_type":"code","source":"model.compile(\n    loss = 'binary_crossentropy',\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)\n\nhistory_1 = model.fit(\n    train_dataset,\n    epochs = 25,\n    steps_per_epoch = len(train_dataset),\n    validation_data = val_dataset,\n    validation_steps = int(0.5 * len(val_dataset)),\n    callbacks = [\n                 tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True),\n                 tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:52:17.27137Z","iopub.execute_input":"2021-12-20T15:52:17.271653Z","iopub.status.idle":"2021-12-20T15:52:42.974742Z","shell.execute_reply.started":"2021-12-20T15:52:17.271623Z","shell.execute_reply":"2021-12-20T15:52:42.971957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:49:12.008536Z","iopub.execute_input":"2021-12-20T15:49:12.008748Z","iopub.status.idle":"2021-12-20T15:49:24.424308Z","shell.execute_reply.started":"2021-12-20T15:49:12.008715Z","shell.execute_reply":"2021-12-20T15:49:24.423606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# saving test df predictions to csv file","metadata":{}},{"cell_type":"code","source":"pred = model.predict([input_test_ids, input_test_masks])\npred = np.squeeze(pred).round()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:49:55.337615Z","iopub.execute_input":"2021-12-20T15:49:55.338176Z","iopub.status.idle":"2021-12-20T15:50:00.546294Z","shell.execute_reply.started":"2021-12-20T15:49:55.338139Z","shell.execute_reply":"2021-12-20T15:50:00.545478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['id'] = test_df['id']\ndf['target'] = pred.astype(int)\n\ndf.to_csv('submission.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-20T15:50:37.770554Z","iopub.execute_input":"2021-12-20T15:50:37.771024Z","iopub.status.idle":"2021-12-20T15:50:37.793847Z","shell.execute_reply.started":"2021-12-20T15:50:37.770985Z","shell.execute_reply":"2021-12-20T15:50:37.793042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}