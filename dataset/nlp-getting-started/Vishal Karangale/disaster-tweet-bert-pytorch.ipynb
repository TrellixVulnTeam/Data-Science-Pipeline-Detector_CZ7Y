{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-10T03:15:29.695024Z","iopub.execute_input":"2022-03-10T03:15:29.695297Z","iopub.status.idle":"2022-03-10T03:15:38.486008Z","shell.execute_reply.started":"2022-03-10T03:15:29.695221Z","shell.execute_reply":"2022-03-10T03:15:38.485193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nimport json\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# nltk imports for preprocessing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nimport random\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Hugging face Transformers Import\nimport transformers\nfrom transformers import BertModel, BertTokenizerFast\nfrom transformers import DistilBertModel, DistilBertTokenizerFast, AdamW","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:38.488387Z","iopub.execute_input":"2022-03-10T03:15:38.488938Z","iopub.status.idle":"2022-03-10T03:15:45.78945Z","shell.execute_reply.started":"2022-03-10T03:15:38.488897Z","shell.execute_reply":"2022-03-10T03:15:45.788694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Seed","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=None, seed_torch=True):\n    if seed is None:\n        seed = np.random.choice(2 ** 32)\n    random.seed(seed)\n    np.random.seed(seed)\n    if seed_torch:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n\n    print(f'Random seed {seed} has been set.')\n    \nset_seed(seed=101)\n\n# device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:45.792018Z","iopub.execute_input":"2022-03-10T03:15:45.792313Z","iopub.status.idle":"2022-03-10T03:15:45.844627Z","shell.execute_reply.started":"2022-03-10T03:15:45.792276Z","shell.execute_reply":"2022-03-10T03:15:45.843887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:45.846671Z","iopub.execute_input":"2022-03-10T03:15:45.848006Z","iopub.status.idle":"2022-03-10T03:15:45.914512Z","shell.execute_reply.started":"2022-03-10T03:15:45.847964Z","shell.execute_reply":"2022-03-10T03:15:45.913768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:45.917333Z","iopub.execute_input":"2022-03-10T03:15:45.917602Z","iopub.status.idle":"2022-03-10T03:15:45.936536Z","shell.execute_reply.started":"2022-03-10T03:15:45.917575Z","shell.execute_reply":"2022-03-10T03:15:45.935903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:45.93778Z","iopub.execute_input":"2022-03-10T03:15:45.938056Z","iopub.status.idle":"2022-03-10T03:15:45.947221Z","shell.execute_reply.started":"2022-03-10T03:15:45.938013Z","shell.execute_reply":"2022-03-10T03:15:45.946328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# downloading stopwords\nnltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint(f\"stopwords: {STOPWORDS[:5]}\")\n\nporter = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:45.948746Z","iopub.execute_input":"2022-03-10T03:15:45.949091Z","iopub.status.idle":"2022-03-10T03:15:46.068784Z","shell.execute_reply.started":"2022-03-10T03:15:45.949053Z","shell.execute_reply":"2022-03-10T03:15:46.06792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text Preprocessing Function\ndef preprocess(text, stopwords=STOPWORDS):\n    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in paranthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:46.070266Z","iopub.execute_input":"2022-03-10T03:15:46.07053Z","iopub.status.idle":"2022-03-10T03:15:46.079462Z","shell.execute_reply.started":"2022-03-10T03:15:46.070483Z","shell.execute_reply":"2022-03-10T03:15:46.078537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.text = train_df.text.apply(preprocess)\ntest_df.text = test_df.text.apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:46.080885Z","iopub.execute_input":"2022-03-10T03:15:46.08113Z","iopub.status.idle":"2022-03-10T03:15:46.639985Z","shell.execute_reply.started":"2022-03-10T03:15:46.081096Z","shell.execute_reply":"2022-03-10T03:15:46.63925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text'][0]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:46.642691Z","iopub.execute_input":"2022-03-10T03:15:46.643139Z","iopub.status.idle":"2022-03-10T03:15:46.650585Z","shell.execute_reply.started":"2022-03-10T03:15:46.643099Z","shell.execute_reply":"2022-03-10T03:15:46.64987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Tokenizer","metadata":{}},{"cell_type":"code","source":"# Downloading pretrained BERT tokenizer\n# model_name = 'bert-base-cased'\n# tokenizer = BertTokenizerFast.from_pretrained(model_name)\n\nmodel_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:46.65183Z","iopub.execute_input":"2022-03-10T03:15:46.652263Z","iopub.status.idle":"2022-03-10T03:15:48.505199Z","shell.execute_reply.started":"2022-03-10T03:15:46.652224Z","shell.execute_reply":"2022-03-10T03:15:48.504435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_df[\"text\"].tolist()]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:48.50923Z","iopub.execute_input":"2022-03-10T03:15:48.511586Z","iopub.status.idle":"2022-03-10T03:15:48.923276Z","shell.execute_reply.started":"2022-03-10T03:15:48.511546Z","shell.execute_reply":"2022-03-10T03:15:48.92263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, df, tokenizer, istrain=True):\n        self.text = df[\"text\"].tolist()\n        self.tokenizer = tokenizer\n        self.istrain = istrain\n        \n        if self.istrain:\n            self.labels = df['target'].tolist()\n            \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        \n        if self.istrain:\n            label = self.labels[index]\n            return text, label\n        \n        return text\n    \n    def collate_fn(self, batch):\n        \"\"\"Processing on a batch\"\"\"\n        # Getting Input\n        batch = np.array(batch)\n        \n        if self.istrain:\n            texts = batch[:,0]\n        else:\n            texts = batch\n\n        # tokenizing text inputs\n        tokenized_text = self.tokenizer(texts.tolist(), return_tensors='pt', max_length=25, padding='max_length', truncation=True)\n        \n        if self.istrain:\n            target = batch[:, 1]\n            target = torch.LongTensor(target.astype(np.int32))\n            return tokenized_text, target\n\n        return tokenized_text\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        dataloader = DataLoader(dataset=self, batch_size=batch_size, collate_fn=self.collate_fn, shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n        return dataloader","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:48.927491Z","iopub.execute_input":"2022-03-10T03:15:48.929578Z","iopub.status.idle":"2022-03-10T03:15:48.944744Z","shell.execute_reply.started":"2022-03-10T03:15:48.929536Z","shell.execute_reply":"2022-03-10T03:15:48.944105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:48.948717Z","iopub.execute_input":"2022-03-10T03:15:48.950747Z","iopub.status.idle":"2022-03-10T03:15:48.969295Z","shell.execute_reply.started":"2022-03-10T03:15:48.95071Z","shell.execute_reply":"2022-03-10T03:15:48.967247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:48.970359Z","iopub.execute_input":"2022-03-10T03:15:48.971131Z","iopub.status.idle":"2022-03-10T03:15:48.984174Z","shell.execute_reply.started":"2022-03-10T03:15:48.971089Z","shell.execute_reply":"2022-03-10T03:15:48.983277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating Training Dataset and DataLoader\ntrain_dataset = TweetDataset(new_train_df, tokenizer=tokenizer, istrain=True)\ntrain_dataloader = train_dataset.create_dataloader(batch_size=32, shuffle=True)\n\n# creating Validation Dataset and DataLoader\nval_dataset = TweetDataset(val_df, tokenizer=tokenizer, istrain=True)\nval_dataloader = val_dataset.create_dataloader(batch_size=32, shuffle=True)\n\n# creating Test Dataset\ntest_dataset = TweetDataset(test_df, tokenizer=tokenizer, istrain=False)\ntest_dataloader = test_dataset.create_dataloader(batch_size=32, shuffle=False)\n\ntest_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:48.98541Z","iopub.execute_input":"2022-03-10T03:15:48.986116Z","iopub.status.idle":"2022-03-10T03:15:48.999417Z","shell.execute_reply.started":"2022-03-10T03:15:48.986075Z","shell.execute_reply":"2022-03-10T03:15:48.998505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:49.000704Z","iopub.execute_input":"2022-03-10T03:15:49.001202Z","iopub.status.idle":"2022-03-10T03:15:49.008275Z","shell.execute_reply.started":"2022-03-10T03:15:49.001164Z","shell.execute_reply":"2022-03-10T03:15:49.007557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch = next(iter(train_dataloader))\n# tokenized_text = batch[:-1]\n\n# tokenized_text['input_ids'] = tokenized_text['input_ids'].to(device)\n# tokenized_text['attention_mask'] = tokenized_text['attention_mask'].to(device)\n# tokenized_text","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:15:49.009319Z","iopub.execute_input":"2022-03-10T03:15:49.009715Z","iopub.status.idle":"2022-03-10T03:15:49.027583Z","shell.execute_reply.started":"2022-03-10T03:15:49.00968Z","shell.execute_reply":"2022-03-10T03:15:49.026742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_classes, freeze_bert=True):\n        super(Model, self).__init__()\n        \n        self.bert_model = DistilBertModel.from_pretrained(model_name)\n        self.bi_lstm = nn.LSTM(768, 256, batch_first=True, num_layers=1, bidirectional=True, dropout=0.4)\n        self.fc_1 = nn.Linear(2*256, 128)\n        self.fc_final = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.5)\n        \n        if freeze_bert:\n            for param in self.bert_model.parameters():\n                param.requires_grad = False\n        else:\n            for param in self.bert_model.parameters():\n                param.requires_grad = True\n                \n    def forward(self, text_seq):\n        bert_output = self.bert_model(**text_seq)\n        x = bert_output['last_hidden_state']\n        x, _ = self.bi_lstm(x)\n        x = x[:, 0]\n        x = F.relu(self.fc_1(x))\n        x = self.dropout(x)\n        x = self.fc_final(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:17.480619Z","iopub.execute_input":"2022-03-10T03:20:17.480935Z","iopub.status.idle":"2022-03-10T03:20:17.491753Z","shell.execute_reply.started":"2022-03-10T03:20:17.480903Z","shell.execute_reply":"2022-03-10T03:20:17.490981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(num_classes=2, freeze_bert=False)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:17.741044Z","iopub.execute_input":"2022-03-10T03:20:17.74125Z","iopub.status.idle":"2022-03-10T03:20:18.936295Z","shell.execute_reply.started":"2022-03-10T03:20:17.741225Z","shell.execute_reply":"2022-03-10T03:20:18.935512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:18.937956Z","iopub.execute_input":"2022-03-10T03:20:18.938413Z","iopub.status.idle":"2022-03-10T03:20:18.946206Z","shell.execute_reply.started":"2022-03-10T03:20:18.938372Z","shell.execute_reply":"2022-03-10T03:20:18.945401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:22.435465Z","iopub.execute_input":"2022-03-10T03:20:22.435726Z","iopub.status.idle":"2022-03-10T03:20:22.440585Z","shell.execute_reply.started":"2022-03-10T03:20:22.435696Z","shell.execute_reply":"2022-03-10T03:20:22.439779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE=1e-5\npatience = 8\nNUM_EPOCHS = 50\n\n# Define Loss\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n# optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# learning rate schedular\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:22.629161Z","iopub.execute_input":"2022-03-10T03:20:22.629372Z","iopub.status.idle":"2022-03-10T03:20:22.640859Z","shell.execute_reply.started":"2022-03-10T03:20:22.629346Z","shell.execute_reply":"2022-03-10T03:20:22.640034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model training\nfrom tqdm.notebook import tqdm\n\nsave_path='Bert-Model-1.pt'\n\nresults = {\n    'train_losses': [],\n    'valid_losses': [],\n    'train_acc': [],\n    'valid_acc': [],\n}\n\nbest_val_loss = np.inf\n\nfor epoch in range(NUM_EPOCHS):\n    print(f'<----- Epoch: {epoch+1} ----->')\n    train_loss = 0\n    valid_loss = 0\n\n    train_acc = 0\n    valid_acc = 0\n\n    # training model on training dataset\n    model.train()\n    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n\n        text_seq, labels = batch\n\n        input_ids, attention_mask = text_seq['input_ids'].to(device), text_seq['attention_mask'].to(device)\n        labels = labels.to(device)\n\n        inputs = {'input_ids':input_ids, 'attention_mask': attention_mask}\n\n        optimizer.zero_grad()\n\n        output = model.forward(inputs)\n        loss = loss_fn(output, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        output_class = torch.sigmoid(output).argmax(dim=1)\n        train_acc += (output_class == labels).sum().item()/len(output)\n\n    # evaluating model on validation dataset\n    model.eval()\n    for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n    \n        text_seq, labels = batch\n\n        input_ids, attention_mask = text_seq['input_ids'].to(device), text_seq['attention_mask'].to(device)\n        labels = labels.to(device)\n\n        inputs = {'input_ids':input_ids, 'attention_mask': attention_mask}\n\n        output = model(inputs)\n        loss = loss_fn(output, labels)\n\n        valid_loss += loss.item()\n\n        output_class = torch.sigmoid(output).argmax(dim=1)\n        valid_acc += (output_class == labels).sum().item()/len(output)\n\n    # calculating losses\n    train_loss = train_loss / len(train_dataloader)\n    valid_loss = valid_loss / len(val_dataloader)\n\n    # calculating accuracy\n    train_acc = train_acc / len(train_dataloader)\n    valid_acc = valid_acc / len(val_dataloader)\n\n    scheduler.step(valid_loss)\n\n    # Early stopping\n    if valid_loss < best_val_loss:\n        best_val_loss = valid_loss\n        print('Saving Model!!')\n        torch.save(model.state_dict(), save_path)\n        _patience = patience  # reset _patience\n    else:\n        _patience -= 1\n        if not _patience:  # 0\n            print(\"Stopping early!\")\n            break\n\n    # storing losses and accuracy\n    results['train_losses'].append(train_loss)\n    results['valid_losses'].append(valid_loss)\n    results['train_acc'].append(train_acc)\n    results['valid_acc'].append(valid_acc)\n\n    print('Training Loss: {:.6f} \\tTraining Accuracy: {:.2f}'.format(train_loss, train_acc))\n    print('Validation Loss: {:.6f} \\tValidation Accuracy: {:.2f}'.format(valid_loss, valid_acc))\n    print(f\"learning_rate: {optimizer.param_groups[0]['lr']:.2E} \\tpatience: {_patience}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:20:23.612245Z","iopub.execute_input":"2022-03-10T03:20:23.612514Z","iopub.status.idle":"2022-03-10T03:22:26.528301Z","shell.execute_reply.started":"2022-03-10T03:20:23.612475Z","shell.execute_reply":"2022-03-10T03:22:26.527496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting Loss and Accuracy Graphs","metadata":{}},{"cell_type":"code","source":"# code to plot loss and accuracy\ndef plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n    epochs = len(train_loss)\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n    ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Epoch vs Loss')\n    ax1.legend()\n\n    ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n    ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Epoch vs Accuracy')\n    ax2.legend()\n    fig.set_size_inches(15.5, 5.5)\n    #plt.show()\n    \nplot_loss_accuracy(results['train_losses'], results['train_acc'], results['valid_losses'], results['valid_acc'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:22:57.958974Z","iopub.execute_input":"2022-03-10T03:22:57.95946Z","iopub.status.idle":"2022-03-10T03:22:58.354148Z","shell.execute_reply.started":"2022-03-10T03:22:57.959422Z","shell.execute_reply":"2022-03-10T03:22:58.353461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction on test Data","metadata":{}},{"cell_type":"code","source":"def pred_step(model, dataloader, device):\n    \"\"\"Validation or test step.\"\"\"\n    # Set model to eval mode\n    model.eval()\n    y_trues, y_probs = [], []\n\n    # Iterate over val batches\n    with torch.inference_mode():\n        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n            # Step\n            text_seq = batch\n\n            input_ids, attention_mask = text_seq['input_ids'].to(device), text_seq['attention_mask'].to(device)\n            inputs = {'input_ids':input_ids, 'attention_mask': attention_mask}\n            \n            z = model(inputs)  # Forward pass\n\n            # Store outputs\n            y_prob = torch.sigmoid(z).cpu().numpy()\n            y_probs.extend(y_prob)\n            \n    return np.vstack(y_probs)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:23:02.103157Z","iopub.execute_input":"2022-03-10T03:23:02.103715Z","iopub.status.idle":"2022-03-10T03:23:02.110879Z","shell.execute_reply.started":"2022-03-10T03:23:02.103676Z","shell.execute_reply":"2022-03-10T03:23:02.109981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('./Bert-Model-1.pt'))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:23:03.583974Z","iopub.execute_input":"2022-03-10T03:23:03.584593Z","iopub.status.idle":"2022-03-10T03:23:03.721369Z","shell.execute_reply.started":"2022-03-10T03:23:03.584558Z","shell.execute_reply":"2022-03-10T03:23:03.720541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = pred_step(model, test_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:23:05.020674Z","iopub.execute_input":"2022-03-10T03:23:05.021389Z","iopub.status.idle":"2022-03-10T03:23:07.105162Z","shell.execute_reply.started":"2022-03-10T03:23:05.021338Z","shell.execute_reply":"2022-03-10T03:23:07.104412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = preds.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:23:07.786839Z","iopub.execute_input":"2022-03-10T03:23:07.787381Z","iopub.status.idle":"2022-03-10T03:23:07.791677Z","shell.execute_reply.started":"2022-03-10T03:23:07.787343Z","shell.execute_reply":"2022-03-10T03:23:07.7905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['id'] = test_df['id']\ndf['target'] = test_pred.astype(int)\n\ndf.to_csv('submission.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-10T03:23:08.060667Z","iopub.execute_input":"2022-03-10T03:23:08.0609Z","iopub.status.idle":"2022-03-10T03:23:08.082981Z","shell.execute_reply.started":"2022-03-10T03:23:08.060869Z","shell.execute_reply":"2022-03-10T03:23:08.082347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}