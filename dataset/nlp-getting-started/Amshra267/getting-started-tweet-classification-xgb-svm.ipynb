{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import PorterStemmer, SnowballStemmer, WordNetLemmatizer \nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.preprocessing import  MinMaxScaler, RobustScaler, StandardScaler, LabelEncoder as le\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading files","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list down some elements of test data\ntest_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the pandas groupby function to calculate samples per labels\n\ncount_data = train_data[['text','target']].groupby('target').count().reset_index()\ncount_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting instances per labels\n\nsns.barplot(x = 'target',y = 'text', data = count_data)\nplt.title('no. of instances vs labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## merge train and test data to perform common operations on both\n## due to merging an extra column of target will form, which contains nan values\n\ndataset = pd.concat([train_data, test_data])\nprint(dataset.shape)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking null values in both entire dataset and train data\nprint(dataset.isnull().sum())\nprint('-'*100)\nprint(train_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking null values in both entire dataset and train data\nprint(dataset.info())\nprint('-'*100)\nprint(train_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['len_letters'] = dataset['text'].apply(len)\ntrain_data['len_letters'] = train_data['text'].apply(len)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(ncols = 2)\nsns.distplot(train_data['len_letters'][train_data['target'] == 1 ],label = 'disaster',color = 'r' ,ax = axes[0]) ## denoting disaster tweets length\nsns.distplot(train_data['len_letters'][train_data['target'] == 0 ],label = 'non-disaster',color = 'g', ax= axes[1] ) ## denoting not disaster tweets length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting length of data corresponding to every label\ntrain_data[train_data['target']==1].describe()  # for disastorous label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_data[train_data['target']==0].describe()  # for non-disastorous label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling every unknown keyword with not_known\ndataset['keyword'] = dataset['keyword'].fillna('not_known')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['text'][dataset['keyword'] == 'blaze'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it seems from above that the word blaze doesn't add any information due to different semantic meanings in different samples, let's n investigate about location as it may provide some info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['location'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HUGE!! Unknown locations, let's diving deeper,\nMay be most unknown locations were related to non disaster ones","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# now individual label by label\nprint('For disastrous, unknown locations : ',train_data['location'][train_data['target']==1].isnull().sum())\nprint('For non disastrous, unknown locations : ',train_data['location'][train_data['target']==0].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assumption is mostly correct","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['location'][train_data['target']==1].sample(5) # randomly seen some samples origin location","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling localation with unknown\ndataset['location'].fillna('unknown', inplace = True)\ntrain_data['location'].fillna('unknown', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['location'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now encoding labels using their frequency wise ratio for locations, may be the no. of times a place occurs may affect the tweet\nnum_locations = Counter(dataset['location'])\nnum_locations\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"look like no relation of frequency, hence using label encoding for it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding\ndataset['location'] = le().fit_transform(dataset['location'])\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now time for analysing the text\n\n#converting the text into lower case\ndataset['text'] = dataset['text'].map(lambda x: x.lower())\ntrain_data['text'] = train_data['text'].map(lambda x: x.lower())\n\ndataset['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dividing training data text on the basis of labels and then find common words in it\n\ndis_text = train_data['text'][train_data['target']==1]\ndis_text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndis_text = train_data['text'][train_data['target']==0]\nndis_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Tokenize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_count = dis_text.map(lambda x: nltk.word_tokenize(x)) # for disastrous \nndis_count = ndis_text.map(lambda x : nltk.word_tokenize(x)) # for non disastrous\ndataset['text'] = dataset['text'].map(lambda x : nltk.word_tokenize(x)) # for whole dataset tokenize\nndis_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's investigate which words are more common in each cases\nfrom nltk import FreqDist\n# for target =1\ncollection_words_dist = []\nfor i in list(dis_count):\n    collection_words_dist.extend(i)\n    \nmap_1 = FreqDist(collection_words_dist)\nmap_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot some of them\nplt.figure(figsize = (8,8))\nsns.barplot(x = list(dict(map_1.most_common(15)).keys()), y = list(dict(map_1.most_common(15)).values())) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for target =0\ncollection_words_ndist = []\nfor i in list(ndis_count):\n    collection_words_ndist.extend(i)\n    \nmap_0 = FreqDist(collection_words_ndist)\nmap_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot some of them\nplt.figure(figsize = (8,8))\nsns.barplot(x = list(dict(map_0.most_common(15)).keys()), y = list(dict(map_0.most_common(15)).values())) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Key takeaways \n1. most of the frequent occuring words are common except some words like I, hence we need to keep i as a tweet related to personal intrested may be mostly non-disastrous\n2. we need to lemmatize the text to create further root level meaning\n3. all the symbols will be assumed to add some type of information hence keeping it also.\n4. but we need to remove some patterns as it will common in both and may deviate the tweets pattern","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ### Firstly,Applying lematization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lematiing the text to obtain root level text\n## lematization is beneficial if appropriate pos tags is used \n# function to map nltk tags with wordnet tags\nlemmatizer = WordNetLemmatizer()\ndef nltk_tags_2_word_tags(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n    \n# function to create lemmatized sentences from tokenized words\ndef lemmatized_sentences(tokenized_sentence):\n    pos = nltk.pos_tag(tokenized_sentence)  # returns a tuple of words with their nltk tags\n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tags_2_word_tags(x[1])), pos)\n    \n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)\n    \n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now converting our tokenized lowered words into lemmatized sentences\n\ndis_lem = dis_count.apply(lemmatized_sentences)\nndis_lem = ndis_count.apply(lemmatized_sentences)\ndataset['text'] = dataset['text'].apply(lemmatized_sentences)\n\n# finally dataset is lemmatized let's see\ndataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_lem ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### text cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re   #regex library \n\n# function to remove patterns\ndef remove_pattern(input_txt, pattern):\n    reg_obj = re.compile(pattern)\n    input_txt = reg_obj.sub(r'', input_txt)\n        \n    return input_txt   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing twitter handles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing https type symbol","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference : https://www.kaggle.com/shahules/tweets-complete-eda-and-basic-modeling\n\n\ndataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,'https?://\\S+|www\\.\\S+'))\ndataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,'<.*?>'))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove punctuations, special characters, numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,\"[^a-zA-Z# ]\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now using tf-idf to create new words using bigram + unigram\n\ntfidf = TfidfVectorizer(ngram_range = (1,1),max_df=0.90, min_df=2,stop_words = 'english')\ntext_set = tfidf.fit_transform(dataset['text'])\ntext_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Adding location to this sparse matrix*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\ndataset_dtm = hstack((text_set,np.array(dataset['location'])[:,None]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dtm=text_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dtm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### slicing back into train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dtm = dataset_dtm.tocsr()  # converting to sparse row format\nx_train = dataset_dtm[0:len(train_data)]\nx_test = dataset_dtm[len(train_data):]\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_data['target']\nlen(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL BUILDING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*kfolds for cross validation*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits = 5 )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(kernel = 'rbf',probability = True))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\n#classifiers.append(LGBMClassifier(objective='classification', random_state=random_state))\n#classifiers.append(AdaBoostClassifier(ExtraTreesClassifier(random_state=2,max_depth = None,min_samples_split= 2,min_samples_leaf = 1,bootstrap = False,n_estimators =320), random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression())\nclassifiers.append(XGBClassifier(random_state=random_state))\n#classifiers.append(LinearDiscriminantAnalysis())\n\"\"\"\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(-cross_val_score(classifier,x_train, y = y_train, scoring = 'accuracy', cv = kfold , n_jobs =-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVR\",\"DecisionTree\",\"lgbm\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"KNeighboors\",\"LogisticRegression\",\"xgboost\",\"LDA\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\ncv_res\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter = {'solver':['liblinear','lbfgs'],\n            'max_iter':[200,400]}\n\nLogis_clf = LogisticRegression()\n\nlreg = GridSearchCV(Logis_clf, param_grid = parameter, cv = 3, verbose=True, n_jobs=-1)\nlreg.fit(x_train, y_train) # training the model\n\nlreg_best = lreg.best_estimator_\n\nprint(lreg.best_score_)\nprint(lreg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### stacking base models\n\nhere we are feeding raw models into stacking as we except that the meta model will extract best out of everyone","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# feeding raw models into stacking ensemble as the metal model will extract tht best out of each one\nfrom vecstack import stacking\nfrom sklearn.metrics import accuracy_score,f1_score\n\nS_train, S_test = stacking(classifiers,                   \n                           x_train, y_train, x_test,   \n                           regression= False,\n                          \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=True,\n         \n                           save_dir=None, \n             \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"argmax_train = []\nargmax_test = []\nfor i in range(0,S_train.shape[1],2):\n    argmax_train.append( np.argmax(S_train[:,i:i+2],axis=1))\n    argmax_test.append( np.argmax(S_test[:,i:i+2],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"argmax_train = np.array(argmax_train,dtype= np.int64).T\nargmax_test = np.array(argmax_test,dtype= np.int64).T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"argmax_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"argmax_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here using overall probabilities for meta model\n## from sklearn.metrics import f1_score\nmodelc = LogisticRegression()\n    \nmodel1c = modelc.fit(S_train, y_train)\ny_pred1c = model1c.predict_proba(S_train)\ny_predc = model1c.predict_proba(S_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1,axis=1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here using predictions for metal model\n## from sklearn.metrics import f1_score\nmodel = XGBClassifier(random_state=2, objective = 'reg:linear', n_jobs=-1, learning_rate= 0.5, \n                      n_estimators=30, max_depth=20)\n    \nmodel1 = model.fit(argmax_train, y_train)\ny_pred1 = model1.predict_proba(argmax_train)\ny_pred = model1.predict_proba(argmax_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1,axis=1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking the distribution of prediction\nsns.distplot(y_pred)\nsns.distplot(y_predc)\n             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = np.argmax(y_pred+y_predc,axis=1)\n\nsample_submission.to_csv('submission_with_stacking.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this notebook helpful **Please Upvote**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}