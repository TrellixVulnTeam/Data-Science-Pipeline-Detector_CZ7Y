{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the necessary library\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px \nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport collections","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of missing words in columns 'keyowrd' and 'location'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of missing words in columns 'keyowrd' and 'location'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['keyword'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the number of unique keywords in trainset\n\ndf_train['keyword'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the number of unique keywords in test dataset\n\ndf_test['keyword'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that there are 4342 disaster tweets and 3271 Non-disaster tweets in train dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train['target'].value_counts()[:].index\nvalues = df_train['target'].value_counts()[:].values\n\ncolors=['#2678bf', '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see how our Real and Fake tweets looks like in our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real = df_train[df_train['target'] == 1]['text']\nreal.values[1:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the above tweets mentioned about the disaster tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = df_train[df_train['target'] == 0]['text']\nfake.values[1:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the above tweets have no informations about disaster","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploring the 'location' column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the ambigious locations name with Standard names\ndf_train['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottom_5_location = df_train.sort_values(\"location\", ascending=True).head(5)\n\nfig = px.bar(bottom_5_location,\n            x = \"keyword\",\n            y=\"location\",\n            orientation='v',\n            height=800,\n            title=\"Bottom 5 location \",\n            color=\"keyword\"\n            )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above one is the plot of location with particular type of keywords used at that place ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train[df_train['target'] == 1]['keyword'].value_counts()[:10].index\nvalues = df_train[df_train['target'] == 1]['keyword'].value_counts()[:10].values\n\ncolors = df_train['keyword']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above one give output as top 10 commonly used key words during the time of Disaster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train[df_train['target'] == 0]['keyword'].value_counts()[:10].index\nvalues = df_train[df_train['target'] == 0]['keyword'].value_counts()[:10].values\n\ncolors = df_train['keyword']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above one give output as top 10 commonly used key words during the time of Disaster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train['location'].value_counts()[:10].index\nvalues = df_train['location'].value_counts()[:10].values\n\ncolors = df_train['location']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By the above plot we may say that these are the locations with maximum number of chances of disasterous and non-disastrous tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=df_train['location'].value_counts()[:5].index, x=df_train['location'].value_counts()[:5], orient='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n* Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a look at both the trainig and test set data\ndf_train['text'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'][:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining a function to clean text ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now applying clean_text function to both train and test datasets\n\ndf_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how has been our train and test datasets have been changed after applying the clean_text function\n\ndf_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice from the above 2 outputs all the unnecessary things of the train and test 'text' have been removed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Plotting the WordCloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(\" \".join(real))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Real Tweets mentioning about Disaster', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(\" \".join(fake))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Fake Tweets mentioning about Disaster', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print()\ntext = \"I love you, don't you\"\n\n# instantiate tokenizer class\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \", text)\nprint(\"Tokenization by whitespace: \", tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer: \", tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation: \", tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression: \", tokenizer4.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\n\n# instantiate tokenizer class\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# Tokenizing the trainig set\ndf_train['text'] = df_train['text'].apply(lambda x: tokenizer.tokenize(x))\ndf_test['text'] = df_test['text'].apply(lambda x: tokenizer.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print()\nprint('Tokenized string:')\ndf_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print()\nprint('Tokenized string:')\ndf_test['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to remove stop words. Stop words are words that don't add significant meaning to the text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definig a function to remove the stopwords\n\ndef remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the stopwords from the train and test set\n\ndf_train['text'] = df_train['text'].apply(lambda x: remove_stopwords(x))\ndf_test['train'] = df_test['text'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizing the Tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming and Lemmatization examples\n\ntext = \"How is the Josh\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer \nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer = nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    \n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf_train['text'] = df_train['text'].apply(lambda x : combine_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x : combine_text(x))\ndf_train['text']\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text preprocessing function\ndef text_preprocessing(text):\n   \n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [word for word in tokenized_text if word not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming tokens to a Vector","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CountVectorizer can do all the above task of preprocessing, tokenization, and stop words removal\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df_train['text'])\ntest_vectors = count_vectorizer.transform(df_test['text'])\n    \n    \n# Keeping only non-zero elements to preserve spaces\nprint(train_vectors[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df_train['text'])\ntest_tfidf = tfidf.transform(df_test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Final Classification Model\n\nSince all the cleaning and preprocessing has been done so the data is ready to fed up in the model for classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"MultinimialNB\": MultinomialNB()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wow our scores are getting even high scores even when applying cross validation.\n# Lets apply the Classifiers 1st on Countvectoizers\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(train_vectors, df_train[\"target\"])\n    training_score = cross_val_score(classifier, train_vectors, df_train[\"target\"], cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets apply the Classifiers tfidf\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(train_tfidf, df_train[\"target\"])\n    training_score = cross_val_score(classifier, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above 2 classification output on countvectorizer and tfidf its clear that counvectorizer is performing much better than the tfidf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nfrom sklearn import model_selection\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_NB_TFIDF.fit(train_tfidf, df_train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using XGBoost on CountVectorizers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import model_selection\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                           subsample=0.8, nthread=10, learning_rate=0.01)\n\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, df_train[\"target\"], cv=5, scoring=\"f1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using XGBoost on TFIDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you found this notebook helpful please upvote it","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}