{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## **0. はじめに**\n\n日本語の処理もまじえながら、コンペデータを使いつつ自分用にNLPを少しずつまとめていきます。<br>\nゼロから作るDeep Learning―自然言語処理編で理解したこともまとめていきます。<br>\nRNNからまとめていきますが、少しずつ書き足していきます。<br>","metadata":{}},{"cell_type":"markdown","source":"#### 0.1　最新版の日本語対応transformer インストール\n\n現在のtransformersはmecabではなくfugashiを使っています。（fugashiはMeCabのラッパー）<br>\n以下を実行するだけで、fugashiも辞書も全部入ります。<br>\n昔はfugashiを別途installしていた。<br>\n\nhttps://qiita.com/m__k/items/863013dbe847dc613844","metadata":{}},{"cell_type":"code","source":"!pip install transformers[ja]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nto\nimport gensim.downloader\nimport transformers\nfrom wordcloud import WordCloud\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv',dtype={'id': np.int16, 'target': np.int8})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-03T12:50:54.937986Z","iopub.execute_input":"2021-11-03T12:50:54.938603Z","iopub.status.idle":"2021-11-03T12:50:54.969843Z","shell.execute_reply.started":"2021-11-03T12:50:54.938562Z","shell.execute_reply":"2021-11-03T12:50:54.969075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Table of Contents:<br>\n1. [refernce](https://www.kaggle.com/tfukuda675/lnp-beginner#1.-reference)<br>\n2. [Unique process of NLP](https://www.kaggle.com/tfukuda675/lnp-beginner#2.-Unique-process-of-NLP)<br>\n 2.1 Tokenization\n 2.2 Padding\n3. [RNN](https://www.kaggle.com/tfukuda675/lnp-beginner#3.-RNN)<br>\n 3.1 simple code<br>\n4. LSTM<br>\n 4.1 simple code<br>\n5. Transformer<br>\n 5.1 base line<br>","metadata":{}},{"cell_type":"markdown","source":"## **1. reference**\n\n#### This kernel includes codes and ideas from kernels below.\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/tfukuda675/nlp-with-disaster-tweets-eda-cleaning-and-bert/edit/run/78390965) by [gunesevitan](https://www.kaggle.com/gunesevitan)\n* [n-Depth Guide 📙 to Google's BERT](https://www.kaggle.com/ratan123/in-depth-guide-to-google-s-bert) by [ratan rohith]\n(https://www.kaggle.com/ratan123)\n* [Introduction to Japanese spaCy/GINZA [日本語/Eng]](https://www.kaggle.com/marutama/introduction-to-japanese-spacy-ginza-eng) by [NIWASHI](https://www.kaggle.com/marutama)\n* <a href=\"https://jp.mathworks.com/content/dam/mathworks/mathworks-dot-com/company/events/webinar-cta/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf#page=9\">Structure of RNN</a>\n* [深層学習／ゼロから作るDeep Learning２　第５章メモ](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)\n* [深層学習／ゼロから作るDeep Learning２　第６章メモ](https://qiita.com/jun40vn/items/e690dfe80faa6512049f)\n* [自然言語処理の必須知識 Transformer を徹底解説！](https://deepsquare.jp/2020/07/transformer/)\n* [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)\n* [colah氏の解説 Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n* [画像用Transformerを利用して衛星画像の分類機械学習モデルを作成する](https://sorabatake.jp/20454/) @空畑\n* [PyTorchで自然言語処理でよく使用されるTransformerを少ないコードで使用してみる](https://www.yurui-deep-learning.com/2021/01/07/pytorch-transformer/)\n* [LSTMによる系列データの予測と分類](https://jp.mathworks.com/content/dam/mathworks/mathworks-dot-com/company/events/webinar-cta/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf) @mathworks\n* [RNNの問題点](https://www.anarchive-beta.com/entry/2021/01/07/180000)\n* [作って理解する Transformer / Attention](https://qiita.com/halhorn/items/c91497522be27bde17ce)\n* [AI界を席巻する「Transformer」をゆっくり解説](https://zenn.dev/attentionplease/articles/1a01887b783494)\n* [論文解説 Attention Is All You Need (Transformer)](https://deeplearning.hatenablog.com/entry/transformer)\n* [自然言語処理の巨獣「Transformer」のSelf-Attention Layer紹介](https://medium.com/lsc-psd/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%81%AE%E5%B7%A8%E7%8D%A3-transformer-%E3%81%AEself-attention-layer%E7%B4%B9%E4%BB%8B-a04dc999efc5)\n* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n* [はじめての自然言語処理](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/)\n\n<br><br>\n#### 各Layerの説明\n* [positional encoding](https://qiita.com/halhorn/items/c91497522be27bde17ce#positional-encoding)\n* [Feed Forward Net](https://www.hellocybernetics.tech/entry/2016/05/22/014656)\n  隠れ層1層の全結合NN\n* [Batch Normalize](https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600#%E5%85%A8%E7%B5%90%E5%90%88nn-%E3%81%AE-batch-normalization)\n* [Layer Normalization](https://data-analytics.fun/2020/07/16/understanding-layer-normalization/)\n* [gMLP](https://deepsquare.jp/2021/05/gmlp/)\n\n<br><br>\n#### よく忘れる単語まとめ\n* [ノルム](https://manabitimes.jp/math/1269)\n* [アマダール積](https://python.atelierkobato.com/hadamard/)<br>\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0866ce505a5888cd4db8e6ba2b710fe8cd5a6578\" width=\"500\">\n* [アフィン変換](https://qiita.com/koshian2/items/c133e2e10c261b8646bf)<br>\n全結合層による変換の事。幾何学の分野ではアフィン変換に相当するので、ここではアフィン変換と読んでいる。<br>\n* tanhとsigmoidについて<br>\ntanhは、何かしらの情報の出力についている。情報に対する強弱と解釈できる。<br>\nsigmoidは、データをどれだけ通すかという割合。<br>\n* [Batch Normalization](https://yaakublog.com/batch-normalization)<br>\n* [評価関数まとめ](https://amateur-engineer-blog.com/machine-learning-metrics/)<br>\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Unique process of NLP**\n\nRNN, LSTM, transformer cannot treat texts directory.<br>\nAt first, we need to run tokernize. This process transform words to ids.<br>\nNext, run padding process. this process add special id to align the length to the longest sentence.<br>\nThose process use language model, BERT etc,.<br>\n<br>\n文字列をそのまま処理できない為、まずトークン化を行います。<br>\nその後、各センテンスの長さを揃える為、paddingを行います。<br>\nこれらのプロセスでは、BERTで知られるようなモデルを使います。<br>\n当然モデルの影響も大きいです。<br>\nモデルについては別途まとめます。<br>\n\n#### 2.1 Tokenization\n文字列をそのまま学習する事はできません。<br>\n学習できるように、数値に置き換える処理が必要です。それをトークン化(Tokenize)と言います。<br>\n以下の画像の上2つがトークン化に相当します。<br>\n<img src=\"https://camo.qiitausercontent.com/51b2ff47f2bb952f38a63b02d056ebfc7dea097d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36323435302f35323261393063362d613732352d333731362d346161382d6637626563396430396262622e706e67\" width=600>\n<br>\n\n#### 2.2 Tokenization result of 1st text\n\nShow BERT tokenizer result with transformer bert model.<br>\nThe 'input_ids' result is tokenization text.<br>\n<br>\n日本語を扱う場合は、日本語のtokenizerを利用する。<br>","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n##== whwn you want to treat Japanese character,\n# !pip install transformers[ja]\n#tokenizer = transformers.AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\ntokenizer(train.text[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:52.423605Z","iopub.execute_input":"2021-11-02T09:57:52.423904Z","iopub.status.idle":"2021-11-02T09:57:58.32201Z","shell.execute_reply.started":"2021-11-02T09:57:52.423865Z","shell.execute_reply":"2021-11-02T09:57:58.321204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Padding\n学習時には入力は固定長が期待されています。<br>\nトークン化したものの長さを揃えるため、paddingで\"0\"などを追加します。<br>\n<br>\n\npadding前のトークン化済みデータ\n```\nraw_inputs = [\n    [711, 632, 71],\n    [73, 8, 3215, 55, 927],\n    [83, 91, 1, 645, 1253, 927],\n]\n```\npadding後\n```\n## Result\n[[ 711  632   71    0    0    0]\n [  73    8 3215   55  927    0]\n [  83   91    1  645 1253  927]]\n```\n<br>","metadata":{}},{"cell_type":"markdown","source":"## **3. RNN** \n\n#### 3.1 RNN one layer\n\nUnderstand rnn structure via python code.<br>\n\n#### 式の説明\n##### **forwarについて**<br>\n<img src=\"https://camo.qiitausercontent.com/d0046c189cf724199009651c48433d0097df591a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f34303634653037662d623962622d303432392d616339322d6338366233313837616164622e706e67\" width=600>\n<br><br>\n\n##### **backwardについて**<br>\n\n<img src=\"https://camo.qiitausercontent.com/8942bf9b069aece2d1c19a676a089414643a02c8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31396133383030642d336236342d393133652d396165342d3731616664653534356138652e706e67\" width=600>\n\n<br><br>\n#### **コードの説明**\nthree dots については[こちら](https://note.nkmk.me/python-numpy-ellipsis/)<br>","metadata":{}},{"cell_type":"code","source":"class RNN:\n    def __init__(self, Wx, Wh, b):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n    \n    def forward(self, x, h_prev):\n        Wx, Wh, b = self.params\n        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n        h_next = np.tanh(t)\n        \n        self.cache = (x, h_prev, h_next)\n        return h_next\n    \n    def backward(self, dh_next):\n        Wx, Wh, d = self.params\n        x, h_prev, h_next = self.cache\n        \n        dt = dh_next * (1 - h_next ** 2)\n        db = np.sum(dt, axis=0)\n        dWh = np.dot(h_prev.T, dt)\n        dh_prev = np.dot(x.T, dt)\n        dx = np.dot(dt, Wx.T)\n        \n        self.grads[0][...] = dWx  ## three dots\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n        \n        return dx, dh_prev","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:58.323297Z","iopub.execute_input":"2021-11-02T09:57:58.323758Z","iopub.status.idle":"2021-11-02T09:57:58.334794Z","shell.execute_reply.started":"2021-11-02T09:57:58.323715Z","shell.execute_reply":"2021-11-02T09:57:58.334009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.２ Time RNN one layer\n\nRNNを横にT個並べたもの。<br>\nRNNで一つのセンテンス長を取り扱い、横にT個並べる事で時間方向の解像度を作り出している。<br>\n\n#### 式の説明\n<br>\n\n##### **forwardについて**\n<br>\n<img src=\"https://camo.qiitausercontent.com/3d6925879a4d36d8cd7562a9adf75d76f43f281e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f65363761373333612d623134662d373232362d646262382d3061393135396663396434312e706e67\" width=\"600\">\n<br><br>\n\n##### **backwordについて**\n<br>\n<img src=\"https://camo.qiitausercontent.com/78da7c43ce5218b6eb574f29e3ffb574f977b92e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f39383436336136652d313534392d356264322d373530312d6635353066346538303932302e706e67\" width=\"400\">\n<br><br>\n","metadata":{}},{"cell_type":"code","source":"class TimeRNN:\n    def __init__(self, Wx, Wh, b, stateful=False):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.layers = None\n\n        self.h, self.dh = None, None\n        self.stateful = stateful\n\n    def forward(self, xs):\n        Wx, Wh, b = self.params\n        N, T, D = xs.shape\n        D, H = Wx.shape\n\n        self.layers = []\n        hs = np.empty((N, T, H), dtype='f')\n\n        if not self.stateful or self.h is None:\n            self.h = np.zeros((N, H), dtype='f')\n\n        for t in range(T):\n            layer = RNN(*self.params)  ## <= RNNをT個生成している。\n            self.h = layer.forward(xs[:, t, :], self.h)\n            hs[:, t, :] = self.h\n            self.layers.append(layer)\n\n        return hs\n    \n    def backward(self, dhs):\n        Wx, Wh, b = self.params\n        N, T, H = dhs.shape\n        D, H = Wx.shape\n\n        dxs = np.empty((N, T, D), dtype='f')\n        dh = 0\n        grads = [0, 0, 0]\n        for t in reversed(range(T)):\n            layer = self.layers[t]\n            dx, dh = layer.backward(dhs[:, t, :] + dh)\n            dxs[:, t, :] = dx\n\n            for i, grad in enumerate(layer.grads):\n                grads[i] += grad\n\n        for i, grad in enumerate(grads):\n            self.grads[i][...] = grad\n        self.dh = dh\n\n        return dxs\n\n    def set_state(self, h):\n        self.h = h\n\n    def reset_state(self):\n        self.h = None","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:58.336378Z","iopub.execute_input":"2021-11-02T09:57:58.336764Z","iopub.status.idle":"2021-11-02T09:57:58.351507Z","shell.execute_reply.started":"2021-11-02T09:57:58.336725Z","shell.execute_reply":"2021-11-02T09:57:58.350728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **3.3 RNNLM**\n\nTimeRNN　layerに加え、TimeAffine　layer、 TimeEmbedding layer、TimeSoftmaxWithLoss　layerを組み合わせて<br>\nRNNLMを作る。\n\n\n##### **Structure of RNNLM**\n\n<img src =\"https://camo.qiitausercontent.com/ac8303dfae114898ac9d1f752e0b9378ba975784/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f61346461333038372d623736642d353636322d386539322d3364306131333062643737342e706e67\" width=\"500\">\n<br><br>\n\n##### **Time Embedding**\n\n<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"500\">\n<br>\nTime Embeddingレイヤは、xsから1列づつデータを切り出しEmbeddingレイヤに入力して、<br>\nその出力をout(N, T, D)に溜め込む、という動作をforループでT回繰り返すというものです。\n<br><br>\n\n##### **Time Affine Layer**\n\n<img src=\"https://camo.qiitausercontent.com/40771ddfdce111906aa55c50a000af060e146803/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f37393361623630622d336263332d653063332d663166342d3064316436653038333730352e706e67\" width=\"500\">\n<br>\nTime Affineレイヤは、Affineレイヤの入出力に、時間軸方向のTに対応出来るようにそれぞれreshapeを付け加えたものです。<br>\n<br><br>\n\n##### **Time SoftMax WithLoss**\n<img src=\"https://camo.qiitausercontent.com/e2d2604218ab475a245c7bae0a160e7f97416108/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f63616563366363642d373362332d663733612d653230352d3737363838326639346230652e706e67\" width=\"400\">\n<br>\nTime Softmax with Loss レイヤー は、xt,ttxt,ttのSotmax with Loss をT個合算してTで割るレイヤーです。<br>\n\n#### **コードの説明**\nthree dots については[こちら](https://note.nkmk.me/python-numpy-ellipsis/)<br>","metadata":{}},{"cell_type":"code","source":"class SimpleRnnlm:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        # 重みの初期化\n        embed_W = (rn(V, D) / 100).astype('f')\n        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n        rnn_b = np.zeros(H).astype('f')\n        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        # レイヤの生成\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n            TimeAffine(affine_W, affine_b)\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.rnn_layer = self.layers[1]\n\n        # すべての重みと勾配をリストにまとめる\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, ts):\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        loss = self.loss_layer.forward(xs, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        self.rnn_layer.reset_state()\n        \nclass TimeEmbedding:\n    def __init__(self, W):\n        self.params = [W]\n        self.grads = [np.zeros_like(W)]\n        self.layers = None\n        self.W = W\n\n    def forward(self, xs):\n        N, T = xs.shape\n        V, D = self.W.shape\n\n        out = np.empty((N, T, D), dtype='f')\n        self.layers = []\n\n        for t in range(T):\n            layer = Embedding(self.W)\n            out[:, t, :] = layer.forward(xs[:, t])\n            self.layers.append(layer)\n\n        return out\n\n    def backward(self, dout):\n        N, T, D = dout.shape\n\n        grad = 0\n        for t in range(T):\n            layer = self.layers[t]\n            layer.backward(dout[:, t, :])\n            grad += layer.grads[0]\n\n        self.grads[0][...] = grad\n        return None\n\nclass TimeAffine:\n    def __init__(self, W, b):\n        self.params = [W, b]\n        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n        self.x = None\n\n    def forward(self, x):\n        N, T, D = x.shape\n        W, b = self.params\n\n        rx = x.reshape(N*T, -1)\n        out = np.dot(rx, W) + b\n        self.x = x\n        return out.reshape(N, T, -1)\n\n    def backward(self, dout):\n        x = self.x\n        N, T, D = x.shape\n        W, b = self.params\n\n        dout = dout.reshape(N*T, -1)\n        rx = x.reshape(N*T, -1)\n\n        db = np.sum(dout, axis=0)\n        dW = np.dot(rx.T, dout)\n        dx = np.dot(dout, W.T)\n        dx = dx.reshape(*x.shape)\n\n        self.grads[0][...] = dW\n        self.grads[1][...] = db\n\n        return dx\n    \nclass TimeSoftmaxWithLoss:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.cache = None\n        self.ignore_label = -1\n\n    def forward(self, xs, ts):\n        N, T, V = xs.shape\n\n        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n            ts = ts.argmax(axis=2)\n\n        mask = (ts != self.ignore_label)\n\n        # バッチ分と時系列分をまとめる（reshape）\n        xs = xs.reshape(N * T, V)\n        ts = ts.reshape(N * T)\n        mask = mask.reshape(N * T)\n\n        ys = softmax(xs)\n        ls = np.log(ys[np.arange(N * T), ts])\n        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n        loss = -np.sum(ls)\n        loss /= mask.sum()\n\n        self.cache = (ts, ys, mask, (N, T, V))\n        return loss\n\n    def backward(self, dout=1):\n        ts, ys, mask, (N, T, V) = self.cache\n\n        dx = ys\n        dx[np.arange(N * T), ts] -= 1\n        dx *= dout\n        dx /= mask.sum()\n        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n\n        dx = dx.reshape((N, T, V))\n\n        return dx","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. LSTM**\n\n1つのLSTMの中に、forget agteと、input gateとstateを記憶するcellと、outputゲートが<br>\n入力側から並列に、それぞれのoutputが直列につながっている。<br>\nこの構造がわかれば、それぞれのパーツは簡単。<br>\n\n#### 4.1.1 Confirm \"gradient vanishing problem\"\n\n<br>\n\n#### 4.1.2 Confirm \"gradient exploding problem\"\n\n<br>\n\n#### 4.1.3 diff RNN and LSTM\n\n* RNN<br>\n<img src=\"https://camo.qiitausercontent.com/b643944d722e601f9e3d4b7856cd096895f5ce1f/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d53696d706c65524e4e2e706e67\" width=500>\n<br><br>\n* LSTM<br>\n<img src=\"https://camo.qiitausercontent.com/a12fe62032a6633b05b8ef7c2512d1e54c9c4afd/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d636861696e2e706e67\" width=500><br><br>\n\n<br>\n\n#### ** 4.1.4 LSTM Core Idea\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.<br>\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" width=500><br><br>\n\n#### **コードの説明**\n\nDx4Hで、4がついている理由は、fとgとiとoのWを横につなげて一気に扱っている為。\n","metadata":{}},{"cell_type":"markdown","source":"#### ４.2 LSTM one layer\n\nUnderstand LSTM structure via python code.<br>\n\n#### 式の説明\n##### **forwarについて** <br>\n<img src=\"https://camo.qiitausercontent.com/b9387ed37d1f247f8f679184f8dd2efd1e7a709a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f35306164343264652d643433642d333162392d333761652d3637666462313032393939612e706e67\" width=600>\n<br><br>\n\n##### **backwardについて** <br>\n<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=600>\n<br><br>\n","metadata":{}},{"cell_type":"code","source":"class LSTM:\n    def __init__(self, Wx, Wh, b):\n\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n\n    def forward(self, x, h_prev, c_prev):\n        Wx, Wh, b = self.params\n        N, H = h_prev.shape\n\n        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n\n        f = A[:, :H]\n        g = A[:, H:2*H]\n        i = A[:, 2*H:3*H]\n        o = A[:, 3*H:]\n\n        f = sigmoid(f)\n        g = np.tanh(g)\n        i = sigmoid(i)\n        o = sigmoid(o)\n\n        c_next = f * c_prev + g * i\n        h_next = o * np.tanh(c_next)\n\n        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n        return h_next, c_next\n\n    def backward(self, dh_next, dc_next):\n        Wx, Wh, b = self.params\n        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n\n        tanh_c_next = np.tanh(c_next)\n\n        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n\n        dc_prev = ds * f\n\n        di = ds * g\n        df = ds * c_prev\n        do = dh_next * tanh_c_next\n        dg = ds * i\n\n        di *= i * (1 - i)\n        df *= f * (1 - f)\n        do *= o * (1 - o)\n        dg *= (1 - g ** 2)\n\n        dA = np.hstack((df, dg, di, do))\n\n        dWh = np.dot(h_prev.T, dA)\n        dWx = np.dot(x.T, dA)\n        db = dA.sum(axis=0)\n\n        self.grads[0][...] = dWx\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n\n        dx = np.dot(dA, Wx.T)\n        dh_prev = np.dot(dA, Wh.T)\n\n        return dx, dh_prev, dc_prev","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Transformer**\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Visualization**\n\n#### **6.1 word cloud**","metadata":{}},{"cell_type":"code","source":"words = [ w for t in train[\"text\"].str.split().tolist() for w in t]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T12:53:52.597063Z","iopub.execute_input":"2021-11-03T12:53:52.597866Z","iopub.status.idle":"2021-11-03T12:53:52.636973Z","shell.execute_reply.started":"2021-11-03T12:53:52.597819Z","shell.execute_reply":"2021-11-03T12:53:52.635924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(words[:200]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T12:54:57.997779Z","iopub.execute_input":"2021-11-03T12:54:57.99808Z","iopub.status.idle":"2021-11-03T12:54:58.480008Z","shell.execute_reply.started":"2021-11-03T12:54:57.998046Z","shell.execute_reply":"2021-11-03T12:54:58.479218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}