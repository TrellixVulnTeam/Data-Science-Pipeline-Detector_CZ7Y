{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nimport spacy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T00:09:09.20986Z","iopub.execute_input":"2022-04-29T00:09:09.210728Z","iopub.status.idle":"2022-04-29T00:09:09.220064Z","shell.execute_reply.started":"2022-04-29T00:09:09.210684Z","shell.execute_reply":"2022-04-29T00:09:09.219276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T00:09:09.23649Z","iopub.execute_input":"2022-04-29T00:09:09.23692Z","iopub.status.idle":"2022-04-29T00:09:09.263005Z","shell.execute_reply.started":"2022-04-29T00:09:09.236883Z","shell.execute_reply":"2022-04-29T00:09:09.26206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T00:09:09.264655Z","iopub.execute_input":"2022-04-29T00:09:09.26488Z","iopub.status.idle":"2022-04-29T00:09:09.276604Z","shell.execute_reply.started":"2022-04-29T00:09:09.264851Z","shell.execute_reply":"2022-04-29T00:09:09.275947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = tweet_df.drop('target', axis=1), tweet_df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T00:09:09.277674Z","iopub.execute_input":"2022-04-29T00:09:09.278321Z","iopub.status.idle":"2022-04-29T00:09:09.298136Z","shell.execute_reply.started":"2022-04-29T00:09:09.278278Z","shell.execute_reply":"2022-04-29T00:09:09.29741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n#to add to process_tweets(): try porter stemmer and not removing stopwords\ndef process_tweets(tweets):\n    processed_tweets = []\n    for tweet in tweets.text:\n        processed_tweet = []\n        words = nlp(tweet)\n        for word in words:\n            word = word.lemma_\n            word = word.lower()\n            #if word not in stopwords.words('english'):\n            processed_tweet.append(word)\n        processed_tweets.append(processed_tweet)\n    return processed_tweets\n    \ndef build_freqs(processed_tweets, labels):\n    freq_dict = dict()\n    for tweet, label in zip(processed_tweets, labels):\n        for word in tweet:\n            if (word, label) in freq_dict:\n                freq_dict[(word, label)] += 1\n            else:\n                freq_dict[(word, label)] = 1\n    return freq_dict\n\ndef get_features(freqs, processed_tweet):\n    disaster = 0\n    non_disaster = 0\n    num_words = len(processed_tweet)\n    num_chars = 0\n    avg_word_len = num_chars / num_words\n    num_hashtags = 0\n    num_links = 0\n    for word in set(processed_tweet): \n        disaster += freqs.get((word, 1), 0)\n        non_disaster += freqs.get((word, 0), 0)\n        num_chars += len(word)\n        if word[0] == '#':\n            num_hashtags += 1\n        if word[:5] == 'http':\n            num_links += 1\n    return (disaster, non_disaster, num_chars, num_words, num_hashtags, num_links, (disaster / (non_disaster + 1)), avg_word_len)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T00:09:09.300333Z","iopub.execute_input":"2022-04-29T00:09:09.300767Z","iopub.status.idle":"2022-04-29T00:09:10.031319Z","shell.execute_reply.started":"2022-04-29T00:09:09.300721Z","shell.execute_reply":"2022-04-29T00:09:10.03033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = LogisticRegression()\nmodel = XGBClassifier(random_state=42, n_estimators=350, max_depth=3, learning_rate=0.01, booster='dart')\nprocessed_tweets = process_tweets(X_train)\nfreqs = build_freqs(processed_tweets, y_train)\nfeatures = pd.DataFrame([get_features(freqs, tweet) for tweet in processed_tweets])\nmodel.fit(features, y_train)\ntest_processed_tweets = process_tweets(X_test)\ntest_features = pd.DataFrame([get_features(freqs, tweet) for tweet in test_processed_tweets])\npreds = model.predict(test_features)\nprint('The F1 score was {}'.format(f1_score(preds, y_test))) # 0.5716417910447761 V7, 0.5993031358885018 keeping stopwords\n# 0.6368932038834951 with word count and char count 0.6336123631680618 with number of hashtags 0.6336123631680618 with num links\n# 0.6179084073820915 with disaster/non_disaster ratio 0.6198347107438017 without bias","metadata":{"execution":{"iopub.status.busy":"2022-04-29T00:09:10.032932Z","iopub.execute_input":"2022-04-29T00:09:10.033188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_tweets = process_tweets(X)\nfreqs = build_freqs(processed_tweets, y)\nfeatures = pd.DataFrame([get_features(freqs, tweet) for tweet in processed_tweets])\nmodel.fit(features, y)\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nprocessed_tweets = process_tweets(test)\nfeatures = pd.DataFrame([get_features(freqs, tweet) for tweet in processed_tweets])\npreds = model.predict(features)\nsubmission = pd.DataFrame({'id': test.id, 'target': preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}