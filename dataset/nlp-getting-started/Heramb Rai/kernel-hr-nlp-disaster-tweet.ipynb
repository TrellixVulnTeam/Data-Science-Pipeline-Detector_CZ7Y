{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Input Data and Pre-processing**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Fetching training data\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fetching testing data\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the dimensions of datasets\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking missing values in both train and test set\nprint(train.isnull().sum())\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the missing values of \"keyword\" as Unknown so that those can be counted as well\ntrain.keyword.fillna(\"Unknown\", inplace=True)\ntest.keyword.fillna(\"Unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the non-required fields i.e. location and id from the dataset\ntrain.drop(\"location\", axis=1, inplace=True)\ntest.drop(\"location\", axis=1, inplace=True)\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final check to verify the missing values\nprint(train.isnull().sum())\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text Pre-processing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing stopwords from NLTK\nimport nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-processing the text with the removal of irrelevant characters, symbols and stopwords\nimport re\nimport string\nfrom nltk.corpus import stopwords\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\ndef text_prepare(text):\n    \"\"\"\n        text: a string   \n        return: modified initial string\n    \"\"\"\n    text = text.lower() #lowercase text  \n    text = REPLACE_BY_SPACE_RE.sub(' ',text) #replace REPLACE_BY_SPACE_RE symbols by space in text    \n    text = BAD_SYMBOLS_RE.sub('',text) #delete symbols which are in BAD_SYMBOLS_RE from text \n    text = re.sub('https','',text)\n    text = re.sub('http','',text)\n    text = re.sub('tco','',text)\n    temp = [s.strip() for s in text.split() if s not in STOPWORDS] #delete stopwords from text\n    new_text = ''\n    for i in temp:\n        new_text +=i+' '\n    text = new_text\n    return text.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the above preprocessing to the train set\ntrain['text'] = train['text'].map(text_prepare)\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the above preprocessing to the test set\ntest['text'] = test['text'].map(text_prepare)\ntest['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning the text using tokens and stemming\nstopword =  nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\ndef clean(text):\n    no_punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+',no_punct) #tokenization\n    text_stem = ([ps.stem(word) for word in tokens if word not in stopword]) #stemming\n    return text_stem","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA and Visualizations**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Common words in the tweets\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(train['text']))\nplt.imshow(wordcloud,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common words in the disaster tweets\ndisaster = train.text[train.target[train.target==1].index]\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(disaster))\nplt.imshow(wordcloud,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common words in the non-disaster tweets\nndisaster = train.text[train.target[train.target==0].index]\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(ndisaster))\nplt.imshow(wordcloud,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweet words frequency plot\ntrain_new = train.copy(deep=True)\ntrain_new['words'] = train_new['text'].apply(lambda x: clean(x))\nAll_words = []\nfor words in train_new['words']:\n    for word in words:\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of characters in tweets\ndef length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ntrain_new['length'] = train_new['text'].apply(length)\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train_new[train_new['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Non-Disaster')\nplt.hist(train_new[train_new['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Disaster')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_new[train_new['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_new[train_new['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_new[train_new['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_new[train_new['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average word length in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train_new[train_new['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=train_new[train_new['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text Vectorization and Finalizaing Independent and dependent variables**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing text vectorization so that data can be fed to the model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer(analyzer= clean) #Using the above text cleaning to clean the data\nXtf_idfVector = tf_idf.fit_transform(train['text'])\nXtest_idfVector = tf_idf.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing one hot encoding for categorical variables\ntrain_mod = pd.get_dummies(data=train, columns=['keyword'])\ntest_mod = pd.get_dummies(data=test, columns=['keyword'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping irrelevant columns from the modified data\ntrain_mod.drop('text', axis=1, inplace=True)\ntrain_mod.drop('target', axis=1, inplace=True)\ntest_mod.drop('text', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the final dataframe consisting of keywords and text i.e. X Variable\nXfeatures_data = pd.concat([train_mod, \n                            pd.DataFrame(Xtf_idfVector.toarray())], axis = 1)\nX_test = pd.concat([test_mod, \n                    pd.DataFrame(Xtest_idfVector.toarray())], axis = 1)\n\n# Target variable i.e. Y Variable\ny_data = train.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictions with classifiers : RF, NB, XGB, SVC, Voting, NN**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using randomforest classifier to train the data\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 50, max_depth= 20, n_jobs= -1)\nmodel1 = rf.fit(Xfeatures_data,y_data)\n\n#Predict Output using RF\npredicted_RF= rf.predict(X_test) \npredicted_RF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RF Score : 0.71165","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using naive_bayes classifier to train the data\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nmodel2 = nb.fit(Xfeatures_data,y_data)\n\n#Predict Output using NB\npredicted_NB= nb.predict(X_test) \npredicted_NB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NB Score : 0.79550","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using xgboost classifier to train the data\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nmodel3 = xgb.fit(Xfeatures_data,y_data)\n\n#Predict Output using XGB\npredicted_XG= xgb.predict(X_test) \npredicted_XG","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB Score : 0.78732","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using SVC classifier to train the data\nfrom sklearn.svm import SVC\nsvc = SVC()\nmodel4 = svc.fit(Xfeatures_data,y_data)\n\n#Predict Output using SVC\npredicted_SC= svc.predict(X_test) \npredicted_SC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC Score : 0.79447","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Voting classifier to train the data\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators=[('SVC', svc), ('XGB', xgb), ('NB', nb), ('RF', rf)], voting='hard')\nmodel5 = voting_clf.fit(Xfeatures_data,y_data)\n\n#Predict Output using Voting\npredicted_VC= voting_clf.predict(X_test) \npredicted_VC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting Score : 0.78016","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Neural_networks to train the data\nfrom keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nmodel = Sequential()\nmodel.add(Dense(units = 512 , activation = 'relu' , input_dim = Xfeatures_data.shape[1]))\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dense(units = 100 , activation = 'relu'))\nmodel.add(Dense(units = 10 , activation = 'relu'))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel6 = model.fit(Xfeatures_data,y_data, batch_size=128, epochs=10)\n\n#Predict Output using NN\npredicted_NN= model.predict_classes(X_test) \npredicted_NN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NN Score : 0.78936","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving output results of NB to csv file\npredicted_df = pd.DataFrame(predicted_NB)\npredicted_df.to_csv('out.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Don't Forget to Upvote, It's Free","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}