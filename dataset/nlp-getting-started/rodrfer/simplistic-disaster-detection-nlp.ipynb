{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we will see how a more simplistic bag of words (simplistic when compared to distributional word embeddings or contextualized language models) approach with classic machine learning models perform on the task of disaster classification on tweets.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-27T17:21:16.056775Z","iopub.execute_input":"2022-01-27T17:21:16.057211Z","iopub.status.idle":"2022-01-27T17:21:16.067365Z","shell.execute_reply.started":"2022-01-27T17:21:16.057182Z","shell.execute_reply":"2022-01-27T17:21:16.066878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading training and testing data from the csv files.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:16.07559Z","iopub.execute_input":"2022-01-27T17:21:16.075961Z","iopub.status.idle":"2022-01-27T17:21:16.122319Z","shell.execute_reply.started":"2022-01-27T17:21:16.075926Z","shell.execute_reply":"2022-01-27T17:21:16.121297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook we will only be using data from the text field as features.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop([\"keyword\",\"location\"],1)\ntest_df = test_df.drop([\"keyword\",\"location\"],1)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:16.123832Z","iopub.execute_input":"2022-01-27T17:21:16.12401Z","iopub.status.idle":"2022-01-27T17:21:16.135947Z","shell.execute_reply.started":"2022-01-27T17:21:16.123985Z","shell.execute_reply":"2022-01-27T17:21:16.135271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To preprocess the text we will transform all letters to lower case, remove punctuation, tokenize, filter out stopwords and numbers and stem the remaining tokens.","metadata":{}},{"cell_type":"code","source":"ps=PorterStemmer()\nstop_words = set(stopwords.words(\"english\"))\ntranslate_table = dict((ord(char), None) for char in string.punctuation) \n\ndef preprocess(text):\n    text = text.lower()\n    text = text.translate(translate_table)\n    tokens = word_tokenize(text)\n    tokens = [ps.stem(token) for token in tokens if token.isalpha() and not token in stop_words]\n    tokens = \" \".join(tokens)\n        \n    return tokens\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:16.137028Z","iopub.execute_input":"2022-01-27T17:21:16.1376Z","iopub.status.idle":"2022-01-27T17:21:16.14847Z","shell.execute_reply.started":"2022-01-27T17:21:16.137566Z","shell.execute_reply":"2022-01-27T17:21:16.147724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example of the desired preprocessed tokens\npreprocess(\"Oh my god there was a 7.2 #earthquake in Lisbon\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:16.150471Z","iopub.execute_input":"2022-01-27T17:21:16.150688Z","iopub.status.idle":"2022-01-27T17:21:16.166178Z","shell.execute_reply.started":"2022-01-27T17:21:16.15066Z","shell.execute_reply":"2022-01-27T17:21:16.165601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing the text data\ntrain_df['text'] = train_df['text'].apply(preprocess) \ntest_df['text'] = test_df['text'].apply(preprocess) ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:16.166962Z","iopub.execute_input":"2022-01-27T17:21:16.167138Z","iopub.status.idle":"2022-01-27T17:21:22.957717Z","shell.execute_reply.started":"2022-01-27T17:21:16.167113Z","shell.execute_reply":"2022-01-27T17:21:22.956567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resulting data\ntrain_df['text']","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:22.959329Z","iopub.execute_input":"2022-01-27T17:21:22.959545Z","iopub.status.idle":"2022-01-27T17:21:22.9674Z","shell.execute_reply.started":"2022-01-27T17:21:22.959519Z","shell.execute_reply":"2022-01-27T17:21:22.96689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage we will use a bag of words approach with the Tf-Idf weighting methodology to turn words into vectors. We limit the words to the 4000 most frequent ones.","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=4000)\nX = vectorizer.fit_transform(train_df['text'])\nX.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:22.968203Z","iopub.execute_input":"2022-01-27T17:21:22.968498Z","iopub.status.idle":"2022-01-27T17:21:23.150174Z","shell.execute_reply.started":"2022-01-27T17:21:22.968477Z","shell.execute_reply":"2022-01-27T17:21:23.149308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To assess which model to submit with, we will test some by splitting the training data into a training and testing subset.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, train_df['target'], test_size = 0.2)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:23.151167Z","iopub.execute_input":"2022-01-27T17:21:23.151675Z","iopub.status.idle":"2022-01-27T17:21:23.158085Z","shell.execute_reply.started":"2022-01-27T17:21:23.151645Z","shell.execute_reply":"2022-01-27T17:21:23.15761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be training a Logistic Regression, Support Vector Machine and Random Forrest models, with sklearn's default parameters","metadata":{}},{"cell_type":"code","source":"LR_model = LogisticRegression().fit(X_train, y_train)\nSVM_model = svm.SVC().fit(X_train, y_train)\nRF_model = RandomForestClassifier().fit(X_train, y_train)\nLR_model.predict(X_test)\nSVM_model.predict(X_test)\nRF_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:23.159149Z","iopub.execute_input":"2022-01-27T17:21:23.159378Z","iopub.status.idle":"2022-01-27T17:21:36.38213Z","shell.execute_reply.started":"2022-01-27T17:21:23.159323Z","shell.execute_reply":"2022-01-27T17:21:36.381166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can test their performance.","metadata":{}},{"cell_type":"code","source":"LR_score= LR_model.score(X_test, y_test)\nSVM_score=SVM_model.score(X_test, y_test)\nRF_score=RF_model.score(X_test, y_test)\nprint(LR_score, SVM_score, RF_score)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:36.384407Z","iopub.execute_input":"2022-01-27T17:21:36.384628Z","iopub.status.idle":"2022-01-27T17:21:37.323897Z","shell.execute_reply.started":"2022-01-27T17:21:36.384603Z","shell.execute_reply":"2022-01-27T17:21:37.323133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In most tests the results were very close but SVM tended to provide the best score.\n\nOnce we decided which model to use we train it using all the training samples.\n\nWe may now transform the competition's test set into a document-term matrix, to be able to make new predictions.","metadata":{}},{"cell_type":"code","source":"SVM_model = svm.SVC().fit(X, train_df[\"target\"])\ntest_X= vectorizer.transform(test_df['text'])\npredictions = SVM_model.predict(test_X)\npredictions","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:37.324929Z","iopub.execute_input":"2022-01-27T17:21:37.325142Z","iopub.status.idle":"2022-01-27T17:21:37.466115Z","shell.execute_reply.started":"2022-01-27T17:21:37.325114Z","shell.execute_reply":"2022-01-27T17:21:37.465391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can create a dataframe containing each testing sample's id and predicted target, and put it into a csv file to submit.","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(test_df[\"id\"], columns=[\"id\"])\nsubmission_df[\"target\"] = predictions\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:37.467427Z","iopub.execute_input":"2022-01-27T17:21:37.467639Z","iopub.status.idle":"2022-01-27T17:21:37.483599Z","shell.execute_reply.started":"2022-01-27T17:21:37.46761Z","shell.execute_reply":"2022-01-27T17:21:37.48277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', header=True, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:21:37.484708Z","iopub.execute_input":"2022-01-27T17:21:37.484895Z","iopub.status.idle":"2022-01-27T17:21:37.499987Z","shell.execute_reply.started":"2022-01-27T17:21:37.484869Z","shell.execute_reply":"2022-01-27T17:21:37.499495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}