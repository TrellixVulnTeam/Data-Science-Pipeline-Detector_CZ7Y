{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets\n\n##  所需要的库\n\n```python\nimport re\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n\n```\n\n##  数据的预处理\n\n> 1. 导入数据\n>\n>    ```python\n>    train_df = pd.read_csv('../data/Natural Language Processing with Disaster Tweets/train.csv')\n>    test_df = pd.read_csv('../data/Natural Language Processing with Disaster Tweets/test.csv')\n>    ```\n\n> 2. 数据清洗\n>\n>    ```python\n>    def clean_text(text):\n>        text= text.lower()\n>        text= re.sub('[0-9]', '', text)\n>        text= re.sub('#', '', text)  \n>        text= re.sub('-', '', text)  \n>        text= re.sub('()', '', text) \n>        text= re.sub('=>', '', text) \n>        text= re.sub('|', '', text) \n>        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n>        return text\n>    # 该数据还能进行更多的数据清洗，这边只是做了一个范例。\n>    ```\n>\n>    ```python\n>    # 将数据清洗的操作应用到数据中，并将数据进行保存。\n>    train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n>    test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n>    train_df.to_csv('../data/Natural Language Processing with Disaster Tweets/train_clean.csv', index=None)\n>    test_df.to_csv('../data/Natural Language Processing with Disaster Tweets/test_clean.csv', index=None)\n>    ```\n\n## 用BERT进行fine-tuning\n\n> 1. 模型选择\n>\n>    ```python\n>    # 本次用蒸馏过的bert模型来进行微调，相较于原本的bert，体型更小并且精度基本没有损失。\n>    # 其中uncased表示按照小写字母来进行处理。\n>    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n>    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n>    ```\n\n> 2. 数据集的加载\n>\n>    ```python\n>    # 这里用huggingface的dataset包来进行数据读取。\n>    train_datasets = load_dataset('csv', data_files={'train': '../data/Natural Language Processing with Disaster Tweets/train_clean.csv'})\n>    test_datasets = load_dataset('csv', data_files={'test': '../data/Natural Language Processing with Disaster Tweets/test_clean.csv'})\n>    \n>    # 删除了一些无关的列属性，也可以将这些列属性加入文本来进行训练\n>    train_datasets = train_datasets.remove_columns(['id', 'keyword', 'location'])\n>    test_id = list(test_datasets['test']['id']) # 将test_id存起来，方便后面生成提交的文件\n>    test_datasets = test_datasets.remove_columns(['id', 'keyword', 'location'])\n>    \n>    # 这个操作可以将训练集分出一部分当作验证集，来进行超参数选择，本次不进行这个，直接进行训练和预测\n>    # train_datasets = train_datasets['train'].train_test_split(test_size=0.1)\n>    ```\n\n> 3. tokenizer进行bert输入层操作\n>\n>    ```python\n>    # 选取最长长度为64，不足的补0，超过的进行截断\n>    def tokenize_function(examples):\n>        return tokenizer(examples[\"text\"], padding='max_length', max_length=64, truncation=True)\n>    ```\n>\n>    ```python\n>    # 将训练集和测试集进行token操作，返回的结果会多出attention_mask、input_ids和已经进行编码的text，可自行打印\n>    train_datasets = train_datasets.map(tokenize_function, batched=True)\n>    test_datasets = test_datasets.map(tokenize_function, batched=True)\n>    ```\n\n> 4. 准确率计算函数\n>\n>    ```python\n>    # 该准确率计算函数要使用可能需要有验证集\n>    metric =load_metric(\"../metrics/accuracy/accuracy.py\")# 该函数可能需要下载到本地才能进行使用。\n>    def compute_metrics(eval_pred):\n>        logits, labels = eval_pred\n>        predictions = np.argmax(logits, axis=-1)\n>        return metric.compute(predictions=predictions, references=labels)\n>    ```\n\n> 5. 模型定义和超参数选择\n>\n>    ```python\n>    # TrainingArguments还有很多其他的属性，建议可以去看官方的文档\n>    # 由于是直接进行训练，没有用验证集，故将一些属性注释掉\n>    training_args = TrainingArguments(\n>        output_dir='./results',         \t# 模型结果的保存路径       \n>        num_train_epochs=3,             \t# 模型训练次数\n>        per_device_train_batch_size=64, \t# 每个显卡上训练的batch大小\n>        #per_device_eval_batch_size = 16,   # 每个显卡上验证的batch大小\n>        warmup_steps=500,                   # 热学习率\n>        weight_decay=0.01,             \t\t# 权重衰退\n>        #evaluation_strategy = \"epoch\",     # 评价模型方式\n>        save_strategy = \"epoch\",            # 保存模型方式\n>        learning_rate=2e-5,\t\t\t\t\t# 学习率\n>        #load_best_model_at_end=True,\t\t# 是否在学习结束后加载最好模型\n>        #metric_for_best_model='accuracy'   # 评价最好模型标准\n>        logging_steps = 100\t\t\t\t\t# 打印log的操作步数\n>    )\n>    \n>    trainer = Trainer(\n>        model=model,                        \n>        args=training_args,             \n>        train_dataset=train_datasets['train'], \n>        #eval_dataset=train_datasets['test'],  # 这是之前分的验证集\n>        #compute_metrics=compute_metrics       # 这是之前定义的计算准确率的函数\n>    )\n>    \n>    # 将这些超参数进行定义后，就可以直接开始训练\n>    # pytorch可直接调用api，但是其他的需要参考文档\n>    trainer.train()\n>    ```\n\n> 6. 预测\n>\n>    ```python\n>    output = trainer.predict(test_dataset=test_datasets['test']) # 进行预测\n>    predictions = torch.argmax(torch.from_numpy(output[0]), dim=-1) # 对预测结果取概率最大的标签，可以打印进行查看\n>    \n>    # 这是生成kaggle提交的结果\n>    submission = pd.DataFrame({'id':test_id, 'target':list(predictions.numpy())})\n>    submission.to_csv('submission.csv', index=False)\n>    ```\n\n## 结果\n\n本次kaggle的score为0.81520，但是受限于设备和其他原因，应该还是可以从以下进行改进\n\n> 1. 数据清洗这块，只是粗看了一些较多的噪声，可以进行进一步处理，增加数据的质量。\n> 2. 本次只使用了text这个属性，keyword和location并没有使用，可以进行拼接或者其他操作来提高准确率。\n> 3. 由于训练是租用云gpu，所以很多参数并没有好好调整，包括预训练模型的选择也只是选择了一个很常用的模型，有条件的可以选择一个更大的模型并且参数可以进行更进一步调整。\n> 4. 本代码可能不一定能直接跑起来，主要注意各种文件的路径，如何预训练模型下不下来，可以去https://huggingface.co/models 进行下载，主要下载config、pytorch_model、vocab这三个文件，放在一个文件夹即可。\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}