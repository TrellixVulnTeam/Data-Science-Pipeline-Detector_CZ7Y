{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nimport matplotlib.pylab as plt\nimport tensorflow as tf\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation\nfrom keras.models import Model\nfrom keras import initializers, optimizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint\nimport seaborn, re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint('tf version ' + tf.version.VERSION)\nprint('GPU is avaiable' if tf.test.is_gpu_available() else 'GPU is NOT avaialable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nfig, axes = plt.subplots(1, 3, figsize=(11,3))\n\ntrain_df.text.str.len().groupby(train_df.target).mean().plot(kind='bar', color='c', ax = axes[0])\naxes[0].set_title('Avg txt length')\n\nseaborn.distplot(train_df[train_df.target==0].text.str.len(), ax=axes[1], color='b', label='fake')\naxes[1].legend()\n\nseaborn.distplot(train_df[train_df.target==1].text.str.len(), ax=axes[2], color='r', label='real')\naxes[2].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ngrams(txts, ngram_range=(2,2)):\n    vec = CountVectorizer(ngram_range=ngram_range).fit(txts)\n    BoW = vec.transform(txts)\n    sum_words = BoW.sum(axis=0) \n    wfreq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    wfreq = sorted(wfreq, key = lambda x: x[1], reverse=True)\n    return wfreq\n\ntrain_ngrams = get_ngrams(train_df.text)\nprint(train_ngrams[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(txt):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',txt)\n\ntrain_df.text = train_df.text.apply(lambda x: remove_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ngrams = get_ngrams(train_df.text)\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots(1, 1, figsize=(9,15))\n\nword, freq = map(list, zip(*train_ngrams[:80]))\nseaborn.barplot(x=freq, y=word);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.text.tolist(),\n                                                      train_df.target.tolist(),\n                                                      test_size=0.2)\n\n## Bag of Words\ncount_vec = CountVectorizer()\ncnt_train = count_vec.fit_transform(X_train)\ncnt_valid = count_vec.fit_transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TF-IDF\ntfidf_vec = TfidfVectorizer()\ntfidf_train = tfidf_vec.fit_transform(X_train)\ntfidf_valid = tfidf_vec.fit_transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib as mpl\n\ndef plot_LSA(data, labels, ax):\n    lsa = TruncatedSVD(n_components=2)\n    lsa.fit(data)\n    lsa_scores = lsa.transform(data)\n    color_mapper = {label:idx for idx,label in enumerate(set(labels))}\n    color_column = [color_mapper[label] for label in labels]\n    ax.scatter(lsa_scores[:,0], lsa_scores[:,1], s=20, alpha=0.5, c=labels)\n\nfig, axes = plt.subplots(1, 2, figsize=(9, 4))          \nplot_LSA(cnt_train, y_train, axes[0])\naxes[0].set_title('BoW')\nplot_LSA(tfidf_train, y_train, axes[1])\naxes[1].set_title('TF-IDF');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import tokenize\n\ndocs = []\nfor txt in train_df.text :\n    docs.append([w for w in tokenize.word_tokenize(txt.lower())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_one_hot (y, C) :\n    Y = np.eye(C)[y.reshape(-1)]\n    return Y\n\ny_train_oh = convert_to_one_hot(np.array(y_train), C=2)\ny_valid_oh = convert_to_one_hot(np.array(y_valid), C=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_glove_vecs (gfile) :\n    with open(gfile, 'r') as f :\n        words = set()\n        word2vec_map = {}\n        for line in f :\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word2vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n\n        ii = 1\n        word2idx = {}\n        idx2word = {}\n        for w in words :\n            word2idx[w] = ii\n            idx2word[ii] = w\n            ii = ii+1\n\n    return word2vec_map, word2idx, idx2word\n\nword2vec_map, word2idx, idx2word = read_glove_vecs('../input/glove-twitter/glove.twitter.27B.100d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in word2vec_map.keys() :\n    if len(word2vec_map[w]) ==  len(word2vec_map['love']) - 1:\n        word2vec_map[w] = np.pad(word2vec_map[w], [(0,1)], mode='constant') \nprint(word2vec_map['love'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sente2indices (X, word2idx, maxLen=50) :\n    m = np.shape(X)[0]\n    ids = np.zeros((m, maxLen))\n    for ii in range(m) :\n        words = X[ii].lower().split()\n        for idx, w in enumerate(words) :\n            if w in word2idx.keys() :\n                ids[ii, idx] = word2idx[w]\n\n    return ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding_layer (word2vec_map, word2idx) :\n    vocab_len = len(word2idx) + 1\n    emb_dim = np.shape(word2vec_map['love'])[0]\n    emb_mat = np.zeros((vocab_len, emb_dim))\n    for w, ii in word2idx.items() :\n        emb_mat[ii, :] = word2vec_map[w]\n\n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_mat])\n    return embedding_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def txt_cls (input_shape, word2vec_map, word2idx) :\n    X_input = Input(input_shape)\n    embedding_layer = create_embedding_layer(word2vec_map, word2idx)\n    X = embedding_layer(X_input)\n    X = LSTM(units=32, activation='tanh', use_bias=True, bias_initializer='zeros',\n        kernel_initializer='glorot_uniform', return_sequences=True)(X)\n    X = Dropout(0.2)(X)\n    X = LSTM(units=32, activation='tanh', use_bias=True, bias_initializer='zeros',\n        kernel_initializer='glorot_uniform', return_sequences=False)(X)\n    X = Dropout(0.2)(X)\n    X = Dense(2, activation=None, use_bias=True, bias_initializer='zeros',\n        kernel_initializer='glorot_uniform')(X)\n    X = Activation('sigmoid')(X)\n\n    model = Model(inputs=X_input, outputs=X)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 1\nmaxLen = 50\nclsModel = txt_cls((maxLen,), word2vec_map, word2idx)\nclsModel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## 2\noptim = optimizers.Adam(lr=0.0005)\nclsModel.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"## 3\ncp_fpath = \"clsModel.hdf5\"\nmdlcp_cb = ModelCheckpoint(cp_fpath, monitor='val_accuracy', mode='max', save_best_only=True,\n                          save_weights_only=True, verbose=1)\n\nX_train_ids = sente2indices(X_train, word2idx, maxLen)\nX_valid_ids = sente2indices(X_valid, word2idx, maxLen)\nclsRes = clsModel.fit(x=X_train_ids, \n                      y=y_train_oh,\n                      validation_data=(X_valid_ids, y_valid_oh),\n                      epochs=100, \n                      batch_size=512, \n                      shuffle=True,\n                      callbacks=[mdlcp_cb],\n                      verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.style.use('dark_background')\nfig, axes = plt.subplots(1, 2, figsize=(9,4))\n\naxes[0].plot(clsRes.history['loss'], 'w', label='train')\naxes[0].plot(clsRes.history['val_loss'], 'orange', label='valid')\naxes[0].set_ylabel('loss')\n\naxes[1].plot(clsRes.history['accuracy'], 'w', label='train')\naxes[1].plot(clsRes.history['val_accuracy'], 'orange', label='valid')\naxes[1].set_ylabel('accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clsModel_json = clsModel.to_json()\nwith open('clsModel.json', 'w') as myfile:\n     myfile.write(clsModel_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clsModel = txt_cls((maxLen,), word2vec_map, word2idx)\nclsModel.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\nclsModel.load_weights('/kaggle/working/clsModel.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print mislabelled examples\npred = clsModel.predict(X_valid_ids)\nfor ii in range(50):\n    x = X_valid_ids\n    num = np.argmax(pred[ii])\n    if(num != y_valid[ii]):\n        print('pred: ' + str(num) + \n              ', true: ' + str(y_valid[ii]) + \n               ' --> ', X_valid[ii] + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print correctly labelled examples\npred = clsModel.predict(X_valid_ids)\nfor ii in range(20):\n    x = X_valid_ids\n    num = np.argmax(pred[ii])\n    if(num == y_valid[ii]):\n        print('pred: ' + str(num) + \n              ', true: ' + str(y_valid[ii]) + \n               ' --> ', X_valid[ii] + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## 4\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_df.text = test_df.text.apply(lambda x: remove_url(x))\n\nX_test_ids = sente2indices(test_df.text.tolist(), word2idx, maxLen)\ny_test_oh = clsModel.predict(X_test_ids)\ny_test = [np.argmax(x) for x in y_test_oh]\ntest_df['target'] = y_test\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsub_df.target = y_test\nsub_df.head()\nsub_df.to_csv(\"sub-glove.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}