{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#560010'>INTRODUCTION</span>\n\nThis notebook is a beginner kernel about NLP. For submission will evaluate different models based on Accuracy and F1-Score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>1.Import Libraries</span>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score , confusion_matrix, f1_score\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport xgboost as xgboost\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\n\n# Plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True) # #do not miss this line\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom wordcloud import WordCloud,STOPWORDS\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>2.Read Datas</span>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndata_test = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>3.Data Analysis</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train.head())\ndisplay(data_test.head())\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Columns\n<p><strong style='font-weight:bold;color:#561225'>id</strong>: a unique identifier for each tweet</p>\n<p><strong style='font-weight:bold;color:#561225'>text </strong>: the text of the tweet </p>\n<p><strong style='font-weight:bold;color:#561225'>location </strong>: the location the tweet was sent from (may be blank)</p>\n<p><strong style='font-weight:bold;color:#561225'>keyword </strong>: a particular keyword from the tweet (may be blank)</p>\n<p><strong style='font-weight:bold;color:#561225'>target </strong>: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set contains {} rows and {} cols\".format(data_train.shape[0],data_train.shape[1]))\nprint(\"Test set contains {} rows and {} cols\".format(data_test.shape[0],data_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.location.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values in train set\ndata_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values in test set\ndata_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many missing values in the location column in both train and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data_train[['text', 'target']]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = data_test[['text']]\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>4.Data Pre-Processing</span>\n\nTweets are not syntactically well constructed. So preprocess will be applied.\n\n*  Convert all letters in tweets to lower case,\n*  Tokenization (disassembling according to desired features)\n*  Remove punctuation marks in the text,\n*  Removing Stopwords (commonly used words: the, at, and…)\n*  Removing URLs, mentions and usernames,\n*  Removing numerical expressions,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_tweets(text):\n    text = re.sub('https?://[A-Za-z0-9./]*','', text) # Remove https..(URL)\n    text = re.sub('[0-9]*','', text) # Removed digits\n    text = re.sub('RT @[\\w]*:','', text) # Removed RT \n    text = re.sub('@[A-Za-z0-9]+', '', text) # Removed @mention\n    text = re.sub('&amp; ','',text) # Removed &(and) \n    return text\n\ndef remove_punctuations(text):\n    text = ' '.join([i for i in text if i not in frozenset(string.punctuation)])\n    return text\n\nstop = stopwords.words('english')\nstop_list = ['u','û_']\nfor i in range(len(stop_list)):\n    stop.append(stop_list[i])\n\ndef remove_stopword(text):\n    words = [w for w in text if w not in stop]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cleaned_text'] = train['text'].apply(clean_tweets)\ntrain['cleaned_text'] = train['cleaned_text'].apply(lambda x: x.lower()) \ntokenizer = RegexpTokenizer(r'\\w+')\ntrain['cleaned_text'] = train['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_stopword) \ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations) \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['cleaned_text'] = test['text'].apply(clean_tweets)\ntest['cleaned_text'] = test['cleaned_text'].apply(lambda x: x.lower()) \ntokenizer = RegexpTokenizer(r'\\w+')\ntest['cleaned_text'] = test['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\ntest['cleaned_text'] = test['cleaned_text'].apply(remove_stopword) \ntest['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations) \ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization\n\n<p><strong style='font-weight:bold;color:#561225'>Stemming,</strong> refers to reducing a word to its root form. \n\n<p><strong style='font-weight:bold;color:#561225'>Lemmatization,</strong> on the other hand, takes into consideration the morphological analysis of the words. <br>\n\nStemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word.\n\n![](https://qph.fs.quoracdn.net/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n    \nLet's use lemmatization technique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cleaned_text'] = train['cleaned_text'].apply(lemmatize_text)\ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations) \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['cleaned_text'] = test['cleaned_text'].apply(lemmatize_text)\ntest['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations) \ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>5.Visualization</span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The Target Distribution.\n\nWe will see the target distribution with visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 4), subplot_kw=dict(aspect=\"equal\"))\nlabels=['Not-Disaster', 'Disaster']\nwedges, texts, autotexts = ax.pie(data_train.target.value_counts(),autopct=\"%1.2f%%\", colors=['#66b3ff','#cc1d00'], \n                                            explode = (0,0.07), startangle=90,\n                                            textprops={'fontsize': 15, 'color':'#f5f5f5'})\nplt.title('The Target Distribution', fontsize=16, weight=\"bold\")\nax.legend(wedges, labels,\n          title=\"Ingredients\",\n          loc=\"center left\",\n          bbox_to_anchor=(1.2, 0, 0, 1))\n\nplt.setp(autotexts, weight=\"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x=['Disaster', 'Not-Disaster'], \n                        y=[len(data_train[data_train['target']== 1]),len(data_train[data_train['target']== 0])])])\nfig.update_traces(marker_color='indianred', marker_line_color='rgb(58,48,107)',\n                  marker_line_width=1.5, opacity=0.7)\nfig.update_layout(title_text='The Target Distribution',autosize=False,width=400,height=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that class 0 (Not-Disaster Tweets) is more than class 1 (Disaster Tweets).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### WordCloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster = train[train['target']==1]['cleaned_text']\nnon_disaster = train[train['target']==0]['cleaned_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Disaster Tweet: {} \\nNot-Disaster Tweet: {}'.format(disaster.values[2],non_disaster.values[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\npath = '../input/twitter-logo/twitter_logo.png'\nmask = np.array(Image.open(path).convert('L'))\nmask.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grey_color_func(word, font_size, position, orientation, **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % np.random.randint(60, 100)\n\nfig, (plt1, plt2) = plt.subplots(1, 2, figsize=[14, 6])\nwordcloud = WordCloud(\n                        background_color='#123456',\n                        random_state = 42,\n                        max_words= 50,\n                        mask=mask,\n                        contour_width=1,\n                        contour_color=\"#b5b5b5\"\n                     ).generate(''.join(disaster))\n\nplt1.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\nplt1.axis(\"off\")\nplt1.set_title('Disaster Tweets',fontsize=30);\n\nwordcloud = WordCloud(\n                        background_color='#123456',\n                        random_state = 42,\n                        max_words= 50,\n                        mask=mask,\n                        contour_width=1,\n                        contour_color=\"#b5b5b5\"\n                     ).generate(''.join(non_disaster))\n\nplt2.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\nplt2.axis(\"off\")\nplt2.set_title('Not-Disaster Tweets',fontsize=30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>6.Converting Tokens to a Vector</span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=300\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=stop)\ntrain_vectors = count_vectorizer.fit_transform(train['text']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('In Train Set, the most common {} words:\\n{} '.format(max_features,count_vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF\n\n<i style='font-weight:bold;color:#561225'>TF: </i>Term Frequency, which measures how frequently a term occurs in a document.<br>\n* TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). <br>\n\n<i style='font-weight:bold;color:#561225'>IDF: </i>Inverse Document Frequency, which measures how important a term is.<br>\n* IDF(t) = log_e(Total number of documents / Number of documents with term t in it).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Term Frequency\ntf = (train.cleaned_text).apply(lambda x : pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.columns = ['words','frequence']\ntf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfreq = tf[tf['frequence']>100.0]\nplt.subplots(figsize = (18,5))\nchart = sns.barplot(x=tfreq.words, y=tfreq.frequence, palette=sns.color_palette(\"coolwarm\",7), edgecolor=\".3\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=75)\nchart.set_title('Frequencies of the Most Common Words');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf_ngram = TfidfVectorizer(ngram_range=(1,2))\ntf_idf_ngram.fit(train.cleaned_text)\nx_train_tf_bigram = tf_idf_ngram.transform(train.cleaned_text) #.todense()\nx_test_tf_bigram = tf_idf_ngram.transform(test.cleaned_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_tf_bigram.shape,x_test_tf_bigram.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf_ngram.get_feature_names()[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>7. Text Classification Models</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = x_train_tf_bigram\ny = train.target.values\n\n# Train-Test Splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Train Data splitted successfully')\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_accuracy = pd.DataFrame(columns=[\"Model\",\"Accuracy\",\"F1_score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_LR = LogisticRegression(C=2,dual=True, solver='liblinear',random_state=0)\nclf_LR.fit(X_train,y_train)\n\n# Predicting \ny_pred_LR = clf_LR.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_LR) * 100\nf1score = f1_score(y_test, y_pred_LR) * 100\nprint(\"Logistic Regression Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Logistic Regression F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'LogisticRegression','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_NB = MultinomialNB()\nclf_NB.fit(X_train,y_train)\n\n# Predicting \ny_pred_NB = clf_NB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_NB) * 100\nf1score = f1_score(y_test, y_pred_NB) * 100\nprint(\"MultinomialNB Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"MultinomialNB F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'NaiveBayes','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Nearest Neighbors Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_KNN = KNeighborsClassifier(n_neighbors = 7,weights = 'distance')\nclf_KNN.fit(X_train, y_train)\n\n# Predicting \ny_pred_KNN = clf_KNN.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_KNN) * 100\nf1score = f1_score(y_test, y_pred_KNN) * 100\nprint(\"K-Nearest Neighbors Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"K-Nearest Neighbors F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'K-NearestNeighbors','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_RF = RandomForestClassifier(random_state=0)\nclf_RF.fit(X_train,y_train) \n\n# Predicting \ny_pred_RF = clf_RF.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_RF) * 100\nf1score = f1_score(y_test, y_pred_RF) * 100\nprint(\"Random Forest Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Random Forest F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'RandomForest','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_DT = DecisionTreeClassifier(criterion= 'entropy', random_state=0)\nclf_DT.fit(X_train,y_train) \n\n# Predicting \ny_pred_DT = clf_DT.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_DT) * 100\nf1score = f1_score(y_test, y_pred_DT) * 100\nprint(\"Decision Tree Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Decision Tree F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'DecisionTree','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_GB = GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=20, random_state=0)\nclf_GB.fit(X_train,y_train)\n\n# Predicting \ny_pred_GB = clf_GB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_GB) * 100\nf1score = f1_score(y_test, y_pred_GB) * 100\nprint(\"Gradient Boosting Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Gradient Boosting Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'GradientBoostingClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBOOST Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_XGB = xgboost.XGBClassifier(n_estimators=400, random_state=0, learning_rate=0.05, booster=\"gbtree\",\n                                n_jobs=-1, max_depth=20)\nclf_XGB.fit(X_train,y_train)\n# Predicting \ny_pred_XGB = clf_XGB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_XGB) * 100\nf1score = f1_score(y_test, y_pred_XGB) * 100\nprint(\"XGBOOST Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"XGBOOST Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'XGBOOSTClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGB Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Train set\nclf_LGB = LGBMClassifier(n_estimators=1300, learning_rate=0.05, random_state=0, max_depth=20, n_jobs=-1)\nclf_LGB.fit(X_train,y_train)\n\n# Predicting \ny_pred_LGB = clf_LGB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_LGB) * 100\nf1score = f1_score(y_test, y_pred_LGB) * 100\nprint(\"LightGB Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"LightGB Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'LightGBClassifier','Accuracy':accuracy, 'F1_score': f1score},ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy and F1-score Comparison of Models\ntrace1=go.Bar(\n                x=df_accuracy.Model,\n                y=df_accuracy.Accuracy,\n                name=\"Accuracy\",\n                marker=dict(color = 'rgba(50, 240,120, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text='Accuracy')\ntrace2=go.Bar(\n                x=df_accuracy.Model,\n                y=df_accuracy.F1_score,\n                name=\"F1-score\",\n                marker=dict(color = 'rgba(240,120,10 , 0.7)', \n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text='F1-score')\n\nedit_df=[trace1,trace2]\nlayout=go.Layout(barmode=\"group\", xaxis_tickangle=-60, title=\"Accuracy and F1-score of Models\")\nfig=dict(data=edit_df,layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this kernel, 8 machine learning algorithms were used.As result;\n* The accuracy and the f1-score of Decision Tree Algorithm is lower than other algorithms.\n* The F1-score of the XGBOOST Algorithm is the highest compared to the others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>8. Prediction and Submission</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = clf_XGB.predict(x_test_tf_bigram)\nsubmission['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_final= submission[['id','target']]\nsubmission_final.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='font-weight:bold;color:#561225'>9. References</span>\n\n* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n* https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro\n* https://www.kaggle.com/kushbhatnagar/disaster-tweets-eda-nlp-classifier-models/notebook\n* https://www.kaggle.com/elcaiseri/nlp-the-simplest-way","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style='font-weight:bold;color:#123456'><i>I hope you find this kernel useful. If you like it please do an upvote.</i><p> ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}