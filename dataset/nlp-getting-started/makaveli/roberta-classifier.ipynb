{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tokenizers\nimport transformers\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAX_LEN = 64\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nLEARNING_RATE = 3e-5\nEPOCHS = 3\nTRAINING_FILE = \"../input/nlp-getting-started/train.csv\"\nTEST_FILE = \"../input/nlp-getting-started/test.csv\"\nROBERTA_PATH = \"../input/tf-roberta\"\n# TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n#         vocab_file=f\"{ROBERTA_PATH}/vocab-roberta-base.json\", \n#         merges_file=f\"{ROBERTA_PATH}/merges-roberta-base.txt\", \n#         lowercase=True,\n#         add_prefix_space=True\n#     )\nroberta_tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base', lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAINING_FILE).fillna('')\ntest = pd.read_csv(TEST_FILE).fillna('')\nprint(\"Training samples: {}\".format(train.shape[0]))\nprint(\"Test samples: {}\".format(test.shape[0]))\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove urls"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove urls\nurl = \"Great paper by Kalchbrenner https://arxiv.org/pdf/1404.2188.pdf?utm_medium=App.net&utm_source=PourOver\"\n\ndef remove_urls(text):\n    re_url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return re_url.sub('', text).strip()\n\nprint(remove_urls(url))\ntrain['text'] = train['text'].apply(lambda x : remove_urls(x))\ntest['text'] = test['text'].apply(lambda x : remove_urls(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove Html "},{"metadata":{"trusted":true},"cell_type":"code","source":"html = \"\"\"<div>\n<h1>Hey</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">removed tags</a>\n</div>\"\"\"\n# remove html tags\ndef remove_html(text):\n    re_html = re.compile(r'<.*?>')\n    return re_html.sub('', text)\n\nprint(remove_html(html))\ntrain['text'] = train['text'].apply(lambda x : remove_html(x)) \ntest['text'] = test['text'].apply(lambda x : remove_html(x)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove emojis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text).strip()\n\nprint(remove_emoji(\"Difficult kernel ðŸ˜”ðŸ˜”\"))\ntrain['text'] = train['text'].apply(lambda x: remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove punctuations\npunct = 'Cristiano. is #king .l'\nimport string\ndef remove_puncts(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table).strip()\n\nprint(remove_puncts(punct))\ntrain['text'] = train['text'].apply(lambda x: remove_puncts(x))\ntest['text'] = test['text'].apply(lambda x: remove_puncts(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = train.target.nunique()\nnum_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train = train.shape[0]\ninput_ids = np.ones((n_train, MAX_LEN), dtype='int32')\nmask = np.zeros((n_train, MAX_LEN), dtype='int32')\n\n# roberta tokenizer\nfor k in range(train.shape[0]):\n    text = train.loc[k, 'text']\n    output = roberta_tokenizer.encode_plus(text, max_length=MAX_LEN, pad_to_max_length=True)\n    input_ids[k] = output['input_ids']\n    mask[k] = output[\"attention_mask\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_test = test.shape[0]\ninput_ids_t = np.ones((n_test, MAX_LEN), dtype='int32')\nmask_t = np.zeros((n_test, MAX_LEN), dtype='int32')\n\n# roberta tokenizer\nfor k in range(test.shape[0]):\n    text = test.loc[k, 'text']\n    output = roberta_tokenizer.encode_plus(text, max_length=MAX_LEN, pad_to_max_length=True)\n    input_ids_t[k] = output['input_ids']\n    mask_t[k] = output[\"attention_mask\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset():\n    xtrain = [input_ids, mask]\n    xtest = [input_ids_t, mask_t]\n    \n    ytrain = tf.keras.utils.to_categorical(train['target'].values.reshape(-1, 1))\n    return xtrain, ytrain, xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, ytrain, xtest = create_dataset()\nprint(\"X train : {0}\".format(len(xtrain[0])))\nprint(\"Y train : {0}\".format(len(ytrain)))\nprint(\"X test : {0}\".format(len(xtest[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    roberta = transformers.TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n    optim = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=2.0)\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    roberta.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n    return roberta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"Folds = 5\nkfold = KFold(n_splits=Folds)\npredictions = list()\n\nfor i, (train_idx, test_idx) in enumerate(kfold.split(xtrain[0])):\n    xtrain_fold = [xtrain[i][train_idx] for i in range(len(xtrain))]\n    xvalid_fold = [xtrain[i][test_idx] for i in range(len(xtrain))]\n    \n    ytrain_fold = ytrain[train_idx]\n    yvalid_fold = ytrain[test_idx]\n    \n    # class weights to deal with class imbalance\n    positive = train.iloc[train_idx, :].target.value_counts()[0]\n    negative = train.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive / (positive + negative)\n    neg_weight = negative / (positive + negative)\n\n    class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n    \n    tf.keras.backend.clear_session()\n    \n    roberta = build_model()\n    roberta.fit(xtrain_fold, ytrain_fold, \n                batch_size=TRAIN_BATCH_SIZE, \n                epochs=EPOCHS, \n                class_weight=class_weight,\n                validation_data=(xvalid_fold, yvalid_fold))\n    val_preds = roberta.predict(xvalid_fold, batch_size=VALID_BATCH_SIZE, verbose=1)\n    val_preds = np.argmax(val_preds, axis=1).flatten()\n    print(metrics.accuracy_score(train.iloc[test_idx, :].target.values, val_preds))\n\n    preds = roberta.predict(xtest, batch_size=TRAIN_BATCH_SIZE, verbose=1)\n    predictions.append(preds)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\npredictions = np.average(predictions, axis=0)\npredictions = np.argmax(predictions, axis=1).flatten()\nsample_submission['target'] = predictions\nsample_submission['target'].value_counts()\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}