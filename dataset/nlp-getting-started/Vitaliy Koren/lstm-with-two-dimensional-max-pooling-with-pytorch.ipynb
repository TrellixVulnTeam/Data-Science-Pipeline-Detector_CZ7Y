{"cells":[{"metadata":{},"cell_type":"markdown","source":"Inspired by model from [paper](https://www.aclweb.org/anthology/C16-1329.pdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"03hv__R3RwYv","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import torch\nfrom torchtext import data\n\nimport pandas as pd\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchtext\n\nimport spacy\n\nfrom torch.autograd import Variable\n\nimport time\nimport copy\nfrom torch.optim import lr_scheduler\n\nfrom sklearn.model_selection import train_test_split\nfrom torchtext.vocab import Vectors, GloVe\nfrom matplotlib.pyplot import plot, hist, xlabel, legend","execution_count":null,"outputs":[]},{"metadata":{"id":"gFvlkOA--Uo9","colab_type":"text"},"cell_type":"markdown","source":"Let's have a look at our data"},{"metadata":{"id":"_U_xlEJm-R0F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"2e590703-dc70-4406-cad0-d9945556907b","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"1hMJElin-y9d","colab_type":"text"},"cell_type":"markdown","source":"Now we will clean it. Firstly, we will **remove urls**:"},{"metadata":{"id":"IUml8wDp_c-c","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import re\nurl = re.compile(r'https?://\\S+|www\\.\\S+')\ntrain = train_data['text'].apply(lambda tweet: url.sub(r'',tweet))\ntest = test_data['text'].apply(lambda tweet: url.sub(r'',tweet))","execution_count":null,"outputs":[]},{"metadata":{"id":"MOCyHbGWATkV","colab_type":"text"},"cell_type":"markdown","source":"Next step will be **lowercasing** and noise removal"},{"metadata":{"id":"YsVlhbWY_m8d","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train=train.str.lower().str.replace(\"[^a-z]\", \" \")\ntest=test.str.lower().str.replace(\"[^a-z]\", \" \")","execution_count":null,"outputs":[]},{"metadata":{"id":"_DZRF2_JAk9G","colab_type":"text"},"cell_type":"markdown","source":"Splitting text for **lemmatization ** and** stop-word removal**"},{"metadata":{"id":"8lzc7AQPAcJi","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"tokenized_train = train.apply(lambda tweet: tweet.split())\ntokenized_test = test.apply(lambda tweet: tweet.split())","execution_count":null,"outputs":[]},{"metadata":{"id":"CFgYGiBXBHX-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"9d79a441-9e77-42dd-a338-b8b263e9f661","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \nstop_words = stopwords.words('english')\n\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\ntokenized_train = tokenized_train.apply(lambda tweet: [lem.lemmatize(word) for word in tweet if word not in stop_words])\ntokenized_test = tokenized_test.apply(lambda tweet: [lem.lemmatize(word) for word in tweet if word not in stop_words])","execution_count":null,"outputs":[]},{"metadata":{"id":"7g23A1tABO-Y","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"detokenized_train = [] \nfor i in range(len(train)): \n    t = ' '.join(tokenized_train[i]) \n    detokenized_train.append(t) \n\ndetokenized_test = [] \nfor i in range(len(test)): \n    t = ' '.join(tokenized_test[i]) \n    detokenized_test.append(t) \n\ntrain_data['text'] = detokenized_train\ntest_data['text'] = detokenized_test","execution_count":null,"outputs":[]},{"metadata":{"id":"_GnuHGgOBgFh","colab_type":"text"},"cell_type":"markdown","source":"So, now it looks like that:"},{"metadata":{"id":"9xclg_GBBe2i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"ffc2b10e-df4e-436b-bffa-cf81503f7b3a","trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"4G9KTeQtCBwJ","colab_type":"text"},"cell_type":"markdown","source":"Leave only the necessary columns"},{"metadata":{"id":"4zJDG7S5Bu7k","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train = train_data[['text', 'target']]\ntest = test_data[['text']]","execution_count":null,"outputs":[]},{"metadata":{"id":"peua7x2iCne0","colab_type":"text"},"cell_type":"markdown","source":"Splitting to **train** and **validation** sets"},{"metadata":{"id":"7u7XB3uNtOLO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"X = train['text']\ny = train['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"aSgAYFM7tb_H","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_data = pd.concat([X_train, y_train], axis=1)\nval_data = pd.concat([X_val, y_val], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"scxrFAlnC7jY","colab_type":"text"},"cell_type":"markdown","source":"And saving this sets:"},{"metadata":{"id":"7yaOo_uWtiXU","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"!mkdir torchtext_data","execution_count":null,"outputs":[]},{"metadata":{"id":"BW38ecWttlIi","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_data.to_csv(\"torchtext_data/train.csv\", index=False)\nval_data.to_csv(\"torchtext_data/val.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"sHUY6fj9tqaC","colab_type":"code","outputId":"d6619d38-d479-4b1e-b4aa-16fa2c2c40b7","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"is_cuda = torch.cuda.is_available()\nprint(\"Cuda Status on system is {}\".format(is_cuda))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"id":"6JLMm6ShDq2i","colab_type":"text"},"cell_type":"markdown","source":"Also, we should know length of tweets:"},{"metadata":{"id":"qTp8Dj6dD8fr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":335},"outputId":"b55023d2-93cc-4385-9067-c9f2e39dc654","trusted":true},"cell_type":"code","source":"tweet_len=train['text'].str.split().map(lambda x: len(x))\nhist(tweet_len,color='blue')","execution_count":null,"outputs":[]},{"metadata":{"id":"kF3VAvQyEzOx","colab_type":"text"},"cell_type":"markdown","source":"So, I'd like to choose fix_length = 17. It length of sequence, which we will input in our LSTM, shorter will be padded by zeros, when longer will be truncated"},{"metadata":{"id":"DTjHwrCUt4Ww","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"fix_length = 17\nTEXT = data.Field(sequential=True, tokenize=\"spacy\", fix_length=fix_length)\nLABEL = data.LabelField(dtype=torch.long, sequential=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"m6-etSW8F4M-","colab_type":"text"},"cell_type":"markdown","source":"Define train and validation datasets:"},{"metadata":{"id":"3cUWVr4btwjK","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_data, val_data = data.TabularDataset.splits(\n    path=\"torchtext_data/\", train=\"train.csv\", \n    test=\"val.csv\",format=\"csv\", skip_header=True, \n    fields=[('Text', TEXT), ('Label', LABEL)]\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"rM8NG_S5t8po","colab_type":"code","outputId":"b51a94eb-e3b4-4b5c-b2b5-7069c31910e8","colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"code","source":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of testing examples: {len(val_data)}')","execution_count":null,"outputs":[]},{"metadata":{"id":"bvzeMeyEGIq_","colab_type":"text"},"cell_type":"markdown","source":"Building vocabulary using GloVe with dim = 300\nIt can take some time for downloading"},{"metadata":{"id":"zUN2Lnf3uC3Q","colab_type":"code","outputId":"41579813-b940-4eed-ac69-2041b16a7dd6","colab":{"base_uri":"https://localhost:8080/","height":70},"trusted":true},"cell_type":"code","source":"TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\nLABEL.build_vocab(train_data)\n\nprint(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\nprint(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"bo-M67N1G1Zg","colab_type":"text"},"cell_type":"markdown","source":"Defining iterators:"},{"metadata":{"id":"NRyXiGtduJQx","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"batch_size = 16\n \ntrain_iterator, val_iterator = data.BucketIterator.splits(\n    (train_data, val_data), sort_key=lambda x: len(x.Text),\n    batch_size=batch_size,\n    device=device)","execution_count":null,"outputs":[]},{"metadata":{"id":"my_yK2fVIdfG","colab_type":"text"},"cell_type":"markdown","source":"Finaly, our model:"},{"metadata":{"id":"T8xTAJsXuM7L","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class LSTM2DMaxPoolClassifier(nn.Module):\n\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, num_layers, weights):\n\t\tsuper(LSTM2DMaxPoolClassifier, self).__init__()\n\t\t\n\t\t\"\"\"\n\t\tArguments\n\t\t---------\n\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n\t\toutput_size : 2 = (pos, neg)\n\t\thidden_sie : Size of the hidden_state of the LSTM\n\t\tvocab_size : Size of the vocabulary containing unique words\n\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\t\t\n\t\t\"\"\"\n\t\tself.hidden_size = hidden_size\n\t\tself.batch_size = batch_size\n\t\tself.num_layers = num_layers\n\t\t\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n\t\tself.lstm = nn.LSTM(embedding_length, hidden_size, num_layers, batch_first = True)\n\t\tself.maxpool = nn.MaxPool1d(4) # Where 4 is kernal size\n\t\tself.label = nn.Linear(hidden_size//4, output_size)  #//4 for maxpool\n\t\t\n\tdef forward(self, input_sentence, batch_size=None):\n\t\n\t\t\"\"\" \n\t\tParameters\n\t\t----------\n\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\t\t\n\t\tReturns\n\t\t-------\n\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n\t\tfinal_output.shape = (batch_size, output_size)\n\t\t\n\t\t\"\"\"\n\t\t\n\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n\t\tif batch_size is None:\n\t\t  h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM, num_layers*2 for biderection\n\t\t  c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM, num_layers*2 for biderection\n\t\telse:\n\t\t\th_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n\t\t\tc_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n\n\t\tpooled = self.maxpool(output)\n\n\t\tfinal_output = self.label(pooled[:, -1, :]) #the same if we would use final_hidden_state\n\n\t\treturn final_output","execution_count":null,"outputs":[]},{"metadata":{"id":"gpjrg_VLuN95","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"word_embeddings = TEXT.vocab.vectors\noutput_size = 2\nnum_layers = 1\nhidden_size = 32\nembedding_length = 300\nvocab_size = len(TEXT.vocab)","execution_count":null,"outputs":[]},{"metadata":{"id":"a0_b8wGJuQam","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"model = LSTM2DMaxPoolClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, num_layers, word_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"id":"-AW3dQDbuS4B","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"id":"PR-S28cFuU9e","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"model = model.to(device)\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"ePvKJhnluWdS","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"dataiter_dict = {'train': train_iterator, 'val': val_iterator}\ndataset_sizes = {'train':len(train_data), 'val':len(val_data)}","execution_count":null,"outputs":[]},{"metadata":{"id":"JX10NlX5uY1S","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n    print('starting')\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 200\n\n    val_loss = []\n    train_loss = []\n    val_acc = []\n    train_acc = []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            sentiment_corrects = 0\n            tp = 0.0\n            tn = 0.0\n            fp = 0.0\n            fn = 0.0\n                      \n            # Iterate over data.\n            for batch in dataiter_dict[phase]:\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    text = batch.Text\n                    label = batch.Label\n                    label = torch.autograd.Variable(label).long()\n                    if torch.cuda.is_available():\n                      text = text.cuda()\n                      label = label.cuda()\n                    if (batch.Text.size()[1] is not batch_size):\n                      continue\n                    \n                    outputs = model(text)\n\n                    outputs = F.softmax(outputs,dim=-1)\n                    \n                    loss = criterion(outputs, label)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        \n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * text.size(0)\n                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == label)\n\n                tp += torch.sum(torch.max(outputs, 1)[1] & label)\n                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-label)\n                fp += torch.sum(torch.max(outputs, 1)[1] & 1-label)\n                fn += torch.sum(1-torch.max(outputs, 1)[1] & label)\n\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n \n            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n\n            if phase == 'train':\n                train_acc.append(sentiment_acc)\n                train_loss.append(epoch_loss)\n            elif phase == 'val':\n                val_acc.append(sentiment_acc)\n                val_loss.append(epoch_loss)\n\n            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n            print('{} sentiment_acc: {:.4f}'.format(\n                phase, sentiment_acc))\n\n            if phase == 'val' and epoch_loss < best_loss:\n                print('saving with loss of {}'.format(epoch_loss),\n                      'improved over previous {}'.format(best_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), 'lstm_model_test.pth')\n\n        print()\n\n    confusion_matrix = [[tp, fp],[fn, tn]]\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val loss: {:4f}'.format(float(best_loss)))\n    results = {'time': time_elapsed, 'conf_matr': confusion_matrix,\n               'val_loss': val_loss, 'train_loss': train_loss, 'val_acc': val_acc, 'train_acc': train_acc}\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, results","execution_count":null,"outputs":[]},{"metadata":{"id":"gTzRjTu-I4t2","colab_type":"text"},"cell_type":"markdown","source":"Fit the model:"},{"metadata":{"id":"AirEPE06ub21","colab_type":"code","outputId":"7f157ce6-e4da-486e-fdd4-92ceafb18320","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"model_fit, res = train_model(model, criterion, optimizer, exp_lr_scheduler,\n                       num_epochs=8)","execution_count":null,"outputs":[]},{"metadata":{"id":"IUF0KNtOKsZ3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":296},"outputId":"d0ca7f8e-e7fe-4462-fc63-68370ac29a54","trusted":true},"cell_type":"code","source":"plot(res['train_acc'], label = 'Train Accuracy')\nplot(res['val_acc'], label = 'Test Accuracy')\nxlabel('Epoch')\nlegend()","execution_count":null,"outputs":[]},{"metadata":{"id":"2V_CIb4fMzPn","colab_type":"text"},"cell_type":"markdown","source":"Let's make a prediction:"},{"metadata":{"id":"m88_S0PYwukB","colab_type":"code","outputId":"f862508a-7f66-48a0-8bbf-f1142a540fd9","colab":{"base_uri":"https://localhost:8080/","height":197},"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Eus_mJN3vq-i","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test.to_csv(\"torchtext_data/test.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"4HuUmjMOv0-R","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test_data = data.TabularDataset(\n    path=\"torchtext_data/test.csv\", format=\"csv\", skip_header=True, \n    fields=[('Text', TEXT)]\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"beNccPzqw6k9","colab_type":"code","outputId":"8219dc48-348f-4690-bdf5-26062932b4c4","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"print(f'Number of testing examples: {len(test_data)}')","execution_count":null,"outputs":[]},{"metadata":{"id":"KFrwW21oxFy0","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"batch_size = 16\n\n# keep in mind the sort_key option \ntest_iterator = data.BucketIterator(\n    test_data,\n    train=False,\n    sort = False,\n    sort_within_batch=False,\n    repeat=False,\n    batch_size=batch_size,\n    device=device)","execution_count":null,"outputs":[]},{"metadata":{"id":"TNcvjMe7utUd","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"model.eval()\npred = []\n\nfor batch in test_iterator:\n  text = batch.Text\n  if torch.cuda.is_available():\n    text = text.cuda()\n  if (batch.Text.size()[1] is not batch_size):\n    continue\n  outputs = model(text)\n  outputs = F.softmax(outputs,dim=-1)\n  outputs = torch.max(outputs, 1)[1]\n  pred.append(outputs.cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"id":"dhE6Xr9u2zPM","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"pred = np.array(pred)\npred = pred.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"QMnR1vpC18rO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsub = pd.concat([test['id'], pd.Series(pred, name='target')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-9het8S14nGq","colab_type":"code","outputId":"60668510-447a-472c-c565-1baac512f2ab","colab":{"base_uri":"https://localhost:8080/","height":197},"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tXBHSRax5qgH","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"sub = sub.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"mCk3z34E6v0S","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"sub = sub.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"id":"jzJZbesq7B0Y","colab_type":"code","outputId":"089eab67-e063-4140-8ccb-dcf4d02f73c1","colab":{"base_uri":"https://localhost:8080/","height":406},"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"id":"eO2XCkq07EfP","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"sub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"L9GsVYiR7KzQ","colab_type":"code","outputId":"07b90aae-729b-4ee8-d16a-3b5064ef79e6","colab":{"base_uri":"https://localhost:8080/","height":194},"trusted":true},"cell_type":"code","source":"!head sub.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"NLPnicetry.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}