{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T15:11:06.099642Z","iopub.execute_input":"2021-07-21T15:11:06.100083Z","iopub.status.idle":"2021-07-21T15:11:06.115794Z","shell.execute_reply.started":"2021-07-21T15:11:06.099997Z","shell.execute_reply":"2021-07-21T15:11:06.114782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The author Fanglida Yan has used code from these references in the notebook. <br>\nhttps://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e <br>\nhttps://www.youtube.com/watch?v=hhjn4HVEdy0 <br>\n\n0. lower case <br>\n1. turn key words into lists <br>\n2. extract hashtags and create a new feature column <br>\n3. remove digits (01234), urls (http://...), mentions (@...) and hashtags (#...) <br>\n4. recover abbreviations (change they'll to they will, etc) <br>\n5. remove punctuations <br>\n6. tokenization <br>\n7. remove stop words <br>\n8. lemmatization <br>\n12. word embedding","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:06.117204Z","iopub.execute_input":"2021-07-21T15:11:06.117516Z","iopub.status.idle":"2021-07-21T15:11:06.285699Z","shell.execute_reply.started":"2021-07-21T15:11:06.117487Z","shell.execute_reply":"2021-07-21T15:11:06.284843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**0. lower case**","metadata":{}},{"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : x.lower())\ntest['text']=test['text'].apply(lambda x : x.lower())\n\ndef lower_keywords(keywords):\n    if keywords == keywords:\n        keywords=keywords.lower()\n    return keywords\n        \ntrain['keyword']=train['keyword'].apply(lambda x : lower_keywords(x))\ntest['keyword']=test['keyword'].apply(lambda x : lower_keywords(x))\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:06.287073Z","iopub.execute_input":"2021-07-21T15:11:06.287522Z","iopub.status.idle":"2021-07-21T15:11:06.332244Z","shell.execute_reply.started":"2021-07-21T15:11:06.287478Z","shell.execute_reply":"2021-07-21T15:11:06.33126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. turn keywords into lists**","metadata":{}},{"cell_type":"code","source":"def keywords_to_list(keywords):\n    if keywords!=keywords: # nan value is not equal to itself\n        return []\n    else:\n        return keywords.split('%20')\n            \ntrain['keyword']=train['keyword'].apply(lambda x : keywords_to_list(x))\ntest['keyword']=test['keyword'].apply(lambda x : keywords_to_list(x))\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:06.333967Z","iopub.execute_input":"2021-07-21T15:11:06.334534Z","iopub.status.idle":"2021-07-21T15:11:06.362641Z","shell.execute_reply.started":"2021-07-21T15:11:06.334489Z","shell.execute_reply":"2021-07-21T15:11:06.361674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. extract hashtags and create a new feature column**","metadata":{}},{"cell_type":"code","source":"import re\ntrain['hashtag'] = train['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\ntest['hashtag'] = test['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n\ntest.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:06.363939Z","iopub.execute_input":"2021-07-21T15:11:06.364224Z","iopub.status.idle":"2021-07-21T15:11:06.418296Z","shell.execute_reply.started":"2021-07-21T15:11:06.364196Z","shell.execute_reply":"2021-07-21T15:11:06.41721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. remove digits (01234), urls (http://...), mentions (@...) and hashtags (#...)**","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:05:52.812981Z","iopub.execute_input":"2021-07-10T20:05:52.813336Z","iopub.status.idle":"2021-07-10T20:05:52.818792Z","shell.execute_reply.started":"2021-07-10T20:05:52.813298Z","shell.execute_reply":"2021-07-10T20:05:52.817594Z"}}},{"cell_type":"code","source":"!pip install tweet-preprocessor\nimport preprocessor","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:06.420002Z","iopub.execute_input":"2021-07-21T15:11:06.42042Z","iopub.status.idle":"2021-07-21T15:11:15.298582Z","shell.execute_reply.started":"2021-07-21T15:11:06.420375Z","shell.execute_reply":"2021-07-21T15:11:15.297583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: preprocessor.clean(x))\ntest['text'] = test['text'].apply(lambda x: preprocessor.clean(x))\n\ndef clear_list(lista):\n    try:\n        for i,ele in enumerate(lista):\n            lista[i]=preprocessor.clean(ele)\n        return lista\n    except:\n        print(lista)\n\ntrain['hashtag'] = train['hashtag'].apply(lambda x: clear_list(x))\ntest['hashtag'] = test['hashtag'].apply(lambda x: clear_list(x))\n\ntrain['keyword'] = train['keyword'].apply(lambda x: clear_list(x))\ntest['keyword'] = test['keyword'].apply(lambda x: clear_list(x))\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:15.303188Z","iopub.execute_input":"2021-07-21T15:11:15.303521Z","iopub.status.idle":"2021-07-21T15:11:17.908554Z","shell.execute_reply.started":"2021-07-21T15:11:15.303477Z","shell.execute_reply":"2021-07-21T15:11:17.907626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. recover abbreviations (change they'll to they will, etc)**","metadata":{}},{"cell_type":"markdown","source":"I copied the code from the follow url by Yann Dubois <br>\nhttps://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python","metadata":{}},{"cell_type":"code","source":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntrain['text'] = train['text'].apply(lambda x: decontracted(x))\ntest['text'] = test['text'].apply(lambda x: decontracted(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:17.910125Z","iopub.execute_input":"2021-07-21T15:11:17.910394Z","iopub.status.idle":"2021-07-21T15:11:18.065836Z","shell.execute_reply.started":"2021-07-21T15:11:17.910369Z","shell.execute_reply":"2021-07-21T15:11:18.064723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. remove punctuations**","metadata":{"execution":{"iopub.status.busy":"2021-07-11T15:05:03.869944Z","iopub.execute_input":"2021-07-11T15:05:03.870473Z","iopub.status.idle":"2021-07-11T15:05:03.876345Z","shell.execute_reply.started":"2021-07-11T15:05:03.870424Z","shell.execute_reply":"2021-07-11T15:05:03.875037Z"}}},{"cell_type":"code","source":"import re\n\ndef remove_punc(lista):\n    for i,ele in enumerate(lista):\n        lista[i] = re.sub(r'[^\\w\\s]', '', ele)\n        lista[i] = re.sub('_', ' ', lista[i]) # the previous row doesn't remove underscore\n    return lista\n\ntrain['text']=train['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ntrain['text']=train['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\ntest['text']=test['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ntest['text']=test['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\n\ntrain['hashtag']=train['hashtag'].apply(lambda x: remove_punc(x))\ntest['hashtag']=test['hashtag'].apply(lambda x: remove_punc(x))\n\ntrain['keyword']=train['keyword'].apply(lambda x: remove_punc(x))\ntest['keyword']=test['keyword'].apply(lambda x: remove_punc(x))\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:18.06703Z","iopub.execute_input":"2021-07-21T15:11:18.067305Z","iopub.status.idle":"2021-07-21T15:11:18.234423Z","shell.execute_reply.started":"2021-07-21T15:11:18.06728Z","shell.execute_reply":"2021-07-21T15:11:18.233551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. tokenization**","metadata":{}},{"cell_type":"code","source":"import nltk\n\ntrain['text']=train['text'].apply(lambda x: nltk.word_tokenize(x))\ntest['text']=test['text'].apply(lambda x: nltk.word_tokenize(x))\n\ntest.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:18.235705Z","iopub.execute_input":"2021-07-21T15:11:18.235978Z","iopub.status.idle":"2021-07-21T15:11:21.933236Z","shell.execute_reply.started":"2021-07-21T15:11:18.235952Z","shell.execute_reply":"2021-07-21T15:11:21.932157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**7. remove stop words**","metadata":{"execution":{"iopub.status.busy":"2021-07-11T15:29:23.514033Z","iopub.execute_input":"2021-07-11T15:29:23.514407Z","iopub.status.idle":"2021-07-11T15:29:23.519772Z","shell.execute_reply.started":"2021-07-11T15:29:23.514362Z","shell.execute_reply":"2021-07-11T15:29:23.518454Z"}}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words=stopwords.words('english')\nstop_words.append('u') # 'i love u' is the semantically the same as 'i love you'\nstop_words.append('one') # want to remove numbers\nstop_words.append('two')\nstop_words.append('three')\nstop_words.append('four')\nstop_words.append('five')\nstop_words.append('six')\nstop_words.append('seven')\nstop_words.append('eight')\nstop_words.append('nine')\nstop_words.append('ten')\n\ndef remove_stop_words(lista):\n    pt=0 # don't use a for loop because len(lista) keeps changing as we remove stop words.\n    while pt<len(lista):\n        if lista[pt] in stop_words:\n            lista.remove(lista[pt])\n        else:\n            pt+=1\n    return lista\n\ntrain['text']=train['text'].apply(lambda x : remove_stop_words(x))\ntrain['hashtag']=train['hashtag'].apply(lambda x : remove_stop_words(x))\ntrain['keyword']=train['keyword'].apply(lambda x : remove_stop_words(x))\n\ntest['text']=test['text'].apply(lambda x : remove_stop_words(x))\ntest['hashtag']=test['hashtag'].apply(lambda x : remove_stop_words(x))\ntest['keyword']=test['keyword'].apply(lambda x : remove_stop_words(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:21.934969Z","iopub.execute_input":"2021-07-21T15:11:21.935563Z","iopub.status.idle":"2021-07-21T15:11:22.405585Z","shell.execute_reply.started":"2021-07-21T15:11:21.935517Z","shell.execute_reply":"2021-07-21T15:11:22.404512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**8. lemmatization. ('us' is lemmatized to 'u')** ","metadata":{"execution":{"iopub.status.busy":"2021-07-11T15:38:43.412772Z","iopub.execute_input":"2021-07-11T15:38:43.413299Z","iopub.status.idle":"2021-07-11T15:38:43.570107Z","shell.execute_reply.started":"2021-07-11T15:38:43.413264Z","shell.execute_reply":"2021-07-11T15:38:43.569281Z"}}},{"cell_type":"code","source":"from nltk import WordNetLemmatizer ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:22.407016Z","iopub.execute_input":"2021-07-21T15:11:22.407398Z","iopub.status.idle":"2021-07-21T15:11:22.41249Z","shell.execute_reply.started":"2021-07-21T15:11:22.407361Z","shell.execute_reply":"2021-07-21T15:11:22.411324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WordNetLemmatizer().lemmatize('us')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:22.413991Z","iopub.execute_input":"2021-07-21T15:11:22.414386Z","iopub.status.idle":"2021-07-21T15:11:24.509367Z","shell.execute_reply.started":"2021-07-21T15:11:22.414346Z","shell.execute_reply":"2021-07-21T15:11:24.508536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_list(lista):\n    for i, ele in enumerate(lista):\n        lista[i]=WordNetLemmatizer().lemmatize(ele)\n    return lista\n\ntrain['text']=train['text'].apply(lambda x : lemmatize_list(x))\ntrain['hashtag']=train['hashtag'].apply(lambda x : lemmatize_list(x))\ntrain['keyword']=train['keyword'].apply(lambda x : lemmatize_list(x))\n\ntest['text']=test['text'].apply(lambda x : lemmatize_list(x))\ntest['hashtag']=test['hashtag'].apply(lambda x : lemmatize_list(x))\ntest['keyword']=test['keyword'].apply(lambda x : lemmatize_list(x))\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:24.510541Z","iopub.execute_input":"2021-07-21T15:11:24.510845Z","iopub.status.idle":"2021-07-21T15:11:25.189421Z","shell.execute_reply.started":"2021-07-21T15:11:24.510817Z","shell.execute_reply":"2021-07-21T15:11:25.188385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**9. save the preprocessed files**","metadata":{}},{"cell_type":"code","source":"train.to_csv('preprocess_train.csv')\ntest.to_csv('preprocess_test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:25.190847Z","iopub.execute_input":"2021-07-21T15:11:25.191256Z","iopub.status.idle":"2021-07-21T15:11:25.307336Z","shell.execute_reply.started":"2021-07-21T15:11:25.191215Z","shell.execute_reply":"2021-07-21T15:11:25.306568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**10. find maximum tweet length, maximum hashtag length, maximum keywords length**","metadata":{}},{"cell_type":"code","source":"maxi=0\n\nfor ele in train['text']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['text']:\n    maxi=max(maxi,len(ele))\n    \nmaxi_text=maxi\nmaxi_text","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:25.308531Z","iopub.execute_input":"2021-07-21T15:11:25.309089Z","iopub.status.idle":"2021-07-21T15:11:25.325824Z","shell.execute_reply.started":"2021-07-21T15:11:25.309048Z","shell.execute_reply":"2021-07-21T15:11:25.324795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxi=0\n\nfor ele in train['hashtag']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['hashtag']:\n    maxi=max(maxi,len(ele))\n    \nmaxi\nmaxi_hashtag=maxi\nmaxi_hashtag","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:25.327354Z","iopub.execute_input":"2021-07-21T15:11:25.327983Z","iopub.status.idle":"2021-07-21T15:11:25.347017Z","shell.execute_reply.started":"2021-07-21T15:11:25.32793Z","shell.execute_reply":"2021-07-21T15:11:25.345738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxi=0\n\nfor ele in train['keyword']:\n    maxi=max(maxi,len(ele))\n    \nfor ele in test['keyword']:\n    maxi=max(maxi,len(ele))\n    \nmaxi_keyword=maxi\nmaxi_keyword","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:25.34861Z","iopub.execute_input":"2021-07-21T15:11:25.349035Z","iopub.status.idle":"2021-07-21T15:11:25.371866Z","shell.execute_reply.started":"2021-07-21T15:11:25.349003Z","shell.execute_reply":"2021-07-21T15:11:25.370766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**11. install and understand word embedding**","metadata":{"execution":{"iopub.status.busy":"2021-07-12T01:44:43.713185Z","iopub.execute_input":"2021-07-12T01:44:43.713788Z","iopub.status.idle":"2021-07-12T01:44:43.719503Z","shell.execute_reply.started":"2021-07-12T01:44:43.713751Z","shell.execute_reply":"2021-07-12T01:44:43.718072Z"}}},{"cell_type":"code","source":"!pip3 install spacy\n!python3 -m spacy download en_core_web_lg","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:11:25.375075Z","iopub.execute_input":"2021-07-21T15:11:25.375352Z","iopub.status.idle":"2021-07-21T15:12:21.050758Z","shell.execute_reply.started":"2021-07-21T15:11:25.375326Z","shell.execute_reply":"2021-07-21T15:12:21.049536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:21.057075Z","iopub.execute_input":"2021-07-21T15:12:21.057366Z","iopub.status.idle":"2021-07-21T15:12:30.679812Z","shell.execute_reply.started":"2021-07-21T15:12:21.057324Z","shell.execute_reply":"2021-07-21T15:12:30.67878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef distance(vec1,vec2):\n    sum=0\n    for i in range(len(vec1)):\n        sum+=(vec1[i]-vec2[i])**2\n    return math.sqrt(sum)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:30.681046Z","iopub.execute_input":"2021-07-21T15:12:30.681523Z","iopub.status.idle":"2021-07-21T15:12:30.685829Z","shell.execute_reply.started":"2021-07-21T15:12:30.681475Z","shell.execute_reply":"2021-07-21T15:12:30.685112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(\"father grandfather\") # change your words here ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:30.686967Z","iopub.execute_input":"2021-07-21T15:12:30.687434Z","iopub.status.idle":"2021-07-21T15:12:30.724796Z","shell.execute_reply.started":"2021-07-21T15:12:30.687394Z","shell.execute_reply":"2021-07-21T15:12:30.724029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc[0].vector.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:30.7259Z","iopub.execute_input":"2021-07-21T15:12:30.726347Z","iopub.status.idle":"2021-07-21T15:12:30.731976Z","shell.execute_reply.started":"2021-07-21T15:12:30.726291Z","shell.execute_reply":"2021-07-21T15:12:30.730997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(distance(doc[0].vector, doc[1].vector)) # the smaller the more similar\nprint(doc[0].similarity(doc[1])) # the larger the more similar","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:30.733393Z","iopub.execute_input":"2021-07-21T15:12:30.733999Z","iopub.status.idle":"2021-07-21T15:12:30.747279Z","shell.execute_reply.started":"2021-07-21T15:12:30.733958Z","shell.execute_reply":"2021-07-21T15:12:30.746253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**12. use word embedding and create training set, the training set has dimension (m,23+13+2,300)**","metadata":{}},{"cell_type":"code","source":"m=train.shape[0]\nstore_train=np.zeros((m,23+13+2,300))\nfor i in range(m): # m\n    if i % 100 == 99:\n        print(i)\n    for j in range(len(train['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n        store_train[i,j,:]=nlp(train['text'][i][j])[0].vector\n    for j in range(len(train['hashtag'][i])):\n        try:\n            store_train[i,23+j,:]=nlp(train['hashtag'][i][j])[0].vector\n        except:\n            store_train[i,23+j,:]=nlp(train['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n    for j in range(len(train['keyword'][i])):\n        store_train[i,36+j,:]=nlp(train['keyword'][i][j])[0].vector","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:12:30.748822Z","iopub.execute_input":"2021-07-21T15:12:30.749179Z","iopub.status.idle":"2021-07-21T15:21:42.18449Z","shell.execute_reply.started":"2021-07-21T15:12:30.749149Z","shell.execute_reply":"2021-07-21T15:21:42.182714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('store_train.npy',store_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:21:42.188801Z","iopub.execute_input":"2021-07-21T15:21:42.189387Z","iopub.status.idle":"2021-07-21T15:21:43.565783Z","shell.execute_reply.started":"2021-07-21T15:21:42.189331Z","shell.execute_reply":"2021-07-21T15:21:43.564739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m=test.shape[0]\nstore_test=np.zeros((m,23+13+2,300))\nfor i in range(m): # m\n    if i % 100 == 99:\n        print(i)\n    for j in range(len(test['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n        store_test[i,j,:]=nlp(test['text'][i][j])[0].vector\n    for j in range(len(test['hashtag'][i])):\n        try:\n            store_test[i,23+j,:]=nlp(test['hashtag'][i][j])[0].vector\n        except:\n            store_test[i,23+j,:]=nlp(test['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n    for j in range(len(test['keyword'][i])):\n        store_test[i,36+j,:]=nlp(test['keyword'][i][j])[0].vector","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:21:43.567292Z","iopub.execute_input":"2021-07-21T15:21:43.5677Z","iopub.status.idle":"2021-07-21T15:25:40.636626Z","shell.execute_reply.started":"2021-07-21T15:21:43.567665Z","shell.execute_reply":"2021-07-21T15:25:40.635294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('store_test.npy',store_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:25:40.638214Z","iopub.execute_input":"2021-07-21T15:25:40.638589Z","iopub.status.idle":"2021-07-21T15:25:41.231122Z","shell.execute_reply.started":"2021-07-21T15:25:40.638555Z","shell.execute_reply":"2021-07-21T15:25:41.230104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**13. slice the train and test sets**","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:16:08.117274Z","iopub.execute_input":"2021-07-20T17:16:08.117703Z","iopub.status.idle":"2021-07-20T17:16:15.139526Z","shell.execute_reply.started":"2021-07-20T17:16:08.117665Z","shell.execute_reply":"2021-07-20T17:16:15.138234Z"}}},{"cell_type":"code","source":"store_train_text=store_train[:,:maxi_text,:]\nstore_train_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\nstore_train_keyword=store_train[:,-maxi_keyword:,:]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:25:41.242093Z","iopub.execute_input":"2021-07-21T15:25:41.242403Z","iopub.status.idle":"2021-07-21T15:25:41.253616Z","shell.execute_reply.started":"2021-07-21T15:25:41.242371Z","shell.execute_reply":"2021-07-21T15:25:41.252696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(store_train_text.shape)\nprint(store_train_hashtag.shape)\nprint(store_train_keyword.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:25:41.255013Z","iopub.execute_input":"2021-07-21T15:25:41.255311Z","iopub.status.idle":"2021-07-21T15:25:41.267942Z","shell.execute_reply.started":"2021-07-21T15:25:41.255283Z","shell.execute_reply":"2021-07-21T15:25:41.26696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_test_text=store_train[:,:maxi_text,:]\nstore_test_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\nstore_test_keyword=store_train[:,-maxi_keyword:,:]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:25:41.269185Z","iopub.execute_input":"2021-07-21T15:25:41.269499Z","iopub.status.idle":"2021-07-21T15:25:41.281843Z","shell.execute_reply.started":"2021-07-21T15:25:41.269445Z","shell.execute_reply":"2021-07-21T15:25:41.280746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**14. build the model**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras.backend as K\nfrom keras.layers import Input, Dropout, GRU, BatchNormalization, TimeDistributed, Reshape, Dense, Conv1D, Concatenate\nfrom keras import Model\nimport keras","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:25:41.283469Z","iopub.execute_input":"2021-07-21T15:25:41.283922Z","iopub.status.idle":"2021-07-21T15:25:47.774797Z","shell.execute_reply.started":"2021-07-21T15:25:41.283874Z","shell.execute_reply":"2021-07-21T15:25:47.773754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_text=Input(shape=(store_train_text.shape[1],store_train_text.shape[2]))\n# input_hashtag=Input(shape=(store_train_hashtag.shape[1],store_train_hashtag.shape[2]))\n# input_keyword=Input(shape=(store_train_keyword.shape[1],store_train_keyword.shape[2]))\n\n# mid1=GRU(units=128, return_sequences=True)(input_text)\n# mid1=Dropout(0.8)(mid1)\n# mid1=BatchNormalization()(mid1)  \n\n# mid1=GRU(units=16, return_sequences=True)(mid1)\n# mid1=Dropout(0.8)(mid1)\n# mid1=BatchNormalization()(mid1)  \n\n# mid1=GRU(units=1, return_sequences=False)(mid1)\n# #mid1=Dropout(0.8)(mid1)\n# #mid1=BatchNormalization()(mid1)\n# print(mid1.shape)\n\n# # mid1=Dropout(0.8)(mid1)\n# # mid1=TimeDistributed(Dense(1, activation = \"relu\"))(mid1)\n# # mid1=Reshape((mid1.shape[1],))(mid1)\n# # # mid1 has shape (m,23)\n\n# mid2=TimeDistributed(Dense(128, activation = \"relu\"))(input_hashtag)\n# # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n# print(mid2.shape)\n# mid2=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid2)\n# mid2=Reshape((mid2.shape[1],))(mid2)\n# # now mid2 has shape (m,13)\n# #print(mid2.shape)\n\n# # mid3=Conv1D(30, kernel_size=1, strides=1, padding='valid')(input_keyword)\n# # # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n# # mid3=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid3)\n# # mid3=Reshape((mid3.shape[1],))(mid3)\n\n# # mid=Concatenate(axis=-1)([mid1,mid2,mid3])\n# # #print(mid1.shape,mid2.shape,mid3.shape)\n# # output=Dense(2, activation=\"softmax\")(mid)\n# # #print(output.shape)\n\n# # model=Model(inputs=[input_text,input_hashtag, input_keyword], outputs=outputs)\n# # model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T15:40:34.319674Z","iopub.execute_input":"2021-07-21T15:40:34.320128Z","iopub.status.idle":"2021-07-21T15:40:34.325859Z","shell.execute_reply.started":"2021-07-21T15:40:34.320089Z","shell.execute_reply":"2021-07-21T15:40:34.324596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp=Input(shape=(store_train.shape[1],store_train_text.shape[2]))\n\nmid=GRU(units=300, return_sequences=True)(inp)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=GRU(units=300, return_sequences=True)(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=GRU(units=300, return_sequences=True)(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid)  \n\nmid=Dropout(0.6)(mid)\nmid=TimeDistributed(Dense(1,activation='relu'))(mid)\nmid=Reshape((mid.shape[1],))(mid)\nmid=Dropout(0.6)(mid)\nmid=BatchNormalization()(mid) \noutp=Dense(2,activation='softmax')(mid)\n\n\nmodel=Model(inputs=inp, outputs=outp) ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T21:44:33.343821Z","iopub.execute_input":"2021-07-21T21:44:33.3446Z","iopub.status.idle":"2021-07-21T21:44:35.74987Z","shell.execute_reply.started":"2021-07-21T21:44:33.344561Z","shell.execute_reply":"2021-07-21T21:44:35.748982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**15. create labels for the training sets**","metadata":{}},{"cell_type":"markdown","source":"understand  np.random.shuffle and np.random.seed","metadata":{}},{"cell_type":"code","source":"np.random.seed(3)\nlis1=np.array([[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9]])\nlis2=np.array([1,2,3,4,5,6,7,8,9])\nnp.random.shuffle(lis1)\nnp.random.seed(3)\nnp.random.shuffle(lis2)\nprint(lis1)\nprint(lis2)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T16:41:37.257823Z","iopub.execute_input":"2021-07-21T16:41:37.258416Z","iopub.status.idle":"2021-07-21T16:41:37.268474Z","shell.execute_reply.started":"2021-07-21T16:41:37.25837Z","shell.execute_reply":"2021-07-21T16:41:37.267663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_train=np.load('store_train.npy')\n\nm=store_train.shape[0]\ntrain_Y=np.zeros((m,2))\nfor i in range(m):\n    train_Y[i,train.iloc[i]['target']]=1","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:34:04.876829Z","iopub.execute_input":"2021-07-21T17:34:04.877209Z","iopub.status.idle":"2021-07-21T17:34:06.2674Z","shell.execute_reply.started":"2021-07-21T17:34:04.877178Z","shell.execute_reply":"2021-07-21T17:34:06.266317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sed=13\nnp.random.seed(sed)\nnp.random.shuffle(store_train)\nnp.random.seed(sed)\nnp.random.shuffle(train_Y)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:34:07.773814Z","iopub.execute_input":"2021-07-21T17:34:07.774201Z","iopub.status.idle":"2021-07-21T17:34:07.960655Z","shell.execute_reply.started":"2021-07-21T17:34:07.774165Z","shell.execute_reply":"2021-07-21T17:34:07.959584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T21:44:46.955624Z","iopub.execute_input":"2021-07-21T21:44:46.956176Z","iopub.status.idle":"2021-07-21T21:44:46.97044Z","shell.execute_reply.started":"2021-07-21T21:44:46.956141Z","shell.execute_reply":"2021-07-21T21:44:46.968953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(store_train[0:-500,:,:], train_Y[0:-500,:], batch_size=64, epochs=50, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T21:44:57.946Z","iopub.execute_input":"2021-07-21T21:44:57.946372Z","iopub.status.idle":"2021-07-21T22:26:30.620063Z","shell.execute_reply.started":"2021-07-21T21:44:57.946341Z","shell.execute_reply":"2021-07-21T22:26:30.618688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**16. evaluate the cross validation set**","metadata":{}},{"cell_type":"code","source":"model.evaluate(store_train[-500:,:,:], train_Y[-500:,:])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T22:26:37.809395Z","iopub.execute_input":"2021-07-21T22:26:37.809886Z","iopub.status.idle":"2021-07-21T22:26:40.712392Z","shell.execute_reply.started":"2021-07-21T22:26:37.809847Z","shell.execute_reply":"2021-07-21T22:26:40.711655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**17. predict the test set**","metadata":{}},{"cell_type":"code","source":"test_Y=model.predict(store_test)\n\ntest_label=[]\n\nfor i in range(test_Y.shape[0]):\n    if test_Y[i,1]>=0.5:\n        test_label.append(1)\n    else:\n        test_label.append(0)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T22:26:43.339942Z","iopub.execute_input":"2021-07-21T22:26:43.340623Z","iopub.status.idle":"2021-07-21T22:26:54.831921Z","shell.execute_reply.started":"2021-07-21T22:26:43.340581Z","shell.execute_reply":"2021-07-21T22:26:54.830794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**18. submit**","metadata":{}},{"cell_type":"code","source":"submission=pd.DataFrame({'id': test['id'], 'target':test_label})\nprint(submission.head(10))\n\nfilename = 'submission_nlp_tweets.csv'\n\nsubmission.to_csv(filename,index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T22:26:54.833667Z","iopub.execute_input":"2021-07-21T22:26:54.834001Z","iopub.status.idle":"2021-07-21T22:26:54.857795Z","shell.execute_reply.started":"2021-07-21T22:26:54.83397Z","shell.execute_reply":"2021-07-21T22:26:54.856492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}