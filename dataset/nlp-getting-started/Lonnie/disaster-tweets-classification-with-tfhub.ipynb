{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Disaster Tweets Classification with TFHub","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:45:58.547423Z","iopub.execute_input":"2021-08-12T09:45:58.547822Z","iopub.status.idle":"2021-08-12T09:45:58.554441Z","shell.execute_reply.started":"2021-08-12T09:45:58.547789Z","shell.execute_reply":"2021-08-12T09:45:58.553213Z"}}},{"cell_type":"markdown","source":"## Table of Contents\n- Overview\n- Import Packages and Datasets\n- Data Wrangling\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n- Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Overview\nIn this notebook I will build a Text Classifier to read tweets dataset to predict Tweets Disaster.","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:25:03.395939Z","iopub.execute_input":"2021-07-30T17:25:03.396519Z","iopub.status.idle":"2021-07-30T17:25:03.400557Z","shell.execute_reply.started":"2021-07-30T17:25:03.396471Z","shell.execute_reply":"2021-07-30T17:25:03.39968Z"}}},{"cell_type":"markdown","source":"## Import Packages and Datasets ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport time\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow_hub as hub","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:10:49.649997Z","iopub.execute_input":"2022-04-24T03:10:49.650316Z","iopub.status.idle":"2022-04-24T03:10:54.760049Z","shell.execute_reply.started":"2022-04-24T03:10:49.650246Z","shell.execute_reply":"2022-04-24T03:10:54.759038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:10:54.762267Z","iopub.execute_input":"2022-04-24T03:10:54.762872Z","iopub.status.idle":"2022-04-24T03:10:54.825051Z","shell.execute_reply.started":"2022-04-24T03:10:54.762832Z","shell.execute_reply":"2022-04-24T03:10:54.824061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:03.849699Z","iopub.execute_input":"2022-04-24T03:12:03.850098Z","iopub.status.idle":"2022-04-24T03:12:03.86109Z","shell.execute_reply.started":"2022-04-24T03:12:03.850064Z","shell.execute_reply":"2022-04-24T03:12:03.860131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:05.738058Z","iopub.execute_input":"2022-04-24T03:12:05.73843Z","iopub.status.idle":"2022-04-24T03:12:05.76918Z","shell.execute_reply.started":"2022-04-24T03:12:05.738399Z","shell.execute_reply":"2022-04-24T03:12:05.768443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.location.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:07.937896Z","iopub.execute_input":"2022-04-24T03:12:07.938236Z","iopub.status.idle":"2022-04-24T03:12:07.953172Z","shell.execute_reply.started":"2022-04-24T03:12:07.938205Z","shell.execute_reply":"2022-04-24T03:12:07.952323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Wrangling\nLet's see null values for each column.","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:10.681115Z","iopub.execute_input":"2022-04-24T03:12:10.681465Z","iopub.status.idle":"2022-04-24T03:12:10.69091Z","shell.execute_reply.started":"2022-04-24T03:12:10.681434Z","shell.execute_reply":"2022-04-24T03:12:10.690023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:12.353327Z","iopub.execute_input":"2022-04-24T03:12:12.353689Z","iopub.status.idle":"2022-04-24T03:12:12.364586Z","shell.execute_reply.started":"2022-04-24T03:12:12.353657Z","shell.execute_reply":"2022-04-24T03:12:12.363474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntrain[\"location\"].replace(np.NAN, \"\", inplace=True)\ntest[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntest[\"location\"].replace(np.NAN, \"\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:25.191971Z","iopub.execute_input":"2022-04-24T03:12:25.192317Z","iopub.status.idle":"2022-04-24T03:12:25.203449Z","shell.execute_reply.started":"2022-04-24T03:12:25.192287Z","shell.execute_reply":"2022-04-24T03:12:25.202082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:27.687831Z","iopub.execute_input":"2022-04-24T03:12:27.688139Z","iopub.status.idle":"2022-04-24T03:12:27.698192Z","shell.execute_reply.started":"2022-04-24T03:12:27.68811Z","shell.execute_reply":"2022-04-24T03:12:27.697245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:29.689359Z","iopub.execute_input":"2022-04-24T03:12:29.689674Z","iopub.status.idle":"2022-04-24T03:12:29.698879Z","shell.execute_reply.started":"2022-04-24T03:12:29.689646Z","shell.execute_reply":"2022-04-24T03:12:29.697837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:38:49.025447Z","iopub.execute_input":"2021-07-30T17:38:49.027434Z","iopub.status.idle":"2021-07-30T17:38:49.031233Z","shell.execute_reply.started":"2021-07-30T17:38:49.027392Z","shell.execute_reply":"2021-07-30T17:38:49.029903Z"}}},{"cell_type":"code","source":"contents = []\nfor data in [train, test]:\n    for i in range(data.shape[0]):\n        item = data.iloc[i]\n        sentence = item[\"keyword\"] + \" \" + item[\"text\"] + \" \" + item[\"location\"]\n        sentence = sentence.strip().lower()\n        contents.append(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:12:32.61339Z","iopub.execute_input":"2022-04-24T03:12:32.613714Z","iopub.status.idle":"2022-04-24T03:12:34.04798Z","shell.execute_reply.started":"2022-04-24T03:12:32.613685Z","shell.execute_reply":"2022-04-24T03:12:34.047019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = contents[:len(train)]\nx_test = contents[len(train):]\ny_train = train[\"target\"]\nprint(len(x_train), len(x_test), y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:13:50.351063Z","iopub.execute_input":"2022-04-24T03:13:50.351407Z","iopub.status.idle":"2022-04-24T03:13:50.357414Z","shell.execute_reply.started":"2022-04-24T03:13:50.351376Z","shell.execute_reply":"2022-04-24T03:13:50.356243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=44)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:13:53.424346Z","iopub.execute_input":"2022-04-24T03:13:53.424662Z","iopub.status.idle":"2022-04-24T03:13:53.445561Z","shell.execute_reply.started":"2022-04-24T03:13:53.424631Z","shell.execute_reply":"2022-04-24T03:13:53.444727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(x_train), len(y_train), len(x_val), len(y_val))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:13:56.307492Z","iopub.execute_input":"2022-04-24T03:13:56.307803Z","iopub.status.idle":"2022-04-24T03:13:56.314777Z","shell.execute_reply.started":"2022-04-24T03:13:56.307773Z","shell.execute_reply":"2022-04-24T03:13:56.31344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:13:58.697597Z","iopub.execute_input":"2022-04-24T03:13:58.697933Z","iopub.status.idle":"2022-04-24T03:13:58.706515Z","shell.execute_reply.started":"2022-04-24T03:13:58.697902Z","shell.execute_reply":"2022-04-24T03:13:58.70544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nkeras_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input((), dtype=tf.string),\n    keras_layer,\n    tf.keras.layers.Reshape((1, -1)),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation=\"swish\"),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(32, activation=\"swish\"),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:21:39.351119Z","iopub.execute_input":"2022-04-24T03:21:39.35148Z","iopub.status.idle":"2022-04-24T03:21:41.108343Z","shell.execute_reply.started":"2022-04-24T03:21:39.35145Z","shell.execute_reply":"2022-04-24T03:21:41.107522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:21:45.221642Z","iopub.execute_input":"2022-04-24T03:21:45.221955Z","iopub.status.idle":"2022-04-24T03:21:45.262735Z","shell.execute_reply.started":"2022-04-24T03:21:45.221927Z","shell.execute_reply":"2022-04-24T03:21:45.261968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\nhistory = {\n    \"train_loss\": [],\n    \"valid_loss\": [],\n    \"train_accuracy\": [],\n    \"valid_accuracy\": []\n}\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    begin_time = time.time()\n    train_losses = []\n    valid_losses = []\n    correct_count = 0\n    total_count = 0\n    total_train_count = 0\n    for (x_batch, y_true) in train_dataset:\n        with tf.GradientTape() as tape:\n            y_pred = model(x_batch)\n            predict_labels = tf.cast(y_pred > 0.5, dtype=y_true.dtype)\n            loss_value = loss_object(y_true, y_pred)\n        gradients = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n        train_losses.append(loss_value)\n        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int32))\n        total_train_count += y_true.shape[0]\n    train_loss = tf.reduce_mean(train_losses)\n    train_accuracy = correct_count / total_train_count\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_accuracy\"].append(train_accuracy)\n    correct_count = 0\n    total_count = 0\n    total_valid_count = 0\n    for (x_batch, y_true) in val_dataset:\n        y_pred = model(x_batch)\n        predict_labels = tf.cast(y_pred > 0.5, dtype=y_true.dtype)\n        loss_value = loss_object(y_true, y_pred)\n        valid_losses.append(loss_value)\n        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int32))\n        total_valid_count += y_true.shape[0]\n    valid_loss = tf.reduce_mean(valid_losses)\n    valid_accuracy = correct_count / total_valid_count\n    history[\"valid_loss\"].append(valid_loss)\n    history[\"valid_accuracy\"].append(valid_accuracy)\n    elapsed_time = time.time() -  begin_time\n    print(\"Epoch: %d / %d\"%(epoch + 1, num_epochs))\n    print(\"%.2fs Loss: %.2f Accuracy: %.2f Validation Loss: %.2f Validation Accuracy: %.2f\"%(elapsed_time, train_loss, train_accuracy, valid_loss, valid_accuracy))\nfor key in history:\n    history[key] = list(np.array(history[key]))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:22:07.965707Z","iopub.execute_input":"2022-04-24T03:22:07.966111Z","iopub.status.idle":"2022-04-24T03:22:56.693475Z","shell.execute_reply.started":"2022-04-24T03:22:07.966068Z","shell.execute_reply":"2022-04-24T03:22:56.692632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"**Loss and Accuracy over time**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(history).plot(kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:25:00.445878Z","iopub.execute_input":"2022-04-24T03:25:00.44623Z","iopub.status.idle":"2022-04-24T03:25:00.657362Z","shell.execute_reply.started":"2022-04-24T03:25:00.446196Z","shell.execute_reply":"2022-04-24T03:25:00.656422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.array(model.predict(x_val) >= 0.5, dtype=int).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:25:35.471868Z","iopub.execute_input":"2022-04-24T03:25:35.47221Z","iopub.status.idle":"2022-04-24T03:25:35.645712Z","shell.execute_reply.started":"2022-04-24T03:25:35.472181Z","shell.execute_reply":"2022-04-24T03:25:35.64484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nsns.heatmap(cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:25:41.580556Z","iopub.execute_input":"2022-04-24T03:25:41.580866Z","iopub.status.idle":"2022-04-24T03:25:41.585001Z","shell.execute_reply.started":"2022-04-24T03:25:41.580837Z","shell.execute_reply":"2022-04-24T03:25:41.583933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy Score\", accuracy_score(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:26:31.198993Z","iopub.execute_input":"2022-04-24T03:26:31.199348Z","iopub.status.idle":"2022-04-24T03:26:31.205963Z","shell.execute_reply.started":"2022-04-24T03:26:31.199318Z","shell.execute_reply":"2022-04-24T03:26:31.205087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(\"Classification Report\", classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:27:07.22518Z","iopub.execute_input":"2022-04-24T03:27:07.225612Z","iopub.status.idle":"2022-04-24T03:27:07.239508Z","shell.execute_reply.started":"2022-04-24T03:27:07.225571Z","shell.execute_reply":"2022-04-24T03:27:07.238484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"y_test = np.array(model.predict(x_test) > 0.5, dtype=np.int).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:27:47.464565Z","iopub.execute_input":"2022-04-24T03:27:47.464887Z","iopub.status.idle":"2022-04-24T03:27:47.751376Z","shell.execute_reply.started":"2022-04-24T03:27:47.464858Z","shell.execute_reply":"2022-04-24T03:27:47.750395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"id\": test[\"id\"], \"target\": y_test})","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:27:50.269035Z","iopub.execute_input":"2022-04-24T03:27:50.269481Z","iopub.status.idle":"2022-04-24T03:27:50.276795Z","shell.execute_reply.started":"2022-04-24T03:27:50.269441Z","shell.execute_reply":"2022-04-24T03:27:50.27556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:27:52.39467Z","iopub.execute_input":"2022-04-24T03:27:52.394991Z","iopub.status.idle":"2022-04-24T03:27:52.404175Z","shell.execute_reply.started":"2022-04-24T03:27:52.394962Z","shell.execute_reply":"2022-04-24T03:27:52.40329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:27:54.796618Z","iopub.execute_input":"2022-04-24T03:27:54.796968Z","iopub.status.idle":"2022-04-24T03:27:54.812073Z","shell.execute_reply.started":"2022-04-24T03:27:54.796936Z","shell.execute_reply":"2022-04-24T03:27:54.811221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nNow the Model can achive 78% accuracy both in validation dataset and test dataset which shown in Kaggle leader board. There's still a lot to improve.\n","metadata":{}}]}