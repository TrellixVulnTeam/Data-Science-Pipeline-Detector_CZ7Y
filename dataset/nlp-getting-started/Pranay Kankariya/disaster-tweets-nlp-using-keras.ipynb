{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndata = data.drop(['id','location'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stopwords\nstop = (stopwords.words('english'))\npunctuation = list(string.punctuation)\nfor i in punctuation:\n    stop.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning Data\n\nstemmer = SnowballStemmer('english',ignore_stopwords=True)\nlemmatizer = WordNetLemmatizer()\n\ndef remove_stopwords(text):\n    sentences = []\n    for word in text.split():\n        if word.lower().strip() not in stop and len(word)>3:\n            word = lemmatizer.lemmatize(word)\n            sentences.append(word.lower().strip())\n    return \" \".join(sentences)\n\ndef remove_punctuations(text):\n    punc = re.compile(r'[%s]'%string.punctuation)\n    return punc.sub(r'',text)\n                      \ndef remove_urls(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_squarebrackets(text):\n    square = re.compile(r'\\[.*?\\]')\n    return square.sub(r'',text)\n\ndef remove_tags(text):\n    tags = re.compile(r'<.*?>')\n    return tags.sub(r'',text)\n    \ndef remove_numbers(text):\n    num = re.compile(r'\\w*\\d\\w*')\n    return num.sub(r'',text)\n    \ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'',text)\n\ndef clean(text):\n    text = remove_punctuations(text)\n    text = remove_urls(text)\n    text = remove_tags(text)\n    text = remove_squarebrackets(text)\n    text = remove_emoji(text)\n    text = remove_numbers(text)\n    text = remove_stopwords(text)\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x:clean(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count of tweets\nsns.countplot(data['target'],palette='RdBu_r')\nplt.title(\"Non-Disaster vs Disaster Tweets\")\nprint(\"No of Non-Disaster Tweets: \" ,data['target'].value_counts()[0])\nprint(\"No of Disaster Tweets: \" ,data['target'].value_counts()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wordcloud\nfig,ax = plt.subplots(figsize=(12,16))\nplt.axis('off')\n\nplt.subplot(2,1,1)\ntext = \" \".join(data[data['target']==0]['text'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Non-Disaster Tweet\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(2,1,2)\ntext = \" \".join(data[data['target']==1]['text'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Disaster Tweet\")\nplt.axis('off')\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No Of Characters In A Tweet\nfig,ax = plt.subplots(figsize=(12,6))\nfig.suptitle(\"NO OF CHARACTERS IN A TWEET\")\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Disaster Tweets\")\nwords = data[data['target']==0]['text'].str.len()\nsns.distplot(words,kde=True)\n\nplt.subplot(1,2,2)\nplt.title(\"Disaster Tweets\")\nwords = data[data['target']==1]['text'].str.len()\nsns.distplot(words,kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Average Word Length In A Tweet\nfig,ax = plt.subplots(figsize=(12,6))\nfig.suptitle(\"AVERAGE WORD LENGTH IN A Tweet\")\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Disaster Tweets\")\nword_length = data[data['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word_length.map(lambda x:np.mean(x)),kde=True)\n\nplt.subplot(1,2,2)\nplt.title(\"Disaster Tweets\")\nword_length = data[data['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word_length.map(lambda x:np.mean(x)),kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keywords\nplt.figure(figsize=(14,6))\nsns.barplot(x=data['keyword'].value_counts()[:20],y=data['keyword'].value_counts()[:20].index,palette='RdBu_r')\nplt.xlabel(\"Count\")\nplt.ylabel(\"Keyword\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data\nx_train,x_test,y_train,y_test = train_test_split(data['text'],data['target'],test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizer\nvocab_size=10000\nembedding_dim=200\nmax_length=100\ntrunc_type=\"post\"\npad_type=\"post\"\noov_tok=\"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(list(x_train)+list(x_test))\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\ntrain_padded = pad_sequences(train_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)\n\ntest_sequences = tokenizer.texts_to_sequences(x_test)\ntest_padded = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"words =\", len(word_index))\nprint(\"train =\",len(train_padded))\nprint(\"test =\",len(test_padded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GloVe Embeddings\nembeddings_index={}\nwith open(\"../input/glove6b/glove.6B.200d.txt\",'r',encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nembeddings_matrix = np.zeros((len(word_index)+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neural Network\nmodel = Sequential()\n\nmodel.add(Embedding(len(word_index)+1,embedding_dim,input_length=max_length,weights=[embeddings_matrix]))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(Bidirectional(LSTM(128,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\nearlystop = EarlyStopping(monitor='val_loss',patience=3,verbose=1)\nlearning_reduce = ReduceLROnPlateau(patience=2,monitor=\"val_acc\",verbose=1,min_lr=0.00001,factor=0.5,cooldown=1)\ncallbacks = [earlystop,learning_reduce]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch=10\nhistory = model.fit(train_padded,y_train,epochs=epoch,validation_data=(test_padded,y_test),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(test_padded)\nprint(\"Accuracy: \",accuracy_score(y_test,y_pred).round(3))\nprint(\"Precision: \",precision_score(y_test,y_pred).round(3))\nprint(\"Recall: \",recall_score(y_test,y_pred).round(3))\nprint(\"F1-Score: \",f1_score(y_test,y_pred).round(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\ncm = pd.DataFrame(cm , index = ['Non-Disaster','Disaster'] , columns = ['Non-Disaster','Disaster'])\nsns.heatmap(cm,cmap= \"Blues\",annot=True,fmt='')\nplt.title(\"Confusion Matrix\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest['text'] = test['text'].apply(lambda x:clean(x))\n\ntesting_sequences = tokenizer.texts_to_sequences(test['text'])\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)\n\npredictions = model.predict(testing_padded)\n\nsubmission['target'] = (predictions>0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}