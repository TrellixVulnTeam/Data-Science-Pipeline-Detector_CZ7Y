{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Chuanyang Jin, Alex Yan","metadata":{}},{"cell_type":"markdown","source":"The code below is for practicing different methods, which includes:\nPreprocessing (filtering out meaningless messages and stop words);\n3 ways of data transformation (Token Count, TF-IDF, BERT);\n6 ways of training (Logistic Regression, SVM, Neural Networks, Naive Bayes, Random Forest, Gradient Boosting) with different regularizations, kernels, structures, etc.;\nand an ensemble model.","metadata":{}},{"cell_type":"markdown","source":"## File Loading","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None  # removing annoying warnings\nimport re\nimport seaborn as sns   \nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')  # disable warnings\n\nfrom sklearn import feature_extraction\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n\n# Reading in the dataset\ntrain_df = pd.read_csv(\"train.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We first clean up the meaningless words from the samples to reduce feature number & prevent overfitting.","metadata":{}},{"cell_type":"code","source":"# Clean up the text data\ndef clean_text(texts):\n    # convert into lower letters to filter out repetition in features\n    texts[:] = [text.lower() for text in texts]          \n\n    # http:// ... / word  or  http:// ... / word                 \n    texts[:] = [re.sub(r'https?:\\/\\/.*\\/\\w*', 'URL', text) for text in texts]   \n\n    # meaningless things like mentioning a friend (e.g. @username)\n    texts[:] = [re.sub(r'@\\w+([-.]\\w+)*', '', text) for text in texts]          \n\n    # meaningless symbols followed by &, such as '&amp', which is a meaningless web \n    texts[:] = [re.sub(r'&\\w+([-.]\\w+)*', '', text) for text in texts]\n\n\n\nclean_text(train_df['text'])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation 1\nUse sklearn's built in feature extraction to create a sparse martrix where every word appeared in the dataset becomes a feature.","metadata":{}},{"cell_type":"code","source":"# Using sklearn.feature_extraction.text.CountVectorizer()\n# This converts a collection of text documents to a matrix of token counts,\n# and produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words='english')\nX_trainval = count_vectorizer.fit_transform(train_df[\"text\"])\nfeature_transform = 'Token Count'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation 2\nUse another sklearn feature extraction method to apply tf-idf transformation to the feature vector.","metadata":{}},{"cell_type":"code","source":"# Using sklearn.feature_extraction.text.TfidfVectorizer()\n# This converts a collection of text documents to a matrix of token counts, but adds tf-idf\n# and produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n\ntfidf_vectorizer = feature_extraction.text.TfidfVectorizer(stop_words='english')\nX_trainval = tfidf_vectorizer.fit_transform(train_df[\"text\"])\nfeature_transform = 'TF-IDF'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation 3\nWe can also apply BERT pre-training to our feature to reduce feature number.","metadata":{}},{"cell_type":"code","source":"# Using a pre-trained BERT model\n# It produces 384 features.\n\nbert_model = SentenceTransformer('all-MiniLM-L12-v1')\nX_trainval = bert_model.encode(train_df[\"text\"])\nfeature_transform = 'BERT'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Before Training\nSplit training set and validation set using train_test_split","metadata":{}},{"cell_type":"code","source":"# Split the training set into training and validation set\n\ny_label = train_df['target']\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_label, test_size=0.1, random_state=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 1: Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Fit the logistic regression model on the training set\ndef logistic(C, penalty, solver):\n    logreg = LogisticRegression(C = C, max_iter = 100000, multi_class = 'ovr', penalty = penalty, solver = solver)\n    logreg.fit(X_train, y_train)\n    # Find the predicted values on the validation set\n\n    y_hat_logreg = logreg.predict(X_val)\n    # Switch between training and testing results\n    # y_hat_logreg = logreg.predict(X_train)\n    \n    return y_hat_logreg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conducting the logistic regression:","metadata":{}},{"cell_type":"code","source":"logreg_result_l1 = []\nlogreg_result_l2 = []\nlogreg_result_none = []\nfor C in [0.01, 0.1, 1, 10, 100, 1000, 10000]:\n    for penalty in ['none','l1','l2']:\n        if penalty == 'none':\n            y_hat_logreg = logistic(C, penalty, solver = 'sag') # liblinear does not support no regularization\n            print(f\"current Iteration: C={C}, penalty: {penalty}\")\n            logreg_result_none.append(y_hat_logreg)\n        else:\n            y_hat_logreg = logistic(C, penalty, solver = 'liblinear')\n            print(f\"current Iteration: C={C}, penalty: {penalty}\")\n            if penalty == 'l1':\n                logreg_result_l1.append(y_hat_logreg)\n            if penalty == 'l2':\n                logreg_result_l2.append(y_hat_logreg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Precision, recall and fscore on the validation set\nf1_lr_none = []\nf1_lr_l1 = []\nf1_lr_l2 = []\n\n# Testing:\n\nfor each in logreg_result_none:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_lr_none.append(fscore)\nfor each in logreg_result_l1:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_lr_l1.append(fscore)\nfor each in logreg_result_l2:\n    prec, recal, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_lr_l2.append(fscore)\n\n# Switch between training & testing results\n\n#Training:\n\n# for each in logreg_result_none:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_lr_none.append(fscore)\n# for each in logreg_result_l1:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_lr_l1.append(fscore)\n# for each in logreg_result_l2:\n#     prec, recal, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_lr_l2.append(fscore)\n\nprint(f1_lr_none, f1_lr_l1, f1_lr_l2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix -> for visualization presented in paper only\n\nconfusion_matrix_logreg = confusion_matrix(y_val, logreg_result_l2[3])\nsns.heatmap(confusion_matrix_logreg, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of performance based on hyperparameters\n\n# plt.title(f'C vs. Training F1 score ({feature_transform})')\nplt.title(f'C vs. Testing F1 score ({feature_transform})')\nplt.xlabel('C')\nplt.ylabel('F1 score')\nC_used = ['0.01','0.1', '1', '10', '100', '1000', '10000']\npenalties = ['none','l1','l2']\nplt.plot(C_used, f1_lr_none, color='g', label = 'none')\nplt.plot(C_used, f1_lr_l1, color='b', label = 'L1')\nplt.plot(C_used, f1_lr_l2, color='r', label = 'L2')\nplt.legend(loc='lower right')\nplt.ylim(0,1)\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_hat_logreg = logistic(0.6, 'l2',  solver = 'liblinear')    # fscore: 0.793985\n# y_hat_logreg = logistic(0.7, 'l2',  solver = 'liblinear')    # fscore: 0.795796\n# y_hat_logreg = logistic(0.75, 'l2',  solver = 'liblinear')   # fscore: 0.796992   Best!\ny_hat_logreg = logistic(0.8, 'l2',  solver = 'liblinear')      # fscore: 0.796992   Best!\n# y_hat_logreg = logistic(0.85, 'l2',  solver = 'liblinear')   # fscore: 0.795796\n# y_hat_logreg = logistic(0.9, 'l2',  solver = 'liblinear')    # fscore: 0.795796\n# y_hat_logreg = logistic(1, 'l2',  solver = 'liblinear')      # fscore: 0.795796\n# y_hat_logreg = logistic(1.2, 'l2',  solver = 'liblinear')    # fscore: 0.793985\n# y_hat_logreg = logistic(1.4, 'l2',  solver = 'liblinear')    # fscore: 0.791541\n_, _, fscore, _ = precision_recall_fscore_support(y_val, y_hat_logreg, average='binary') \nprint(fscore)\n\nconfusion_matrix_logreg = confusion_matrix(y_val, y_hat_logreg)\nsns.heatmap(confusion_matrix_logreg, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 2: SVM","metadata":{}},{"cell_type":"code","source":"# Fit the SVM model on the training set\ndef svm(C, kernel):\n    svm = SVC(C = C, kernel = kernel)\n    svm.fit(X_train, y_train)\n    # Find the predicted values on the validation set\n\n    y_hat_svm = svm.predict(X_val)\n    # y_hat_svm = svm.predict(X_train)\n\n    return y_hat_svm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_result_rbf = []\nsvm_result_linear = []\nsvm_result_poly = []\nfor C in [0.01, 0.1, 1, 10, 100, 1000]:\n    for kernel in ['rbf','linear','poly']:\n        y_hat_svm = svm(C, kernel)\n        print(f\"current Iteration: C={C}, kernel: {kernel}\")\n        if kernel == 'rbf':\n            svm_result_rbf.append(y_hat_svm)\n        elif kernel == 'linear':\n            svm_result_linear.append(y_hat_svm)\n        elif kernel == 'poly':\n            svm_result_poly.append(y_hat_svm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Precision, recall and fscore on the validation set\nf1_svm_rbf = []\nf1_svm_linear = []\nf1_svm_poly = []\n\n\n# for each in svm_result_rbf:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_svm_rbf.append(fscore)\n# for each in svm_result_linear:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_svm_linear.append(fscore)\n# for each in svm_result_poly:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_svm_poly.append(fscore)\n\n# Switch between training & testing fscores\n\nfor each in svm_result_rbf:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_svm_rbf.append(fscore)\nfor each in svm_result_linear:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_svm_linear.append(fscore)\nfor each in svm_result_poly:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_svm_poly.append(fscore)\n\n\nprint(f1_svm_rbf, f1_svm_linear, f1_svm_poly)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\n\nconfusion_matrix_svm = confusion_matrix(y_val, svm_result_linear[2])\nsns.heatmap(confusion_matrix_svm, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of performance based on hyperparameters\n\nplt.title(f'C vs. Testing F1 score ({feature_transform})')\n# plt.title(f'C vs. Training F1 score ({feature_transform})')\nplt.xlabel('C')\nplt.ylabel('F1 score')\nC_used = ['0.01','0.1', '1', '10', '100', '1000']\nplt.plot(C_used, f1_svm_rbf, color='g', label='RBF')\nplt.plot(C_used, f1_svm_linear, color='b', label='Linear')\nplt.plot(C_used, f1_svm_poly, color='r', label='Polynomial')\nplt.ylim(0,1)\nplt.legend(loc='lower right')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_hat_svm = svm(0.8, 'rbf')  # fscore: 0.792570\n# y_hat_svm = svm(0.8, 'rbf')  # fscore: 0.8\n# y_hat_svm = svm(1, 'rbf')  # fscore: 0.7987711\n# y_hat_svm = svm(1.2, 'rbf')  # fscore: 0.804314\n# y_hat_svm = svm(1.4, 'rbf')  # fscore: 0.807396\n# y_hat_svm = svm(1.6, 'rbf')  # fscore: 0.811060    Best!\ny_hat_svm = svm(1.7, 'rbf')  # fscore: 0.811060      Best!\n# y_hat_svm = svm(1.8, 'rbf')  # fscore: 0.811060    Best!\n# y_hat_svm = svm(2, 'rbf')  # fscore: 0.807988\n# y_hat_svm = svm(3, 'rbf')  # fscore: 0.8\n_, _, fscore, _ = precision_recall_fscore_support(y_val, y_hat_svm, average='binary') \nprint(fscore)\n\nconfusion_matrix_svm = confusion_matrix(y_val, y_hat_svm)\nsns.heatmap(confusion_matrix_svm, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 3: Neural Network","metadata":{}},{"cell_type":"code","source":"# Fit the neural network model on the training set\ndef neural_network(alpha, activation, nn_structure):\n    nn = MLPClassifier(hidden_layer_sizes=nn_structure, alpha = alpha, activation = activation, max_iter=10000)\n    nn.fit(X_train, y_train)\n    # Find the predicted values on the validation set\n\n    y_hat_nn = nn.predict(X_val)\n    # Switch between training & testing set\n    # y_hat_nn = nn.predict(X_train)\n\n    return y_hat_nn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_result_logistic = []\nnn_result_relu = []\nnn_result_tanh = []\n\nnn_structure = (256, 48)\n# Tuning neural network structure is omitted here for the sake of saving time\n# This is the best nn_structure observed by tuning nn_structure manually\n\nfor alpha in [0.0001,0.001,0.01,0.1, 1]:\n    for activation in ['logistic','relu','tanh']:\n        y_hat_nn = neural_network(alpha, activation, nn_structure)\n        print(f\"current Iteration: alpha={alpha}, activation: {activation}\")\n        if activation == 'logistic':\n            nn_result_logistic.append(y_hat_nn)\n        if activation == 'relu':\n            nn_result_relu.append(y_hat_nn)\n        if activation == 'tanh':\n            nn_result_tanh.append(y_hat_nn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_nn_logistic = []\nf1_nn_relu = []\nf1_nn_tanh = []\n\nfor each in nn_result_logistic:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_nn_logistic.append(fscore)\nfor each in nn_result_relu:\n    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_nn_relu.append(fscore)\nfor each in nn_result_tanh:\n    prec, recal, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n    f1_nn_tanh.append(fscore)\n\n# Switch between training and validation\n\n# for each in nn_result_logistic:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_nn_logistic.append(fscore)\n# for each in nn_result_relu:\n#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_nn_relu.append(fscore)\n# for each in nn_result_tanh:\n#     prec, recal, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n#     f1_nn_tanh.append(fscore)\n\n\nprint(f1_nn_logistic, f1_nn_relu, f1_nn_tanh)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\n\nconfusion_matrix_nn = confusion_matrix(y_val, nn_result_relu[4])\nsns.heatmap(confusion_matrix_nn, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of performance based on hyperparameters\n\nplt.title(f'lambda (L2 penalty) vs. Testing F1 score ({feature_transform})')\n# plt.title(f'lambda (L2 penalty) vs. Training F1 score ({feature_transform})')\nplt.xlabel('lambda')\nplt.ylabel('F1 score')\nC_used = ['0.0001','0.001','0.01','0.1', '1']\nplt.plot(C_used, f1_nn_logistic, color='g', label='logistic')\nplt.plot(C_used, f1_nn_relu, color='b', label='ReLU')\nplt.plot(C_used, f1_nn_tanh, color='r', label='tanh')\nplt.ylim(0,1)\nplt.legend(loc='lower left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 4: Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Fit the naive_bayes model on the training set\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.naive_bayes import ComplementNB\n# nb = BernoulliNB(alpha=0.1)     # fscore:  0.7891737891737892\nnb = BernoulliNB(alpha=1)         # fscore:  0.7891737891737892\n# nb = BernoulliNB(alpha=10)      # fscore:  0.7891737891737892\n\n# nb = BernoulliNB(binarize=0.001)         # fscore:  0.7857142857142857\n# nb = BernoulliNB(binarize=0.0001)         # fscore:  0.7874465049928673\n\n# nb = BernoulliNB(fit_prior=False)         # fscore:  0.788135593220339\n\n\n# nb = GaussianNB()         # fscore:  0.7839771101573677\n# nb = CategoricalNB()      # fscore: 0.0\n# nb = ComplementNB()       # cannot deal with negative values\n# nb = MultinomialNB()      # cannot deal with negative values\n\nnb.fit(X_train, y_train)       \n# Find the predicted values on the validation set\ny_hat_nb = nb.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Precision, recall and fscore on the validation set\nfrom sklearn.metrics import precision_recall_fscore_support\nprec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_nb,average='binary')\nprint('prec: ', prec)\nprint('recal: ', recal)\nprint('fscore: ', fscore)\n\n# Confusion Matrix\nconfusion_matrix_nb = confusion_matrix(y_val, y_hat_nb)\nsns.heatmap(confusion_matrix_nb, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 5: Random Forest","metadata":{}},{"cell_type":"code","source":"# Fit the random forest model on the training set\nfrom sklearn.ensemble import RandomForestClassifier\n# n_estimators = number of trees\n# rf = RandomForestClassifier(n_estimators=10)        # fscore:  0.6885245901639344\n# rf = RandomForestClassifier(n_estimators=50)        # fscore:  0.7576243980738362\n# rf = RandomForestClassifier(n_estimators=90)        # fscore:  0.770440\nrf = RandomForestClassifier(n_estimators=100)         # fscore:  0.7868338557993729\n# rf = RandomForestClassifier(n_estimators=110)       # fscore:  0.7641509433962265\n# rf = RandomForestClassifier(n_estimators=200)       # fscore:  0.7670364500792393\n# rf = RandomForestClassifier(n_estimators=1000)      # fscore:  0.7733755942947703\nrf.fit(X_train, y_train)\n# Find the predicted values on the validation set\ny_hat_rf = rf.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Precision, recall and fscore on the validation set\nfrom sklearn.metrics import precision_recall_fscore_support\nprec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_rf,average='binary')\nprint('prec: ', prec)\nprint('recal: ', recal)\nprint('fscore: ', fscore)\n\n# Confusion Matrix\nconfusion_matrix_rf = confusion_matrix(y_val, y_hat_rf)\nsns.heatmap(confusion_matrix_rf, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 6: Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Fit the gradient boosting model on the training set\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)      # fscore: 0.7914110429447853\n# gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=99)      # fscore: 0.7914110429447853\n# gb = GradientBoostingClassifier(learning_rate=0.08, n_estimators=125)   # fscore:  0.7870370370370371\n# gb = GradientBoostingClassifier(learning_rate=1.1, n_estimators=90)     # fscore:  0.7478260869565216\ngb.fit(X_train, y_train)\n# Find the predicted values on the validation set\ny_hat_gb = gb.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find Precision, recall and fscore on the validation set\nfrom sklearn.metrics import precision_recall_fscore_support\nprec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_gb,average='binary')\nprint('prec: ', prec)\nprint('recal: ', recal)\nprint('fscore: ', fscore)\n\n# Confusion Matrix\nconfusion_matrix_gb = confusion_matrix(y_val, y_hat_gb)\nsns.heatmap(confusion_matrix_gb, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_1 = y_hat_logreg\ny_hat_2 = y_hat_svm\ny_hat_3 = nn_result_relu[4]\n_, _, fscore1, _ = precision_recall_fscore_support(y_val, y_hat_1,average='binary')\nprint('fscore: ', fscore1)\n_, _, fscore2, _ = precision_recall_fscore_support(y_val, y_hat_2,average='binary')\nprint('fscore: ', fscore2)\n_, _, fscore3, _ = precision_recall_fscore_support(y_val, y_hat_3,average='binary')\nprint('fscore: ', fscore3)\n\ny_hat_4 = []\nfor i in range(len(y_hat_logreg)):\n    y_hat_4.append((y_hat_1[i]+(y_hat_2[i]+(y_hat_3[i]))) // 2)\n_, _, fscore4, _ = precision_recall_fscore_support(y_val, y_hat_4,average='binary')\nprint('fscore: ', fscore4)\n\ny_hat_5 = []\nfor i in range(len(y_hat_logreg)):\n    y_hat_5.append((y_hat_1[i]+(y_hat_2[i]+(y_hat_3[i]))) // 3)\n_, _, fscore5, _ = precision_recall_fscore_support(y_val, y_hat_5,average='binary')\nprint('fscore: ', fscore5)\n\n# Highest fscore method!!!!!\ny_hat_gb\ny_hat_6 = []\nfor i in range(len(y_hat_logreg)):\n    if (y_hat_1[i] == y_hat_3[i]) and (y_hat_3[i] == y_hat_gb[i]):\n        y_hat_6.append(y_hat_1[i])\n    else:\n        y_hat_6.append(y_hat_2[i])\n_, _, fscore6, _ = precision_recall_fscore_support(y_val, y_hat_6,average='binary')\nprint('fscore: ', fscore6)\n\n# Confusion Matrix\nconfusion_matrix_6 = confusion_matrix(y_val, y_hat_6)\nsns.heatmap(confusion_matrix_6, annot=True, fmt='d', cmap=\"Oranges\")","metadata":{},"execution_count":null,"outputs":[]}]}