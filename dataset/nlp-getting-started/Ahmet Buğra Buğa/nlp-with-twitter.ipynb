{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"keyword\",\"location\"],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning & Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping Numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].str.replace(\"[\\d]\",\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping punctuation marks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].str.replace(\"[^\\w\\s]\",\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nsw = stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sw.append(\"u\")\nsw.append(\"im\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import Word\nnltk.download(\"wordnet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"We're\", \"We are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"That's\", \"That is\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"won't\", \"will not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"they're\", \"they are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Can't\", \"Cannot\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"wasn't\", \"was not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"don\\x89Ûªt\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"aren't\", \"are not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"isn't\", \"is not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"You're\", \"You are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I'M\", \"I am\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"shouldn't\", \"should not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"wouldn't\", \"would not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"i'm\", \"I am\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"We've\", \"We have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Didn't\", \"Did not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"it's\", \"it is\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"can't\", \"cannot\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"don't\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"you're\", \"you are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I've\", \"I have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Don't\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I'll\", \"I will\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Let's\", \"Let us\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Could've\", \"Could have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"youve\", \"you have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"It's\", \"It is\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1 = (train[\"text\"][0:1000]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1.columns = [\"words\",\"tf\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf1[tf1[\"tf\"] > 30].sort_values(by = \"tf\" , ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.plot.bar( x = \"words\", y = \"tf\", color = \"green\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,5):\n    text = train[\"text\"][i]\n    wordcloud = WordCloud().generate(text)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(train[\"text\"],train[\"target\"],\n                                                test_size = 0.3,\n                                                random_state = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nvectorizer.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_count = vectorizer.transform(x_train)\nx_test_count = vectorizer.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = linear_model.LogisticRegression()\nlr_model = lr.fit(x_train_count,y_train)\ny_pred = lr_model.predict(x_test_count)\n\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop([\"keyword\",\"location\"], axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep(test):\n    test[\"text\"] = test[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    test[\"text\"] = test[\"text\"].str.replace(\"[\\d]\",\"\")\n    \n    test[\"text\"] = test [\"text\"].str.replace(\"[^\\w\\s]\",\"\")\n    \n    test[\"text\"] = test[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n    \n    test[\"text\"] = test[\"text\"].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x.split()))\n    \n    test[\"text\"] = test[\"text\"].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\")\n    \n    test[\"text\"] = test[\"text\"].str.replace(r'(((http)(s)?|www(.)?)(://)?\\S+)',\"\")\n    \n    test[\"text\"] = test[\"text\"].str.replace(r\"\\x89ÛÓ\", \"\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"he's\", \"he is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"there's\", \"there is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"We're\", \"We are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"That's\", \"That is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"won't\", \"will not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"they're\", \"they are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Can't\", \"Cannot\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"wasn't\", \"was not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"don\\x89Ûªt\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"aren't\", \"are not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"isn't\", \"is not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"You're\", \"You are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I'M\", \"I am\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"shouldn't\", \"should not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"wouldn't\", \"would not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"i'm\", \"I am\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"We've\", \"We have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Didn't\", \"Did not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"it's\", \"it is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"can't\", \"cannot\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"don't\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"you're\", \"you are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I've\", \"I have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Don't\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I'll\", \"I will\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Let's\", \"Let us\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Could've\", \"Could have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"youve\", \"you have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"It's\", \"It is\")\n    \n    return test\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = prep(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = df[\"text\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nvectorizer.fit(test_x)\nvectorizer.transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_vec = vectorizer.transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_vec = vectorizer.transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_vec,y_train)\ny_pred = nb_model.predict(x_test_vec)\ny_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary_ = {}\ndictionary_[\"id\"] = test.id\ndictionary_[\"target\"] = y_pred\nsubmission = pd.DataFrame(dictionary_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission_1.csv\" , index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}