{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os # im,porting os\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Importing Necessary Packages </h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n#Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('ggplot')\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# File system manangement\nimport os\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#importing tensorflow libraries\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Importing Data</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pycomp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycomp.viz.insights import *\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Exploratory Text-Data Analysis</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train['target'].value_counts())\nplt.figure(figsize=(8,6))\ncarrier_count = train[\"target\"].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(carrier_count.index, carrier_count.values, alpha=1,edgecolor='k',palette='rocket')\nplt.title('Frequency Distribution of target')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('target', fontsize=12)\nplt.xticks((0,1),('Fake', 'Real'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {1: 'Real', 0: 'Fake'}\nplot_donut_chart(df=train, col='target', label_names=mapping, colors=[\"#ff7f51\",\"#ff9b54\"],\n                 title='Target Value Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\ntrain['length'] = train['text'].apply(len)\ndata = [go.Box(y=train[train['target']==0]['length'],name='Fake'),\n        go.Box(y=train[train['target']==1]['length'],name='Real')]\nlayout = go.Layout(title = 'Comparison of text length in Tweets')\nfig = go.Figure(data=data, layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['id','keyword','location','length'],axis=1,inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature=train.drop('target',axis=1)\ntrain_target=train.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feature=test.drop(['id','keyword','location'],axis=1,inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_feature.isna().sum())\ndisplay(train_target.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_feature.shape,train_feature.dtypes)\ndisplay(train_target.shape,train_target.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Text Preprocessing</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # Convert to lower\n    text = re.sub('\\[.*?\\]', '', text) #remove texts in square brackets\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)#remove links\n    text = re.sub('<.*?>+', '', text)#remove special characters\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)#remove punctuation\n    text = re.sub('\\n', '', text)#remove words containing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain_feature['text'] = train_feature['text'].apply(lambda x: clean_text(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain_feature['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_feature['text'] = train_feature['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain_feature['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \"\"\"Removing stopwords belonging to english language\n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\ntrain_feature['text'] = train_feature['text'].apply(lambda x : remove_stopwords(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x : remove_stopwords(x))\ntrain_feature.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_feature['text'] = train_feature['text'].apply(lambda x : combine_text(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x : combine_text(x))\ntrain_feature['text']\ntrain_feature.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Uni-gram,Bi-gram,Tri-Gram for Train Set</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(train_feature['text'], 30)\ndf2 = pd.DataFrame (common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', \n                        linecolor='black',title='Top 30 unigrams used in Tweets',color='#48cae4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(train_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#720026')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(train_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#e9b827')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Uni-gram,Bi-gram,Tri-Gram for Test Set</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(test_feature['text'], 30)\ndf2 = pd.DataFrame (common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', \n                        linecolor='black',title='Top 30 unigrams used in Tweets',color='#48cae4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(test_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#720026')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(test_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#e9b827')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">WordCloud Train Set</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(16,8))\nwc = WordCloud(background_color=\"black\", max_words=150,max_font_size=150,random_state=42)\nwc.generate(' '.join(train_feature['text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">WordCloud Test Set</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(16,8))\nwc = WordCloud(background_color=\"black\", max_words=150,max_font_size=150,random_state=42)\nwc.generate(' '.join(test_feature['text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Tf-idf & Multinomial Naive-Bayes</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf= TfidfVectorizer()\nX=tf_idf.fit_transform(train_feature.text)\nX=X.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_val, y_train, y_val = train_test_split(X,train['target'], test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)} \nmodel_nv=MultinomialNB()\nclf = GridSearchCV(model_nv,parameters,cv=10, scoring='accuracy')\nclf.fit(X_train, y_train)\nprint(\"The best Score\",clf.best_score_)\nprint(\"-------\")\nprint(\"The best Estimator\",clf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=clf.predict(X_val)\naccuracy=accuracy_score(y_val,y_pred)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_Pred=clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val,Y_Pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">AdaBoost-GridSearch</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  AdaBoostClassifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8),random_state = 42)\nparameters = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"random\"],#\"algorithm\" : [\"SAMME\",\"SAMME.R\"]\n              \"n_estimators\" :[100],\n              \"learning_rate\":  [0.05, 0.5, 1]}\nada_clf = GridSearchCV(ada_clf, parameters, cv=3, scoring=\"accuracy\")\nada_clf.fit(X_train, y_train)\nprint(f'Best parameters {ada_clf.best_params_}')\nprint('-----')\nprint(f'Mean cross-validated accuracy score of the best_estimator: '+f'{ada_clf.best_score_:.3f}')\n# Ada_clf = AdaBoostClassifier(DecisionTreeClassifier,n_estimators=5,random_state=1)\n# Ada_clf.fit(X_train, y_train)\n# y_pred = Ada_clf.predict(X_val)\n# score = metrics.accuracy_score(y_val,y_pred)\n# print(\"accuracy: %0.3f\" %score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy:\",ada_clf.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_Pred=ada_clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val,Y_Pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">CatBoost-GridSearch</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## hyperparameter tuning example grid for catboost : \nimport catboost as cb\nfrom catboost import CatBoostClassifier\n\nparameters = {'depth': [4, 7, 10],\n              'learning_rate' : [0.03, 0.1, 0.15],\n              'l2_leaf_reg': [1,9],\n              'iterations': [100]}\ncb_clf = cb.CatBoostClassifier()\ncb_clf = GridSearchCV(cb_clf, parameters, scoring=\"roc_auc\", cv = 5)\ncb_clf.fit(X_train,y_train)\nprint(f'Best parameters {cb_clf.best_params_}')\nprint('-----')\nprint(f'Mean cross-validated accuracy score of the best_estimator: '+f'{cb_clf.best_score_:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy:\",cb_clf.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_Pred=cb_clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Neural Net</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size=len(train_feature['text'])+1 #deciding My Vocabulary Size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_representation=[one_hot(words,voc_size) for words in train_feature['text']]\nonehot_representation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embedding Representation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_length=120 #Since to make each sentence of equal length we are padding. \nembedding=pad_sequences(onehot_representation,padding='post',maxlen=sent_length)\nprint(embedding)\ndisplay(len(embedding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating model\nembedding_vector_features=200\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length,trainable=True)) #Embedding Layer\nmodel.add(LSTM(100)) # 1LSTM Layer with 128 Neurons\n#model.add(LSTM(output_nodes, dropout = dropout, recurrent_dropout = recurrent_dropout))\nmodel.add(Dense(1,activation='sigmoid'))#Since Classification type problem so Dense Layer\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final=np.array(embedding)\ny_final=np.array(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final.shape,y_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20,batch_size=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size=len(test_feature['text'])+1 #deciding My Vocabulary Size\ntest_onehot=[one_hot(words,voc_size) for words in test_feature['text']]\nsent_length=120 #Since to make each sentence of equal length we are padding. \ntest_embedding=pad_sequences(test_onehot,padding='post',maxlen=sent_length)\nprint(test_embedding)\ndisplay(len(test_embedding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final=np.array(test_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict_classes(test_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target']=y_pred\nsubmission.to_csv('new_submission.csv') \nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}