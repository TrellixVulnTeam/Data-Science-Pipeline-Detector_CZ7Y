{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-20T22:09:34.773377Z","iopub.execute_input":"2022-04-20T22:09:34.77361Z","iopub.status.idle":"2022-04-20T22:09:34.788601Z","shell.execute_reply.started":"2022-04-20T22:09:34.773584Z","shell.execute_reply":"2022-04-20T22:09:34.787824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.795595Z","iopub.execute_input":"2022-04-20T22:09:34.796056Z","iopub.status.idle":"2022-04-20T22:09:34.800458Z","shell.execute_reply.started":"2022-04-20T22:09:34.796024Z","shell.execute_reply":"2022-04-20T22:09:34.799473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.817567Z","iopub.execute_input":"2022-04-20T22:09:34.818131Z","iopub.status.idle":"2022-04-20T22:09:34.890546Z","shell.execute_reply.started":"2022-04-20T22:09:34.818089Z","shell.execute_reply":"2022-04-20T22:09:34.889613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Investigate train dataset to understand what kind of data is presented and which of it can be useful","metadata":{}},{"cell_type":"markdown","source":"### a) train and test analysis","metadata":{}},{"cell_type":"code","source":"train_data.shape, test_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.892066Z","iopub.execute_input":"2022-04-20T22:09:34.892282Z","iopub.status.idle":"2022-04-20T22:09:34.901172Z","shell.execute_reply.started":"2022-04-20T22:09:34.892254Z","shell.execute_reply":"2022-04-20T22:09:34.900243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.902489Z","iopub.execute_input":"2022-04-20T22:09:34.903258Z","iopub.status.idle":"2022-04-20T22:09:34.925257Z","shell.execute_reply.started":"2022-04-20T22:09:34.90317Z","shell.execute_reply":"2022-04-20T22:09:34.924399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.926904Z","iopub.execute_input":"2022-04-20T22:09:34.927791Z","iopub.status.idle":"2022-04-20T22:09:34.939067Z","shell.execute_reply.started":"2022-04-20T22:09:34.927757Z","shell.execute_reply":"2022-04-20T22:09:34.938365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.940385Z","iopub.execute_input":"2022-04-20T22:09:34.940696Z","iopub.status.idle":"2022-04-20T22:09:34.972156Z","shell.execute_reply.started":"2022-04-20T22:09:34.940658Z","shell.execute_reply":"2022-04-20T22:09:34.971123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.973418Z","iopub.execute_input":"2022-04-20T22:09:34.973646Z","iopub.status.idle":"2022-04-20T22:09:34.981578Z","shell.execute_reply.started":"2022-04-20T22:09:34.973616Z","shell.execute_reply":"2022-04-20T22:09:34.980583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:34.982809Z","iopub.execute_input":"2022-04-20T22:09:34.983086Z","iopub.status.idle":"2022-04-20T22:09:35.001153Z","shell.execute_reply.started":"2022-04-20T22:09:34.983051Z","shell.execute_reply":"2022-04-20T22:09:35.000348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check non-empty keyword examples\ntrain_data[train_data.keyword.notna()].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.003266Z","iopub.execute_input":"2022-04-20T22:09:35.003698Z","iopub.status.idle":"2022-04-20T22:09:35.017964Z","shell.execute_reply.started":"2022-04-20T22:09:35.003668Z","shell.execute_reply":"2022-04-20T22:09:35.017498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what kind of keyword is presented?\n# check all unique keyword values\n\ntrain_data.keyword.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.018765Z","iopub.execute_input":"2022-04-20T22:09:35.019218Z","iopub.status.idle":"2022-04-20T22:09:35.02592Z","shell.execute_reply.started":"2022-04-20T22:09:35.019179Z","shell.execute_reply":"2022-04-20T22:09:35.025066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many tweets from train dataset have non-empty keyword?\nprint((train_data[train_data.keyword.notna()].shape[0]/train_data.shape[0]) * 100, '%')\n\n# almost every record has keyword so it can help to make a better prediction","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.02726Z","iopub.execute_input":"2022-04-20T22:09:35.027485Z","iopub.status.idle":"2022-04-20T22:09:35.037614Z","shell.execute_reply.started":"2022-04-20T22:09:35.027455Z","shell.execute_reply":"2022-04-20T22:09:35.037009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many tweets from train dataset have non-empty location feature?\nprint((train_data[train_data.location.notna()].shape[0]/train_data.shape[0]) * 100, '%')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.038665Z","iopub.execute_input":"2022-04-20T22:09:35.039224Z","iopub.status.idle":"2022-04-20T22:09:35.049903Z","shell.execute_reply.started":"2022-04-20T22:09:35.03917Z","shell.execute_reply":"2022-04-20T22:09:35.049005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'location' feature has significant amout of missing values\n# lets check what kind of words this feature contains\n\ntrain_data.location.unique()[:40]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.052091Z","iopub.execute_input":"2022-04-20T22:09:35.052931Z","iopub.status.idle":"2022-04-20T22:09:35.06471Z","shell.execute_reply.started":"2022-04-20T22:09:35.052884Z","shell.execute_reply":"2022-04-20T22:09:35.063686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Location' feature seems not really informative so I'm going to focus on 'text' and 'keyword' features at this moment ","metadata":{}},{"cell_type":"markdown","source":"### b) Target value","metadata":{}},{"cell_type":"code","source":"# check whether target value is skewed\n\ntrain_data.target.value_counts().plot.bar()\n\n# it is clear that target value is not skewed ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.066409Z","iopub.execute_input":"2022-04-20T22:09:35.067321Z","iopub.status.idle":"2022-04-20T22:09:35.241083Z","shell.execute_reply.started":"2022-04-20T22:09:35.067278Z","shell.execute_reply":"2022-04-20T22:09:35.240371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Clean the data","metadata":{}},{"cell_type":"code","source":"# to lower\ndef to_lower(text):\n    return text.lower()\n\nexample=\"Hi! My name is Pete.\"\nto_lower(example)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.242498Z","iopub.execute_input":"2022-04-20T22:09:35.242964Z","iopub.status.idle":"2022-04-20T22:09:35.250382Z","shell.execute_reply.started":"2022-04-20T22:09:35.242926Z","shell.execute_reply":"2022-04-20T22:09:35.249634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove stopwords\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in stop])\n\nexample = 'I love my cat and dog. You should see them'\nprint(remove_stopwords(example))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.252647Z","iopub.execute_input":"2022-04-20T22:09:35.253126Z","iopub.status.idle":"2022-04-20T22:09:35.907645Z","shell.execute_reply.started":"2022-04-20T22:09:35.253091Z","shell.execute_reply":"2022-04-20T22:09:35.906769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# URL\n\nimport re\nfrom urllib.parse import urlparse\n\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+')\n    return url.sub(r'',text)\n\ndef find_url(text):\n    return \" \".join([urlparse(match.group(0)).netloc for match in re.finditer(r\"https?://\\S+|www\\.\\S+\", text)]) or 'no'\n\n    \nexample = \"New competition launched here :https://www.kaggle.com/c/nlp-getting-started or here: http://www.kaggle.com/c/nlp-getting-started\"\nprint(remove_url(example))\nprint(find_url(example))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.908783Z","iopub.execute_input":"2022-04-20T22:09:35.908998Z","iopub.status.idle":"2022-04-20T22:09:35.917926Z","shell.execute_reply.started":"2022-04-20T22:09:35.90897Z","shell.execute_reply":"2022-04-20T22:09:35.916857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HASHTAGS\n\ndef remove_hashtags(text):\n    hashtag = re.compile(r'#\\w+')\n    return hashtag.sub(r'', text)\n\ndef find_hashtags(text):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", text)]) or 'no'\n\nexample = 'Hi#hhhh #ttttt'\nprint(remove_hashtags(example))\nprint(find_hashtags(example))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.919307Z","iopub.execute_input":"2022-04-20T22:09:35.920317Z","iopub.status.idle":"2022-04-20T22:09:35.928371Z","shell.execute_reply.started":"2022-04-20T22:09:35.920264Z","shell.execute_reply":"2022-04-20T22:09:35.927615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MENTIONS\n\ndef remove_mentions(text):\n    mention = re.compile(r'@\\w+')\n    return mention.sub(r'', text)\n\ndef find_mentions(text):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", text)]) or 'no'\n\nexample = 'Hi @POTUS yeeeeeh'\nprint(remove_mentions(example))\nprint(find_mentions(example))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.929494Z","iopub.execute_input":"2022-04-20T22:09:35.929703Z","iopub.status.idle":"2022-04-20T22:09:35.938904Z","shell.execute_reply.started":"2022-04-20T22:09:35.929675Z","shell.execute_reply":"2022-04-20T22:09:35.938385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all non-alphabetic chars (punctuation, numbers, emojies ...)\ndef remove_non_alphabetic(text):\n    alpha = re.compile(r'[^a-zA-Z]') \n    return alpha.sub(r' ', text)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.939894Z","iopub.execute_input":"2022-04-20T22:09:35.940102Z","iopub.status.idle":"2022-04-20T22:09:35.946923Z","shell.execute_reply.started":"2022-04-20T22:09:35.940074Z","shell.execute_reply":"2022-04-20T22:09:35.946237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_text(text):\n    text = to_lower(text)\n    text = remove_url(text)\n    text = remove_hashtags(text)\n    text = remove_mentions(text)\n    text = remove_non_alphabetic(text)\n    text = remove_stopwords(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.947876Z","iopub.execute_input":"2022-04-20T22:09:35.948068Z","iopub.status.idle":"2022-04-20T22:09:35.960654Z","shell.execute_reply.started":"2022-04-20T22:09:35.948041Z","shell.execute_reply":"2022-04-20T22:09:35.959389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data.pop('target')\ny","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.9617Z","iopub.execute_input":"2022-04-20T22:09:35.962247Z","iopub.status.idle":"2022-04-20T22:09:35.973837Z","shell.execute_reply.started":"2022-04-20T22:09:35.96219Z","shell.execute_reply":"2022-04-20T22:09:35.973174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([train_data, test_data])\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.974884Z","iopub.execute_input":"2022-04-20T22:09:35.975236Z","iopub.status.idle":"2022-04-20T22:09:35.982265Z","shell.execute_reply.started":"2022-04-20T22:09:35.975179Z","shell.execute_reply":"2022-04-20T22:09:35.981696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill na keywords with 'na' label\n\ndata.keyword.fillna('na', inplace = True) \ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:35.984975Z","iopub.execute_input":"2022-04-20T22:09:35.98519Z","iopub.status.idle":"2022-04-20T22:09:35.999238Z","shell.execute_reply.started":"2022-04-20T22:09:35.985162Z","shell.execute_reply":"2022-04-20T22:09:35.998329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.keyword = data.keyword.astype(str)\ndata.text = data.text.astype(str)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.000248Z","iopub.execute_input":"2022-04-20T22:09:36.000455Z","iopub.status.idle":"2022-04-20T22:09:36.018231Z","shell.execute_reply.started":"2022-04-20T22:09:36.000428Z","shell.execute_reply":"2022-04-20T22:09:36.017668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['keyword', 'text']\nfor c in columns:\n    data[c + '_clean'] = data[c].apply(process_text)\n\n#add new features\ndata['mentions'] = data.text.apply(find_mentions)\ndata['hashtags'] = data.text.apply(find_hashtags)\ndata['url'] = data.text.apply(find_url)\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.019157Z","iopub.execute_input":"2022-04-20T22:09:36.019784Z","iopub.status.idle":"2022-04-20T22:09:36.573923Z","shell.execute_reply.started":"2022-04-20T22:09:36.019755Z","shell.execute_reply":"2022-04-20T22:09:36.573174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's check some random tweets\nimport random\n\n# generate 20 random indices\nrand_idx = [random.randint(0, data.shape[0]) for i in range(20)]\nfor i in rand_idx:\n    print(i, data.text_clean.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.575131Z","iopub.execute_input":"2022-04-20T22:09:36.575421Z","iopub.status.idle":"2022-04-20T22:09:36.589779Z","shell.execute_reply.started":"2022-04-20T22:09:36.575384Z","shell.execute_reply":"2022-04-20T22:09:36.58914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word frequency","metadata":{}},{"cell_type":"code","source":"# get word frequency to check whether some strange outliers are presented\ndef create_corpus(column_series):\n    return column_series.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.590652Z","iopub.execute_input":"2022-04-20T22:09:36.59088Z","iopub.status.idle":"2022-04-20T22:09:36.595068Z","shell.execute_reply.started":"2022-04-20T22:09:36.590849Z","shell.execute_reply":"2022-04-20T22:09:36.594518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_disaster = create_corpus(data.text_clean[y == 1])\nd_dis = {}\nfor s in corpus_disaster:\n    for word in s.split():\n        d_dis[word] = d_dis.get(word, 0) + 1\n            \ncorpus_non_disaster = create_corpus(data.text_clean[y == 0])\nd_non_dis = {}\nfor s in corpus_non_disaster:\n    for word in s.split():\n        d_non_dis[word] = d_non_dis.get(word, 0) + 1\nlen(d_dis), len(d_non_dis)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.596344Z","iopub.execute_input":"2022-04-20T22:09:36.596538Z","iopub.status.idle":"2022-04-20T22:09:36.672036Z","shell.execute_reply.started":"2022-04-20T22:09:36.596515Z","shell.execute_reply":"2022-04-20T22:09:36.671159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_d_dis = sorted(d_dis.items(), key = lambda x:x[1])\nsorted_d_non_dis = sorted(d_non_dis.items(), key = lambda x:x[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:09:36.673432Z","iopub.execute_input":"2022-04-20T22:09:36.673658Z","iopub.status.idle":"2022-04-20T22:09:36.685528Z","shell.execute_reply.started":"2022-04-20T22:09:36.673629Z","shell.execute_reply":"2022-04-20T22:09:36.684829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# most frequent words\ndf1 = pd.DataFrame(sorted_d_dis[-30:], columns = ['word', 'freq'])\ndf1['target'] = 1\n\ndf2 = pd.DataFrame(sorted_d_non_dis[-30:], columns = ['word', 'freq'])\ndf2['target'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:14:36.125465Z","iopub.execute_input":"2022-04-20T22:14:36.126472Z","iopub.status.idle":"2022-04-20T22:14:36.134527Z","shell.execute_reply.started":"2022-04-20T22:14:36.126408Z","shell.execute_reply":"2022-04-20T22:14:36.133642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndf = pd.concat([df1, df2])\n\nplt.figure(figsize = (10, 20))\nsns.barplot(y = \"word\", hue = \"target\", x = \"freq\", data=df.sort_values(by = 'freq', ascending=False), orient = 'h')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:15:14.11656Z","iopub.execute_input":"2022-04-20T22:15:14.116854Z","iopub.status.idle":"2022-04-20T22:15:15.396792Z","shell.execute_reply.started":"2022-04-20T22:15:14.116822Z","shell.execute_reply":"2022-04-20T22:15:15.396357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Preparing text corpus: sequences, OHE matrix, stemming, lemmatization","metadata":{}},{"cell_type":"markdown","source":"### a) Get data corpus/OHE matrix/sequences","metadata":{}},{"cell_type":"code","source":"corpus_text = create_corpus(data.text_clean)\ncorpus_text[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:16:41.589601Z","iopub.execute_input":"2022-04-20T22:16:41.589825Z","iopub.status.idle":"2022-04-20T22:16:41.595813Z","shell.execute_reply.started":"2022-04-20T22:16:41.589803Z","shell.execute_reply":"2022-04-20T22:16:41.595247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove all spaces in keyword feature to get only one word as a keyword\ncorpus_keyword = create_corpus(data.keyword_clean.str.replace(\" \", \"\"))\ncorpus_keyword[100]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:16:41.697789Z","iopub.execute_input":"2022-04-20T22:16:41.698688Z","iopub.status.idle":"2022-04-20T22:16:41.713565Z","shell.execute_reply.started":"2022-04-20T22:16:41.698646Z","shell.execute_reply":"2022-04-20T22:16:41.712795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keyword_clean mentions hashtags url - CountVectorizer\n# text_clean - TF-idf Transformer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# keyword_clean\nvec_kw = CountVectorizer()\nmatrix_kw = vec_kw.fit_transform(corpus_keyword)\ndata_kw = pd.DataFrame(matrix_kw.toarray(), columns=vec_kw.get_feature_names_out())\n\n# URL\nvec_url = CountVectorizer(min_df = 3)\nmatrix_url = vec_url.fit_transform(data.url)\ndata_url = pd.DataFrame(matrix_url.toarray(), columns=vec_url.get_feature_names_out())\n\n# mentions\nvec_mentions = CountVectorizer(min_df = 5)\nmatrix_mentions = vec_mentions.fit_transform(data.mentions)\ndata_mentions = pd.DataFrame(matrix_mentions.toarray(), columns = vec_mentions.get_feature_names_out())\n\n# hashtags\nvec_hashtags = CountVectorizer(min_df = 5)\nmatrix_hashtags = vec_hashtags.fit_transform(data.hashtags)\ndata_hashtags = pd.DataFrame(matrix_hashtags.toarray(), columns = vec_hashtags.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:16:41.806261Z","iopub.execute_input":"2022-04-20T22:16:41.80654Z","iopub.status.idle":"2022-04-20T22:16:42.09159Z","shell.execute_reply.started":"2022-04-20T22:16:41.806508Z","shell.execute_reply":"2022-04-20T22:16:42.09078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tf-idf for text_clean\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 5, ngram_range = (1,2)) \ntext_vec = vec_text.fit_transform(corpus_text)\ndata_text_clean = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:16:42.092916Z","iopub.execute_input":"2022-04-20T22:16:42.093555Z","iopub.status.idle":"2022-04-20T22:16:42.608145Z","shell.execute_reply.started":"2022-04-20T22:16:42.093523Z","shell.execute_reply":"2022-04-20T22:16:42.607552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is no data_kw!\n# run models with diff list features and found out that without data_kw we get better result!\ndf_clean = pd.concat([data_url, data_mentions, data_hashtags, data_text_clean], axis = 1)\ndf_clean.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:52.612501Z","iopub.execute_input":"2022-04-20T22:38:52.612939Z","iopub.status.idle":"2022-04-20T22:38:52.915493Z","shell.execute_reply.started":"2022-04-20T22:38:52.6129Z","shell.execute_reply":"2022-04-20T22:38:52.914715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b) Lemmatization","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef corpus_lem(corpus):\n    return [' '.join(wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in sentence.split()) for sentence in corpus]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:52.916814Z","iopub.execute_input":"2022-04-20T22:38:52.91699Z","iopub.status.idle":"2022-04-20T22:38:52.92169Z","shell.execute_reply.started":"2022-04-20T22:38:52.916969Z","shell.execute_reply":"2022-04-20T22:38:52.920843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text feature lemmitizing\ncorpus_text_lem = corpus_lem(corpus_text)\ncorpus_text[2], corpus_text_lem[2], corpus_text[8], corpus_text_lem[8]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:52.97382Z","iopub.execute_input":"2022-04-20T22:38:52.974009Z","iopub.status.idle":"2022-04-20T22:38:53.673103Z","shell.execute_reply.started":"2022-04-20T22:38:52.973988Z","shell.execute_reply":"2022-04-20T22:38:53.672391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_clean feature\nvec_text_lem = TfidfVectorizer(min_df = 5, ngram_range = (1,2)) \ntext_lem_vec = vec_text_lem.fit_transform(corpus_text_lem)\ndata_text_clean_lem = pd.DataFrame(text_lem_vec.toarray(), columns = vec_text_lem.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:53.674643Z","iopub.execute_input":"2022-04-20T22:38:53.675356Z","iopub.status.idle":"2022-04-20T22:38:54.229013Z","shell.execute_reply.started":"2022-04-20T22:38:53.67532Z","shell.execute_reply":"2022-04-20T22:38:54.228096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lem = pd.concat([data_url, data_mentions, data_hashtags, data_text_clean_lem], axis = 1)\ndf_lem.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:54.230181Z","iopub.execute_input":"2022-04-20T22:38:54.230429Z","iopub.status.idle":"2022-04-20T22:38:54.600972Z","shell.execute_reply.started":"2022-04-20T22:38:54.230398Z","shell.execute_reply":"2022-04-20T22:38:54.600075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c) Stemming","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\n\ndef corpus_stemming(corpus):\n    return [' '.join(ps.stem(word) for word in sentence.split()) for sentence in corpus]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:54.604986Z","iopub.execute_input":"2022-04-20T22:38:54.605348Z","iopub.status.idle":"2022-04-20T22:38:54.611512Z","shell.execute_reply.started":"2022-04-20T22:38:54.605311Z","shell.execute_reply":"2022-04-20T22:38:54.610183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text feature stemming\ncorpus_text_stem = corpus_stemming(corpus_text)\ncorpus_text[0], corpus_text_stem[0], corpus_text[1], corpus_text_stem[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:54.612706Z","iopub.execute_input":"2022-04-20T22:38:54.612987Z","iopub.status.idle":"2022-04-20T22:38:58.412091Z","shell.execute_reply.started":"2022-04-20T22:38:54.612957Z","shell.execute_reply":"2022-04-20T22:38:58.411548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_clean feature\nvec_text_stem = TfidfVectorizer(min_df = 5, ngram_range = (1,2)) \ntext_stem_vec = vec_text_stem.fit_transform(corpus_text_stem)\ndata_text_clean_stem = pd.DataFrame(text_stem_vec.toarray(), columns = vec_text_stem.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:58.413102Z","iopub.execute_input":"2022-04-20T22:38:58.413672Z","iopub.status.idle":"2022-04-20T22:38:58.914371Z","shell.execute_reply.started":"2022-04-20T22:38:58.41364Z","shell.execute_reply":"2022-04-20T22:38:58.913432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stem = pd.concat([data_url, data_mentions, data_hashtags, data_text_clean_stem], axis = 1)\ndf_stem.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:58.91556Z","iopub.execute_input":"2022-04-20T22:38:58.915799Z","iopub.status.idle":"2022-04-20T22:38:59.222752Z","shell.execute_reply.started":"2022-04-20T22:38:58.915766Z","shell.execute_reply":"2022-04-20T22:38:59.221893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Brute Force: Bag of Words","metadata":{}},{"cell_type":"markdown","source":"#### Sequence of Dense layers for OHE train data","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras import Input\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, SpatialDropout1D, Concatenate\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import regularizers","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:59.223876Z","iopub.execute_input":"2022-04-20T22:38:59.224586Z","iopub.status.idle":"2022-04-20T22:38:59.229624Z","shell.execute_reply.started":"2022-04-20T22:38:59.224556Z","shell.execute_reply":"2022-04-20T22:38:59.228946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_loss_acc(history):\n    loss = history.history['loss']\n    acc = history.history['accuracy']\n    val_loss = history.history['val_loss']\n    val_acc = history.history['val_accuracy']\n    \n    epochs = range(1, len(loss) + 1)\n    \n    plt.figure(figsize=(16, 5))\n    #accuracy\n    plt.subplot(1,2,1)\n    plt.plot(epochs, acc, 'bo', label = 'Training accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation accuracy')\n    plt.legend()\n    \n    #loss\n    plt.subplot(1,2,2)\n    plt.plot(epochs, loss, 'bo', label = 'Trainig loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n    plt.legend()\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:59.230646Z","iopub.execute_input":"2022-04-20T22:38:59.231311Z","iopub.status.idle":"2022-04-20T22:38:59.239714Z","shell.execute_reply.started":"2022-04-20T22:38:59.23128Z","shell.execute_reply":"2022-04-20T22:38:59.239242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission(y, file_name):\n    y=np.round(y).astype(int).reshape(test_data.shape[0])\n    sub=pd.DataFrame({'id': test_data.id,'target':y})\n    sub.to_csv(file_name,index=False)\n    sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:59.241394Z","iopub.execute_input":"2022-04-20T22:38:59.241673Z","iopub.status.idle":"2022-04-20T22:38:59.25006Z","shell.execute_reply.started":"2022-04-20T22:38:59.241651Z","shell.execute_reply":"2022-04-20T22:38:59.249639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean_train = df_clean.iloc[:train_data.shape[0]]\ndf_clean_test = df_clean.iloc[train_data.shape[0]:]\n\ndf_lem_train = df_lem.iloc[:train_data.shape[0]]\ndf_lem_test = df_lem.iloc[train_data.shape[0]:]\n\ndf_stem_train = df_stem.iloc[:train_data.shape[0]]\ndf_stem_test = df_stem.iloc[train_data.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:59.250866Z","iopub.execute_input":"2022-04-20T22:38:59.251113Z","iopub.status.idle":"2022-04-20T22:38:59.263063Z","shell.execute_reply.started":"2022-04-20T22:38:59.251092Z","shell.execute_reply":"2022-04-20T22:38:59.262627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression models","metadata":{}},{"cell_type":"code","source":"# build simple logistic regression (+ Random Forest Classifier)\n# to use their scores as basic\n# if simple classifier show some good results probably it won't be necessary to build more complicated NN model (spoiler: no)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nlog_reg_model = LogisticRegression(max_iter = 500, random_state = 42)\nlog_reg_score = cross_val_score(log_reg_model, df_clean_train, y, cv = 5)\nlog_reg_score_lem = cross_val_score(log_reg_model, df_lem_train, y, cv = 5)\nlog_reg_score_stem = cross_val_score(log_reg_model, df_stem_train, y, cv = 5)\nprint('Data clean: %.2f', log_reg_score.mean())\nprint('Data lemm: %.2f', log_reg_score_lem.mean())\nprint('Data stem: %.2f', log_reg_score_stem.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:38:59.264128Z","iopub.execute_input":"2022-04-20T22:38:59.264449Z","iopub.status.idle":"2022-04-20T22:39:25.996986Z","shell.execute_reply.started":"2022-04-20T22:38:59.26442Z","shell.execute_reply":"2022-04-20T22:39:25.996359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_forest_cl = RandomForestClassifier(n_estimators = 1000, max_depth = 30, random_state = 42)\n\nrand_forest_score = cross_val_score(rand_forest_cl, df_clean_train, y, cv = 3)\nrand_forest_score_lem = cross_val_score(rand_forest_cl, df_lem_train, y, cv = 3)\nrand_forest_score_stem = cross_val_score(rand_forest_cl, df_stem_train, y, cv = 3)\n\nprint('Data clean: %.2f', rand_forest_score.mean())\nprint('Data lemm: %.2f', rand_forest_score_lem.mean())\nprint('Data stem: %.2f', rand_forest_score_stem.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:42:58.620908Z","iopub.execute_input":"2022-04-20T23:42:58.621726Z","iopub.status.idle":"2022-04-20T23:47:08.12176Z","shell.execute_reply.started":"2022-04-20T23:42:58.621655Z","shell.execute_reply":"2022-04-20T23:47:08.121004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN models","metadata":{}},{"cell_type":"code","source":"# build simple NN model\n\ndef get_model(shape):\n    model = Sequential(\n            [\n                Dense(64, activation = 'relu', input_shape = (shape,), kernel_regularizer = regularizers.l2(0.011)),\n                Dropout(0.1),\n                Dense(16, activation = 'relu', kernel_regularizer = regularizers.l2(0.004)),\n                Dense(1, activation = 'sigmoid')\n            ])\n\n    optimzer = Adam(learning_rate = 1e-3)\n    model.compile(loss = 'binary_crossentropy', optimizer = optimzer, metrics = ['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:43:25.443464Z","iopub.execute_input":"2022-04-20T22:43:25.444058Z","iopub.status.idle":"2022-04-20T22:43:25.44984Z","shell.execute_reply.started":"2022-04-20T22:43:25.444028Z","shell.execute_reply":"2022-04-20T22:43:25.449357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acc ~0.75\nmodel = get_model(df_clean_train.shape[1])\nhistory = model.fit(df_clean_train.to_numpy(), y, batch_size = 128, epochs = 6, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:43:25.450788Z","iopub.execute_input":"2022-04-20T22:43:25.451304Z","iopub.status.idle":"2022-04-20T22:43:29.611895Z","shell.execute_reply.started":"2022-04-20T22:43:25.451276Z","shell.execute_reply":"2022-04-20T22:43:29.610875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lem = get_model(df_lem_train.shape[1])\nhistory_lem = model_lem.fit(df_lem_train.to_numpy(), y, batch_size = 128, epochs = 10, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history_lem)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:44:30.286279Z","iopub.execute_input":"2022-04-20T22:44:30.2866Z","iopub.status.idle":"2022-04-20T22:44:34.398085Z","shell.execute_reply.started":"2022-04-20T22:44:30.286562Z","shell.execute_reply":"2022-04-20T22:44:34.397174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_stem = get_model(df_stem_train.shape[1])\nhistory_stem = model_stem.fit(df_stem_train.to_numpy(), y, batch_size = 128, epochs = 10, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history_stem)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:44:34.399546Z","iopub.execute_input":"2022-04-20T22:44:34.399757Z","iopub.status.idle":"2022-04-20T22:44:38.394005Z","shell.execute_reply.started":"2022-04-20T22:44:34.399729Z","shell.execute_reply":"2022-04-20T22:44:38.393238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(model_lem.predict(df_lem_test.to_numpy()), '1 BoW_lem.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:18.46426Z","iopub.execute_input":"2022-04-20T23:50:18.46557Z","iopub.status.idle":"2022-04-20T23:50:19.01368Z","shell.execute_reply.started":"2022-04-20T23:50:18.4655Z","shell.execute_reply":"2022-04-20T23:50:19.012763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Embedding layer + Recurrent layer (LSTM) and fitting on sequences","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:19.015739Z","iopub.execute_input":"2022-04-20T23:50:19.016031Z","iopub.status.idle":"2022-04-20T23:50:19.022125Z","shell.execute_reply.started":"2022-04-20T23:50:19.016Z","shell.execute_reply":"2022-04-20T23:50:19.021504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text feature\n\ntokenizer_text = Tokenizer()\ntokenizer_text.fit_on_texts(corpus_text)\nseq_text = tokenizer_text.texts_to_sequences(corpus_text)\nvocab_size_text = len(tokenizer_text.word_index)\n\nmax_len_text = 50\nseq_text = pad_sequences(seq_text, maxlen = max_len_text, truncating = 'post', padding = 'post')\nvocab_size_text","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:19.025297Z","iopub.execute_input":"2022-04-20T23:50:19.026219Z","iopub.status.idle":"2022-04-20T23:50:19.595452Z","shell.execute_reply.started":"2022-04-20T23:50:19.026157Z","shell.execute_reply":"2022-04-20T23:50:19.594553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### lemmatizated text corpus","metadata":{}},{"cell_type":"code","source":"tokenizer_text_lem = Tokenizer()\ntokenizer_text_lem.fit_on_texts(corpus_text_lem)\nmatrix_text_lem = tokenizer_text_lem.texts_to_matrix(corpus_text_lem)\nseq_text_lem = tokenizer_text_lem.texts_to_sequences(corpus_text_lem)\nvocab_size_text_lem = len(tokenizer_text_lem.word_index)\n\nseq_text_lem = pad_sequences(seq_text_lem, maxlen = max_len_text, truncating = 'post', padding = 'post')\nvocab_size_text_lem","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:19.597418Z","iopub.execute_input":"2022-04-20T23:50:19.597711Z","iopub.status.idle":"2022-04-20T23:50:20.556155Z","shell.execute_reply.started":"2022-04-20T23:50:19.597673Z","shell.execute_reply":"2022-04-20T23:50:20.555616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# original sequences\nseq_text_train = seq_text[:train_data.shape[0]]\nseq_text_test = seq_text[train_data.shape[0]:]\n\n#lemmatized text \nseq_text_lem_train = seq_text_lem[:train_data.shape[0]]\nseq_text_lem_test = seq_text_lem[train_data.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:20.557147Z","iopub.execute_input":"2022-04-20T23:50:20.5578Z","iopub.status.idle":"2022-04-20T23:50:20.561978Z","shell.execute_reply.started":"2022-04-20T23:50:20.557774Z","shell.execute_reply":"2022-04-20T23:50:20.561238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. GloVe","metadata":{}},{"cell_type":"code","source":"# load 100 dimensional GloVe list of words\n\nembedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:20.563563Z","iopub.execute_input":"2022-04-20T23:50:20.563747Z","iopub.status.idle":"2022-04-20T23:50:33.334135Z","shell.execute_reply.started":"2022-04-20T23:50:20.563721Z","shell.execute_reply":"2022-04-20T23:50:33.333247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(embedding_dict.items())[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:33.33532Z","iopub.execute_input":"2022-04-20T23:50:33.335549Z","iopub.status.idle":"2022-04-20T23:50:33.461015Z","shell.execute_reply.started":"2022-04-20T23:50:33.33552Z","shell.execute_reply":"2022-04-20T23:50:33.459981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this part here just to play with GloVe\nfrom scipy import spatial\n\ndef find_similar_word(emmbedes):\n    return sorted(embedding_dict.keys(), \n                  key=lambda word: spatial.distance.euclidean(embedding_dict[word], emmbedes))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:33.46251Z","iopub.execute_input":"2022-04-20T23:50:33.46319Z","iopub.status.idle":"2022-04-20T23:50:33.469453Z","shell.execute_reply.started":"2022-04-20T23:50:33.463146Z","shell.execute_reply":"2022-04-20T23:50:33.468667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this code is here just to play with GloVe\nfind_similar_word(embedding_dict['pilot'])[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:33.470841Z","iopub.execute_input":"2022-04-20T23:50:33.471056Z","iopub.status.idle":"2022-04-20T23:50:38.697992Z","shell.execute_reply.started":"2022-04-20T23:50:33.471027Z","shell.execute_reply":"2022-04-20T23:50:38.697115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text feature\nvocab_size = vocab_size_text + 1\nembedding_matrix_text = np.zeros((vocab_size, 100))\n\nfor word,i in tokenizer_text.word_index.items():\n    if i > vocab_size:\n        continue\n    \n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix_text[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:38.700527Z","iopub.execute_input":"2022-04-20T23:50:38.700739Z","iopub.status.idle":"2022-04-20T23:50:38.741446Z","shell.execute_reply.started":"2022-04-20T23:50:38.700711Z","shell.execute_reply":"2022-04-20T23:50:38.740684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text feature\nvocab_size_lem = vocab_size_text_lem + 1\nembedding_matrix_text_lem = np.zeros((vocab_size_lem, 100))\n\nfor word,i in tokenizer_text_lem.word_index.items():\n    if i > vocab_size_lem:\n        continue\n    \n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix_text_lem[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:38.742479Z","iopub.execute_input":"2022-04-20T23:50:38.74267Z","iopub.status.idle":"2022-04-20T23:50:38.779489Z","shell.execute_reply.started":"2022-04-20T23:50:38.742645Z","shell.execute_reply":"2022-04-20T23:50:38.778645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# Embedding(GloVe) + Recurrent layer (LSTM)\n\nmodel = Sequential(\n        [\n            Embedding(vocab_size, 100, input_length = max_len_text, \n                      embeddings_initializer = Constant(embedding_matrix_text), trainable = False),\n            LSTM(70),\n            Dense(64, activation = 'relu'),\n            Dense(1, activation = 'sigmoid')\n        ])\n\noptimzer = Adam(learning_rate = 1e-4)\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimzer, metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:38.782903Z","iopub.execute_input":"2022-04-20T23:50:38.783136Z","iopub.status.idle":"2022-04-20T23:50:39.094227Z","shell.execute_reply.started":"2022-04-20T23:50:38.783107Z","shell.execute_reply":"2022-04-20T23:50:39.093352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acc ~0.80\nhistory = model.fit(x = seq_text_train, y = y, batch_size = 128, epochs = 15, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:50:39.095645Z","iopub.execute_input":"2022-04-20T23:50:39.095911Z","iopub.status.idle":"2022-04-20T23:51:16.676857Z","shell.execute_reply.started":"2022-04-20T23:50:39.095881Z","shell.execute_reply":"2022-04-20T23:51:16.676154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding(GloVe) + Recurrent layer (LSTM) + Lemmatization\n\nmodel_lem = Sequential(\n        [\n            Embedding(vocab_size_lem, 100, input_length = max_len_text, \n                      embeddings_initializer = Constant(embedding_matrix_text_lem), trainable = False),\n            SpatialDropout1D(0.1),\n            LSTM(80),\n            Dense(64, activation = 'relu'),\n            Dense(1, activation = 'sigmoid')\n        ])\n\noptimzer = Adam(learning_rate = 1e-4)\nmodel_lem.compile(loss = 'binary_crossentropy', optimizer = optimzer, metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:51:16.678163Z","iopub.execute_input":"2022-04-20T23:51:16.678548Z","iopub.status.idle":"2022-04-20T23:51:16.951162Z","shell.execute_reply.started":"2022-04-20T23:51:16.678517Z","shell.execute_reply":"2022-04-20T23:51:16.950064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acc ~0.80\nhistory = model_lem.fit(x = seq_text_lem_train, y = y, batch_size = 128, epochs = 15, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:51:16.952655Z","iopub.execute_input":"2022-04-20T23:51:16.953516Z","iopub.status.idle":"2022-04-20T23:51:58.205372Z","shell.execute_reply.started":"2022-04-20T23:51:16.953459Z","shell.execute_reply":"2022-04-20T23:51:58.204494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(model.predict(seq_text_test), '2 Glove.csv')\nsubmission(model_lem.predict(seq_text_lem_test), '3 Glove + Lemm.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:51:58.206774Z","iopub.execute_input":"2022-04-20T23:51:58.207521Z","iopub.status.idle":"2022-04-20T23:52:01.33544Z","shell.execute_reply.started":"2022-04-20T23:51:58.207481Z","shell.execute_reply":"2022-04-20T23:52:01.334824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix_feat = pd.concat([data_kw, data_mentions, data_hashtags], axis = 1).to_numpy() # remove data_url\n\nmatrix_train = matrix_feat[:train_data.shape[0]]\nmatrix_test = matrix_feat[train_data.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T00:23:11.036002Z","iopub.execute_input":"2022-04-21T00:23:11.036421Z","iopub.status.idle":"2022-04-21T00:23:11.071486Z","shell.execute_reply.started":"2022-04-21T00:23:11.036373Z","shell.execute_reply":"2022-04-21T00:23:11.070629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_text_train.shape, matrix_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T00:23:11.34392Z","iopub.execute_input":"2022-04-21T00:23:11.344838Z","iopub.status.idle":"2022-04-21T00:23:11.350333Z","shell.execute_reply.started":"2022-04-21T00:23:11.344783Z","shell.execute_reply":"2022-04-21T00:23:11.349792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets try to improve score adding other features\n\ninput_text = Input(shape = (max_len_text, ))\nemb_layer = Embedding(vocab_size, 100, embeddings_initializer = Constant(embedding_matrix_text), trainable = False)(input_text)\nlstm_layer = LSTM(70)(emb_layer)\ndense_layer_1 = Dense(64, activation = 'relu')(lstm_layer)\n\ninput_feat = Input(shape = (matrix_feat.shape[1], ))\ndense_layer_2 = Dense(64, activation = 'relu')(input_feat)\n\nconcat_layer = Concatenate()([dense_layer_1, dense_layer_2])\ndense_layer_3 = Dense(16, activation = 'relu')(concat_layer)\noutput = Dense(1, activation = 'sigmoid')(dense_layer_3)\n\nmodel = Model(inputs = [input_text, input_feat], outputs = output)\n\noptimzer = Adam(learning_rate = 1e-4)\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimzer, metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T00:23:11.62006Z","iopub.execute_input":"2022-04-21T00:23:11.620764Z","iopub.status.idle":"2022-04-21T00:23:11.839101Z","shell.execute_reply.started":"2022-04-21T00:23:11.620723Z","shell.execute_reply":"2022-04-21T00:23:11.838382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acc ~0.80\nhistory = model.fit(x = [seq_text_train, matrix_train], y = y, batch_size = 128, epochs = 15, validation_split = 0.2, verbose = 0)\nprint_loss_acc(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T00:23:12.007321Z","iopub.execute_input":"2022-04-21T00:23:12.007572Z","iopub.status.idle":"2022-04-21T00:23:50.623266Z","shell.execute_reply.started":"2022-04-21T00:23:12.007544Z","shell.execute_reply":"2022-04-21T00:23:50.622369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(model.predict([seq_text_test, matrix_test]), '4 Glove + Feat.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T00:08:45.127003Z","iopub.execute_input":"2022-04-21T00:08:45.127426Z","iopub.status.idle":"2022-04-21T00:08:46.762734Z","shell.execute_reply.started":"2022-04-21T00:08:45.127389Z","shell.execute_reply":"2022-04-21T00:08:46.76188Z"},"trusted":true},"execution_count":null,"outputs":[]}]}